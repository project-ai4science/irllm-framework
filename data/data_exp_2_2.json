[
  {
    "id":"2411.01019",
    "a_title":"A lightweight Convolutional Neural Network based on U shape structure\n  and Attention Mechanism for Anterior Mediastinum Segmentation",
    "a_abstract":"To automatically detect Anterior Mediastinum Lesions (AMLs) in the Anterior\nMediastinum (AM), the primary requirement will be an automatic segmentation\nmodel specifically designed for the AM. The prevalence of AML is extremely low,\nmaking it challenging to conduct screening research similar to lung cancer\nscreening. Retrospectively reviewing chest CT scans over a specific period to\ninvestigate the prevalence of AML requires substantial time. Therefore,\ndeveloping an Artificial Intelligence (AI) model to find location of AM helps\nradiologist to enhance their ability to manage workloads and improve diagnostic\naccuracy for AMLs. In this paper, we introduce a U-shaped structure network to\nsegment AM. Two attention mechanisms were used for maintaining long-range\ndependencies and localization. In order to have the potential of Multi-Head\nSelf-Attention (MHSA) and a lightweight network, we designed a parallel MHSA\nnamed Wide-MHSA (W-MHSA). Maintaining long-range dependencies is crucial for\nsegmentation when we upsample feature maps. Therefore, we designed a Dilated\nDepth-Wise Parallel Path connection (DDWPP) for this purpose. In order to\ndesign a lightweight architecture, we introduced an expanding convolution block\nand combine it with the proposed W-MHSA for feature extraction in the encoder\npart of the proposed U-shaped network. The proposed network was trained on 2775\nAM cases, which obtained an average Dice Similarity Coefficient (DSC) of\n87.83%, mean Intersection over Union (IoU) of 79.16%, and Sensitivity of\n89.60%. Our proposed architecture exhibited superior segmentation performance\ncompared to the most advanced segmentation networks, such as Trans Unet,\nAttention Unet, Res Unet, and Res Unet++.",
    "explanation":"It's leveraging AI in another diffrent domain, namely medical science, to detect Anterior Mediastinum Lesions.",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b6",
      "b1"
    ],
    "c_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "c_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "c_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":[
      "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats"
    ],
    "c_abstract":[
      "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10302",
    "c_title":[
      "Pushing the Boundary of Quantum Advantage in Hard Combinatorial\n  Optimization with Probabilistic Computers"
    ],
    "c_abstract":[
      "Recent demonstrations on specialized benchmarks have reignited excitement for\nquantum computers, yet whether they can deliver an advantage for practical\nreal-world problems remains an open question. Here, we show that probabilistic\ncomputers (p-computers) when co-designed with hardware to implement powerful\nMonte Carlo algorithms surpass state-of-the-art quantum annealers\n[\\href{https:\/\/www.nature.com\/articles\/s41586-023-05867-2}{King et al., Nature\n(2023)}] in solving hard optimization problems. We focus on two key algorithms:\ndiscrete-time simulated quantum annealing (DT-SQA) and adaptive parallel\ntempering (APT), both applied to 3D spin glasses. For DT-SQA, we find that\nincreasing the number of replicas improves residual energy scaling, while\nparallelizing fewer replicas across independent runs also achieves comparable\nscaling. Both strategies align with the theoretical expectations from extreme\nvalue theory. In addition, APT outperforms DT-SQA when supported by non-local\nisoenergetic cluster moves. Finite-size scaling analysis suggests a universal\nbehavior that explains the superior performance of APT over both DT-SQA and\nquantum annealing. We show that these algorithms are readily implementable in\nmodern hardware thanks to the mature semiconductor technology. Unlike software\nsimulations, replicas can be monolithically housed on a single chip and a large\nnumber of spins can be updated in parallel and asynchronously, similar to a\nquantum annealer. We project that custom Field Programmable Gate Arrays (FPGA)\nor specialized chips leveraging massive parallelism can further accelerate\nthese algorithms by orders of magnitude, while drastically improving energy\nefficiency. Our results challenge the notion of a practical quantum advantage\nin optimization and present p-computers as scalable, energy-efficient hardware\nfor real-world optimization problems."
    ],
    "c_categories":[
      "cond-mat.dis-nn",
      "cs.ET",
      "quant-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17264",
    "c_title":[
      "Constructing Simultaneous Confidence Bands for Errors-in-variables\n  Curves with Application to the Lorenz Curve"
    ],
    "c_abstract":[
      "Errors-in-variables curves are curves where errors exist not only in the\nindependent variable but also in the dependent variable. We address the\nchallenge of constructing simultaneous confidence bands (SCBs) for such curves.\nOur method finds application in the Lorenz curve, which represents the\nconcentration of income or wealth. Unlike ordinary regression curves, the\nLorenz curve incorporates errors in its explanatory variable and requires a\nfundamentally different treatment. To the best of our knowledge, the\ndevelopment of SCBs for such curves has not been explored in previous research.\nUsing the Lorenz curve as a case study, this paper proposes a novel approach to\naddress this challenge."
    ],
    "c_categories":[
      "stat.AP",
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18849",
    "c_title":[
      "Fourier analysis of equivariant quantum cohomology"
    ],
    "c_abstract":[
      "Equivariant quantum cohomology possesses the structure of a difference module\nby shift operators (Seidel representation) of equivariant parameters. Teleman's\nconjecture suggests that shift operators and equivariant parameters acting on\nQH_T(X) should be identified, respectively, with the Novikov variables and the\nquantum connection of the GIT quotient X\/\/T. This can be interpreted as a form\nof Fourier duality between equivariant quantum cohomology (D-module) of X and\nquantum cohomology (D-module) of the GIT quotient X\/\/T.\n  We introduce the notion of \"quantum volume,\" derived from Givental's path\nintegral over the Floer fundamental cycle, and present a conjectural Fourier\nduality relationship between the T-equivariant quantum volume of X and the\nquantum volume of X\/\/T. We also explore the \"reduction conjecture,\" developed\nin collaboration with Fumihiko Sanda, which expresses the I-function of X\/\/T as\na discrete Fourier transform of the equivariant J-function of X. Furthermore,\nwe demonstrate how to use Fourier analysis of equivariant quantum cohomology to\nobserve toric mirror symmetry and prove a decomposition of quantum cohomology\nD-modules of projective bundles or blowups."
    ],
    "c_categories":[
      "hep-th",
      "math.AG",
      "math.SG"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05372",
    "c_title":[
      "Self-diffusion in isotropic and liquid crystalline phases of fd virus\n  colloidal rods: a combined single particle tracking and differential dynamic\n  microscopy study"
    ],
    "c_abstract":[
      "In this article, we investigate the dynamics of self-organised suspensions\nformed by rod-like fd virus colloids. Two methods have been employed for\nanalysing fluorescence microscopy movies: single particle tracking (SPT) in\ndirect space and differential dynamic microscopy (DDM) in reciprocal space. We\nperform a quantitative analysis on this anisotropic system with complex\ndiffusion across different self-assembled states, ranging from dilute and\nsemi-dilute liquids to nematic and smectic organisations. By leveraging the\ncomplementary strengths of SPT and DDM, we provide new insights in the dynamics\nof viral colloidal rods, such as long time diffusion coefficients in the\nsmectic phase. We further discuss the advantages and limitations of both\nmethods for studying the intricate dynamics of anisotropic colloidal systems."
    ],
    "c_categories":[
      "cond-mat.soft"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.13043",
    "c_title":[
      "AInstein: Numerical Einstein Metrics via Machine Learning"
    ],
    "c_abstract":[
      "A new semi-supervised machine learning package is introduced which\nsuccessfully solves the Euclidean vacuum Einstein equations with a cosmological\nconstant, without any symmetry assumptions. The model architecture contains\nsubnetworks for each patch in the manifold-defining atlas. Each subnetwork\npredicts the components of a metric in that patch, with the associated Einstein\nconditions, of the form $R_{\\mu \\nu} - \\lambda g_{\\mu \\nu} = 0$, being used as\nindependent loss components; in our conventions, $\\mu,\\nu = 1, 2, \\cdots, n$,\nwhere $n$ is the dimension of the Riemannian manifold and $\\lambda \\in \\{+1, 0,\n-1\\}$. To ensure the consistency of the global structure of the manifold,\nanother loss component is introduced across the patch subnetworks which\nenforces the coordinate transformation between the patches, $ g' = J g J^T$,\nfor an appropriate analytically known Jacobian $J$. We test our method for the\ncase of spheres represented with 2 patches in dimensions $2,3,4,5$; in\ndimensions $2, 3$ the geometries have been fully classified, however it is\nunknown whether a Ricci-flat metric can be put on spheres in dimensions $4, 5$,\nwhich we provide numerical evidence against."
    ],
    "c_categories":[
      "gr-qc",
      "hep-th",
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07709",
    "c_title":[
      "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
    ],
    "c_abstract":[
      "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08177",
    "c_title":[
      "SycEval: Evaluating LLM Sycophancy"
    ],
    "c_abstract":[
      "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15726",
    "c_title":[
      "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat"
    ],
    "c_abstract":[
      "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10768",
    "c_title":[
      "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science"
    ],
    "c_abstract":[
      "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07487",
    "c_title":[
      "Data and System Perspectives of Sustainable Artificial Intelligence"
    ],
    "c_abstract":[
      "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06450",
    "c_title":[
      "Statistical Inference of the Matthews Correlation Coefficient for\n  Multiclass Classification"
    ],
    "c_abstract":[
      "Classification problems are essential statistical tasks that form the\nfoundation of decision-making across various fields, including patient\nprognosis and treatment strategies for critical conditions. Consequently,\nevaluating the performance of classification models is of significant\nimportance, and numerous evaluation metrics have been proposed. Among these,\nthe Matthews correlation coefficient (MCC), also known as the phi coefficient,\nis widely recognized as a reliable metric that provides balanced measurements\neven in the presence of class imbalance. However, with the increasing\nprevalence of multiclass classification problems involving three or more\nclasses, macro-averaged and micro-averaged extensions of MCC have been\nemployed, despite a lack of clear definitions or established references for\nthese extensions. In the present study, we propose a formal framework for MCC\ntailored to multiclass classification problems using macro-averaged and\nmicro-averaged approaches. Moreover, discussions on the use of these extended\nMCCs for multiclass problems often rely solely on point estimates, potentially\noverlooking the statistical significance and reliability of the results. To\naddress this gap, we introduce several methods for constructing asymptotic\nconfidence intervals for the proposed metrics. Furthermore, we extend these\nmethods to include the construction of asymptotic confidence intervals for\ndifferences in the proposed metrics, specifically for paired study designs. The\nutility of our methods is evaluated through comprehensive simulations and\nreal-world data analyses."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.16156",
    "c_title":[
      "A Review of Causal Decision Making"
    ],
    "c_abstract":[
      "To make effective decisions, it is important to have a thorough understanding\nof the causal relationships among actions, environments, and outcomes. This\nreview aims to surface three crucial aspects of decision-making through a\ncausal lens: 1) the discovery of causal relationships through causal structure\nlearning, 2) understanding the impacts of these relationships through causal\neffect learning, and 3) applying the knowledge gained from the first two\naspects to support decision making via causal policy learning. Moreover, we\nidentify challenges that hinder the broader utilization of causal\ndecision-making and discuss recent advances in overcoming these challenges.\nFinally, we provide future research directions to address these challenges and\nto further enhance the implementation of causal decision-making in practice,\nwith real-world applications illustrated based on the proposed causal\ndecision-making. We aim to offer a comprehensive methodology and practical\nimplementation framework by consolidating various methods in this area into a\nPython-based collection. URL:\nhttps:\/\/causaldm.github.io\/Causal-Decision-Making."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19817",
    "c_title":[
      "Axion dark matter searches from the standard halo over the tidal stream\n  to the big flow"
    ],
    "c_abstract":[
      "The sensitivity of axion dark matter searches depends on the signal\n  window that results from the velocity dispersion of axion dark\n  matter. Since the ratio of signal windows is about 6500 between the\n  standard halo and the big flow axion dark matter, each axion dark\n  matter search usually uses a separate data acquisition (DAQ) channel\n  with a different frequency resolution bandwidth (RBW).\n  In this work, we demonstrate axion dark matter searches covering the\n  standard halo, the tidal stream, and the big flow employing a DAQ\n  channel starting with a single high resolution RBW, without\n  sacrificing the DAQ efficiency, where the DAQ process includes\n  online fast Fourier transforms and writing the outputs to disk.\n  Assuming the total amount of data is sensitive to\n  Dine-Fischler-Srednicki-Zhitnitskii (DFSZ) axion dark matter that\n  follows the standard halo model and makes up 100\\% of the local dark\n  matter density, the same data can also be used for the tidal stream\n  and the big flow axion dark matter searches that would be sensitive\n  to DFSZ axion dark matter that constitute 19.2\\% and 12.4\\% of the\n  local dark matter densities, respectively, at a 90\\% confidence\n  level.\n  We also report that the filtering of the individual power spectra\n  acquired with a relatively high resolution RBW e.g., for the big\n  flow search can prevent a possible significant degradation in the\n  signal to noise ratio from the searches in the lower resolution\n  RBW's, i.e., the standard halo and tidal stream searches."
    ],
    "c_categories":[
      "astro-ph.CO",
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17738",
    "c_title":[
      "White's conjecture for matroids and inner projections"
    ],
    "c_abstract":[
      "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties."
    ],
    "c_categories":[
      "math.AC",
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.02326",
    "c_title":[
      "Rotational Flow Dominates Abrupt Seasonal Change in Zonally Asymmetric\n  Tropical Meridional Circulation"
    ],
    "c_abstract":[
      "The seasonality of the tropical meridional circulation evolves differently\nacross different regions, governs the onset and retreat of monsoons and\nmigration of tropical precipitation, thereby influencing agricultural\nproductivity and disaster preparedness in the tropics and subtropics. By\ndefining a pseudo meridional overturning streamfunction ({\\Psi}pseudo) and\ndefining a new vector-type, dual-component index (ASCI), we diagnose zonally\nasymmetric abrupt seasonal change (ASC) of tropical meridional circulation.\n{\\Psi}pseudo converges to traditional, meridional overturning streamfunction\n({\\Psi}m) after being averaged over a zonal circle around any latitude. By\napplying the Helmholtz decomposition to horizontal velocity fields so as to\ndecompose {\\Psi}pseudo into rotational and divergent components, we\nquantitatively compare the contributions of horizontally rotational and\ndivergent flows to the abrupt seasonal change. We find that the zonal sectors\nassociated with strong deep convection exhibit the most pronounced ASC of\ntropical meridional circulation, and all of subregions exhibiting ASC contain\nlandmass with low heat inertia. Particularly, in contrast to the case of\nzonally symmetric Hadley cell, rotational flow, rather than the thermal-direct\ndivergent flow, dominates the zonally asymmetric ASC in the tropics, although\nthe divergent flow also contributes to the ASC over the zonal sectors\nassociated with deep convection. We suggest that the interplay between tropical\nRossby-type eddies with extratropical eddies and tropical circulation is\nessential to the zonally asymmetric ASC of tropical Hadley circulation."
    ],
    "c_categories":[
      "physics.ao-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"BarcodeMamba: State Space Models for Biodiversity Analysis",
    "a_abstract":"DNA barcodes are crucial in biodiversity analysis for building automatic\nidentification systems that recognize known species and discover unseen\nspecies. Unlike human genome modeling, barcode-based invertebrate\nidentification poses challenges in the vast diversity of species and taxonomic\ncomplexity. Among Transformer-based foundation models, BarcodeBERT excelled in\nspecies-level identification of invertebrates, highlighting the effectiveness\nof self-supervised pretraining on barcode-specific datasets. Recently,\nstructured state space models (SSMs) have emerged, with a time complexity that\nscales sub-quadratically with the context length. SSMs provide an efficient\nparameterization of sequence modeling relative to attention-based\narchitectures. Given the success of Mamba and Mamba-2 in natural language, we\ndesigned BarcodeMamba, a performant and efficient foundation model for DNA\nbarcodes in biodiversity analysis. We conducted a comprehensive ablation study\non the impacts of self-supervised training and tokenization methods, and\ncompared both versions of Mamba layers in terms of expressiveness and their\ncapacity to identify \"unseen\" species held back from training. Our study shows\nthat BarcodeMamba has better performance than BarcodeBERT even when using only\n8.3% as many parameters, and improves accuracy to 99.2% on species-level\naccuracy in linear probing without fine-tuning for \"seen\" species. In our\nscaling study, BarcodeMamba with 63.6% of BarcodeBERT's parameters achieved\n70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing for unseen\nspecies. The code repository to reproduce our experiments is available at\nhttps:\/\/github.com\/bioscan-ml\/BarcodeMamba.",
    "explanation":"The paper talks about the use of BarcodeMamba for better scores in DNA barcode analysis of genomes.",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b0"
    ],
    "c_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "c_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15985",
    "c_title":[
      "Exploring the Reliability of Self-explanation and its Relationship with\n  Classification in Language Model-driven Financial Analysis"
    ],
    "c_abstract":[
      "Language models (LMs) have exhibited exceptional versatility in reasoning and\nin-depth financial analysis through their proprietary information processing\ncapabilities. Previous research focused on evaluating classification\nperformance while often overlooking explainability or pre-conceived that\nrefined explanation corresponds to higher classification accuracy. Using a\npublic dataset in finance domain, we quantitatively evaluated self-explanations\nby LMs, focusing on their factuality and causality. We identified the\nstatistically significant relationship between the accuracy of classifications\nand the factuality or causality of self-explanations. Our study built an\nempirical foundation for approximating classification confidence through\nself-explanations and for optimizing classification via proprietary reasoning."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06479",
    "c_title":[
      "ExKG-LLM: Leveraging Large Language Models for Automated Expansion of\n  Cognitive Neuroscience Knowledge Graphs"
    ],
    "c_abstract":[
      "The paper introduces ExKG-LLM, a framework designed to automate the expansion\nof cognitive neuroscience knowledge graphs (CNKG) using large language models\n(LLMs). It addresses limitations in existing tools by enhancing accuracy,\ncompleteness, and usefulness in CNKG. The framework leverages a large dataset\nof scientific papers and clinical reports, applying state-of-the-art LLMs to\nextract, optimize, and integrate new entities and relationships. Evaluation\nmetrics include precision, recall, and graph density. Results show significant\nimprovements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score\n(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density\nslightly decreased, reflecting a broader but more fragmented structure.\nEngagement rates rose by 20%, while CNKG diameter increased to 15, indicating a\nmore distributed structure. Time complexity improved to O(n log n), but space\ncomplexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates\npotential for enhancing knowledge generation, semantic search, and clinical\ndecision-making in cognitive neuroscience, adaptable to broader scientific\nfields."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11951",
    "c_title":[
      "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning"
    ],
    "c_abstract":[
      "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04512",
    "c_title":[
      "Safety is Essential for Responsible Open-Ended Systems"
    ],
    "c_abstract":[
      "AI advancements have been significantly driven by a combination of foundation\nmodels and curiosity-driven learning aimed at increasing capability and\nadaptability. A growing area of interest within this field is Open-Endedness -\nthe ability of AI systems to continuously and autonomously generate novel and\ndiverse artifacts or solutions. This has become relevant for accelerating\nscientific discovery and enabling continual adaptation in AI agents. This\nposition paper argues that the inherently dynamic and self-propagating nature\nof Open-Ended AI introduces significant, underexplored risks, including\nchallenges in maintaining alignment, predictability, and control. This paper\nsystematically examines these challenges, proposes mitigation strategies, and\ncalls for action for different stakeholders to support the safe, responsible\nand successful development of Open-Ended AI."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07374",
    "c_title":[
      "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!"
    ],
    "c_abstract":[
      "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06450",
    "c_title":[
      "Statistical Inference of the Matthews Correlation Coefficient for\n  Multiclass Classification"
    ],
    "c_abstract":[
      "Classification problems are essential statistical tasks that form the\nfoundation of decision-making across various fields, including patient\nprognosis and treatment strategies for critical conditions. Consequently,\nevaluating the performance of classification models is of significant\nimportance, and numerous evaluation metrics have been proposed. Among these,\nthe Matthews correlation coefficient (MCC), also known as the phi coefficient,\nis widely recognized as a reliable metric that provides balanced measurements\neven in the presence of class imbalance. However, with the increasing\nprevalence of multiclass classification problems involving three or more\nclasses, macro-averaged and micro-averaged extensions of MCC have been\nemployed, despite a lack of clear definitions or established references for\nthese extensions. In the present study, we propose a formal framework for MCC\ntailored to multiclass classification problems using macro-averaged and\nmicro-averaged approaches. Moreover, discussions on the use of these extended\nMCCs for multiclass problems often rely solely on point estimates, potentially\noverlooking the statistical significance and reliability of the results. To\naddress this gap, we introduce several methods for constructing asymptotic\nconfidence intervals for the proposed metrics. Furthermore, we extend these\nmethods to include the construction of asymptotic confidence intervals for\ndifferences in the proposed metrics, specifically for paired study designs. The\nutility of our methods is evaluated through comprehensive simulations and\nreal-world data analyses."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.16156",
    "c_title":[
      "A Review of Causal Decision Making"
    ],
    "c_abstract":[
      "To make effective decisions, it is important to have a thorough understanding\nof the causal relationships among actions, environments, and outcomes. This\nreview aims to surface three crucial aspects of decision-making through a\ncausal lens: 1) the discovery of causal relationships through causal structure\nlearning, 2) understanding the impacts of these relationships through causal\neffect learning, and 3) applying the knowledge gained from the first two\naspects to support decision making via causal policy learning. Moreover, we\nidentify challenges that hinder the broader utilization of causal\ndecision-making and discuss recent advances in overcoming these challenges.\nFinally, we provide future research directions to address these challenges and\nto further enhance the implementation of causal decision-making in practice,\nwith real-world applications illustrated based on the proposed causal\ndecision-making. We aim to offer a comprehensive methodology and practical\nimplementation framework by consolidating various methods in this area into a\nPython-based collection. URL:\nhttps:\/\/causaldm.github.io\/Causal-Decision-Making."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19817",
    "c_title":[
      "Axion dark matter searches from the standard halo over the tidal stream\n  to the big flow"
    ],
    "c_abstract":[
      "The sensitivity of axion dark matter searches depends on the signal\n  window that results from the velocity dispersion of axion dark\n  matter. Since the ratio of signal windows is about 6500 between the\n  standard halo and the big flow axion dark matter, each axion dark\n  matter search usually uses a separate data acquisition (DAQ) channel\n  with a different frequency resolution bandwidth (RBW).\n  In this work, we demonstrate axion dark matter searches covering the\n  standard halo, the tidal stream, and the big flow employing a DAQ\n  channel starting with a single high resolution RBW, without\n  sacrificing the DAQ efficiency, where the DAQ process includes\n  online fast Fourier transforms and writing the outputs to disk.\n  Assuming the total amount of data is sensitive to\n  Dine-Fischler-Srednicki-Zhitnitskii (DFSZ) axion dark matter that\n  follows the standard halo model and makes up 100\\% of the local dark\n  matter density, the same data can also be used for the tidal stream\n  and the big flow axion dark matter searches that would be sensitive\n  to DFSZ axion dark matter that constitute 19.2\\% and 12.4\\% of the\n  local dark matter densities, respectively, at a 90\\% confidence\n  level.\n  We also report that the filtering of the individual power spectra\n  acquired with a relatively high resolution RBW e.g., for the big\n  flow search can prevent a possible significant degradation in the\n  signal to noise ratio from the searches in the lower resolution\n  RBW's, i.e., the standard halo and tidal stream searches."
    ],
    "c_categories":[
      "astro-ph.CO",
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17738",
    "c_title":[
      "White's conjecture for matroids and inner projections"
    ],
    "c_abstract":[
      "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties."
    ],
    "c_categories":[
      "math.AC",
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.02326",
    "c_title":[
      "Rotational Flow Dominates Abrupt Seasonal Change in Zonally Asymmetric\n  Tropical Meridional Circulation"
    ],
    "c_abstract":[
      "The seasonality of the tropical meridional circulation evolves differently\nacross different regions, governs the onset and retreat of monsoons and\nmigration of tropical precipitation, thereby influencing agricultural\nproductivity and disaster preparedness in the tropics and subtropics. By\ndefining a pseudo meridional overturning streamfunction ({\\Psi}pseudo) and\ndefining a new vector-type, dual-component index (ASCI), we diagnose zonally\nasymmetric abrupt seasonal change (ASC) of tropical meridional circulation.\n{\\Psi}pseudo converges to traditional, meridional overturning streamfunction\n({\\Psi}m) after being averaged over a zonal circle around any latitude. By\napplying the Helmholtz decomposition to horizontal velocity fields so as to\ndecompose {\\Psi}pseudo into rotational and divergent components, we\nquantitatively compare the contributions of horizontally rotational and\ndivergent flows to the abrupt seasonal change. We find that the zonal sectors\nassociated with strong deep convection exhibit the most pronounced ASC of\ntropical meridional circulation, and all of subregions exhibiting ASC contain\nlandmass with low heat inertia. Particularly, in contrast to the case of\nzonally symmetric Hadley cell, rotational flow, rather than the thermal-direct\ndivergent flow, dominates the zonally asymmetric ASC in the tropics, although\nthe divergent flow also contributes to the ASC over the zonal sectors\nassociated with deep convection. We suggest that the interplay between tropical\nRossby-type eddies with extratropical eddies and tropical circulation is\nessential to the zonally asymmetric ASC of tropical Hadley circulation."
    ],
    "c_categories":[
      "physics.ao-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04869",
    "c_title":[
      "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer"
    ],
    "c_abstract":[
      "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.21125",
    "c_title":[
      "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes"
    ],
    "c_abstract":[
      "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18758",
    "c_title":[
      "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features"
    ],
    "c_abstract":[
      "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17418",
    "c_title":[
      "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns"
    ],
    "c_abstract":[
      "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11217",
    "c_title":[
      "CoverM: Read alignment statistics for metagenomics"
    ],
    "c_abstract":[
      "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02510",
    "c_title":[
      "A note on conformal-biharmonic hypersurfaces"
    ],
    "c_abstract":[
      "The conformal bienergy functional $E_2^c$ was recently introduced as a\nmodified version of the classical bienergy functional $E_2$ in order to ensure\nthe validity of some conformal invariance properties. The critical points of\n$E_2^c$ are called conformal-biharmonic and denoted $c$-biharmonic. In this\npaper we study the $c$-biharmonic hypersurfaces $M^m$ with constant principal\ncurvatures in the product space $ {\\mathbb L}^m(\\varepsilon) \\times \\mathbb{R}\n$, where $ {\\mathbb L}^m(\\varepsilon) $ denotes a space form of constant\nsectional curvature $ \\varepsilon $. Specifically, we demonstrate that $ M^m $\nis either totally geodesic or a cylindrical hypersurface of the form $ M^{m-1}\n\\times \\mathbb{R} $, where $ M^{m-1} $ is a $c$-biharmonic isoparametric\nhypersurface in $ {\\mathbb L}^m(\\varepsilon) $. To provide further insight, we\ndescribe the structure of $c$-biharmonic isoparametric hypersurfaces in space\nforms. In the final part, as a preliminary effort to understand $c$-biharmonic\nhypersurfaces $ M^m $ in $ {\\mathbb L}^m(\\varepsilon) \\times \\mathbb{R} $ with\nnon constant mean curvature, we establish that a totally umbilical\n$c$-biharmonic hypersurface must necessarily be totally geodesic."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06091",
    "c_title":[
      "First Search for Neutral Current Coherent Single-Photon Production in\n  MicroBooNE"
    ],
    "c_abstract":[
      "This article presents the first search for neutrino-induced neutral current\ncoherent single-photon production (NC coherent 1$\\gamma$). The search makes use\nof data from the MicroBooNE 85-tonne active volume liquid argon time projection\nchamber detector, situated in the Fermilab Booster Neutrino Beam (BNB), with an\naverage neutrino energy of $\\langle E_{\\nu}\\rangle \\sim 0.8$ GeV. A targeted\nselection of candidate neutrino interactions with a single photon-like\nelectromagnetic shower in the final state and no visible vertex activity was\ndeveloped to search for the NC coherent 1$\\gamma$ process, along with two\nauxiliary selections used to constrain the dominant background from NC$\\pi^0$\nproduction. With an integrated exposure of $6.87 \\times 10^{20}$ protons on\ntarget delivered by the BNB, we set the world's first limit for this rare\nprocess, corresponding to an upper limit on the flux-averaged cross section of\n$\\sigma<1.49 \\times 10^{-41}\\text{cm}^2$ at 90\\% C.L."
    ],
    "c_categories":[
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07576",
    "c_title":[
      "One-loop matching for leading-twist generalised\n  transverse-momentum-dependent distributions"
    ],
    "c_abstract":[
      "We present the one-loop matching coefficients necessary to match all of the\nleading-twist generalised transverse-momentum-dependent distributions (GTMDs)\nonto generalised parton distributions (GPDs). Matching functions are extracted\nby computing the first radiative corrections to partonic bilocal correlators\nwith staple-like Wilson lines, as appropriate for high-energy collisions. These\ncorrelators are characterised by a transverse displacement and skewed\nkinematics of external states. Using the proton helicity basis, they are\nparametrised in terms of GTMDs, which are subsequently related to leading-twist\nGPDs. Our results provide new insights into the complex dynamics of GTMDs\ngenerated by radiative corrections. In particular, we show that time-reversal\neven and odd contributions to GTMDs in the so-called ERBL region mix both under\nmatching and evolution. Finally, we present a selection of numerical results\nand comment on the quantitative behaviour of GTMDs."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09332",
    "c_title":[
      "Automatic exposure volumetric additive manufacturing"
    ],
    "c_abstract":[
      "Tomographic volumetric additive manufacturing (VAM) achieves high print speed\nand design freedom by continuous volumetric light patterning. This differs from\ntraditional vat photopolymerization techniques that use brief sequential (2D)\nplane- or (1D) point-localized exposures. The drawback to volumetric light\npatterning is the small exposure window. Overexposure quickly leads to cured\nout-of-part voxels due to the nonzero background dose arising from light\nprojection through the build volume. For tomographic VAM, correct exposure time\nis critical to achieving high repeatability, however, we find that correct\nexposure time varies by nearly 40% depending on resin history. Currently,\ntomographic VAM exposure is timed based on subjective human determination of\nprint completion, which is tedious and yields poor repeatability. Here, we\nimplement a robust auto exposure routine for tomographic VAM using real-time\nprocessing of light scattering data, yielding accurate and repeatable prints\nwithout human intervention. The resulting print fidelity and repeatability\napproaches, and in some cases, exceeds that of commercial resin 3D printers. We\nshow that auto exposure VAM generalizes well to a wide variety of print\ngeometries with small positive and negative features. The repeatability and\naccuracy of auto exposure VAM allows for building multi-part objects,\nfulfilling a major requirement of additive manufacturing technologies."
    ],
    "c_categories":[
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14758",
    "c_title":[
      "Cyclic Voltammetry of Ion-Coupled Electron Transfer Reactions for\n  Diagnosing Energy Storage Materials"
    ],
    "c_abstract":[
      "The methods of Nicholson and Shain and Randles-Sevcik are the paradigms of\nvoltammetry of redox species. However, as they were originally developed for\naqueous redox couples, they cannot be directly applied to solid redox films\nsuch as those of battery materials. Herein, for the first time, we present a\ncyclic voltammetry model based on semi-infinite linear diffusion for\nion-coupled electron transfer reactions. The simulated CVs contain parameters\nsuch as capacity, ion activity, formal potential of proton-coupled electron\ntransfer, and scan rate that are physically more meaningful than those of the\ncurrent models in characterizing energy storage materials. We apply this model\nto the MnO2 cathode material as a proof of concept and discuss the significance\nof the simulation parameters for determining the energetics of the underlying\nphase transitions and the charge storage mechanisms. According to the present\nmodel, two linear regression lines can be established from relatively simple\nvoltammetry experiments for characterizing energy storage materials: 1) The\nregression line of the CV mid-peak potential vs. Log of ion activity (or pH),\nin which the slope and the intercept provide information on the type of\ncharge-carrier ions and their solvation state, respectively. 2) The regression\nline of the capacity vs. the inverse of the square root of scan rate, where the\nslope and intercept indicate the contributions of bulk and surface charges in\nthin redox films, respectively."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"Tumor Location-weighted MRI-Report Contrastive Learning: A Framework for\n  Improving the Explainability of Pediatric Brain Tumor Diagnosis",
    "a_abstract":"Despite the promising performance of convolutional neural networks (CNNs) in\nbrain tumor diagnosis from magnetic resonance imaging (MRI), their integration\ninto the clinical workflow has been limited. That is mainly due to the fact\nthat the features contributing to a model's prediction are unclear to\nradiologists and hence, clinically irrelevant, i.e., lack of explainability. As\nthe invaluable sources of radiologists' knowledge and expertise, radiology\nreports can be integrated with MRI in a contrastive learning (CL) framework,\nenabling learning from image-report associations, to improve CNN\nexplainability. In this work, we train a multimodal CL architecture on 3D brain\nMRI scans and radiology reports to learn informative MRI representations.\nFurthermore, we integrate tumor location, salient to several brain tumor\nanalysis tasks, into this framework to improve its generalizability. We then\napply the learnt image representations to improve explainability and\nperformance of genetic marker classification of pediatric Low-grade Glioma, the\nmost prevalent brain tumor in children, as a downstream task. Our results\nindicate a Dice score of 31.1% between the model's attention maps and manual\ntumor segmentation (as an explainability measure) with test classification\nperformance of 87.7%, significantly outperforming the baselines. These\nenhancements can build trust in our model among radiologists, facilitating its\nintegration into clinical practices for more efficient tumor diagnosis.",
    "explanation":"The article presents a research involving the use of Computer Science methods to treat problems in Medicine. In this case, applying the use of Convolutional Neural Networks (CNN) for the recognition of pediatric tumors in medical images and Contrastive Learning (CL) to improve the explainability of the model.",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b0"
    ],
    "c_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "c_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11062",
    "c_title":[
      "Active Learning from Scene Embeddings for End-to-End Autonomous Driving"
    ],
    "c_abstract":[
      "In the field of autonomous driving, end-to-end deep learning models show\ngreat potential by learning driving decisions directly from sensor data.\nHowever, training these models requires large amounts of labeled data, which is\ntime-consuming and expensive. Considering that the real-world driving data\nexhibits a long-tailed distribution where simple scenarios constitute a\nmajority part of the data, we are thus inspired to identify the most\nchallenging scenarios within it. Subsequently, we can efficiently improve the\nperformance of the model by training with the selected data of the highest\nvalue. Prior research has focused on the selection of valuable data by\nempirically designed strategies. However, manually designed methods suffer from\nbeing less generalizable to new data distributions. Observing that the BEV\n(Bird's Eye View) features in end-to-end models contain all the information\nrequired to represent the scenario, we propose an active learning framework\nthat relies on these vectorized scene-level features, called SEAD. The\nframework selects initial data based on driving-environmental information and\nincremental data based on BEV features. Experiments show that we only need 30\\%\nof the nuScenes training data to achieve performance close to what can be\nachieved with the full dataset. The source code will be released."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.13828",
    "c_title":[
      "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical\n  Anomaly Detection"
    ],
    "c_abstract":[
      "Unsupervised anomaly detection using deep learning has garnered significant\nresearch attention due to its broad applicability, particularly in medical\nimaging where labeled anomalous data are scarce. While earlier approaches\nleverage generative models like autoencoders and generative adversarial\nnetworks (GANs), they often fall short due to overgeneralization. Recent\nmethods explore various strategies, including memory banks, normalizing flows,\nself-supervised learning, and knowledge distillation, to enhance\ndiscrimination. Among these, knowledge distillation, particularly reverse\ndistillation, has shown promise. Following this paradigm, we propose a novel\nscale-aware contrastive reverse distillation model that addresses two key\nlimitations of existing reverse distillation methods: insufficient feature\ndiscriminability and inability to handle anomaly scale variations.\nSpecifically, we introduce a contrastive student-teacher learning approach to\nderive more discriminative representations by generating and exploring\nout-of-normal distributions. Further, we design a scale adaptation mechanism to\nsoftly weight contrastive distillation losses at different scales to account\nfor the scale variation issue. Extensive experiments on benchmark datasets\ndemonstrate state-of-the-art performance, validating the efficacy of the\nproposed method. Code is available at https:\/\/github.com\/MedAITech\/SCRD4AD."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12148",
    "c_title":[
      "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation"
    ],
    "c_abstract":[
      "The remarkable success of the autoregressive paradigm has made significant\nadvancement in Multimodal Large Language Models (MLLMs), with powerful models\nlike Show-o, Transfusion and Emu3 achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we propose HermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take the homologous data as input to curate homologous\npreference data of both understanding and generation. Through Pair-DPO and\nself-play iterative optimization, HermesFlow effectively aligns multimodal\nunderstanding and generation using homologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential of HermesFlow as a general\nalignment framework for next-generation multimodal foundation models. Code:\nhttps:\/\/github.com\/Gen-Verse\/HermesFlow"
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12168",
    "c_title":[
      "Learning Extremely High Density Crowds as Active Matters"
    ],
    "c_abstract":[
      "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.14101",
    "c_title":[
      "StreamingRAG: Real-time Contextual Retrieval and Generation Framework"
    ],
    "c_abstract":[
      "Extracting real-time insights from multi-modal data streams from various\ndomains such as healthcare, intelligent transportation, and satellite remote\nsensing remains a challenge. High computational demands and limited knowledge\nscope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)\non these data streams. Traditional Retrieval-Augmented Generation (RAG) systems\naddress knowledge limitations of these models, but suffer from slow\npreprocessing, making them unsuitable for real-time analysis. We propose\nStreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG\nconstructs evolving knowledge graphs capturing scene-object-entity\nrelationships in real-time. The knowledge graph achieves temporal-aware scene\nrepresentations using MM-LLMs and enables timely responses for specific events\nor user queries. StreamingRAG addresses limitations in existing methods,\nachieving significant improvements in real-time analysis (5-6x faster\nthroughput), contextual accuracy (through a temporal knowledge graph), and\nreduced resource consumption (using lightweight models by 2-3x)."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02510",
    "c_title":[
      "A note on conformal-biharmonic hypersurfaces"
    ],
    "c_abstract":[
      "The conformal bienergy functional $E_2^c$ was recently introduced as a\nmodified version of the classical bienergy functional $E_2$ in order to ensure\nthe validity of some conformal invariance properties. The critical points of\n$E_2^c$ are called conformal-biharmonic and denoted $c$-biharmonic. In this\npaper we study the $c$-biharmonic hypersurfaces $M^m$ with constant principal\ncurvatures in the product space $ {\\mathbb L}^m(\\varepsilon) \\times \\mathbb{R}\n$, where $ {\\mathbb L}^m(\\varepsilon) $ denotes a space form of constant\nsectional curvature $ \\varepsilon $. Specifically, we demonstrate that $ M^m $\nis either totally geodesic or a cylindrical hypersurface of the form $ M^{m-1}\n\\times \\mathbb{R} $, where $ M^{m-1} $ is a $c$-biharmonic isoparametric\nhypersurface in $ {\\mathbb L}^m(\\varepsilon) $. To provide further insight, we\ndescribe the structure of $c$-biharmonic isoparametric hypersurfaces in space\nforms. In the final part, as a preliminary effort to understand $c$-biharmonic\nhypersurfaces $ M^m $ in $ {\\mathbb L}^m(\\varepsilon) \\times \\mathbb{R} $ with\nnon constant mean curvature, we establish that a totally umbilical\n$c$-biharmonic hypersurface must necessarily be totally geodesic."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06091",
    "c_title":[
      "First Search for Neutral Current Coherent Single-Photon Production in\n  MicroBooNE"
    ],
    "c_abstract":[
      "This article presents the first search for neutrino-induced neutral current\ncoherent single-photon production (NC coherent 1$\\gamma$). The search makes use\nof data from the MicroBooNE 85-tonne active volume liquid argon time projection\nchamber detector, situated in the Fermilab Booster Neutrino Beam (BNB), with an\naverage neutrino energy of $\\langle E_{\\nu}\\rangle \\sim 0.8$ GeV. A targeted\nselection of candidate neutrino interactions with a single photon-like\nelectromagnetic shower in the final state and no visible vertex activity was\ndeveloped to search for the NC coherent 1$\\gamma$ process, along with two\nauxiliary selections used to constrain the dominant background from NC$\\pi^0$\nproduction. With an integrated exposure of $6.87 \\times 10^{20}$ protons on\ntarget delivered by the BNB, we set the world's first limit for this rare\nprocess, corresponding to an upper limit on the flux-averaged cross section of\n$\\sigma<1.49 \\times 10^{-41}\\text{cm}^2$ at 90\\% C.L."
    ],
    "c_categories":[
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07576",
    "c_title":[
      "One-loop matching for leading-twist generalised\n  transverse-momentum-dependent distributions"
    ],
    "c_abstract":[
      "We present the one-loop matching coefficients necessary to match all of the\nleading-twist generalised transverse-momentum-dependent distributions (GTMDs)\nonto generalised parton distributions (GPDs). Matching functions are extracted\nby computing the first radiative corrections to partonic bilocal correlators\nwith staple-like Wilson lines, as appropriate for high-energy collisions. These\ncorrelators are characterised by a transverse displacement and skewed\nkinematics of external states. Using the proton helicity basis, they are\nparametrised in terms of GTMDs, which are subsequently related to leading-twist\nGPDs. Our results provide new insights into the complex dynamics of GTMDs\ngenerated by radiative corrections. In particular, we show that time-reversal\neven and odd contributions to GTMDs in the so-called ERBL region mix both under\nmatching and evolution. Finally, we present a selection of numerical results\nand comment on the quantitative behaviour of GTMDs."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09332",
    "c_title":[
      "Automatic exposure volumetric additive manufacturing"
    ],
    "c_abstract":[
      "Tomographic volumetric additive manufacturing (VAM) achieves high print speed\nand design freedom by continuous volumetric light patterning. This differs from\ntraditional vat photopolymerization techniques that use brief sequential (2D)\nplane- or (1D) point-localized exposures. The drawback to volumetric light\npatterning is the small exposure window. Overexposure quickly leads to cured\nout-of-part voxels due to the nonzero background dose arising from light\nprojection through the build volume. For tomographic VAM, correct exposure time\nis critical to achieving high repeatability, however, we find that correct\nexposure time varies by nearly 40% depending on resin history. Currently,\ntomographic VAM exposure is timed based on subjective human determination of\nprint completion, which is tedious and yields poor repeatability. Here, we\nimplement a robust auto exposure routine for tomographic VAM using real-time\nprocessing of light scattering data, yielding accurate and repeatable prints\nwithout human intervention. The resulting print fidelity and repeatability\napproaches, and in some cases, exceeds that of commercial resin 3D printers. We\nshow that auto exposure VAM generalizes well to a wide variety of print\ngeometries with small positive and negative features. The repeatability and\naccuracy of auto exposure VAM allows for building multi-part objects,\nfulfilling a major requirement of additive manufacturing technologies."
    ],
    "c_categories":[
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.14758",
    "c_title":[
      "Cyclic Voltammetry of Ion-Coupled Electron Transfer Reactions for\n  Diagnosing Energy Storage Materials"
    ],
    "c_abstract":[
      "The methods of Nicholson and Shain and Randles-Sevcik are the paradigms of\nvoltammetry of redox species. However, as they were originally developed for\naqueous redox couples, they cannot be directly applied to solid redox films\nsuch as those of battery materials. Herein, for the first time, we present a\ncyclic voltammetry model based on semi-infinite linear diffusion for\nion-coupled electron transfer reactions. The simulated CVs contain parameters\nsuch as capacity, ion activity, formal potential of proton-coupled electron\ntransfer, and scan rate that are physically more meaningful than those of the\ncurrent models in characterizing energy storage materials. We apply this model\nto the MnO2 cathode material as a proof of concept and discuss the significance\nof the simulation parameters for determining the energetics of the underlying\nphase transitions and the charge storage mechanisms. According to the present\nmodel, two linear regression lines can be established from relatively simple\nvoltammetry experiments for characterizing energy storage materials: 1) The\nregression line of the CV mid-peak potential vs. Log of ion activity (or pH),\nin which the slope and the intercept provide information on the type of\ncharge-carrier ions and their solvation state, respectively. 2) The regression\nline of the capacity vs. the inverse of the square root of scan rate, where the\nslope and intercept indicate the contributions of bulk and surface charges in\nthin redox films, respectively."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12092",
    "c_title":[
      "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications"
    ],
    "c_abstract":[
      "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09984",
    "c_title":[
      "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach"
    ],
    "c_abstract":[
      "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05493",
    "c_title":[
      "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space"
    ],
    "c_abstract":[
      "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11395",
    "c_title":[
      "Targeting C99 Mediated Metabolic Disruptions with Ketone Therapy in\n  Alzheimer's Disease"
    ],
    "c_abstract":[
      "The role of ketone bodies in Alzheimers disease (AD) remains incompletely\nunderstood, particularly regarding their influence on amyloid pathology. While\nbeta}hydroxybutyrate (BHB) has been implicated in neuroprotection, direct\nevidence for its effects on amyloid beta(Abeta) deposition, aggregation, or\nclearance is lacking. Furthermore, whether BHB acts as a disease modifying\nfactor or merely confers transient metabolic benefits remains unclear.\nAddressing this gap is crucial for evaluating the therapeutic potential of\nketone metabolism in AD. Here, we investigated the impact of ketone bodies on\namyloidogenic toxicity using a Drosophila melanogaster model with targeted\nexpression of human amyloid precursor protein (APP), beta secretase 1 (BACE1),\nAbeta, and the C99 fragment, an essential intermediate in Abeta generation.\nSurprisingly, we found that Abeta alone elicited minimal neurotoxicity, whereas\nC99 expression induced pronounced pathological effects, suggesting a critical,\nunderappreciated role of C99 in AD progression. Further analysis revealed that\nC99 driven toxicity was associated with autophagic and lysosomal dysfunction,\nleading to impaired protein clearance, oxidative stress, and mitochondrial\nabnormalities. Using confocal microscopy and lysosomal pH sensitive markers, we\ndemonstrated that BHB treatment restored lysosomal function and alleviated\nthese pathological changes. Protein protein interaction network analysis in C99\nexpressing Drosophila brains identified protein phosphatase methylesterase 1\n(PPME1) activation as a key driver of autophagic impairment, further supported\nby machine learning predictions. Finally, mathematical similarity analysis of\nPPI networks suggested that BHB may exert its neuroprotective effects through\nmTOR inhibition, positioning it as a potential endogenous modulator of AD\nrelated pathology."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07798",
    "c_title":[
      "Risk and Protective Factors in Parkinsons Disease"
    ],
    "c_abstract":[
      "Understanding the risk and protective factors associated with Parkinsons\ndisease (PD) is crucial for improving outcomes for patients, individuals at\nrisk, healthcare providers, and healthcare systems. Studying these factors not\nonly enhances our knowledge of the disease but also aids in developing\neffective prevention, management, and treatment strategies. This paper reviews\nthe key risk and protective factors associated with PD, with a particular focus\non the biological mechanisms underlying these factors. Risk factors include\ngenetic mutations, racial predispositions, and environmental exposures, all of\nwhich contribute to an increased likelihood of developing PD or accelerating\ndisease progression. Conversely, protective factors such as regular physical\nexercise, adherence to a Mediterranean diet, and higher urate levels have\ndemonstrated potential to reduce inflammation and support mitochondrial\nfunction, thereby mitigating disease risk. However, identifying and validating\nthese factors presents significant challenges. To overcome challenges, we\npropose several solutions and recommendations. Future research should\nprioritize the development of standardized biomarkers for early diagnosis,\ninvestigate gene-environment interactions in greater depth, and refine animal\nmodels to better mimic human PD pathology. Additionally, we offer actionable\nrecommendations for PD prevention and management, tailored to healthy\nindividuals, patients diagnosed with PD, and healthcare systems. These\nstrategies aim to improve clinical outcomes, enhance quality of life, and\noptimize healthcare delivery for PD."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15121",
    "c_title":[
      "Photomolecular Effect as A Potential Explanation for The Cloud\n  Absorption Anomaly"
    ],
    "c_abstract":[
      "Cloud absorption is acknowledged as the biggest source of uncertainty in the\nclimate models. For over 70 years, many experiments have reported clouds\nabsorbing more solar radiation than theory could predict. In the visible\nspectrum, simulations based on optical constants of water lead to negligible\ncloud absorption. This result had been explored by some experimentalists to\ncalibrate the cloud absorption measurements. However, the author and his\ncollaborators recently discovered that visible light can directly cleave off\nwater molecular clusters at liquid-air interfaces (PNAS, e2312751120, 2023;\ne2320844121, 2024), which is named the photomolecular effect in analogy to the\nphotoelectric effect. This discovery suggests that a crucial piece of physics\nhas been missing in the existing theories: light can be absorbed at water-air\ninterface. The photomolecular effect can be simulated by generalizing the\nboundary conditions for the Maxwell equations using Feibelman parameters that\nwere derived in the past research on the surface photoelectric effect and\nsurface plasmons. In this work, the author uses simulation to show that\nincluding the photomolecular effect at the air-water interface can potentially\nexplain the cloud absorption anomaly. Although the lack of accurate Feibelman\nparameter values prevents a direct comparison of the current theory with\nexperiments, this work points to an unexplored mechanism for explaining the\ncloud absorption anomaly and calls for further investigation on the impacts of\nphotomolecular effect in the climate modeling."
    ],
    "c_categories":[
      "physics.ao-ph",
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11945",
    "c_title":[
      "Effective medium theory for the electrical conductivity of random\n  metallic nanowire networks"
    ],
    "c_abstract":[
      "Interest in studying the conductive properties of networks made from randomly\ndistributed nanowires is due to their numerous technological applications.\nAlthough the sheet resistance of such networks can be calculated directly, the\ncalculations require many characteristics of the system (distributions of\nlengths, diameters and resistances of nanowires, distribution of junction\nresistance), the measurement of which is difficult. Furthermore, such\ncalculations can hardly offer an analytical dependence of the sheet resistance\non the basic physical parameters of the systems under consideration. Although\nvarious theoretical approaches offer such analytical dependencies, they are\noften based on more or less reasonable assumptions rather than rigorously\nproven statements. Here, we offer an approach based on Foster's theorem to\nreveal a dependence of the sheet resistance of dense nanowire networks on the\nmain parameters of such networks. This theorem offers an additional perspective\non the effective medium theory and extends our insight."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16659",
    "c_title":[
      "Optimizing Input Data Collection for Ranking and Selection"
    ],
    "c_abstract":[
      "We study a ranking and selection (R&S) problem when all solutions share\ncommon parametric Bayesian input models updated with the data collected from\nmultiple independent data-generating sources. Our objective is to identify the\nbest system by designing a sequential sampling algorithm that collects input\nand simulation data given a budget. We adopt the most probable best (MPB) as\nthe estimator of the optimum and show that its posterior probability of\noptimality converges to one at an exponential rate as the sampling budget\nincreases. Assuming that the input parameters belong to a finite set, we\ncharacterize the $\\epsilon$-optimal static sampling ratios for input and\nsimulation data that maximize the convergence rate. Using these ratios as\nguidance, we propose the optimal sampling algorithm for R&S (OSAR) that\nachieves the $\\epsilon$-optimal ratios almost surely in the limit. We further\nextend OSAR by adopting the kernel ridge regression to improve the simulation\noutput mean prediction. This not only improves OSAR's finite-sample\nperformance, but also lets us tackle the case where the input parameters lie in\na continuous space with a strong consistency guarantee for finding the optimum.\nWe numerically demonstrate that OSAR outperforms a state-of-the-art competitor."
    ],
    "c_categories":[
      "math.OC",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.04791",
    "c_title":[
      "Development and Benchmarking of JANGOFETT: A Novel Geant4-Operated\n  Fission Event Tracking Tool"
    ],
    "c_abstract":[
      "Experiments measuring fission observables encounter false coincidences\narising from timing overlap of separate fission product decays. Simulations of\nboth fission observables and particles in detector systems exist, but have not\nyet been combined to produce accurate event-by-event outputs in a\ntime-dependent manner. Geant4 is a powerful simulation tool for nuclear physics\nstudies, but it does not handle multiple initial particles in a single\nsimulation instance, nor does it feature high fidelity fission sampling.\nJANGOFETT: A Novel Geant4-Operated Fission Event Tracking Tool has been\ndeveloped to address this challenge. The tool utilizes simulated fission data\nfrom an external program in conjunction with Geant4, which has been modified to\nproduce a single timeline of events over an entire simulated experiment. The\nphysical accuracy of the simulated overlapping energy depositions within\ndetectors has been verified via simulation of fission products from the\nspontaneous fission of 252Cf."
    ],
    "c_categories":[
      "nucl-ex",
      "nucl-th",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18297",
    "c_title":[
      "Cayley graphs on elementary abelian groups of extreme degree have\n  complete cores"
    ],
    "c_abstract":[
      "Ne\\v{s}et\\v{r}il and \\v{S}\\'{a}mal asked whether every cubelike graph has a\ncubelike core. Man\\v{c}inska, Pivotto, Roberson and Royle answered this\nquestion in the affirmative for cubelike graphs whose core has at most $32$\nvertices. When the core of a cubelike graph has at most $16$ vertices, they\ngave a list of these cores, from which it follows that every cubelike graph\nwith degree strictly less than $5$ has a complete core. We prove the following\nextension: if the degree of a cubelike graph is either strictly less than $5$\nor at least $5$ less than the number of its vertices, then its core is complete\nand induced by a $\\mathbb{F}_2$-vector subspace of its vertices. Thus we also\nanswer Ne\\v{s}et\\v{r}il and \\v{S}\\'{a}mal's question in the affirmative for\ncubelike graphs with degree at least $5$ less than the number of vertices. Our\nresult is sharp as the $5$-regular folded $5$-cube and its graph complement are\nboth non-complete cubelike graph cores. We also prove analogous results for\nCayley graphs on elementary abelian $p$-groups for odd primes $p$."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading\n  with Cataract",
    "a_abstract":"Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a\ncommon complication of diabetes. As two different imaging tools for DR grading,\ncolor fundus photography (CFP) and infrared fundus photography (IFP) are\nhighly-correlated and complementary in clinical applications. To the best of\nour knowledge, this is the first study that explores a novel multi-modal deep\nlearning framework to fuse the information from CFP and IFP towards more\naccurate DR grading. Specifically, we construct a dual-stream architecture\nCross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus\nimage modalities. In particular, a meticulously engineered Cross-Fundus\nAttention (CFA) module is introduced to capture the correspondence between CFP\nand IFP images. Moreover, we adopt both the single-modality and multi-modality\nsupervisions to maximize the overall performance for DR grading. Extensive\nexperiments on a clinical dataset consisting of 1,713 pairs of multi-modal\nfundus images demonstrate the superiority of our proposed method. Our code will\nbe released for public access.",
    "explanation":"The work combines transformers with two distinct methods that evaluate the quality of retinopathy",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b2"
    ],
    "c_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "c_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06501",
    "c_title":[
      "Learning Clustering-based Prototypes for Compositional Zero-shot\n  Learning"
    ],
    "c_abstract":[
      "Learning primitive (i.e., attribute and object) concepts from seen\ncompositions is the primary challenge of Compositional Zero-Shot Learning\n(CZSL). Existing CZSL solutions typically rely on oversimplified data\nassumptions, e.g., modeling each primitive with a single centroid primitive\nrepresentation, ignoring the natural diversities of the attribute (resp.\nobject) when coupled with different objects (resp. attribute). In this work, we\ndevelop ClusPro, a robust clustering-based prototype mining framework for CZSL\nthat defines the conceptual boundaries of primitives through a set of\ndiversified prototypes. Specifically, ClusPro conducts within-primitive\nclustering on the embedding space for automatically discovering and dynamically\nupdating prototypes. These representative prototypes are subsequently used to\nrepaint a well-structured and independent primitive embedding space, ensuring\nintra-primitive separation and inter-primitive decorrelation through\nprototype-based contrastive learning and decorrelation learning. Moreover,\nClusPro efficiently performs prototype clustering in a non-parametric fashion\nwithout the introduction of additional learnable parameters or computational\nbudget during testing. Experiments on three benchmarks demonstrate ClusPro\noutperforms various top-leading CZSL solutions under both closed-world and\nopen-world settings."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19797",
    "c_title":[
      "MFSR: Multi-fractal Feature for Super-resolution Reconstruction with\n  Fine Details Recovery"
    ],
    "c_abstract":[
      "In the process of performing image super-resolution processing, the\nprocessing of complex localized information can have a significant impact on\nthe quality of the image generated. Fractal features can capture the rich\ndetails of both micro and macro texture structures in an image. Therefore, we\npropose a diffusion model-based super-resolution method incorporating fractal\nfeatures of low-resolution images, named MFSR. MFSR leverages these fractal\nfeatures as reinforcement conditions in the denoising process of the diffusion\nmodel to ensure accurate recovery of texture information. MFSR employs\nconvolution as a soft assignment to approximate the fractal features of\nlow-resolution images. This approach is also used to approximate the density\nfeature maps of these images. By using soft assignment, the spatial layout of\nthe image is described hierarchically, encoding the self-similarity properties\nof the image at different scales. Different processing methods are applied to\nvarious types of features to enrich the information acquired by the model. In\naddition, a sub-denoiser is integrated in the denoising U-Net to reduce the\nnoise in the feature maps during the up-sampling process in order to improve\nthe quality of the generated images. Experiments conducted on various face and\nnatural image datasets demonstrate that MFSR can generate higher quality\nimages."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.07418",
    "c_title":[
      "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive\n  Diffusion"
    ],
    "c_abstract":[
      "The task of video generation requires synthesizing visually realistic and\ntemporally coherent video frames. Existing methods primarily use asynchronous\nauto-regressive models or synchronous diffusion models to address this\nchallenge. However, asynchronous auto-regressive models often suffer from\ninconsistencies between training and inference, leading to issues such as error\naccumulation, while synchronous diffusion models are limited by their reliance\non rigid sequence length. To address these issues, we introduce Auto-Regressive\nDiffusion (AR-Diffusion), a novel model that combines the strengths of\nauto-regressive and diffusion models for flexible, asynchronous video\ngeneration. Specifically, our approach leverages diffusion to gradually corrupt\nvideo frames in both training and inference, reducing the discrepancy between\nthese phases. Inspired by auto-regressive generation, we incorporate a\nnon-decreasing constraint on the corruption timesteps of individual frames,\nensuring that earlier frames remain clearer than subsequent ones. This setup,\ntogether with temporal causal attention, enables flexible generation of videos\nwith varying lengths while preserving temporal coherence. In addition, we\ndesign two specialized timestep schedulers: the FoPP scheduler for balanced\ntimestep sampling during training, and the AD scheduler for flexible timestep\ndifferences during inference, supporting both synchronous and asynchronous\ngeneration. Extensive experiments demonstrate the superiority of our proposed\nmethod, which achieves competitive and state-of-the-art results across four\nchallenging benchmarks."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15005",
    "c_title":[
      "Universal Scene Graph Generation"
    ],
    "c_abstract":[
      "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.10674",
    "c_title":[
      "Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D\n  Object Recognition"
    ],
    "c_abstract":[
      "Recent open-world representation learning approaches have leveraged CLIP to\nenable zero-shot 3D object recognition. However, performance on real point\nclouds with occlusions still falls short due to unrealistic pretraining\nsettings. Additionally, these methods incur high inference costs because they\nrely on Transformer's attention modules. In this paper, we make two\ncontributions to address these limitations. First, we propose occlusion-aware\ntext-image-point cloud pretraining to reduce the training-testing domain gap.\nFrom 52K synthetic 3D objects, our framework generates nearly 630K partial\npoint clouds for pretraining, consistently improving real-world recognition\nperformances of existing popular 3D networks. Second, to reduce computational\nrequirements, we introduce DuoMamba, a two-stream linear state space model\ntailored for point clouds. By integrating two space-filling curves with 1D\nconvolutions, DuoMamba effectively models spatial dependencies between point\ntokens, offering a powerful alternative to Transformer. When pretrained with\nour framework, DuoMamba surpasses current state-of-the-art methods while\nreducing latency and FLOPs, highlighting the potential of our approach for\nreal-world applications. Our code and data are available at\nhttps:\/\/ndkhanh360.github.io\/project-occtip."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15121",
    "c_title":[
      "Photomolecular Effect as A Potential Explanation for The Cloud\n  Absorption Anomaly"
    ],
    "c_abstract":[
      "Cloud absorption is acknowledged as the biggest source of uncertainty in the\nclimate models. For over 70 years, many experiments have reported clouds\nabsorbing more solar radiation than theory could predict. In the visible\nspectrum, simulations based on optical constants of water lead to negligible\ncloud absorption. This result had been explored by some experimentalists to\ncalibrate the cloud absorption measurements. However, the author and his\ncollaborators recently discovered that visible light can directly cleave off\nwater molecular clusters at liquid-air interfaces (PNAS, e2312751120, 2023;\ne2320844121, 2024), which is named the photomolecular effect in analogy to the\nphotoelectric effect. This discovery suggests that a crucial piece of physics\nhas been missing in the existing theories: light can be absorbed at water-air\ninterface. The photomolecular effect can be simulated by generalizing the\nboundary conditions for the Maxwell equations using Feibelman parameters that\nwere derived in the past research on the surface photoelectric effect and\nsurface plasmons. In this work, the author uses simulation to show that\nincluding the photomolecular effect at the air-water interface can potentially\nexplain the cloud absorption anomaly. Although the lack of accurate Feibelman\nparameter values prevents a direct comparison of the current theory with\nexperiments, this work points to an unexplored mechanism for explaining the\ncloud absorption anomaly and calls for further investigation on the impacts of\nphotomolecular effect in the climate modeling."
    ],
    "c_categories":[
      "physics.ao-ph",
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11945",
    "c_title":[
      "Effective medium theory for the electrical conductivity of random\n  metallic nanowire networks"
    ],
    "c_abstract":[
      "Interest in studying the conductive properties of networks made from randomly\ndistributed nanowires is due to their numerous technological applications.\nAlthough the sheet resistance of such networks can be calculated directly, the\ncalculations require many characteristics of the system (distributions of\nlengths, diameters and resistances of nanowires, distribution of junction\nresistance), the measurement of which is difficult. Furthermore, such\ncalculations can hardly offer an analytical dependence of the sheet resistance\non the basic physical parameters of the systems under consideration. Although\nvarious theoretical approaches offer such analytical dependencies, they are\noften based on more or less reasonable assumptions rather than rigorously\nproven statements. Here, we offer an approach based on Foster's theorem to\nreveal a dependence of the sheet resistance of dense nanowire networks on the\nmain parameters of such networks. This theorem offers an additional perspective\non the effective medium theory and extends our insight."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.16659",
    "c_title":[
      "Optimizing Input Data Collection for Ranking and Selection"
    ],
    "c_abstract":[
      "We study a ranking and selection (R&S) problem when all solutions share\ncommon parametric Bayesian input models updated with the data collected from\nmultiple independent data-generating sources. Our objective is to identify the\nbest system by designing a sequential sampling algorithm that collects input\nand simulation data given a budget. We adopt the most probable best (MPB) as\nthe estimator of the optimum and show that its posterior probability of\noptimality converges to one at an exponential rate as the sampling budget\nincreases. Assuming that the input parameters belong to a finite set, we\ncharacterize the $\\epsilon$-optimal static sampling ratios for input and\nsimulation data that maximize the convergence rate. Using these ratios as\nguidance, we propose the optimal sampling algorithm for R&S (OSAR) that\nachieves the $\\epsilon$-optimal ratios almost surely in the limit. We further\nextend OSAR by adopting the kernel ridge regression to improve the simulation\noutput mean prediction. This not only improves OSAR's finite-sample\nperformance, but also lets us tackle the case where the input parameters lie in\na continuous space with a strong consistency guarantee for finding the optimum.\nWe numerically demonstrate that OSAR outperforms a state-of-the-art competitor."
    ],
    "c_categories":[
      "math.OC",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04791",
    "c_title":[
      "Development and Benchmarking of JANGOFETT: A Novel Geant4-Operated\n  Fission Event Tracking Tool"
    ],
    "c_abstract":[
      "Experiments measuring fission observables encounter false coincidences\narising from timing overlap of separate fission product decays. Simulations of\nboth fission observables and particles in detector systems exist, but have not\nyet been combined to produce accurate event-by-event outputs in a\ntime-dependent manner. Geant4 is a powerful simulation tool for nuclear physics\nstudies, but it does not handle multiple initial particles in a single\nsimulation instance, nor does it feature high fidelity fission sampling.\nJANGOFETT: A Novel Geant4-Operated Fission Event Tracking Tool has been\ndeveloped to address this challenge. The tool utilizes simulated fission data\nfrom an external program in conjunction with Geant4, which has been modified to\nproduce a single timeline of events over an entire simulated experiment. The\nphysical accuracy of the simulated overlapping energy depositions within\ndetectors has been verified via simulation of fission products from the\nspontaneous fission of 252Cf."
    ],
    "c_categories":[
      "nucl-ex",
      "nucl-th",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.18297",
    "c_title":[
      "Cayley graphs on elementary abelian groups of extreme degree have\n  complete cores"
    ],
    "c_abstract":[
      "Ne\\v{s}et\\v{r}il and \\v{S}\\'{a}mal asked whether every cubelike graph has a\ncubelike core. Man\\v{c}inska, Pivotto, Roberson and Royle answered this\nquestion in the affirmative for cubelike graphs whose core has at most $32$\nvertices. When the core of a cubelike graph has at most $16$ vertices, they\ngave a list of these cores, from which it follows that every cubelike graph\nwith degree strictly less than $5$ has a complete core. We prove the following\nextension: if the degree of a cubelike graph is either strictly less than $5$\nor at least $5$ less than the number of its vertices, then its core is complete\nand induced by a $\\mathbb{F}_2$-vector subspace of its vertices. Thus we also\nanswer Ne\\v{s}et\\v{r}il and \\v{S}\\'{a}mal's question in the affirmative for\ncubelike graphs with degree at least $5$ less than the number of vertices. Our\nresult is sharp as the $5$-regular folded $5$-cube and its graph complement are\nboth non-complete cubelike graph cores. We also prove analogous results for\nCayley graphs on elementary abelian $p$-groups for odd primes $p$."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11130",
    "c_title":[
      "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering"
    ],
    "c_abstract":[
      "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18661",
    "c_title":[
      "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling"
    ],
    "c_abstract":[
      "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17455",
    "c_title":[
      "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective"
    ],
    "c_abstract":[
      "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08533",
    "c_title":[
      "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant"
    ],
    "c_abstract":[
      "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.15443",
    "c_title":[
      "Are Elites Meritocratic and Efficiency-Seeking? Evidence from MBA\n  Students"
    ],
    "c_abstract":[
      "Elites disproportionately influence policymaking, yet little is known about\ntheir fairness and efficiency preferences -- key determinants of support for\nredistributive policies. We investigate these preferences using an incentivized\nlab experiment with a group of future elites -- Ivy League MBA students. We\nfind that elites implement more unequal earnings distributions than the average\nAmerican, are highly sensitive to both merit-based inequality and efficiency\ncosts of redistribution, and are less likely to hold strict meritocratic views.\nThese findings provide novel insights into how elites' redistributive\npreferences may shape high levels of inequality and limited redistributive\npolicy in the United States."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19179",
    "c_title":[
      "Learning Non-Local Molecular Interactions via Equivariant Local\n  Representations and Charge Equilibration"
    ],
    "c_abstract":[
      "Graph Neural Network (GNN) potentials relying on chemical locality offer\nnear-quantum mechanical accuracy at significantly reduced computational costs.\nBy propagating local information to distance particles, Message-passing neural\nnetworks (MPNNs) extend the locality concept to model interactions beyond their\nlocal neighborhood. Still, this locality precludes modeling long-range effects,\nsuch as charge transfer, electrostatic interactions, and dispersion effects,\nwhich are critical to adequately describe many real-world systems. In this\nwork, we propose the Charge Equilibration Layer for Long-range Interactions\n(CELLI) to address the challenging modeling of non-local interactions and the\nhigh computational cost of MPNNs. This novel architecture generalizes the\nfourth-generation high-dimensional neural network (4GHDNN) concept, integrating\nthe charge equilibration (Qeq) method into a model-agnostic building block for\nmodern equivariant GNN potentials. A series of benchmarks show that CELLI can\nextend the strictly local Allegro architecture to model highly non-local\ninteractions and charge transfer. Our architecture generalizes to diverse\ndatasets and large structures, achieving an accuracy comparable to MPNNs at\nabout twice the computational efficiency."
    ],
    "c_categories":[
      "cs.LG",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07934",
    "c_title":[
      "Monotonicity and convergence of two-relaxation-times lattice Boltzmann\n  schemes for a non-linear conservation law"
    ],
    "c_abstract":[
      "We address the convergence analysis of lattice Boltzmann methods for scalar\nnon-linear conservation laws, focusing on two-relaxation-times (TRT) schemes.\nUnlike Finite Difference\/Finite Volume methods, lattice Boltzmann schemes offer\nexceptional computational efficiency and parallelization capabilities. However,\ntheir monotonicity and $L^{\\infty}$-stability remain underexplored. Extending\nexisting results on simpler BGK schemes, we derive conditions ensuring that TRT\nschemes are monotone and stable by leveraging their unique relaxation\nstructure. Our analysis culminates in proving convergence of the numerical\nsolution to the weak entropy solution of the conservation law. Compared to BGK\nschemes, TRT schemes achieve reduced numerical diffusion while retaining\nprovable convergence. Numerical experiments validate and illustrate the\ntheoretical findings."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12505",
    "c_title":[
      "A Conservative Partially Hyperbolic Dichotomy: Hyperbolicity versus\n  Nonhyperbolic Measures"
    ],
    "c_abstract":[
      "In a conservative and partially hyperbolic three-dimensional setting, we\nstudy three representative classes of diffeomorphisms: those homotopic to\nAnosov (or Derived from Anosov diffeomorphisms), diffeomorphisms in\nneighborhoods of the time-one map of the geodesic flow on a surface of negative\ncurvature, and accessible and dynamically coherent skew products with circle\nfibers. In any of these classes, we establish the following dichotomy: either\nthe diffeomorphism is Anosov, or it possesses nonhyperbolic ergodic measures.\nOur approach is perturbation-free and combines recent advances in the study of\nstably ergodic diffeomorphisms with a variation of the periodic approximation\nmethod to obtain ergodic measures.\n  A key result in our construction, independent of conservative hypotheses, is\nthe construction of nonhyperbolic ergodic measures for sets with a minimal\nstrong unstable foliation that satisfy the mostly expanding property. This\napproach enables us to obtain nonhyperbolic ergodic measures in other contexts,\nincluding some subclasses of the so-called anomalous partially hyperbolic\ndiffeomorphisms that are not dynamically coherent."
    ],
    "c_categories":[
      "math.DS"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06363",
    "c_title":[
      "Improved Regret Analysis in Gaussian Process Bandits: Optimality for\n  Noiseless Reward, RKHS norm, and Non-Stationary Variance"
    ],
    "c_abstract":[
      "We study the Gaussian process (GP) bandit problem, whose goal is to minimize\nregret under an unknown reward function lying in some reproducing kernel\nHilbert space (RKHS). The maximum posterior variance analysis is vital in\nanalyzing near-optimal GP bandit algorithms such as maximum variance reduction\n(MVR) and phased elimination (PE). Therefore, we first show the new upper bound\nof the maximum posterior variance, which improves the dependence of the noise\nvariance parameters of the GP. By leveraging this result, we refine the MVR and\nPE to obtain (i) a nearly optimal regret upper bound in the noiseless setting\nand (ii) regret upper bounds that are optimal with respect to the RKHS norm of\nthe reward function. Furthermore, as another application of our proposed bound,\nwe analyze the GP bandit under the time-varying noise variance setting, which\nis the kernelized extension of the linear bandit with heteroscedastic noise.\nFor this problem, we show that MVR and PE-based algorithms achieve noise\nvariance-dependent regret upper bounds, which matches our regret lower bound."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"Designing a Light-based Communication System with a Biomolecular\n  Receiver",
    "a_abstract":"Biological systems transduce signals from their surroundings in numerous\nways. This paper introduces a communication system using the light-gated ion\nchannel Channelrhodopsin-2 (ChR2), which causes an ion current to flow in\nresponse to light. Our design includes a ChR2-based receiver along with\nencoding, modulation techniques and detection. Analyzing the resulting\ncommunication system, we discuss the effect of different parameters on the\nperformance of the system. Finally, we discuss its potential design in the\ncontext of bio-engineering and light-based communication and show that the data\nrate scales up with the number of receptors, indicating that high-speed\ncommunication may be possible.",
    "explanation":"The paper is interdisciplinary because it aims to use channelrhodopsin-2 (ChR2), a biomolecule, as a receiver to design a light-based communication system, which is a work related to engineering.",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b2",
      "b0"
    ],
    "c_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "c_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "c_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02782",
    "c_title":[
      "A Comprehensive Survey on Feature Extraction Techniques Using I\/Q\n  Imbalance in RFFI"
    ],
    "c_abstract":[
      "The proliferation of Internet of Things (IoT) devices has increased the need\nfor secure authentication. While traditional encryption-based solutions can be\nrobust, they often impose high computational and energy overhead on\nresource-limited IoT nodes. As an alternative, radio frequency fingerprint\nidentification (RFFI) leverages hardware-induced imperfections-such as\nInphase\/Quadrature (I\/Q) imbalance-in Radio Frequency (RF) front-end components\nas unique identifiers that are inherently difficult to clone or spoof. Despite\nrecent advances, significant challenges remain in standardizing feature\nextraction methods, maintaining high accuracy across diverse environments, and\nefficiently handling large-scale IoT deployments. This paper addresses these\ngaps by providing a comprehensive review of feature extraction techniques that\nutilize I\/Q imbalance for RFFI. We also discuss other hardware-based RF\nfingerprinting sources, including power amplifier nonlinearity and oscillator\nimperfections, and examine modern machine learning (ML) and deep learning (DL)\napproaches that enhance device identification performance."
    ],
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07799",
    "c_title":[
      "Atomic Norm Soft Thresholding for Sparse Time-frequency Representation"
    ],
    "c_abstract":[
      "Time-frequency (TF) representation of non-stationary signals typically\nrequires the effective concentration of energy distribution along the\ninstantaneous frequency (IF) ridge, which exhibits intrinsic sparsity. Inspired\nby the sparse optimization over continuum via atomic norm, a novel atomic norm\nsoft thresholding for sparse TF representation (AST-STF) method is proposed,\nwhich ensures accurate TF localization under the strong duality. Numerical\nexperiments demonstrate that the performance of the proposed method surpasses\nthat of conventional methods."
    ],
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.20648",
    "c_title":[
      "Reducing Complexity of Data-Aided Channel Estimation in RIS-Assisted\n  Communications"
    ],
    "c_abstract":[
      "We consider the data-aided channel estimation (CE) problem in a\nreconfigurable intelligent surface (RIS)-assisted wireless communication\nsystem, where the channel and information symbols are estimated jointly during\nthe CE phase, differently from pure pilot-aided methods. We propose a two-stage\nsemi-blind receiver that jointly estimates the combined channel and the data\nsymbols, followed by channel decoupling. To this end, we derive a new modeling\nframework whose first stage recasts the received signal to allow for the joint\nestimation of the combined channel and transmitted symbols. In the second\nstage, channel decoupling is easily achieved via Khatri-Rao factorization,\nyielding a refined channel estimate. Our solution yields accurate estimates of\nthe cascaded channel at lower computational complexity. Simulation results\nreveal a similar performance of the proposed method to that of the competitor\nwhile providing a substantially reduced computational cost."
    ],
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11949",
    "c_title":[
      "Low Range-Doppler Sidelobe ISAC Waveform Design: A Low-Complexity\n  Approach"
    ],
    "c_abstract":[
      "Integrated sensing and communication (ISAC) is a pivotal enabler for\nnext-generation wireless networks. A key challenge in ISAC systems lies in\ndesigning dual-functional waveforms that can achieve satisfactory radar sensing\naccuracy by effectively suppressing range-Doppler sidelobes. However, existing\nsolutions are often computationally intensive, limiting their practicality in\nmulti-input multi-output (MIMO) orthogonal frequency division multiplexing\n(OFDM) ISAC deployments. This paper presents a novel low-complexity algorithm\nleveraging the augmented Lagrangian method (ALM) and Riemannian conjugate\ngradient (RCG) optimization techniques to address these challenges. The\nproposed algorithm achieves superior sidelobe suppression compared to\nstate-of-the-art methods while dramatically reducing computational complexity,\nmaking it highly suitable for real-world MIMO-OFDM ISAC systems. Simulation\nresults demonstrate that the proposed approach not only outperforms existing\nbenchmarks in sidelobe reduction but also accelerates convergence, ensuring\nefficient performance across communication and sensing tasks."
    ],
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.14404",
    "c_title":[
      "A Concise Tutorial for Analyzing Electromagnetic Degrees of Freedom for\n  Continuous-Aperture Array (CAPA) Systems"
    ],
    "c_abstract":[
      "A concise tutorial is provided for analysis of the spatial degrees of freedom\n(DoFs) in continuous-aperture array (CAPA)-based continuous electromagnetic\n(EM) channels. First, a simplified spatial model is introduced using the\nFresnel approximation. By leveraging this model and Landau's theorem, a\nclosed-form expression for the spatial DoFs is derived. The results show that\nthe number of DoFs is proportional to the transmit and receive aperture sizes\nand inversely proportional to the propagation distance. Numerical results are\npresented to illustrate the properties of EM DoFs in CAPA-based channels."
    ],
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15443",
    "c_title":[
      "Are Elites Meritocratic and Efficiency-Seeking? Evidence from MBA\n  Students"
    ],
    "c_abstract":[
      "Elites disproportionately influence policymaking, yet little is known about\ntheir fairness and efficiency preferences -- key determinants of support for\nredistributive policies. We investigate these preferences using an incentivized\nlab experiment with a group of future elites -- Ivy League MBA students. We\nfind that elites implement more unequal earnings distributions than the average\nAmerican, are highly sensitive to both merit-based inequality and efficiency\ncosts of redistribution, and are less likely to hold strict meritocratic views.\nThese findings provide novel insights into how elites' redistributive\npreferences may shape high levels of inequality and limited redistributive\npolicy in the United States."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.19179",
    "c_title":[
      "Learning Non-Local Molecular Interactions via Equivariant Local\n  Representations and Charge Equilibration"
    ],
    "c_abstract":[
      "Graph Neural Network (GNN) potentials relying on chemical locality offer\nnear-quantum mechanical accuracy at significantly reduced computational costs.\nBy propagating local information to distance particles, Message-passing neural\nnetworks (MPNNs) extend the locality concept to model interactions beyond their\nlocal neighborhood. Still, this locality precludes modeling long-range effects,\nsuch as charge transfer, electrostatic interactions, and dispersion effects,\nwhich are critical to adequately describe many real-world systems. In this\nwork, we propose the Charge Equilibration Layer for Long-range Interactions\n(CELLI) to address the challenging modeling of non-local interactions and the\nhigh computational cost of MPNNs. This novel architecture generalizes the\nfourth-generation high-dimensional neural network (4GHDNN) concept, integrating\nthe charge equilibration (Qeq) method into a model-agnostic building block for\nmodern equivariant GNN potentials. A series of benchmarks show that CELLI can\nextend the strictly local Allegro architecture to model highly non-local\ninteractions and charge transfer. Our architecture generalizes to diverse\ndatasets and large structures, achieving an accuracy comparable to MPNNs at\nabout twice the computational efficiency."
    ],
    "c_categories":[
      "cs.LG",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07934",
    "c_title":[
      "Monotonicity and convergence of two-relaxation-times lattice Boltzmann\n  schemes for a non-linear conservation law"
    ],
    "c_abstract":[
      "We address the convergence analysis of lattice Boltzmann methods for scalar\nnon-linear conservation laws, focusing on two-relaxation-times (TRT) schemes.\nUnlike Finite Difference\/Finite Volume methods, lattice Boltzmann schemes offer\nexceptional computational efficiency and parallelization capabilities. However,\ntheir monotonicity and $L^{\\infty}$-stability remain underexplored. Extending\nexisting results on simpler BGK schemes, we derive conditions ensuring that TRT\nschemes are monotone and stable by leveraging their unique relaxation\nstructure. Our analysis culminates in proving convergence of the numerical\nsolution to the weak entropy solution of the conservation law. Compared to BGK\nschemes, TRT schemes achieve reduced numerical diffusion while retaining\nprovable convergence. Numerical experiments validate and illustrate the\ntheoretical findings."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12505",
    "c_title":[
      "A Conservative Partially Hyperbolic Dichotomy: Hyperbolicity versus\n  Nonhyperbolic Measures"
    ],
    "c_abstract":[
      "In a conservative and partially hyperbolic three-dimensional setting, we\nstudy three representative classes of diffeomorphisms: those homotopic to\nAnosov (or Derived from Anosov diffeomorphisms), diffeomorphisms in\nneighborhoods of the time-one map of the geodesic flow on a surface of negative\ncurvature, and accessible and dynamically coherent skew products with circle\nfibers. In any of these classes, we establish the following dichotomy: either\nthe diffeomorphism is Anosov, or it possesses nonhyperbolic ergodic measures.\nOur approach is perturbation-free and combines recent advances in the study of\nstably ergodic diffeomorphisms with a variation of the periodic approximation\nmethod to obtain ergodic measures.\n  A key result in our construction, independent of conservative hypotheses, is\nthe construction of nonhyperbolic ergodic measures for sets with a minimal\nstrong unstable foliation that satisfy the mostly expanding property. This\napproach enables us to obtain nonhyperbolic ergodic measures in other contexts,\nincluding some subclasses of the so-called anomalous partially hyperbolic\ndiffeomorphisms that are not dynamically coherent."
    ],
    "c_categories":[
      "math.DS"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06363",
    "c_title":[
      "Improved Regret Analysis in Gaussian Process Bandits: Optimality for\n  Noiseless Reward, RKHS norm, and Non-Stationary Variance"
    ],
    "c_abstract":[
      "We study the Gaussian process (GP) bandit problem, whose goal is to minimize\nregret under an unknown reward function lying in some reproducing kernel\nHilbert space (RKHS). The maximum posterior variance analysis is vital in\nanalyzing near-optimal GP bandit algorithms such as maximum variance reduction\n(MVR) and phased elimination (PE). Therefore, we first show the new upper bound\nof the maximum posterior variance, which improves the dependence of the noise\nvariance parameters of the GP. By leveraging this result, we refine the MVR and\nPE to obtain (i) a nearly optimal regret upper bound in the noiseless setting\nand (ii) regret upper bounds that are optimal with respect to the RKHS norm of\nthe reward function. Furthermore, as another application of our proposed bound,\nwe analyze the GP bandit under the time-varying noise variance setting, which\nis the kernelized extension of the linear bandit with heteroscedastic noise.\nFor this problem, we show that MVR and PE-based algorithms achieve noise\nvariance-dependent regret upper bounds, which matches our regret lower bound."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04258",
    "c_title":[
      "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors"
    ],
    "c_abstract":[
      "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.00508",
    "c_title":[
      "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL"
    ],
    "c_abstract":[
      "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09979",
    "c_title":[
      "Silicon is the next frontier in plant synthetic biology"
    ],
    "c_abstract":[
      "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11326",
    "c_title":[
      "Deep Learning of Proteins with Local and Global Regions of Disorder"
    ],
    "c_abstract":[
      "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18909",
    "c_title":[
      "Nonsuppressible viremia during HIV-1 therapy meets molecular virology"
    ],
    "c_abstract":[
      "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15703",
    "c_title":[
      "Floquet optical selection rules in black phosphorus"
    ],
    "c_abstract":[
      "The optical selection rules endorsed by symmetry are crucial for\nunderstanding the optical properties of quantum materials and the associated\nultrafast spectral phenomena. Herein, we introduce momentum-resolved Floquet\noptical selection rules using the group theory to elucidate the pump-probe\nphotoemission spectral distributions of monolayer black phosphorus (BP), which\nare governed by the symmetries of both the material and the lasers. Using\ntime-dependent density functional theory (TDDFT), we further investigate the\ndynamical evolution of Floquet(-Volkov) states in the photoemission spectra of\nmonolayer BP, revealing their spectral weights at specific momenta for each\nsideband. These observations are comprehensively explained by the proposed\nFloquet optical selection rules. Our framework not only clarifies experimental\nphotoemission spectra but also uncovers novel characteristics under different\npump-probe configurations. Our results are expected to deepen the understanding\nof light-induced ultrafast spectra in BP and can be further extended to other\nFloquet systems."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17310",
    "c_title":[
      "Hyperfine and Zeeman Optical Pumping and Transverse Laser Cooling of a\n  Thermal Atomic Beam of Dysprosium Using a Single 421 nm Laser"
    ],
    "c_abstract":[
      "We demonstrate the effect of Zeeman and hyperfine optical pumping and\ntransverse laser cooling of a dysprosium (Dy) atomic beam on the $4f^{10}6s^2(J\n= 8) \\rightarrow 4f^{10}6s6p(J = 9)$ transition at 421.291 nm. For $^{163}$Dy,\nan electro-optic modulator is used to generate five frequency sidebands\nrequired to pump the atoms to the $F = 10.5$ ground state hyperfine level and\nthe light polarization is chosen to pump the atoms to the $m_F = 10.5$ Zeeman\nsublevel. The atoms are simultaneously laser-cooled using a standing wave\northogonal to the atomic beam. The resulting polarized and cooled atomic beam\nwill be used in fundamental physics experiments taking advantage of the\naccidental degeneracy of excited states in Dy including the ongoing measurement\nof parity violation in this system."
    ],
    "c_categories":[
      "physics.atom-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10158",
    "c_title":[
      "Combinatorial Reinforcement Learning with Preference Feedback"
    ],
    "c_abstract":[
      "In this paper, we consider combinatorial reinforcement learning with\npreference feedback, where a learning agent sequentially offers an action--an\nassortment of multiple items to--a user, whose preference feedback follows a\nmultinomial logistic (MNL) model. This framework allows us to model real-world\nscenarios, particularly those involving long-term user engagement, such as in\nrecommender systems and online advertising. However, this framework faces two\nmain challenges: (1) the unknown value of each item, unlike traditional MNL\nbandits that only address single-step preference feedback, and (2) the\ndifficulty of ensuring optimism while maintaining tractable assortment\nselection in the combinatorial action space with unknown values. In this paper,\nwe assume a contextual MNL preference model, where the mean utilities are\nlinear, and the value of each item is approximated by a general function. We\npropose an algorithm, MNL-VQL, that addresses these challenges, making it both\ncomputationally and statistically efficient. As a special case, for linear MDPs\n(with the MNL preference feedback), we establish the first regret lower bound\nin this framework and show that MNL-VQL achieves nearly minimax-optimal regret.\nTo the best of our knowledge, this is the first work to provide statistical\nguarantees in combinatorial RL with preference feedback."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.18183",
    "c_title":[
      "On the relative Nullstellensatz in nonarchimedean geometry"
    ],
    "c_abstract":[
      "We establish a relative version of the Nullstellensatz for algebras\ntopologically of finite type over a given Banach Tate ring $A$, under the\nassumption that the corresponding statement holds for rational localizations of\n$A$. This applies in particular to pseudoaffinoid algebras and to the\ncoordinate rings of affinoid subspaces of a Fargues--Fontaine curve."
    ],
    "c_categories":[
      "math.AG",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18669",
    "c_title":[
      "Lie theory of the slice Riemannian geometry on the quaternionic unit\n  ball"
    ],
    "c_abstract":[
      "The quaternionic unit ball carries a Riemannian metric built using regular\nM\\\"obius transformations: the slice Riemannian metric. We prove that the\ngeometry induced by this metric is strongly related to the group\n$\\mathrm{Sp}(1,1)$. We also develop the foundations for a Lie theoretic study\nof the slice Riemannian metric. In particular, we compute its isometry group\nand prove that it is built from symmetries of the Lie group $\\mathrm{Sp}(1,1)$.\nWe also compare the slice Riemannian geometry with the quaternionic Poincar\\'e\ngeometry, where the latter is considered within the setup of Riemannian\nsymmetric spaces."
    ],
    "c_categories":[
      "math.CV",
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"Artificial Intelligence-Enhanced Couinaud Segmentation for Precision\n  Liver Cancer Therapy",
    "a_abstract":"Precision therapy for liver cancer necessitates accurately delineating liver\nsub-regions to protect healthy tissue while targeting tumors, which is\nessential for reducing recurrence and improving survival rates. However, the\nsegmentation of hepatic segments, known as Couinaud segmentation, is\nchallenging due to indistinct sub-region boundaries and the need for extensive\nannotated datasets. This study introduces LiverFormer, a novel Couinaud\nsegmentation model that effectively integrates global context with low-level\nlocal features based on a 3D hybrid CNN-Transformer architecture. Additionally,\na registration-based data augmentation strategy is equipped to enhance the\nsegmentation performance with limited labeled data. Evaluated on CT images from\n123 patients, LiverFormer demonstrated high accuracy and strong concordance\nwith expert annotations across various metrics, allowing for enhanced treatment\nplanning for surgery and radiation therapy. It has great potential to reduces\ncomplications and minimizes potential damages to surrounding tissue, leading to\nimproved outcomes for patients undergoing complex liver cancer treatments.",
    "explanation":"The paper presents a tool that uses Convolutional Neural Networks (CNN) and Transformers, technologies from Computer Science, to improve the accuracy and efficiency of Couinaud segmentation in liver cancer treatment, a challenge in the field of Medicine.",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b13"
    ],
    "c_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "c_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18661",
    "c_title":[
      "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling"
    ],
    "c_abstract":[
      "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03661",
    "c_title":[
      "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues"
    ],
    "c_abstract":[
      "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12234",
    "c_title":[
      "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer"
    ],
    "c_abstract":[
      "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13516",
    "c_title":[
      "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols"
    ],
    "c_abstract":[
      "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15703",
    "c_title":[
      "Floquet optical selection rules in black phosphorus"
    ],
    "c_abstract":[
      "The optical selection rules endorsed by symmetry are crucial for\nunderstanding the optical properties of quantum materials and the associated\nultrafast spectral phenomena. Herein, we introduce momentum-resolved Floquet\noptical selection rules using the group theory to elucidate the pump-probe\nphotoemission spectral distributions of monolayer black phosphorus (BP), which\nare governed by the symmetries of both the material and the lasers. Using\ntime-dependent density functional theory (TDDFT), we further investigate the\ndynamical evolution of Floquet(-Volkov) states in the photoemission spectra of\nmonolayer BP, revealing their spectral weights at specific momenta for each\nsideband. These observations are comprehensively explained by the proposed\nFloquet optical selection rules. Our framework not only clarifies experimental\nphotoemission spectra but also uncovers novel characteristics under different\npump-probe configurations. Our results are expected to deepen the understanding\nof light-induced ultrafast spectra in BP and can be further extended to other\nFloquet systems."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17310",
    "c_title":[
      "Hyperfine and Zeeman Optical Pumping and Transverse Laser Cooling of a\n  Thermal Atomic Beam of Dysprosium Using a Single 421 nm Laser"
    ],
    "c_abstract":[
      "We demonstrate the effect of Zeeman and hyperfine optical pumping and\ntransverse laser cooling of a dysprosium (Dy) atomic beam on the $4f^{10}6s^2(J\n= 8) \\rightarrow 4f^{10}6s6p(J = 9)$ transition at 421.291 nm. For $^{163}$Dy,\nan electro-optic modulator is used to generate five frequency sidebands\nrequired to pump the atoms to the $F = 10.5$ ground state hyperfine level and\nthe light polarization is chosen to pump the atoms to the $m_F = 10.5$ Zeeman\nsublevel. The atoms are simultaneously laser-cooled using a standing wave\northogonal to the atomic beam. The resulting polarized and cooled atomic beam\nwill be used in fundamental physics experiments taking advantage of the\naccidental degeneracy of excited states in Dy including the ongoing measurement\nof parity violation in this system."
    ],
    "c_categories":[
      "physics.atom-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10158",
    "c_title":[
      "Combinatorial Reinforcement Learning with Preference Feedback"
    ],
    "c_abstract":[
      "In this paper, we consider combinatorial reinforcement learning with\npreference feedback, where a learning agent sequentially offers an action--an\nassortment of multiple items to--a user, whose preference feedback follows a\nmultinomial logistic (MNL) model. This framework allows us to model real-world\nscenarios, particularly those involving long-term user engagement, such as in\nrecommender systems and online advertising. However, this framework faces two\nmain challenges: (1) the unknown value of each item, unlike traditional MNL\nbandits that only address single-step preference feedback, and (2) the\ndifficulty of ensuring optimism while maintaining tractable assortment\nselection in the combinatorial action space with unknown values. In this paper,\nwe assume a contextual MNL preference model, where the mean utilities are\nlinear, and the value of each item is approximated by a general function. We\npropose an algorithm, MNL-VQL, that addresses these challenges, making it both\ncomputationally and statistically efficient. As a special case, for linear MDPs\n(with the MNL preference feedback), we establish the first regret lower bound\nin this framework and show that MNL-VQL achieves nearly minimax-optimal regret.\nTo the best of our knowledge, this is the first work to provide statistical\nguarantees in combinatorial RL with preference feedback."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.18183",
    "c_title":[
      "On the relative Nullstellensatz in nonarchimedean geometry"
    ],
    "c_abstract":[
      "We establish a relative version of the Nullstellensatz for algebras\ntopologically of finite type over a given Banach Tate ring $A$, under the\nassumption that the corresponding statement holds for rational localizations of\n$A$. This applies in particular to pseudoaffinoid algebras and to the\ncoordinate rings of affinoid subspaces of a Fargues--Fontaine curve."
    ],
    "c_categories":[
      "math.AG",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18669",
    "c_title":[
      "Lie theory of the slice Riemannian geometry on the quaternionic unit\n  ball"
    ],
    "c_abstract":[
      "The quaternionic unit ball carries a Riemannian metric built using regular\nM\\\"obius transformations: the slice Riemannian metric. We prove that the\ngeometry induced by this metric is strongly related to the group\n$\\mathrm{Sp}(1,1)$. We also develop the foundations for a Lie theoretic study\nof the slice Riemannian metric. In particular, we compute its isometry group\nand prove that it is built from symmetries of the Lie group $\\mathrm{Sp}(1,1)$.\nWe also compare the slice Riemannian geometry with the quaternionic Poincar\\'e\ngeometry, where the latter is considered within the setup of Riemannian\nsymmetric spaces."
    ],
    "c_categories":[
      "math.CV",
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16065",
    "c_title":[
      "CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person\n  Re-Identification"
    ],
    "c_abstract":[
      "The Visual Language Model, known for its robust cross-modal capabilities, has\nbeen extensively applied in various computer vision tasks. In this paper, we\nexplore the use of CLIP (Contrastive Language-Image Pretraining), a\nvision-language model pretrained on large-scale image-text pairs to align\nvisual and textual features, for acquiring fine-grained and domain-invariant\nrepresentations in generalizable person re-identification. The adaptation of\nCLIP to the task presents two primary challenges: learning more fine-grained\nfeatures to enhance discriminative ability, and learning more domain-invariant\nfeatures to improve the model's generalization capabilities. To mitigate the\nfirst challenge thereby enhance the ability to learn fine-grained features, a\nthree-stage strategy is proposed to boost the accuracy of text descriptions.\nInitially, the image encoder is trained to effectively adapt to person\nre-identification tasks. In the second stage, the features extracted by the\nimage encoder are used to generate textual descriptions (i.e., prompts) for\neach image. Finally, the text encoder with the learned prompts is employed to\nguide the training of the final image encoder. To enhance the model's\ngeneralization capabilities to unseen domains, a bidirectional guiding method\nis introduced to learn domain-invariant image features. Specifically,\ndomain-invariant and domain-relevant prompts are generated, and both positive\n(pulling together image features and domain-invariant prompts) and negative\n(pushing apart image features and domain-relevant prompts) views are used to\ntrain the image encoder. Collectively, these strategies contribute to the\ndevelopment of an innovative CLIP-based framework for learning fine-grained\ngeneralized features in person re-identification."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10069",
    "c_title":[
      "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks"
    ],
    "c_abstract":[
      "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05862",
    "c_title":[
      "Language-Inspired Relation Transfer for Few-shot Class-Incremental\n  Learning"
    ],
    "c_abstract":[
      "Depicting novel classes with language descriptions by observing few-shot\nsamples is inherent in human-learning systems. This lifelong learning\ncapability helps to distinguish new knowledge from old ones through the\nincrease of open-world learning, namely Few-Shot Class-Incremental Learning\n(FSCIL). Existing works to solve this problem mainly rely on the careful tuning\nof visual encoders, which shows an evident trade-off between the base knowledge\nand incremental ones. Motivated by human learning systems, we propose a new\nLanguage-inspired Relation Transfer (LRT) paradigm to understand objects by\njoint visual clues and text depictions, composed of two major steps. We first\ntransfer the pretrained text knowledge to the visual domains by proposing a\ngraph relation transformation module and then fuse the visual and language\nembedding by a text-vision prototypical fusion module. Second, to mitigate the\ndomain gap caused by visual finetuning, we propose context prompt learning for\nfast domain alignment and imagined contrastive learning to alleviate the\ninsufficient text data during alignment. With collaborative learning of domain\nalignments and text-image transfer, our proposed LRT outperforms the\nstate-of-the-art models by over $13\\%$ and $7\\%$ on the final session of\nmini-ImageNet and CIFAR-100 FSCIL benchmarks."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15934",
    "c_title":[
      "Dissecting Human Body Representations in Deep Networks Trained for\n  Person Identification"
    ],
    "c_abstract":[
      "Long-term body identification algorithms have emerged recently with the\nincreased availability of high-quality training data. We seek to fill knowledge\ngaps about these models by analyzing body image embeddings from four body\nidentification networks trained with 1.9 million images across 4,788 identities\nand 9 databases. By analyzing a diverse range of architectures (ViT, SWIN-ViT,\nCNN, and linguistically primed CNN), we first show that the face contributes to\nthe accuracy of body identification algorithms and that these algorithms can\nidentify faces to some extent -- with no explicit face training. Second, we\nshow that representations (embeddings) generated by body identification\nalgorithms encode information about gender, as well as image-based information\nincluding view (yaw) and even the dataset from which the image originated.\nThird, we demonstrate that identification accuracy can be improved without\nadditional training by operating directly and selectively on the learned\nembedding space. Leveraging principal component analysis (PCA), identity\ncomparisons were consistently more accurate in subspaces that eliminated\ndimensions that explained large amounts of variance. These three findings were\nsurprisingly consistent across architectures and test datasets. This work\nrepresents the first analysis of body representations produced by long-term\nre-identification networks trained on challenging unconstrained datasets."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.17706",
    "c_title":[
      "IBURD: Image Blending for Underwater Robotic Detection"
    ],
    "c_abstract":[
      "We present an image blending pipeline, \\textit{IBURD}, that creates realistic\nsynthetic images to assist in the training of deep detectors for use on\nunderwater autonomous vehicles (AUVs) for marine debris detection tasks.\nSpecifically, IBURD generates both images of underwater debris and their\npixel-level annotations, using source images of debris objects, their\nannotations, and target background images of marine environments. With Poisson\nediting and style transfer techniques, IBURD is even able to robustly blend\ntransparent objects into arbitrary backgrounds and automatically adjust the\nstyle of blended images using the blurriness metric of target background\nimages. These generated images of marine debris in actual underwater\nbackgrounds address the data scarcity and data variety problems faced by\ndeep-learned vision algorithms in challenging underwater conditions, and can\nenable the use of AUVs for environmental cleanup missions. Both quantitative\nand robotic evaluations of IBURD demonstrate the efficacy of the proposed\napproach for robotic detection of marine debris."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12407",
    "c_title":[
      "Quantum-mechanical numerical model of interaction between dark atom and\n  nucleus of substance"
    ],
    "c_abstract":[
      "The hypothesis of composite $XHe$ dark atoms may provide solution to the\nlong-standing problem of direct searches for dark matter particles. The main\nproblem of the $XHe$ dark atom is its ability to strongly interact with the\nnucleus of substance, arising from the unshielded nuclear attraction between\nthe helium nucleus and the nucleus of matter. It is assumed that in order to\nprevent the destruction of the bound structure of dark atom, the effective\npotential of interaction between $XHe$ and the nucleus of substance must have\ndipole Coulomb barrier that prevents the fusion of dark matter atom particles\nwith the nucleus of substance. The problem in describing the interaction\nbetween dark atom and substance nucleus is the three-body problem, for which an\nexact analytical solution is not available. Consequently, to assess the\nphysical meaning of the proposed scenario, it is essential to develop a\nnumerical approach. Our approach involves consistently developing an accurate\nquantum mechanical description of this three-body system, comprising bound dark\natom and the external nucleus of substance. We incorporate the necessary\neffects and interactions to enhance the precision of the results, which helps\nto elucidate the most significant aspects of the proposed dark atom scenario."
    ],
    "c_categories":[
      "hep-ph",
      "physics.atom-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.00833",
    "c_title":[
      "The Second Main Theorem with moving hypersurfaces in subgeneral position"
    ],
    "c_abstract":[
      "In this paper, we prove a second main theorem for a holomorphic curve $f$\ninto $\\mathbb P^N (\\mathbb C)$ with a family of slowly moving hypersurfaces\n$D_1,...,D_q$ with respect to $f$ in $m$-subgeneral position, proving an\ninequality with factor $3 \\over 2$. The motivation comes from the recent result\nof Heier and Levin."
    ],
    "c_categories":[
      "math.CV"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.08475",
    "c_title":[
      "The generic extension map and modular standard modules"
    ],
    "c_abstract":[
      "In this paper we study two classes of $\\ell$-modular standard modules of the\ngeneral linear group. The first class is obtained by reducing existing standard\nmodules over $\\overline{\\mathbb{Q}}_\\ell$ to $\\overline{\\mathbb{F}}_\\ell$ with\nrespect to their natural integral structure. The second class is obtained by\nstudying the generic extension map of the cyclical quiver, which was motivated\nby the construction of certain monomial bases of quantum algebras. In the later\ncase we also manage to prove a modular version of the Langlands classification,\nsimilar to the work of Langlands and Zelevinsky over $\\mathbb{C}$. We moreover\ncompute the $\\ell$-modular Rankin-Selberg $L$-function of both classes and\ncheck that they agree with the $L$-functions of their $\\mathrm{C}$-parameters\nconstructed by Kurinczuk and Matringe."
    ],
    "c_categories":[
      "math.NT",
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17585",
    "c_title":[
      "How do Massive Primordial Black Holes Impact the Formation of the First\n  Stars and Galaxies?"
    ],
    "c_abstract":[
      "We investigate the impact of massive primordial black holes (PBHs; $m_{\\rm\nBH}\\sim 10^6~M_{\\odot}$) on the star formation and first galaxy assembly\nprocess using high-resolution hydrodynamical simulations from $z = 1100$ to $z\n\\sim 9$. We find that PBH accretion is self-regulated by feedback, suppressing\nmass growth unless feedback is weak. PBHs accelerate structure formation by\nseeding dark matter halos and gravitationally attracting gas, but strong\nfeedback can delay cooling and suppress star formation. In addition, the\npresence of baryon-dark matter streaming creates an offset between the PBH\nlocation and the peaks induced in gas density, promoting earlier and more\nefficient star formation compared to standard $\\Lambda$CDM. By $z \\sim 10$,\nPBH-seeded galaxies form dense star clusters, with PBH-to-stellar mass ratios\ncomparable to observed high-$z$ AGN like UHZ-1. Our results support PBHs as\nviable SMBH seeds but do not exclude alternative scenarios. We emphasize that\nPBH-seeding provides a natural explanation for some of the newly-discovered\novermassive SMBHs at high redshift, in particular those with extreme ratios of\nBH-to-dynamical (virial) mass that challenge standard formation channels.\nFuture studies with ultra-deep JWST surveys, the Roman Space Telescope, and\nradio surveys with facilities such as SKA and HERA will be critical in\ndistinguishing PBH-driven SMBH growth from other pathways."
    ],
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11879",
    "c_title":[
      "Magnetic Fields or Overstable Convective Modes in HR 7495: Exploring the\n  Underlying Causes of the Spike in the 'Hump & Spike' Features"
    ],
    "c_abstract":[
      "More than 200 A- and F-type stars observed with Kepler exhibit a distinctive\n'hump & spike' feature in their Fourier spectra. The hump is commonly\ninterpreted as unresolved Rossby modes, while the spike has been linked to\nrotational modulation. Two competing interpretations exist for the spike:\nmagnetic phenomena, such as stellar spots, or Overstable Convective (OsC) modes\nresonantly exciting low-frequency g modes within the stellar envelope.\n  We analysed photometric data from Kepler and TESS for HR 7495, the brightest\n'hump & spike' star (V=5.06), covering 4.5 years and four seasons,\nrespectively. Additionally, radial velocity measurements and\nspectropolarimetric data were used to investigate magnetic fields and surface\nfeatures. Furthermore, we analysed model-based artificial light and radial\nvelocity curves to examine the influence of OsC modes on the phase-folded light\ncurves.\n  The phase-folded light curves show that the spike characteristics of HR 7495\nalign more closely with rotational modulation by stellar spots than with OsC\nmodes. No significant magnetic fields were detected, limiting the field's\npossible amplitude and geometry. This supports the hypothesis of a subsurface\nconvective layer operating a dynamo, producing low-amplitude, complex magnetic\nfields. The variability patterns suggest multiple evolving spots. A comparison\nof contemporaneously observed light and RV data with modelled OsC modes reveals\na 0.5 phase offset, strongly disfavouring pulsations as the cause of the spike.\n  While the evolutionary stage of HR 7495 does not entirely preclude the\npossibility of OsC modes, the observational data overwhelmingly support the\nstellar spots hypothesis. Our analysis, combined with previous literature,\nsuggests that if not all A- and F-type, at least the 'hump & spike' stars,\nharbour an undetected weak magnetic field, likely driven by a dynamo mechanism."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"Automated Classification of Cell Shapes: A Comparative Evaluation of\n  Shape Descriptors",
    "a_abstract":"This study addresses the challenge of classifying cell shapes from noisy\ncontours, such as those obtained through cell instance segmentation of\nhistological images. We assess the performance of various features for shape\nclassification, including Elliptical Fourier Descriptors, curvature features,\nand lower dimensional representations. Using an annotated synthetic dataset of\nnoisy contours, we identify the most suitable shape descriptors and apply them\nto a set of real images for qualitative analysis. Our aim is to provide a\ncomprehensive evaluation of descriptors for classifying cell shapes, which can\nsupport cell type identification and tissue characterization-critical tasks in\nboth biological research and histopathological assessments.",
    "explanation":"This study addresses the challenge of classifying cell shapes from noisy contours, such as those obtained through cell instance segmentation of histological images.\n\nOur aim is to provide a comprehensive evaluation of descriptors for classifying cell shapes, which can support cell type identification and tissue characterization\u2014critical tasks in both biological research and histopathological assessments.\n",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b4"
    ],
    "c_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "c_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05607",
    "c_title":[
      "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features"
    ],
    "c_abstract":[
      "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.16996",
    "c_title":[
      "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation"
    ],
    "c_abstract":[
      "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01376",
    "c_title":[
      "Pushing the boundaries of Structure-Based Drug Design through\n  Collaboration with Large Language Models"
    ],
    "c_abstract":[
      "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03606",
    "c_title":[
      "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery"
    ],
    "c_abstract":[
      "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07671",
    "c_title":[
      "Steering Protein Family Design through Profile Bayesian Flow"
    ],
    "c_abstract":[
      "Protein family design emerges as a promising alternative by combining the\nadvantages of de novo protein design and mutation-based directed evolution.In\nthis paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for\nspecifically generative modeling of protein families. ProfileBFN extends the\ndiscrete Bayesian Flow Network from an MSA profile perspective, which can be\ntrained on single protein sequences by regarding it as a degenerate profile,\nthereby achieving efficient protein family design by avoiding large-scale MSA\ndata construction and training. Empirical results show that ProfileBFN has a\nprofound understanding of proteins. When generating diverse and novel family\nproteins, it can accurately capture the structural characteristics of the\nfamily. The enzyme produced by this method is more likely than the previous\napproach to have the corresponding function, offering better odds of generating\ndiverse proteins with the desired functionality."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12407",
    "c_title":[
      "Quantum-mechanical numerical model of interaction between dark atom and\n  nucleus of substance"
    ],
    "c_abstract":[
      "The hypothesis of composite $XHe$ dark atoms may provide solution to the\nlong-standing problem of direct searches for dark matter particles. The main\nproblem of the $XHe$ dark atom is its ability to strongly interact with the\nnucleus of substance, arising from the unshielded nuclear attraction between\nthe helium nucleus and the nucleus of matter. It is assumed that in order to\nprevent the destruction of the bound structure of dark atom, the effective\npotential of interaction between $XHe$ and the nucleus of substance must have\ndipole Coulomb barrier that prevents the fusion of dark matter atom particles\nwith the nucleus of substance. The problem in describing the interaction\nbetween dark atom and substance nucleus is the three-body problem, for which an\nexact analytical solution is not available. Consequently, to assess the\nphysical meaning of the proposed scenario, it is essential to develop a\nnumerical approach. Our approach involves consistently developing an accurate\nquantum mechanical description of this three-body system, comprising bound dark\natom and the external nucleus of substance. We incorporate the necessary\neffects and interactions to enhance the precision of the results, which helps\nto elucidate the most significant aspects of the proposed dark atom scenario."
    ],
    "c_categories":[
      "hep-ph",
      "physics.atom-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.00833",
    "c_title":[
      "The Second Main Theorem with moving hypersurfaces in subgeneral position"
    ],
    "c_abstract":[
      "In this paper, we prove a second main theorem for a holomorphic curve $f$\ninto $\\mathbb P^N (\\mathbb C)$ with a family of slowly moving hypersurfaces\n$D_1,...,D_q$ with respect to $f$ in $m$-subgeneral position, proving an\ninequality with factor $3 \\over 2$. The motivation comes from the recent result\nof Heier and Levin."
    ],
    "c_categories":[
      "math.CV"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08475",
    "c_title":[
      "The generic extension map and modular standard modules"
    ],
    "c_abstract":[
      "In this paper we study two classes of $\\ell$-modular standard modules of the\ngeneral linear group. The first class is obtained by reducing existing standard\nmodules over $\\overline{\\mathbb{Q}}_\\ell$ to $\\overline{\\mathbb{F}}_\\ell$ with\nrespect to their natural integral structure. The second class is obtained by\nstudying the generic extension map of the cyclical quiver, which was motivated\nby the construction of certain monomial bases of quantum algebras. In the later\ncase we also manage to prove a modular version of the Langlands classification,\nsimilar to the work of Langlands and Zelevinsky over $\\mathbb{C}$. We moreover\ncompute the $\\ell$-modular Rankin-Selberg $L$-function of both classes and\ncheck that they agree with the $L$-functions of their $\\mathrm{C}$-parameters\nconstructed by Kurinczuk and Matringe."
    ],
    "c_categories":[
      "math.NT",
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17585",
    "c_title":[
      "How do Massive Primordial Black Holes Impact the Formation of the First\n  Stars and Galaxies?"
    ],
    "c_abstract":[
      "We investigate the impact of massive primordial black holes (PBHs; $m_{\\rm\nBH}\\sim 10^6~M_{\\odot}$) on the star formation and first galaxy assembly\nprocess using high-resolution hydrodynamical simulations from $z = 1100$ to $z\n\\sim 9$. We find that PBH accretion is self-regulated by feedback, suppressing\nmass growth unless feedback is weak. PBHs accelerate structure formation by\nseeding dark matter halos and gravitationally attracting gas, but strong\nfeedback can delay cooling and suppress star formation. In addition, the\npresence of baryon-dark matter streaming creates an offset between the PBH\nlocation and the peaks induced in gas density, promoting earlier and more\nefficient star formation compared to standard $\\Lambda$CDM. By $z \\sim 10$,\nPBH-seeded galaxies form dense star clusters, with PBH-to-stellar mass ratios\ncomparable to observed high-$z$ AGN like UHZ-1. Our results support PBHs as\nviable SMBH seeds but do not exclude alternative scenarios. We emphasize that\nPBH-seeding provides a natural explanation for some of the newly-discovered\novermassive SMBHs at high redshift, in particular those with extreme ratios of\nBH-to-dynamical (virial) mass that challenge standard formation channels.\nFuture studies with ultra-deep JWST surveys, the Roman Space Telescope, and\nradio surveys with facilities such as SKA and HERA will be critical in\ndistinguishing PBH-driven SMBH growth from other pathways."
    ],
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11879",
    "c_title":[
      "Magnetic Fields or Overstable Convective Modes in HR 7495: Exploring the\n  Underlying Causes of the Spike in the 'Hump & Spike' Features"
    ],
    "c_abstract":[
      "More than 200 A- and F-type stars observed with Kepler exhibit a distinctive\n'hump & spike' feature in their Fourier spectra. The hump is commonly\ninterpreted as unresolved Rossby modes, while the spike has been linked to\nrotational modulation. Two competing interpretations exist for the spike:\nmagnetic phenomena, such as stellar spots, or Overstable Convective (OsC) modes\nresonantly exciting low-frequency g modes within the stellar envelope.\n  We analysed photometric data from Kepler and TESS for HR 7495, the brightest\n'hump & spike' star (V=5.06), covering 4.5 years and four seasons,\nrespectively. Additionally, radial velocity measurements and\nspectropolarimetric data were used to investigate magnetic fields and surface\nfeatures. Furthermore, we analysed model-based artificial light and radial\nvelocity curves to examine the influence of OsC modes on the phase-folded light\ncurves.\n  The phase-folded light curves show that the spike characteristics of HR 7495\nalign more closely with rotational modulation by stellar spots than with OsC\nmodes. No significant magnetic fields were detected, limiting the field's\npossible amplitude and geometry. This supports the hypothesis of a subsurface\nconvective layer operating a dynamo, producing low-amplitude, complex magnetic\nfields. The variability patterns suggest multiple evolving spots. A comparison\nof contemporaneously observed light and RV data with modelled OsC modes reveals\na 0.5 phase offset, strongly disfavouring pulsations as the cause of the spike.\n  While the evolutionary stage of HR 7495 does not entirely preclude the\npossibility of OsC modes, the observational data overwhelmingly support the\nstellar spots hypothesis. Our analysis, combined with previous literature,\nsuggests that if not all A- and F-type, at least the 'hump & spike' stars,\nharbour an undetected weak magnetic field, likely driven by a dynamo mechanism."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.03137",
    "c_title":[
      "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver"
    ],
    "c_abstract":[
      "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08235",
    "c_title":[
      "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in\n  Agentic Tasks"
    ],
    "c_abstract":[
      "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving\ncapabilities, but their effectiveness in interactive environments can be\nlimited. This paper introduces and analyzes overthinking in LRMs. A phenomenon\nwhere models favor extended internal reasoning chains over environmental\ninteraction. Through experiments on software engineering tasks using SWE Bench\nVerified, we observe three recurring patterns: Analysis Paralysis, Rogue\nActions, and Premature Disengagement. We propose a framework to study these\nbehaviors, which correlates with human expert assessments, and analyze 4018\ntrajectories. We observe that higher overthinking scores correlate with\ndecreased performance, with reasoning models exhibiting stronger tendencies\ntoward overthinking compared to non-reasoning models. Our analysis reveals that\nsimple efforts to mitigate overthinking in agentic environments, such as\nselecting the solution with the lower overthinking score, can improve model\nperformance by almost 30% while reducing computational costs by 43%. These\nresults suggest that mitigating overthinking has strong practical implications.\nWe suggest that by leveraging native function-calling capabilities and\nselective reinforcement learning overthinking tendencies could be mitigated. We\nalso open-source our evaluation framework and dataset to facilitate research in\nthis direction at https:\/\/github.com\/AlexCuadron\/Overthinking."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00726",
    "c_title":[
      "Perspectives for Direct Interpretability in Multi-Agent Deep\n  Reinforcement Learning"
    ],
    "c_abstract":[
      "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in\nsolving complex problems in robotics or games, yet most of the trained models\nare hard to interpret. While learning intrinsically interpretable models\nremains a prominent approach, its scalability and flexibility are limited in\nhandling complex tasks or multi-agent dynamics. This paper advocates for direct\ninterpretability, generating post hoc explanations directly from trained\nmodels, as a versatile and scalable alternative, offering insights into agents'\nbehaviour, emergent phenomena, and biases without altering models'\narchitectures. We explore modern methods, including relevance backpropagation,\nknowledge edition, model steering, activation patching, sparse autoencoders and\ncircuit discovery, to highlight their applicability to single-agent,\nmulti-agent, and training process challenges. By addressing MADRL\ninterpretability, we propose directions aiming to advance active topics such as\nteam identification, swarm coordination and sample efficiency."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07970",
    "c_title":[
      "Comprehensive Metapath-based Heterogeneous Graph Transformer for\n  Gene-Disease Association Prediction"
    ],
    "c_abstract":[
      "Discovering gene-disease associations is crucial for understanding disease\nmechanisms, yet identifying these associations remains challenging due to the\ntime and cost of biological experiments. Computational methods are increasingly\nvital for efficient and scalable gene-disease association prediction.\nGraph-based learning models, which leverage node features and network\nrelationships, are commonly employed for biomolecular predictions. However,\nexisting methods often struggle to effectively integrate node features,\nheterogeneous structures, and semantic information. To address these\nchallenges, we propose COmprehensive MEtapath-based heterogeneous graph\nTransformer(COMET) for predicting gene-disease associations. COMET integrates\ndiverse datasets to construct comprehensive heterogeneous networks,\ninitializing node features with BioGPT. We define seven Metapaths and utilize a\ntransformer framework to aggregate Metapath instances, capturing global\ncontexts and long-distance dependencies. Through intra- and inter-metapath\naggregation using attention mechanisms, COMET fuses latent vectors from\nmultiple Metapaths to enhance GDA prediction accuracy. Our method demonstrates\nsuperior robustness compared to state-of-the-art approaches. Ablation studies\nand visualizations validate COMET's effectiveness, providing valuable insights\nfor advancing human health research."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12761",
    "c_title":[
      "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning"
    ],
    "c_abstract":[
      "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19222",
    "c_title":[
      "Enhanced Efficiency in Shear-Loaded Brownian Gyrators"
    ],
    "c_abstract":[
      "A Brownian gyrator is a system in which a particle experiences thermal noise\nfrom two distinct heat baths. This nonequilibrium setup inherently generates a\nnonzero torque, leading to gyrating motion around a potential energy minimum.\nAs a minimal model for a heat engine, the Brownian gyrator provides valuable\ninsights into energy conversion and nonequilibrium dynamics. Here, we\ninvestigate the effect of an externally imposed shear flow on a Brownian\ngyrator, treating it as a mechanical load. The shear flow introduces a tunable\nmechanism that allows the system to operate either as a heat engine, extracting\nwork from the temperature gradient, or as a refrigerator, transferring heat\nfrom the colder to the hotter bath. Focusing on the heat engine regime, we\nanalytically derive the steady-state probability distribution to compute the\naverage torque exerted by the gyrator and quantify the mechanical power\nextracted from the shear. Our results show a remarkable increase in efficiency\ncompared to the standard Brownian gyrator without shear, approaching Carnot\nefficiency at maximum power. Surprisingly, we also find that while the system\ncan operate efficiently as a heat engine, it may become unstable before\nreaching the stall condition, highlighting a fundamental trade-off between\nefficiency and stability in shear-driven microscopic engines."
    ],
    "c_categories":[
      "cond-mat.soft",
      "cond-mat.stat-mech",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16776",
    "c_title":[
      "Designing Minimalistic Variational Quantum Ansatz Inspired by\n  Algorithmic Cooling"
    ],
    "c_abstract":[
      "This study introduces a novel minimalistic variational quantum ansatz\ninspired by algorithmic cooling principles. The proposed Heat Exchange\nalgorithmic cooling ansatz (HE ansatz) facilitates efficient population\nredistribution without requiring bath resets, simplifying implementation on\nnoisy intermediate-scale quantum (NISQ) devices. The HE ansatz achieves\nsuperior approximation ratios with the complete network \\textsc{Maxcut}\noptimization problem compared to the conventional Hardware efficient and QAOA\nansatz. We also proposed a new variational algorithm that utilize HE ansatz to\ncompute the ground state of impure dissipative-system variational quantum\neigensolver (dVQE) which achieved a sub-$1\\%$ error in ground-state energy\ncalculations of the 1D Heisenberg chain with impurity and successfully\nsimulates the edge effect of impure spin chain, highlighting its potential for\napplications in quantum many-body physics. These results underscore the\ncompatibility of the ansatz with hardware-efficient implementations, offering a\nscalable approach for solving complex quantum problems in disordered and open\nquantum systems."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12369",
    "c_title":[
      "An a posteriori data-driven method for phase-averaged optical\n  measurements"
    ],
    "c_abstract":[
      "Phase-averaging is a fundamental approach for investigating periodic and\nnon-stationary phenomena. In fluid dynamics, these can be generated by rotating\nblades such as propellers\/turbines or by pulsed jets. Traditional\nphase-averaging approaches often rely on synchronized data acquisition systems,\nwhich might require high-speed cameras, light sources, and precise delay\ngenerators and encoders, making them expensive and sometimes unfeasible. This\nwork proposes an a posteriori data-driven approach that reconstructs phase\ninformation from randomly acquired uncorrelated photographic frames (snapshots)\nusing the ISOMAP algorithm. The technique enables accurate reordering of\nsnapshots in the phase space and subsequent computation of the phase-averaged\nflow field without the need for synchronization. The framework was validated\nthrough numerical simulations and experimental fluid dynamics datasets from an\noptical setup featuring single- and multi-propeller configurations. The results\ndemonstrate that the proposed method effectively captures the periodic flow\ncharacteristics while addressing the challenges related to synchronization and\nhardware limitations. Furthermore, the ability to apply this technique to\narchival datasets extends its applicability to a wide range of experimental\nfluid dynamics studies. This approach provides a scalable and cost-effective\nalternative to traditional methods for the analysis of periodic phenomena."
    ],
    "c_categories":[
      "physics.data-an",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09729",
    "c_title":[
      "Stochastic Geometry for Modeling and Analysis of Sensing and\n  Communications: A Survey"
    ],
    "c_abstract":[
      "One of the most promising technologies for next-generation wireless networks\nis integrated communication and sensing (ISAC). It is considered a key enabler\nfor applications that require both enhanced communication and accurate sensing\ncapabilities. Examples of such applications include smart environments,\naugmented and virtual reality, or the internet of things, where the\ncapabilities of intelligent sensing and broadband communications are vital.\nTherefore, ISAC has attracted the research interest of both academia and\nindustry, and many investigations have been carried out over the past decade.\nThe articles in the literature include system models, performance evaluation,\nand optimization studies of several ISAC alternative designs. Stochastic\ngeometry is the study and analysis of random spatial patterns, and as such,\nstochastic geometry tools have been considered for the performance evaluation\nof wireless networks with different types of nodes. In this paper, we aim to\nprovide a comprehensive survey of current research progress in performance\nevaluation of ISAC systems using stochastic geometry tools. The survey covers\nterrestrial, aerial, and vehicular networks, where the random spatial location\nof the corresponding network elements and propagation scatterers and\/or\nblockages is treated with various point processes. The paper starts with a\nshort tutorial on ISAC technology, stochastic geometry tools, and metrics used\nin performance evaluation of communication and sensing. Then, the technical\ncomponents of the system models utilized in the surveyed papers are discussed.\nSubsequently, we present the key results of the literature in all types of\nnetworks using three levels of integration: sensing-assisted communication,\ncommunication-assisted sensing, and joint sensing and communication. Finally,\nfuture research challenges and promising directions are discussed."
    ],
    "c_categories":[
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11860",
    "c_title":[
      "Bayesian Despeckling of Structured Sources"
    ],
    "c_abstract":[
      "Speckle noise is a fundamental challenge in coherent imaging systems,\nsignificantly degrading image quality. Over the past decades, numerous\ndespeckling algorithms have been developed for applications such as Synthetic\nAperture Radar (SAR) and digital holography. In this paper, we aim to establish\na theoretically grounded approach to despeckling. We propose a method\napplicable to general structured stationary stochastic sources. We demonstrate\nthe effectiveness of the proposed method on piecewise constant sources.\nAdditionally, we theoretically derive a lower bound on the despeckling\nperformance for such sources. The proposed depseckler applied to the 1-Markov\nstructured sources achieves better reconstruction performance with no strong\nsimplification of the ground truth signal model or speckle noise."
    ],
    "c_categories":[
      "cs.IT",
      "cs.LG",
      "math.IT",
      "stat.AP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum\n  annotations",
    "a_abstract":"In drug discovery, accurate lung tumor segmentation is an important step for\nassessing tumor size and its progression using \\textit{in-vivo} imaging such as\nMRI. While deep learning models have been developed to automate this process,\nthe focus has predominantly been on human subjects, neglecting the pivotal role\nof animal models in pre-clinical drug development. In this work, we focus on\noptimizing lung tumor segmentation in mice. First, we demonstrate that the\nnnU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Most\nimportantly, we achieve better results with nnU-Net 3D models than 2D models,\nindicating the importance of spatial context for segmentation tasks in MRI mice\nscans. This study demonstrates the importance of 3D input over 2D input images\nfor lung tumor segmentation in MRI scans. Finally, we outperform the prior\nstate-of-the-art approach that involves the combined segmentation of lungs and\ntumors within the lungs. Our work achieves comparable results using only lung\ntumor annotations requiring fewer annotations, saving time and annotation\nefforts. This work\n(https:\/\/anonymous.4open.science\/r\/lung-tumour-mice-mri-64BB) is an important\nstep in automating pre-clinical animal studies to quantify the efficacy of\nexperimental drugs, particularly in assessing tumor changes.",
    "explanation":"In this work, we focus on optimizing lung tumor segmen-\ntation in mice. First, we demonstrate that the nnU-Net model outper-\nforms the U-Net, U-Net3+, and DeepMeta models.",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b12"
    ],
    "c_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "c_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.21130",
    "c_title":[
      "Fast and Accurate Gigapixel Pathological Image Classification with\n  Hierarchical Distillation Multi-Instance Learning"
    ],
    "c_abstract":[
      "Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17488",
    "c_title":[
      "ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing"
    ],
    "c_abstract":[
      "Recent approaches using large-scale pretrained diffusion models for image\ndehazing improve perceptual quality but often suffer from hallucination issues,\nproducing unfaithful dehazed image to the original one. To mitigate this, we\npropose ProDehaze, a framework that employs internal image priors to direct\nexternal priors encoded in pretrained models. We introduce two types of\n\\textit{selective} internal priors that prompt the model to concentrate on\ncritical image areas: a Structure-Prompted Restorer in the latent space that\nemphasizes structure-rich regions, and a Haze-Aware Self-Correcting Refiner in\nthe decoding process to align distributions between clearer input regions and\nthe output. Extensive experiments on real-world datasets demonstrate that\nProDehaze achieves high-fidelity results in image dehazing, particularly in\nreducing color shifts. Our code is at https:\/\/github.com\/TianwenZhou\/ProDehaze."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06219",
    "c_title":[
      "Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for\n  Generalizable RGB-Depth Driving Scene Parsing"
    ],
    "c_abstract":[
      "Recent vision foundation models (VFMs), typically based on Vision Transformer\n(ViT), have significantly advanced numerous computer vision tasks. Despite\ntheir success in tasks focused solely on RGB images, the potential of VFMs in\nRGB-depth driving scene parsing remains largely under-explored. In this\narticle, we take one step toward this emerging research area by investigating a\nfeasible technique to fully exploit VFMs for generalizable RGB-depth driving\nscene parsing. Specifically, we explore the inherent characteristics of RGB and\ndepth data, thereby presenting a Heterogeneous Feature Integration Transformer\n(HFIT). This network enables the efficient extraction and integration of\ncomprehensive heterogeneous features without re-training ViTs. Relative depth\nprediction results from VFMs, used as inputs to the HFIT side adapter, overcome\nthe limitations of the dependence on depth maps. Our proposed HFIT demonstrates\nsuperior performance compared to all other traditional single-modal and\ndata-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the\nCityscapes and KITTI Semantics datasets. We believe this novel strategy paves\nthe way for future innovations in VFM-based data-fusion techniques for driving\nscene parsing. Our source code is publicly available at\nhttps:\/\/mias.group\/HFIT."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06652",
    "c_title":[
      "Adding Additional Control to One-Step Diffusion with Joint Distribution\n  Matching"
    ],
    "c_abstract":[
      "While diffusion distillation has enabled one-step generation through methods\nlike Variational Score Distillation, adapting distilled models to emerging new\ncontrols -- such as novel structural constraints or latest user preferences --\nremains challenging. Conventional approaches typically requires modifying the\nbase diffusion model and redistilling it -- a process that is both\ncomputationally intensive and time-consuming. To address these challenges, we\nintroduce Joint Distribution Matching (JDM), a novel approach that minimizes\nthe reverse KL divergence between image-condition joint distributions. By\nderiving a tractable upper bound, JDM decouples fidelity learning from\ncondition learning. This asymmetric distillation scheme enables our one-step\nstudent to handle controls unknown to the teacher model and facilitates\nimproved classifier-free guidance (CFG) usage and seamless integration of human\nfeedback learning (HFL). Experimental results demonstrate that JDM surpasses\nbaseline methods such as multi-step ControlNet by mere one-step in most cases,\nwhile achieving state-of-the-art performance in one-step text-to-image\nsynthesis through improved usage of CFG or HFL integration."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10695",
    "c_title":[
      "Exploring Transferable Homogeneous Groups for Compositional Zero-Shot\n  Learning"
    ],
    "c_abstract":[
      "Conditional dependency present one of the trickiest problems in Compositional\nZero-Shot Learning, leading to significant property variations of the same\nstate (object) across different objects (states). To address this problem,\nexisting approaches often adopt either all-to-one or one-to-one representation\nparadigms. However, these extremes create an imbalance in the seesaw between\ntransferability and discriminability, favoring one at the expense of the other.\nComparatively, humans are adept at analogizing and reasoning in a hierarchical\nclustering manner, intuitively grouping categories with similar properties to\nform cohesive concepts. Motivated by this, we propose Homogeneous Group\nRepresentation Learning (HGRL), a new perspective formulates state (object)\nrepresentation learning as multiple homogeneous sub-group representation\nlearning. HGRL seeks to achieve a balance between semantic transferability and\ndiscriminability by adaptively discovering and aggregating categories with\nshared properties, learning distributed group centers that retain\ngroup-specific discriminative features. Our method integrates three core\ncomponents designed to simultaneously enhance both the visual and prompt\nrepresentation capabilities of the model. Extensive experiments on three\nbenchmark datasets validate the effectiveness of our method."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19222",
    "c_title":[
      "Enhanced Efficiency in Shear-Loaded Brownian Gyrators"
    ],
    "c_abstract":[
      "A Brownian gyrator is a system in which a particle experiences thermal noise\nfrom two distinct heat baths. This nonequilibrium setup inherently generates a\nnonzero torque, leading to gyrating motion around a potential energy minimum.\nAs a minimal model for a heat engine, the Brownian gyrator provides valuable\ninsights into energy conversion and nonequilibrium dynamics. Here, we\ninvestigate the effect of an externally imposed shear flow on a Brownian\ngyrator, treating it as a mechanical load. The shear flow introduces a tunable\nmechanism that allows the system to operate either as a heat engine, extracting\nwork from the temperature gradient, or as a refrigerator, transferring heat\nfrom the colder to the hotter bath. Focusing on the heat engine regime, we\nanalytically derive the steady-state probability distribution to compute the\naverage torque exerted by the gyrator and quantify the mechanical power\nextracted from the shear. Our results show a remarkable increase in efficiency\ncompared to the standard Brownian gyrator without shear, approaching Carnot\nefficiency at maximum power. Surprisingly, we also find that while the system\ncan operate efficiently as a heat engine, it may become unstable before\nreaching the stall condition, highlighting a fundamental trade-off between\nefficiency and stability in shear-driven microscopic engines."
    ],
    "c_categories":[
      "cond-mat.soft",
      "cond-mat.stat-mech",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16776",
    "c_title":[
      "Designing Minimalistic Variational Quantum Ansatz Inspired by\n  Algorithmic Cooling"
    ],
    "c_abstract":[
      "This study introduces a novel minimalistic variational quantum ansatz\ninspired by algorithmic cooling principles. The proposed Heat Exchange\nalgorithmic cooling ansatz (HE ansatz) facilitates efficient population\nredistribution without requiring bath resets, simplifying implementation on\nnoisy intermediate-scale quantum (NISQ) devices. The HE ansatz achieves\nsuperior approximation ratios with the complete network \\textsc{Maxcut}\noptimization problem compared to the conventional Hardware efficient and QAOA\nansatz. We also proposed a new variational algorithm that utilize HE ansatz to\ncompute the ground state of impure dissipative-system variational quantum\neigensolver (dVQE) which achieved a sub-$1\\%$ error in ground-state energy\ncalculations of the 1D Heisenberg chain with impurity and successfully\nsimulates the edge effect of impure spin chain, highlighting its potential for\napplications in quantum many-body physics. These results underscore the\ncompatibility of the ansatz with hardware-efficient implementations, offering a\nscalable approach for solving complex quantum problems in disordered and open\nquantum systems."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12369",
    "c_title":[
      "An a posteriori data-driven method for phase-averaged optical\n  measurements"
    ],
    "c_abstract":[
      "Phase-averaging is a fundamental approach for investigating periodic and\nnon-stationary phenomena. In fluid dynamics, these can be generated by rotating\nblades such as propellers\/turbines or by pulsed jets. Traditional\nphase-averaging approaches often rely on synchronized data acquisition systems,\nwhich might require high-speed cameras, light sources, and precise delay\ngenerators and encoders, making them expensive and sometimes unfeasible. This\nwork proposes an a posteriori data-driven approach that reconstructs phase\ninformation from randomly acquired uncorrelated photographic frames (snapshots)\nusing the ISOMAP algorithm. The technique enables accurate reordering of\nsnapshots in the phase space and subsequent computation of the phase-averaged\nflow field without the need for synchronization. The framework was validated\nthrough numerical simulations and experimental fluid dynamics datasets from an\noptical setup featuring single- and multi-propeller configurations. The results\ndemonstrate that the proposed method effectively captures the periodic flow\ncharacteristics while addressing the challenges related to synchronization and\nhardware limitations. Furthermore, the ability to apply this technique to\narchival datasets extends its applicability to a wide range of experimental\nfluid dynamics studies. This approach provides a scalable and cost-effective\nalternative to traditional methods for the analysis of periodic phenomena."
    ],
    "c_categories":[
      "physics.data-an",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09729",
    "c_title":[
      "Stochastic Geometry for Modeling and Analysis of Sensing and\n  Communications: A Survey"
    ],
    "c_abstract":[
      "One of the most promising technologies for next-generation wireless networks\nis integrated communication and sensing (ISAC). It is considered a key enabler\nfor applications that require both enhanced communication and accurate sensing\ncapabilities. Examples of such applications include smart environments,\naugmented and virtual reality, or the internet of things, where the\ncapabilities of intelligent sensing and broadband communications are vital.\nTherefore, ISAC has attracted the research interest of both academia and\nindustry, and many investigations have been carried out over the past decade.\nThe articles in the literature include system models, performance evaluation,\nand optimization studies of several ISAC alternative designs. Stochastic\ngeometry is the study and analysis of random spatial patterns, and as such,\nstochastic geometry tools have been considered for the performance evaluation\nof wireless networks with different types of nodes. In this paper, we aim to\nprovide a comprehensive survey of current research progress in performance\nevaluation of ISAC systems using stochastic geometry tools. The survey covers\nterrestrial, aerial, and vehicular networks, where the random spatial location\nof the corresponding network elements and propagation scatterers and\/or\nblockages is treated with various point processes. The paper starts with a\nshort tutorial on ISAC technology, stochastic geometry tools, and metrics used\nin performance evaluation of communication and sensing. Then, the technical\ncomponents of the system models utilized in the surveyed papers are discussed.\nSubsequently, we present the key results of the literature in all types of\nnetworks using three levels of integration: sensing-assisted communication,\ncommunication-assisted sensing, and joint sensing and communication. Finally,\nfuture research challenges and promising directions are discussed."
    ],
    "c_categories":[
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11860",
    "c_title":[
      "Bayesian Despeckling of Structured Sources"
    ],
    "c_abstract":[
      "Speckle noise is a fundamental challenge in coherent imaging systems,\nsignificantly degrading image quality. Over the past decades, numerous\ndespeckling algorithms have been developed for applications such as Synthetic\nAperture Radar (SAR) and digital holography. In this paper, we aim to establish\na theoretically grounded approach to despeckling. We propose a method\napplicable to general structured stationary stochastic sources. We demonstrate\nthe effectiveness of the proposed method on piecewise constant sources.\nAdditionally, we theoretically derive a lower bound on the despeckling\nperformance for such sources. The proposed depseckler applied to the 1-Markov\nstructured sources achieves better reconstruction performance with no strong\nsimplification of the ground truth signal model or speckle noise."
    ],
    "c_categories":[
      "cs.IT",
      "cs.LG",
      "math.IT",
      "stat.AP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01319",
    "c_title":[
      "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability"
    ],
    "c_abstract":[
      "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":[
      "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats"
    ],
    "c_abstract":[
      "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08533",
    "c_title":[
      "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant"
    ],
    "c_abstract":[
      "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04504",
    "c_title":[
      "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates"
    ],
    "c_abstract":[
      "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12234",
    "c_title":[
      "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer"
    ],
    "c_abstract":[
      "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16905",
    "c_title":[
      "Enhanced Dissipation via Time-Dependent Velocity Fields: Acceleration\n  and Intermittency"
    ],
    "c_abstract":[
      "Motivated by mixing processes in analytical laboratories, this work\ninvestigates enhanced dissipation in non-autonomous flows. Specifically, we\nstudy the evolution of concentrations governed by the advection-diffusion\nequation, where the velocity field is modelled as the product of a shear flow\n(with simple critical points) and a time-dependent function $\\xi(t)$. The main\nobjective of this paper is to derive explicit, quantitative estimates for the\nenergy decay rates, which are shown to depend sensitively on the properties of\n$\\xi$. We identify a class of time-dependent functions $\\xi$ that are bounded\nabove and below by increasing functions of time. For this class, we demonstrate\nsuper-enhanced dissipation, characterized by energy decay rates faster than\nthose observed in autonomous cases (e.g., when $\\xi \\equiv 1$, as in Coti\nZelati and Gallay (2023)). Additionally, we explore enhanced dissipation in the\ncontext of intermittent flows, where the velocity fields may be switched on and\noff over time. In these cases, the dissipation rates are comparable to those of\nautonomous flows but with modified constants. To illustrate our results, we\nanalyse two prototypical intermittent flows: one that exhibits a gradual\nturn-on and turn-off phase, and another that undergoes a significant\nacceleration following a slow initial activation phase. For the second, we\nderive the corresponding dissipation rate using a gluing argument. Both results\nare achieved through the application of the hypocoercivity framework (\\`a la\nBedrossian and Coti Zelati (2017)), adapted to an augmented functional with\ntime-dependent weights. These weights are designed to dynamically counteract\nthe potential growth of $\\xi$, ensuring robust estimates for the energy decay."
    ],
    "c_categories":[
      "math-ph",
      "math.AP",
      "math.MP",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08318",
    "c_title":[
      "Dimension of dismantlable lattices"
    ],
    "c_abstract":[
      "In $1941$ Dushnik and Miller introduced the concept of dimension of a poset.\nIn $2020$ Bhavale and Waphare introduced the concept of an RC-lattice as a\nlattice in which all the reducible elements are lying on a chain. In this\npaper, we obtain the dimension of adjunct sum of two lattices. We obtain a\nbound on the dimension of a dismantlable lattice in terms of its nullity. We\nalso prove that the dimension of an RC-lattice on $n$ elements is at the most\nthree. Consequently, we prove that an RC-lattice is non planar if and only if\nits dimension is three."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06068",
    "c_title":[
      "Bi-Hermitian and locally conformally K\\\"ahler surfaces"
    ],
    "c_abstract":[
      "We report on a few interrelations between bi-Hermitian metrics and locally\nconformally K\\\"ahler metrics on complex surfaces."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18519",
    "c_title":[
      "Newton-Okounkov polygons with a small number of vertices and Picard\n  number"
    ],
    "c_abstract":[
      "Newton-Okounkov bodies serve as a bridge between algebraic geometry and\nconvex geometry, enabling the application of combinatorial and geometric\nmethods to the study of linear systems on algebraic varieties. This paper\ncontributes to understanding the algebro-geometric information encoded in the\ncollection of all Newton-Okounkov bodies on a given surface, focusing on\nNewton-Okounkov polygons with few vertices and on elliptic K3 surfaces.\n  Let S be an algebraic surface and mv(S) be the maximum number of vertices of\nthe Newton-Okounkov bodies of S. We prove that mv(S) = 4 if and only if its\nPicard number \\rho(S) is at least 2 and S contains no negative irreducible\ncurve. Additionally, if S contains a negative curve, then \\rho(S) = 2 if and\nonly if mv(S) = 5.\n  Furthermore, we provide an example involving two elliptic K3 surfaces to\ndemonstrate that when \\rho(S) \\geq 3, mv(S) no longer determines the Picard\nnumber \\rho(S)."
    ],
    "c_categories":[
      "math.AG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05174",
    "c_title":[
      "General expression for three-loop $\\beta$-functions of $\\cal N =$ 1\n  supersymmetric theories with multiple gauge couplings regularized by higher\n  covariant derivatives"
    ],
    "c_abstract":[
      "For $\\cal N =$ 1 supersymmetric theories with multiple gauge couplings\nregularized by higher covariant derivatives, a general expression for\nthree-loop gauge $\\beta$-functions is obtained. For this purpose, using general\nstatements about the validity of the NSVZ relations, a result is constructed\nfor the $\\beta$-functions defined in terms of the bare couplings. It is\ndemonstrated that in the particular case of MSSM, it precisely reproduces the\nknown expressions found earlier in another way. For the case where Yukawa\ncouplings are absent, similar expressions for three-loop $\\beta$-functions are\nalso obtained in terms of the renormalized couplings in an arbitrary\nsubtraction scheme and, in particular, in the $\\overline{\\mbox{DR}}$ scheme."
    ],
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"Exploring the Potentials and Challenges of Using Large Language Models\n  for the Analysis of Transcriptional Regulation of Long Non-coding RNAs",
    "a_abstract":"Research on long non-coding RNAs (lncRNAs) has garnered significant attention\ndue to their critical roles in gene regulation and disease mechanisms. However,\nthe complexity and diversity of lncRNA sequences, along with the limited\nknowledge of their functional mechanisms and the regulation of their\nexpressions, pose significant challenges to lncRNA studies. Given the\ntremendous success of large language models (LLMs) in capturing complex\ndependencies in sequential data, this study aims to systematically explore the\npotential and limitations of LLMs in the sequence analysis related to the\ntranscriptional regulation of lncRNA genes. Our extensive experiments\ndemonstrated promising performance of fine-tuned genome foundation models on\nprogressively complex tasks. Furthermore, we conducted an insightful analysis\nof the critical impact of task complexity, model selection, data quality, and\nbiological interpretability for the studies of the regulation of lncRNA gene\nexpression.",
    "explanation":"Given the tremendous success of large language mod-\nels (LLMs) in capturing complex dependencies in sequential data, this study aims to systematically explore the potential and limitations of LLMs in the sequence analysis related to the transcriptional regulation of lncRNA genes. ",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b20"
    ],
    "c_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "c_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16905",
    "c_title":[
      "Enhanced Dissipation via Time-Dependent Velocity Fields: Acceleration\n  and Intermittency"
    ],
    "c_abstract":[
      "Motivated by mixing processes in analytical laboratories, this work\ninvestigates enhanced dissipation in non-autonomous flows. Specifically, we\nstudy the evolution of concentrations governed by the advection-diffusion\nequation, where the velocity field is modelled as the product of a shear flow\n(with simple critical points) and a time-dependent function $\\xi(t)$. The main\nobjective of this paper is to derive explicit, quantitative estimates for the\nenergy decay rates, which are shown to depend sensitively on the properties of\n$\\xi$. We identify a class of time-dependent functions $\\xi$ that are bounded\nabove and below by increasing functions of time. For this class, we demonstrate\nsuper-enhanced dissipation, characterized by energy decay rates faster than\nthose observed in autonomous cases (e.g., when $\\xi \\equiv 1$, as in Coti\nZelati and Gallay (2023)). Additionally, we explore enhanced dissipation in the\ncontext of intermittent flows, where the velocity fields may be switched on and\noff over time. In these cases, the dissipation rates are comparable to those of\nautonomous flows but with modified constants. To illustrate our results, we\nanalyse two prototypical intermittent flows: one that exhibits a gradual\nturn-on and turn-off phase, and another that undergoes a significant\nacceleration following a slow initial activation phase. For the second, we\nderive the corresponding dissipation rate using a gluing argument. Both results\nare achieved through the application of the hypocoercivity framework (\\`a la\nBedrossian and Coti Zelati (2017)), adapted to an augmented functional with\ntime-dependent weights. These weights are designed to dynamically counteract\nthe potential growth of $\\xi$, ensuring robust estimates for the energy decay."
    ],
    "c_categories":[
      "math-ph",
      "math.AP",
      "math.MP",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08318",
    "c_title":[
      "Dimension of dismantlable lattices"
    ],
    "c_abstract":[
      "In $1941$ Dushnik and Miller introduced the concept of dimension of a poset.\nIn $2020$ Bhavale and Waphare introduced the concept of an RC-lattice as a\nlattice in which all the reducible elements are lying on a chain. In this\npaper, we obtain the dimension of adjunct sum of two lattices. We obtain a\nbound on the dimension of a dismantlable lattice in terms of its nullity. We\nalso prove that the dimension of an RC-lattice on $n$ elements is at the most\nthree. Consequently, we prove that an RC-lattice is non planar if and only if\nits dimension is three."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06068",
    "c_title":[
      "Bi-Hermitian and locally conformally K\\\"ahler surfaces"
    ],
    "c_abstract":[
      "We report on a few interrelations between bi-Hermitian metrics and locally\nconformally K\\\"ahler metrics on complex surfaces."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18519",
    "c_title":[
      "Newton-Okounkov polygons with a small number of vertices and Picard\n  number"
    ],
    "c_abstract":[
      "Newton-Okounkov bodies serve as a bridge between algebraic geometry and\nconvex geometry, enabling the application of combinatorial and geometric\nmethods to the study of linear systems on algebraic varieties. This paper\ncontributes to understanding the algebro-geometric information encoded in the\ncollection of all Newton-Okounkov bodies on a given surface, focusing on\nNewton-Okounkov polygons with few vertices and on elliptic K3 surfaces.\n  Let S be an algebraic surface and mv(S) be the maximum number of vertices of\nthe Newton-Okounkov bodies of S. We prove that mv(S) = 4 if and only if its\nPicard number \\rho(S) is at least 2 and S contains no negative irreducible\ncurve. Additionally, if S contains a negative curve, then \\rho(S) = 2 if and\nonly if mv(S) = 5.\n  Furthermore, we provide an example involving two elliptic K3 surfaces to\ndemonstrate that when \\rho(S) \\geq 3, mv(S) no longer determines the Picard\nnumber \\rho(S)."
    ],
    "c_categories":[
      "math.AG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05174",
    "c_title":[
      "General expression for three-loop $\\beta$-functions of $\\cal N =$ 1\n  supersymmetric theories with multiple gauge couplings regularized by higher\n  covariant derivatives"
    ],
    "c_abstract":[
      "For $\\cal N =$ 1 supersymmetric theories with multiple gauge couplings\nregularized by higher covariant derivatives, a general expression for\nthree-loop gauge $\\beta$-functions is obtained. For this purpose, using general\nstatements about the validity of the NSVZ relations, a result is constructed\nfor the $\\beta$-functions defined in terms of the bare couplings. It is\ndemonstrated that in the particular case of MSSM, it precisely reproduces the\nknown expressions found earlier in another way. For the case where Yukawa\ncouplings are absent, similar expressions for three-loop $\\beta$-functions are\nalso obtained in terms of the renormalized couplings in an arbitrary\nsubtraction scheme and, in particular, in the $\\overline{\\mbox{DR}}$ scheme."
    ],
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11041",
    "c_title":[
      "Enhancing Semantic Consistency of Large Language Models through Model\n  Editing: An Interpretability-Oriented Approach"
    ],
    "c_abstract":[
      "A Large Language Model (LLM) tends to generate inconsistent and sometimes\ncontradictory outputs when presented with a prompt that has equivalent\nsemantics but is expressed differently from the original prompt. To achieve\nsemantic consistency of an LLM, one of the key approaches is to finetune the\nmodel with prompt-output pairs with semantically equivalent meanings. Despite\nits effectiveness, a data-driven finetuning method incurs substantial\ncomputation costs in data preparation and model optimization. In this regime,\nan LLM is treated as a ``black box'', restricting our ability to gain deeper\ninsights into its internal mechanism. In this paper, we are motivated to\nenhance the semantic consistency of LLMs through a more interpretable method\n(i.e., model editing) to this end. We first identify the model components\n(i.e., attention heads) that have a key impact on the semantic consistency of\nan LLM. We subsequently inject biases into the output of these model components\nalong the semantic-consistency activation direction. It is noteworthy that\nthese modifications are cost-effective, without reliance on mass manipulations\nof the original model parameters. Through comprehensive experiments on the\nconstructed NLU and open-source NLG datasets, our method demonstrates\nsignificant improvements in the semantic consistency and task performance of\nLLMs. Additionally, our method exhibits promising generalization capabilities\nby performing well on tasks beyond the primary tasks."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04173",
    "c_title":[
      "Lexical Substitution is not Synonym Substitution: On the Importance of\n  Producing Contextually Relevant Word Substitutes"
    ],
    "c_abstract":[
      "Lexical Substitution is the task of replacing a single word in a sentence\nwith a similar one. This should ideally be one that is not necessarily only\nsynonymous, but also fits well into the surrounding context of the target word,\nwhile preserving the sentence's grammatical structure. Recent advances in\nLexical Substitution have leveraged the masked token prediction task of\nPre-trained Language Models to generate replacements for a given word in a\nsentence. With this technique, we introduce ConCat, a simple augmented approach\nwhich utilizes the original sentence to bolster contextual information sent to\nthe model. Compared to existing approaches, it proves to be very effective in\nguiding the model to make contextually relevant predictions for the target\nword. Our study includes a quantitative evaluation, measured via sentence\nsimilarity and task performance. In addition, we conduct a qualitative human\nanalysis to validate that users prefer the substitutions proposed by our\nmethod, as opposed to previous methods. Finally, we test our approach on the\nprevailing benchmark for Lexical Substitution, CoInCo, revealing potential\npitfalls of the benchmark. These insights serve as the foundation for a\ncritical discussion on the way in which Lexical Substitution is evaluated."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.10427",
    "c_title":[
      "VisTW: Benchmarking Vision-Language Models for Traditional Chinese in\n  Taiwan"
    ],
    "c_abstract":[
      "In this paper, we propose a comprehensive evaluation benchmark for Visual\nLanguage Models (VLM) in Traditional Chinese. Our evaluation suite, the first\nof its kind, contains two complementary components: (1) VisTW-MCQ, a collection\nof manually curated exam multi-choice questions from 21 academic subjects\ndesigned to test the broad knowledge and reasoning capabilities of VLMs; and\n(2) VisTW-Dialogue, an open dialogue benchmark comprising 131 image-question\npairs manually created to evaluate VLMs' ability in free-form dialogue\ngeneration within Taiwanese cultural contexts. These benchmarks address a\ncritical gap in the evaluation landscape, where existing benchmarks\npredominantly focus on English or Simplified Chinese, neglecting the unique\nlinguistic and cultural aspects of Traditional Chinese used in regions like\nTaiwan and Hong Kong. Our analysis reveals significant performance differences\nacross various VLMs and highlights specific challenges in processing\nTraditional Chinese visual content."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03863",
    "c_title":[
      "Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A\n  Multi-Dialectal Bavarian Case Study"
    ],
    "c_abstract":[
      "Reliable slot and intent detection (SID) is crucial in natural language\nunderstanding for applications like digital assistants. Encoder-only\ntransformer models fine-tuned on high-resource languages generally perform well\non SID. However, they struggle with dialectal data, where no standardized form\nexists and training data is scarce and costly to produce. We explore zero-shot\ntransfer learning for SID, focusing on multiple Bavarian dialects, for which we\nrelease a new dataset for the Munich dialect. We evaluate models trained on\nauxiliary tasks in Bavarian, and compare joint multi-task learning with\nintermediate-task training. We also compare three types of auxiliary tasks:\ntoken-level syntactic tasks, named entity recognition (NER), and language\nmodelling. We find that the included auxiliary tasks have a more positive\neffect on slot filling than intent classification (with NER having the most\npositive effect), and that intermediate-task training yields more consistent\nperformance gains. Our best-performing approach improves intent classification\nperformance on Bavarian dialects by 5.1 and slot filling F1 by 8.4 percentage\npoints."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04718",
    "c_title":[
      "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?"
    ],
    "c_abstract":[
      "Text Style Transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, same as in other natural language processing (NLP) tasks,\nhowever, automatic metrics for TST have not received as much attention as\nmetrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks-sentiment transfer and\ndetoxification-in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nLarge Language Models (LLMs) as tools for TST evaluation. Our findings\nhighlight that certain advanced NLP metrics and experimental-hybrid-techniques,\nprovide better insights than existing TST metrics for delivering more accurate,\nconsistent, and reproducible TST evaluations."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05286",
    "c_title":[
      "Molecular Properties from Quantum Krylov Subspace Diagonalization"
    ],
    "c_abstract":[
      "Quantum Krylov subspace diagonalization is a prominent candidate for early\nfault tolerant quantum simulation of many-body and molecular systems, but so\nfar the focus has been mainly on computing ground-state energies. We go beyond\nthis by deriving analytical first-order derivatives for quantum Krylov methods\nand show how to obtain relaxed one and two particle reduced density matrices of\nthe Krylov eigenstates. The direct approach to measuring these matrices\nrequires a number of distinct measurement that scales quadratically with the\nKrylov dimension $D$. Here, we show how to reduce this scaling to a constant.\nThis is done by leveraging quantum signal processing to prepare Krylov\neigenstates, including exited states, in depth linear in $D$. We also compare\nseveral measurement schemes for efficiently obtaining the expectation value of\nan operator with states prepared using quantum signal processing. We validate\nour approach by computing the nuclear gradient of a small molecule and\nestimating its variance."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06564",
    "c_title":[
      "Robust Scatter Matrix Estimation for Elliptical Distributions in\n  Polynomial Time"
    ],
    "c_abstract":[
      "We study the problem of computationally efficient robust estimation of\nscatter matrices of elliptical distributions under the strong contamination\nmodel. We design polynomial time algorithms that achieve dimension-independent\nerror in Frobenius norm.\n  Our first result is a sequence of efficient algorithms that approaches nearly\noptimal error. Specifically, under a mild assumption on the eigenvalues of the\nscatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator\nthat, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$\nsuch that $ \\Vert{\\Sigma^{-1\/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\,\n\\Sigma^{-1\/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$,\nwhere $\\varepsilon$ is the fraction of corruption. We do not require any\nassumptions on the moments of the distribution, while all previously known\ncomputationally efficient algorithms for robust covariance\/scatter estimation\nwith dimension-independent error rely on strong assumptions on the moments,\nsuch as sub-Gaussianity or (certifiable) hypercontractivity.\n  Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$\n(that, in particular, is satisfied by all matrices with constant condition\nnumber),\n  we provide a fast (sub-quadratic in the input size) algorithm that, given\nnearly optimal number of samples $n = \\tilde{O}(d^2\/\\varepsilon)$, in time\n$\\tilde{O}({nd^2 poly(1\/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that\n$\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert \\cdot\n\\sqrt{\\varepsilon})$.\n  Our approach is based on robust covariance estimation of the spatial sign\n(the projection onto the sphere of radius $\\sqrt{d}$) of elliptical\ndistributions."
    ],
    "c_categories":[
      "cs.DS",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.12933",
    "c_title":[
      "Electric Polarizability of Charged Kaons from Lattice QCD Four-Point\n  Functions"
    ],
    "c_abstract":[
      "We study the electric polarizability of a charged kaon from four-point\nfunctions in lattice QCD as an alternative to the background field method.\nLattice four-point correlation functions are constructed from quark and gluon\nfields to be used in Monte Carlo simulations. The elastic form factor (charge\nradius) is needed in the method which can be obtained from the same four-point\nfunctions at large current separations. Preliminary results from the connected\nquark-line diagrams are presented."
    ],
    "c_categories":[
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15397",
    "c_title":[
      "Astrophysical properties of static black holes embedded in a Dehnen type\n  dark matter halo with the presence of quintessential field"
    ],
    "c_abstract":[
      "From an astrophysical perspective, the composition of black holes (BHs), dark\nmatter (DM), and dark energy can be an intriguing physical system. In this\nstudy, we consider Schwarzschild BHs embedded in a Dehnen-type DM halo\nexhibiting a quintessential field. This study examines the horizons, shadows,\ndeflection angle, and quasinormal modes (QNMs) of the effective BH spacetime\nand how they are affected by the dark sector. The Schwarzschild BH embodied in\na Dehnen-type DM halo exhibiting a quintessential field possesses two horizons:\nthe event horizon and the cosmological horizon. We demonstrate that all dark\nsector parameters increase the event horizon while decreasing the cosmological\nhorizon. We analyze the BH shadow and emphasize the impact of DM and\nquintessence parameters on them. We show that the dark sector casts larger\nshadows than a Schwarzschild BH in a vacuum. Further, we delve into the weak\ngravitational lensing deflection angle using the Gauss-Bonnet theorem (GBT). We\nthen investigate QNMs using the 6th order WKB approach. To visually underscore\nthe dark sector parameters, we present figures that illustrate the impact of\nvarying the parameters of the Dehnen-type DM halo as well as quintessence. Our\nfindings show that the gravitational waves emitted from BHs with a dark sector\nhave a lower frequency and decay rate compared to those emitted from BHs in a\nvacuum."
    ],
    "c_categories":[
      "gr-qc"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06673",
    "c_title":[
      "Selecting Optimal Sampling Rate for Stable Super-Resolution"
    ],
    "c_abstract":[
      "We investigate the recovery of nodes and amplitudes from noisy frequency\nsamples in spike train signals, also known as the super-resolution (SR)\nproblem. When the node separation falls below the Rayleigh limit, the problem\nbecomes ill-conditioned. Admissible sampling rates, or decimation parameters,\nimprove the conditioning of the SR problem, enabling more accurate recovery. We\npropose an efficient preprocessing method to identify the optimal sampling\nrate, significantly enhancing the performance of SR techniques."
    ],
    "c_categories":[
      "cs.IT",
      "cs.NA",
      "math.IT",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"Why do we regularise in every iteration for imaging inverse problems?",
    "a_abstract":"Regularisation is commonly used in iterative methods for solving imaging\ninverse problems. Many algorithms involve the evaluation of the proximal\noperator of the regularisation term in every iteration, leading to a\nsignificant computational overhead since such evaluation can be costly. In this\ncontext, the ProxSkip algorithm, recently proposed for federated learning\npurposes, emerges as an solution. It randomly skips regularisation steps,\nreducing the computational time of an iterative algorithm without affecting its\nconvergence. Here we explore for the first time the efficacy of ProxSkip to a\nvariety of imaging inverse problems and we also propose a novel PDHGSkip\nversion. Extensive numerical results highlight the potential of these methods\nto accelerate computations while maintaining high-quality reconstructions.",
    "explanation":"Regularisation is commonly used in iterative methods for solving imaging inverse problems. Here we explore for the first time the efficacy of ProxSkip to a variety of imaging inverse problems and we also propose a novel PDHGSkip version.",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b3"
    ],
    "c_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "c_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.09799",
    "c_title":[
      "Scan-Adaptive MRI Undersampling Using Neighbor-based Optimization (SUNO)"
    ],
    "c_abstract":[
      "Accelerated MRI involves collecting partial k-space measurements to reduce\nacquisition time, patient discomfort, and motion artifacts, and typically uses\nregular undersampling patterns or hand-designed schemes. Recent works have\nstudied population-adaptive sampling patterns that are learned from a group of\npatients (or scans) based on population-specific metrics. However, such a\ngeneral sampling pattern can be sub-optimal for any specific scan since it may\nlack scan or slice adaptive details. To overcome this issue, we propose a\nframework for jointly learning scan-adaptive Cartesian undersampling patterns\nand a corresponding reconstruction model from a training set. We use an\nalternating algorithm for learning the sampling patterns and reconstruction\nmodel where we use an iterative coordinate descent (ICD) based offline\noptimization of scan-adaptive k-space sampling patterns for each example in the\ntraining set. A nearest neighbor search is then used to select the\nscan-adaptive sampling pattern at test time from initially acquired\nlow-frequency k-space information. We applied the proposed framework (dubbed\nSUNO) to the fastMRI multi-coil knee and brain datasets, demonstrating improved\nperformance over currently used undersampling patterns at both 4x and 8x\nacceleration factors in terms of both visual quality and quantitative metrics.\nThe code for the proposed framework is available at\nhttps:\/\/github.com\/sidgautam95\/adaptive-sampling-mri-suno."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.13988",
    "c_title":[
      "A Lightweight Model for Perceptual Image Compression via Implicit Priors"
    ],
    "c_abstract":[
      "Perceptual image compression has shown strong potential for producing\nvisually appealing results at low bitrates, surpassing classical standards and\npixel-wise distortion-oriented neural methods. However, existing methods\ntypically improve compression performance by incorporating explicit semantic\npriors, such as segmentation maps and textual features, into the encoder or\ndecoder, which increases model complexity by adding parameters and\nfloating-point operations. This limits the model's practicality, as image\ncompression often occurs on resource-limited mobile devices. To alleviate this\nproblem, we propose a lightweight perceptual Image Compression method using\nImplicit Semantic Priors (ICISP). We first develop an enhanced visual state\nspace block that exploits local and global spatial dependencies to reduce\nredundancy. Since different frequency information contributes unequally to\ncompression, we develop a frequency decomposition modulation block to\nadaptively preserve or reduce the low-frequency and high-frequency information.\nWe establish the above blocks as the main modules of the encoder-decoder, and\nto further improve the perceptual quality of the reconstructed images, we\ndevelop a semantic-informed discriminator that uses implicit semantic priors\nfrom a pretrained DINOv2 encoder. Experiments on popular benchmarks show that\nour method achieves competitive compression performance and has significantly\nfewer network parameters and floating point operations than the existing\nstate-of-the-art."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03780",
    "c_title":[
      "Convergent Primal-Dual Plug-and-Play Image Restoration: A General\n  Algorithm and Applications"
    ],
    "c_abstract":[
      "We propose a general deep plug-and-play (PnP) algorithm with a theoretical\nconvergence guarantee. PnP strategies have demonstrated outstanding performance\nin various image restoration tasks by exploiting the powerful priors underlying\nGaussian denoisers. However, existing PnP methods often lack theoretical\nconvergence guarantees under realistic assumptions due to their ad-hoc nature,\nresulting in inconsistent behavior. Moreover, even when convergence guarantees\nare provided, they are typically designed for specific settings or require a\nconsiderable computational cost in handling non-quadratic data-fidelity terms\nand additional constraints, which are key components in many image restoration\nscenarios. To tackle these challenges, we integrate the PnP paradigm with\nprimal-dual splitting (PDS), an efficient proximal splitting methodology for\nsolving a wide range of convex optimization problems, and develop a general\nconvergent PnP framework. Specifically, we establish theoretical conditions for\nthe convergence of the proposed PnP algorithm under a reasonable assumption.\nFurthermore, we show that the problem solved by the proposed PnP algorithm is\nnot a standard convex optimization problem but a more general monotone\ninclusion problem, where we provide a mathematical representation of the\nsolution set. Our approach efficiently handles a broad class of image\nrestoration problems with guaranteed theoretical convergence. Numerical\nexperiments on specific image restoration tasks validate the practicality and\neffectiveness of our theoretical results."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.15246",
    "c_title":[
      "End-to-end localized deep learning for Cryo-ET"
    ],
    "c_abstract":[
      "Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular\nenvironments. Accurate reconstruction of high-resolution volumes is complicated\nby the very low signal-to-noise ratio and a restricted range of sample tilts,\ncreating a missing wedge of Fourier information. Recent self-supervised deep\nlearning approaches, which post-process initial reconstructions done by\nfiltered backprojection (FBP), have significantly improved reconstruction\nquality, but they are computationally expensive, demand large memory, and\nrequire retraining for each new dataset. End-to-end supervised learning is an\nappealing alternative but is impeded by the lack of ground truth and the large\nmemory demands of high-resolution volumetric data. Training on synthetic data\noften leads to overfitting and poor generalization to real data, and, to date,\nno general end-to-end deep learning reconstructors exist for cryo-ET. In this\nwork, we introduce CryoLithe, a local, memory-efficient reconstruction network\nthat directly estimates the volume from an aligned tilt-series, overcoming the\nsuboptimal FBP. We demonstrate that leveraging transform-domain locality makes\nour network robust to distribution shifts, enabling effective supervised\ntraining and giving excellent results on real data -- without retraining or\nfine-tuning."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.03079",
    "c_title":[
      "Poisson Flow Joint Model for Multiphase contrast-enhanced CT"
    ],
    "c_abstract":[
      "In clinical practice, multiphase contrast-enhanced CT (MCCT) is important for\nphysiological and pathological imaging with contrast injection, which undergoes\nnon-contrast, venous, and delayed phases. Inevitably, the accumulated radiation\ndose to a patient is higher for multiphase scans than for a plain CT scan.\nLow-dose CECT is thus highly desirable, but it often leads to suboptimal image\nquality due to reduced radiation dose. Recently, a generalized Poisson flow\ngenerative model (PFGM++) was proposed to unify the diffusion model and the\nPoisson flow generative models (PFGM), and outperform either of them with an\noptimized dimensionality of the augmentation data space, holding a significant\npromise for generic or conditional image generation. In this paper, we propose\na Poisson flow joint model (PFJM) for low-dose MCCT to suppress image noise and\npreserve clinical features. Our model is built on the PFGM++ architecture to\ntransform the multiphase imaging problem into learning the joint distribution\nof routine-dose MCCT images by optimizing a task-specific generation path with\nrespect to the dimensionality D of the augmented data space. Then, our PFJM\nmodel takes the joint low-dose MCCT images as the condition and robustly drives\nthe generative trajectory towards the solution in the routine-dose MCCT domain.\nExtensive experiments demonstrate that our model is favorably compared with\ncompeting models, with MAE of 8.99 HU, SSIM of 98.75% and PSNR of 48.24db, as\naveraged over all the phases."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.13735",
    "c_title":[
      "Deep Chandra observations of PLCKG287.0+32.9: a clear detection of a\n  shock front in a heated former cool core"
    ],
    "c_abstract":[
      "The massive, hot galaxy cluster PSZ2 G286.98+32.90 (hereafter PLCKG287,\nz=0.383) hosts a giant radio halo and two prominent radio relics which are\nsigns of a disturbed dynamical state. However, despite optical and radio\nobservations indicate a clear multiple merger, the X-ray emission of the\ncluster, derived from XMM-Newton observations, shows only moderate disturbance.\nWe present new 200 ks Chandra observations of PLCKG287. We detect a shock front\nto the NW direction at a distance of ~390 kpc from the X-ray peak,\ncharacterized by a Mach number M~1.3, as well as a cold front at a distance of\n~300 kpc from the X-ray peak, nested in the same direction of the shock in a\ntypical configuration expected by a merger. We also find evidence for X-ray\ndepressions to the E and W, that could be the signature of feedback from the\nactive galactic nucleus (AGN). The radial profile of the thermodynamic\nquantities show a temperature and abundance peak in the cluster center, where\nalso the pressure and entropy have a rapid increase. Based on these properties,\nwe argue that PLCKG287 is what remains of a cool core after a heating event. We\nestimate that both the shock energy and the AGN feedback energy, implied by the\nanalysis of the X-ray cavities, are sufficient to heat the core to the observed\ntemperature of ~17 keV in the central ~160 kpc. We discuss the possible origin\nof the detected shock by investigating alternative scenarios of merger and AGN\noutburst, finding that they are both energetically viable. However, no single\nmodel seems able to explain all the X-ray features detected in this system.\nThis suggests that the combined action of merger and central AGN feedback is\nlikely necessary to explain the reheated cool core, the large-scale shock and\nthe cold front. The synergy of these two processes may act in shaping the\ndistribution of cool core and non cool core clusters. [Abridged]"
    ],
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10619",
    "c_title":[
      "Behavior of Ising spins and ecological oscillators on dynamically\n  rewired small world networks"
    ],
    "c_abstract":[
      "Many ecological populations are known to display a cyclic behavior with\nperiod 2. Previous work has shown that when a metapopulation (group of coupled\npopulations) with such dynamics is allowed to interact via nearest neighbor\ndispersal in two dimensions, it undergoes a phase transition from disordered\n(spatially asynchronous) to ordered (spatially synchronous) that falls under\nthe 2-D Ising universality class. While nearest neighbor dispersal may\nsatisfactorily describe how most individuals migrate between habitats, we\nshould expect a small fraction of individuals to venture on a journey to\nfurther locations. We model this behavior by considering ecological oscillators\non dynamically rewired small-world networks, in which at each time step a\nfraction $p$ of the nearest neighbor interactions is replaced by a new\ninteraction with a random node on the network. We measure how this connectivity\nchange affects the critical point for synchronizing ecological oscillators. Our\nresults indicate that increasing the amount of long-range interaction\n(increasing $p$) favors the ordered regime, but the presence of memory in\necological oscillators leads to quantitative differences in how much long-range\ndispersal is needed to order the network, relative to an analogous network of\nIsing spins. We also show that, even for very small values of $p$, the phase\ntransition falls into the mean-field universality class, and argue that\necosystems where dispersal can occasionally happen across the system's length\nscale will display a phase transition in the mean-field universality class."
    ],
    "c_categories":[
      "cond-mat.stat-mech"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.18530",
    "c_title":[
      "Tracking the photoinduced dynamics of a dark excitonic state in\n  single-layer WS$_2$ via resonant Autler-Townes splitting"
    ],
    "c_abstract":[
      "Excitons in a monolayer transition metal dichalcogenides (1L-TMD) are highly\nbound states characterized by a Rydberg-like spectrum of discrete energy\nlevels. Within this spectrum, states with odd-parity are known as dark excitons\nbecause transitions to the ground state are forbidden by selection rules. This\nmakes their stationary and transient characterization challenging using linear\noptical techniques. Here, we demonstrate that the dynamics of a 2p dark\nexcitonic state in 1L-WS$_2$ can be directly retrieved by measuring the\nAutler-Townes splitting of bright states in a three-pulse experiment. The\nsplitting of the bright 1s excitonic state, observed by detuning a mid-infrared\ncontrol field across the 1s-2p transition, provides an accurate\ncharacterization of the 2p state, which is used here to reveal its dynamics\nfollowing a sudden photoinjection of free carriers in the conduction band. We\nobserve a qualitatively different dynamics of the 1s and 2p levels, which is\nindicative of symmetry-dependent screening and exciton-exciton interactions.\nThese findings provide new insights into many-body effects in TMDs, offering\npotential avenues for advancing the next generation optoelectronics."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.06333",
    "c_title":[
      "VLA+VLBA to ngVLA Transition Option Concepts"
    ],
    "c_abstract":[
      "The next-generation Very Large Array (ngVLA) is intended to be the premier\ncentimeter-wavelength facility for astronomy and astrophysics, building on the\nsubstantial scientific legacies of the Karl G. Jansky Very Large Array (VLA)\nand the Very Long Baseline Array (VLBA). The ngVLA would open a new window on\nthe Universe through ultra-sensitive imaging of thermal line and continuum\nemission to milliarcsecond resolution, while delivering unprecedented\nbroad-band continuum imaging and polarimetry of non-thermal emission. The ngVLA\nwould provide a critical electromagnetic complement to a suite of particle\ndetectors and gravitational-wave observatories, as well as space- and\nground-based telescopes operating from infrared to gamma-ray wavelengths, hence\nenabling multi-messenger and multi-band astronomy and astrophysics. Current\nconstruction plans call for the ngVLA to leverage some of the physical\ninfrastructure of both the VLA and the VLBA, potentially drawing on overlapping\npersonnel and information infrastructure. Multiple options can be envisioned\nfor a VLA+VLBA to ngVLA transition. In order to assess risks and benefits of\npossible transition plans, the ngVLA project established the VLA+VLBA to ngVLA\nTransition Advisory Group (TAG). The primary deliverable from the TAG is a\n``VLA+VLBA to ngVLA Transition Option Concepts'' report (this report) that\nincludes a prioritized list of transition options."
    ],
    "c_categories":[
      "astro-ph.IM",
      "gr-qc"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "Parameter-Free FISTA by Adaptive Restart and Backtracking"
    ],
    "b_abstract":[
      "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.09469",
    "c_title":[
      "Over-Relaxation in Alternating Projections"
    ],
    "c_abstract":[
      "We improve upon the current bound on convergence rates of the Gauss-Seidel,\nKaczmarz, and more generally projection methods where projections are visited\nin randomized order. The tighter bound reveals a practical approach to speed up\nconvergence by over-relaxation -- a longstanding challenge that has been\ndifficult to overcome for general problems with deterministic Succession of\nOver-Relaxations."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14981",
    "c_title":[
      "Convex Analysis in Spectral Decomposition Systems"
    ],
    "c_abstract":[
      "This work is concerned with convex analysis of so-called spectral functions\nof matrices that only depend on eigenvalues of the matrix. An abstract\nframework of spectral decomposition systems is proposed that covers a wide\nrange of previously studied settings, including eigenvalue decomposition of\nHermitian matrices and singular value decomposition of rectangular matrices and\nallows deriving new results in more general settings such as Euclidean Jordan\nalgebras. The main results characterize convexity, lower semicontinuity,\nFenchel conjugates, convex subdifferentials, and Bregman proximity operators of\nspectral functions in terms of the reduced functions. As a byproduct, a\ngeneralization of the Ky Fan majorization theorem is obtained."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07105",
    "c_title":[
      "Numerical solution of optimal control problems using quadratic transport\n  regularization"
    ],
    "c_abstract":[
      "We address optimal control problems on the space of measures for an objective\ncontaining a smooth functional and an optimal transport regularization. That\nis, the quadratic Monge-Kantorovich distance between a given prior measure and\nthe control is penalized in the objective. We consider optimality conditions\nand reparametrize the problem using the celebrated structure theorem by\nBrenier. The optimality conditions can be formulated as a piecewise\ndifferentiable equation. This is utilized to formulate solution algorithms and\nto analyze their local convergence properties. We present a numerical example\nto illustrate the theoretical findings."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.19619",
    "c_title":[
      "Cost-optimal Management of a Residential Heating System With a\n  Geothermal Energy Storage Under Uncertainty"
    ],
    "c_abstract":[
      "In this paper, we consider a residential heating system with renewable and\nnon-renewable heat generation and different consumption units and investigate a\nstochastic optimal control problem for its cost-optimal management. As a\nspecial feature, the heating system is equipped with a geothermal storage that\nenables the intertemporal transfer of thermal energy by storing surplus heat\nfor later use. In addition to the numerous technical challenges, economic\nissues such as cost-optimal control also play a central role in the design and\noperation of such systems. The latter leads to challenging mathematical\noptimization problems, as the response of the storage to charging and\ndischarging decisions depends on the spatial temperature distribution in the\nstorage. We take into account uncertainties regarding randomly fluctuating heat\ngeneration from renewable energies and the environmental conditions that\ndetermine heat demand and supply. The dynamics of the multidimensional\ncontrolled state processes is governed by a partial, a random ordinary and two\nstochastic differential equations. We first apply a spatial discretization to\nthe partial differential equation and use model reduction techniques to reduce\nthe dimension of the associated system of ordinary differential equations.\nFinally, a time-discretization leads to a Markov decision process for which we\napply a state discretization to determine approximations of the cost-optimal\ncontrol and the associated value function."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.03390",
    "c_title":[
      "State-of-the-art Methods for Pseudo-Boolean Solving with SCIP"
    ],
    "c_abstract":[
      "The Pseudo-Boolean problem deals with linear or polynomial constraints with\ninteger coefficients over Boolean variables. The objective lies in optimizing a\nlinear objective function, or finding a feasible solution, or finding a\nsolution that satisfies as many constraints as possible. In the 2024\nPseudo-Boolean competition, solvers incorporating the SCIP framework won five\nout of six categories it was competing in. From a total of 1,207 instances,\nSCIP successfully solved 759, while its parallel version FiberSCIP solved 776.\nBased on the results from the competition, we further enhanced SCIP's\nPseudo-Boolean capabilities. This article discusses the results and presents\nthe winning algorithmic ideas."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10333",
    "c_title":[
      "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem"
    ],
    "c_abstract":[
      "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19281",
    "c_title":[
      "Statistical Physics of Deep Neural Networks: Generalization Capability,\n  Beyond the Infinite Width, and Feature Learning"
    ],
    "c_abstract":[
      "Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassing\nhuman performance. Yet their internal processes remain elusive, frequently\ndescribed as \"black boxes.\" While performance can be refined experimentally,\nachieving a fundamental grasp of their inner workings is still a challenge.\n  Statistical Mechanics has long tackled computational problems, and this\nthesis applies physics-based insights to understand DNNs via three\ncomplementary approaches.\n  First, by averaging over data, we derive an asymptotic bound on\ngeneralization that depends solely on the size of the last layer, rather than\non the total number of parameters -- revealing how deep architectures process\ninformation differently across layers.\n  Second, adopting a data-dependent viewpoint, we explore a finite-width\nthermodynamic limit beyond the infinite-width regime. This leads to: (i) a\nclosed-form expression for the generalization error in a finite-width\none-hidden-layer network (regression task); (ii) an approximate partition\nfunction for deeper architectures; and (iii) a link between deep networks in\nthis thermodynamic limit and Student's t-processes.\n  Finally, from a task-explicit perspective, we present a preliminary analysis\nof how DNNs interact with a controlled dataset, investigating whether they\ntruly internalize its structure -- collapsing to the teacher -- or merely\nmemorize it. By understanding when a network must learn data structure rather\nthan just memorize, it sheds light on fostering meaningful internal\nrepresentations.\n  In essence, this thesis leverages the synergy between Statistical Physics and\nMachine Learning to illuminate the inner behavior of DNNs."
    ],
    "c_categories":[
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18165",
    "c_title":[
      "SU(6) model revisited"
    ],
    "c_abstract":[
      "We discuss the vacuum structure of the SU(6) model, a chiral gauge theory,\nfrom the perspective of anomaly matching. To this end, we first identify all\npossible 't Hooft anomalies in the UV theory using the Stora-Zumino procedure.\nSubsequently, we construct an effective theory by applying the idea of the\nWess-Zumino-Witten action to derive the topological terms that encode the 't\nHooft anomalies. As a result, we demonstrate that a low-energy effective theory\nreproducing one of the anomalies, namely the mixed anomaly, is described by a\nZ3-valued scalar field. On the other hand, the effective theory that accounts\nfor the discrete chiral self-anomaly is significantly more intricate, and\nelucidating its structure remains an ongoing challenge."
    ],
    "c_categories":[
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12335",
    "c_title":[
      "Robust Super-Moir\\'e in Large Angle Single-Twist Bilayers"
    ],
    "c_abstract":[
      "Forming long wavelength moir\\'e superlattices (MSL) at small-angle twist van\nder Waals (vdW) bilayers has been a key approach to creating moir\\'e flat\nbands. The small-angle twist, however, leads to strong lattice reconstruction,\ncausing domain walls and moir\\'e disorders, which pose considerable challenges\nin engineering such platforms. At large twist angles, the rigid lattices render\na more robust, but shorter wavelength MSL, making it difficult to engineer flat\nbands. Here, we depict a novel approach to tailoring robust super-moir\\'e (SM)\nstructures that combines the advantages of both small-twist and large-twist\ntransition metal dichalcogenides (TMDs) bilayers using only a single twist\nangle near a commensurate angle. Structurally, we unveil the spontaneous\nformation of a periodic arrangement of three inequivalent commensurate moir\\'e\n(CM) stacking, where the angle deviation from the commensurate angle can tune\nthe periodicity. Electronically, we reveal a large set of van Hove\nsingularities (VHSs) that indicate strong band hybridization, leading to flat\nbands near the valence band maximum. Our study paves the way for a new platform\nof robust SM bilayers with structural rigidity and controllable wavelength,\nextending the investigation of the interplay among band topology, quantum\ngeometry, and moir\\'e superconductivity to the large twist angle regime."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12129",
    "c_title":[
      "Floquet engineering of point-gapped topological superconductors"
    ],
    "c_abstract":[
      "Non-Hermitian systems exhibit two distinct topological classifications based\non their gap structure: line-gap and point-gap topologies. Although point-gap\ntopology is intrinsic to non-Hermitian systems, its systematic construction\nremains a challenge. Here, we present the Floquet engineering approach for\nrealizing point-gapped topological superconductors. By combining Floquet theory\nwith particle-hole symmetry (PHS), we show that a point gap hosting robust\nMajorana edge modes emerges at the overlap of Floquet bands with opposite\nwinding numbers. In the thermodynamic limit, even weak non-Hermiticity opens a\npoint gap from a gapless spectrum, driving a topological phase transition and\nbreaking non-Bloch parity-time ($\\mathcal{PT}$) symmetry. This transition is\naccompanied by the appearance of the Floquet $Z_2$ skin effect. Furthermore,\nthe point-gapped topological phase and the non-Bloch $\\mathcal{PT}$ symmetry\nexhibit size-dependent phenomena driven by the critical skin effect. Our work\noffers a new pathway for exploring the point-gapped topological phases in\nnon-Hermitian systems."
    ],
    "c_categories":[
      "cond-mat.supr-con",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00688",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
    ],
    "b_abstract":[
      "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13320",
    "c_title":[
      "Radiative corrections on vortical spin polarization in hot QCD matter"
    ],
    "c_abstract":[
      "We investigate the radiative corrections on spin polarization of relativistic\nfermions induced by vortical fields in thermal-equilibrium QCD matter at weak\ncoupling. Such corrections stem from the self-energy gradients in quantum\nkinetic theory, which are further obtained by a more systematic and general\napproach through the Keldysh equation. By applying the hard-thermal-loop\napproximation, we obtain new corrections upon the spin-polarization spectrum\nand also the axial-charge current in connection to the axial\/chiral vortical\neffect for massive quarks up to the leading order of the QCD coupling. Further\ninfluence on spin alignment of vector mesons from similar effects is also\nanalyzed."
    ],
    "c_categories":[
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"Beyond Monte Carlo: Harnessing Diffusion Models to Simulate Financial\n  Market Dynamics",
    "a_abstract":"We propose a highly efficient and accurate methodology for generating\nsynthetic financial market data using a diffusion model approach. The synthetic\ndata produced by our methodology align closely with observed market data in\nseveral key aspects: (i) they pass the two-sample Cramer - von Mises test for\nportfolios of assets, and (ii) Q - Q plots demonstrate consistency across\nquantiles, including in the tails, between observed and generated market data.\nMoreover, the covariance matrices derived from a large set of synthetic market\ndata exhibit significantly lower condition numbers compared to the estimated\ncovariance matrices of the observed data. This property makes them suitable for\nuse as regularized versions of the latter. For model training, we develop an\nefficient and fast algorithm based on numerical integration rather than Monte\nCarlo simulations. The methodology is tested on a large set of equity data.",
    "explanation":"The paper proposes the use of a new method using diffusion model generative methodology to produce synthetic market scenarios.",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":[
      "b24"
    ],
    "c_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "c_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.08065",
    "c_title":[
      "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion\n  Probabilistic Model"
    ],
    "c_abstract":[
      "Vessel trajectory prediction is a critical component for ensuring maritime\ntraffic safety and avoiding collisions. Due to the inherent uncertainty in\nvessel behavior, trajectory prediction systems must adopt a multimodal approach\nto accurately model potential future motion states. However, existing vessel\ntrajectory prediction methods lack the ability to comprehensively model\nbehavioral multi-modality. To better capture multimodal behavior in interactive\nscenarios, we propose modeling interactions as dynamic graphs, replacing\ntraditional aggregation-based techniques that rely on vessel states. By\nleveraging the natural multimodal capabilities of diffusion models, we frame\nthe trajectory prediction task as an inverse process of motion uncertainty\ndiffusion, wherein uncertainties across potential navigational areas are\nprogressively eliminated until the desired trajectories is produced. In\nsummary, we pioneer the integration of Spatio-Temporal Graph (STG) with\ndiffusion models in ship trajectory prediction. Extensive experiments on real\nAutomatic Identification System (AIS) data validate the superiority of our\napproach."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.00870",
    "c_title":[
      "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In\n  Open Domains"
    ],
    "c_abstract":[
      "We explore neuro-symbolic approaches to generalize actionable knowledge,\nenabling embodied agents to tackle complex tasks more effectively in\nopen-domain environments. A key challenge for embodied agents is the\ngeneralization of knowledge across diverse environments and situations, as\nlimited experiences often confine them to their prior knowledge. To address\nthis issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual\nlearner that emulates the hypothetico-deductive model by continually\nformulating and validating knowledge from limited experiences through the\ncombined use of Large Language Models (LLMs) and symbolic tools. Specifically,\nwe devise a contrastive generality improvement scheme within NeSyC, which\niteratively generates hypotheses using LLMs and conducts contrastive validation\nvia symbolic tools. This scheme reinforces the justification for admissible\nactions while minimizing the inference of inadmissible ones. Additionally, we\nincorporate a memory-based monitoring scheme that efficiently detects action\nerrors and triggers the knowledge refinement process across domains.\nExperiments conducted on diverse embodied task benchmarks-including ALFWorld,\nVirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate\nthat NeSyC is highly effective in solving complex embodied tasks across a range\nof open-domain environments."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.08068",
    "c_title":[
      "A Roadmap to Guide the Integration of LLMs in Hierarchical Planning"
    ],
    "c_abstract":[
      "Recent advances in Large Language Models (LLMs) are fostering their\nintegration into several reasoning-related fields, including Automated Planning\n(AP). However, their integration into Hierarchical Planning (HP), a subfield of\nAP that leverages hierarchical knowledge to enhance planning performance,\nremains largely unexplored. In this preliminary work, we propose a roadmap to\naddress this gap and harness the potential of LLMs for HP. To this end, we\npresent a taxonomy of integration methods, exploring how LLMs can be utilized\nwithin the HP life cycle. Additionally, we provide a benchmark with a\nstandardized dataset for evaluating the performance of future LLM-based HP\napproaches, and present initial results for a state-of-the-art HP planner and\nLLM planner. As expected, the latter exhibits limited performance (3\\% correct\nplans, and none with a correct hierarchical decomposition) but serves as a\nvaluable baseline for future approaches."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.08547",
    "c_title":[
      "Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data"
    ],
    "c_abstract":[
      "The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.07445",
    "c_title":[
      "Online inductive learning from answer sets for efficient reinforcement\n  learning exploration"
    ],
    "c_abstract":[
      "This paper presents a novel approach combining inductive logic programming\nwith reinforcement learning to improve training performance and explainability.\nWe exploit inductive learning of answer set programs from noisy examples to\nlearn a set of logical rules representing an explainable approximation of the\nagent policy at each batch of experience. We then perform answer set reasoning\non the learned rules to guide the exploration of the learning agent at the next\nbatch, without requiring inefficient reward shaping and preserving optimality\nwith soft bias. The entire procedure is conducted during the online execution\nof the reinforcement learning algorithm. We preliminarily validate the efficacy\nof our approach by integrating it into the Q-learning algorithm for the Pac-Man\nscenario in two maps of increasing complexity. Our methodology produces a\nsignificant boost in the discounted return achieved by the agent, even in the\nfirst batches of training. Moreover, inductive learning does not compromise the\ncomputational time required by Q-learning and learned rules quickly converge to\nan explanation of the agent policy."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.15148",
    "c_title":[
      "Antifragility and response to damage in the synchronization of\n  oscillators on networks"
    ],
    "c_abstract":[
      "In this paper, we introduce a mathematical framework to assess the impact of\ndamage, defined as the reduction of weight in a specific link, on identical\noscillator systems governed by the Kuramoto model and coupled through weighted\nnetworks. We analyze how weight modifications in a single link affect the\nsystem when its global function is to achieve the synchronization of coupled\noscillators starting from random initial phases. We introduce different\nmeasures that allow the identification of cases where damage enhances\nsynchronization (antifragile response), deteriorates it (fragile response), or\nhas no significant impact. Using numerical solutions of the Kuramoto model, we\ninvestigate the effects of damage on network links where antifragility emerges.\nOur analysis includes lollipop graphs of varying sizes and a comprehensive\nevaluation and all the edges of 109 non-isomorphic graphs with six nodes. The\napproach is general and can be applied to study antifragility in other\noscillator systems with different coupling mechanisms, offering a pathway for\nthe quantitative exploration of antifragility in diverse complex systems."
    ],
    "c_categories":[
      "cond-mat.stat-mech",
      "nlin.AO",
      "physics.soc-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.10766",
    "c_title":[
      "Theory of spin magnetization driven by chiral phonons"
    ],
    "c_abstract":[
      "We construct a general theory of spin magnetization driven by chiral phonons\nunder an adiabatic process, in which atoms rotate around their equilibrium\npositions with a low phonon frequency. Here the spin magnetization originates\nfrom the modulated electronic states with spin-orbital coupling by atomic\nrotations. Under the adiabatic approximation, the time-dependent spin\nmagnetization can be calculated by a Berry-phase method. In this paper, we\nfocus on its time average, which is evaluated by assuming that the phonon\ndisplacement is small. As a result, the time average of the spin magnetization\nis concisely formulated in the form of the Berry curvature defined in the\nphonon-displacement space as an intrinsic property of atomic rotations. Our\nformula for spin magnetization reflects the chiral nature of phonons, and is\nconvenient for $ab$ $initio$ calculations."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.08128",
    "c_title":[
      "Intersecting Families of Spanning Trees"
    ],
    "c_abstract":[
      "A family $\\mathcal{F}$ of spanning trees of the complete graph on $n$\nvertices $K_n$ is \\emph{$t$-intersecting} if any two members have a forest on\n$t$ edges in common. We prove an Erd\\H{o}s--Ko--Rado result for\n$t$-intersecting families of spanning trees of $K_n$. In particular, we show\nthere exists a constant $C > 0$ such that for all $n \\geq C (\\log n) t$ the\nlargest $t$-intersecting families are the families consisting of all trees that\ncontain a fixed set of $t$ disjoint edges (as well as the stars on $n$ vertices\nfor $t = 1$). The proof uses the spread approximation technique in conjunction\nwith the Lopsided Lov\\'asz Local Lemma."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.08975",
    "c_title":[
      "Exploring certain geometric and harmonic properties of the Berger-type\n  metric conformal deformation on the Para-K\\\"ahler-Norden manifold"
    ],
    "c_abstract":[
      "This work presents a novel class of metrics on a para-K\\\"{a}hler-Norden\nmanifold $(M^{2m},F,g)$, derived from a conformal deformation of the\nBerger-type metric associated with the metric $g$. Initially, we examine the\nLevi-Civita link associated with this metric. Secondly, we delineate all\nvarieties of curvature for a manifold $M$ equipped with a conformal deformation\nof Berger-type metric for $g$. Finally, we studied a certain class of harmonic\nmaps."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.03956",
    "c_title":[
      "Ribbing patterns in inertial rotary drag-out"
    ],
    "c_abstract":[
      "We report pattern formation in an otherwise non-uniform and unsteady flow\narising in high-speed liquid entrainment conditions on the outer wall of a wide\nrotating drum. We show that the coating flow in this rotary drag-out undergoes\naxial modulations to form an array of roughly vertical thin liquid sheets which\nslowly drift from the middle of the drum towards its side walls. Thus, the\nnumber of sheets fluctuate in time such that the most probable rib spacing\nvaries ever so slightly with the speed, and a little less weakly with the\nviscosity. We propose that these axial patterns are generated due to a primary\ninstability driven by an adverse pressure gradient in the meniscus region of\nthe rotary drag-out flow, similar to the directional Saffman-Taylor\ninstability, as is well-known for ribbing in film-splitting flows. Rib spacing\nbased on this mechanistic model turns out to be proportional to the capillary\nlength, wherein the scaling factor can be determined based on existing models\nfor film entrainment at both low and large Capillary numbers. In addition, we\nperformed Direct Numerical Simulations, which reproduce the experimental\nphenomenology and the associated wavelength. We further include two numerical\ncases wherein either the liquid density, or the liquid surface tension are\nquadrupled while keeping all other parameters identical with experiments. The\nrib spacings of these cases are in agreement with the predictions of our model."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15557",
    "c_title":[
      "Preventing Household Bankruptcy: The One-Third Rule in Financial\n  Planning with Mathematical Validation and Game-Theoretic Insights"
    ],
    "c_abstract":[
      "This paper analyzes the 1\/3 Financial Rule, a method of allocating income\nequally among debt repayment, savings, and living expenses. Through\nmathematical modeling, game theory, behavioral finance, and technological\nanalysis, we examine the rule's potential for supporting household financial\nstability and reducing bankruptcy risk. The research develops theoretical\nfoundations using utility maximization theory, demonstrating how equal\nallocation emerges as a solution under standard economic assumptions. The\ngame-theoretic analysis explores the rule's effectiveness across different\nhousehold structures, revealing potential strategic advantages in financial\ndecision-making. We investigate psychological factors influencing financial\nchoices, including cognitive biases and neurobiological mechanisms that impact\neconomic behavior. Technological approaches, such as AI-driven personalization,\nblockchain tracking, and smart contract applications, are examined for their\npotential to support financial planning. Empirical validation using U.S. Census\ndata and longitudinal studies assesses the rule's performance across various\nhousehold types. Stress testing under different economic conditions provides\ninsights into its adaptability and resilience. The research integrates\nmathematical analysis with behavioral insights and technological perspectives\nto develop a comprehensive approach to household financial management."
    ],
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.14393",
    "c_title":[
      "Innovative Financing Solutions: A Transformative Driver for Financial\n  Performance of Businesses in Morocco"
    ],
    "c_abstract":[
      "In a rapidly evolving landscape marked by continuous change and complex\nchallenges, effective cash management stands as a cornerstone for ensuring\nbusiness sustainability and driving performance. To address these pressing\ndemands, cash managersare increasingly turning to innovative financing\nsolutions such as venture capital, green finance, crowdfunding, advanced\nservices from Pan-African banks, and blockchain technology. These cutting-edge\ntools are pivotal in bolstering resilience against market volatility,\necological transitions, and the accelerating pace of technological change. The\npresent article aims to examine how such innovative financial approaches can\nserve as strategic drivers, enabling businesses to transform challenges into\nopportunities. The analysis underscores that rethinking cash management through\ninnovation is a critical pathway toboost the performance of Moroccan companies.\nTherefore, embracing these forward-thinking strategies unlocks new avenues for\ndevelopment empowering them to adapt with agility amidst the uncertainties of a\nshifting environment."
    ],
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12010",
    "c_title":[
      "The role of FDI along transitional dynamics of the host country in an\n  endogenous growth model"
    ],
    "c_abstract":[
      "We investigate the role of foreign direct investment (FDI) in the\ntransitional dynamics of host countries by using an optimal growth model. FDI\nmay be beneficial for the host country because local people can work for\nmultinational firms to get a favorable salary. However, if the host country\nonly focuses on FDI, it may face a middle-income trap. We show that if the host\ncountry invests in research and development, its economy may have sustained\ngrowth. Moreover, in this case, FDI helps the host country only at the first\nstages of its development process."
    ],
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.08875",
    "c_title":[
      "Utilizing Pre-trained and Large Language Models for 10-K Items\n  Segmentation"
    ],
    "c_abstract":[
      "Extracting specific items from 10-K reports remains challenging due to\nvariations in document formats and item presentation. Traditional rule-based\nitem segmentation approaches often yield suboptimal results. This study\nintroduces two advanced item segmentation methods leveraging language models:\n(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize\nGPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings\nwith a Bi-LSTM model in a hierarchical structure to overcome context window\nconstraints. Trained and evaluated on 3,737 annotated 10-K reports,\nBERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),\nconditional random field (0.9818), and rule-based methods (0.9048) for core\nitems (1, 1A, 3, and 7). These approaches enhance item segmentation\nperformance, improving text analytics in accounting and finance. BERT4ItemSeg\noffers satisfactory item segmentation performance, while GPT4ItemSeg can easily\nadapt to regulatory changes. Together, they offer practical benefits for\nresearchers and practitioners, enabling reliable empirical studies and\nautomated 10-K item segmentation functionality."
    ],
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17778",
    "c_title":[
      "Heterogeneity of household stock portfolios in a national market"
    ],
    "c_abstract":[
      "We study the long term dynamics of the stock portfolios owned by single\nFinnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001\nand 2021. Using the Herfindahl-Hirschman index as a measure of concentration\nfor the composition of stock portfolios, we investigate the concentration of\nFinnish household portfolios both at the level of each individual household and\ntracking the time evolution of an aggregated Finnish household portfolio. We\nalso consider aggregated portfolios of two other macro categories of investors\none comprising Finnish institutional investors and the other comprising foreign\ninvestors. Different macro categories of investors present a different degree\nof concentration of aggregated stock portfolios with highest concentration\nobserved for foreign investors. For individual Finnish retail investors,\nportfolio concentration estimated by the Herfindahl-Hirschman index presents\nhigh values for more than half of the total number of retail investors. In\nspite of the observation that retail stock portfolios are often composed by\njust a few stocks, the concentration of the aggregated stock portfolio for\nFinnish retail investors has a portfolio concentration comparable with the one\nof Finnish institutional investors. Within retail investors, stock portfolios\nof women present a similar pattern of portfolios of men but with a systematic\nhigher level of concentration observed for women both at individual and at\naggregated level."
    ],
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07126",
    "c_title":[
      "ATOMS: ALMA Three-millimeter Observations of Massive Star-forming\n  regions -XXI. A Large-sample Observational Study of Ethanol and Dimethyl\n  Ether in Hot Cores"
    ],
    "c_abstract":[
      "Hot cores, as a stage of massive star formation, exhibit abundant line\nemissions of COMs. We present a deep line survey of two isomers of C$_2$H$_6$O:\nethanol (C$_2$H$_5$OH; EA), and dimethyl ether (CH$_3$OCH$_3$; DE) as well as\ntheir possible precursor CH$_3$OH towards 60 hot cores by using the ALMA 3 mm\nline observations. EA is detected in 40 hot cores and DE is detected in 59 hot\ncores. Of these, EA and DE are simultaneously detected in 39 hot cores. We\ncalculate rotation temperatures and column densities of EA and DE by using the\nXCLASS software. The average rotation temperature of EA is higher than that of\nDE, whereas the average column density of EA is lower than that of DE. Combined\nwith previous studies of hot cores and hot corinos, we find strong column\ndensity correlations among EA and DE ($\\rho$ = 0.92), EA and CH$_3$OH ($\\rho$ =\n0.82), as well as DE and CH$_3$OH ($\\rho$ = 0.80). The column density ratios of\nEA\/DE versus the column densities of CH$_3$OH remain nearly constant with\nvalues within ~ 1 order of magnitude. These strong correlations and the stable\nratios, suggest that EA, DE, and CH$_3$OH could be chemically linked, with\nCH$_3$OH potentially serving as a precursor for EA and DE. Compared with\nchemical models, the three different warm-up timescale models result in the\nsystematic overproduction of EA and the systematic underproduction of DE.\nTherefore, our large sample observations can provide crucial constraints on\nchemical models."
    ],
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12997",
    "c_title":[
      "A permutation group acting transitively on certain collections of models"
    ],
    "c_abstract":[
      "It is shown, from $\\sigma$-centered Martin's Axiom, that there exists a\nproper dense subgroup of the symmetric group on a countably infinite set whose\nnatural action on sufficiently flexible relational structures is transitive.\nThis allows us to give consistent positive answers to some questions of Peter\nM. Neumann from the 1980s."
    ],
    "c_categories":[
      "math.GR",
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09112",
    "c_title":[
      "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision\n  Networks for Photometric Redshift Estimation"
    ],
    "c_abstract":[
      "We present Mantis Shrimp, a multi-survey deep learning model for photometric\nredshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and\ninfrared (UnWISE) imagery. Machine learning is now an established approach for\nphotometric redshift estimation, with generally acknowledged higher performance\nin areas with a high density of spectroscopically identified galaxies over\ntemplate-based methods. Multiple works have shown that image-based\nconvolutional neural networks can outperform tabular-based color\/magnitude\nmodels. In comparison to tabular models, image models have additional design\ncomplexities: it is largely unknown how to fuse inputs from different\ninstruments which have different resolutions or noise properties. The Mantis\nShrimp model estimates the conditional density estimate of redshift using\ncutout images. The density estimates are well calibrated and the point\nestimates perform well in the distribution of available spectroscopically\nconfirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and\ncatastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion\napproaches (e.g., resampling and stacking images from different instruments)\nmatch the performance of late fusion approaches (e.g., concatenating latent\nspace representations), so that the design choice ultimately is left to the\nuser. Finally, we study how the models learn to use information across bands,\nfinding evidence that our models successfully incorporates information from all\nsurveys. The applicability of our model to the analysis of large populations of\ngalaxies is limited by the speed of downloading cutouts from external servers;\nhowever, our model could be useful in smaller studies such as generating priors\nover redshift for stellar population synthesis."
    ],
    "c_categories":[
      "astro-ph.IM",
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16068",
    "c_title":[
      "Poisson kernels on the half-plane are bell-shaped"
    ],
    "c_abstract":[
      "Consider a second-order elliptic operator $L$ in the half-plane $\\mathbb R\n\\times (0, \\infty)$ with coefficients depending only on the second coordinate.\nThe Poisson kernel for $L$ is used in the representation of positive\n$L$-harmonic functions, that is, solutions of $L u = 0$. In probabilistic\nterms, the Poisson kernel is the density function of the distribution of the\ndiffusion in $\\mathbb R \\times (0, \\infty)$ with generator $L$ at the hitting\ntime of the boundary. We prove that the Poisson kernel for $L$ is bell-shaped:\nits $n$th derivative changes sign $n$ times. In particular, it is unimodal and\nit has two inflection points (it is concave, then convex, then concave again)."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.05738",
    "c_title":[
      "Learning conformational ensembles of proteins based on backbone geometry"
    ],
    "c_abstract":[
      "Deep generative models have recently been proposed for sampling protein\nconformations from the Boltzmann distribution, as an alternative to often\nprohibitively expensive Molecular Dynamics simulations. However, current\nstate-of-the-art approaches rely on fine-tuning pre-trained folding models and\nevolutionary sequence information, limiting their applicability and efficiency,\nand introducing potential biases. In this work, we propose a flow matching\nmodel for sampling protein conformations based solely on backbone geometry. We\nintroduce a geometric encoding of the backbone equilibrium structure as input\nand propose to condition not only the flow but also the prior distribution on\nthe respective equilibrium structure, eliminating the need for evolutionary\ninformation. The resulting model is orders of magnitudes faster than current\nstate-of-the-art approaches at comparable accuracy and can be trained from\nscratch in a few GPU days. In our experiments, we demonstrate that the proposed\nmodel achieves competitive performance with reduced inference time, across not\nonly an established benchmark of naturally occurring proteins but also de novo\nproteins, for which evolutionary information is scarce."
    ],
    "c_categories":[
      "cond-mat.stat-mech",
      "cs.LG",
      "physics.comp-ph",
      "q-bio.BM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"Adding Error Bars to Evals: A Statistical Approach to Language Model\n  Evaluations",
    "a_abstract":"Evaluations are critical for understanding the capabilities of large language\nmodels (LLMs). Fundamentally, evaluations are experiments; but the literature\non evaluations has largely ignored the literature from other sciences on\nexperiment analysis and planning. This article shows researchers with some\ntraining in statistics how to think about and analyze data from language model\nevaluations. Conceptualizing evaluation questions as having been drawn from an\nunseen super-population, we present formulas for analyzing evaluation data,\nmeasuring differences between two models, and planning an evaluation\nexperiment. We make a number of specific recommendations for running language\nmodel evaluations and reporting experiment results in a way that minimizes\nstatistical noise and maximizes informativeness.",
    "explanation":"This is an interdisciplinary work because it combines two different subjects: LLM and statistics. This suggests that we could also consider the error bars in the evaluations. ",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b6"
    ],
    "c_title":[
      "The Llama 3 Herd of Models"
    ],
    "c_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.19308",
    "c_title":[
      "WOFOSTGym: A Crop Simulator for Learning Annual and Perennial Crop\n  Management Strategies"
    ],
    "c_abstract":[
      "We introduce WOFOSTGym, a novel crop simulation environment designed to train\nreinforcement learning (RL) agents to optimize agromanagement decisions for\nannual and perennial crops in single and multi-farm settings. Effective crop\nmanagement requires optimizing yield and economic returns while minimizing\nenvironmental impact, a complex sequential decision-making problem well suited\nfor RL. However, the lack of simulators for perennial crops in multi-farm\ncontexts has hindered RL applications in this domain. Existing crop simulators\nalso do not support multiple annual crops. WOFOSTGym addresses these gaps by\nsupporting 23 annual crops and two perennial crops, enabling RL agents to learn\ndiverse agromanagement strategies in multi-year, multi-crop, and multi-farm\nsettings. Our simulator offers a suite of challenging tasks for learning under\npartial observability, non-Markovian dynamics, and delayed feedback.\nWOFOSTGym's standard RL interface allows researchers without agricultural\nexpertise to explore a wide range of agromanagement problems. Our experiments\ndemonstrate the learned behaviors across various crop varieties and soil types,\nhighlighting WOFOSTGym's potential for advancing RL-driven decision support in\nagriculture."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.10075",
    "c_title":[
      "Parallelizing Multi-objective A* Search"
    ],
    "c_abstract":[
      "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.07077",
    "c_title":[
      "Rule-Based Conflict-Free Decision Framework in Swarm Confrontation"
    ],
    "c_abstract":[
      "Traditional rule-based decision-making methods with interpretable advantage,\nsuch as finite state machine, suffer from the jitter or deadlock(JoD) problems\nin extremely dynamic scenarios. To realize agent swarm confrontation, decision\nconflicts causing many JoD problems are a key issue to be solved. Here, we\npropose a novel decision-making framework that integrates probabilistic finite\nstate machine, deep convolutional networks, and reinforcement learning to\nimplement interpretable intelligence into agents. Our framework overcomes state\nmachine instability and JoD problems, ensuring reliable and adaptable decisions\nin swarm confrontation. The proposed approach demonstrates effective\nperformance via enhanced human-like cooperation and competitive strategies in\nthe rigorous evaluation of real experiments, outperforming other methods."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.02882",
    "c_title":[
      "Bringing Comparative Cognition To Computers"
    ],
    "c_abstract":[
      "Researchers are increasingly subjecting artificial intelligence systems to\npsychological testing. But to rigorously compare their cognitive capacities\nwith humans and other animals, we must avoid both over- and under-stating our\nsimilarities and differences. By embracing a comparative approach, we can\nintegrate AI cognition research into the broader cognitive sciences."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.12782",
    "c_title":[
      "VidCapBench: A Comprehensive Benchmark of Video Captioning for\n  Controllable Text-to-Video Generation"
    ],
    "c_abstract":[
      "The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps:\/\/github.com\/VidCapBench\/VidCapBench."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.10761",
    "c_title":[
      "Quantum teleportation between simulated binary black holes"
    ],
    "c_abstract":[
      "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "gr-qc",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.05271",
    "c_title":[
      "The double minimum E(3)$^1\\Sigma^{+}_{\\mathrm{u}}$ state in Cs$_2$"
    ],
    "c_abstract":[
      "The double minimum E$^1\\Sigma^{+}_{\\mathrm{u}}$ state in caesium dimer was\ninvestigated by analysing spectra of the E$^1\\Sigma^{+}_{\\mathrm{u}}$\n$\\leftarrow$ X$^1\\Sigma^{+}_{\\mathrm{g}}$ band system, simplified by\npolarisation labelling. A total of 6727 rotationally resolved transitions to\nlevels situated in the inner well and above the internal barrier were\nidentified and a potential energy curve allowing to reproduce their energies\nwas constructed using the Fourier grid Hamiltonian and inverted perturbation\napproach methods. The unambiguousness of the potential curve in view of lack of\ndata related to levels located in the outer well is discussed."
    ],
    "c_categories":[
      "physics.atom-ph",
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.14014",
    "c_title":[
      "Estimating ionization states and continuum lowering from ab initio path\n  integral Monte Carlo simulations for warm dense hydrogen"
    ],
    "c_abstract":[
      "Warm dense matter (WDM) is an active field of research, with applications\nranging from astrophysics to inertial confinement fusion. Ionization degree and\ncontinuum lowering are important quantities to understand how materials behave\nunder these conditions, but can be difficult to diagnose since experimental\ncampaigns are limited and often require model-dependent analysis. This is\nespecially true for hydrogen, which has a comparably low scattering cross\nsection, making high quality data particularly difficult to obtain.\nConsequently, building equation of state tables often relies on exact\nsimulations in combination with untested approximations to extract properties\nfrom experiments. Here, we investigate an approach for extracting the\nionization potential depression and ionization degree -- quantities which are\notherwise not directly accessible from the physical model -- from exact ab\ninitio path integral Monte Carlo (PIMC) simulations utilizing a chemical model.\nIn contrast to experimental measurements, where noise and non-equilibrium\neffects add to the uncertainty of the inferred parameters, PIMC simulations\nprovide a clean signal with well-defined thermodynamic conditions. Comparisons\nagainst commonly used models show a qualitative agreement, but we find\ndeviations primarily for the high density and high temperature cases. We also\ndemonstrate the decreasing sensitivity of the dynamic structure factor with\nrespect to both ionization and continuum lowering for increasing scattering\nangles in x-ray Thomson scattering experiments. Our work has important\nimplications for the design of future experiments, but also offers qualitative\nunderstanding of structure factors and the imaginary-time correlation function\nobtained from exact quantum Monte Carlo simulations."
    ],
    "c_categories":[
      "physics.chem-ph",
      "physics.plasm-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.07866",
    "c_title":[
      "Multifractal-enriched mobility edges and emergent quantum phases in\n  Rydberg atomic arrays"
    ],
    "c_abstract":[
      "Anderson localization describes disorder-induced phase transitions,\ndistinguishing between localized and extended states. In quasiperiodic systems,\na third multifractal state emerges, characterized by unique energy and wave\nfunctions. However, critical indicators for differentiating these states, such\nas Lyapunov exponents (LEs) and inverse participation ratios (IPRs), have yet\nto be experimentally detected. To address these challenges, we introduce\nexactly solvable one-dimensional quasiperiodic lattice models with flat bands,\nanalytically determining phase boundaries using Avila's global theorem. We\npropose experimental realizations using Rydberg atom arrays, enabling the\ndistinction of localized, extended, and multifractal states with as few as 18\nqubits. Importantly, we develop a robust spectroscopic method for the\nexperimental measurement of LEs and IPRs. Our work opens new avenues for the\nexperimental exploration of Anderson localization and multifractal states in\nartificial quantum systems."
    ],
    "c_categories":[
      "cond-mat.dis-nn",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.11664",
    "c_title":[
      "Constraints on the anomalous Higgs boson couplings in $Z\\gamma\\gamma$\n  production at muon collider"
    ],
    "c_abstract":[
      "We investigate the sensitivities on the Wilson coefficients of dimension-six\noperators associated with the anomalous $H\\gamma\\gamma$, $H\\gamma Z$ and $HZZ$\nvertices through the process $\\mu^- \\mu^+ \\rightarrow Z\\gamma\\gamma$ at future\nmuon collider. Signal events involving Higgs-gauge boson interactions and\nrelevant backgrounds events at the muon collider designed with center-of-mass\nenergy of 10 TeV and integrated luminosity of 10 ab$^{-1}$ are generated in\nMadgraph within the framework of the model-independent Standard Model effective\nfield theory and detector effects are also included with corresponding detector\ncards in Delphes. The limits at 95% C.L. on the coefficients\n$\\overline{c}_{HB}$ and $\\overline{c}_{HW}$ are reported to be [-0.0052;\n0.0040] and [-0.0034; 0.0023], respectively, and compared with the experimental\nand phenomenological limits."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03065",
    "c_title":[
      "Meta-analysis of median survival times with inverse-variance weighting"
    ],
    "c_abstract":[
      "We consider the problem of meta-analyzing outcome measures based on median\nsurvival times, such as the difference of median survival times between groups.\nPrimary studies with time-to-event outcomes often report estimates of median\nsurvival times and corresponding confidence intervals based on the Kaplan-Meier\nestimator. However, applying conventional inverse-variance weighted methods to\nmeta-analyze outcome measures based on median survival is often challenging\nbecause within-study standard error estimates are typically not available in\nthis setting. In this article, we consider an inverse-variance weighted\napproach to meta-analyze median survival times that estimates the within-study\nstandard errors from the reported confidence intervals. We conduct a series of\nsimulation studies evaluating the performance of this approach at the study\nlevel (i.e., for estimating the standard error of median survival) and the\nmeta-analytic level (i.e., for estimating the pooled median, difference of\nmedians, and ratio of medians). We find that this approach often performs\ncomparable to a benchmark approach that uses the true within-study standard\nerrors for meta-analyzing median-based outcome measures. We then illustrate an\napplication of this approach in a meta-analysis evaluating survival benefits of\nbeing assigned to experimental arms versus comparator arms in randomized trials\nfor non-small cell lung cancer therapies."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18740",
    "c_title":[
      "On Robust Aggregation for Distributed Data"
    ],
    "c_abstract":[
      "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02846",
    "c_title":[
      "Don't Let Your Likert Scales Grow Up To Be Visual Analog Scales:\n  Understanding the Relationship Between Number of Response Categories and\n  Measurement Error"
    ],
    "c_abstract":[
      "The use of Visual Analog Scales (VAS), which can be broadly conceptualized as\nitems where the response scale is 0-100, has surged recently due to the\nconvenience of digital assessments. However, there is no consensus as to\nwhether the use of VAS scales is optimal in a measurement sense. Put\ndifferently, in the 90+ years since Likert introduced his eponymous scale, the\nfield does not know how to determine the optimal number of response options for\na given item. In the current work, we investigate the optimal number of\nresponse categories using a series of simulations. We find that when the\nmeasurement error of an item is not dependent on the number of response\ncategories, there is no true optimum; rather, reliability increases with number\nof response options and then plateaus. However, under the more realistic\nassumption that the measurement error of an item increases with the number of\nresponse categories, we find a clear optimum that depends on the rate of that\nincrease. If measurement error increases with the number of response\ncategories, then conversion of any Likert scale item to VAS will result in a\ndrastic decrease in reliability. Finally, if researchers do want to change the\nresponse scale of a validated measure, they must re-validate the new measure as\nthe measurement error of the scale is likely to change."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15414",
    "c_title":[
      "Modal Clustering for Categorical Data"
    ],
    "c_abstract":[
      "Despite the inherent lack of a ground truth in clustering, a broad consensus\nis overall acknowledged in defining the concept of cluster in the continuous\nsetting. Conversely, this remains controversial in the presence of categorical\ndata. We propose a novel notion of cluster based on the dual concepts of high\nfrequency and variable association. We show how the concept of high frequency\naligns with the cluster notion provided by modal clustering in the continuous\nsetting, which allows us to borrow and adapt existing operational tools to\ndevelop a novel procedure. The method is illustrated on some real data and\ntested via simulations."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07450",
    "c_title":[
      "Functional Linear Cox Regression Model with Frailty"
    ],
    "c_abstract":[
      "This paper presents a functional linear Cox regression model with frailty to\ntackle unobserved heterogeneity in survival data with functional covariates.\nWhile traditional Cox models are common, they struggle to incorporate frailty\neffects that represent individual differences not captured by observed\ncovariates. Our model combines scalar and functional covariates with a frailty\nterm to address these unmeasured influences, creating a robust framework for\nhigh-dimensional survival analysis. We estimate parameters using functional\nprincipal component analysis and apply penalized partial likelihood for the\nfrailty structure. A simulation study shows that our model outperforms\ntraditional approaches in estimation accuracy and predictive capacity,\nespecially with high frailty. We also analyze data from the National Health and\nNutrition Examination Survey, highlighting significant links between physical\nactivity and mortality in frail subpopulations. Our findings demonstrate the\nmodel's effectiveness in managing complex survival data, with potential\napplications in biomedical research related to unobserved heterogeneity. The\nmethod is available as an R package."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.14272",
    "c_title":[
      "Spectral properties of the neutron star low-mass X-ray binary 4U\n  1636-53, XTE J1739-285 and MAXI J1816-195"
    ],
    "c_abstract":[
      "We investigated simultaneous NICER plus NuSTAR observations of three neutron\nstar low-mass X-ray binary 4U 1636-53, XTE J1739-285 and MAXI J1816-195 using\nthe latest reflection models, with the seed photons feeding into the corona\noriginating from either the neutron star (NS) or the accretion disk. We found\nthat, for the sources in the hard spectral state, more than $\\sim$ 50% of the\nNS photons enter into the corona if NS provides seed photons, while only $\\sim$\n3%-5% disk photons go to the corona if seed photons come from the disk. This\nfinding, together with the derived small height of the corona, favors the\nlamp-post geometry or boundary layer scenario where the corona is close to the\ncentral neutron star. Additionally, we found that the source of the seed\nphotons has big influence in the significance of the NS radiation, especially\nfor the soft spectral state. This result may help explain why the NS radiation\nin MAXI J1816-195 is weak in the previous work. More importantly, for the first\ntime, we explored the properties of the corona in the NS systems with the\ncompactness ($l-\\theta$) diagram. We found that the corona in the NS systems\nall lie in the left side of the pair-production forbidden region, away from the\npredicted pair-production lines. This finding indicates that either the corona\nin these NS systems is not pair-dominated, possibly due to the additional\ncooling from NS photons, or the corona is composed of both thermal and\nnon-thermal electrons."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15275",
    "c_title":[
      "A Tale of Two Sides of Wafer: Physical Implementation and Block-Level\n  PPA on Flip FET with Dual-sided Signals"
    ],
    "c_abstract":[
      "As the conventional scaling of logic devices comes to an end, functional\nwafer backside and 3D transistor stacking are consensus for next-generation\nlogic technology, offering considerable design space extension for powers,\nsignals or even devices on the wafer backside. The Flip FET (FFET), a novel\ntransistor architecture combining 3D transistor stacking and fully functional\nwafer backside, was recently proposed. With symmetric dual-sided standard cell\ndesign, the FFET can deliver around 12.5% cell area scaling and faster but more\nenergy-efficient libraries beyond other stacked transistor technologies such as\nCFET. Besides, thanks to the novel cell design with dual-sided pins, the FFET\nsupports dual-sided signal routing, delivering better routability and larger\nbackside design space. In this work, we demonstrated a comprehensive FFET\nevaluation framework considering physical implementation and block-level\npower-performance-area (PPA) assessment for the first time, in which key\nfunctions are dual-sided routing and dual-sided RC extraction. A 32-bit RISC-V\ncore was used for the evaluation here. Compared to the CFET with single-sided\nsignals, the FFET with single-sided signals achieved 23.3% post-P&R core area\nreduction, 25.0% higher frequency and 11.9% lower power at the same\nutilization, and 16.0 % higher frequency at the same core area. Meanwhile, the\nFFET supports dual-sided signals, which can further benefit more from flexible\nallocation of cell input pins on both sides. By optimizing the input pin\ndensity and BEOL routing layer number on each side, 10.6% frequency gain was\nrealized without power degradation compared to the one with single-sided signal\nrouting. Moreover, the routability and power efficiency of FFET barely degrades\neven with the routing layer number reduced from 12 to 5 on each side,\nvalidating the great space for cost-friendly design enabled by FFET."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "cs.AR",
      "physics.app-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03590",
    "c_title":[
      "Topological phases of non-interacting systems: A general approach based\n  on states"
    ],
    "c_abstract":[
      "In this work we provide a classification scheme for topological phases of\ncertain systems whose observable algebra is described by a trivial\n$C^*$-bundles. The classification is based on the study of the homotopy classes\nof \\emph{configurations}, which are maps from a \\emph{quantum parameter space}\nto the space of pure states of a reference \\emph{fiber} $C^*$-algebra. Both the\nquantum parameter space and the fiber algebra are naturally associated with the\nobservable algebra. A list of various examples described in the last section\nshows that the common classification scheme of non-interacting topological\ninsulators of type A is recovered inside this new formalism."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "math-ph",
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01027",
    "c_title":[
      "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and\n  Guarantees"
    ],
    "c_abstract":[
      "Learning-to-Defer (L2D) facilitates optimal task allocation between AI\nsystems and decision-makers. Despite its potential, we show that current\ntwo-stage L2D frameworks are highly vulnerable to adversarial attacks, which\ncan misdirect queries or overwhelm decision agents, significantly degrading\nsystem performance. This paper conducts the first comprehensive analysis of\nadversarial robustness in two-stage L2D frameworks. We introduce two novel\nattack strategies -- untargeted and targeted -- that exploit inherent\nstructural vulnerabilities in these systems. To mitigate these threats, we\npropose SARD, a robust, convex, deferral algorithm rooted in Bayes and\n$(\\mathcal{R},\\mathcal{G})$-consistency. Our approach guarantees optimal task\nallocation under adversarial perturbations for all surrogates in the\ncross-entropy family. Extensive experiments on classification, regression, and\nmulti-task benchmarks validate the robustness of SARD."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15636",
    "c_title":[
      "Investigating Circularity in India's Textile Industry: Overcoming\n  Challenges and Leveraging Digitization for Growth"
    ],
    "c_abstract":[
      "India's growing population and economy have significantly increased the\ndemand and consumption of natural resources. As a result, the potential\nbenefits of transitioning to a circular economic model have been extensively\ndiscussed and debated among various Indian stakeholders, including\npolicymakers, industry leaders, and environmental advocates. Despite the\nnumerous initiatives, policies, and transnational strategic partnerships of the\nIndian government, most small and medium enterprises in India face significant\nchallenges in implementing circular economy practices. This is due to the lack\nof a clear pathway to measure the current state of the circular economy in\nIndian industries and the absence of a framework to address these challenges.\nThis paper examines the circularity of the 93-textile industry in India using\nthe C-Readiness Tool. The analysis comprehensively identified 9 categories with\n34 barriers to adopting circular economy principles in the textile sector\nthrough a narrative literature review. The identified barriers were further\ncompared against the findings from a C-readiness tool assessment, which\nrevealed prominent challenges related to supply chain coordination, consumer\nengagement, and regulatory compliance within the industry's circularity\nefforts. In response to these challenges, the article proposes a strategic\nroadmap that leverages digital technologies to drive the textile industry\ntowards a more sustainable and resilient industrial model."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"White-Box Diffusion Transformer for single-cell RNA-seq generation",
    "a_abstract":"As a powerful tool for characterizing cellular subpopulations and cellular\nheterogeneity, single cell RNA sequencing (scRNA-seq) technology offers\nadvantages of high throughput and multidimensional analysis. However, the\nprocess of data acquisition is often constrained by high cost and limited\nsample availability. To overcome these limitations, we propose a hybrid model\nbased on Diffusion model and White-Box transformer that aims to generate\nsynthetic and biologically plausible scRNA-seq data. Diffusion model\nprogressively introduce noise into the data and then recover the original data\nthrough a denoising process, a forward and reverse process that is particularly\nsuitable for generating complex data distributions. White-Box transformer is a\ndeep learning architecture that emphasizes mathematical interpretability. By\nminimizing the encoding rate of the data and maximizing the sparsity of the\nrepresentation, it not only reduces the computational burden, but also provides\nclear insight into underlying structure. Our White-Box Diffusion Transformer\ncombines the generative capabilities of Diffusion model with the mathematical\ninterpretability of White-Box transformer. Through experiments using six\ndifferent single-cell RNA-Seq datasets, we visualize both generated and real\ndata using t-SNE dimensionality reduction technique, as well as quantify\nsimilarity between generated and real data using various metrics to demonstrate\ncomparable performance of White-Box Diffusion Transformer and Diffusion\nTransformer in generating scRNA-seq data alongside significant improvements in\ntraining efficiency and resource utilization. Our code is available at\nhttps:\/\/github.com\/lingximamo\/White-Box-Diffusion-Transformer",
    "explanation":"As a powerful tool for characterizing cellular subpopulations and cellular heterogeneity, single cell\nRNA sequencing (scRNA-seq) technology offers advantages of high throughput and multidimensional\nanalysis. However, the process of data acquisition is often constrained by high cost and limited\nsample availability. To overcome these limitations, we propose a model based on Diffusion model\nand White-Box transformer that aims to generate synthetic and biologically plausible scRNA-seq\ndata.",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b25"
    ],
    "c_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "c_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04583",
    "c_title":[
      "Overcoming Fake Solutions in Semi-Dual Neural Optimal Transport: A\n  Smoothing Approach for Learning the Optimal Transport Plan"
    ],
    "c_abstract":[
      "We address the convergence problem in learning the Optimal Transport (OT)\nmap, where the OT Map refers to a map from one distribution to another while\nminimizing the transport cost. Semi-dual Neural OT, a widely used approach for\nlearning OT Maps with neural networks, often generates fake solutions that fail\nto transfer one distribution to another accurately. We identify a sufficient\ncondition under which the max-min solution of Semi-dual Neural OT recovers the\ntrue OT Map. Moreover, to address cases when this sufficient condition is not\nsatisfied, we propose a novel method, OTP, which learns both the OT Map and the\nOptimal Transport Plan, representing the optimal coupling between two\ndistributions. Under sharp assumptions on the distributions, we prove that our\nmodel eliminates the fake solution issue and correctly solves the OT problem.\nOur experiments show that the OTP model recovers the optimal transport map\nwhere existing methods fail and outperforms current OT-based models in\nimage-to-image translation tasks. Notably, the OTP model can learn stochastic\ntransport maps when deterministic OT Maps do not exist, such as one-to-many\ntasks like colorization."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07741",
    "c_title":[
      "Advancing climate model interpretability: Feature attribution for Arctic\n  melt anomalies"
    ],
    "c_abstract":[
      "The focus of our work is improving the interpretability of anomalies in\nclimate models and advancing our understanding of Arctic melt dynamics. The\nArctic and Antarctic ice sheets are experiencing rapid surface melting and\nincreased freshwater runoff, contributing significantly to global sea level\nrise. Understanding the mechanisms driving snowmelt in these regions is\ncrucial. ERA5, a widely used reanalysis dataset in polar climate studies,\noffers extensive climate variables and global data assimilation. However, its\nsnowmelt model employs an energy imbalance approach that may oversimplify the\ncomplexity of surface melt. In contrast, the Glacier Energy and Mass Balance\n(GEMB) model incorporates additional physical processes, such as snow\naccumulation, firn densification, and meltwater percolation\/refreezing,\nproviding a more detailed representation of surface melt dynamics. In this\nresearch, we focus on analyzing surface snowmelt dynamics of the Greenland Ice\nSheet using feature attribution for anomalous melt events in ERA5 and GEMB\nmodels. We present a novel unsupervised attribution method leveraging\ncounterfactual explanation method to analyze detected anomalies in ERA5 and\nGEMB. Our anomaly detection results are validated using MEaSUREs ground-truth\ndata, and the attributions are evaluated against established feature ranking\nmethods, including XGBoost, Shapley values, and Random Forest. Our attribution\nframework identifies the physics behind each model and the climate features\ndriving melt anomalies. These findings demonstrate the utility of our\nattribution method in enhancing the interpretability of anomalies in climate\nmodels and advancing our understanding of Arctic melt dynamics."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.13318",
    "c_title":[
      "VUS: Effective and Efficient Accuracy Measures for Time-Series Anomaly\n  Detection"
    ],
    "c_abstract":[
      "Anomaly detection (AD) is a fundamental task for time-series analytics with\nimportant implications for the downstream performance of many applications. In\ncontrast to other domains where AD mainly focuses on point-based anomalies\n(i.e., outliers in standalone observations), AD for time series is also\nconcerned with range-based anomalies (i.e., outliers spanning multiple\nobservations). Nevertheless, it is common to use traditional point-based\ninformation retrieval measures, such as Precision, Recall, and F-score, to\nassess the quality of methods by thresholding the anomaly score to mark each\npoint as an anomaly or not. However, mapping discrete labels into continuous\ndata introduces unavoidable shortcomings, complicating the evaluation of\nrange-based anomalies. Notably, the choice of evaluation measure may\nsignificantly bias the experimental outcome. Despite over six decades of\nattention, there has never been a large-scale systematic quantitative and\nqualitative analysis of time-series AD evaluation measures. This paper\nextensively evaluates quality measures for time-series AD to assess their\nrobustness under noise, misalignments, and different anomaly cardinality\nratios. Our results indicate that measures producing quality values\nindependently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for\ntime-series AD. Motivated by this observation, we first extend the AUC-based\nmeasures to account for range-based anomalies. Then, we introduce a new family\nof parameter-free and threshold-independent measures, Volume Under the Surface\n(VUS), to evaluate methods while varying parameters. We also introduce two\noptimized implementations for VUS that reduce significantly the execution time\nof the initial implementation. Our findings demonstrate that our four measures\nare significantly more robust in assessing the quality of time-series AD\nmethods."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.20957",
    "c_title":[
      "Reward Dimension Reduction for Scalable Multi-Objective Reinforcement\n  Learning"
    ],
    "c_abstract":[
      "In this paper, we introduce a simple yet effective reward dimension reduction\nmethod to tackle the scalability challenges of multi-objective reinforcement\nlearning algorithms. While most existing approaches focus on optimizing two to\nfour objectives, their abilities to scale to environments with more objectives\nremain uncertain. Our method uses a dimension reduction approach to enhance\nlearning efficiency and policy performance in multi-objective settings. While\nmost traditional dimension reduction methods are designed for static datasets,\nour approach is tailored for online learning and preserves Pareto-optimality\nafter transformation. We propose a new training and evaluation framework for\nreward dimension reduction in multi-objective reinforcement learning and\ndemonstrate the superiority of our method in environments including one with\nsixteen objectives, significantly outperforming existing online dimension\nreduction methods."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.19104",
    "c_title":[
      "Neural Collapse Beyond the Unconstrained Features Model: Landscape,\n  Dynamics, and Generalization in the Mean-Field Regime"
    ],
    "c_abstract":[
      "Neural Collapse is a phenomenon where the last-layer representations of a\nwell-trained neural network converge to a highly structured geometry. In this\npaper, we focus on its first (and most basic) property, known as NC1: the\nwithin-class variability vanishes. While prior theoretical studies establish\nthe occurrence of NC1 via the data-agnostic unconstrained features model, our\nwork adopts a data-specific perspective, analyzing NC1 in a three-layer neural\nnetwork, with the first two layers operating in the mean-field regime and\nfollowed by a linear layer. In particular, we establish a fundamental\nconnection between NC1 and the loss landscape: we prove that points with small\nempirical loss and gradient norm (thus, close to being stationary)\napproximately satisfy NC1, and the closeness to NC1 is controlled by the\nresidual loss and gradient norm. We then show that (i) gradient flow on the\nmean squared error converges to NC1 solutions with small empirical loss, and\n(ii) for well-separated data distributions, both NC1 and vanishing test loss\nare achieved simultaneously. This aligns with the empirical observation that\nNC1 emerges during training while models attain near-zero test error. Overall,\nour results demonstrate that NC1 arises from gradient training due to the\nproperties of the loss landscape, and they show the co-occurrence of NC1 and\nsmall test error for certain data distributions."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15880",
    "c_title":[
      "Entropy Evolution Towards DARKexp of IllustrisTNG Dark Matter Halos"
    ],
    "c_abstract":[
      "Dark matter (DM) halos simulated via N-body techniques are known to exhibit\nnearly universal profiles at equilibrium; however, their origin remains\nuncertain despite thorough investigation. This work aims to probe the origin of\nsimulated DM halo structure by testing DARKexp, a first-principles approach\nthat describes the maximum entropy state of collisionless, self-gravitating\nsystems in equilibrium via their energy distributions, and proposes an entropy\nfunctional that is expected to increase during evolution. We fit the DARKexp\nenergy distribution to a set of massive equilibrium halos from the cosmological\nsimulation IllustrisTNG. For the first time, we calculate the entropy of these\nhalos as a function of cosmological time by tracking halos that are in\nequilibrium at z = 0.0, to z = 3.0 and calculating their entropy at various\nepochs between. We find that DARKexp provides an excellent fit to the energy\ndistributions of equilibrium DM halos and that such halos exhibit an overall\nincrease in entropy during evolution. Our results indicate that DM halos evolve\nto become their maximum entropy state at equilibrium and that this state is\ndescribed by DARKexp."
    ],
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16839",
    "c_title":[
      "Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans"
    ],
    "c_abstract":[
      "Among generative neural models, flow matching techniques stand out for their\nsimple applicability and good scaling properties. Here, velocity fields of\ncurves connecting a simple latent and a target distribution are learned. Then\nthe corresponding ordinary differential equation can be used to sample from a\ntarget distribution, starting in samples from the latent one. This paper\nreviews from a mathematical point of view different techniques to learn the\nvelocity fields of absolutely continuous curves in the Wasserstein geometry. We\nshow how the velocity fields can be characterized and learned via i) transport\nplans (couplings) between latent and target distributions, ii) Markov kernels\nand iii) stochastic processes, where the latter two include the coupling\napproach, but are in general broader. Besides this main goal, we show how flow\nmatching can be used for solving Bayesian inverse problems, where the\ndefinition of conditional Wasserstein distances plays a central role. Finally,\nwe briefly address continuous normalizing flows and score matching techniques,\nwhich approach the learning of velocity fields of curves from other directions."
    ],
    "c_categories":[
      "cs.LG",
      "math.PR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02203",
    "c_title":[
      "Relationship between polymorphic structures and magnetic properties of\n  La$_{2-x}A'_{x}$Ni$_{7}$ compounds ($A$' = Sm, Gd)"
    ],
    "c_abstract":[
      "In this study, the crystal structure and magnetic properties of La$_{2-x}\nA'_{x}$Ni$_{7}$ compounds with magnetic rare earth elements ($A$' = Sm, Gd)\nhave been investigated combining X-ray powder diffraction and magnetic\nmeasurements. These intergrowth compounds crystallize in a mixture of 2$H$\nhexagonal (Ce$_{2}$Ni$_{7}$-type) and 3$R$ rhombohedral (Gd$_{2}$Co$_{7}$-type)\npolymorphic structures which are related to the stacking of [$AB_{5}$] and\n[$A_{2}B_{4}$] subunits along the $c$-axis.The average cell volume decreases\nlinearly versus $A$' content, whereas the $c\/a$ ratio reaches a minimum at $x$\n= 1, due to geometric constraints upon $A$' for La substitution between the two\ndifferent subunits. The magnetic properties strongly depend on the structure\ntype and the $A$' content. Hexagonal La$_{2}$Ni$_{7}$ is a weak antiferromagnet\n(wAFM) at low field and temperature and undergoes metamagnetic transitions\ntowards weak ferromagnetic state (wFM) under applied field. Under an applied\nfield of 0.1 T, La$_{2-x}A'_{x}$Ni$_{7}$ intermetallic compounds display two\ndifferent transition temperatures $T_{1}$ and $T_{2}$ that both increase with\n$x$. $T_{1}$ is associated with a wFM-wAFM transition in the 2$H$ phase for\n$A$'= Sm, whereas $T_{2}$ is related to the Curie temperature of both 2$H$ and\n3$R$ phases. A metamagnetic behaviour is observed between $T_{1}$ and $T_{2}$\nwith transition field $\\mu_{0}H_{Trans}$ between 2 and 3.5 T for compounds with\n$A$' = Sm. The La$_{2-x}Sm_{x}$Ni$_{7}$ compounds ($x$ > 0} behave as hard\nmagnets with a large coercive field $\\mu_{0}H_{C}$ at low temperature\n($\\mu_{0}H_{C}$ > 9 T at 5 K for $x$ = 2), whereas the La$_{2-x}\nGd_{x}$Ni$_{7}$ compounds ($x$ > 0) are soft ferrimagnets with a linear\nincrease of the saturation magnetization versus Gd content."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.13562",
    "c_title":[
      "Charge calibration of MALTA2, a radiation hard depleted monolithic\n  active pixel sensor"
    ],
    "c_abstract":[
      "MALTA2 is a depleted monolithic active pixel sensor (DMAPS) designed for\ntracking at high rates and typically low detection threshold of\n$\\sim150\\,\\mathrm{e^-}$. A precise knowledge of the threshold is crucial to\nunderstanding the charge collection in the pixel and specifying the environment\nfor sensor application. A simple procedure is developed to calibrate the\nthreshold to unit electrons making use of a dedicated charge injection circuit\nand an Fe-55 source with dominant charge deposition of $1600\\, \\mathrm{e^-}$.\nThe injection voltage is determined which corresponds to the injection under\nFe-55 exposure and is the basis for charge calibration. The charge injection\ncircuit incorporates a capacitance with design value of $\\mathrm{C_{inj}}=$ 230\naF. Experimentally, the average capacitance value for non-irradiated samples is\nfound to be $\\mathrm{C_{inj,exp}}=$ 257 aF. The 12 % divergence motivates the\nneed for the presented precise calibration procedure, which is proposed to be\nperformed for each MALTA2 sensor."
    ],
    "c_categories":[
      "hep-ex",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.12949",
    "c_title":[
      "Deriving Perelman's Entropy from Colding's Monotonic Volume"
    ],
    "c_abstract":[
      "In his groundbreaking work from 2002, Perelman introduced two fundamental\nmonotonic quantities: the reduced volume and the entropy. While the reduced\nvolume was motivated by the Bishop-Gromov volume comparison applied to a\nsuitably constructed $N$-space, which becomes Ricci-flat as $N\\rightarrow\n\\infty$, Perelman did not provide a corresponding explanation for the origin of\nthe entropy. In this article, we demonstrate that Perelman's entropy emerges as\nthe limit of Colding's monotonic volume for harmonic functions on Ricci-flat\nmanifolds, when appropriately applied to Perelman's $N$-space."
    ],
    "c_categories":[
      "math.AP",
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15472",
    "c_title":[
      "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search"
    ],
    "c_abstract":[
      "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04822",
    "c_title":[
      "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)"
    ],
    "c_abstract":[
      "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04869",
    "c_title":[
      "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer"
    ],
    "c_abstract":[
      "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18758",
    "c_title":[
      "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features"
    ],
    "c_abstract":[
      "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06845",
    "c_title":[
      "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization"
    ],
    "c_abstract":[
      "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01646",
    "c_title":[
      "More separations of cardinal characteristics of the strong measure zero\n  ideal"
    ],
    "c_abstract":[
      "Let $\\mathcal{N}$ be the $\\sigma$-ideal of the null sets of reals. We\nintroduce a new property of forcing notions that enable control of the\nadditivity of $\\mathcal{N}$ after finite support iterations. This is applied to\nanswer some open questions from the work of Brendle, the first author, and\nMej\\'ia~\\cite{BCM2}."
    ],
    "c_categories":[
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09216",
    "c_title":[
      "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal"
    ],
    "c_abstract":[
      "Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy\/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties."
    ],
    "c_categories":[
      "cond-mat.supr-con"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13736",
    "c_title":[
      "Discrete Layered Entropy, Conditional Compression and a Tighter Strong\n  Functional Representation Lemma"
    ],
    "c_abstract":[
      "We study a quantity called discrete layered entropy, which approximates the\nShannon entropy within a logarithmic gap. Compared to the Shannon entropy, the\ndiscrete layered entropy is piecewise linear, approximates the expected length\nof the optimal one-to-one non-prefix-free encoding, and satisfies an elegant\nconditioning property. These properties make it useful for approximating the\nShannon entropy in linear programming, studying the optimal length of\nconditional encoding, and bounding the entropy of monotonic mixture\ndistributions. In particular, it can give a bound for the strong functional\nrepresentation lemma that improves upon the best bound (as long as the mutual\ninformation is at least 2)."
    ],
    "c_categories":[
      "cs.IT",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11367",
    "c_title":[
      "Spectrality of a measure consisting of two line segments"
    ],
    "c_abstract":[
      "Take an interval $[t, t+1]$ on the $x$-axis together with the same interval\non the $y$-axis and let $\\rho$ be the normalized one-dimensional Lebesgue\nmeasure on this set of two segments. Continuing the work done by Lai, Liu and\nPrince (2021) as well as Ai, Lu and Zhou (2023) we examine the spectrality of\nthis measure for all different values of $t$ (being spectral means that there\nis an orthonormal basis for $L^2(\\rho)$ consisting of exponentials $e^{2\\pi i\n(\\lambda_1 x + \\lambda_2 y)}$). We almost complete the study showing that for\n$-\\frac12<t<0$ and for all $t \\notin {\\mathbb Q}$ the measure $\\rho$ is not\nspectral. The only remaining undecided case is the case $t=-\\frac12$ (plus\nspace). We also observe that in all known cases of spectral instances of this\nmeasure the spectrum is contained in a line and we give an easy necessary and\nsufficient condition for such measures to have a line spectrum."
    ],
    "c_categories":[
      "math.CA",
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13399",
    "c_title":[
      "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research"
    ],
    "c_abstract":[
      "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https:\/\/huggingface.co\/datasets\/jmhb\/microvqa, and project page at\nhttps:\/\/jmhb0.github.io\/microvqa."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "q-bio.CB"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"Quantifying perturbation impacts for large language models",
    "a_abstract":"We consider the problem of quantifying how an input perturbation impacts the\noutputs of large language models (LLMs), a fundamental task for model\nreliability and post-hoc interpretability. A key obstacle in this domain is\ndisentangling the meaningful changes in model responses from the intrinsic\nstochasticity of LLM outputs. To overcome this, we introduce Distribution-Based\nPerturbation Analysis (DBPA), a framework that reformulates LLM perturbation\nanalysis as a frequentist hypothesis testing problem. DBPA constructs empirical\nnull and alternative output distributions within a low-dimensional semantic\nsimilarity space via Monte Carlo sampling. Comparisons of Monte Carlo estimates\nin the reduced dimensionality space enables tractable frequentist inference\nwithout relying on restrictive distributional assumptions. The framework is\nmodel-agnostic, supports the evaluation of arbitrary input perturbations on any\nblack-box LLM, yields interpretable p-values, supports multiple perturbation\ntesting via controlled error rates, and provides scalar effect sizes for any\nchosen similarity or distance metric. We demonstrate the effectiveness of DBPA\nin evaluating perturbation impacts, showing its versatility for perturbation\nanalysis.",
    "explanation":"\"We consider the problem of quantifying how an input perturbation impacts the outputs of large language models (LLMs), a fundamental task for model reliability and post-hoc interpretability.\"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b1"
    ],
    "c_title":[
      "How resilient are language models to text perturbations"
    ],
    "c_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10197",
    "c_title":[
      "MathConstruct: Challenging LLM Reasoning with Constructive Proofs"
    ],
    "c_abstract":[
      "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.09022",
    "c_title":[
      "Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key\n  to Model Reasoning"
    ],
    "c_abstract":[
      "Transformer-based language models have achieved significant success; however,\ntheir internal mechanisms remain largely opaque due to the complexity of\nnon-linear interactions and high-dimensional operations. While previous studies\nhave demonstrated that these models implicitly embed reasoning trees, humans\ntypically employ various distinct logical reasoning mechanisms to complete the\nsame task. It is still unclear which multi-step reasoning mechanisms are used\nby language models to solve such tasks. In this paper, we aim to address this\nquestion by investigating the mechanistic interpretability of language models,\nparticularly in the context of multi-step reasoning tasks. Specifically, we\nemploy circuit analysis and self-influence functions to evaluate the changing\nimportance of each token throughout the reasoning process, allowing us to map\nthe reasoning paths adopted by the model. We apply this methodology to the\nGPT-2 model on a prediction task (IOI) and demonstrate that the underlying\ncircuits reveal a human-interpretable reasoning process used by the model."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.13388",
    "c_title":[
      "Reflection of Episodes: Learning to Play Game from Expert and Self\n  Experiences"
    ],
    "c_abstract":[
      "StarCraft II is a complex and dynamic real-time strategy (RTS) game\nenvironment, which is very suitable for artificial intelligence and\nreinforcement learning research. To address the problem of Large Language\nModel(LLM) learning in complex environments through self-reflection, we propose\na Reflection of Episodes(ROE) framework based on expert experience and\nself-experience. This framework first obtains key information in the game\nthrough a keyframe selection method, then makes decisions based on expert\nexperience and self-experience. After a game is completed, it reflects on the\nprevious experience to obtain new self-experience. Finally, in the experiment,\nour method beat the robot under the Very Hard difficulty in TextStarCraft II.\nWe analyze the data of the LLM in the process of the game in detail, verified\nits effectiveness."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.10160",
    "c_title":[
      "CSSDM Ontology to Enable Continuity of Care Data Interoperability"
    ],
    "c_abstract":[
      "The rapid advancement of digital technologies and recent global pandemic\nscenarios have led to a growing focus on how these technologies can enhance\nhealthcare service delivery and workflow to address crises. Action plans that\nconsolidate existing digital transformation programs are being reviewed to\nestablish core infrastructure and foundations for sustainable healthcare\nsolutions. Reforming health and social care to personalize home care, for\nexample, can help avoid treatment in overcrowded acute hospital settings and\nimprove the experiences and outcomes for both healthcare professionals and\nservice users. In this information-intensive domain, addressing the\ninteroperability challenge through standards-based roadmaps is crucial for\nenabling effective connections between health and social care services. This\napproach facilitates safe and trustworthy data workflows between different\nhealthcare system providers. In this paper, we present a methodology for\nextracting, transforming, and loading data through a semi-automated process\nusing a Common Semantic Standardized Data Model (CSSDM) to create personalized\nhealthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology\nof ISO 13940 ContSys and incorporates FHIR-based specifications to support\nstructural attributes for generating KGs. We propose that the CSSDM facilitates\ndata harmonization and linking, offering an alternative approach to\ninteroperability. This approach promotes a novel form of collaboration between\ncompanies developing health information systems and cloud-enabled health\nservices. Consequently, it provides multiple stakeholders with access to\nhigh-quality data and information sharing."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.18784",
    "c_title":[
      "LLM-Generated Heuristics for AI Planning: Do We Even Need\n  Domain-Independence Anymore?"
    ],
    "c_abstract":[
      "Domain-independent heuristics have long been a cornerstone of AI planning,\noffering general solutions applicable across a wide range of tasks without\nrequiring domain-specific engineering. However, the advent of large language\nmodels (LLMs) presents an opportunity to generate heuristics tailored to\nspecific planning problems, potentially challenging the necessity of domain\nindependence as a strict design principle. In this paper, we explore the use of\nLLMs to automatically derive planning heuristics from task descriptions\nrepresented as successor generators and goal tests written in general purpose\nprogramming language. We investigate the trade-offs between domain-specific\nLLM-generated heuristics and traditional domain-independent methods in terms of\ncomputational efficiency and explainability. Our experiments demonstrate that\nLLMs can create heuristics that achieve state-of-the-art performance on some\nstandard IPC domains, as well as their ability to solve problems that lack an\nadequate Planning Domain Definition Language ({\\sc pddl}) representation. We\ndiscuss whether these results signify a paradigm shift and how they can\ncomplement existing approaches."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.05028",
    "c_title":[
      "Near-Optimal Online Learning for Multi-Agent Submodular Coordination:\n  Tight Approximation and Communication Efficiency"
    ],
    "c_abstract":[
      "Coordinating multiple agents to collaboratively maximize submodular functions\nin unpredictable environments is a critical task with numerous applications in\nmachine learning, robot planning and control. The existing approaches, such as\nthe OSG algorithm, are often hindered by their poor approximation guarantees\nand the rigid requirement for a fully connected communication graph. To address\nthese challenges, we firstly present a $\\textbf{MA-OSMA}$ algorithm, which\nemploys the multi-linear extension to transfer the discrete submodular\nmaximization problem into a continuous optimization, thereby allowing us to\nreduce the strict dependence on a complete graph through consensus techniques.\nMoreover, $\\textbf{MA-OSMA}$ leverages a novel surrogate gradient to avoid\nsub-optimal stationary points. To eliminate the computationally intensive\nprojection operations in $\\textbf{MA-OSMA}$, we also introduce a\nprojection-free $\\textbf{MA-OSEA}$ algorithm, which effectively utilizes the KL\ndivergence by mixing a uniform distribution. Theoretically, we confirm that\nboth algorithms achieve a regret bound of\n$\\widetilde{O}(\\sqrt{\\frac{C_{T}T}{1-\\beta}})$ against a\n$(\\frac{1-e^{-c}}{c})$-approximation to the best comparator in hindsight, where\n$C_{T}$ is the deviation of maximizer sequence, $\\beta$ is the spectral gap of\nthe network and $c$ is the joint curvature of submodular objectives. This\nresult significantly improves the $(\\frac{1}{1+c})$-approximation provided by\nthe state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness\nof our proposed algorithms through simulation-based multi-target tracking."
    ],
    "c_categories":[
      "cs.LG",
      "cs.MA",
      "math.OC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.06436",
    "c_title":[
      "Line Operators in the Left-Right Symmetric Model"
    ],
    "c_abstract":[
      "In this paper, we studied line operators in the Left-Right Symmetric Model.\nThe gauge group of Left-Right Symmetric Electroweak Model is ${G} = SU(3)\n\\times SU(2)_{L} \\times SU(2)_{R} \\times U(1)_{B - L}$. We derived the spectrum\nof line operators in all possible scenarios within left-right symmetric models.\nWe then studied the $\\theta$ angles in left-right symmetric model. We also\ndiscuss the effect of symmetry breaking on the spectrum of line operators and\n$\\theta$ angles."
    ],
    "c_categories":[
      "hep-ph",
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10767",
    "c_title":[
      "Wrapping nonspherical vesicles at bio-membranes"
    ],
    "c_abstract":[
      "The wrapping of particles and vesicles by lipid bilayer membranes is a\nfundamental process in cellular transport and targeted drug delivery. Here, we\ninvestigate the wrapping behavior of nonspherical vesicles, such as\nellipsoidal, prolate, oblate, and stomatocytes, by systematically varying the\nbending rigidity of the vesicle membrane and the tension of the planar\nmembrane. Using the Helfrich Hamiltonian, triangulated membrane models, and\nenergy minimization techniques, we predict multiple stable wrapping states and\nidentify the conditions for their coexistence. Our results demonstrate that\nsofter vesicles bind more easily to planar membranes; however, achieving\ncomplete wrapping requires significantly higher adhesion strengths compared to\nrigid particles. As membrane tension increases, deep-wrapped states disappear\nat a triple point where shallow-wrapped, deep-wrapped, and complete-wrapped\nstates coexist. The coordinates of the triple point are highly sensitive to the\nvesicle shape and stiffness. For stomatocytes, increasing stiffness shifts the\ntriple point to higher adhesion strengths and membrane tensions, while for\noblates it shifts to lower values, influenced by shape changes during wrapping.\nOblate shapes are preferred in shallow-wrapped states and stomatocytes in\ndeep-wrapped states. In contrast to hard particles, where optimal adhesion\nstrength for complete wrapping occurs at tensionless membranes, complete\nwrapping of soft vesicles requires finite membrane tension for optimal adhesion\nstrength. These findings provide new insights into the interplay between\nvesicle deformability, shape, and membrane properties, advancing our\nunderstanding of endocytosis and the design of advanced biomimetic delivery\nsystems."
    ],
    "c_categories":[
      "cond-mat.soft"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10233",
    "c_title":[
      "Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via\n  Hierarchical and Parallel Decoding"
    ],
    "c_abstract":[
      "The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge\nin warehouse logistics, where pickers must navigate a mixed-shelves environment\nto retrieve SKUs efficiently. Traditional heuristics and optimization-based\napproaches struggle with scalability, while recent machine learning methods\noften rely on sequential decision-making, leading to high solution latency and\nsuboptimal agent coordination. In this work, we propose a novel hierarchical\nand parallel decoding approach for solving the min-max variant of the MSPRP via\nmulti-agent reinforcement learning. While our approach generates a joint\ndistribution over agent actions, allowing for fast decoding and effective\npicker coordination, our method introduces a sequential action selection to\navoid conflicts in the multi-dimensional action space. Experiments show\nstate-of-the-art performance in both solution quality and inference speed,\nparticularly for large-scale and out-of-distribution instances. Our code is\npublicly available at http:\/\/github.com\/LTluttmann\/marl4msprp."
    ],
    "c_categories":[
      "cs.LG",
      "cs.MA",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
    ],
    "b_abstract":[
      "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.05007",
    "c_title":[
      "Quantum-enhanced causal discovery for a small number of samples"
    ],
    "c_abstract":[
      "The discovery of causal relationships from observed data has attracted\nsignificant interest from disciplines such as economics, social sciences,\nepidemiology, and biology. In practical applications, considerable knowledge of\nthe underlying systems is often unavailable, and real data are often associated\nwith nonlinear causal structures, which make the direct use of most\nconventional causality analysis methods difficult. This study proposes a novel\nquantum Peter-Clark (qPC) algorithm for causal discovery that does not assume\nany underlying model structures. Based on the independence conditional tests in\na class of reproducing kernel Hilbert spaces characterized by quantum circuits,\nthe proposed qPC algorithm can explore causal relationships from the observed\ndata drawn from arbitrary distributions. We conducted systematic experiments on\nfundamental graph parts of causal structures, demonstrating that the qPC\nalgorithm exhibits a significantly better performance, particularly with\nsmaller sample sizes compared to its classical counterpart. Furthermore, we\nproposed a novel optimization approach based on Kernel Target Alignment (KTA)\nfor determining hyperparameters of quantum kernels. This method effectively\nreduced the risk of false positives in causal discovery, enabling more reliable\ninference. Our theoretical and experimental results demonstrate that the\nproposed quantum algorithm can empower classical algorithms for robust and\naccurate inference in causal discovery, supporting them in regimes where\nclassical algorithms typically fail. Additionally, the effectiveness of this\nmethod was validated using the Boston Housing dataset as a real-world\napplication. These findings demonstrate the new potential of quantum\ncircuit-based causal discovery methods in addressing practical challenges,\nparticularly in small-sample scenarios where traditional approaches have shown\nlimitations."
    ],
    "c_categories":[
      "cs.AI",
      "cs.LG",
      "quant-ph",
      "stat.ME"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.00579",
    "c_title":[
      "Intrinsic Random Functions and Parametric Covariance Models of\n  Spatio-Temporal Random Processes on the Sphere"
    ],
    "c_abstract":[
      "Identifying an appropriate covariance function is one of the primary\ninterests in spatial and spatio-temporal statistics because it allows\nresearchers to analyze the dependence structure of the random process. For this\npurpose, spatial homogeneity and temporal stationarity are widely used\nassumptions, and many parametric covariance models have been developed under\nthese assumptions. However, these are strong and unrealistic conditions in many\ncases. In addition, on the sphere, although different statistical approaches\nfrom those on Euclidean space should be applied to build a proper covariance\nmodel considering its unique characteristics, relevant studies are rare. In\nthis research, we introduce novel parameterized models of the covariance\nfunction for spatially non-homogeneous and temporally non-stationary random\nprocesses on the sphere. To alleviate the spatial homogeneity assumption and\ntemporal stationarity, and to consider the spherical domain and time domain\ntogether, this research will apply the theories of Intrinsic Random Functions\n(IRF). We also provide a methodology to estimate the associated parameters for\nthe model. Finally, through a simulation study and analysis of a real-world\ndata set about global temperature anomaly, we demonstrate validity of the\nsuggested covariance model with its advantage of interpretability."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09844",
    "c_title":[
      "Design-based causal inference in bipartite experiments"
    ],
    "c_abstract":[
      "Bipartite experiments are widely used across various fields, yet existing\nmethods often rely on strong assumptions about modeling the potential outcomes\nand exposure mapping. In this paper, we explore design-based causal inference\nin bipartite experiments, where treatments are randomized over one set of\nunits, while outcomes are measured over a separate set of units. We first\nformulate the causal inference problem under a design-based framework that\ngeneralizes the classic assumption to account for bipartite interference. We\nthen propose point and variance estimators for the total treatment effect,\nestablish a central limit theorem for the estimator, and propose a conservative\nvariance estimator. Additionally, we discuss a covariate adjustment strategy to\nenhance estimation efficiency."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07084",
    "c_title":[
      "CLaRe: Compact near-lossless Latent Representations of High-Dimensional\n  Object Data"
    ],
    "c_abstract":[
      "Latent feature representation methods play an important role in the dimension\nreduction and statistical modeling of high-dimensional complex data objects.\nHowever, existing approaches to assess the quality of these methods often rely\non aggregated statistics that reflect the central tendency of the distribution\nof information losses, such as average or total loss, which can mask variation\nacross individual observations. We argue that controlling average performance\nis insufficient to guarantee that statistical analysis in the latent space\nreflects the data-generating process and instead advocate for controlling the\nworst-case generalization error, or a tail quantile of the generalization error\ndistribution. Our framework, CLaRe (Compact near-lossless Latent\nRepresentations), introduces a systematic way to balance compactness of the\nrepresentation with preservation of information when assessing and selecting\namong latent feature representation methods. To facilitate the application of\nthe CLaRe framework, we have developed GLaRe (Graphical Analysis of Latent\nRepresentations), an open-source R package that implements the framework and\nprovides graphical summaries of the full generalization error distribution. We\ndemonstrate the utility of CLaRe through three case studies on high-dimensional\ndatasets from diverse fields of application. We apply the CLaRe framework to\nselect among principal components, wavelets and autoencoder representations for\neach dataset. The case studies reveal that the optimal latent feature\nrepresentation varies depending on dataset characteristics, emphasizing the\nimportance of a flexible evaluation framework."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16874",
    "c_title":[
      "A dynamic copula model for probabilistic forecasting of non-Gaussian\n  multivariate time series"
    ],
    "c_abstract":[
      "Multivariate time series (MTS) data often include a heterogeneous mix of\nnon-Gaussian distributional features (asymmetry, multimodality, heavy tails)\nand data types (continuous and discrete variables). Traditional MTS methods\nbased on convenient parametric distributions are typically ill-equipped to\nmodel this heterogeneity. Copula models provide an appealing alternative, but\npresent significant obstacles for fully Bayesian inference and probabilistic\nforecasting. To overcome these challenges, we propose a novel and general\nstrategy for posterior approximation in MTS copula models and apply it to a\nGaussian copula built from a dynamic factor model. This framework provides\nscalable, fully Bayesian inference for cross-sectional and serial dependencies\nand nonparametrically learns heterogeneous marginal distributions. We validate\nthis approach by establishing posterior consistency and confirm excellent\nfinite-sample performance even under model misspecification using simulated\ndata. We apply our method to crime count and macroeconomic MTS data and find\nsuperior probabilistic forecasting performance compared to popular MTS models.\nThese results demonstrate that the proposed method is a versatile,\ngeneral-purpose utility for probabilistic forecasting of MTS that works well\nacross of range of applications with minimal user input."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15146",
    "c_title":[
      "A note on defining positive definite functions"
    ],
    "c_abstract":[
      "A fundamental requirement in spatial statistics is that covariance functions\nare positive definite. While many positive definite functions are known for\nEuclidean spaces, their positive definiteness may not extend to non-Euclidean\nspaces. We present sufficient conditions to derive valid positive definite\nfunctions for spatial statistics on non-Euclidean geometries. Our approach\nleverages overlooked results due to Schoenberg (1938) to establish conditions\nunder which covariance functions that are valid on Euclidean spaces remain\nvalid on the domain of interest. This approach provides a more accessible and\ndirect framework for spatial statisticians with diverse backgrounds."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13901",
    "c_title":[
      "Is the distribution of resolvable uncertainty Type I extreme value? A\n  Test for Random Coefficient Models using Choice Probabilities"
    ],
    "c_abstract":[
      "Stated choice probabilities are increasingly used in conjunction with the\nrandom-coefficient model (RCM) to describe individual preferences. They allow\nsurvey respondents to express uncertainty about the future or the\nincompleteness of a hypothetical scenario: the resolvable uncertainty.\nParametric assumptions such as a Type I extreme value (EV1) distribution are\nalmost always imposed on this uncertainty to identify and estimate the\nassociated RCM. This paper proposes the first test for these parametric\nassumptions, based on a nonparametric identification result for the population\ndistribution of the interquantile range of the resolvable uncertainty. In all\nfour empirical applications considered, the test finds strong evidence against\nthe EV1 assumption."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00579",
    "c_title":[
      "Abel's Functional Equation and Interrelations"
    ],
    "c_abstract":[
      "Convex solutions $A,B,I,J$ of four Abel equations are numerically studied. We\ndo not know exact formulas for any of these functions, but conjecture that\n$A,B$ and $I,J$ are closely related. [Corrigendum at end.]"
    ],
    "c_categories":[
      "cs.DM",
      "math.CA",
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10957",
    "c_title":[
      "Skillful Nowcasting of Convective Clouds With a Cascade Diffusion Model"
    ],
    "c_abstract":[
      "Accurate nowcasting of convective clouds from satellite imagery is essential\nfor mitigating the impacts of meteorological disasters, especially in\ndeveloping countries and remote regions with limited ground-based observations.\nRecent advances in deep learning have shown promise in video prediction;\nhowever, existing models frequently produce blurry results and exhibit reduced\naccuracy when forecasting physical fields. Here, we introduce SATcast, a\ndiffusion model that leverages a cascade architecture and multimodal inputs for\nnowcasting cloud fields in satellite imagery. SATcast incorporates physical\nfields predicted by FuXi, a deep-learning weather model, alongside past\nsatellite observations as conditional inputs to generate high-quality future\ncloud fields. Through comprehensive evaluation, SATcast outperforms\nconventional methods on multiple metrics, demonstrating its superior accuracy\nand robustness. Ablation studies underscore the importance of its multimodal\ndesign and the cascade architecture in achieving reliable predictions. Notably,\nSATcast maintains predictive skill for up to 24 hours, underscoring its\npotential for operational nowcasting applications."
    ],
    "c_categories":[
      "cs.CV",
      "physics.ao-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02303",
    "c_title":[
      "Iterative Refinement and Flexible Iteratively Reweighed Solvers for\n  Linear Inverse Problems with Sparse Solutions"
    ],
    "c_abstract":[
      "This paper presents a new algorithmic framework for computing sparse\nsolutions to large-scale linear discrete ill-posed problems. The approach is\nmotivated by recent perspectives on iteratively reweighted norm schemes, viewed\nthrough the lens of iterative refinement. This framework leverages the\nefficiency and fast convergence of flexible Krylov methods while achieving\nhigher accuracy through suitable restarts. Additionally, we demonstrate that\nthe proposed methods outperform other flexible Krylov approaches in\nmemory-limited scenarios. Relevant convergence theory is discussed, and the\nperformance of the proposed algorithms is illustrated through a range of\nnumerical examples, including image deblurring and computed tomography."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00868",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "How resilient are language models to text perturbations"
    ],
    "b_abstract":[
      "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17859",
    "c_title":[
      "Hybrid stars with large quark cores within the parity doublet model and\n  modified NJL model"
    ],
    "c_abstract":[
      "Using the parity doublet model (PDM) for hadronic matter and a modified\nNambu-Jona-Lasinio (NJL) model for quark matter, we investigate the potential\nexistence of two- and three-flavor quark matter in neutron star cores. Both\nmodels respect chiral symmetry, and a sharp first-order phase transition is\nimplemented via Maxwell construction. We find stable neutron stars with quark\ncores within a specific parameter space that satisfies current astronomical\nobservations. Typical neutron stars with masses around $1.4 \\ M_\\odot$ may\npossess deconfined quark matter in their centers. The hybrid star scenario with\na two-flavor quark core offers enough parameter space to allow the neutron\nstars with large quark cores exceeding $\\sim 1\\ M_\\odot$, and allow the early\ndeconfinement position before $2\\ \\rho_0$, where $\\rho_0$ is the nuclear\nsaturation density. The observations of gravitational wave event GW170817\nsuggest a relatively large chiral invariant mass $m_0=600\\ \\rm MeV$ in the PDM\nfor scenarios involving three-flavor quark matter cores. The maximum mass of\nthe hybrid star with a quark core is found to be approximately $2.2\\ M_\\odot$\nfor both two- or three-flavor quark matter in their centers."
    ],
    "c_categories":[
      "astro-ph.HE",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"Linear Quadratic Mean Field Games with Quantile-Dependent Cost\n  Coefficients",
    "a_abstract":"This paper studies a class of linear quadratic mean field games where the\ncoefficients of quadratic cost functions depend on both the mean and the\nvariance of the population's state distribution through its quantile function.\nSuch a formulation allows for modelling agents that are sensitive to not only\nthe population average but also the population variance. The corresponding mean\nfield game equilibrium is identified, which involves solving two coupled\ndifferential equations: one is a Riccati equation and the other the variance\nevolution equation. Furthermore, the conditions for the existence and\nuniqueness of the mean field equilibrium are established. Finally, numerical\nresults are presented to illustrate the behavior of two coupled differential\nequations and the performance of the mean field game solution.",
    "explanation":"\"This paper studies a class of linear quadratic mean field games where the coefficients of quadratic cost functions depend on both the mean and the variance of the population\u2019s state distribution through its quantile function. \"\n\n\n",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":[
      "b3"
    ],
    "c_title":[
      "Linear-quadratic mean field games"
    ],
    "c_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":true,
    "research_type":"basic"
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.12533",
    "c_title":[
      "Multi-objective and hierarchical control for coupled stochastic\n  parabolic systems"
    ],
    "c_abstract":[
      "We study the Stackelberg-Nash null controllability of a coupled system\ngoverned by two linear forward stochastic parabolic equations. The system\nincludes one leader control localized in a subset of the domain, two additional\nleader controls in the diffusion terms, and \\( m \\) follower controls, where \\(\nm \\geq 2 \\). We consider two different scenarios for the followers: first, when\nthe followers minimize a functional involving both components of the system's\nstate, and second, when they minimize a functional involving only the second\ncomponent of the state. For fixed leader controls, we first establish the\nexistence and uniqueness of the Nash equilibrium in both scenarios and provide\nits characterization. As a byproduct, the problem is reformulated as a\nclassical null controllability issue for the associated coupled\nforward-backward stochastic parabolic system. To address this, we derive new\nCarleman estimates for the adjoint stochastic systems. As far as we know, this\nproblem is among the first to be discussed for stochastic coupled systems."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.05430",
    "c_title":[
      "A dimension reduction procedure for the design of lattice-spring systems\n  with minimal fabrication cost and required multi-functional properties"
    ],
    "c_abstract":[
      "We show that the problem of the design of the lattices of elastoplastic\ncurrent conducting springs with optimal multi-functional properties leads to an\nanalytically tractable problem. Specifically, focusing on a lattice with a\nsmall number of springs, we use the technique of inequalities to reduce the\nnumber variables and to compute the minimal cost of lattice fabrication\nexplicitly."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.03169",
    "c_title":[
      "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$"
    ],
    "c_abstract":[
      "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.03390",
    "c_title":[
      "State-of-the-art Methods for Pseudo-Boolean Solving with SCIP"
    ],
    "c_abstract":[
      "The Pseudo-Boolean problem deals with linear or polynomial constraints with\ninteger coefficients over Boolean variables. The objective lies in optimizing a\nlinear objective function, or finding a feasible solution, or finding a\nsolution that satisfies as many constraints as possible. In the 2024\nPseudo-Boolean competition, solvers incorporating the SCIP framework won five\nout of six categories it was competing in. From a total of 1,207 instances,\nSCIP successfully solved 759, while its parallel version FiberSCIP solved 776.\nBased on the results from the competition, we further enhanced SCIP's\nPseudo-Boolean capabilities. This article discusses the results and presents\nthe winning algorithmic ideas."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.10193",
    "c_title":[
      "Theorems of nonlinear separation of co-radiant sets and optimality\n  conditions for approximate and proper approximate solutions of vector\n  optimization problems"
    ],
    "c_abstract":[
      "This paper deals with \\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper\nefficient points with respect to a co-radiant set in a vector optimization\nproblem. In the first part of the paper, we establish a new nonlinear\nseparation theorem for co-radiant sets in normed spaces. Subsequently, we\nobtain necessary and sufficient conditions by means of scalarization for both\n\\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper efficient points in a general\nframework, without any requirements on the co-radiant set or any convexity\nassumption on the sets under consideration.Consequently, our results have a\nwider range of applicability than previously stated in the literature."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.11511",
    "c_title":[
      "Alzheimer's Disease Classification Using Retinal OCT: TransnetOCT and\n  Swin Transformer Models"
    ],
    "c_abstract":[
      "Retinal optical coherence tomography (OCT) images are the biomarkers for\nneurodegenerative diseases, which are rising in prevalence. Early detection of\nAlzheimer's disease using retinal OCT is a primary challenging task. This work\nutilizes advanced deep learning techniques to classify retinal OCT images of\nsubjects with Alzheimer's disease (AD) and healthy controls (CO). The goal is\nto enhance diagnostic capabilities through efficient image analysis. In the\nproposed model, Raw OCT images have been preprocessed with ImageJ and given to\nvarious deep-learning models to evaluate the accuracy. The best classification\narchitecture is TransNetOCT, which has an average accuracy of 98.18% for input\nOCT images and 98.91% for segmented OCT images for five-fold cross-validation\ncompared to other models, and the Swin Transformer model has achieved an\naccuracy of 93.54%. The evaluation accuracy metric demonstrated TransNetOCT and\nSwin transformer models capability to classify AD and CO subjects reliably,\ncontributing to the potential for improved diagnostic processes in clinical\nsettings."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.04391",
    "c_title":[
      "Comparison of bulk properties of wet granular materials using different\n  capillary force approximations"
    ],
    "c_abstract":[
      "We perform Discrete Element Method simulations of wet granular matter in a\nsplit-bottom shear cell. To calculate the capillary forces from the liquid\nbridges between the grains, we used three different approximations. The\nsimulations of the shear cell showed a linear increase in bulk cohesion with\nthe surface tension of the liquid, consistently for all approximations.\nHowever, the macroscopic friction coefficient shows only a weak dependence on\nsurface tension."
    ],
    "c_categories":[
      "cond-mat.soft"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.12145",
    "c_title":[
      "Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented\n  Generation with Flexible User Control"
    ],
    "c_abstract":[
      "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications."
    ],
    "c_categories":[
      "cs.AI",
      "cs.IR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.09046",
    "c_title":[
      "Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate\n  Multi-Criteria Recommendation"
    ],
    "c_abstract":[
      "Multi-criteria (MC) recommender systems, which utilize MC rating information\nfor recommendation, are increasingly widespread in various e-commerce domains.\nHowever, the MC recommendation using training-based collaborative filtering,\nrequiring consideration of multiple ratings compared to single-criterion\ncounterparts, often poses practical challenges in achieving state-of-the-art\nperformance along with scalable model training. To solve this problem, we\npropose CA-GF, a training-free MC recommendation method, which is built upon\ncriteria-aware graph filtering for efficient yet accurate MC recommendations.\nSpecifically, first, we construct an item-item similarity graph using an MC\nuser-expansion graph. Next, we design CA-GF composed of the following key\ncomponents, including 1) criterion-specific graph filtering where the optimal\nfilter for each criterion is found using various types of polynomial low-pass\nfilters and 2) criteria preference-infused aggregation where the smoothed\nsignals from each criterion are aggregated. We demonstrate that CA-GF is (a)\nefficient: providing the computational efficiency, offering the extremely fast\nruntime of less than 0.2 seconds even on the largest benchmark dataset, (b)\naccurate: outperforming benchmark MC recommendation methods, achieving\nsubstantial accuracy gains up to 24% compared to the best competitor, and (c)\ninterpretable: providing interpretations for the contribution of each criterion\nto the model prediction based on visualizations."
    ],
    "c_categories":[
      "cs.AI",
      "cs.IR",
      "cs.IT",
      "cs.LG",
      "cs.SI",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Mean\u2010field games with differing beliefs for algorithmic trading"
    ],
    "b_abstract":[
      "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
    ],
    "b_categories":[
      "q-fin.MF"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.14817",
    "c_title":[
      "A Cutting Mechanics-based Machine Learning Modeling Method to Discover\n  Governing Equations of Machining Dynamics"
    ],
    "c_abstract":[
      "This paper proposes a cutting mechanics-based machine learning (CMML)\nmodeling method to discover governing equations of machining dynamics. The main\nidea of CMML design is to integrate existing physics in cutting mechanics and\nunknown physics in data to achieve automated model discovery, with the\npotential to advance machining modeling. Based on existing physics in cutting\nmechanics, CMML first establishes a general modeling structure governing\nmachining dynamics, that is represented by a set of unknown differential\nalgebraic equations. CMML can therefore achieve data-driven discovery of these\nunknown equations through effective cutting mechanics-based nonlinear learning\nfunction space design and discrete optimization-based learning algorithm.\nExperimentally verified time domain simulation of milling is used to validate\nthe proposed modeling method. Numerical results show CMML can discover the\nexact milling dynamics models with process damping and edge force from noisy\ndata. This indicates that CMML has the potential to be used for advancing\nmachining modeling in practice with the development of effective metrology\nsystems."
    ],
    "c_categories":[
      "cs.CE",
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.01040",
    "c_title":[
      "Pricing time-capped American options using Least Squares Monte Carlo\n  method"
    ],
    "c_abstract":[
      "In this paper, we adopt the least squares Monte Carlo (LSMC) method to price\ntime-capped American options. The aforementioned cap can be an independent\nrandom variable or dependent on asset price at random time. We allow various\ntime caps. In particular, we give an algorithm for pricing the American options\ncapped by the first drawdown epoch. We focus on the geometric L\\'evy market. We\nprove that our estimator converges to the true price as one takes the\ndiscretisation step tending to zero and the number of trajectories going to\ninfinity."
    ],
    "c_categories":[
      "q-fin.MF"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.13742",
    "c_title":[
      "Decentralized Annuity: A Quest for the Holy Grail of Lifetime Financial\n  Security"
    ],
    "c_abstract":[
      "This paper presents a novel framework for decentralized annuities, aiming to\naddress the limitations of traditional pension systems such as defined\ncontribution (DC) and defined benefit (DB) plans, while providing lifetime\nfinancial support. It sheds light on often ignored pitfalls within current\nretirement schemes and introduces individual rationality properties. The\nresearch delves into various fairness concepts that underpin existing plans,\nemphasizing that decentralized annuities, while meeting similar fairness\ncriteria, offer enhanced flexibility for individual rationality and improved\nsocial welfare for all participants. Using theoretical models and examples, we\ndemonstrate the potential of decentralized annuities to outperform self-managed\nplans (DC) and to produce effects comparable to defined benefit (DB) plans,\nparticularly within larger participant pools. The paper concludes by exploring\nthe managerial implications of decentralized annuities and laying the\ngroundwork for the further advancement of equitable and sustainable\ndecentralized annuity systems."
    ],
    "c_categories":[
      "q-fin.MF"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.14158",
    "c_title":[
      "Capturing Smile Dynamics with the Quintic Volatility Model: SPX,\n  Skew-Stickiness Ratio and VIX"
    ],
    "c_abstract":[
      "We introduce the two-factor Quintic Ornstein-Uhlenbeck model, where\nvolatility is modeled as a polynomial of degree five based on the sum of two\nOrnstein-Uhlenbeck processes driven by the same Brownian Motion, each\nmean-reverting at a different speed. We demonstrate that the Quintic model\neffectively captures the volatility surfaces of SPX and VIX while aligning with\nthe skew-stickiness ratio (SSR) across maturities ranging from a few days to\nover two years. Furthermore, the Quintic model shows consistency with key\nempirical stylized facts, notably reproducing the Zumbach effect."
    ],
    "c_categories":[
      "q-fin.MF"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.12195",
    "c_title":[
      "An Optimal Transport approach to arbitrage correction: Application to\n  volatility Stress-Tests"
    ],
    "c_abstract":[
      "We present a method based on optimal transport to remove arbitrage\nopportunities within a finite set of option prices. The method is notably\nintended for regulatory stress-tests, which impose to apply important local\ndistortions to implied volatility surfaces. The resulting stressed option\nprices are naturally associated to a family of signed marginal measures: we\nformulate the process of removing arbitrage as a projection onto the subset of\nmartingale measures with respect to a Wasserstein metric in the space of signed\nmeasures. We show how this projection problem can be recast as an optimal\ntransport problem; in view of the numerical solution, we apply an entropic\nregularization technique. For the regularized problem, we derive a strong\nduality formula, show convergence results as the regularization parameter\napproaches zero, and formulate a multi-constrained Sinkhorn algorithm, where\neach iteration involves, at worse, finding the root of an explicit scalar\nfunction. The convergence of this algorithm is also established. We compare our\nmethod with the existing approach by [Cohen, Reisinger and Wang, Appl.\\ Math.\\\nFin.\\ 2020] across various scenarios and test cases."
    ],
    "c_categories":[
      "q-fin.MF"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.01748",
    "c_title":[
      "On consistency of optimal portfolio choice for state-dependent\n  exponential utilities"
    ],
    "c_abstract":[
      "In an arbitrage-free simple market, we demonstrate that for a class of\nstate-dependent exponential utilities, there exists a unique prediction of the\nrandom risk aversion that ensures the consistency of optimal strategies across\nany time horizon. Our solution aligns with the theory of forward performances,\nwith the added distinction of identifying, among the infinite possible\nsolutions, the one for which the profile remains optimal at all times for the\nmarket-adjusted system of preferences adopted."
    ],
    "c_categories":[
      "q-fin.MF"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.15192",
    "c_title":[
      "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
    ],
    "c_abstract":[
      "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
    ],
    "c_categories":[
      "cs.DC",
      "cs.ET"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.14560",
    "c_title":[
      "Structural, electronic, vibrational, optical, piezoelectric, thermal and\n  thermoelectric properties of double perovskite BCZT from first-principles\n  calculations"
    ],
    "c_abstract":[
      "Double perovskite material such as BCZT\n(Ba$_{0.875}$Ca$_{0.125}$(Zr$_{0.125}$Ti$_{0.875}$)O$_{3}$) is well known for\nits high value of piezoceramic properties and Curie temperature which has\npotential applications in sensors, actuators, optoelectronic and thermoelectric\ndevices. Based on its composition and physical parameters such as pressure and\ntemperature, experimentally, BCZT shows different crystal structures\n(rhombohedral, tetragonal and orthorhombic) with multiferroic properties. Here,\nwe have designed these materials by comparing experimental stoichiometry and\nevaluated their stability by calculating the tolerance factor, formation\nenergy, and cohesive energy. The structural, electronic and vibrational\nproperties of BCZT are explored using generalized gradient approximation (GGA)\nwithin the framework of density functional theory. We have also shown variation\nof piezoelectric and optical properties through multiple phases using time\ndependent density functional theory. The electronic band gap, optical response\nin the visible light range, as well as piezoelectric, electrical, thermal, and\nthermoelectric properties, demonstrate excellent characteristics, making this\nmaterial a promising lead-free ferroelectric candidate for various energy\nharvesting applications. Boltzmann transport theory is used for the calculation\nof Seebeck coefficient, electron thermal conductivity, and electrical\nconductivity to estimate the power factor and figure of merit which represents\nthe efficiency of the material. The high values at the Fermi level suggest that\nthese materials are well-suited for future device applications."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci",
      "cond-mat.other"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.18098",
    "c_title":[
      "Disentangling Safe and Unsafe Corruptions via Anisotropy and Locality"
    ],
    "c_abstract":[
      "State-of-the-art machine learning systems are vulnerable to small\nperturbations to their input, where ``small'' is defined according to a threat\nmodel that assigns a positive threat to each perturbation. Most prior works\ndefine a task-agnostic, isotropic, and global threat, like the $\\ell_p$ norm,\nwhere the magnitude of the perturbation fully determines the degree of the\nthreat and neither the direction of the attack nor its position in space\nmatter. However, common corruptions in computer vision, such as blur,\ncompression, or occlusions, are not well captured by such threat models. This\npaper proposes a novel threat model called \\texttt{Projected Displacement} (PD)\nto study robustness beyond existing isotropic and global threat models. The\nproposed threat model measures the threat of a perturbation via its alignment\nwith \\textit{unsafe directions}, defined as directions in the input space along\nwhich a perturbation of sufficient magnitude changes the ground truth class\nlabel. Unsafe directions are identified locally for each input based on\nobserved training data. In this way, the PD threat model exhibits anisotropy\nand locality. Experiments on Imagenet-1k data indicate that, for any input, the\nset of perturbations with small PD threat includes \\textit{safe} perturbations\nof large $\\ell_p$ norm that preserve the true label, such as noise, blur and\ncompression, while simultaneously excluding \\textit{unsafe} perturbations that\nalter the true label. Unlike perceptual threat models based on embeddings of\nlarge-vision models, the PD threat model can be readily computed for arbitrary\nclassification tasks without pre-training or finetuning. Further additional\ntask annotation such as sensitivity to image regions or concept hierarchies can\nbe easily integrated into the assessment of threat and thus the PD threat model\npresents practitioners with a flexible, task-driven threat specification."
    ],
    "c_categories":[
      "cs.CV",
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.16557",
    "c_title":[
      "CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction\n  through Generative Artificial Intelligence"
    ],
    "c_abstract":[
      "Context-aware AR instruction enables adaptive and in-situ learning\nexperiences. However, hardware limitations and expertise requirements constrain\nthe creation of such instructions. With recent developments in Generative\nArtificial Intelligence (Gen-AI), current research tries to tackle these\nconstraints by deploying AI-generated content (AIGC) in AR applications.\nHowever, our preliminary study with six AR practitioners revealed that the\ncurrent AIGC lacks contextual information to adapt to varying application\nscenarios and is therefore limited in authoring. To utilize the strong\ngenerative power of GenAI to ease the authoring of AR instruction while\ncapturing the context, we developed CARING-AI, an AR system to author\ncontext-aware humanoid-avatar-based instructions with GenAI. By navigating in\nthe environment, users naturally provide contextual information to generate\nhumanoid-avatar animation as AR instructions that blend in the context\nspatially and temporally. We showcased three application scenarios of\nCARING-AI: Asynchronous Instructions, Remote Instructions, and Ad Hoc\nInstructions based on a design space of AIGC in AR Instructions. With two user\nstudies (N=12), we assessed the system usability of CARING-AI and demonstrated\nthe easiness and effectiveness of authoring with Gen-AI."
    ],
    "c_categories":[
      "cs.HC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01668",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Linear-quadratic mean field games"
    ],
    "b_abstract":[
      "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.17347",
    "c_title":[
      "Dereflection Any Image with Diffusion Priors and Diversified Data"
    ],
    "c_abstract":[
      "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"A Semi-Discrete Optimal Transport Scheme for the 3D Incompressible\n  Semi-Geostrophic Equations",
    "a_abstract":"We describe a mesh-free three-dimensional numerical scheme for solving the\nincompressible semi-geostrophic equations based on semi-discrete optimal\ntransport techniques. These results generalise previous two-dimensional\nimplementations. The optimal transport methods we adopt are known for their\nstructural preservation and energy conservation qualities and achieve an\nexcellent level of efficiency and numerical energy-conservation. We use this\nscheme to generate numerical simulations of an important cyclone benchmark\nproblem. To our knowledge, this is the first fully three-dimensional simulation\nof the semi-geostrophic equations, evidencing semi-discrete optimal transport\nas a novel, robust numerical tool for meteorological and oceanographic\nmodelling.",
    "explanation":"We describe a mesh-free three-dimensional numerical scheme for solving the in-\ncompressible semi-geostrophic equations based on semi-discrete optimal transport techniques.\nThese results generalise previous two-dimensional implementations.",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b22"
    ],
    "c_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "c_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "c_categories":[
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":true,
    "research_type":"basic"
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.13569",
    "c_title":[
      "Reverse Faber-Krahn inequalities for the Logarithmic potential operator"
    ],
    "c_abstract":[
      "For a bounded open set $\\Omega \\subset \\mathbb{R}^2,$ we consider the largest\neigenvalue $\\tau_1(\\Omega)$ of the Logarithmic potential operator\n$\\mathcal{L}$. If $diam(\\Omega)\\le 1$, we prove reverse Faber-Krahn type\ninequalities for $\\tau_1(\\Omega)$ under polarization and Schwarz\nsymmetrization. Further, we establish the monotonicity of\n$\\tau_1(\\Omega\\setminus\\mathcal{O})$ with respect to certain translations and\nrotations of the obstacle $\\mathcal{O}$ within $\\Omega$. The analogous results\nare also stated for the largest eigenvalue of the Riesz potential operator.\nFurthermore, we investigate properties of the smallest eigenvalue\n$\\tilde{\\tau}_1(\\Omega)$ for a domain whose transfinite diameter is greater\nthan 1. Finally, we characterize the eigenvalues of $\\mathcal{L}$ on $B_R$,\nincluding the $\\tilde{\\tau}_1(B_R)$ when $R>1$."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03866",
    "c_title":[
      "Disjointly non-singular operators and various topologies on Banach\n  lattices"
    ],
    "c_abstract":[
      "We continue the study of dispersed subspaces and disjointly non-singular\n(DNS) operators on Banach lattices using topological methods. In particular, we\nprovide a simple proof of the fact that in an order continuous Banach lattice\nan operator is DNS if and only if it is $n$-DNS, for some $n\\in\\mathbb{N}$. We\ncharacterize Banach lattices with order continuous dual in terms of dispersed\nsubspaces and absolute weak topology. We also connect these topics with the\nrecently launched study of phase retrieval in Banach lattices."
    ],
    "c_categories":[
      "math.FA",
      "math.GN"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.01083",
    "c_title":[
      "Orlov's theorem over a quasiexcellent ring"
    ],
    "c_abstract":[
      "Following the approach of Kawamata and Canonaco-Stellari, we establish\nOrlov's representability theorem for smooth tame Deligne-Mumford stacks with\nprojective coarse moduli spaces over a quasiexcellent ring of finite Krull\ndimension. This generalizes a previous result of Canonaco-Stellari for smooth\nprojective varieties over a field."
    ],
    "c_categories":[
      "math.AG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.19461",
    "c_title":[
      "Eigenvalue conditions implying edge-disjoint spanning trees and a forest\n  with constraints"
    ],
    "c_abstract":[
      "Let $G$ be a nontrivial graph with minimum degree $\\delta$ and $k$ an integer\nwith $k\\ge 2$. In the literature, there are eigenvalue conditions that imply\n$G$ contains $k$ edge-disjoint spanning trees. We give eigenvalue conditions\nthat imply $G$ contains $k$ edge-disjoint spanning trees and another forest $F$\nwith $|E(F)|>\\frac{\\delta-1}{\\delta}(|V(G)|-1)$, and if $F$ is not a spanning\ntree, then $F$ has a component with at least $\\delta$ edges."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.12319",
    "c_title":[
      "Cluster algebras and skein algebras for surfaces"
    ],
    "c_abstract":[
      "We consider two algebras of curves associated to an oriented surface of\nfinite type - the cluster algebra from combinatorial algebra, and the skein\nalgebra from quantum topology. We focus on generalizations of cluster algebras\nand generalizations of skein algebras that include arcs whose endpoints are\nmarked points on the boundary or in the interior of the surface. We show that\nthe generalizations are closely related by maps that can be explicitly defined,\nand we explore the structural implications, including (non-)finite generation.\nWe also discuss open questions about the algebraic structure of the algebras."
    ],
    "c_categories":[
      "math.AC",
      "math.AG",
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.11471",
    "c_title":[
      "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography"
    ],
    "c_abstract":[
      "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra."
    ],
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.06621",
    "c_title":[
      "Generalized Optimal AMG Convergence Theory for Stokes Equations Using\n  Smooth Aggregation and Vanka Relaxation Strategies"
    ],
    "c_abstract":[
      "This paper discusses our recent generalized optimal algebraic multigrid (AMG)\nconvergence theory applied to the steady-state Stokes equations discretized\nusing Taylor-Hood elements ($\\pmb{ \\mathbb{P}}_2\/\\mathbb{P}_{1}$). The\ngeneralized theory is founded on matrix-induced orthogonality of the left and\nright eigenvectors of a generalized eigenvalue problem involving the system\nmatrix and relaxation operator. This framework establishes a rigorous lower\nbound on the spectral radius of the two-grid error-propagation operator,\nenabling precise predictions of the convergence rate for symmetric indefinite\nproblems, such as those arising from saddle-point systems. We apply this theory\nto the recently developed monolithic smooth aggregation AMG (SA-AMG) solver for\nStokes, constructed using evolution-based strength of connection, standard\naggregation, and smoothed prolongation. The performance of these solvers is\nevaluated using additive and multiplicative Vanka relaxation strategies.\nAdditive Vanka relaxation constructs patches algebraically on each level,\nresulting in a nonsymmetric relaxation operator due to the partition of unity\nbeing applied on one side of the block-diagonal matrix. Although symmetry can\nbe restored by eliminating the partition of unity, this compromises\nconvergence. Alternatively, multiplicative Vanka relaxation updates velocity\nand pressure sequentially within each patch, propagating updates\nmultiplicatively across the domain and effectively addressing velocity-pressure\ncoupling, ensuring a symmetric relaxation. We demonstrate that the generalized\noptimal AMG theory consistently provides accurate lower bounds on the\nconvergence rate for SA-AMG applied to Stokes equations. These findings suggest\npotential avenues for further enhancement in AMG solver design for saddle-point\nsystems."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.10983",
    "c_title":[
      "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit"
    ],
    "c_abstract":[
      "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5."
    ],
    "c_categories":[
      "cs.AR",
      "cs.CR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.16767",
    "c_title":[
      "Production, Characteristics and Biological effects of Protonated Small\n  Water Clusters"
    ],
    "c_abstract":[
      "The production and characteristics of protonated small water clusters (PSWCs)\nwere reported in this work, where in electrospray ionization (ESI) of pure\nwater, the species obtained were singly charged molecular ions consisting of 2,\n3, 4 or 5 water molecules attached to a hydrogen ion, [(H2O)n+H]+, where n = 2,\n3, 4 or 5. We proposed a new type of PSWCs structure: 2, 3, 4, 5 water\nmolecules wrapped around a hydrogen ion which is located at the electrical and\ngeometric center, forming a very stable molecular structure. Furthermore,\nbiological tests of the PSWCs on mitochondrial function of intestinal\nepithelial cells and liver cells in mice showed the better therapeutic effect\non inflammatory bowel diseases compared to that of the biologic agent\nInfliximab."
    ],
    "c_categories":[
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
    ],
    "b_abstract":[
      "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10868",
    "c_title":[
      "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai\n  Legal Question Answering"
    ],
    "c_abstract":[
      "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.12533",
    "c_title":[
      "Multi-objective and hierarchical control for coupled stochastic\n  parabolic systems"
    ],
    "c_abstract":[
      "We study the Stackelberg-Nash null controllability of a coupled system\ngoverned by two linear forward stochastic parabolic equations. The system\nincludes one leader control localized in a subset of the domain, two additional\nleader controls in the diffusion terms, and \\( m \\) follower controls, where \\(\nm \\geq 2 \\). We consider two different scenarios for the followers: first, when\nthe followers minimize a functional involving both components of the system's\nstate, and second, when they minimize a functional involving only the second\ncomponent of the state. For fixed leader controls, we first establish the\nexistence and uniqueness of the Nash equilibrium in both scenarios and provide\nits characterization. As a byproduct, the problem is reformulated as a\nclassical null controllability issue for the associated coupled\nforward-backward stochastic parabolic system. To address this, we derive new\nCarleman estimates for the adjoint stochastic systems. As far as we know, this\nproblem is among the first to be discussed for stochastic coupled systems."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.05430",
    "c_title":[
      "A dimension reduction procedure for the design of lattice-spring systems\n  with minimal fabrication cost and required multi-functional properties"
    ],
    "c_abstract":[
      "We show that the problem of the design of the lattices of elastoplastic\ncurrent conducting springs with optimal multi-functional properties leads to an\nanalytically tractable problem. Specifically, focusing on a lattice with a\nsmall number of springs, we use the technique of inequalities to reduce the\nnumber variables and to compute the minimal cost of lattice fabrication\nexplicitly."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.03169",
    "c_title":[
      "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$"
    ],
    "c_abstract":[
      "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03390",
    "c_title":[
      "State-of-the-art Methods for Pseudo-Boolean Solving with SCIP"
    ],
    "c_abstract":[
      "The Pseudo-Boolean problem deals with linear or polynomial constraints with\ninteger coefficients over Boolean variables. The objective lies in optimizing a\nlinear objective function, or finding a feasible solution, or finding a\nsolution that satisfies as many constraints as possible. In the 2024\nPseudo-Boolean competition, solvers incorporating the SCIP framework won five\nout of six categories it was competing in. From a total of 1,207 instances,\nSCIP successfully solved 759, while its parallel version FiberSCIP solved 776.\nBased on the results from the competition, we further enhanced SCIP's\nPseudo-Boolean capabilities. This article discusses the results and presents\nthe winning algorithmic ideas."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.10193",
    "c_title":[
      "Theorems of nonlinear separation of co-radiant sets and optimality\n  conditions for approximate and proper approximate solutions of vector\n  optimization problems"
    ],
    "c_abstract":[
      "This paper deals with \\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper\nefficient points with respect to a co-radiant set in a vector optimization\nproblem. In the first part of the paper, we establish a new nonlinear\nseparation theorem for co-radiant sets in normed spaces. Subsequently, we\nobtain necessary and sufficient conditions by means of scalarization for both\n\\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper efficient points in a general\nframework, without any requirements on the co-radiant set or any convexity\nassumption on the sets under consideration.Consequently, our results have a\nwider range of applicability than previously stated in the literature."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.11134",
    "c_title":[
      "Analysis of bound and resonant states of doubly heavy tetraquarks with\n  spin $J\\leq 2$"
    ],
    "c_abstract":[
      "Recently, a number of exotic hadrons have been reported in the experiments,\nand most of these states lie slightly below the threshold. Therefore, these\nstates are considered to be hadronic molecules composed of mesons or baryons.\nIn 2022, the doubly charmed tetraquark $T_{cc}$ was reported by the LHCb\nexperiment, which is considered to be composed of two heavy quarks and two\nlight antiquarks, and located slightly below the $DD^\\ast$ threshold. The\nobservation motivates us to study the bound and resonant states of the doubly\nheavy tetraquarks as two meson systems. We employ the one boson exchange\npotential as an interaction between two mesons, where one free parameter has\nbeen determined to reproduce $T_{cc}$ reported by the LHCb experiment. In our\nprevious work, bound states of $D^{(\\ast)}D^{(\\ast)}$ and\n$B^{(\\ast)}B^{(\\ast)}$ molecules were studied, where we respected the heavy\nquark spin symmetry (HQS). Using the same framework, we study not only bound\nbut also resonant states with $J\\leq 2$ in this paper. Furthermore, we discuss\nthe HQS partner structures of $T_{cc}$ and $T_{bb}$ obtained in our study by\nintroducing the light cloud basis."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.16886",
    "c_title":[
      "Insight-HXMT observations of the 2023 outburst in Aql X-1"
    ],
    "c_abstract":[
      "We conducted an analysis of the continuum during the onset and initial\ndecline phases of the 2023 outburst in transient neutron star low-mass X-ray\nbinary Aql X$-$1 using broadband observations from the \\textit{Insight-Hard\nX-ray Modulation Telescope (Insight-HXMT)} instrument. To determine the most\nappropriate model for the continuum of this outburst, we employed three models\nto explore the evolution of the spectral component. These observations revealed\nthat the source transitions from the hard state to the soft state. The\ndisk-corona and sphere-corona models both adequately described the spectra of\nthe hard state, while the double blackbody model became preferable after the\nhard X-ray emission ($>$25 keV) disappeared during the state transition. In the\nsoft state, the total emission is dominated by changes in the disk and other\nblackbody components. The combination of the sphere-corona model and the double\nblackbody model is the most suitable model for this outburst. The results\nsuggest that as the source transitioned into the soft state, the emission from\nthe boundary layer was enhanced, and a hot spot occurred. Notably, we\nidentified two type-I X-ray bursts, one of which exhibited a significant hard\nX-ray deficit (significance $\\sim$ 4.82 $\\sigma$), which indicates that\n\\textit{Insight-HXMT} has the capability to capture the evolution of the corona\nin a single burst."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.20257",
    "c_title":[
      "Symmetry defects and gauging for quantum states with matrix product\n  unitary symmetries"
    ],
    "c_abstract":[
      "In this work, we examine the consequences of the existence of a finite group\nof matrix product unitary (MPU) symmetries for matrix product states (MPS). We\ngeneralize the well-understood picture of onsite unitary symmetries, which give\nrise to virtual symmetry defects given by insertions of operators in the bonds\nof the MPS. In the MPU case, we can define analogous defect tensors, this time\nsitting on lattice sites, that can be created, moved, and fused by local\nunitary operators. We leverage this formalism to study the gauging of MPU\nsymmetries. We introduce a condition, block independence, under which we can\ngauge the symmetries by promoting the symmetry defects to gauge degrees of\nfreedom, yielding an MPS of the same bond dimension that supports a local\nversion of the symmetry given by commuting gauge constraints. Whenever block\nindependence does not hold (which happens, in particular, whenever the symmetry\nrepresentation is anomalous), a modification of our method which we call\nstate-level gauging still gives rise to a locally symmetric MPS by promotion of\nthe symmetry defects, at the expense of producing gauge constraints that do not\ncommute on different sites."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "math-ph",
      "math.MP",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03687",
    "c_title":[
      "Run-and-tumble chemotaxis using reinforcement learning"
    ],
    "c_abstract":[
      "Bacterial cells use run-and-tumble motion to climb up attractant\nconcentration gradient in their environment. By extending the uphill runs and\nshortening the downhill runs the cells migrate towards the higher attractant\nzones. Motivated by this, we formulate a reinforcement learning (RL) algorithm\nwhere an agent moves in one dimension in the presence of an attractant\ngradient. The agent can perform two actions: either persistent motion in the\nsame direction or reversal of direction. We assign costs for these actions\nbased on the recent history of the agent's trajectory. We ask the question:\nwhich RL strategy works best in different types of attractant environment. We\nquantify efficiency of the RL strategy by the ability of the agent (a) to\nlocalize in the favorable zones after large times, and (b) to learn about its\ncomplete environment. Depending on the attractant profile and the initial\ncondition, we find an optimum balance is needed between exploration and\nexploitation to ensure the most efficient performance."
    ],
    "c_categories":[
      "cs.LG",
      "physics.bio-ph",
      "q-bio.CB"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00575",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
    ],
    "b_abstract":[
      "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.03876",
    "c_title":[
      "Approximate Evaluation Method for the Probability of the Union of\n  Independent Events"
    ],
    "c_abstract":[
      "The evaluation of the probability of union of a large number of independent\nevents requires several combinations involving the factorial and the use of\nhigh performance computers with several hours of processing. Bounds and\nsimplifications on the probability of the union are useful in the analysis of\nstochastic problems across various areas including (but not limited to) systems\nreliability, biological systems, real-time fault-tolerant systems, probability\ntheory, information theory and communications. We propose an approximation to\nevaluate the probability of the union of several independent events that uses\nthe arithmetic mean of the probability of all of them. The approximate results\nare very close to, but larger than the exact values. The method allows a much\nsmaller number of operations with a similar result and more simplicity."
    ],
    "c_categories":[
      "cs.IT",
      "cs.NA",
      "math.IT",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"Federated Voxel Scene Graph for Intracranial Hemorrhage",
    "a_abstract":"Intracranial Hemorrhage is a potentially lethal condition whose manifestation\nis vastly diverse and shifts across clinical centers worldwide.\nDeep-learning-based solutions are starting to model complex relations between\nbrain structures, but still struggle to generalize. While gathering more\ndiverse data is the most natural approach, privacy regulations often limit the\nsharing of medical data. We propose the first application of Federated Scene\nGraph Generation. We show that our models can leverage the increased training\ndata diversity. For Scene Graph Generation, they can recall up to 20% more\nclinically relevant relations across datasets compared to models trained on a\nsingle centralized dataset. Learning structured data representation in a\nfederated setting can open the way to the development of new methods that can\nleverage this finer information to regularize across clients more effectively.",
    "explanation":"intracranial Hemorrhage is a potentially lethal condition whose manifestation is vastly diverse and shifts across\nclinical centers worldwide. Deep-learning-based solutions are starting to model complex relations between brain\nstructures, but still struggle to generalize.",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b25"
    ],
    "c_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "c_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10186",
    "c_title":[
      "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education"
    ],
    "c_abstract":[
      "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11959",
    "c_title":[
      "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification"
    ],
    "c_abstract":[
      "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12782",
    "c_title":[
      "VidCapBench: A Comprehensive Benchmark of Video Captioning for\n  Controllable Text-to-Video Generation"
    ],
    "c_abstract":[
      "The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps:\/\/github.com\/VidCapBench\/VidCapBench."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12589",
    "c_title":[
      "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts"
    ],
    "c_abstract":[
      "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12669",
    "c_title":[
      "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite\n  Solar Cell Research"
    ],
    "c_abstract":[
      "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11849",
    "c_title":[
      "Weak solutions and sharp interface limit of the anisotropic\n  Cahn-Hilliard equation with disparate mobility and inhomogeneous potential"
    ],
    "c_abstract":[
      "We study the existence of weak solutions and the corresponding sharp\ninterface limit of an anisotropic Cahn-Hilliard equation with disparate\nmobility, i.e., the mobility is degenerate in one of the two pure phases,\nmaking the diffusion in that phase vanish. The double-well potential is\npolynomial and is weighted by a spatially inhomogeneous coefficient. In the\nlimit when the parameter of the interface width tends to zero, and under an\nenergy convergence assumption, we prove that the weak solutions converge to BV\nsolutions of a weighted anisotropic Hele-Shaw flow. We also add some numerical\nsimulations to analyze the effects of anisotropy on the Cahn-Hilliard equation."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10213",
    "c_title":[
      "The promise of deep-stacking for neutrino astronomy"
    ],
    "c_abstract":[
      "The detection of high-energy astrophysical neutrinos by IceCube has opened\nnew windows for neutrino astronomy, but their sources remains largely\nunresolved. We study a methodology to address this - deep-stacking - that\nexploits correlations between observed neutrinos and comprehensive catalogs of\npotential source populations, including faint, high-redshift sources. By\nstacking signals from numerous weak sources and optimizing source weighting,\nsignificant gains in sensitivity can be achieved, particularly in the\nlow-background regime where individual high-energy neutrinos dominate. We\nprovide a semi-analytic framework to estimate sensitivity improvements for\npopulations of sources under various background scenarios and redshift\nevolutions. Our analysis demonstrates that deep-stacking can increase detection\nsensitivity by a factor of 3 to 5, enabling detailed population studies.\nFurthermore, we discuss the potential to resolve the diffuse neutrino flux and\ninvestigate the redshift evolution of source populations. This approach offers\na direct path toward identifying the primary sites of cosmic-ray acceleration\nand the mechanisms responsible for high-energy neutrino production."
    ],
    "c_categories":[
      "astro-ph.HE",
      "astro-ph.IM"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.05434",
    "c_title":[
      "Peaks sphericity of non-Gaussian random fields"
    ],
    "c_abstract":[
      "We formulate the statistics of peaks of non-Gaussian random fields and\nimplement it to study the sphericity of peaks. For non-Gaussianity of the local\ntype, we present a general formalism valid regardless of how large the\ndeviation from Gaussian statistics is. For general types of non-Gaussianity, we\nprovide a framework that applies to any system with a given power spectrum and\nthe corresponding bispectrum in the regime in which contributions from\nhigher-order correlators can be neglected. We present an explicit expression\nfor the most probable values of the sphericity parameters, including the effect\nof non-Gaussianity on the profile. We show that the effects of small\nperturbative non-Gaussianity on the sphericity parameters are negligible, as\nthey are even smaller than the subleading Gaussian corrections. In contrast, we\nfind that large non-Gaussianity can significantly distort the peak\nconfigurations, making them much less spherical."
    ],
    "c_categories":[
      "astro-ph.CO",
      "gr-qc",
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.18261",
    "c_title":[
      "Bayesian model criticism using uniform parametrization checks"
    ],
    "c_abstract":[
      "Models are often misspecified in practice, making model criticism a key part\nof Bayesian analysis. It is important to detect not only when a model is wrong,\nbut which aspects are wrong, and to do so in a computationally convenient and\nstatistically rigorous way. We introduce a novel method for model criticism\nbased on the fact that if the parameters are drawn from the prior, and the\ndataset is generated according to the assumed likelihood, then a sample from\nthe posterior will be distributed according to the prior. Thus, departures from\nthe assumed likelihood or prior can be detected by testing whether a posterior\nsample could plausibly have been generated by the prior. Building upon this\nidea, we propose to reparametrize all random elements of the likelihood and\nprior in terms of independent uniform random variables, or u-values. This makes\nit possible to aggregate across arbitrary subsets of the u-values for data\npoints and parameters to test for model departures using classical hypothesis\ntests for dependence or non-uniformity. We demonstrate empirically how this\nmethod of uniform parametrization checks (UPCs) facilitates model criticism in\nseveral examples, and we develop supporting theoretical results."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
    ],
    "b_abstract":[
      "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04777",
    "c_title":[
      "Using Label-Free Raman Spectroscopy Integrated with Microfluidic Chips\n  to Probe Ferroptosis Networks in Cells"
    ],
    "c_abstract":[
      "Ferroptosis, a regulated form of cell death driven by oxidative stress and\nlipid peroxidation, has emerged as a pivotal research focus with implications\nacross various cellular contexts. In this study, we employed a multifaceted\napproach, integrating label-free Raman spectroscopy and microfluidics to study\nthe mechanisms underpinning ferroptosis. Our investigations included the\nferroptosis initiation based on the changes in the lipid Raman band at 1436\ncm-1 under different cellular states, the generation of reactive oxygen species\n(ROS), lipid peroxidation, DNA damage\/repair, and mitochondrial dysfunction.\nImportantly, our work highlighted the dynamic role of vital cellular\ncomponents, such as NADPH, ferredoxin clusters, and key genes like GPX-4,\nVDAC2, and NRF2, as they collectively influenced cellular responses to redox\nimbalance and oxidative stress. Quantum mechanical (QM) and molecular docking\nsimulations (MD) provided further evidence of interactions between the\nferredoxin (containing 4Fe-4S clusters), NADPH and ROS which led to the\nproduction of reactive Fe species in the cells. As such, our approach offered a\nreal-time, multidimensional perspective on ferroptosis, surpassing traditional\nbiological methods, and providing valuable insights for therapeutic\ninterventions in diverse biomedical contexts."
    ],
    "c_categories":[
      "physics.bio-ph",
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":[
      "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats"
    ],
    "c_abstract":[
      "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18661",
    "c_title":[
      "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling"
    ],
    "c_abstract":[
      "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11130",
    "c_title":[
      "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering"
    ],
    "c_abstract":[
      "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17455",
    "c_title":[
      "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective"
    ],
    "c_abstract":[
      "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09215",
    "c_title":[
      "New results of Bollob\\'{a}s-type theorem for affine subspaces and\n  projective subspaces"
    ],
    "c_abstract":[
      "Bollob\\'{a}s-type theorem has received a lot of attention due to its\napplication in graph theory. In 2015, G\\'{a}bor Heged{\\\"u}s gave an upper bound\nof bollob\\'{a}s-type affine subspace families for $q\\neq 2$, and constructed an\nalmost sharp affine subspaces pair families. In this note, we prove a new upper\nbound for bollob\\'{a}s-type affine subspaces without the requirement of $q\\neq\n2$, and construct a pair of families of affine subspaces, which shows that our\nupper bound is sharp. We also give an upper bound for bollob\\'{a}s-type\nprojective subspaces, and prove that the Heged{\\\"u}s's conjecture holds when\n$q=2$."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04857",
    "c_title":[
      "Explicit Pfaffian Formula for Amplitudes of Fermionic Gaussian Pure\n  States in Arbitrary Pauli Bases"
    ],
    "c_abstract":[
      "The explicit computation of amplitudes for fermionic Gaussian pure states in\narbitrary Pauli bases is a long-standing challenge in quantum many-body\nphysics, with significant implications for quantum tomography, experimental\nstudies, and quantum dynamics. These calculations are essential for analyzing\ncomplex properties beyond traditional measures, such as formation\nprobabilities, global entanglement, and entropy in non-standard bases, where\nexact and computationally efficient methods remain underdeveloped. In\nparticular, having explicit formulas is crucial for optimizing negative\nlog-likelihood functions in quantum tomography, a key task in the NISQ era. In\nthis work, we present an explicit Pfaffian formula (Theorem 1) for determining\nthese amplitudes in arbitrary Pauli bases, utilizing a Pfaffian of a matrix\nbased on qubit parity. Additionally, we introduce a recursive relation (Theorem\n2) that links the amplitudes of systems with different qubit counts, enabling\nscalable calculations for large systems. Together, these results provide a\nversatile framework for applications in global entanglement, Shannon-R\\'enyi\nentropy, formation probabilities, and quantum tomography, significantly\nexpanding the computational toolkit for analyzing complex quantum systems."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "math-ph",
      "math.MP",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12643",
    "c_title":[
      "Solar Sail Momentum Management with Mass Translation and Reflectivity\n  Devices Using MPC"
    ],
    "c_abstract":[
      "Solar sails enable propellant-free space missions by utilizing solar\nradiation pressure as thrust. However, disturbance torques act on the solar\nsail and effective attitude control leads to the continuous accumulation of\nreaction wheel (RW) angular momentum, necessitating an efficient momentum\nmanagement strategy to prevent RW saturation. This paper presents a novel\nmomentum management controller using model predictive control (MPC) that is\ntailored for solar sails, accommodating the unique actuation mechanisms of an\nactive mass translator (AMT) and reflectivity control devices (RCDs). A\nfirst-order hold discretization and tailored motion costs are applied to the\nAMT translation, while the RCD actuation is handled using pulse-width\nmodulation (PWM)-inspired quantization to address their on-off inputs. To\nenhance prediction accuracy, an iterative backwards-in-time MPC approach is\nintroduced, incorporating the effects of PWM-quantized inputs into the\noptimization process. The dynamic model accounts for the time-dependent center\nof mass (CM) and moment of inertia changes caused by AMT translation, extending\nits applicability to other spacecraft with CM-shifting actuators. Simulation\nresults demonstrate the effectiveness of the proposed framework in RW\ndesaturation, attitude control, and momentum management actuation efficiency,\nhighlighting the potential of integrating MPC to manage coupled nonlinear\ndynamics and discrete actuator constraints for solar sails."
    ],
    "c_categories":[
      "physics.space-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06675",
    "c_title":[
      "Chip-Firing on Infinite $k$-ary Trees"
    ],
    "c_abstract":[
      "We use an infinite $k$-ary tree with a self-loop at the root as our\nunderlying graph. We consider a chip-firing process starting with $N$ chips at\nthe root. We describe the stable configurations. We calculate the number of\nfires for each vertex and the total number of fires. We study a sequence of the\nnumber of root fires for a given $k$ as a function of $N$ and study its\nproperties. We do the same for the total number of fires."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00578",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Voxel Scene Graph for Intracranial Hemorrhage"
    ],
    "b_abstract":[
      "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11471",
    "c_title":[
      "NH-rich organic compounds from the carbonaceous asteroid (162173) Ryugu:\n  nanoscale spectral and isotopic characterizations"
    ],
    "c_abstract":[
      "The detection of spectral bands at 3.06 um by MicrOmega, combined with the\nchemical identification of other NH-containing organic molecules in Ryugu\nsamples, suggests the presence of potential NH-bearing compounds. However, the\nchemical forms of these NH-rich compounds, whether associated with N-rich\norganics, ammonium (NH4+) salts, NH4 or NH-organics-bearing phyllosilicates, or\nother forms, remain to be better understood. In this study, we report the\ncharacterization of two Ryugu particles (C0050 and C0052) using multi-scale\ninfrared (mm-reflectance, micro-FTIR, and nano-AFM-IR) and NanoSIMS techniques\nto constrain the nature and origin of NH-bearing components in the Ryugu\nasteroid. Our findings show that Ryugu's C0052 particle contains rare,\nmicrometer-sized NH-rich organic compounds with peaks at 1660 cm-1 (mainly due\nto C=O stretching of the amide I band) and 1550 cm-1 (mainly due to N-H bending\nvibration mode of the amide II band), indicative of amide-related compounds. In\ncontrast, these compounds are absent in C0050. Notably, nitrogen isotopic\nanalysis reveals that these amides in C0052 are depleted in 15N (d15N = -215\n+\/- 92 permil), confirming their indigenous origin, while carbon and hydrogen\nisotopic compositions are indistinguishable from terrestrial values within\nerrors (d13C = -22 +\/- 52 and dD = 194 +\/- 368 permil). The amides detected in\nC0052 could have formed through hydrothermal alteration from carboxylic acids\nand amines precursors on the Ryugu's parent planetesimal. Alternatively, they\ncould have originated from the irradiation of 15N-depleted N-bearing ice by UV\nlight or galactic cosmic rays, either at the surface of the asteroid in the\nouter Solar System or on mantle of interstellar dust grains in the interstellar\nmedium. Amides delivered to early Earth by primitive small bodies, such as\nasteroid Ryugu, may have played a crucial role in prebiotic chemistry."
    ],
    "c_categories":[
      "astro-ph.EP",
      "physics.geo-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"Fast and scalable Wasserstein-1 neural optimal transport solver for\n  single-cell perturbation prediction",
    "a_abstract":"Predicting single-cell perturbation responses requires mapping between two\nunpaired single-cell data distributions. Optimal transport (OT) theory provides\na principled framework for constructing such mappings by minimizing transport\ncost. Recently, Wasserstein-2 ($W_2$) neural optimal transport solvers\n(\\textit{e.g.}, CellOT) have been employed for this prediction task. However,\n$W_2$ OT relies on the general Kantorovich dual formulation, which involves\noptimizing over two conjugate functions, leading to a complex min-max\noptimization problem that converges slowly. To address these challenges, we\npropose a novel solver based on the Wasserstein-1 ($W_1$) dual formulation.\nUnlike $W_2$, the $W_1$ dual simplifies the optimization to a maximization\nproblem over a single 1-Lipschitz function, thus eliminating the need for\ntime-consuming min-max optimization. While solving the $W_1$ dual only reveals\nthe transport direction and does not directly provide a unique optimal\ntransport map, we incorporate an additional step using adversarial training to\ndetermine an appropriate transport step size, effectively recovering the\ntransport map. Our experiments demonstrate that the proposed $W_1$ neural\noptimal transport solver can mimic the $W_2$ OT solvers in finding a unique and\n``monotonic\" map on 2D datasets. Moreover, the $W_1$ OT solver achieves\nperformance on par with or surpasses $W_2$ OT solvers on real single-cell\nperturbation datasets. Furthermore, we show that $W_1$ OT solver achieves $25\n\\sim 45\\times$ speedup, scales better on high dimensional transportation task,\nand can be directly applied on single-cell RNA-seq dataset with highly variable\ngenes. Our implementation and experiments are open-sourced at\n\\url{https:\/\/github.com\/poseidonchan\/w1ot}.",
    "explanation":"Predicting single-cell perturbation responses requires mapping between two unpaired single-\ncell data distributions. Optimal transport (OT) theory provides a principled framework for constructing\nsuch mappings by minimizing transport cost.",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b11"
    ],
    "c_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "c_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03470",
    "c_title":[
      "Positivstellens\\\"atze for polynomial matrices with universal quantifiers"
    ],
    "c_abstract":[
      "This paper studies Positivstellens\\\"atze for a polynomial matrix subject to\npolynomial matrix inequality constraints with universal quantifiers. We first\npresent a Scherer-Hol-type Positivstellensatz under the Archimedean condition.\nWhen the objective is a scalar polynomial, we further provide a sparse\nScherer-Hol-type Positivstellensatz in the presence of correlative sparsity.\nNext, without assuming the Archimedean condition, we derive\nPutinar-Vasilescu-type, P\\'olya-type, and Lasserre-Netzer-type\nPositivstellens\\\"atze under the same setting. These results can be viewed as\ncommon generalizations of corresponding Positivstellens\\\"atze in the cases of\npolynomials, polynomials with universal quantifiers, and polynomial matrices.\nFor the proofs, techniques from *-algebra, real algebraic geometry, operator\ntheory, and convex optimization are employed. Applications of the established\nPositivstellens\\\"atze to robust polynomial matrix optimization are also\ndiscussed."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.14180",
    "c_title":[
      "Benders decomposition for the large-scale probabilistic set covering\n  problem"
    ],
    "c_abstract":[
      "In this paper, we consider a probabilistic set covering problem (PSCP) in\nwhich each 0-1 row of the constraint matrix is random with a finite discrete\ndistribution, and the objective is to minimize the total cost of the selected\ncolumns such that each row is covered with a prespecified probability. We\ndevelop an effective decomposition algorithm for the PSCP based on the Benders\nreformulation of a standard mixed integer programming (MIP) formulation. The\nproposed Benders decomposition (BD) algorithm enjoys two key advantages: (i)\nthe number of variables in the underlying Benders reformulation is equal to the\nnumber of columns but independent of the number of scenarios of the random\ndata; and (ii) the Benders feasibility cuts can be separated by an efficient\npolynomial-time algorithm, which makes it particularly suitable for solving\nlarge-scale PSCPs. We enhance the BD algorithm by using initial cuts to\nstrengthen the relaxed master problem, implementing an effective heuristic\nprocedure to find high-quality feasible solutions, and adding mixed integer\nrounding enhanced Benders feasibility cuts to tighten the problem formulation.\nNumerical results demonstrate the efficiency of the proposed BD algorithm over\na state-of-the-art MIP solver. Moreover, the proposed BD algorithm can\nefficiently identify optimal solutions for instances with up to 500 rows, 5000\ncolumns, and 2000 scenarios of the random rows."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.08439",
    "c_title":[
      "Revisiting Continuous p-Hub Location Problems with the L1 Metric"
    ],
    "c_abstract":[
      "Motivated by emerging urban applications in commercial, public sector, and\nhumanitarian logistics, we revisit continuous $p$-hub location problems in\nwhich several facilities must be located in a continuous space such that the\nexpected minimum Manhattan travel distance from a random service provider to a\nrandom customer through exactly one hub facility is minimized. In this paper,\nwe begin by deriving closed-form results for a one-dimensional case and\ntwo-dimensional cases with up to two hubs. Subsequently, a simulation-based\napproximation method is proposed for more complex two-dimensional scenarios\nwith more than two hubs. Moreover, an extended problem with multiple service\nproviders is analyzed to reflect real-life service settings. Finally, we apply\nour model and approximation method using publicly available data as a case\nstudy to optimize the deployment of public-access automated external\ndefibrillators in Virginia Beach."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.07034",
    "c_title":[
      "The Optimal Control Problem of Fully Coupled FBSDEs Driven by\n  Sub-diffusion with Applications"
    ],
    "c_abstract":[
      "This paper is devoted to an optimal control problem of fully coupled\nforward-backward stochastic differential equations driven by sub-diffusion,\nwhose solutions are not Markov processes. The stochastic maximum principle is\nobtained, where the control domain may not be convex and the diffusion term is\nindependent of the control variable. Additionally, problem with state\nconstraint is researched by using Ekeland's variational principle. The\ntheoretical results obtained are applied to a cash management optimization\nproblem in bear market, and the optimal strategy is derived."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.10384",
    "c_title":[
      "Stochastic Gradient Descent for Constrained Optimization based on\n  Adaptive Relaxed Barrier Functions"
    ],
    "c_abstract":[
      "This paper presents a novel stochastic gradient descent algorithm for\nconstrained optimization. The proposed algorithm randomly samples constraints\nand components of the finite sum objective function and relies on a relaxed\nlogarithmic barrier function that is appropriately adapted in each optimization\niteration. For a strongly convex objective function and affine inequality\nconstraints, step-size rules and barrier adaptation rules are established that\nguarantee asymptotic convergence with probability one. The theoretical results\nin the paper are complemented by numerical studies which highlight potential\nadvantages of the proposed algorithm for optimization problems with a large\nnumber of constraints."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06906",
    "c_title":[
      "Kinetic model and numerical method for multispecies radiation\n  hydrodynamic system with multiscale nonequilibrium transport"
    ],
    "c_abstract":[
      "This paper presents a comprehensive numerical framework for simulating\nradiation-plasma systems. The radiative transfer process spans multiple flow\nregimes due to varying fluid opacity across different regions, necessitating a\nrobust numerical approach. We employ the multiscale unified gas-kinetic scheme\n(UGKS), which accurately captures photon transport phenomena from free\nstreaming to diffusive wave propagation. The UGKS is also applied to the fluid\nmodel to address the significant mass disparity between electrons and ions, and\ntheir associated transport characteristics in both equilibrium continuum and\nnon-equilibrium rarefied regimes. Our model explicitly incorporates momentum\nand energy exchanges between radiation and fluid fields in the coupled system,\nenabling detailed analysis of the complex interactions between electromagnetic\nand hydrodynamic phenomena. The developed algorithm successfully reproduces\nboth optically thin and optically thick radiation limits while capturing the\ncomplex multiscale nonequilibrium dynamics of the coupled system. This unified\ntreatment eliminates the need for separate numerical schemes in different\nregimes, providing a consistent and computationally effcient approach for the\nentire domain. The effectiveness and versatility of this approach are\ndemonstrated through extensive numerical validation across a wide range of\nphysical parameters and flow conditions."
    ],
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.12746",
    "c_title":[
      "EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering"
    ],
    "c_abstract":[
      "When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted analysis to provide high-quality answers. Current LLM-based\nquestion answering methods lack a detailed definition and learning process for\nevidence analysis, leading to the risk of error propagation and hallucinations\nwhile using evidence. Although increasing the parameter size of LLMs can\nalleviate these issues, it also presents challenges in training and deployment\nwith limited resources. In this study, we propose EvidenceMap, which aims to\nenable a tiny pre-trained language model to explicitly learn multiple aspects\nof biomedical evidence, including supportive evaluation, logical correlation\nand content summarization, thereby latently guiding a small generative model\n(around 3B parameters) to provide textual responses. Experimental results\ndemonstrate that our method, learning evidence analysis by fine-tuning a model\nwith only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and\n5.7% in reference-based quality and accuracy, respectively."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00463",
    "c_title":[
      "Perturbative determination of $\\mathcal{O}(a\\,m)$ improvement on the QCD\n  running coupling"
    ],
    "c_abstract":[
      "We present the perturbative results of the discretization errors proportional\nto the quark mass ($\\mathcal{O}(a m)$) on the QCD running coupling within\nlattice perturbation theory. Our analysis involves calculating the 2-loop\nrenormalization factor $Z_g$ using improved lattice actions for the $SU(N_c)$\ngauge group and $N_f$ multiplets of fermions with a finite quark mass. We\nemploy the background field method to compute $Z_g$, by calculating quantum\ncorrections on both the background and quantum gluon propagator, respecting the\n$\\mathcal{O}(a)$ improvement. This allows us to evaluate the perturbative\n$\\mathcal{O}(a m)$ lattice errors which affect the determination of the running\ncoupling. Eliminating these $\\mathcal{O}(a m)$ effects is crucial for the\nnonperturbative studies of precision determinations of the strong coupling\nconstant using lattice field theory."
    ],
    "c_categories":[
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02502",
    "c_title":[
      "Developing techniques for Simulation of SU(3) Quantum Field Theories on\n  State-of-the-Art Quantum Devices"
    ],
    "c_abstract":[
      "Quantum computing has long been an experimental technology with the potential\nto simulate, at scale, phenomena which on classical devices would be too\nexpensive to simulate at any but the smallest scales. Over the last several\nyears, however, it has entered the NISQ era, where the number of qubits are\nsufficient for quantum advantage but substantial noise on hardware stands in\nthe way of this achievement. This thesis details NISQ device-centered\nimprovements to techniques of quantum simulation of the out-of-equilbrium\nreal-time dynamics of lattice quantum chromodynamics (LQCD) and of dense\n3-flavor neutrino systems on digital quantum devices.\n  The first project concerning LQCD is a comparison of methods for implementing\nthe variational quantum eigensolver (VQE) that initializes the ground state of\nan SU(3) plaquette-chain. The thesis then pivots to a 1+1D lattice of quarks\ninteracting with an SU(3) gauge-field. A VQE-based state-preparation for the\nvacua and a Trotterized time-evolution circuit is designed and applied to the\nproblems of simulating beta and neutrinoless double beta decay. Finally, these\ncircuits are adapted to a version useable on quantum devices with\nnearest-neighbor connectivity with minimal overhead, with an eye towards\nutilizing the higher qubit count of such devices for hadron dynamics and\nscattering.\n  This thesis covers two projects that concern dense 3-flavor neutrino systems.\nThe first details design and testing of Trotterized time-evolution circuits on\nstate-of-the-art quantum devices. The second, motivated by the Gottesman-Knill\ntheorem's result that deviation from stabilizer states (\"magic\") is necessary\nfor a problem to exhibit quantum advantage, details results with implications\nfor the Standard Model in general that the 3 flavor ultradense neutrino systems\nwith the highest, most-persistent magic are those that start with neutrinos in\nall 3 flavors."
    ],
    "c_categories":[
      "hep-lat",
      "nucl-th",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
    ],
    "b_abstract":[
      "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16443",
    "c_title":[
      "Objects matter: object-centric world models improve reinforcement\n  learning in visually complex environments"
    ],
    "c_abstract":[
      "Deep reinforcement learning has achieved remarkable success in learning\ncontrol policies from pixels across a wide range of tasks, yet its application\nremains hindered by low sample efficiency, requiring significantly more\nenvironment interactions than humans to reach comparable performance.\nModel-based reinforcement learning (MBRL) offers a solution by leveraging\nlearnt world models to generate simulated experience, thereby improving sample\nefficiency. However, in visually complex environments, small or dynamic\nelements can be critical for decision-making. Yet, traditional MBRL methods in\npixel-based environments typically rely on auto-encoding with an $L_2$ loss,\nwhich is dominated by large areas and often fails to capture decision-relevant\ndetails. To address these limitations, we propose an object-centric MBRL\npipeline, which integrates recent advances in computer vision to allow agents\nto focus on key decision-related elements. Our approach consists of four main\nsteps: (1) annotating key objects related to rewards and goals with\nsegmentation masks, (2) extracting object features using a pre-trained, frozen\nfoundation vision model, (3) incorporating these object features with the raw\nobservations to predict environmental dynamics, and (4) training the policy\nusing imagined trajectories generated by this object-centric world model.\nBuilding on the efficient MBRL algorithm STORM, we call this pipeline OC-STORM.\nWe demonstrate OC-STORM's practical value in overcoming the limitations of\nconventional MBRL approaches on both Atari games and the visually complex game\nHollow Knight."
    ],
    "c_categories":[
      "cs.CV",
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.05607",
    "c_title":[
      "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features"
    ],
    "c_abstract":[
      "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.16996",
    "c_title":[
      "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation"
    ],
    "c_abstract":[
      "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.00165",
    "c_title":[
      "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2"
    ],
    "c_abstract":[
      "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.09943",
    "c_title":[
      "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications"
    ],
    "c_abstract":[
      "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.05025",
    "c_title":[
      "ProtComposer: Compositional Protein Structure Generation with 3D\n  Ellipsoids"
    ],
    "c_abstract":[
      "We develop ProtComposer to generate protein structures conditioned on spatial\nprotein layouts that are specified via a set of 3D ellipsoids capturing\nsubstructure shapes and semantics. At inference time, we condition on\nellipsoids that are hand-constructed, extracted from existing proteins, or from\na statistical model, with each option unlocking new capabilities.\nHand-specifying ellipsoids enables users to control the location, size,\norientation, secondary structure, and approximate shape of protein\nsubstructures. Conditioning on ellipsoids of existing proteins enables\nredesigning their substructure's connectivity or editing substructure\nproperties. By conditioning on novel and diverse ellipsoid layouts from a\nsimple statistical model, we improve protein generation with expanded Pareto\nfrontiers between designability, novelty, and diversity. Further, this enables\nsampling designable proteins with a helix-fraction that matches PDB proteins,\nunlike existing generative models that commonly oversample conceptually simple\nhelix bundles. Code is available at https:\/\/github.com\/NVlabs\/protcomposer."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.07906",
    "c_title":[
      "Magnon-induced scalar spin chirality in Kagome and honeycomb\n  ferromagnets"
    ],
    "c_abstract":[
      "The scalar spin chirality (SSC), defined as a triple product of spins, is\nessential for describing noncoplanar spin structures and understanding chiral\nphysics in magnetic systems. Traditionally, SSC has been discussed primarily in\nthe context of noncoplanar ground-state spin configurations at zero\ntemperature, as collinear spin systems are generally thought to lack SSC.\nConsequently, whether the SSC can emerge at finite temperatures in spin systems\nwith collinear ground states remains an open question and has yet to be fully\nunderstood. In this study, we theoretically demonstrate that thermally excited\nmagnons can induce SSC even in collinear spin systems. By considering 2D\nferromagnets on Kagome and honeycomb lattices, we demonstrate that the\nDzyaloshinskii-Moriya interactions (DMI) which break the effective\ntime-reversal symmetry in the magnon Hamiltonian can lead to finite SSC at\nfinite temperatures. Using a simple spin model, we show both numerically and\nanalytically that the SSC increases with the magnitude of DMI and temperature.\nFurthermore, calculations based on realistic material parameters reveal that\nthe magnon-induced SSC can achieve a magnitude comparable to those observed in\nnon-coplanar spin configurations. These findings suggest that SSC plays a\nsignificant role even in collinear spin systems, providing new insights into\nthe chiral physics of magnetic materials."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.05147",
    "c_title":[
      "LP-DETR: Layer-wise Progressive Relations for Object Detection"
    ],
    "c_abstract":[
      "This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach\nthat enhances DETR-based object detection through multi-scale relation\nmodeling. Our method introduces learnable spatial relationships between object\nqueries through a relation-aware self-attention mechanism, which adaptively\nlearns to balance different scales of relations (local, medium and global)\nacross decoder layers. This progressive design enables the model to effectively\ncapture evolving spatial dependencies throughout the detection pipeline.\nExtensive experiments on COCO 2017 dataset demonstrate that our method improves\nboth convergence speed and detection accuracy compared to standard\nself-attention module. The proposed method achieves competitive results,\nreaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50\nbackbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore,\nour analysis reveals an interesting pattern: the model naturally learns to\nprioritize local spatial relations in early decoder layers while gradually\nshifting attention to broader contexts in deeper layers, providing valuable\ninsights for future research in object detection."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10403",
    "c_title":[
      "Implementing agile healthcare frame works in the context of low income\n  countries: Proposed Framework and Review"
    ],
    "c_abstract":[
      "Agile healthcare frameworks, derived from methodologies in IT and\nmanufacturing, offer transformative potential for low-income regions. This\nstudy explores Agile integration in resource-constrained environments, focusing\non Ghana. Key benefits include adaptability, iterative planning, and\nstakeholder collaboration to address infrastructure gaps, workforce shortages,\nand the \"know-do gap.\" Digital tools like mobile health (mHealth) applications\nand the District Health Information Management System (DHIMS) demonstrate Agile\nscalability and efficacy in improving outcomes and resource allocation. Policy\nalignment, such as through Ghana's National Health Insurance Scheme (NHIS), is\ncrucial for sustaining these practices. Findings reveal Agile ability to enable\nreal-time decision-making, foster community engagement, and drive\ninterdisciplinary collaboration. This paper provides actionable strategies and\nsystemic innovations, positioning Agile as a scalable model for equitable,\nhigh-quality care delivery in other low-income regions."
    ],
    "c_categories":[
      "cs.CY",
      "cs.ET",
      "cs.IR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.06119",
    "c_title":[
      "Vortex stability in interacting Bose-Einstein condensates"
    ],
    "c_abstract":[
      "We study the stability of vortices in a binary system of Bose-Einstein\ncondensates, with their wave functions modeled by a set of coupled,\ntime-dependent Gross-Pitaevskii equations. Beginning with an effective\ntwo-dimensional system, we identify miscible and immiscible regimes\ncharacterized by the inter- and intra-atomic interactions and the initial\nconfiguration of the system. We then consider a binary system of Bose-Einstein\ncondensates placed in a rotating harmonic trap and study the single vortex\nstate in this system. We derive an approximate form for the energy of a single\nvortex in the binary system and the critical angular velocity for the global\nstability of a vortex at the center of the trap. We also compute the\nmetastability onset angular velocity for the local stability of a vortex at the\ncenter of the trap. Numerical solutions to the Gross-Pitaevskii equations\nsupport these expressions. These rotational results inform us of a novel\nsubphase within the miscible regime of the binary condensate system. We thus\ndemonstrate the non-trivial aspects of vortex stability in interacting binary\nBose-Einstein condensates as a result of their non-linear interactions."
    ],
    "c_categories":[
      "cond-mat.quant-gas"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
    ],
    "b_abstract":[
      "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.05155",
    "c_title":[
      "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier"
    ],
    "c_abstract":[
      "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"PathoGen-X: A Cross-Modal Genomic Feature Trans-Align Network for\n  Enhanced Survival Prediction from Histopathology Images",
    "a_abstract":"Accurate survival prediction is essential for personalized cancer treatment.\nHowever, genomic data - often a more powerful predictor than pathology data -\nis costly and inaccessible. We present the cross-modal genomic feature\ntranslation and alignment network for enhanced survival prediction from\nhistopathology images (PathoGen-X). It is a deep learning framework that\nleverages both genomic and imaging data during training, relying solely on\nimaging data at testing. PathoGen-X employs transformer-based networks to align\nand translate image features into the genomic feature space, enhancing weaker\nimaging signals with stronger genomic signals. Unlike other methods, PathoGen-X\ntranslates and aligns features without projecting them to a shared latent space\nand requires fewer paired samples. Evaluated on TCGA-BRCA, TCGA-LUAD, and\nTCGA-GBM datasets, PathoGen-X demonstrates strong survival prediction\nperformance, emphasizing the potential of enriched imaging models for\naccessible cancer prognosis.",
    "explanation":"Accurate survival prediction is essential for personalized\ncancer treatment. . It is a\ndeep learning framework that leverages both genomic and\nimaging data during training, relying solely on imaging data\nat testing. ",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b4"
    ],
    "c_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "c_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16938",
    "c_title":[
      "Interpretable Machine Learning for Oral Lesion Diagnosis through\n  Prototypical Instances Identification"
    ],
    "c_abstract":[
      "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04567",
    "c_title":[
      "Preference Optimization via Contrastive Divergence: Your Reward Model is\n  Secretly an NLL Estimator"
    ],
    "c_abstract":[
      "Existing studies on preference optimization (PO) have centered on\nconstructing pairwise preference data following simple heuristics, such as\nmaximizing the margin between preferred and dispreferred completions based on\nhuman (or AI) ranked scores. However, none of these heuristics has a full\ntheoretical justification. In this work, we develop a novel PO framework that\nprovides theoretical guidance to effectively sample dispreferred completions.\nTo achieve this, we formulate PO as minimizing the negative log-likelihood\n(NLL) of a probability model and propose to estimate its normalization constant\nvia a sampling strategy. As we will demonstrate, these estimative samples can\nact as dispreferred completions in PO. We then select contrastive divergence\n(CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies\nthe Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the\nparameterized reward model. Finally, we propose the OnMC-PO algorithm, an\nextension of MC-PO to the online setting. On popular alignment benchmarks,\nMC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further\nimprovement."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.16242",
    "c_title":[
      "Reproducibility Study of Cooperation, Competition, and Maliciousness:\n  LLM-Stakeholders Interactive Negotiation"
    ],
    "c_abstract":[
      "This paper presents a reproducibility study and extension of \"Cooperation,\nCompetition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We\nvalidate the original findings using a range of open-weight models (1.5B-70B\nparameters) and GPT-4o Mini while introducing several novel contributions. We\nanalyze the Pareto front of the games, propose a communication-free baseline to\ntest whether successful negotiations are possible without agent interaction,\nevaluate recent small language models' performance, analyze structural\ninformation leakage in model responses, and implement an inequality metric to\nassess negotiation fairness. Our results demonstrate that smaller models (<10B\nparameters) struggle with format adherence and coherent responses, but larger\nopen-weight models can approach proprietary model performance. Additionally, in\nmany scenarios, single-agent approaches can achieve comparable results to\nmulti-agent negotiations, challenging assumptions about the necessity of agent\ncommunication to perform well on the benchmark. This work also provides\ninsights into the accessibility, fairness, environmental impact, and privacy\nconsiderations of LLM-based negotiation systems."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00633",
    "c_title":[
      "Lipschitz Lifelong Monte Carlo Tree Search for Mastering Non-Stationary\n  Tasks"
    ],
    "c_abstract":[
      "Monte Carlo Tree Search (MCTS) has proven highly effective in solving complex\nplanning tasks by balancing exploration and exploitation using Upper Confidence\nBound for Trees (UCT). However, existing work have not considered MCTS-based\nlifelong planning, where an agent faces a non-stationary series of tasks --\ne.g., with varying transition probabilities and rewards -- that are drawn\nsequentially throughout the operational lifetime. This paper presents LiZero\nfor Lipschitz lifelong planning using MCTS. We propose a novel concept of\nadaptive UCT (aUCT) to transfer knowledge from a source task to the\nexploration\/exploitation of a new task, depending on both the Lipschitz\ncontinuity between tasks and the confidence of knowledge in in Monte Carlo\naction sampling. We analyze LiZero's acceleration factor in terms of improved\nsampling efficiency and also develop efficient algorithms to compute aUCT in an\nonline fashion by both data-driven and model-based approaches, whose sampling\ncomplexity and error bounds are also characterized. Experiment results show\nthat LiZero significantly outperforms existing MCTS and lifelong learning\nbaselines in terms of much faster convergence (3$\\sim$4x) to optimal rewards.\nOur results highlight the potential of LiZero to advance decision-making and\nplanning in dynamic real-world environments."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17354",
    "c_title":[
      "HCAST: Human-Calibrated Autonomy Software Tasks"
    ],
    "c_abstract":[
      "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.01962",
    "c_title":[
      "Curvature Perturbations from First-Order Phase Transitions: Implications\n  to Black Holes and Gravitational Waves"
    ],
    "c_abstract":[
      "We employ a covariant formalism to study the evolution of cosmological\nperturbations during a first-order phase transition, addressing in particular\ntheir gauge dependence that have been overlooked so far. Our results reveal\nthat non-covariant treatments employed in previous studies can substantially\noverestimate the production of primordial black holes and scalar-induced\ngravitational waves. Once gauge dependencies are properly accounted for, we\nfind that both effects occur at significantly lower levels than previously\nestimated."
    ],
    "c_categories":[
      "astro-ph.CO",
      "gr-qc",
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.14220",
    "c_title":[
      "Analysis of heralded higher-fidelity two-qubit entangling gates with\n  self-correction"
    ],
    "c_abstract":[
      "For the quantum error correction (QEC) and noisy intermediate-scale quantum\n(NISQ) algorithms to function with high efficiency, the raw fidelity of quantum\nlogic gates on physical qubits needs to satisfy strict requirement. The neutral\natom quantum computing equipped with Rydberg blockade gates has made impressive\nprogress recently, which makes it worthwhile to explore its potential in the\ntwo-qubit entangling gates, including Controlled-PHASE gate and in particular\nthe CZ gate. Provided the quantum coherence is well preserved, improving the\nfidelity of Rydberg blockade gates calls for special mechanisms to deal with\nadverse effects caused by realistic experimental conditions. Here the heralded\nvery-high-fidelity Rydberg blockade Controlled-PHASE gate is designed to\naddress these issues, which contains self-correction and projection as the key\nsteps. This trailblazing method can be built on the basis of the previously\nestablished buffer-atom-mediated gate, and a special form of symmetry under PT\ntransformation plays a crucial role in the process. We further analyze the\nperformance with respect to a few typical sources of imperfections. This\nprocedure can also be regarded as quantum hardware error correction or\nmitigation. While this paper by itself does not cover every single subtle issue\nand still contains many over-simplifications, we find it reasonable to\nanticipate very-high-fidelity two-qubit quantum logic gate operated in the\nsense of heralded but probabilistic, whose gate error can reduce to the level\nof $10^{-4}$--$10^{-6}$ or even lower with reasonably high possibilities."
    ],
    "c_categories":[
      "cs.AR",
      "physics.atom-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00208",
    "c_title":[
      "Thermal Phase Curves in Hot Gas Giant Exoplanets Exhibit a Complex\n  Dependence on Planetary Properties"
    ],
    "c_abstract":[
      "We present a catalog of uniformly processed 3.6-$\\mu$m and 4.5-$\\mu$m band\nexoplanet thermal phase curves based on Infrared Array Camera observations\nobtained from the Spitzer Heritage Archive. The catalog includes phase curve\nmeasurements for 34 planets, 16 of which contain full orbit coverage and have\ndetectable secondary eclipses in both channels. The data are processed in the\nEXCALIBUR pipeline using a uniform analysis consisting of aperture photometry\nand modeling of instrument effects along with the exoplanet signal.\nNearest-neighbors regression with a Gaussian kernel is used to correct for\ninstrumental systematics correlated to the star's centroid position and shape\nin conjunction with a novel test to avoid overfitting. These methods may have\nutility in addressing sub-pixel gain variations present in modern infrared\ndetectors. We analyze the 3.6-$\\mu$m and 4.5-$\\mu$m phase curve properties and\nfind a strong wavelength-dependent difference in how the properties correlate\nwith physical parameters as well as evidence that the phase curve properties\nare determined by multiple physical parameters. We suggest that differences\nbetween the 3.6-$\\mu$m and 4.5-$\\mu$m phase curve properties are due to\n3.6~$\\mu$m observations probing regions of the atmosphere which could include a\ncloud layer. Taken together, the observed phase curve behavior suggests that\ndifferent physical processes are responsible for establishing the thermal phase\ncurve at different pressures, which are probed by different wavelengths, and\nthat further 3D GCM modeling is required to investigate the reason for this\ncomplex dependence on planetary properties."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09005",
    "c_title":[
      "Progenitor Dependence of Neutrino-driven Supernova Explosions with the\n  Aid of Heavy Axion-like Particles"
    ],
    "c_abstract":[
      "We perform spherically symmetric simulations of core-collapse supernovae with\nthe aid of heavy axion-like particles (ALPs) which interact with photons and\nredistribute energy within supernova matter. We explore a wide ALP parameter\nspace that includes MeV-scale ALP mass $m_{\\,a}$ and the ALP-photon coupling\nconstant $g_{\\,a \\gamma} \\sim 10^{\\,-10} \\, \\rm{GeV}^{\\,-1}$ , employing three\nprogenitor models with zero-age main-sequence mass of $11.2\\,M_\\odot$,\n$20.0\\,M_\\odot$, and $25.0\\,M_\\odot$. We find a general trend that, given\n$m_{\\,a}\\lesssim 300\\,$MeV, heavier ALPs are favorable for the shock wave to be\nsuccessfully revived, aiding the onset of the neutrino-driven explosion.\nHowever, if ALPs are heavier than $\\sim 400\\,$MeV, the explosion is failed or\nweaker than that for the models with smaller $m_{\\,a}$, because of an\ninsufficient temperature inside the supernova core to produce heavy ALPs. The\nmaximum temperature in the core depends on the initial progenitor structure.\nOur simulations indicate that the high-temperature environment in the\ncollapsing core of massive progenitors leads to a significant impact of ALPs on\nthe explodability."
    ],
    "c_categories":[
      "astro-ph.HE",
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "A 2021 update on cancer image analytics with deep learning"
    ],
    "b_abstract":[
      "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.18688",
    "c_title":[
      "Quantum Phase Transitions between Symmetry-Enriched Fracton Phases"
    ],
    "c_abstract":[
      "Phases with topological order exhibit further complexity in the presence of\nglobal symmetries: States with the same topological order are distinguished by\nhow their anyonic excitations transform under these symmetries, leading to a\nclassification in terms of symmetry-enriched topological phases. In this work,\nwe develop a generic scheme to study an analogous situation for\nthree-dimensional fracton phases by means of isometric tensor network states\n(isoTNS) with finite bond dimension, which allow us to tune between\nwavefunctions of different symmetry fractionalization. We focus on the X-Cube\nmodel, a paradigmatic fracton model hosting two types of excitations: lineons,\nwhich are mobile in a single direction only, and fractons that are completely\nimmobile as individual particles. By deforming the local tensors that describe\nthe ground state of the fixed point model, we find a family of exact\nwavefunctions for which the symmetry fractionalization under an anti-unitary\nsymmetry on both types of excitations is directly visible. These wavefunctions\nhave non-vanishing correlation lengths and are non-stabilizer states. At the\ncritical points between the phases, power-law correlations are supported in\ncertain spatial directions. Furthermore, based on the isoTNS description of the\nwavefunction, we determine a linear-depth quantum circuit to sequentially\nrealize these states on a quantum processor, including a holographic scheme for\nwhich a pair of two-dimensional qubit arrays suffices to encode the\nthree-dimensional state using measurements. Our approach provides a\nconstruction to enrich phases with exotic topological or fracton order based on\nthe language of tensor networks and offers a tractable route to implement and\ncharacterize fracton order with quantum processors."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11258",
    "c_title":[
      "Enhancing Uncertainty Estimation in Semantic Segmentation via\n  Monte-Carlo Frequency Dropout"
    ],
    "c_abstract":[
      "Monte-Carlo (MC) Dropout provides a practical solution for estimating\npredictive distributions in deterministic neural networks. Traditional dropout,\napplied within the signal space, may fail to account for frequency-related\nnoise common in medical imaging, leading to biased predictive estimates. A\nnovel approach extends Dropout to the frequency domain, allowing stochastic\nattenuation of signal frequencies during inference. This creates diverse global\ntextural variations in feature maps while preserving structural integrity -- a\nfactor we hypothesize and empirically show is contributing to accurately\nestimating uncertainties in semantic segmentation. We evaluated traditional\nMC-Dropout and the MC-frequency Dropout in three segmentation tasks involving\ndifferent imaging modalities: (i) prostate zones in biparametric MRI, (ii)\nliver tumors in contrast-enhanced CT, and (iii) lungs in chest X-ray scans. Our\nresults show that MC-Frequency Dropout improves calibration, convergence, and\nsemantic uncertainty, thereby improving prediction scrutiny, boundary\ndelineation, and has the potential to enhance medical decision-making."
    ],
    "c_categories":[
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05954",
    "c_title":[
      "Derived Models in PFA"
    ],
    "c_abstract":[
      "We discuss a conjecture of Wilson that under the proper forcing axiom,\n$\\Theta_0$ of the derived model at $\\kappa$ is below $\\kappa^+$. We prove the\nconjecture holds for the old derived model. Assuming mouse capturing in the new\nderived model, the conjecture holds there as well. We also show $\\Theta <\n\\kappa^+$ in the case of the old derived model, and under additional hypotheses\nfor the new derived model."
    ],
    "c_categories":[
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01077",
    "c_title":[
      "Learning Stochastic Dynamical Systems with Structured Noise"
    ],
    "c_abstract":[
      "Stochastic differential equations (SDEs) are a ubiquitous modeling framework\nthat finds applications in physics, biology, engineering, social science, and\nfinance. Due to the availability of large-scale data sets, there is growing\ninterest in learning mechanistic models from observations with stochastic\nnoise. In this work, we present a nonparametric framework to learn both the\ndrift and diffusion terms in systems of SDEs where the stochastic noise is\nsingular. Specifically, inspired by second-order equations from classical\nphysics, we consider systems which possess structured noise, i.e. noise with a\nsingular covariance matrix. We provide an algorithm for constructing estimators\ngiven trajectory data and demonstrate the effectiveness of our methods via a\nnumber of examples from physics and biology. As the developed framework is most\nnaturally applicable to systems possessing a high degree of dimensionality\nreduction (i.e. symmetry), we also apply it to the high dimensional\nCucker-Smale flocking model studied in collective dynamics and show that it is\nable to accurately infer the low dimensional interaction kernel from particle\ndata."
    ],
    "c_categories":[
      "cs.LG",
      "cs.NA",
      "math.NA",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.05310",
    "c_title":[
      "Skill and spatial mismatches for sustainable development in Brazil"
    ],
    "c_abstract":[
      "Structural change is necessary for all countries transitioning to a more\nenvironmentally sustainable economy, but what are the likely impacts on\nworkers? Studies often find that green transition scenarios result in net\npositive job creation numbers overall but rarely provide insights into the more\ngranular dynamics of the labour market. This paper combines a dynamic labour\nmarket simulation model with development scenarios focused on agriculture and\ngreen manufacturing. We study how, within the context of a green transition,\nproductivity shifts in different sectors and regions, with differing\nenvironmental impacts, may affect and be constrained by the labour market in\nBrazil. By accounting for labour market frictions associated with skill and\nspatial mismatches, we find that productivity shocks, if not well managed, can\nexacerbate inequality. Agricultural workers tend to be the most negatively\naffected as they are less occupationally and geographically mobile. Our results\nhighlight the importance of well-targeted labour market policies to ensure the\ngreen transition is just and equitable."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00749",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
    ],
    "b_abstract":[
      "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13617",
    "c_title":[
      "The r-Dynamic Chromatic Number is Bounded in the Strong 2-Coloring\n  Number"
    ],
    "c_abstract":[
      "A proper vertex-coloring of a graph is $r$-dynamic if the neighbors of each\nvertex $v$ receive at least $\\min(r, \\mathrm{deg}(v))$ different colors. In\nthis note, we prove that if $G$ has a strong $2$-coloring number at most $k$,\nthen $G$ admits an $r$-dynamic coloring with no more than $(k-1)r+1$ colors. As\na consequence, for every class of graphs of bounded expansion, the $r$-dynamic\nchromatic number is bounded by a linear function in $r$. We give a concrete\nupper bound for graphs of bounded row-treewidth, which includes for example all\nplanar graphs."
    ],
    "c_categories":[
      "cs.DM",
      "math.CO"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"Inverse methods for freeform optical design",
    "a_abstract":"We present a systematic derivation of three mathematical models of increasing\ncomplexity for optical design, based on Hamilton's characteristic functions and\nconservation of luminous flux, and briefly explain the connection with the\nmathematical theory of optimal transport. We outline several iterative\nleast-squares solvers for our models and demonstrate their performance for a\nfew challenging problems.",
    "explanation":"We present a systematic derivation of three mathematical models of increasing\ncomplexity for optical design .We outline several iterative least-squares solvers for our models and demonstrate their\nperformance for a few challenging problems.",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b18"
    ],
    "c_title":[
      "Inverse methods for illumination optics"
    ],
    "c_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "c_categories":[
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":true,
    "research_type":"basic"
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.13132",
    "c_title":[
      "Random Bridges in Spaces of Growing Dimension"
    ],
    "c_abstract":[
      "We investigate the limiting behaviour of the path of random bridges treated\nas random sets in $\\mathbb{R}^{d}$ with the Euclidean metric and the dimension\n$d$ increasing to infinity. The main result states that. in the square\nintegrable case, the limit (in the Gromov-Hausdorff sense) is deterministic,\nnamely, it is $[0,1]$ equipped with the pseudo-metric $\\sqrt{|t-s|(1-|t-s|)}$.\nWe also show that, in the heavy-tailed case with summands regularly varying of\norder $\\alpha \\in (0,1)$, the limiting metric space has a random metric derived\nfrom the bridge variant of a subordinator."
    ],
    "c_categories":[
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.08176",
    "c_title":[
      "A distribution related to Farey sequences -- II"
    ],
    "c_abstract":[
      "This version corrects minor inaccuracies and missprints. One drawing is\nchanged.\n  We continue to study some arithmetical properties of Farey sequences by the\nmethod introduced by F.Boca, C.Cobeli and A.Zaharescu (2001). Let $\\Phi_{Q}$ be\nthe classical Farey sequence of order $Q$. Having the fixed integers\n$D\\geqslant 2$ and $0\\leqslant c\\leqslant D-1$, we colour to the red the\nfractions in $\\Phi_{Q}$ with denominators $\\equiv c \\; \\pmod D$. Consider the\ngaps in $\\Phi_{Q}$ with coloured endpoints, that do not contain the fractions\n$a\/q$ with $q\\equiv c\\;\\pmod D$ inside. The question is to find the limit\nproportions $\\nu(r;D,c)$ (as $Q\\to +\\infty$) of such gaps with precisely $r$\nfractions inside in the whole set of the gaps under considering ($r =\n0,1,2,3,\\ldots$). In fact, the expression for this proportion can be derived\nfrom the general result obtained by C.Cobeli, M.V\\^{a}j\\^{a}itu and A.Zaharescu\n(2014). However, such formula expresses $\\nu(r;D,c)$ in the terms of areas of\nsome polygons related to a special geometrical transform. In the present paper,\nwe obtain explicit formulas for $\\nu(r;D,c)$ for the cases $3$ and $c=1,2$.\nThus this paper cover the case $D=3$."
    ],
    "c_categories":[
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.08789",
    "c_title":[
      "On Erlang Queue with Multiple Arrivals and its Time-changed Variant"
    ],
    "c_abstract":[
      "We introduce and study a queue with the Erlang service system and whose\narrivals are governed by a counting process in which there is a possibility of\nfinitely many arrivals in an infinitesimal time interval. We call it the Erlang\nqueue with multiple arrivals. Some of its distributional properties are\nobtained that includes the state-phase probabilities, the mean queue length and\nthe distribution of busy period etc. Also, we study a time-changed variant of\nit by subordinating it with an independent inverse stable subordinator where we\nobtain its state probabilities and the mean queue length."
    ],
    "c_categories":[
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17342",
    "c_title":[
      "On the subgroup separability of the free product of groups"
    ],
    "c_abstract":[
      "Suppose that $\\mathcal{C}$ is a root class of groups (i.e., a class of groups\nthat contains non-trivial groups and is closed under taking subgroups and\nunrestricted wreath products), $G$ is the free product of residually\n$\\mathcal{C}$-groups $A_{i}$ ($i \\in \\mathcal{I}$), and $H$ is a subgroup of\n$G$ satisfying a non-trivial identity. We prove a criterion for the\n$\\mathcal{C}$-separability of $H$ in $G$. It follows from this criterion that,\nif $\\{\\mathcal{V}_{j} \\mid j \\in \\mathcal{J}\\}$ is a family of group varieties,\neach $\\mathcal{V}_{j}$ ($j \\in \\mathcal{J}$) is distinct from the variety of\nall groups, and $\\mathcal{V} = \\bigcup_{j \\in \\mathcal{J}} \\mathcal{V}_{j}$,\nthen one can give a description of $\\mathcal{C}$-separable\n$\\mathcal{V}$-subgroups of $G$ provided such a description is known for every\ngroup $A_{i}$ ($i \\in \\mathcal{I}$)."
    ],
    "c_categories":[
      "math.GR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.05553",
    "c_title":[
      "Classification of cohomogeneity-one actions on symmetric spaces of\n  noncompact type"
    ],
    "c_abstract":[
      "We complete the classification of isometric cohomogeneity-one actions on all\nsymmetric spaces of noncompact type up to orbit equivalence."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.17422",
    "c_title":[
      "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms"
    ],
    "c_abstract":[
      "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32\/2.29 token\/s\nfor token generation and 6.54\/3.68 token\/s for prompt processing, with a speed\nup of up 2.9x\/3.0x compared to our baseline."
    ],
    "c_categories":[
      "cs.LG",
      "cs.PF"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.15812",
    "c_title":[
      "Data Spatial Programming"
    ],
    "c_abstract":[
      "We introduce a novel programming model, Data Spatial Programming, which\nextends the semantics of Object-Oriented Programming (OOP) by introducing new\nclass-like constructs called archetypes. These archetypes encapsulate spatial\nrelationships between data entities and execution flow in a structured manner,\nenabling more expressive and semantically rich computations over interconnected\ndata structures. By formalizing the relationships between data elements in\nspace, our approach allows for more intuitive modeling of complex systems where\nthe topology of connections is essential to the underlying computational model.\nThis paradigm addresses limitations in traditional OOP when representing\ndynamically evolving networks, agent-based systems, and other\nspatially-oriented computational problems."
    ],
    "c_categories":[
      "cs.MA",
      "cs.PL",
      "cs.SE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.10618",
    "c_title":[
      "PLAID: Supporting Computing Instructors to Identify Domain-Specific\n  Programming Plans at Scale"
    ],
    "c_abstract":[
      "Pedagogical approaches focusing on stereotypical code solutions, known as\nprogramming plans, can increase problem-solving ability and motivate diverse\nlearners. However, plan-focused pedagogies are rarely used beyond introductory\nprogramming. Our formative study (N=10 educators) showed that identifying plans\nis a tedious process. To advance plan-focused pedagogies in application-focused\ndomains, we created an LLM-powered pipeline that automates the effortful parts\nof educators' plan identification process by providing use-case-driven program\nexamples and candidate plans. In design workshops (N=7 educators), we\nidentified design goals to maximize instructors' efficiency in plan\nidentification by optimizing interaction with this LLM-generated content. Our\nresulting tool, PLAID, enables instructors to access a corpus of relevant\nprograms to inspire plan identification, compare code snippets to assist plan\nrefinement, and facilitates them in structuring code snippets into plans. We\nevaluated PLAID in a within-subjects user study (N=12 educators) and found that\nPLAID led to lower cognitive demand and increased productivity compared to the\nstate-of-the-art. Educators found PLAID beneficial for generating instructional\nmaterial. Thus, our findings suggest that human-in-the-loop approaches hold\npromise for supporting plan-focused pedagogies at scale."
    ],
    "c_categories":[
      "cs.HC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.14947",
    "c_title":[
      "Image Restoration Models with Optimal Transport and Total Variation\n  Regularization"
    ],
    "c_abstract":[
      "In this paper, we propose image restoration models using optimal transport\n(OT) and total variation regularization. We present theoretical results of the\nproposed models based on the relations between the dual Lipschitz norm from OT\nand the G-norm introduced by Yves Meyer. We design a numerical method based on\nthe Primal-Dual Hybrid Gradient (PDHG) algorithm for the Wasserstain distance\nand the augmented Lagrangian method (ALM) for the total variation, and the\nconvergence analysis of the proposed numerical method is established. We also\nconsider replacing the total variation in our model by one of its modifications\ndeveloped in \\cite{zhu}, with the aim of suppressing the stair-casing effect\nand preserving image contrasts. Numerical experiments demonstrate the features\nof the proposed models."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Introduction to Nonimaging Optics, second edition"
    ],
    "b_abstract":[
      "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
    ],
    "b_categories":[
      "physics.optics"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.00765",
    "c_title":[
      "AGNNCert: Defending Graph Neural Networks against Arbitrary\n  Perturbations with Deterministic Certification"
    ],
    "c_abstract":[
      "Graph neural networks (GNNs) achieve the state-of-the-art on graph-relevant\ntasks such as node and graph classification. However, recent works show GNNs\nare vulnerable to adversarial perturbations include the perturbation on edges,\nnodes, and node features, the three components forming a graph. Empirical\ndefenses against such attacks are soon broken by adaptive ones. While certified\ndefenses offer robustness guarantees, they face several limitations: 1) almost\nall restrict the adversary's capability to only one type of perturbation, which\nis impractical; 2) all are designed for a particular GNN task, which limits\ntheir applicability; and 3) the robustness guarantees of all methods except one\nare not 100% accurate.\n  We address all these limitations by developing AGNNCert, the first certified\ndefense for GNNs against arbitrary (edge, node, and node feature) perturbations\nwith deterministic robustness guarantees, and applicable to the two most common\nnode and graph classification tasks. AGNNCert also encompass existing certified\ndefenses as special cases. Extensive evaluations on multiple benchmark\nnode\/graph classification datasets and two real-world graph datasets, and\nmultiple GNNs validate the effectiveness of AGNNCert to provably defend against\narbitrary perturbations. AGNNCert also shows its superiority over the\nstate-of-the-art certified defenses against the individual edge perturbation\nand node perturbation."
    ],
    "c_categories":[
      "cs.CR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.16249",
    "c_title":[
      "A GHz fiber comb on silica"
    ],
    "c_abstract":[
      "We present a 1 GHz Yb-fiber laser frequency comb built on silica substrates,\nutilizing \"optical cubes\" to house all optical components, ensuring long-term\nstability and practical operation. Both the femtosecond laser and f-to-2f\ninterferometer are constructed to silica bricks, with a compact footprint of\n290 mm $\\times$ 250 mm, and a total weight of 1.8 kg. This system provides a\nstable repetition rate, offset frequency, and a supercontinuum spanning\n460-1560 nm without requiring amplification. The carrier-envelop offset\nfrequency exhibits exceptional stability, with a fractional frequency\ninstability of $3.07\\times 10^{-18}$ at a 1 second averaging time, improving to\n$2.12\\times 10^{-20}$ at a 10,000 second, maintaining uninterrupted operation\nfor over 60 hours. This work demonstrates a high-performance GHz fiber-based\nfrequency comb, paving the way for applications beyond laboratory environments,\nincluding dual-comb spectroscopy, astronomical spectrograph calibration, and\nportable optical clocks."
    ],
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.11251",
    "c_title":[
      "Demonstration of Time-reversal Symmetric Two-Dimensional Photonic\n  Topological Anderson Insulator"
    ],
    "c_abstract":[
      "Recently, the impact of disorder on topological properties has attracted\nsignificant attention in photonics, especially the intriguing disorder-induced\ntopological phase transitions in photonic topological Anderson insulators\n(PTAIs). However, the reported PTAIs are based on time-reversal symmetry broken\nsystems or quasi-three-dimensional time-reversal invariant system, both of\nwhich would limit the applications in integrated optics. Here, we realize a\ntime-reversal symmetric two-dimensional PTAI on silicon platform within the\nnear-IR wavelength range, taking the advantageous valley degree of freedom of\nphotonic crystal. A low-threshold topological Anderson phase transition is\nobserved by applying disorder to the critical topologically trivial phase.\nConversely, we have also realized extremely robust topologically protected edge\nstates based on the stable topological phase. Both two phenomena are validated\nthrough theoretical Dirac Hamiltonian analysis, numerical simulations, and\nexperimental measurements. Our proposed structure holds promise to achieve\nnear-zero topological phase transition thresholds, which breaks the\nconventional cognition that strong disorder is required to induce the phase\ntransition. It significantly alleviates the difficulty of manipulating disorder\nand could be extended to other systems, such as condensed matter systems where\nstrong disorder is hard to implement. This work is also beneficial to construct\nhighly robust photonic integrated circuits serving for on-chip photonic and\nquantum optic information processing. Moreover, this work also provides an\noutstanding platform to investigate on-chip integrated disordered systems."
    ],
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.03170",
    "c_title":[
      "Revisiting PSF models: unifying framework and high-performance\n  implementation"
    ],
    "c_abstract":[
      "Localization microscopy often relies on detailed models of point spread\nfunctions. For applications such as deconvolution or PSF engineering, accurate\nmodels for light propagation in imaging systems with high numerical aperture\nare required. Different models have been proposed based on 2D Fourier\ntransforms or 1D Bessel integrals. The most precise ones combine a vectorial\ndescription of the electric field and precise aberration models. However, it\nmay be unclear which model to choose, as there is no comprehensive comparison\nbetween the Fourier and Bessel approaches yet. Moreover, many existing\nlibraries are written in Java (e.g. our previous PSF generator software) or\nMATLAB, which hinders the integration into deep learning algorithms. In this\nwork, we start from the original Richards-Wolf integral and revisit both\napproaches in a systematic way. We present a unifying framework in which we\nprove the equivalence between the Fourier and Bessel strategies and detail a\nvariety of correction factors applicable to both of them. Then, we provide a\nhigh-performance implementation of our theoretical framework in the form of an\nopen-source library that is built on top of PyTorch, a popular library for deep\nlearning. It enables us to benchmark the accuracy and computational speed of\ndifferent models, thus allowing for an in-depth comparison of the existing\nmodels for the first time. We show that the Bessel strategy is optimal for\naxisymmetric beams while the Fourier approach can be applied to more general\nscenarios. Our work enables efficient PSF computation on CPU or GPU, which can\nthen be included in simulation and optimization pipelines."
    ],
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.00353",
    "c_title":[
      "Flexible delivery of high-power picosecond laser in purely-single\n  optical mode of anti-resonant hollow-core fiber for micromachining"
    ],
    "c_abstract":[
      "We present the flexible delivery of picosecond laser pulses with up to 20 W\naverage power over a 3-m-long sample of anti-resonant hollow-core fiber\n(AR-HCF) for laser micromachining applications. Our experiments highlight the\nimportance of optical mode purity of the AR-HCF for the manufacturing\nprecision. We demonstrate that compared with an AR-HCF sample with a capillary\nto core (d\/D) ratio of ~0.5, the AR-HCF with a d\/D ratio of ~0.68 exhibits\nbetter capability of high-order-mode suppression, giving rise to improved\nmicromachining quality. Moreover, the AR-HCF delivery system exhibits better\npointing stability and set-up flexibility than the free-space beam delivery\nsystem. These results pave the way to practical applications of AR-HCF in\ndeveloping advanced equipment for ultrafast laser micromachining."
    ],
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.01017",
    "c_title":[
      "Frequency-Division Phase Random Optimization for High-Speed Arbitrary\n  Optical Intensity Waveform Monitoring Using Opto-Electronic Finite Impulse\n  Response Filters"
    ],
    "c_abstract":[
      "We propose and demonstrate a high-bandwidth optical intensity waveform\nmonitoring technique based on frequency-division phase random optimization\n(FD-PRO) using an opto-electronic finite impulse response (OE-FIR) filter. In\nthis technology, phase random optimization (PRO) enable the estimation of the\nsignal spectral phase. Frequency-division analysis (FDA) combined with PRO\nsaves the iteration required for the optimization, accelerating the signal\nspectral phase estimation. FDA also facilitates the estimation of the signal\nspectral amplitude. Using FD-PRO, arbitrary optical intensity waveforms can be\neasily reconstructed without relying on high-speed digital signal processing.\nExperimental results reveal that the temporal waveforms are successfully\nreconstructed at 18-ps resolution."
    ],
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.17567",
    "c_title":[
      "Exploring the Potential of Wireless-enabled Multi-Chip AI Accelerators"
    ],
    "c_abstract":[
      "The insatiable appetite of Artificial Intelligence (AI) workloads for\ncomputing power is pushing the industry to develop faster and more efficient\naccelerators. The rigidity of custom hardware, however, conflicts with the need\nfor scalable and versatile architectures capable of catering to the needs of\nthe evolving and heterogeneous pool of Machine Learning (ML) models in the\nliterature. In this context, multi-chiplet architectures assembling multiple\n(perhaps heterogeneous) accelerators are an appealing option that is\nunfortunately hindered by the still rigid and inefficient chip-to-chip\ninterconnects. In this paper, we explore the potential of wireless technology\nas a complement to existing wired interconnects in this multi-chiplet approach.\nUsing an evaluation framework from the state-of-the-art, we show that wireless\ninterconnects can lead to speedups of 10% on average and 20% maximum. We also\nhighlight the importance of load balancing between the wired and wireless\ninterconnects, which will be further explored in future work."
    ],
    "c_categories":[
      "cs.AI",
      "cs.AR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.03698",
    "c_title":[
      "How vulnerable is my policy? Adversarial attacks on modern behavior\n  cloning policies"
    ],
    "c_abstract":[
      "Learning from Demonstration (LfD) algorithms have shown promising results in\nrobotic manipulation tasks, but their vulnerability to adversarial attacks\nremains underexplored. This paper presents a comprehensive study of adversarial\nattacks on both classic and recently proposed algorithms, including Behavior\nCloning (BC), LSTM-GMM, Implicit Behavior Cloning (IBC), Diffusion Policy (DP),\nand VQ-Behavior Transformer (VQ-BET). We study the vulnerability of these\nmethods to untargeted, targeted and universal adversarial perturbations. While\nexplicit policies, such as BC, LSTM-GMM and VQ-BET can be attacked in the same\nmanner as standard computer vision models, we find that attacks for implicit\nand denoising policy models are nuanced and require developing novel attack\nmethods. Our experiments on several simulated robotic manipulation tasks reveal\nthat most of the current methods are highly vulnerable to adversarial\nperturbations. We also show that these attacks are transferable across\nalgorithms, architectures, and tasks, raising concerning security\nvulnerabilities with potentially a white-box threat model. In addition, we test\nthe efficacy of a randomized smoothing, a widely used adversarial defense\ntechnique, and highlight its limitation in defending against attacks on complex\nand multi-modal action distribution common in complex control tasks. In\nsummary, our findings highlight the vulnerabilities of modern BC algorithms,\npaving way for future work in addressing such limitations."
    ],
    "c_categories":[
      "cs.CR",
      "cs.LG",
      "cs.RO"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.15718",
    "c_title":[
      "Am I eligible? Natural Language Inference for Clinical Trial Patient\n  Recruitment: the Patient's Point of View"
    ],
    "c_abstract":[
      "Recruiting patients to participate in clinical trials can be challenging and\ntime-consuming. Usually, participation in a clinical trial is initiated by a\nhealthcare professional and proposed to the patient. Promoting clinical trials\ndirectly to patients via online recruitment might help to reach them more\nefficiently. In this study, we address the case where a patient is initiating\ntheir own recruitment process and wants to determine whether they are eligible\nfor a given clinical trial, using their own language to describe their medical\nprofile. To study whether this creates difficulties in the patient trial\nmatching process, we design a new dataset and task, Natural Language Inference\nfor Patient Recruitment (NLI4PR), in which patient language profiles must be\nmatched to clinical trials. We create it by adapting the TREC 2022 Clinical\nTrial Track dataset, which provides patients' medical profiles, and rephrasing\nthem manually using patient language. We also use the associated clinical trial\nreports where the patients are either eligible or excluded. We prompt several\nopen-source Large Language Models on our task and achieve from 56.5 to 71.8 of\nF1 score using patient language, against 64.7 to 73.1 for the same task using\nmedical language. When using patient language, we observe only a small loss in\nperformance for the best model, suggesting that having the patient as a\nstarting point could be adopted to help recruit patients for clinical trials.\nThe corpus and code bases are all freely available on our Github and\nHuggingFace repositories."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.09718",
    "c_title":[
      "FLOL: Fast Baselines for Real-World Low-Light Enhancement"
    ],
    "c_abstract":[
      "Low-Light Image Enhancement (LLIE) is a key task in computational photography\nand imaging. The problem of enhancing images captured during night or in dark\nenvironments has been well-studied in the image signal processing literature.\nHowever, current deep learning-based solutions struggle with efficiency and\nrobustness in real-world scenarios (e.g. scenes with noise, saturated pixels,\nbad illumination). We propose a lightweight neural network that combines image\nprocessing in the frequency and spatial domains. Our method, FLOL+, is one of\nthe fastest models for this task, achieving state-of-the-art results on popular\nreal scenes datasets such as LOL and LSRW. Moreover, we are able to process\n1080p images under 12ms. Code and models at https:\/\/github.com\/cidautai\/FLOL"
    ],
    "c_categories":[
      "cs.CV",
      "cs.RO"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "Inverse methods for illumination optics"
    ],
    "b_abstract":[
      "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
    ],
    "b_categories":[
      "math.MP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.14060",
    "c_title":[
      "New Lower Bounds for Stochastic Non-Convex Optimization through\n  Divergence Composition"
    ],
    "c_abstract":[
      "We study fundamental limits of first-order stochastic optimization in a range\nof nonconvex settings, including L-smooth functions satisfying Quasar-Convexity\n(QC), Quadratic Growth (QG), and Restricted Secant Inequalities (RSI). While\nthe convergence properties of standard algorithms are well-understood in\ndeterministic regimes, significantly fewer results address the stochastic case,\nwhere only unbiased and noisy gradients are available. We establish new lower\nbounds on the number of noisy gradient queries to minimize these classes of\nfunctions, also showing that they are tight (up to a logarithmic factor) in all\nthe relevant quantities characterizing each class. Our approach reformulates\nthe optimization task as a function identification problem, leveraging\ndivergence composition arguments to construct a challenging subclass that leads\nto sharp lower bounds. Furthermore, we present a specialized algorithm in the\none-dimensional setting that achieves faster rates, suggesting that certain\ndimensional thresholds are intrinsic to the complexity of non-convex stochastic\noptimization."
    ],
    "c_categories":[
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"Deep Multi-contrast Cardiac MRI Reconstruction via vSHARP with Auxiliary\n  Refinement Network",
    "a_abstract":"Cardiac MRI (CMRI) is a cornerstone imaging modality that provides in-depth\ninsights into cardiac structure and function. Multi-contrast CMRI (MCCMRI),\nwhich acquires sequences with varying contrast weightings, significantly\nenhances diagnostic capabilities by capturing a wide range of cardiac tissue\ncharacteristics. However, MCCMRI is often constrained by lengthy acquisition\ntimes and susceptibility to motion artifacts. To mitigate these challenges,\naccelerated imaging techniques that use k-space undersampling via different\nsampling schemes at acceleration factors have been developed to shorten scan\ndurations. In this context, we propose a deep learning-based reconstruction\nmethod for 2D dynamic multi-contrast, multi-scheme, and multi-acceleration MRI.\nOur approach integrates the state-of-the-art vSHARP model, which utilizes\nhalf-quadratic variable splitting and ADMM optimization, with a Variational\nNetwork serving as an Auxiliary Refinement Network (ARN) to better adapt to the\ndiverse nature of MCCMRI data. Specifically, the subsampled k-space data is fed\ninto the ARN, which produces an initial prediction for the denoising step used\nby vSHARP. This, along with the subsampled k-space, is then used by vSHARP to\ngenerate high-quality 2D sequence predictions. Our method outperforms\ntraditional reconstruction techniques and other vSHARP-based models.",
    "explanation":"Cardiac MRI (CMRI) is a cornerstone imaging modality that\nprovides in-depth insights into cardiac structure and function.  Our approach integrates the state-of-the-art vSHARP model, which uti-\nlizes half-quadratic variable splitting and ADMM optimization, with a\nVariational Network serving as an Auxiliary Refinement Network (ARN)\nto better adapt to the diverse nature of MCCMRI data. ",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b19"
    ],
    "c_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "c_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.06561",
    "c_title":[
      "Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for\n  Mid-term Human Mobility Prediction"
    ],
    "c_abstract":[
      "Predicting individual mobility patterns is crucial across various\napplications. While current methods mainly focus on predicting the next\nlocation for personalized services like recommendations, they often fall short\nin supporting broader applications such as traffic management and epidemic\ncontrol, which require longer period forecasts of human mobility. This study\naddresses mid-term mobility prediction, aiming to capture daily travel patterns\nand forecast trajectories for the upcoming day or week. We propose a novel\nMulti-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to\nefficiently extract spatial and temporal information by decoupling daily\ntrajectories into distinct location-duration chains. Our approach employs a\nhierarchical encoder to model multi-scale temporal patterns, including daily\nrecurrence and weekly periodicity, and utilizes a transformer-based decoder to\nglobally attend to predicted information in the location or duration chain.\nAdditionally, we introduce a spatial heterogeneous graph learner to capture\nmulti-scale spatial relationships, enhancing semantic-rich representations.\nExtensive experiments, including statistical physics analysis, are conducted on\nlarge-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay\nArea, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to\nepidemic modeling in Boston, MSTDP significantly outperforms the\nbest-performing baseline, achieving a remarkable 62.8% reduction in MAE for\ncumulative new cases."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17493",
    "c_title":[
      "Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability"
    ],
    "c_abstract":[
      "Due to the wide employment of automated reasoning in the analysis and\nconstruction of correct systems, the results reported by automated reasoning\nengines must be trustworthy. For Boolean satisfiability (SAT) solvers - and\nmore recently SAT-based maximum satisfiability (MaxSAT) solvers -\ntrustworthiness is obtained by integrating proof logging into solvers, making\nsolvers capable of emitting machine-verifiable proofs to certify correctness of\nthe reasoning steps performed. In this work, we enable for the first time proof\nlogging based on the VeriPB proof format for multi-objective MaxSAT (MO-MaxSAT)\noptimization techniques. Although VeriPB does not offer direct support for\nmulti-objective problems, we detail how preorders in VeriPB can be used to\nprovide certificates for MO-MaxSAT algorithms computing a representative\nsolution for each element in the non-dominated set of the search space under\nPareto-optimality, without extending the VeriPB format or the proof checker. By\nimplementing VeriPB proof logging into a state-of-the-art multi-objective\nMaxSAT solver, we show empirically that proof logging can be made scalable for\nMO-MaxSAT with reasonable overhead."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11429",
    "c_title":[
      "The Explanation Game -- Rekindled (Extended Version)"
    ],
    "c_abstract":[
      "Recent work demonstrated the existence of critical flaws in the current use\nof Shapley values in explainable AI (XAI), i.e. the so-called SHAP scores.\nThese flaws are significant in that the scores provided to a human\ndecision-maker can be misleading. Although these negative results might appear\nto indicate that Shapley values ought not be used in XAI, this paper argues\notherwise. Concretely, this paper proposes a novel definition of SHAP scores\nthat overcomes existing flaws. Furthermore, the paper outlines a practically\nefficient solution for the rigorous estimation of the novel SHAP scores.\nPreliminary experimental results confirm our claims, and further underscore the\nflaws of the current SHAP scores."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.01149",
    "c_title":[
      "A3: Android Agent Arena for Mobile GUI Agents"
    ],
    "c_abstract":[
      "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at https:\/\/yuxiangchai.github.io\/Android-Agent-Arena\/."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12143",
    "c_title":[
      "Small Models Struggle to Learn from Strong Reasoners"
    ],
    "c_abstract":[
      "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12487",
    "c_title":[
      "On-chip GaAs carrier lifetime investigation"
    ],
    "c_abstract":[
      "We propose an alternative way to determine GaAs carrier lifetime using\npump-probe measurement based on fibre optics and integrated waveguides. We find\nthat our GaAs samples have the lifetime ranging from 30-80 ps, supporting the\nbandwidth $\\geq$ 12.5 GHz. The platform utilised in this work could offer a\ncost-effective way to investigate photocarrier lifetime. Moreover, it may have\nthe potential for high-speed switches and detectors, or could be exploited for\npossible all-optical modulation."
    ],
    "c_categories":[
      "physics.app-ph",
      "physics.ins-det",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19344",
    "c_title":[
      "Theory of Quantum-Enhanced Stimulated Raman Scattering"
    ],
    "c_abstract":[
      "Stimulated Raman scattering (SRS) is a powerful method for label-free imaging\nand spectroscopy of materials. Recent experiments have shown that\nquantum-enhanced Raman scattering can surpass the shot noise limit and improve\nthe sensitivity substantially. Here, we introduce a full theory of\nquantum-enhanced SRS based on the framework of quantum metrology. Our results\nenable the assessment of quantum-enhancements of arbitrary measurement\nstrategies and identify optimal measurement observables that extract maximal\ninformation about the signal. We use this to identify the optimal employment of\nsqueezed states in SRS, highlighting the potential to improve quantum gains\nbeyond those observed in recent experiments. Our work establishes the\ntheoretical foundation for understanding and approaching the quantum limits of\nprecision in SRS, and provide a tool to discuss nonlinear spectroscopy and\nimaging more broadly."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.05839",
    "c_title":[
      "De Finetti's problem with fixed transaction costs and regime switching"
    ],
    "c_abstract":[
      "In this paper, we examine a modified version of de Finetti's optimal dividend\nproblem, incorporating fixed transaction costs and altering the surplus process\nby introducing two-valued drift and two-valued volatility coefficients. This\nmodification aims to capture the transitions or adjustments in the company's\nfinancial status. We identify the optimal dividend strategy, which maximizes\nthe expected total net dividend payments (after accounting for transaction\ncosts) until ruin, as a two-barrier impulsive dividend strategy. Notably, the\noptimal strategy can be explicitly determined for almost all scenarios\ninvolving different drifts and volatility coefficients. Our primary focus is on\nexploring how changes in drift and volatility coefficients influence the\noptimal dividend strategy."
    ],
    "c_categories":[
      "q-fin.MF",
      "q-fin.RM"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07167",
    "c_title":[
      "Quantifying Quantumness in (A)dS spacetimes with Unruh-DeWitt Detector"
    ],
    "c_abstract":[
      "Probing quantumness in curved spacetime is regarded as one of fundamental and\nimportant topics in the framework of relativistic quantum information. In this\nwork, we focus on the theoretical feasibility of probing quantum properties in\nde Sitter (dS) and Anti-de Sitter (AdS) spacetimes via detectors. By employing\nthe Unruh-DeWitt detector coupled with a massless scalar field, which is\ntreated as an open system, quantum uncertainty and quantum coherence in both dS\nand AdS spacetimes are investigated. Our analysis reveals that the acceleration\nin dS spacetime and the boundary conditions in AdS spacetime significantly\nimpact the detector's evolution in the initial stage. Notably, both of the\nuncertainty and coherence will oscillate with the initial state being in a\nsuperposition state, however the high temperature is able to suppress their\noscillation. Interestingly, it is found that the constant values of the final\nuncertainty and coherence are identical as those in dS and AdS spacetimes,\nwhich are determined by the ratio of energy gap to temperature. Hence, the\ncurrent exploration offers insight into quantumness in dS and AdS spacetimes,\nand might be helpful to facilitate the curved-spacetime-based quantum\ninformation processing."
    ],
    "c_categories":[
      "hep-th",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
    ],
    "b_abstract":[
      "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11081",
    "c_title":[
      "Robust Functional Ward's Linkages with Applications in EEG data\n  Clustering"
    ],
    "c_abstract":[
      "This paper proposes two new distance measures, called functional Ward's\nlinkages, for functional data clustering that are robust against outliers.\nConventional Ward's linkage defines the distance between two clusters as the\nincrease in sum of squared errors (SSE) upon merging, which can be interpreted\ngraphically as an increase in the diameter. Analogously, functional Ward's\nlinkage defines the distance of two clusters as the increased width of the band\ndelimited by the merged clusters. To address the limitations of conventional\nWard's linkage in handling outliers and contamination, the proposed linkages\nfocus exclusively on the most central curves by leveraging magnitude-shape\noutlyingness measures and modified band depth, respectively. Simulations and\nreal-world electroencephalogram (EEG) data analysis demonstrate that the\nproposed methods outperform other competitive approaches, particularly in the\npresence of various types of outliers and contamination."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04504",
    "c_title":[
      "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates"
    ],
    "c_abstract":[
      "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13516",
    "c_title":[
      "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols"
    ],
    "c_abstract":[
      "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03661",
    "c_title":[
      "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues"
    ],
    "c_abstract":[
      "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11130",
    "c_title":[
      "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering"
    ],
    "c_abstract":[
      "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17870",
    "c_title":[
      "Comportamientos extra\\~nos del infinito: Gr\\'aficas Infinitas"
    ],
    "c_abstract":[
      "Infinitary Combinatorics shows interesting contrasts, with many similarities\nbut also several important differences with its finite analog. The purpose of\nthis paper is to present some concrete examples, both of similarities and of\nradical differences, in order to provide some intuition about the behaviour of\ninfinity in the combinatorial setting. Our examples are taken from the branch\nof mathematics known as Graph Theory.\n  --\n  La combinatoria infinita (tem\\'atica que, a ra\\'{\\i}z del trabajo de Cantor,\nactualmente es posible estudiar de manera completamente formal) nos presenta un\ninteresante contraste de semejanzas y diferencias con su an\\'alogo finito. El\nprop\\'osito de este art\\'{\\i}culo es presentar algunos ejemplos concretos tanto\nde semejanzas, como de diferencias radicales, para proporcionar cierta\nintuici\\'on acerca del comportamiento del infinito en el \\'ambito combinatorio.\nNuestros ejemplos son tomados de la rama de las matem\\'aticas conocida como\nTeor\\'{\\i}a de Gr\\'aficas."
    ],
    "c_categories":[
      "math.CO",
      "math.HO",
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17227",
    "c_title":[
      "The assembly of supermassive black holes at $z<1$ in early-type galaxies\n  from scaling relations"
    ],
    "c_abstract":[
      "The assembly of supermassive black hole (SMBH) mass ($M_{\\bullet}$) and\nstellar mass ($M_{*}$) in galaxies can be studied via the redshift evolution of\nthe $M_{\\bullet}-M_{*}$ relation, but the ways in which selection bias and\nphysical assembly channels affect this evolution are uncertain. To address\nthis, we compare the $M_{\\bullet}-M_{*}$ relation for local massive\n($M_{*}>10^{10.5}$M$_{\\odot}$) quiescent early-type galaxies (ETGs) to that for\nmassive ETGs hosting active galactic nuclei (AGN) at $z\\sim0.8$. The\nrestrictions on stellar mass and galaxy type limit the assembly channels that\nmay connect the two relations. For the local sample we find $\\log(M_{\\bullet})\n= 8.80 + 1.10(\\log{M_{*}-11})$, in line with prior work. For the $z\\sim0.8$\nsample we find a bias-corrected relation: $\\log(M_{\\bullet}) = 7.80 +\n1.25(\\log{M_{*}-11})$. We show, however, that this relation depends on the\nstellar and SMBH mass functions used to compute the selection bias, the virial\nrelation, the virial factor, and the active fraction, which together introduce\nuncertainty of up to $\\sim0.6$\\,dex in the $z\\sim0.8$ relation. Adopting\nreasonable choices of these parameters then our $z\\sim0.8$ relation lies above\nthat for $z\\sim0$ AGN by $\\sim0.5$\\,dex, but below our $z\\sim0$ ETG relation by\n$0.4-1$\\,dex in SMBH mass. We discuss possible sources of this offset,\nincluding further bias corrections, `downsizing\" in SMBH mass assembly, and\npreferential SMBH growth. Our results highlight the need to reduce\nuncertainties from selection and measurement bias in SMBH and stellar masses at\nall redshifts."
    ],
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05917",
    "c_title":[
      "Modeling Concentration Profiles in Electrolytes by Solving 3-D\n  Poisson-Nernst-Planck Equations via Finite Difference Method"
    ],
    "c_abstract":[
      "The Poisson-Nernst-Planck (PNP) equations are fundamental for modeling ion\ntransport in electrochemical systems, capturing the intricate interplay of\nconcentration gradients, electric fields, and ion fluxes essential for\napplications such as energy storage devices and other electrochemical devices.\nThis study introduces a refined numerical framework employing the finite\ndifference method to solve the 3-D PNP equations, enabling precise simulation\nof ion concentration distributions under realistic boundary conditions and\napplied electric fields. By rigorously addressing stability criteria and\nintegrating advanced boundary constraints, including the Butler-Volmer equation\nfor surface reactions, the model provides comprehensive insights into ion\ndynamics, particularly near electrode surfaces where electric field and\nreaction effects dominate. This framework significantly enhances traditional\nPNP modeling by accommodating varied boundary conditions, diffusion anisotropy,\nand complex electrochemical environments, offering a robust tool for\ninvestigating electrochemical processes and guiding the design of advanced\nelectrochemical systems."
    ],
    "c_categories":[
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04265",
    "c_title":[
      "Fermion Dark Matter Effect on Electroweak Phase Transition"
    ],
    "c_abstract":[
      "The addition of extra scalars to the Standard Model (SM) of particle physics\nenriches the vacuum structure and consequently gives rise to strong first-order\nphase transitions (EWPT) in the early universe. We raise the question that how\nthe EWPT is affected by the addition of fermions in models beyond the SM, and\naddress this question by studying the EWPT in a dark matter model comprising a\nsinglet scalar and two Dirac fermions. The singlet scalar develops a nonzero\nvacuum expectation value (VEV), and the lighter fermion plays the role of the\ndark matter. The model evades the stringent direct detection bounds due to the\npresence of two fermions. We first show that applying the high-temperature\napproximation, no first-order phase transition is found. Then we demonstrate\nthat when including the full finite temperature corrections to the effective\npotential, the first-order phase transition becomes possible, nevertheless, all\nthe phase transitions will be weak. We therefore deduce that the addition of\nfermions reduces the strength of the EWPT."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01291",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b19"
    ],
    "b_title":[
      "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
    ],
    "b_abstract":[
      "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06129",
    "c_title":[
      "Turbulence in stratified rotating topographic wakes"
    ],
    "c_abstract":[
      "Turbulence generation mechanisms in stratified, rotating flows past\nthree-dimensional (3D) topography remain underexplored, particularly in\nsubmesoscale (SMS) regimes critical to geophysical applications. Using\nturbulence-resolving large-eddy simulations, we systematically dissect the\ninterplay of stratification and rotation in governing the dynamics of wake\nturbulence. Our parametric study reveals that turbulent dissipation in the near\nwake is dominated by two distinct instabilities: (1) vertical shear-driven\nKelvin-Helmholtz instability (KHI), amplified by oblique dislocation of\nK\\'arm\\'an vortices under strong stratification, and (2) centrifugal\/inertial\ninstability (CI), which peaks at intermediate rotation rates (Rossby number\norder unity, SMS regime) and relatively weaker stratification. Notably, strong\nrotation dampens vertical shear and weakens KHI-driven turbulence, while strong\nstratification imposes smaller vertical length scales that restrict CI-driven\nturbulence. By quantifying dissipation across a broad parameter space of\nstratification and rotation, predictive relationships between the environmental\nparameters and instability dominance is established. These findings highlight\nthe regime dependence of instability mechanisms and may inform targeted\nobservational campaigns and numerical models of oceanic and atmospheric wakes."
    ],
    "c_categories":[
      "physics.ao-ph",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"Disentangled PET Lesion Segmentation",
    "a_abstract":"PET imaging is an invaluable tool in clinical settings as it captures the\nfunctional activity of both healthy anatomy and cancerous lesions. Developing\nautomatic lesion segmentation methods for PET images is crucial since manual\nlesion segmentation is laborious and prone to inter- and intra-observer\nvariability. We propose PET-Disentangler, a 3D disentanglement method that uses\na 3D UNet-like encoder-decoder architecture to disentangle disease and normal\nhealthy anatomical features with losses for segmentation, reconstruction, and\nhealthy component plausibility. A critic network is used to encourage the\nhealthy latent features to match the distribution of healthy samples and thus\nencourages these features to not contain any lesion-related features. Our\nquantitative results show that PET-Disentangler is less prone to incorrectly\ndeclaring healthy and high tracer uptake regions as cancerous lesions, since\nsuch uptake pattern would be assigned to the disentangled healthy component.",
    "explanation":"PET imaging is an invaluable tool in clinical settings as it\ncaptures the functional activity of both healthy anatomy and\ncancerous lesions. Developing automatic lesion segmentation\nmethods for PET images is crucial since manual lesion seg-\nmentation is laborious and prone to inter- and intra-observer\nvariability. ",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b2"
    ],
    "c_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "c_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.07540",
    "c_title":[
      "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol"
    ],
    "c_abstract":[
      "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.05007",
    "c_title":[
      "Analyzing Advanced AI Systems Against Definitions of Life and\n  Consciousness"
    ],
    "c_abstract":[
      "Could artificial intelligence ever become truly conscious in a functional\nsense; this paper explores that open-ended question through the lens of Life, a\nconcept unifying classical biological criteria (Oxford, NASA, Koshland) with\nempirical hallmarks such as adaptive self maintenance, emergent complexity, and\nrudimentary self referential modeling. We propose a number of metrics for\nexamining whether an advanced AI system has gained consciousness, while\nemphasizing that we do not claim all AI stems can become conscious. Rather, we\nsuggest that sufficiently advanced architectures exhibiting immune like\nsabotage defenses, mirror self-recognition analogs, or meta-cognitive updates\nmay cross key thresholds akin to life-like or consciousness-like traits. To\ndemonstrate these ideas, we start by assessing adaptive self-maintenance\ncapability, and introduce controlled data corruption sabotage into the training\nprocess. The result demonstrates AI capability to detect these inconsistencies\nand revert or self-correct analogous to regenerative biological processes. We\nalso adapt an animal-inspired mirror self recognition test to neural\nembeddings, finding that partially trained CNNs can distinguish self from\nforeign features with complete accuracy. We then extend our analysis by\nperforming a question-based mirror test on five state-of-the-art chatbots\n(ChatGPT4, Gemini, Perplexity, Claude, and Copilot) and demonstrated their\nability to recognize their own answers compared to those of the other chatbots."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11770",
    "c_title":[
      "Cognitive-Aligned Document Selection for Retrieval-augmented Generation"
    ],
    "c_abstract":[
      "Large language models (LLMs) inherently display hallucinations since the\nprecision of generated texts cannot be guaranteed purely by the parametric\nknowledge they include. Although retrieval-augmented generation (RAG) systems\nenhance the accuracy and reliability of generative models by incorporating\nexternal documents, these retrieved documents often fail to adequately support\nthe model's responses in practical applications. To address this issue, we\npropose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment\nRe\\textbf{trieval} for verifiable generation), which leverages an LLM to\ndynamically update queries and filter high-quality, reliable retrieval\ndocuments. Specifically, we parse the user query into its syntactic components\nand perform fine-grained grounded alignment with the retrieved documents. For\nquery components that cannot be individually aligned, we propose a dynamic\nsemantic compensation mechanism that iteratively refines and rewrites the query\nwhile continuously updating the retrieval results. This iterative process\ncontinues until the retrieved documents sufficiently support the query's\nresponse. Our approach introduces a novel criterion for filtering retrieved\ndocuments, closely emulating human strategies for acquiring targeted\ninformation. This ensures that the retrieved content effectively supports and\nverifies the generated outputs. On the ALCE benchmark, our method significantly\nsurpasses a wide range of baselines, achieving state-of-the-art performance."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.08020",
    "c_title":[
      "Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through\n  Multi-Agent Reinforcement Learning"
    ],
    "c_abstract":[
      "The effective design of patrol strategies is a difficult and complex problem,\nespecially in medium and large areas. The objective is to plan, in a\ncoordinated manner, the optimal routes for a set of patrols in a given area, in\norder to achieve maximum coverage of the area, while also trying to minimize\nthe number of patrols. In this paper, we propose a multi-agent reinforcement\nlearning (MARL) model, based on a decentralized partially observable Markov\ndecision process, to plan unpredictable patrol routes within an urban\nenvironment represented as an undirected graph. The model attempts to maximize\na target function that characterizes the environment within a given time frame.\nOur model has been tested to optimize police patrol routes in three\nmedium-sized districts of the city of Malaga. The aim was to maximize\nsurveillance coverage of the most crime-prone areas, based on actual crime data\nin the city. To address this problem, several MARL algorithms have been\nstudied, and among these the Value Decomposition Proximal Policy Optimization\n(VDPPO) algorithm exhibited the best performance. We also introduce a novel\nmetric, the coverage index, for the evaluation of the coverage performance of\nthe routes generated by our model. This metric is inspired by the predictive\naccuracy index (PAI), which is commonly used in criminology to detect hotspots.\nUsing this metric, we have evaluated the model under various scenarios in which\nthe number of agents (or patrols), their starting positions, and the level of\ninformation they can observe in the environment have been modified. Results\nshow that the coordinated routes generated by our model achieve a coverage of\nmore than $90\\%$ of the $3\\%$ of graph nodes with the highest crime incidence,\nand $65\\%$ for $20\\%$ of these nodes; $3\\%$ and $20\\%$ represent the coverage\nstandards for police resource allocation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03512",
    "c_title":[
      "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing\n  Multi-Objective Optimization based DPO for Text-to-Image Alignment"
    ],
    "c_abstract":[
      "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that\ngenerated visuals not only accurately encapsulate user intents but also conform\nto stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini\nfiasco, where misaligned outputs triggered significant public backlash,\nunderscore the critical need for robust alignment mechanisms. In contrast,\nLarge Language Models (LLMs) have achieved notable success in alignment.\nBuilding on these advancements, researchers are eager to apply similar\nalignment techniques, such as Direct Preference Optimization (DPO), to T2I\nsystems to enhance image generation fidelity and reliability.\n  We present YinYangAlign, an advanced benchmarking framework that\nsystematically quantifies the alignment fidelity of T2I systems, addressing six\nfundamental and inherently contradictory design objectives. Each pair\nrepresents fundamental tensions in image generation, such as balancing\nadherence to user prompts with creative modifications or maintaining diversity\nalongside visual coherence. YinYangAlign includes detailed axiom datasets\nfeaturing human prompts, aligned (chosen) responses, misaligned (rejected)\nAI-generated outputs, and explanations of the underlying contradictions."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17870",
    "c_title":[
      "Comportamientos extra\\~nos del infinito: Gr\\'aficas Infinitas"
    ],
    "c_abstract":[
      "Infinitary Combinatorics shows interesting contrasts, with many similarities\nbut also several important differences with its finite analog. The purpose of\nthis paper is to present some concrete examples, both of similarities and of\nradical differences, in order to provide some intuition about the behaviour of\ninfinity in the combinatorial setting. Our examples are taken from the branch\nof mathematics known as Graph Theory.\n  --\n  La combinatoria infinita (tem\\'atica que, a ra\\'{\\i}z del trabajo de Cantor,\nactualmente es posible estudiar de manera completamente formal) nos presenta un\ninteresante contraste de semejanzas y diferencias con su an\\'alogo finito. El\nprop\\'osito de este art\\'{\\i}culo es presentar algunos ejemplos concretos tanto\nde semejanzas, como de diferencias radicales, para proporcionar cierta\nintuici\\'on acerca del comportamiento del infinito en el \\'ambito combinatorio.\nNuestros ejemplos son tomados de la rama de las matem\\'aticas conocida como\nTeor\\'{\\i}a de Gr\\'aficas."
    ],
    "c_categories":[
      "math.CO",
      "math.HO",
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17227",
    "c_title":[
      "The assembly of supermassive black holes at $z<1$ in early-type galaxies\n  from scaling relations"
    ],
    "c_abstract":[
      "The assembly of supermassive black hole (SMBH) mass ($M_{\\bullet}$) and\nstellar mass ($M_{*}$) in galaxies can be studied via the redshift evolution of\nthe $M_{\\bullet}-M_{*}$ relation, but the ways in which selection bias and\nphysical assembly channels affect this evolution are uncertain. To address\nthis, we compare the $M_{\\bullet}-M_{*}$ relation for local massive\n($M_{*}>10^{10.5}$M$_{\\odot}$) quiescent early-type galaxies (ETGs) to that for\nmassive ETGs hosting active galactic nuclei (AGN) at $z\\sim0.8$. The\nrestrictions on stellar mass and galaxy type limit the assembly channels that\nmay connect the two relations. For the local sample we find $\\log(M_{\\bullet})\n= 8.80 + 1.10(\\log{M_{*}-11})$, in line with prior work. For the $z\\sim0.8$\nsample we find a bias-corrected relation: $\\log(M_{\\bullet}) = 7.80 +\n1.25(\\log{M_{*}-11})$. We show, however, that this relation depends on the\nstellar and SMBH mass functions used to compute the selection bias, the virial\nrelation, the virial factor, and the active fraction, which together introduce\nuncertainty of up to $\\sim0.6$\\,dex in the $z\\sim0.8$ relation. Adopting\nreasonable choices of these parameters then our $z\\sim0.8$ relation lies above\nthat for $z\\sim0$ AGN by $\\sim0.5$\\,dex, but below our $z\\sim0$ ETG relation by\n$0.4-1$\\,dex in SMBH mass. We discuss possible sources of this offset,\nincluding further bias corrections, `downsizing\" in SMBH mass assembly, and\npreferential SMBH growth. Our results highlight the need to reduce\nuncertainties from selection and measurement bias in SMBH and stellar masses at\nall redshifts."
    ],
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05917",
    "c_title":[
      "Modeling Concentration Profiles in Electrolytes by Solving 3-D\n  Poisson-Nernst-Planck Equations via Finite Difference Method"
    ],
    "c_abstract":[
      "The Poisson-Nernst-Planck (PNP) equations are fundamental for modeling ion\ntransport in electrochemical systems, capturing the intricate interplay of\nconcentration gradients, electric fields, and ion fluxes essential for\napplications such as energy storage devices and other electrochemical devices.\nThis study introduces a refined numerical framework employing the finite\ndifference method to solve the 3-D PNP equations, enabling precise simulation\nof ion concentration distributions under realistic boundary conditions and\napplied electric fields. By rigorously addressing stability criteria and\nintegrating advanced boundary constraints, including the Butler-Volmer equation\nfor surface reactions, the model provides comprehensive insights into ion\ndynamics, particularly near electrode surfaces where electric field and\nreaction effects dominate. This framework significantly enhances traditional\nPNP modeling by accommodating varied boundary conditions, diffusion anisotropy,\nand complex electrochemical environments, offering a robust tool for\ninvestigating electrochemical processes and guiding the design of advanced\nelectrochemical systems."
    ],
    "c_categories":[
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04265",
    "c_title":[
      "Fermion Dark Matter Effect on Electroweak Phase Transition"
    ],
    "c_abstract":[
      "The addition of extra scalars to the Standard Model (SM) of particle physics\nenriches the vacuum structure and consequently gives rise to strong first-order\nphase transitions (EWPT) in the early universe. We raise the question that how\nthe EWPT is affected by the addition of fermions in models beyond the SM, and\naddress this question by studying the EWPT in a dark matter model comprising a\nsinglet scalar and two Dirac fermions. The singlet scalar develops a nonzero\nvacuum expectation value (VEV), and the lighter fermion plays the role of the\ndark matter. The model evades the stringent direct detection bounds due to the\npresence of two fermions. We first show that applying the high-temperature\napproximation, no first-order phase transition is found. Then we demonstrate\nthat when including the full finite temperature corrections to the effective\npotential, the first-order phase transition becomes possible, nevertheless, all\nthe phase transitions will be weak. We therefore deduce that the addition of\nfermions reduces the strength of the EWPT."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
    ],
    "b_abstract":[
      "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06129",
    "c_title":[
      "Turbulence in stratified rotating topographic wakes"
    ],
    "c_abstract":[
      "Turbulence generation mechanisms in stratified, rotating flows past\nthree-dimensional (3D) topography remain underexplored, particularly in\nsubmesoscale (SMS) regimes critical to geophysical applications. Using\nturbulence-resolving large-eddy simulations, we systematically dissect the\ninterplay of stratification and rotation in governing the dynamics of wake\nturbulence. Our parametric study reveals that turbulent dissipation in the near\nwake is dominated by two distinct instabilities: (1) vertical shear-driven\nKelvin-Helmholtz instability (KHI), amplified by oblique dislocation of\nK\\'arm\\'an vortices under strong stratification, and (2) centrifugal\/inertial\ninstability (CI), which peaks at intermediate rotation rates (Rossby number\norder unity, SMS regime) and relatively weaker stratification. Notably, strong\nrotation dampens vertical shear and weakens KHI-driven turbulence, while strong\nstratification imposes smaller vertical length scales that restrict CI-driven\nturbulence. By quantifying dissipation across a broad parameter space of\nstratification and rotation, predictive relationships between the environmental\nparameters and instability dominance is established. These findings highlight\nthe regime dependence of instability mechanisms and may inform targeted\nobservational campaigns and numerical models of oceanic and atmospheric wakes."
    ],
    "c_categories":[
      "physics.ao-ph",
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16467",
    "c_title":[
      "Heavy traffic limit with discontinuous coefficients via a non-standard\n  semimartingale decomposition"
    ],
    "c_abstract":[
      "This paper studies a single server queue in heavy traffic, with general\ninter-arrival and service time distributions, where arrival and service rates\nvary discontinuously as a function of the (diffusively scaled) queue length. It\nis proved that the weak limit is given by the unique-in-law solution to a\nstochastic differential equation in $[0,\\infty)$ with discontinuous drift and\ndiffusion coefficients. The main tool is a semimartingale decomposition for\npoint processes introduced in \\cite{dal-miy}, which is distinct from the\nDoob-Meyer decomposition of a counting process. Whereas the use of this tool is\ndemonstrated here for a particular model, we believe it may be useful for\ninvestigating the scaling limits of queueing models very broadly."
    ],
    "c_categories":[
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05043",
    "c_title":[
      "Finite strain continuum phenomenological model describing the\n  shape-memory effects in multi-phase semi-crystalline networks"
    ],
    "c_abstract":[
      "Thermally-driven semi-crystalline polymer networks are capable to achieve\nboth the one-way shape-memory effect and two-way shape-memory effect under\nstress and stress-free conditions, therefore representing an appealing class of\npolymers for applications requiring autonomous reversible actuation and shape\nchanges. In these materials, the shape-memory effects are achieved by\nleveraging the synergistic interaction between one or more crystalline phases\nand the surrounding amorphous ones that are present within the network itself.\nThe present paper introduces a general framework for the finite strain\ncontinuum phenomenological modeling of the thermo-mechanical and shape-memory\nbehavior of multi-phase semi-crystalline polymer networks. Model formulation,\nincluding the definition of phase and control variables, kinematic assumptions,\nand constitutive specifications, is introduced and thoroughly discussed.\nTheoretical derivations are general and easily adaptable to all cross-linked\nsystems which include two or more crystalline domains or a single crystalline\nphase with a wide melting range and manifest macroscopically the one-way\nshape-memory effect and the two-way shape-memory effect under stress and\nstress-free conditions. Model capabilities are validated against experimental\ndata for copolymer networks with two different crystalline phases characterized\nby well-separated melting and crystallization transitions. Results demonstrate\nthe accuracy of the proposed model in predicting all the phenomena involved and\nin furnishing a useful support for future material and application design\npurposes."
    ],
    "c_categories":[
      "cond-mat.soft"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18142",
    "c_title":[
      "TRIPP: A General Purpose Data Pipeline for Astronomical Image Processing"
    ],
    "c_abstract":[
      "We present the TRansient Image Processing Pipeline (TRIPP), a transient and\nvariable source detection pipeline that employs both difference imaging and\nlight curve analysis techniques for astronomical data. Additionally, we\ndemonstrate TRIPP's rapid analysis capability by detecting transient candidates\nin near-real time. TRIPP was tested using image data of the supernova SN2023ixf\nand from the Local Galactic Transient Survey (LGTS, Thomas et al. (2025))\ncollected by the Las Cumbres Observatory's (LCO) network of 0.4 m telescopes.\nTo verify the methods employed by TRIPP, we compare our results to published\nfindings on the photometry of SN2023ixf. Additionally, we report the ability of\nTRIPP to detect transient signals from optical Search for Extra Terrestrial\nIntelligence (SETI) sources."
    ],
    "c_categories":[
      "astro-ph.IM"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17770",
    "c_title":[
      "Lower Complexity Bounds of First-order Methods for Affinely Constrained\n  Composite Non-convex Problems"
    ],
    "c_abstract":[
      "Many recent studies on first-order methods (FOMs) focus on \\emph{composite\nnon-convex non-smooth} optimization with linear and\/or nonlinear function\nconstraints. Upper (or worst-case) complexity bounds have been established for\nthese methods. However, little can be claimed about their optimality as no\nlower bound is known, except for a few special \\emph{smooth non-convex} cases.\nIn this paper, we make the first attempt to establish lower complexity bounds\nof FOMs for solving a class of composite non-convex non-smooth optimization\nwith linear constraints. Assuming two different first-order oracles, we\nestablish lower complexity bounds of FOMs to produce a (near)\n$\\epsilon$-stationary point of a problem (and its reformulation) in the\nconsidered problem class, for any given tolerance $\\epsilon>0$. Our lower\nbounds indicate that the existence of a non-smooth convex regularizer can\nevidently increase the difficulty of an affinely constrained regularized\nproblem over its nonregularized counterpart. In addition, we show that our\nlower bound of FOMs with the second oracle is tight, with a difference of up to\na logarithmic factor from an upper complexity bound established in the extended\narXiv version of this paper."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01758",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "A review on segmentation of positron emission tomography images"
    ],
    "b_abstract":[
      "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15261",
    "c_title":[
      "Chromatic Quantum Contextuality"
    ],
    "c_abstract":[
      "Chromatic quantum contextuality is a criterion for quantum nonclassicality\ncharacterized by (hyper)graph coloring constraints. By examining the 3-uniform\nYu-Oh hypergraph -- a configuration of quantum observables -- we prove its\nchromatic number exceeds the classical three-color limit. This result\nestablishes chromatic contextuality as a distinct form of quantum\nnonclassicality, diverging from Kochen-Specker-type arguments that rely on the\nabsence of two-valued states. Crucially, the Yu-Oh hypergraph exhibits this\nbehavior even when separable two-valued states exist, demonstrating that\nchromatic contextuality operates independently of traditional noncontextuality\nviolations. Our findings underscore hypergraph coloring as a powerful framework\nfor identifying quantum signatures, enriching the understanding of\ncontextuality beyond conventional paradigms."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"Neurons for Neutrons: A Transformer Model for Computation Load\n  Estimation on Domain-Decomposed Neutron Transport Problems",
    "a_abstract":"Domain decomposition is a technique used to reduce memory overhead on large\nneutron transport problems. Currently, the optimal load-balanced processor\nallocation for these domains is typically determined through small-scale\nsimulations of the problem, which can be time-consuming for researchers and\nmust be repeated anytime a problem input is changed. We propose a Transformer\nmodel with a unique 3D input embedding, and input representations designed for\ndomain-decomposed neutron transport problems, which can predict the subdomain\ncomputation loads generated by small-scale simulations. We demonstrate that\nsuch a model trained on domain-decomposed Small Modular Reactor (SMR)\nsimulations achieves 98.2% accuracy while being able to skip the small-scale\nsimulation step entirely. Tests of the model's robustness on variant fuel\nassemblies, other problem geometries, and changes in simulation parameters are\nalso discussed.",
    "explanation":"Currently, the optimal load-\nbalanced processor allocation for these domains is typically determined\nthrough small-scale simulations of the problem, which can be time-consuming\nfor researchers and must be repeated anytime a problem input is changed.\nWe propose a Transformer model with a unique 3D input embedding, and\ninput representations designed for domain-decomposed neutron transport\nproblems, which can predict the subdomain computation loads generated\nby small-scale simulations.",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b3"
    ],
    "c_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "c_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10153",
    "c_title":[
      "Contemporaneous optical-radio observations of a fast radio burst in a\n  close galaxy pair"
    ],
    "c_abstract":[
      "We present the MeerKAT discovery and MeerLICHT contemporaneous optical\nobservations of the Fast Radio Burst (FRB) 20230808F, which was found to have a\ndispersion measure of $\\mathrm{DM}=653.2\\pm0.4\\mathrm{\\,pc\\,cm^{-3}}$. FRB\n20230808F has a scattering timescale $\\tau_{s}=3.1\\pm0.1\\,\\mathrm{ms}$ at\n$1563.6$ MHz, a rotation measure\n$\\mathrm{RM}=169.4\\pm0.2\\,\\mathrm{rad\\,m^{-2}}$, and a radio fluence\n$F_{\\mathrm{radio}}=1.72\\pm0.01\\,\\mathrm{Jy\\,ms}$. We find no optical\ncounterpart in the time immediately after the FRB, nor in the three months\nafter the FRB during which we continued to monitor the field of the FRB. We set\nan optical upper flux limit in MeerLICHT's $q$-band of $11.7\\,\\mathrm{\\mu Jy}$\nfor a 60 s exposure which started $\\sim3.4$ s after the burst, which\ncorresponds to an optical fluence, $F_{\\mathrm{opt}}$, of\n$0.039\\,\\mathrm{Jy\\,ms}$ on a timescale of $\\sim3.4$ s. We obtain an estimate\nfor the $q-$band luminosity limit of $vL_{v}\\sim\n1.3\\times10^{43}\\,\\mathrm{erg\\,s^{-1}}$. We localise the burst to a close\ngalaxy pair at a redshift of $z_{\\mathrm{spec}}=0.3472\\pm0.0002$. Our time\ndelay of $\\sim3.4$ s between the FRB arrival time and the start of our optical\nexposure is the shortest ever for an as yet non-repeating FRB, and hence the\nclosest to simultaneous optical follow-up that exists for such an FRB."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09998",
    "c_title":[
      "An in-depth study of Gamma rays from the Starburst Galaxy M 82 with\n  VERITAS"
    ],
    "c_abstract":[
      "Assuming Galactic cosmic rays originate in supernovae and the winds of\nmassive stars, starburst galaxies should produce very-high-energy (VHE; E$>$100\nGeV) gamma-ray emission via the interaction of their copious quantities of\ncosmic rays with the large reservoirs of dense gas within the galaxies. Such\nVHE emission was detected by VERITAS from the starburst galaxy M 82 in 2008-09.\nAn extensive, multi-year campaign followed these initial observations, yielding\na total of 254 h of good quality VERITAS data on M 82. Leveraging modern\nanalysis techniques and the larger exposure, these VERITAS data show a more\nstatistically significant VHE signal ($\\sim$6.5 standard deviations\n($\\sigma$)). The corresponding photon spectrum is well fit by a power law\n($\\Gamma = 2.3 \\pm 0.3_{stat} \\pm0.2_{sys}$) and the observed integral flux is\nF($>$450 GeV) = $(3.2 \\pm0.6_{stat} \\pm 0.6_{sys}) \\times\n10^{-13}~\\mathrm{cm^{-2}~s}^{-1}$, or $\\sim$0.4\\% of the Crab Nebula flux above\nthe same energy threshold. The improved VERITAS measurements, when combined\nwith various multi-wavelength data, enable modeling of the underlying emission\nand transport processes. A purely leptonic scenario is found to be a poor\nrepresentation of the gamma-ray spectral energy distribution (SED). A\nlepto-hadronic scenario with cosmic rays following a power-law spectrum in\nmomentum (index $s\\simeq 2.25$), and with significant bremsstrahlung below\n$1$~GeV, provides a good match to the observed SED. The synchrotron emission\nfrom the secondary electrons indicates that efficient non-radiative losses of\ncosmic-ray electrons may be related to advective escape from the starburst\ncore."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17152",
    "c_title":[
      "The origin of the very-high-energy radiation along the jet of Centaurus\n  A"
    ],
    "c_abstract":[
      "As the closest known active galactic nucleus, Centaurus A (Cen A) provides a\nrich environment for astrophysical exploration. It has been observed across\nwavelengths from radio to gamma rays, and indications of ongoing particle\nacceleration have been found on different scales. Recent measurements of\nvery-high-energy (VHE) gamma-rays ($>240$ GeV) by the HESS observatory have\ninferred the presence of ultra-relativistic electrons along Cen A's jet, yet\nthe underlying acceleration mechanism remains uncertain. Various authors have\nproposed that jet substructures, known as knots, may serve as efficient\nparticle accelerators. In this study, we investigate the hypothesis that knots\nare the particle acceleration sites along Cen A's jets. We focus on stationary\nknots, and assume that they result from interactions between the jet and the\nstellar winds of powerful stars. By combining relativistic hydrodynamic\nsimulations and shock acceleration theory with the radio and X-ray data, we\ncompare theoretical predictions with morphological and spectral data from\ndifferent knots. We estimate the maximum electron energy and the resulting VHE\ngamma-ray emission. Our findings suggest that electrons accelerated at the\nknots are responsible for the gamma-ray spectrum detected in the VHE band."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04154",
    "c_title":[
      "Searching for Internal Absorption Signatures in High-Redshift Blazars"
    ],
    "c_abstract":[
      "The gamma-ray emission from Flat Spectrum Radio Quasars (FSRQs), a sub-class\nof blazars, is believed to be generated through interactions of high-energy\nleptons and\/or hadrons in the jet with the ambient photon fields, including\nthose from the accretion disk, the broad line region (BLR), and the dusty\ntorus. However, these same photon fields can also attenuate gamma-rays through\ninternal photon-photon (gamma-gamma) absorption, imprinting characteristic\nspectral features. Investigating the internal absorption is crucial for\nunraveling the complex structure of FSRQs and constraining the poorly known\nlocation of the gamma-ray emission region. In this study, we select a sample of\ngamma-ray detected FSRQs with high redshift (z >= 3), to search for absorption\nfeatures appearing at lower photon energies due to a substantial redshift. We\nextract the Fermi-LAT gamma-ray spectra of these sources and perform physical\nmodeling using a detailed gamma-gamma opacity model, assuming that the BLR\nphoton field dominates the absorption and focusing on the energy range ~25\nGeV\/(1+z), where the absorption feature due to Ly{\\alpha} photons is expected.\nOur analysis reveals a hint of internal absorption for one source (the lowest\nredshift object in our sample, z~3) and provides constraints on the location of\nits gamma-ray emitting region along the jet. For the remaining, higher-redshift\nsources, the limited photon statistics prevent a reliable detection of internal\nopacity features."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17249",
    "c_title":[
      "On the origin of radio polarization in pulsar polar caps"
    ],
    "c_abstract":[
      "A knowledge of polarization properties of coherent radio waves escaping\npulsar polar caps is crucial for calculating radiative transfer through the\nmagnetosphere and for obtaining specific predictions of observable radio\nproperties. We describe the pair cascades in the pulsar polar cap, and for the\nfirst time, determine the Stokes parameters of the escaping radio waves from\nfirst-principle kinetic simulations for a pulsar with an inclination angle of\nthe magnetic axis 60{\\deg}.\n  Our model provides a quantitative and qualitative explanation of the observed\npulsar radio powers and spectra, the pulse profiles and polarization curves,\ntheir temporal variability, the strong Stokes L and weak Stokes V polarization\ncomponents, as well as the fact that linear polarization decreases with\nfrequency and the non-existence of a radius to frequency relationship. We find\nthat the radio emission from the polar cap can produce a diverse range of\nobserved pulsar properties, including single or double peaked profiles. Most of\nthe Stokes V curves from our simulations appear to be antisymmetric, but\nsymmetric curves are also present at some viewing angles. Although the PA swing\nof the radiation from the polar cap can be fitted by the rotating vector model\n(RVM) for most viewing angles, the angles obtained from the RVM do not\ncorrespond to the angular distance of the observer from the magnetic axis.\nInstead, the PA is directly related to the plasma flows in the polar cap and\nnot to the dipole geometry of the magnetic field. The observed range of other\npolarization features, in addition to our results, can be explained by\npropagation effects which are not part of the simulation.\n  Our simulations demonstrate that pair discharges determine the majority of\nits typically observed properties. The usage of RVM for estimations of the\nmagnetic field geometry from observations needs to be reevaluated."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11524",
    "c_title":[
      "The Scaled Polarity transform and related inequalities"
    ],
    "c_abstract":[
      "In this paper we deal with generalizations of the Mahler volume product for\nlog-concave functions. We show that the polarity transform $\\mathcal A$ can be\nrescaled so that the Mahler product it induces has upper and lower bounds of\nthe same asymptotics. We discuss a similar result for the $\\mathcal J$\ntransform.\n  As an application, we extend the K\\\"onig-Milman duality of entropy result to\nthe class of geometric log-concave functions."
    ],
    "c_categories":[
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18026",
    "c_title":[
      "Invariance properties of the solution operator for measure-valued\n  semilinear transport equations"
    ],
    "c_abstract":[
      "We provide conditions under which we prove for measure-valued transport\nequations with non-linear reaction term in the space of finite signed Radon\nmeasures, that positivity is preserved, as well as absolute continuity with\nrespect to Lebesgue measure, if the initial condition has that property.\nMoreover, if the initial condition has $L^p$ regular density, then the solution\nhas the same property."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11635",
    "c_title":[
      "Open orbits in causal flag manifolds, modular flows and wedge regions"
    ],
    "c_abstract":[
      "We study open orbits of symmetric subgroups of a simple connected\n  Lie group G on a causal flag manifold.\n  First we show that a flag manifold M of G carries an invariant causal\nstructure if and only if G is hermitian\n  of tube type and M is the conformal completion\n  of the corresponding simple euclidean Jordan algebra,\n  resp., the Shilov boundary of the associated symmetric tube domain.\n  We then study open orbits in M under symmetric subgroups,\n  also called causal\n  Makarevic spaces, from the perspective of applications in Algebraic Quantum\nField Theory (AQFT). A key motivation is the geometry of corresponding modular\nflows.\n  The open orbits are reductive causal symmetric\n  spaces, which arise in two flavors:\n  compactly causal and non-compactly causal ones.\n  In the non-compactly causal case we determine the corresponding Euler\nelements\n  and their positivity regions. For compactly causal spaces,\n  modular flows do not always exist and we determine\n  when this is the case. Then the positivity regions of the modular flows are\nnot globally hyperbolic,\n  but these spaces contain other interesting\n  globally hyperbolic subsets that can be described\n  in terms of the conformally flat Jordan coordinates via Cayley charts.\n  We discuss the Lorentzian case, involving de Sitter\n  and anti-de Sitter space in some detail."
    ],
    "c_categories":[
      "math-ph",
      "math.DG",
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09121",
    "c_title":[
      "On restricted sumsets with bounded degree relations"
    ],
    "c_abstract":[
      "Given two subsets $A, B \\subseteq \\mathbb{F}_p$ and a relation $\\mathcal{R}$\non $A \\times B$, the restricted sumset of $A, B$ with respect to $\\mathcal{R}$\nis defined as $A +_{\\mathcal{R}} B = \\{ a+b \\colon (a,b) \\notin \\mathcal{R}\n\\}$. When $\\mathcal{R}$ is taken as the equality relation, determining the\nminimum value of $|A +_{\\mathcal{R}} B|$ is the famous Erd\\H{o}s--Heilbronn\nproblem, which was solved separately by Dias da Silva, Hamidoune and Alon,\nNathanson and Ruzsa. Lev later conjectured that if $A, B \\subseteq\n\\mathbb{F}_p$ with $|A| + |B| \\le p$ and $\\mathcal{R}$ is a matching between\nsubsets of $A$ and $B$, then $|A +_{\\mathcal{R}} B| \\ge |A| + |B| - 3$.\n  We confirm this conjecture in the case where $|A| + |B| \\le (1-\\varepsilon)p$\nfor any $\\varepsilon > 0$, provided that $p > p_0$ for some sufficiently large\n$p_0$ depending only on $\\varepsilon$. Our proof builds on a recent work by\nBollob\\'as, Leader, and Tiba, and a rectifiability argument developed by Green\nand Ruzsa. Furthermore, our method extends to cases when $\\mathcal{R}$ is a\ndegree-bounded relation, either on both sides $A$ and $B$ or solely on the\nsmaller set.\n  In addition, we contruct subsets $A \\subseteq \\mathbb{F}_p$ with $|A| =\n\\frac{6p}{11} - O(1)$ such that $|A +_{\\mathcal{R}} A| = p-3$ for any prime\nnumber $p$, where $\\mathcal{R}$ is a matching on $A$. This extends an earlier\nexample by Lev and hightlights a distinction between the combinatorial notion\nof the restricted sumset and the classcial Erd\\H{o}s--Heilbronn problem, where\n$|A +_{\\mathcal{R}} A| \\ge p$ holds given $\\mathcal{R} = \\{(a,a) \\colon a \\in\nA\\}$ is the equality relation on $A$ and $|A| \\ge \\frac{p+3}{2}$."
    ],
    "c_categories":[
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10518",
    "c_title":[
      "Why the Brain Cannot Be a Digital Computer: History-Dependence and the\n  Computational Limits of Consciousness"
    ],
    "c_abstract":[
      "This paper presents a novel information-theoretic proof demonstrating that\nthe human brain as currently understood cannot function as a classical digital\ncomputer. Through systematic quantification of distinguishable conscious states\nand their historical dependencies, we establish that the minimum information\nrequired to specify a conscious state exceeds the physical information capacity\nof the human brain by a significant factor. Our analysis calculates the\nbit-length requirements for representing consciously distinguishable sensory\n\"stimulus frames\" and demonstrates that consciousness exhibits mandatory\ntemporal-historical dependencies that multiply these requirements beyond the\nbrain's storage capabilities. This mathematical approach offers new insights\ninto the fundamental limitations of computational models of consciousness and\nsuggests that non-classical information processing mechanisms may be necessary\nto account for conscious experience."
    ],
    "c_categories":[
      "cs.AI",
      "physics.hist-ph",
      "q-bio.NC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.07540",
    "c_title":[
      "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol"
    ],
    "c_abstract":[
      "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.05007",
    "c_title":[
      "Analyzing Advanced AI Systems Against Definitions of Life and\n  Consciousness"
    ],
    "c_abstract":[
      "Could artificial intelligence ever become truly conscious in a functional\nsense; this paper explores that open-ended question through the lens of Life, a\nconcept unifying classical biological criteria (Oxford, NASA, Koshland) with\nempirical hallmarks such as adaptive self maintenance, emergent complexity, and\nrudimentary self referential modeling. We propose a number of metrics for\nexamining whether an advanced AI system has gained consciousness, while\nemphasizing that we do not claim all AI stems can become conscious. Rather, we\nsuggest that sufficiently advanced architectures exhibiting immune like\nsabotage defenses, mirror self-recognition analogs, or meta-cognitive updates\nmay cross key thresholds akin to life-like or consciousness-like traits. To\ndemonstrate these ideas, we start by assessing adaptive self-maintenance\ncapability, and introduce controlled data corruption sabotage into the training\nprocess. The result demonstrates AI capability to detect these inconsistencies\nand revert or self-correct analogous to regenerative biological processes. We\nalso adapt an animal-inspired mirror self recognition test to neural\nembeddings, finding that partially trained CNNs can distinguish self from\nforeign features with complete accuracy. We then extend our analysis by\nperforming a question-based mirror test on five state-of-the-art chatbots\n(ChatGPT4, Gemini, Perplexity, Claude, and Copilot) and demonstrated their\nability to recognize their own answers compared to those of the other chatbots."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.11770",
    "c_title":[
      "Cognitive-Aligned Document Selection for Retrieval-augmented Generation"
    ],
    "c_abstract":[
      "Large language models (LLMs) inherently display hallucinations since the\nprecision of generated texts cannot be guaranteed purely by the parametric\nknowledge they include. Although retrieval-augmented generation (RAG) systems\nenhance the accuracy and reliability of generative models by incorporating\nexternal documents, these retrieved documents often fail to adequately support\nthe model's responses in practical applications. To address this issue, we\npropose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment\nRe\\textbf{trieval} for verifiable generation), which leverages an LLM to\ndynamically update queries and filter high-quality, reliable retrieval\ndocuments. Specifically, we parse the user query into its syntactic components\nand perform fine-grained grounded alignment with the retrieved documents. For\nquery components that cannot be individually aligned, we propose a dynamic\nsemantic compensation mechanism that iteratively refines and rewrites the query\nwhile continuously updating the retrieval results. This iterative process\ncontinues until the retrieved documents sufficiently support the query's\nresponse. Our approach introduces a novel criterion for filtering retrieved\ndocuments, closely emulating human strategies for acquiring targeted\ninformation. This ensures that the retrieved content effectively supports and\nverifies the generated outputs. On the ALCE benchmark, our method significantly\nsurpasses a wide range of baselines, achieving state-of-the-art performance."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.08020",
    "c_title":[
      "Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through\n  Multi-Agent Reinforcement Learning"
    ],
    "c_abstract":[
      "The effective design of patrol strategies is a difficult and complex problem,\nespecially in medium and large areas. The objective is to plan, in a\ncoordinated manner, the optimal routes for a set of patrols in a given area, in\norder to achieve maximum coverage of the area, while also trying to minimize\nthe number of patrols. In this paper, we propose a multi-agent reinforcement\nlearning (MARL) model, based on a decentralized partially observable Markov\ndecision process, to plan unpredictable patrol routes within an urban\nenvironment represented as an undirected graph. The model attempts to maximize\na target function that characterizes the environment within a given time frame.\nOur model has been tested to optimize police patrol routes in three\nmedium-sized districts of the city of Malaga. The aim was to maximize\nsurveillance coverage of the most crime-prone areas, based on actual crime data\nin the city. To address this problem, several MARL algorithms have been\nstudied, and among these the Value Decomposition Proximal Policy Optimization\n(VDPPO) algorithm exhibited the best performance. We also introduce a novel\nmetric, the coverage index, for the evaluation of the coverage performance of\nthe routes generated by our model. This metric is inspired by the predictive\naccuracy index (PAI), which is commonly used in criminology to detect hotspots.\nUsing this metric, we have evaluated the model under various scenarios in which\nthe number of agents (or patrols), their starting positions, and the level of\ninformation they can observe in the environment have been modified. Results\nshow that the coordinated routes generated by our model achieve a coverage of\nmore than $90\\%$ of the $3\\%$ of graph nodes with the highest crime incidence,\nand $65\\%$ for $20\\%$ of these nodes; $3\\%$ and $20\\%$ represent the coverage\nstandards for police resource allocation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.03512",
    "c_title":[
      "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing\n  Multi-Objective Optimization based DPO for Text-to-Image Alignment"
    ],
    "c_abstract":[
      "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that\ngenerated visuals not only accurately encapsulate user intents but also conform\nto stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini\nfiasco, where misaligned outputs triggered significant public backlash,\nunderscore the critical need for robust alignment mechanisms. In contrast,\nLarge Language Models (LLMs) have achieved notable success in alignment.\nBuilding on these advancements, researchers are eager to apply similar\nalignment techniques, such as Direct Preference Optimization (DPO), to T2I\nsystems to enhance image generation fidelity and reliability.\n  We present YinYangAlign, an advanced benchmarking framework that\nsystematically quantifies the alignment fidelity of T2I systems, addressing six\nfundamental and inherently contradictory design objectives. Each pair\nrepresents fundamental tensions in image generation, such as balancing\nadherence to user prompts with creative modifications or maintaining diversity\nalongside visual coherence. YinYangAlign includes detailed axiom datasets\nfeaturing human prompts, aligned (chosen) responses, misaligned (rejected)\nAI-generated outputs, and explanations of the underlying contradictions."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17730",
    "c_title":[
      "The rational Gurarii space and its linear isometry group"
    ],
    "c_abstract":[
      "We show that the linear isometry group of the rational Gurarii space does not\nhave a comeager conjugacy class. This is because the class of partial\nisometries in rational polyhedral spaces does not have the weak amalgamation\nproperty. Our methods demonstrate also that the class of rational polyhedral\nspaces does not have the Hrushovski property."
    ],
    "c_categories":[
      "math.FA",
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.01574",
    "c_title":[
      "An End-To-End LLM Enhanced Trading System"
    ],
    "c_abstract":[
      "This project introduces an end-to-end trading system that leverages Large\nLanguage Models (LLMs) for real-time market sentiment analysis. By synthesizing\ndata from financial news and social media, the system integrates\nsentiment-driven insights with technical indicators to generate actionable\ntrading signals. FinGPT serves as the primary model for sentiment analysis,\nensuring domain-specific accuracy, while Kubernetes is used for scalable and\nefficient deployment."
    ],
    "c_categories":[
      "q-fin.TR"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.11252",
    "c_title":[
      "Constrained polynomial roots and a modulated approach to Schur stability"
    ],
    "c_abstract":[
      "It is common in stability analysis to linearize a system and investigate the\nspectrum of the Jacobian matrix. This approach faces the challenge of\ndetermining the matrix spectrum when the coefficients depend on parameters or\nwhen the characteristic polynomial is more than quartic. In this paper, we\nreverse the classical process and use the authors' work on global stability to\nfind sufficient conditions on the coefficients that ensure the zeros of the\ncharacteristic polynomial are in the open unit disk. This leads to an algorithm\nthat begins by testing the $\\ell_1$-norm of the polynomial, and if it is not\nless than two, perform an iteration process that can be implemented with\nmoderate effort. We give examples that show the effectiveness of our method\nwhen compared with the Jury's algorithm. Last, we formalize our constructions\nin terms of semialgebraic sets."
    ],
    "c_categories":[
      "math.DS"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.11250",
    "c_title":[
      "CRPS-Based Targeted Sequential Design with Application in Chemical Space"
    ],
    "c_abstract":[
      "Sequential design of real and computer experiments via Gaussian Process (GP)\nmodels has proven useful for parsimonious, goal-oriented data acquisition\npurposes. In this work, we focus on acquisition strategies for a GP model that\nneeds to be accurate within a predefined range of the response of interest.\nSuch an approach is useful in various fields including synthetic chemistry,\nwhere finding molecules with particular properties is essential for developing\nuseful materials and effective medications. GP modeling and sequential design\nof experiments have been successfully applied to a plethora of domains,\nincluding molecule research. Our main contribution here is to use the\nthreshold-weighted Continuous Ranked Probability Score (CRPS) as a basic\nbuilding block for acquisition functions employed within sequential design. We\nstudy pointwise and integral criteria relying on two different weighting\nmeasures and benchmark them against competitors, demonstrating improved\nperformance with respect to considered goals. The resulting acquisition\nstrategies are applicable to a wide range of fields and pave the way to further\ndeveloping sequential design relying on scoring rules."
    ],
    "c_categories":[
      "cs.LG",
      "stat.AP",
      "stat.CO",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03389",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
    ],
    "b_abstract":[
      "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.05388",
    "c_title":[
      "A fast approximate scenario addition method for two-stage robust\n  mixed-integer programs"
    ],
    "c_abstract":[
      "This paper presents a new scenario addition method for two-stage robust\nmixed-integer programs with finite uncertainty sets. Our method combines and\nextends speed-up techniques used in previous scenario addition methods (also\ncalled column-and-constraint generation methods) and introduces several new\ntechniques. In particular, it uses dual bounds for second-stage problems in\norder to allow a faster identification of the next promising scenario to be\nadded to the master problem. Moreover, adaptive time limits are imposed to\navoid getting stuck on particularly hard second-stage problems, and a gap\npropagation between master problem and second-stage problems is used to stop\nsolving them earlier if only a given non-zero optimality gap is to be reached\noverall. This makes our method particularly effective for problems where\nsolving the second-stage problem is computationally challenging. To evaluate\nthe method's performance, we compare it to two recent scenario addition methods\nfrom the literature on two applications: a robust capacitated location routing\nproblem and a robust integrated berth allocation and quay crane assignment and\nscheduling problem. The first problem features a particularly hard second\nstage, and we show that our method is able to solve considerably more and\nlarger instances in a given time limit. Using the second problem, we verify the\ngeneral applicability of our method, even for problems where the second stage\nis relatively easy."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"DNN-based 3D Cloud Retrieval for Variable Solar Illumination and\n  Multiview Spaceborne Imaging",
    "a_abstract":"Climate studies often rely on remotely sensed images to retrieve\ntwo-dimensional maps of cloud properties. To advance volumetric analysis, we\nfocus on recovering the three-dimensional (3D) heterogeneous extinction\ncoefficient field of shallow clouds using multiview remote sensing data.\nClimate research requires large-scale worldwide statistics. To enable scalable\ndata processing, previous deep neural networks (DNNs) can infer at spaceborne\nremote sensing downlink rates. However, prior methods are limited to a fixed\nsolar illumination direction. In this work, we introduce the first scalable\nDNN-based system for 3D cloud retrieval that accommodates varying camera poses\nand solar directions. By integrating multiview cloud intensity images with\ncamera poses and solar direction data, we achieve greater flexibility in\nrecovery. Training of the DNN is performed by a novel two-stage scheme to\naddress the high number of degrees of freedom in this problem. Our approach\nshows substantial improvements over previous state-of-the-art, particularly in\nhandling variations in the sun's zenith angle.",
    "explanation":"Climate studies often rely on remotely sensed im-\nages to retrieve two-dimensional maps of cloud properties. To\nadvance volumetric analysis, we focus on recovering the three-\ndimensional (3D) heterogeneous extinction coefficient field of\nshallow clouds using multiview remote sensing data. \n\n\nTo enable\nscalable data processing, previous deep neural networks (DNNs)\ncan infer at spaceborne remote sensing downlink rates. ",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b8"
    ],
    "c_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "c_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.02330",
    "c_title":[
      "Exploring Simple Siamese Network for High-Resolution Video Quality\n  Assessment"
    ],
    "c_abstract":[
      "In the research of video quality assessment (VQA), two-branch network has\nemerged as a promising solution. It decouples VQA with separate technical and\naesthetic branches to measure the perception of low-level distortions and\nhigh-level semantics respectively. However, we argue that while technical and\naesthetic perspectives are complementary, the technical perspective itself\nshould be measured in semantic-aware manner. We hypothesize that existing\ntechnical branch struggles to perceive the semantics of high-resolution videos,\nas it is trained on local mini-patches sampled from videos. This issue can be\nhidden by apparently good results on low-resolution videos, but indeed becomes\ncritical for high-resolution VQA. This work introduces SiamVQA, a simple but\neffective Siamese network for highre-solution VQA. SiamVQA shares weights\nbetween technical and aesthetic branches, enhancing the semantic perception\nability of technical branch to facilitate technical-quality representation\nlearning. Furthermore, it integrates a dual cross-attention layer for fusing\ntechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy on\nhigh-resolution benchmarks, and competitive results on lower-resolution\nbenchmarks. Codes will be available at: https:\/\/github.com\/srcn-ivl\/SiamVQA"
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.02223",
    "c_title":[
      "DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting"
    ],
    "c_abstract":[
      "Accurate object perception is essential for robotic applications such as\nobject navigation. In this paper, we propose DQO-MAP, a novel object-SLAM\nsystem that seamlessly integrates object pose estimation and reconstruction. We\nemploy 3D Gaussian Splatting for high-fidelity object reconstruction and\nleverage quadrics for precise object pose estimation. Both of them management\nis handled on the CPU, while optimization is performed on the GPU,\nsignificantly improving system efficiency. By associating objects with unique\nIDs, our system enables rapid object extraction from the scene. Extensive\nexperimental results on object reconstruction and pose estimation demonstrate\nthat DQO-MAP achieves outstanding performance in terms of precision,\nreconstruction quality, and computational efficiency. The code and dataset are\navailable at: https:\/\/github.com\/LiHaoy-ux\/DQO-MAP."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.07348",
    "c_title":[
      "Fully Unsupervised Annotation of C. Elegans"
    ],
    "c_abstract":[
      "In this work we present a novel approach for unsupervised multi-graph\nmatching, which applies to problems for which a Gaussian distribution of\nkeypoint features can be assumed. We leverage cycle consistency as loss for\nself-supervised learning, and determine Gaussian parameters through Bayesian\nOptimization, yielding a highly efficient approach that scales to large\ndatasets. Our fully unsupervised approach enables us to reach the accuracy of\nstate-of-the-art supervised methodology for the use case of annotating cell\nnuclei in 3D microscopy images of the worm C. elegans. To this end, our\napproach yields the first unsupervised atlas of C. elegans, i.e. a model of the\njoint distribution of all of its cell nuclei, without the need for any ground\ntruth cell annotation. This advancement enables highly efficient annotation of\ncell nuclei in large microscopy datasets of C. elegans. Beyond C. elegans, our\napproach offers fully unsupervised construction of cell-level atlases for any\nmodel organism with a stereotyped cell lineage, and thus bears the potential to\ncatalyze respective comparative developmental studies in a range of further\nspecies."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.02593",
    "c_title":[
      "CMMLoc: Advancing Text-to-PointCloud Localization with\n  Cauchy-Mixture-Model Based Framework"
    ],
    "c_abstract":[
      "The goal of point cloud localization based on linguistic description is to\nidentify a 3D position using textual description in large urban environments,\nwhich has potential applications in various fields, such as determining the\nlocation for vehicle pickup or goods delivery. Ideally, for a textual\ndescription and its corresponding 3D location, the objects around the 3D\nlocation should be fully described in the text description. However, in\npractical scenarios, e.g., vehicle pickup, passengers usually describe only the\npart of the most significant and nearby surroundings instead of the entire\nenvironment. In response to this $\\textbf{partially relevant}$ challenge, we\npropose $\\textbf{CMMLoc}$, an uncertainty-aware\n$\\textbf{C}$auchy-$\\textbf{M}$ixture-$\\textbf{M}$odel ($\\textbf{CMM}$) based\nframework for text-to-point-cloud $\\textbf{Loc}$alization. To model the\nuncertain semantic relations between text and point cloud, we integrate CMM\nconstraints as a prior during the interaction between the two modalities. We\nfurther design a spatial consolidation scheme to enable adaptive aggregation of\ndifferent 3D objects with varying receptive fields. To achieve precise\nlocalization, we propose a cardinal direction integration module alongside a\nmodality pre-alignment strategy, helping capture the spatial relationships\namong objects and bringing the 3D objects closer to the text modality.\nComprehensive experiments validate that CMMLoc outperforms existing methods,\nachieving state-of-the-art results on the KITTI360Pose dataset. Codes are\navailable in this GitHub repository https:\/\/github.com\/kevin301342\/CMMLoc."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.01845",
    "c_title":[
      "Semantic Segmentation for Sequential Historical Maps by Learning from\n  Only One Map"
    ],
    "c_abstract":[
      "Historical maps are valuable resources that capture detailed geographical\ninformation from the past. However, these maps are typically available in\nprinted formats, which are not conducive to modern computer-based analyses.\nDigitizing these maps into a machine-readable format enables efficient\ncomputational analysis. In this paper, we propose an automated approach to\ndigitization using deep-learning-based semantic segmentation, which assigns a\nsemantic label to each pixel in scanned historical maps. A key challenge in\nthis process is the lack of ground-truth annotations required for training deep\nneural networks, as manual labeling is time-consuming and labor-intensive. To\naddress this issue, we introduce a weakly-supervised age-tracing strategy for\nmodel fine-tuning. This approach exploits the similarity in appearance and\nland-use patterns between historical maps from neighboring time periods to\nguide the training process. Specifically, model predictions for one map are\nutilized as pseudo-labels for training on maps from adjacent time periods.\nExperiments conducted on our newly curated \\textit{Hameln} dataset demonstrate\nthat the proposed age-tracing strategy significantly enhances segmentation\nperformance compared to baseline models. In the best-case scenario, the mean\nIntersection over Union (mIoU) achieved 77.3\\%, reflecting an improvement of\napproximately 20\\% over baseline methods. Additionally, the fine-tuned model\nachieved an average overall accuracy of 97\\%, highlighting the effectiveness of\nour approach for digitizing historical maps."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17730",
    "c_title":[
      "The rational Gurarii space and its linear isometry group"
    ],
    "c_abstract":[
      "We show that the linear isometry group of the rational Gurarii space does not\nhave a comeager conjugacy class. This is because the class of partial\nisometries in rational polyhedral spaces does not have the weak amalgamation\nproperty. Our methods demonstrate also that the class of rational polyhedral\nspaces does not have the Hrushovski property."
    ],
    "c_categories":[
      "math.FA",
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.01574",
    "c_title":[
      "An End-To-End LLM Enhanced Trading System"
    ],
    "c_abstract":[
      "This project introduces an end-to-end trading system that leverages Large\nLanguage Models (LLMs) for real-time market sentiment analysis. By synthesizing\ndata from financial news and social media, the system integrates\nsentiment-driven insights with technical indicators to generate actionable\ntrading signals. FinGPT serves as the primary model for sentiment analysis,\nensuring domain-specific accuracy, while Kubernetes is used for scalable and\nefficient deployment."
    ],
    "c_categories":[
      "q-fin.TR"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.11252",
    "c_title":[
      "Constrained polynomial roots and a modulated approach to Schur stability"
    ],
    "c_abstract":[
      "It is common in stability analysis to linearize a system and investigate the\nspectrum of the Jacobian matrix. This approach faces the challenge of\ndetermining the matrix spectrum when the coefficients depend on parameters or\nwhen the characteristic polynomial is more than quartic. In this paper, we\nreverse the classical process and use the authors' work on global stability to\nfind sufficient conditions on the coefficients that ensure the zeros of the\ncharacteristic polynomial are in the open unit disk. This leads to an algorithm\nthat begins by testing the $\\ell_1$-norm of the polynomial, and if it is not\nless than two, perform an iteration process that can be implemented with\nmoderate effort. We give examples that show the effectiveness of our method\nwhen compared with the Jury's algorithm. Last, we formalize our constructions\nin terms of semialgebraic sets."
    ],
    "c_categories":[
      "math.DS"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.11250",
    "c_title":[
      "CRPS-Based Targeted Sequential Design with Application in Chemical Space"
    ],
    "c_abstract":[
      "Sequential design of real and computer experiments via Gaussian Process (GP)\nmodels has proven useful for parsimonious, goal-oriented data acquisition\npurposes. In this work, we focus on acquisition strategies for a GP model that\nneeds to be accurate within a predefined range of the response of interest.\nSuch an approach is useful in various fields including synthetic chemistry,\nwhere finding molecules with particular properties is essential for developing\nuseful materials and effective medications. GP modeling and sequential design\nof experiments have been successfully applied to a plethora of domains,\nincluding molecule research. Our main contribution here is to use the\nthreshold-weighted Continuous Ranked Probability Score (CRPS) as a basic\nbuilding block for acquisition functions employed within sequential design. We\nstudy pointwise and integral criteria relying on two different weighting\nmeasures and benchmark them against competitors, demonstrating improved\nperformance with respect to considered goals. The resulting acquisition\nstrategies are applicable to a wide range of fields and pave the way to further\ndeveloping sequential design relying on scoring rules."
    ],
    "c_categories":[
      "cs.LG",
      "stat.AP",
      "stat.CO",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Distributed Sky Imaging Radiometry and Tomography"
    ],
    "b_abstract":[
      "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
    ],
    "b_categories":[
      "astro-ph.EP"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.05388",
    "c_title":[
      "A fast approximate scenario addition method for two-stage robust\n  mixed-integer programs"
    ],
    "c_abstract":[
      "This paper presents a new scenario addition method for two-stage robust\nmixed-integer programs with finite uncertainty sets. Our method combines and\nextends speed-up techniques used in previous scenario addition methods (also\ncalled column-and-constraint generation methods) and introduces several new\ntechniques. In particular, it uses dual bounds for second-stage problems in\norder to allow a faster identification of the next promising scenario to be\nadded to the master problem. Moreover, adaptive time limits are imposed to\navoid getting stuck on particularly hard second-stage problems, and a gap\npropagation between master problem and second-stage problems is used to stop\nsolving them earlier if only a given non-zero optimality gap is to be reached\noverall. This makes our method particularly effective for problems where\nsolving the second-stage problem is computationally challenging. To evaluate\nthe method's performance, we compare it to two recent scenario addition methods\nfrom the literature on two applications: a robust capacitated location routing\nproblem and a robust integrated berth allocation and quay crane assignment and\nscheduling problem. The first problem features a particularly hard second\nstage, and we show that our method is able to solve considerably more and\nlarger instances in a given time limit. Using the second problem, we verify the\ngeneral applicability of our method, even for problems where the second stage\nis relatively easy."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13332",
    "c_title":[
      "Solar System objects observed with TESS -- Early Data Release 2: I.\n  Spin-shape recovery potential of multi-epoch TESS observations"
    ],
    "c_abstract":[
      "Using multidirectional measurements from the Transiting Exoplanet Survey\nSatellite (TESS), we investigated the viability of determining the approximate\nshape and spin axis orientations for 44 selected main belt asteroids, using\nlight curve inversion, assuming Lommel-Seeliger ellipsoids. This study aims to\ninvestigate the applicability of low-degree-of-freedom shape models in those\ncases when rotation periods can be accurately determined, but light curves are\nonly available in a limited number of geometries or orbital phases. Our results\nare compared with the shape and spin axis solutions obtained for the same set\nof asteroids by more complex light curve inversion methods using mainly\nground-based measurements, available via the Database of Asteroid Models from\nInversion Techniques (DAMIT).The best-fit spin-axis orientations show a\nmoderately good match with the DAMIT solutions; however, a better agreement is\nreached with triaxial ellipsoid solution obtained from other large, independent\nsurveys. This suggests that while TESS-only data works well for finding\nrotation periods, it has its limitations when determining asteroid shape and\nspin-axis orientation. We discuss the challenges and potential applications of\nthis approach for studying large number of asteroids observed by TESS."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04350",
    "c_title":[
      "Influence of Planetary Rotation on Supersonic Flow of Lava Planets: A\n  Two-Dimensional Horizontal Model Analysis"
    ],
    "c_abstract":[
      "The study of lava planets has attracted significant attention recently\nbecause of their close proximity to their host stars, which enhances their\ndetectability for atmospheric characterization. Previous studies showed that\nthe atmospheric flow becomes supersonic if the atmosphere was dominated by\nrocky vapor evaporated from the magma ocean around the substellar point of\nsmall lava planets. These studies often assumed an axisymmetric flow about the\naxis from the substellar point to the antistellar point but ignored the effect\nof planetary rotation on the climate. The spin rate of lava planets can be\nrather fast due to their close-in orbits, which can break the aforementioned\nsymmetry and induce the asymmetric flow component. Here, we introduce a\ntwo-dimensional framework to explore the influence of planetary rotation on the\natmospheric dynamics of these lava planets for the first time, and assess the\nsensitivity and range of application of our model. Starting from the\nestablished one-dimensional axisymmetric atmospheric solution, we obtain the\ngoverning equation for the asymmetric flow by expanding with respect to 1\/Ro\n(Ro denotes Rossby number and exceeds unity for typical lava planets). The\nasymmetric component of supersonic flow is pivotal for future research on the\nobservation of these atmospheres, flow patterns of the magma ocean currents\ndriven by atmospheric winds, and deformation of the planetary shape over long\ntimescales."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.15196",
    "c_title":[
      "Detectability of oxygen fugacity regimes in the magma ocean world 55\n  Cancri e at high spectral resolution"
    ],
    "c_abstract":[
      "Ultra-short Period exoplanets (USPs) like 55 Cnc e, hosting dayside magma\noceans, present unique opportunities to study surface-atmosphere interactions.\nThe composition of a vaporised mineral atmosphere enveloping the dayside is\ndictated by that of the surface magma ocean, which in turn is sensitive to its\noxygen fugacity ($f$O$_2$). Observability estimations and characterisation of\nthe atmospheric emission of 55 Cnc e have mostly remained limited to low\nspectral resolution space-based studies. Here, we aim to examine ground-based\nhigh-resolution observabilities of a diverse set of mineral atmospheres\nproduced across a grid of mantle $f$O$_2$s varying over 12 orders of magnitude.\nWe assume a Bulk Silicate Earth mantle composition and a substellar dayside\ntemperature of T = 2500K in the near infrared wavelength (NIR) region. This\nspectral range is often featureless for this class of atmospheres at\nlow-resolution. Coupling our newly developed simulator for synthesising\nrealistic observations from high-resolution ground-based spectrographs (Ratri)\nto a pre-developed high-resolution cross-correlation spectroscopy (HRCCS)\nanalysis pipeline (Upamana), we find that this array of mineral atmospheres\nwould all be detectable with 11 hours of observing time of the dayside of 55\nCnc e with CARMENES and each individual scenario can be correctly\ndifferentiated within 1$\\sigma$. Our analysis is readily able to distinguish\nbetween a planet with an Earth-like redox state (with $f$O$_2$ $\\sim$3.5\nlog$_{10}$ units above the iron-w\\\"ustite, IW buffer) from a Mercury-like\nplanet ($f$O$_2$ $\\sim$5 log$_{10}$ units below IW). We thus conclude that the\nHRCCS technique holds promise for cataloguing the diversity of redox states\namong the rocky exoplanetary population."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.00576",
    "c_title":[
      "A 16 Myr super-Neptune in Upper-Centaurus Lupus and a preliminary survey\n  of transiting planets in Sco-Cen with TESS"
    ],
    "c_abstract":[
      "Measuring the properties of planets younger than about 50 Myr helps to test\ndifferent planetary formation and evolution models. NASA's Transiting Exoplanet\nSurvey Satellite (TESS) has observed nearly the entire sky, including a wide\nrange of star-forming regions and young stellar clusters, expanding our census\nof the newborn planet population. In this work, we present the discovery of the\nTIC 88785435 planetary system located in the Upper-Centaurus Lupus (UCL) region\nof the Scorpius-Centaurus OB association (Sco-Cen) and a preliminary survey of\nthe planet population within Sco-Cen. TIC 88785435 is a pre-main sequence, K7V\ndwarf ($M_\\star = 0.72M_\\odot$, $R_\\star = 0.91R_\\odot$, $T_\\mathrm{eff}$ =\n3998K, V = 11.7 mag) located within the bounds of UCL. We investigate the\ndistribution of rotation periods measured from the TESS long-cadence data and\nthe Halpha and Li abundances from the spectra of TIC 88785435. TESS\nlong-candence data reveal that TIC 88785435 hosts a transiting super-Neptune\n($R_b = 5.03R_\\oplus$, P = 10.51 days), TIC 88785435 b. Ground-based follow-up\nvalidates the planetary nature of TIC 88785435 b. Using the TESS data, we\nperform a preliminary survey to investigate how TIC 88785435 b compares to the\npopulation of newly born planets located within Sco-Cen."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04383",
    "c_title":[
      "Confirmation of four hot Jupiters detected by TESS using follow-up\n  spectroscopy from MaHPS at Wendelstein together with NEID and TRES"
    ],
    "c_abstract":[
      "We report the confirmation and characterization of four hot Jupiter-type\nexoplanets initially detected by TESS: TOI-1295 b, TOI-2580 b, TOI-6016 b, and\nTOI-6130 b. Using observations with the high-resolution echelle spectrograph\nMaHPS on the 2.1m telescope at Wendelstein Observatory, together with NEID at\nKitt Peak National Observatory and TRES at the Fred Lawrence Whipple\nObservatory, we confirmed the planetary nature of these four planet candidates.\nWe also performed precise mass measurements. All four planets are found to be\nhot Jupiters with orbital periods between 2.4 and 4.0 days. The sizes of these\nplanets range from 1.29 to 1.64 Jupiter radii, while their masses range from\n0.6 to 1.5 Jupiter masses. Additionally, we investigated whether there are\nsigns of other planets in the systems but have found none. Lastly, we compared\nthe radii of our four objects to the results of an empirical study of radius\ninflation and see that all four demonstrate a good fit with the current models.\nThese four planets belong to the first array of planets confirmed with MaHPS\ndata, supporting the ability of the spectrograph to detect planets around\nfainter stars as faint as V=12."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10272",
    "c_title":[
      "Symmetry and classification of positive solutions of some weighted\n  elliptic equations"
    ],
    "c_abstract":[
      "We study the weighted elliptic equation \\begin{equation} -div(|x|^{-2a}\\nabla\nu)=|x|^{-bp}|u|^{p-2}u~~~\\mbox{in}~\\mathbb{R}^N\n~~~~~~~~~~~~~~~~~~~~(0.1)\\end{equation} with $N\\geq 2$, which arises from the\nCaffarelli-Kohn-Nirenberg inequalities. Under the assumptions of finite energy\nand $a_1+a_2=N-2$, for nonnegative solutions we prove the equivalence between\nequation (0.1) with $a=a_1$ and equation (0.1) with $a=a_2$. Without finite\nenergy assumptions, for $2\\leq p<2^*$ we give the optimal parameter range in\nwhich nonnegative solutions of (0.1) in $\\mathbf{L}^\\infty_{Loc}(\\mathbb{R}^N)$\nmust be radially symmetric, and give a complete classification for these\nsolutions in this range."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06886",
    "c_title":[
      "On the extreme complexity of certain nearly regular graphs"
    ],
    "c_abstract":[
      "The complexity of a graph is the number of its labeled spanning trees. It is\ndemonstrated that the seven known triangle-free strongly regular graphs, such\nas the Higman-Sims graph, are graphs of maximal complexity among all graphs of\nthe same order and degree; their complements are shown to be of minimal\ncomplexity. A generalization to nearly regular graphs with two distinct\neigevalues of the Laplacian is presented. Conjectures and applications of these\nresults to biological problems on neuronal activity are described."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12543",
    "c_title":[
      "Regularizing effect of the spatially homogeneous Landau equation with\n  soft potential"
    ],
    "c_abstract":[
      "This paper investigates the Cauchy problem of the spatially homogeneous\nLandau equation with soft potential under the perturbation framework to global\nequilibrium. We prove that the solution to the Cauchy problem exhibits\nanalyticity in the time variable and the Gelfand-Shilov regularizing effect in\nthe velocity variables."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01509",
    "c_title":[
      "Recommendations for visual predictive checks in Bayesian workflow"
    ],
    "c_abstract":[
      "A key step in the Bayesian workflow for model building is the graphical\nassessment of model predictions, whether these are drawn from the prior or\nposterior predictive distribution. The goal of these assessments is to identify\nwhether the model is a reasonable (and ideally accurate) representation of the\ndomain knowledge and\/or observed data. There are many commonly used visual\npredictive checks which can be misleading if their implicit assumptions do not\nmatch the reality. Thus, there is a need for more guidance for selecting,\ninterpreting, and diagnosing appropriate visualizations. As a visual predictive\ncheck itself can be viewed as a model fit to data, assessing when this model\nfails to represent the data is important for drawing well-informed conclusions.\n  We present recommendations and diagnostic tools to mitigate ad-hoc\ndecision-making in visual predictive checks. These contributions aim to improve\nthe robustness and interpretability of Bayesian model criticism practices. We\noffer recommendations for appropriate visual predictive checks for observations\nthat are: continuous, discrete, or a mixture of the two. We also discuss\ndiagnostics to aid in the selection of visual methods. Specifically, in the\ndetection of an incorrect assumption of continuously-distributed data:\nidentifying when data is likely to be discrete or contain discrete components,\ndetecting and estimating possible bounds in data, and a diagnostic of the\ngoodness-of-fit to data for density plots made through kernel density\nestimates."
    ],
    "c_categories":[
      "stat.CO",
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04682",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b8"
    ],
    "b_title":[
      "Variable Imaging Projection Cloud Scattering Tomography"
    ],
    "b_abstract":[
      "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09301",
    "c_title":[
      "Computing Connection Matrices of Conley Complexes via Algebraic Morse\n  Theory"
    ],
    "c_abstract":[
      "Given a poset-graded chain complex of vector spaces, a Conley complex is the\nminimal chain-homotopic reduction of the initial complex that respects the\nposet grading. A connection matrix is a matrix representing the differential of\nthe Conley complex. In this work, we give an algebraic derivation of the Conley\ncomplex and its connection matrix using homological perturbation theory and\nalgebraic Morse theory. Under this framework, we use a graded splitting of\nrelative chain groups to determine the connection matrix, rather than Forman's\nacyclic partial matching in the usual discrete Morse theory setting. This\nsplitting is obtained by means of the clearing optimisation, a commonly used\ntechnique in persistent homology. Finally, we show how this algebraic\nperspective yields an algorithm for computing the connection matrix via column\nreductions on the differential of the initial complex."
    ],
    "c_categories":[
      "math.AT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"Unleashing the power of novel conditional generative approaches for new\n  materials discovery",
    "a_abstract":"For a very long time, computational approaches to the design of new materials\nhave relied on an iterative process of finding a candidate material and\nmodeling its properties. AI has played a crucial role in this regard, helping\nto accelerate the discovery and optimization of crystal properties and\nstructures through advanced computational methodologies and data-driven\napproaches. To address the problem of new materials design and fasten the\nprocess of new materials search, we have applied latest generative approaches\nto the problem of crystal structure design, trying to solve the inverse\nproblem: by given properties generate a structure that satisfies them without\nutilizing supercomputer powers. In our work we propose two approaches: 1)\nconditional structure modification: optimization of the stability of an\narbitrary atomic configuration, using the energy difference between the most\nenergetically favorable structure and all its less stable polymorphs and 2)\nconditional structure generation. We used a representation for materials that\nincludes the following information: lattice, atom coordinates, atom types,\nchemical features, space group and formation energy of the structure. The loss\nfunction was optimized to take into account the periodic boundary conditions of\ncrystal structures. We have applied Diffusion models approach, Flow matching,\nusual Autoencoder (AE) and compared the results of the models and approaches.\nAs a metric for the study, physical PyMatGen matcher was employed: we compare\ntarget structure with generated one using default tolerances. So far, our\nmodifier and generator produce structures with needed properties with accuracy\n41% and 82% respectively. To prove the offered methodology efficiency,\ninference have been carried out, resulting in several potentially new\nstructures with formation energy below the AFLOW-derived convex hulls.",
    "explanation":"\" In our work we propose two approaches: 1) conditional structure modification: optimization of the stability of an arbitrary atomic configuration, using the energy difference between the most energetically favorable structure and all its less stable polymorphs and 2) conditional structure generation.\"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b6"
    ],
    "c_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "c_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.00594",
    "c_title":[
      "Estimation of total body fat using symbolic regression and evolutionary\n  algorithms"
    ],
    "c_abstract":[
      "Body fat percentage is an increasingly popular alternative to Body Mass Index\nto measure overweight and obesity, offering a more accurate representation of\nbody composition. In this work, we evaluate three evolutionary computation\ntechniques, Grammatical Evolution, Context-Free Grammar Genetic Programming,\nand Dynamic Structured Grammatical Evolution, to derive an interpretable\nmathematical expression to estimate the percentage of body fat that are also\naccurate. Our primary objective is to obtain a model that balances accuracy\nwith explainability, making it useful for clinical and health applications. We\ncompare the performance of the three variants on a public anthropometric\ndataset and compare the results obtained with the QLattice framework.\nExperimental results show that grammatical evolution techniques can obtain\ncompetitive results in performance and interpretability."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.05558",
    "c_title":[
      "Quantum Simplicial Neural Networks"
    ],
    "c_abstract":[
      "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.04224",
    "c_title":[
      "Speeding up Local Search for the Indicator-based Subset Selection\n  Problem by a Candidate List Strategy"
    ],
    "c_abstract":[
      "In evolutionary multi-objective optimization, the indicator-based subset\nselection problem involves finding a subset of points that maximizes a given\nquality indicator. Local search is an effective approach for obtaining a\nhigh-quality subset in this problem. However, local search requires high\ncomputational cost, especially as the size of the point set and the number of\nobjectives increase. To address this issue, this paper proposes a candidate\nlist strategy for local search in the indicator-based subset selection problem.\nIn the proposed strategy, each point in a given point set has a candidate list.\nDuring search, each point is only eligible to swap with unselected points in\nits associated candidate list. This restriction drastically reduces the number\nof swaps at each iteration of local search. We consider two types of candidate\nlists: nearest neighbor and random neighbor lists. This paper investigates the\neffectiveness of the proposed candidate list strategy on various Pareto fronts.\nThe results show that the proposed strategy with the nearest neighbor list can\nsignificantly speed up local search on continuous Pareto fronts without\nsignificantly compromising the subset quality. The results also show that the\nsequential use of the two lists can address the discontinuity of Pareto fronts."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.14490",
    "c_title":[
      "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields"
    ],
    "c_abstract":[
      "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.12743",
    "c_title":[
      "Cancermorphic Computing Toward Multilevel Machine Intelligence"
    ],
    "c_abstract":[
      "Despite their potential to address crucial bottlenecks in computing\narchitectures and contribute to the pool of biological inspiration for\nengineering, pathological biological mechanisms remain absent from\ncomputational theory. We hereby introduce the concept of cancer-inspired\ncomputing as a paradigm drawing from the adaptive, resilient, and evolutionary\nstrategies of cancer, for designing computational systems capable of thriving\nin dynamic, adversarial or resource-constrained environments. Unlike known\nbioinspired approaches (e.g., evolutionary and neuromorphic architectures),\ncancer-inspired computing looks at emulating the uniqueness of cancer cells\nsurvival tactics, such as somatic mutation, metastasis, angiogenesis and immune\nevasion, as parallels to desirable features in computing architectures, for\nexample decentralized propagation and resource optimization, to impact areas\nlike fault tolerance and cybersecurity. While the chaotic growth of cancer is\ncurrently viewed as uncontrollable in biology, randomness-based algorithms are\nalready being successfully demonstrated in enhancing the capabilities of other\ncomputing architectures, for example chaos computing integration. This vision\nfocuses on the concepts of multilevel intelligence and context-driven mutation,\nand their potential to simultaneously overcome plasticity-limited neuromorphic\napproaches and the randomness of chaotic approaches. The introduction of this\nconcept aims to generate interdisciplinary discussion to explore the potential\nof cancer-inspired mechanisms toward powerful and resilient artificial systems."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.10272",
    "c_title":[
      "Symmetry and classification of positive solutions of some weighted\n  elliptic equations"
    ],
    "c_abstract":[
      "We study the weighted elliptic equation \\begin{equation} -div(|x|^{-2a}\\nabla\nu)=|x|^{-bp}|u|^{p-2}u~~~\\mbox{in}~\\mathbb{R}^N\n~~~~~~~~~~~~~~~~~~~~(0.1)\\end{equation} with $N\\geq 2$, which arises from the\nCaffarelli-Kohn-Nirenberg inequalities. Under the assumptions of finite energy\nand $a_1+a_2=N-2$, for nonnegative solutions we prove the equivalence between\nequation (0.1) with $a=a_1$ and equation (0.1) with $a=a_2$. Without finite\nenergy assumptions, for $2\\leq p<2^*$ we give the optimal parameter range in\nwhich nonnegative solutions of (0.1) in $\\mathbf{L}^\\infty_{Loc}(\\mathbb{R}^N)$\nmust be radially symmetric, and give a complete classification for these\nsolutions in this range."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.06886",
    "c_title":[
      "On the extreme complexity of certain nearly regular graphs"
    ],
    "c_abstract":[
      "The complexity of a graph is the number of its labeled spanning trees. It is\ndemonstrated that the seven known triangle-free strongly regular graphs, such\nas the Higman-Sims graph, are graphs of maximal complexity among all graphs of\nthe same order and degree; their complements are shown to be of minimal\ncomplexity. A generalization to nearly regular graphs with two distinct\neigevalues of the Laplacian is presented. Conjectures and applications of these\nresults to biological problems on neuronal activity are described."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.12543",
    "c_title":[
      "Regularizing effect of the spatially homogeneous Landau equation with\n  soft potential"
    ],
    "c_abstract":[
      "This paper investigates the Cauchy problem of the spatially homogeneous\nLandau equation with soft potential under the perturbation framework to global\nequilibrium. We prove that the solution to the Cauchy problem exhibits\nanalyticity in the time variable and the Gelfand-Shilov regularizing effect in\nthe velocity variables."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.01509",
    "c_title":[
      "Recommendations for visual predictive checks in Bayesian workflow"
    ],
    "c_abstract":[
      "A key step in the Bayesian workflow for model building is the graphical\nassessment of model predictions, whether these are drawn from the prior or\nposterior predictive distribution. The goal of these assessments is to identify\nwhether the model is a reasonable (and ideally accurate) representation of the\ndomain knowledge and\/or observed data. There are many commonly used visual\npredictive checks which can be misleading if their implicit assumptions do not\nmatch the reality. Thus, there is a need for more guidance for selecting,\ninterpreting, and diagnosing appropriate visualizations. As a visual predictive\ncheck itself can be viewed as a model fit to data, assessing when this model\nfails to represent the data is important for drawing well-informed conclusions.\n  We present recommendations and diagnostic tools to mitigate ad-hoc\ndecision-making in visual predictive checks. These contributions aim to improve\nthe robustness and interpretability of Bayesian model criticism practices. We\noffer recommendations for appropriate visual predictive checks for observations\nthat are: continuous, discrete, or a mixture of the two. We also discuss\ndiagnostics to aid in the selection of visual methods. Specifically, in the\ndetection of an incorrect assumption of continuously-distributed data:\nidentifying when data is likely to be discrete or contain discrete components,\ndetecting and estimating possible bounds in data, and a diagnostic of the\ngoodness-of-fit to data for density plots made through kernel density\nestimates."
    ],
    "c_categories":[
      "stat.CO",
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
    ],
    "b_abstract":[
      "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
    ],
    "b_categories":[
      "physics.comp-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.09301",
    "c_title":[
      "Computing Connection Matrices of Conley Complexes via Algebraic Morse\n  Theory"
    ],
    "c_abstract":[
      "Given a poset-graded chain complex of vector spaces, a Conley complex is the\nminimal chain-homotopic reduction of the initial complex that respects the\nposet grading. A connection matrix is a matrix representing the differential of\nthe Conley complex. In this work, we give an algebraic derivation of the Conley\ncomplex and its connection matrix using homological perturbation theory and\nalgebraic Morse theory. Under this framework, we use a graded splitting of\nrelative chain groups to determine the connection matrix, rather than Forman's\nacyclic partial matching in the usual discrete Morse theory setting. This\nsplitting is obtained by means of the clearing optimisation, a commonly used\ntechnique in persistent homology. Finally, we show how this algebraic\nperspective yields an algorithm for computing the connection matrix via column\nreductions on the differential of the initial complex."
    ],
    "c_categories":[
      "math.AT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06578",
    "c_title":[
      "Generalized exchange cluster algorithm to compute efficiently\n  covariances and susceptibilities in Monte Carlo"
    ],
    "c_abstract":[
      "We present a Monte Carlo method to compute efficiently susceptibilites or\ncovariances of two physical variables. The method relies on a generalization of\nthe exchange cluster algorithm to any model of interacting particles with any\n$2$-body interactions. The principle is to select clusters of variables\nbelonging to two independent replicas of the system. An improved estimator of\nthe covariance of two physical variables (in one replica) is then proposed.\nThis estimator has the zero-variance property in the limit wh ere these\nvariables are independent.In practice the scaling of the statistical\nfluctuations as a function of the number of degrees of freedom $N$ is reduced\nfrom $O(N ^2)$ to $O(N)$. This lower scaling is illustrated on a Lennard Jones\nmodel."
    ],
    "c_categories":[
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17608",
    "c_title":[
      "Accelerating detector simulations with Celeritas: profiling and\n  performance optimizations"
    ],
    "c_abstract":[
      "Celeritas is a GPU-optimized MC particle transport code designed to meet the\ngrowing computational demands of next-generation HEP experiments. It provides\nefficient simulation of EM physics processes in complex geometries with\nmagnetic fields, detector hit scoring, and seamless integration into\nGeant4-driven applications to offload EM physics to GPUs. Recent efforts have\nfocused on performance optimizations and expanding profiling capabilities. This\npaper presents some key advancements, including the integration of the Perfetto\nsystem profiling tool for detailed performance analysis and the development of\ntrack-sorting methods to improve computational efficiency."
    ],
    "c_categories":[
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.20696",
    "c_title":[
      "5f Electron Induced Spin Transport by Sandwich-Type Phthalocyanine"
    ],
    "c_abstract":[
      "In this study, we employed the non-equilibrium Green's function method\ncombined with density functional theory to investigate the spin transport\nproperties of the actinide sandwich phthalocyanine molecule U(Pc)2.This study\naims to provide beneficial assistance for the development of actinide\nphthalocyanine molecular spintronic devices."
    ],
    "c_categories":[
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06403",
    "c_title":[
      "Global physics-informed neural networks (GPINNs): from local point-wise\n  constraint to global nodal association"
    ],
    "c_abstract":[
      "Recently, physics-informed neural networks (PINNs) and their variants have\ngained significant popularity as a scientific computing method for solving\npartial differential equations (PDEs), whereas accuracy is still its main\nshortcoming. Despite numerous development efforts, there is no literature\ndemonstrating that these methods surpass classic numerical algorithms in\nsolving the forward issue. In this paper, by analyzing the disparities between\nPINNs and traditional numerical methods based on mesh discretization, we\ninvestigate the underlying causes for the in adequate precision of PINNs and\nintroduce a novel approach named global physics-informed neural networks\n(GPINNs). Inspired by the crucial concept of global nodal association in\nconventional numerical algorithms, GPINNs leverages the prior field\ndistribution information from pre-trained PINNs to estimate the association\nweights between arbitrary nodes in space. GPINNs can not only be regarded as a\nmeshless approach but also be demonstrated, both theoretically and in practical\ncircumstances, to have the ability of second-order convergence when trained\nwith equidistant nodes. Overall, GPINNs may be seen as an ideal approach to\ninheriting the merits of scientific machine learning (SciML) and conventional\nnumerical computing, which also represent the first SciML algorithm to surpass\nstandard numerical methods in terms of accuracy."
    ],
    "c_categories":[
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.10946",
    "c_title":[
      "Random batch sum-of-Gaussians algorithm for molecular dynamics\n  simulations of Yukawa systems in three dimensions"
    ],
    "c_abstract":[
      "Yukawa systems have drawn widespread interest across various applications. In\nthis paper, we introduce a novel random batch sum-of-Gaussians (RBSOG)\nalgorithm for molecular dynamics simulations of 3D Yukawa systems with periodic\nboundary conditions. We develop a sum-of-Gaussians (SOG) decomposition of the\nYukawa kernel, dividing the interactions into near-field and far-field\ncomponents. The near-field component, singular but compactly supported in a\nlocal domain, is calculated directly. The far-field component, represented as a\nsum of smooth Gaussians, is treated using the random batch approximation in\nFourier space with an adaptive importance sampling strategy to reduce the\nvariance of force calculations. Unlike the traditional Ewald decomposition,\nwhich introduces discontinuities and significant truncation error at the\ncutoff, the SOG decomposition achieves high-order smoothness and accuracy near\nthe cutoff, allowing for efficient and energy-stable simulations. Additionally,\nby avoiding the use of the fast Fourier transform, our method achieves optimal\nO(N) complexity while maintaining high parallel scalability. Finally, unlike\nprevious random batch approaches, the proposed adaptive importance sampling\nstrategy achieves nearly optimal variance reduction across the regime of the\ncoupling parameters. Rigorous theoretical analyses are presented. We validate\nthe performance of RBSOG method through simulations of one-component plasma\nunder weak and strong coupling conditions, using up to 10^6 particles and 1024\nCPU cores. As a practical application in fusion ignition, we simulate\nhigh-temperature, high-density deuterium-\\alpha mixtures to study the energy\nexchange between deuterium and high-energy \\alpha particles. The RBSOG method\ncan be readily extended to other dielectric response functions, offering a\npromising approach for large-scale simulations."
    ],
    "c_categories":[
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03526",
    "c_title":[
      "A Novel First-order Method with Event-driven Objective Evaluations"
    ],
    "c_abstract":[
      "Arising in semi-parametric statistics, control applications, and as\nsub-problems in global optimization methods, certain optimization problems can\nhave objective functions requiring numerical integration to evaluate, yet\ngradient function evaluations that are relatively cheap. For such problems,\ntypical optimization methods that require multiple evaluations of the objective\nfor each new iterate become computationally expensive. In light of this,\noptimization methods that avoid objective function evaluations are attractive,\nyet we show anti-convergence behavior for these methods on the problem class of\ninterest. To address this gap, we develop a novel gradient algorithm that only\nevaluates the objective function when specific events are triggered and propose\na step size scheme that greedily takes advantage of the properties induced by\nthese triggering events. We prove that our methodology has global convergence\nguarantees under the most general smoothness conditions, and show through\nextensive numerical results that our method performs favorably on optimization\nproblems arising in semi-parametric statistics."
    ],
    "c_categories":[
      "math.OC",
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08385",
    "c_title":[
      "On the density of Toeplitz operators in the Toeplitz algebra over the\n  Bergman space of the unit ball"
    ],
    "c_abstract":[
      "We use quantum harmonic analysis and representation theory to provide a new\nproof of Xia's theorem: \"Toeplitz operators are norm dense in the Toeplitz\nalgebra over the Bergman space of the unit ball.\""
    ],
    "c_categories":[
      "math.FA",
      "math.OA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17845",
    "c_title":[
      "Private Information Retrieval on Multigraph-Based Replicated Storage"
    ],
    "c_abstract":[
      "We consider the private information retrieval (PIR) problem for a\nmultigraph-based replication system, where each set of $r$ files is stored on\ntwo of the servers according to an underlying $r$-multigraph. Our goal is to\nestablish upper and lower bounds on the PIR capacity of the $r$-multigraph.\nSpecifically, we first propose a construction for multigraph-based PIR systems\nthat leverages the symmetry of the underlying graph-based PIR scheme, deriving\na capacity lower bound for such multigraphs. Then, we establish a general upper\nbound using linear programming, expressed as a function of the underlying graph\nparameters. Our bounds are demonstrated to be tight for PIR systems on\nmultipaths for even number of vertices."
    ],
    "c_categories":[
      "cs.CR",
      "cs.IT",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12389",
    "c_title":[
      "Homogenization and Mean-Field Approximation for Multi-Player Games"
    ],
    "c_abstract":[
      "We investigate how the framework of mean-field games may be used to\ninvestigate strategic interactions in large heterogeneous populations. We\nconsider strategic interactions in a population of players which may be\npartitioned into near-homogeneous sub-populations subject to peer group effects\nand interactions across groups. We prove a quantitative homogenization result\nfor multi-player games in this setting: we show that $\\epsilon$-Nash equilibria\nof a general multi-player game with heterogeneity may be computed in terms of\nthe Nash equilibria of an auxiliary multi-population mean-field game. We\nprovide explicit and non-asymptotic bounds for the distance from optimality in\nterms of the number of players and the deviations from homogeneity in\nsub-populations. The best mean-field approximation corresponds to an optimal\npartition into sub-populations, which may be formulated as the solution of a\nmixed-integer program."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
    ],
    "b_abstract":[
      "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11683",
    "c_title":[
      "MealMeter: Using Multimodal Sensing and Machine Learning for\n  Automatically Estimating Nutrition Intake"
    ],
    "c_abstract":[
      "Accurate estimation of meal macronutrient composition is a pre-perquisite for\nprecision nutrition, metabolic health monitoring, and glycemic management.\nTraditional dietary assessment methods, such as self-reported food logs or diet\nrecalls are time-intensive and prone to inaccuracies and biases. Several\nexisting AI-driven frameworks are data intensive. In this study, we propose\nMealMeter, a machine learning driven method that leverages multimodal sensor\ndata of wearable and mobile devices. Data are collected from 12 participants to\nestimate macronutrient intake. Our approach integrates physiological signals\n(e.g., continuous glucose, heart rate variability), inertial motion data, and\nenvironmental cues to model the relationship between meal intake and metabolic\nresponses. Using lightweight machine learning models trained on a diverse\ndataset of labeled meal events, MealMeter predicts the composition of\ncarbohydrates, proteins, and fats with high accuracy. Our results demonstrate\nthat multimodal sensing combined with machine learning significantly improves\nmeal macronutrient estimation compared to the baselines including foundation\nmodel and achieves average mean absolute errors (MAE) and average root mean\nsquared relative errors (RMSRE) as low as 13.2 grams and 0.37, respectively,\nfor carbohydrates. Therefore, our developed system has the potential to\nautomate meal tracking, enhance dietary interventions, and support personalized\nnutrition strategies for individuals managing metabolic disorders such as\ndiabetes and obesity."
    ],
    "c_categories":[
      "stat.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"Integrating Large Language Models for Genetic Variant Classification",
    "a_abstract":"The classification of genetic variants, particularly Variants of Uncertain\nSignificance (VUS), poses a significant challenge in clinical genetics and\nprecision medicine. Large Language Models (LLMs) have emerged as transformative\ntools in this realm. These models can uncover intricate patterns and predictive\ninsights that traditional methods might miss, thus enhancing the predictive\naccuracy of genetic variant pathogenicity.\n  This study investigates the integration of state-of-the-art LLMs, including\nGPN-MSA, ESM1b, and AlphaMissense, which leverage DNA and protein sequence data\nalongside structural insights to form a comprehensive analytical framework for\nvariant classification. Our approach evaluates these integrated models using\nthe well-annotated ProteinGym and ClinVar datasets, setting new benchmarks in\nclassification performance. The models were rigorously tested on a set of\nchallenging variants, demonstrating substantial improvements over existing\nstate-of-the-art tools, especially in handling ambiguous and clinically\nuncertain variants.\n  The results of this research underline the efficacy of combining multiple\nmodeling approaches to significantly refine the accuracy and reliability of\ngenetic variant classification systems. These findings support the deployment\nof these advanced computational models in clinical environments, where they can\nsignificantly enhance the diagnostic processes for genetic disorders,\nultimately pushing the boundaries of personalized medicine by offering more\ndetailed and actionable genetic insights.",
    "explanation":"This study investigates the integration of state-of-the-art LLMs,\nincluding GPN-MSA, ESM1b, and AlphaMissense, which leverage DNA and protein sequence data\nalongside structural insights to form a comprehensive analytical framework for variant classification.",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b1"
    ],
    "c_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "c_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08363",
    "c_title":[
      "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry"
    ],
    "c_abstract":[
      "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06253",
    "c_title":[
      "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models"
    ],
    "c_abstract":[
      "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.21125",
    "c_title":[
      "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes"
    ],
    "c_abstract":[
      "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.02284",
    "c_title":[
      "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome"
    ],
    "c_abstract":[
      "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11180",
    "c_title":[
      "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets"
    ],
    "c_abstract":[
      "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03535",
    "c_title":[
      "The anomalously slow dynamics of inhomogeneous quantum annealing"
    ],
    "c_abstract":[
      "Inhomogeneous quantum annealing (IQA), in which transverse fields are turned\noff one by one rather than simultaneously, has been proposed as an effective\nway to avoid the first-order phase transitions that impede conventional quantum\nannealing (QA). Here we explicitly study the dynamics of IQA, rather than\nmerely the thermodynamics, and find that it is appreciably slower than the\nphase diagram would suggest. Interestingly, this slowdown manifests both when\nIQA succeeds in circumventing phase transitions and when it fails. Even in the\nabsence of transitions, such as for the mean-field models that have been\nanalyzed previously, IQA is slower than expected by a factor of the number of\nspins $N$. More significantly, we show that in non-mean-field models,\nfirst-order transitions are likely to be quite common, and the gap at such\ntransitions is not merely exponential in $N$ but exactly zero. Thus IQA cannot\nreach the ground state on any timescale. Both of these results can be\nunderstood through the simple observation that a spin's magnetization becomes\nconserved once its field is turned off during the IQA protocol."
    ],
    "c_categories":[
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04198",
    "c_title":[
      "The Merging Galaxy Cluster Environment Affects the Morphology of\n  Radio-AGN"
    ],
    "c_abstract":[
      "It has previously been found that the galaxy cluster environment can affect\nthe fueling and evolution of Active Galactic Nuclei (AGN). This work examines\nthe effect of the merging cluster environment on the properties of radio-AGN by\ncomparing the radio morphology of cluster members in a sample of four merging\nand eight relaxed galaxy clusters at low redshift (z<0.2). Using 144-MHz data\nfrom the LOFAR Two-metre Sky Survey (LoTSS) and Zooniverse, we classify the\nradio morphology of the radio-detected cluster members using the following\nmorphology classes: compact, compact extended, extended, jetted, and disturbed.\nWe find that the merging cluster environment has a statistically significant,\nhigher population proportion of disturbed (bent and head tail) sources,\nindicating that the merging environment can affect the morphology of cluster\nradio-AGN. We also investigate the number of AGN that are detected in the radio\ndata only, and the number that are detected in both the radio and optical data\nin mergers and non-mergers. We find that the merging cluster environment has a\nhigher population proportion of AGN that are identified only as radio-AGN\ncompared to AGN that are identified as both radio and optical AGN. Overall, we\nfind that the merging environment affects certain radio-AGN (disturbed and only\nradio identified AGN), but not all."
    ],
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10405",
    "c_title":[
      "Optimization techniques for modeling with piecewise-linear functions"
    ],
    "c_abstract":[
      "In this paper we aim to construct piecewise-linear (PWL) approximations for\nfunctions of multiple variables and to build compact mixed-integer linear\nprogramming (MILP) formulations to represent the resulting PWL function. On the\none hand, we describe a simple heuristic to iteratively construct a\ntriangulation with a small number of triangles, while decreasing the error of\nthe piecewise-linear approximation. On the other hand, we extend known\ntechniques for modeling PWLs in MILPs more efficiently than state-of-the-art\nmethods permit. The crux of our method is that the MILP model is a result of\nsolving some hard combinatorial optimization problems, for which we present\nheuristic algorithms. The effectiveness of our techniques is demonstrated by a\nseries of computational experiments including a short-term hydropower\nscheduling problem"
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08800",
    "c_title":[
      "Diophantine enumeration of Dynkin friezes"
    ],
    "c_abstract":[
      "We prove the Fontaine-Plamondon conjecture and show that there are precisely\n$4400$ and $26952$ positive integral $E_7$-friezes and $E_8$-friezes\nrespectively, completing the enumerative classification of all positive\nintegral friezes of Dynkin type. We also give new Diophantine proofs of the\nenumeration theorems for friezes of all other Dynkin types, which were\npreviously proved using discrete geometry, algebraic combinatorics, and the\ntheory of cluster algebras. In general, we count Dynkin friezes of rank $n$ by\ndetermining the positive integral points on certain $n$-dimensional affine\nvarieties with infinitely many integral points."
    ],
    "c_categories":[
      "math.CO",
      "math.NT",
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
    ],
    "b_abstract":[
      "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.05234",
    "c_title":[
      "Unveiling Biases in AI: ChatGPT's Political Economy Perspectives and\n  Human Comparisons"
    ],
    "c_abstract":[
      "We explore the political and ideological positioning of ChatGPT, a leading\nlarge language model (LLM), by comparing its responses to political economy\nquestions from the European Social Survey (ESS). The questions concern\nenvironmental sustainability, civil rights, income inequality, and government\nsize. ChatGPT's self-assessed placement on a left-right political spectrum is\ncompared to the ideological stances of individuals providing similar answers in\nthe ESS dataset. Results highlight a significant left-oriented bias in\nChatGPT's answers, particularly on environmental and civil rights topics,\ndiverging from its same self-declared center-left stance. These findings\nunderscore the need for transparency in AI systems to prevent potential\nideological influences on users. We conclude by discussing the implications for\nAI governance, debiasing strategies, and educational use."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04506",
    "c_title":[
      "When One LLM Drools, Multi-LLM Collaboration Rules"
    ],
    "c_abstract":[
      "This position paper argues that in many realistic (i.e., complex,\ncontextualized, subjective) scenarios, one LLM is not enough to produce a\nreliable output. We challenge the status quo of relying solely on a single\ngeneral-purpose LLM and argue for multi-LLM collaboration to better represent\nthe extensive diversity of data, skills, and people. We first posit that a\nsingle LLM underrepresents real-world data distributions, heterogeneous skills,\nand pluralistic populations, and that such representation gaps cannot be\ntrivially patched by further training a single LLM. We then organize existing\nmulti-LLM collaboration methods into a hierarchy, based on the level of access\nand information exchange, ranging from API-level, text-level, logit-level, to\nweight-level collaboration. Based on these methods, we highlight how multi-LLM\ncollaboration addresses challenges that a single LLM struggles with, such as\nreliability, democratization, and pluralism. Finally, we identify the\nlimitations of existing multi-LLM methods and motivate future work. We envision\nmulti-LLM collaboration as an essential path toward compositional intelligence\nand collaborative AI development."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00916",
    "c_title":[
      "The Accuracy, Robustness, and Readability of LLM-Generated\n  Sustainability-Related Word Definitions"
    ],
    "c_abstract":[
      "A common language with standardized definitions is crucial for effective\nclimate discussions. However, concerns exist about LLMs misrepresenting climate\nterms. We compared 300 official IPCC glossary definitions with those generated\nby GPT-4o-mini, Llama3.1 8B, and Mistral 7B, analyzing adherence, robustness,\nand readability using SBERT sentence embeddings. The LLMs scored an average\nadherence of $0.57-0.59 \\pm 0.15$, and their definitions proved harder to read\nthan the originals. Model-generated definitions vary mainly among words with\nmultiple or ambiguous definitions, showing the potential to highlight terms\nthat need standardization. The results show how LLMs could support\nenvironmental discourse while emphasizing the need to align model outputs with\nestablished terminology for clarity and consistency."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11113",
    "c_title":[
      "Valuable Hallucinations: Realizable Non-realistic Propositions"
    ],
    "c_abstract":[
      "This paper introduces the first formal definition of valuable hallucinations\nin large language models (LLMs), addressing a gap in the existing literature.\nWe provide a systematic definition and analysis of hallucination value,\nproposing methods for enhancing the value of hallucinations. In contrast to\nprevious works, which often treat hallucinations as a broad flaw, we focus on\nthe potential value that certain types of hallucinations can offer in specific\ncontexts. Hallucinations in LLMs generally refer to the generation of\nunfaithful, fabricated, inconsistent, or nonsensical content. Rather than\nviewing all hallucinations negatively, this paper gives formal representations\nand manual judgments of \"valuable hallucinations\" and explores how realizable\nnon-realistic propositions--ideas that are not currently true but could be\nachievable under certain conditions--can have constructive value. We present\nexperiments using the Qwen2.5 model and HalluQA dataset, employing ReAct\nprompting (which involves reasoning, confidence assessment, and answer\nverification) to control and optimize hallucinations. Our findings show that\nReAct prompting results in a 5.12\\% reduction in overall hallucinations and an\nincrease in the proportion of valuable hallucinations from 6.45\\% to 7.92\\%.\nThese results demonstrate that systematically controlling hallucinations can\nimprove their usefulness without compromising factual reliability."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.02460",
    "c_title":[
      "Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large\n  Language Models in Medical Applications"
    ],
    "c_abstract":[
      "Large language models hold promise for addressing medical challenges, such as\nmedical diagnosis reasoning, research knowledge acquisition, clinical\ndecision-making, and consumer health inquiry support. However, they often\ngenerate hallucinations due to limited medical knowledge. Incorporating\nexternal knowledge is therefore critical, which necessitates multi-source\nknowledge acquisition. We address this challenge by framing it as a source\nplanning problem, which is to formulate context-appropriate queries tailored to\nthe attributes of diverse sources. Existing approaches either overlook source\nplanning or fail to achieve it effectively due to misalignment between the\nmodel's expectation of the sources and their actual content. To bridge this\ngap, we present MedOmniKB, a repository comprising multigenre and\nmulti-structured medical knowledge sources. Leveraging these sources, we\npropose the Source Planning Optimisation method, which enhances multi-source\nutilisation. Our approach involves enabling an expert model to explore and\nevaluate potential plans while training a smaller model to learn source\nalignment. Experimental results demonstrate that our method substantially\nimproves multi-source planning performance, enabling the optimised small model\nto achieve state-of-the-art results in leveraging diverse medical knowledge\nsources."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.05867",
    "c_title":[
      "Self-Training Large Language Models for Tool-Use Without Demonstrations"
    ],
    "c_abstract":[
      "Large language models (LLMs) remain prone to factual inaccuracies and\ncomputational errors, including hallucinations and mistakes in mathematical\nreasoning. Recent work augmented LLMs with tools to mitigate these\nshortcomings, but often requires curated gold tool-use demonstrations. In this\npaper, we investigate whether LLMs can learn to use tools without\ndemonstrations. First, we analyse zero-shot prompting strategies to guide LLMs\nin tool utilisation. Second, we propose a self-training method to synthesise\ntool-use traces using the LLM itself. We compare supervised fine-tuning and\npreference fine-tuning techniques for fine-tuning the model on datasets\nconstructed using existing Question Answering (QA) datasets, i.e., TriviaQA and\nGSM8K. Experiments show that tool-use enhances performance on a long-tail\nknowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads\nto mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our\nfindings highlight the potential and challenges of integrating external tools\ninto LLMs without demonstrations."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.14988",
    "c_title":[
      "Fault-Tolerant Optical Quantum Computation using 3D Hybrid Cluster\n  States"
    ],
    "c_abstract":[
      "Hybridizing different physical systems or degrees of freedom offers\nsignificant advantages for realizing practical, universal, scalable, and\nfault-tolerant quantum computation (FTQC). Here, we propose optical FTQC\nschemes with low squeezing thresholds by leveraging the strengths of both\ndiscrete-variable (DV) and continuous-variable (CV) systems while utilizing\nfrequency, time, and orbital angular momentum degrees of freedom. First, we\ndesign an optical entanglement generator (OEG) capable of producing various\ntypes of entangled pairs, including cluster pairs, hybrid entangled pairs, and\nGKP Bell pairs, which can be flexibly chosen by adjusting the measurement\nbasis. Additionally, the OEG features extra ports for directly inputting\n(outputting) data (result) states via quantum teleportation, eliminating the\nneed for optical switches. Secondly, large-scale one-dimensional,\ntwo-dimensional, and three-dimensional (3D) hybrid cluster states, composed of\nDV Gottesman-Kitaev-Preskill (GKP) qubits and CV squeezed states, are\ndeterministically generated using the entangled pairs passed through a\ntime-delay system. Moreover, we optimize the surface-GKP code to further reduce\nlogical errors during the stabilizer measurements in the surface code. By\ncombining the 3D cubic hybrid cluster state with the modified surface-GKP code\nand accounting for full circuit-level noise, FTQC is achieved with a squeezing\nthreshold of 10 dB. Moreover, our method can also generate a 3D macronode\nRaussendorf-Harrington-Goyal (RHG) cluster state, facilitating an alternative\nFTQC scheme via the RHG-GKP code. Our work provides a viable pathway toward\nfuture optical FTQC architectures."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12867",
    "c_title":[
      "Combinatorial Design of Floppy Modes and Frustrated Loops in\n  Metamaterials"
    ],
    "c_abstract":[
      "Metamaterials are a promising platform for a range of applications, from\nshock absorption to mechanical computing. These functionalities typically rely\non floppy modes or mechanically frustrated loops, both of which are difficult\nto design. Here, we introduce a combinatorial approach that allows to create an\narbitrarily large number of floppy modes and frustrated loops. The design\nfreedom of the mode shapes enables us to easily introduce kinematic\nincompatibility to turn them into frustrated loops. We demonstrate that floppy\nmodes can be sequentially buckled by using a specific instance of elastoplastic\nbuckling, and we utilize the designability of floppy chains and frustrated\nloops to demonstrate matrix-vector multiplication in materia."
    ],
    "c_categories":[
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00742",
    "c_title":[
      "An alternative $\\mathbb{Q}$-form of the cyclotomic double shuffle Lie\n  algebra"
    ],
    "c_abstract":[
      "We present an alternative $\\mathbb{Q}$-form for Racinet's cyclotomic double\nshuffle Lie algebra, inspired by the double shuffle relations among congruent\nmultiple zeta values studied by Yuan and Zhao. Our main result establishes an\ninvariance characterization theorem, demonstrating how these two\n$\\mathbb{Q}$-forms can be reconstructed from each other under Galois action."
    ],
    "c_categories":[
      "math.NT",
      "math.QA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15713",
    "c_title":[
      "Modeling shared micromobility as a label propagation process for\n  detecting the overlapping communities"
    ],
    "c_abstract":[
      "Shared micro-mobility such as e-scooters has gained significant popularity in\nmany cities. However, existing methods for detecting community structures in\nmobility networks often overlook potential overlaps between communities. In\nthis study, we conceptualize shared micro-mobility in urban spaces as a process\nof information exchange, where locations are connected through e-scooters,\nfacilitating the interaction and propagation of community affiliations. As a\nresult, similar locations are assigned the same label. Based on this concept,\nwe developed a Geospatial Interaction Propagation model (GIP) by designing a\nSpeaker-Listener Label Propagation Algorithm (SLPA) that accounts for\ngeographic distance decay, incorporating anomaly detection to ensure the\nderived community structures reflect meaningful spatial patterns. We applied\nthis model to detect overlapping communities within the e-scooter system in\nWashington, D.C. The results demonstrate that our algorithm outperforms\nexisting model of overlapping community detection in both efficiency and\nmodularity. However, existing methods for detecting community structures in\nmobility networks often overlook potential overlaps between communities. In\nthis study, we conceptualize shared micro-mobility in urban spaces as a process\nof information exchange, where locations are connected through e-scooters,\nfacilitating the interaction and propagation of community affiliations. As a\nresult, similar locations are assigned the same label. Based on this concept,\nwe developed a Geospatial Interaction Propagation model (GIP) by designing a\nSpeaker-Listener Label Propagation Algorithm (SLPA) that accounts for\ngeographic distance decay, incorporating anomaly detection to ensure the\nderived community structures reflect meaningful spatial patterns."
    ],
    "c_categories":[
      "cs.SI",
      "physics.app-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05055",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
    ],
    "b_abstract":[
      "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.12864",
    "c_title":[
      "The $r$-chain minimal and maximal excludant sizes of an overpartition"
    ],
    "c_abstract":[
      "Recently, Andrews and Newman studied the minimal excludant of a partition,\nwhich is the smallest positive integer that is not a part of a partition. Chern\nintroduced the maximal excludant of a partition, which is the largest\nnonnegative integer smaller than the largest part that is not a part of a\npartition. Bhoria, Eyyunnib, Maji and Li investigated the $r$-chain minimal and\nmaximal excludants of a partition. In this article, we consider the $r$-chain\nminimal and maximal excludant sizes of an overpartition."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"scReader: Prompting Large Language Models to Interpret scRNA-seq Data",
    "a_abstract":"Large language models (LLMs) have demonstrated remarkable advancements,\nprimarily due to their capabilities in modeling the hidden relationships within\ntext sequences. This innovation presents a unique opportunity in the field of\nlife sciences, where vast collections of single-cell omics data from multiple\nspecies provide a foundation for training foundational models. However, the\nchallenge lies in the disparity of data scales across different species,\nhindering the development of a comprehensive model for interpreting genetic\ndata across diverse organisms. In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of LLMs with\ndomain-specific representation models for single-cell omics data\ninterpretation. We begin by focusing on genes as the fundamental unit of\nrepresentation. Gene representations are initialized using functional\ndescriptions, leveraging the strengths of mature language models such as\nLLaMA-2. By inputting single-cell gene-level expression data with prompts, we\neffectively model cellular representations based on the differential expression\nlevels of genes across various species and cell types. In the experiments, we\nconstructed developmental cells from humans and mice, specifically targeting\ncells that are challenging to annotate. We evaluated our methodology through\nbasic tasks such as cell annotation and visualization analysis. The results\ndemonstrate the efficacy of our approach compared to other methods using LLMs,\nhighlighting significant improvements in accuracy and interoperability. Our\nhybrid approach enhances the representation of single-cell data and offers a\nrobust framework for future research in cross-species genetic analysis.",
    "explanation":". In this study, we propose an innovative hybrid\napproach that integrates the general knowledge capabilities of\nLLMs with domain-specific representation models for single-cell\nomics data interpretation.",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b18"
    ],
    "c_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "c_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.02284",
    "c_title":[
      "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome"
    ],
    "c_abstract":[
      "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11180",
    "c_title":[
      "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets"
    ],
    "c_abstract":[
      "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04822",
    "c_title":[
      "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)"
    ],
    "c_abstract":[
      "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04869",
    "c_title":[
      "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer"
    ],
    "c_abstract":[
      "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11831",
    "c_title":[
      "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS"
    ],
    "c_abstract":[
      "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14988",
    "c_title":[
      "Fault-Tolerant Optical Quantum Computation using 3D Hybrid Cluster\n  States"
    ],
    "c_abstract":[
      "Hybridizing different physical systems or degrees of freedom offers\nsignificant advantages for realizing practical, universal, scalable, and\nfault-tolerant quantum computation (FTQC). Here, we propose optical FTQC\nschemes with low squeezing thresholds by leveraging the strengths of both\ndiscrete-variable (DV) and continuous-variable (CV) systems while utilizing\nfrequency, time, and orbital angular momentum degrees of freedom. First, we\ndesign an optical entanglement generator (OEG) capable of producing various\ntypes of entangled pairs, including cluster pairs, hybrid entangled pairs, and\nGKP Bell pairs, which can be flexibly chosen by adjusting the measurement\nbasis. Additionally, the OEG features extra ports for directly inputting\n(outputting) data (result) states via quantum teleportation, eliminating the\nneed for optical switches. Secondly, large-scale one-dimensional,\ntwo-dimensional, and three-dimensional (3D) hybrid cluster states, composed of\nDV Gottesman-Kitaev-Preskill (GKP) qubits and CV squeezed states, are\ndeterministically generated using the entangled pairs passed through a\ntime-delay system. Moreover, we optimize the surface-GKP code to further reduce\nlogical errors during the stabilizer measurements in the surface code. By\ncombining the 3D cubic hybrid cluster state with the modified surface-GKP code\nand accounting for full circuit-level noise, FTQC is achieved with a squeezing\nthreshold of 10 dB. Moreover, our method can also generate a 3D macronode\nRaussendorf-Harrington-Goyal (RHG) cluster state, facilitating an alternative\nFTQC scheme via the RHG-GKP code. Our work provides a viable pathway toward\nfuture optical FTQC architectures."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12867",
    "c_title":[
      "Combinatorial Design of Floppy Modes and Frustrated Loops in\n  Metamaterials"
    ],
    "c_abstract":[
      "Metamaterials are a promising platform for a range of applications, from\nshock absorption to mechanical computing. These functionalities typically rely\non floppy modes or mechanically frustrated loops, both of which are difficult\nto design. Here, we introduce a combinatorial approach that allows to create an\narbitrarily large number of floppy modes and frustrated loops. The design\nfreedom of the mode shapes enables us to easily introduce kinematic\nincompatibility to turn them into frustrated loops. We demonstrate that floppy\nmodes can be sequentially buckled by using a specific instance of elastoplastic\nbuckling, and we utilize the designability of floppy chains and frustrated\nloops to demonstrate matrix-vector multiplication in materia."
    ],
    "c_categories":[
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.00742",
    "c_title":[
      "An alternative $\\mathbb{Q}$-form of the cyclotomic double shuffle Lie\n  algebra"
    ],
    "c_abstract":[
      "We present an alternative $\\mathbb{Q}$-form for Racinet's cyclotomic double\nshuffle Lie algebra, inspired by the double shuffle relations among congruent\nmultiple zeta values studied by Yuan and Zhao. Our main result establishes an\ninvariance characterization theorem, demonstrating how these two\n$\\mathbb{Q}$-forms can be reconstructed from each other under Galois action."
    ],
    "c_categories":[
      "math.NT",
      "math.QA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15713",
    "c_title":[
      "Modeling shared micromobility as a label propagation process for\n  detecting the overlapping communities"
    ],
    "c_abstract":[
      "Shared micro-mobility such as e-scooters has gained significant popularity in\nmany cities. However, existing methods for detecting community structures in\nmobility networks often overlook potential overlaps between communities. In\nthis study, we conceptualize shared micro-mobility in urban spaces as a process\nof information exchange, where locations are connected through e-scooters,\nfacilitating the interaction and propagation of community affiliations. As a\nresult, similar locations are assigned the same label. Based on this concept,\nwe developed a Geospatial Interaction Propagation model (GIP) by designing a\nSpeaker-Listener Label Propagation Algorithm (SLPA) that accounts for\ngeographic distance decay, incorporating anomaly detection to ensure the\nderived community structures reflect meaningful spatial patterns. We applied\nthis model to detect overlapping communities within the e-scooter system in\nWashington, D.C. The results demonstrate that our algorithm outperforms\nexisting model of overlapping community detection in both efficiency and\nmodularity. However, existing methods for detecting community structures in\nmobility networks often overlook potential overlaps between communities. In\nthis study, we conceptualize shared micro-mobility in urban spaces as a process\nof information exchange, where locations are connected through e-scooters,\nfacilitating the interaction and propagation of community affiliations. As a\nresult, similar locations are assigned the same label. Based on this concept,\nwe developed a Geospatial Interaction Propagation model (GIP) by designing a\nSpeaker-Listener Label Propagation Algorithm (SLPA) that accounts for\ngeographic distance decay, incorporating anomaly detection to ensure the\nderived community structures reflect meaningful spatial patterns."
    ],
    "c_categories":[
      "cs.SI",
      "physics.app-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Llamafactory: Unified efficient fine-tuning of 100+ language models"
    ],
    "b_abstract":[
      "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12864",
    "c_title":[
      "The $r$-chain minimal and maximal excludant sizes of an overpartition"
    ],
    "c_abstract":[
      "Recently, Andrews and Newman studied the minimal excludant of a partition,\nwhich is the smallest positive integer that is not a part of a partition. Chern\nintroduced the maximal excludant of a partition, which is the largest\nnonnegative integer smaller than the largest part that is not a part of a\npartition. Bhoria, Eyyunnib, Maji and Li investigated the $r$-chain minimal and\nmaximal excludants of a partition. In this article, we consider the $r$-chain\nminimal and maximal excludant sizes of an overpartition."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12583",
    "c_title":[
      "LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful\n  Synthetic Data"
    ],
    "c_abstract":[
      "Despite the growing development of long-context large language models (LLMs),\ndata-centric approaches relying on synthetic data have been hindered by issues\nrelated to faithfulness, which limit their effectiveness in enhancing model\nperformance on tasks such as long-context reasoning and question answering\n(QA). These challenges are often exacerbated by misinformation caused by lack\nof verification, reasoning without attribution, and potential knowledge\nconflicts. We propose LongFaith, a novel pipeline for synthesizing faithful\nlong-context reasoning instruction datasets. By integrating ground truth and\ncitation-based reasoning prompts, we eliminate distractions and improve the\naccuracy of reasoning chains, thus mitigating the need for costly verification\nprocesses. We open-source two synthesized datasets, LongFaith-SFT and\nLongFaith-PO, which systematically address multiple dimensions of faithfulness,\nincluding verified reasoning, attribution, and contextual grounding. Extensive\nexperiments on multi-hop reasoning datasets and LongBench demonstrate that\nmodels fine-tuned on these datasets significantly improve performance. Our\nablation studies highlight the scalability and adaptability of the LongFaith\npipeline, showcasing its broad applicability in developing long-context LLMs."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.08454",
    "c_title":[
      "Stick to Facts: Towards Fidelity-oriented Product Description Generation"
    ],
    "c_abstract":[
      "Different from other text generation tasks, in product description\ngeneration, it is of vital importance to generate faithful descriptions that\nstick to the product attribute information. However, little attention has been\npaid to this problem. To bridge this gap, we propose a model named\nFidelity-oriented Product Description Generator (FPDG). FPDG takes the entity\nlabel of each word into account, since the product attribute information is\nalways conveyed by entity words. Specifically, we first propose a Recurrent\nNeural Network (RNN) decoder based on the Entity-label-guided Long Short-Term\nMemory (ELSTM) cell, taking both the embedding and the entity label of each\nword as input. Second, we establish a keyword memory that stores the entity\nlabels as keys and keywords as values, allowing FPDG to attend to keywords by\nattending to their entity labels. Experiments conducted on a large-scale\nreal-world product description dataset show that our model achieves\nstate-of-the-art performance in terms of both traditional generation metrics\nand human evaluations. Specifically, FPDG increases the fidelity of the\ngenerated descriptions by 25%."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.13737",
    "c_title":[
      "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
    ],
    "c_abstract":[
      "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.16894",
    "c_title":[
      "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment"
    ],
    "c_abstract":[
      "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15594",
    "c_title":[
      "SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention"
    ],
    "c_abstract":[
      "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07664",
    "c_title":[
      "Classification of Gapped Domain Walls of Topological Orders in 2+1\n  dimensions: A Levin-Wen Model Realization"
    ],
    "c_abstract":[
      "This paper introduces a novel systematic construction of gapped domain walls\n(GDWs) within the Levin-Wen (LW) model, advancing our understanding of\ntopological phases. By gluing two LW models along their open sides in a\ncompatible way, we achieve a complete GDW classification by subsets of bulk\ninput data, encompassing $e$-$m$ exchanging GDWs. A generalized bimodule\nstructure is introduced to capture domain-wall excitations. Furthermore, we\ndemonstrate that folding along any GDW yields a gapped boundary (GB) described\nby a Frobenius algebra of the input UFC for the folded model, thus bridging GDW\nand GB classifications within a unified framework."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.02934",
    "c_title":[
      "Train on classical, deploy on quantum: scaling generative quantum\n  machine learning to a thousand qubits"
    ],
    "c_abstract":[
      "We propose an approach to generative quantum machine learning that overcomes\nthe fundamental scaling issues of variational quantum circuits. The core idea\nis to use a class of generative models based on instantaneous quantum\npolynomial circuits, which we show can be trained efficiently on classical\nhardware. Although training is classically efficient, sampling from these\ncircuits is widely believed to be classically hard, and so computational\nadvantages are possible when sampling from the trained model on quantum\nhardware. By combining our approach with a data-dependent parameter\ninitialisation strategy, we do not encounter issues of barren plateaus and\nsuccessfully circumvent the poor scaling of gradient estimation that plagues\ntraditional approaches to quantum circuit optimisation. We investigate and\nevaluate our approach on a number of real and synthetic datasets, training\nmodels with up to one thousand qubits and hundreds of thousands of parameters.\nWe find that the quantum models can successfully learn from high dimensional\ndata, and perform surprisingly well compared to simple energy-based classical\ngenerative models trained with a similar amount of hyperparameter optimisation.\nOverall, our work demonstrates that a path to scalable quantum generative\nmachine learning exists and can be investigated today at large scales."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04829",
    "c_title":[
      "Structures of various types of symmetry in the solar activity cycle"
    ],
    "c_abstract":[
      "The solar cycle is a complex phenomenon. To comprehensively understand it, we\nhave to study various tracers. The most important component of this complex is\nthe solar dynamo, which is understood as self-excitation of the solar magnetic\nfield in the form of traveling waves somewhere in the convection zone. Along\nwith the solar dynamo, the formation of the solar cycle involves other\nprocesses that are associated with the dynamo but are not its necessary part.\nWe give a review of such phenomena that have not yet been explained in terms of\ndynamo theory. We consider the manifestations of the solar cycle in harmonics\nof the solar large-scale surface magnetic field, including zonal, sectorial,\nand tesseral harmonics; analyze their contribution to magnetic energy; and\nidentify phases of the activity cycle using harmonics of different types of\nsymmetry. The universal magnetic scenario of a solar activity cycle does not\ndepend on its number and height. At the beginning of the cycle on the\nphotosphere, the zonal harmonics account for 37-42% of the total energy (not\n100%, as assumed in simplified descriptions). Sectorial harmonics do not\ndisappear at all but account for 5-10% of the total energy. At this stage, the\ngreatest energy (about 40%) is contained in the tesseral harmonics. As the\ncycle develops, the relative energy of zonal harmonics gradually decreases,\nreaching a minimum of 15-18% immediately before the onset of the sunspot\nmaximum. The relative energy of sectorial harmonics increases and reaches a\nmaximum (60-65%) somewhat later than the calendar date of the sunspot maximum.\nA particular feature of the tesseral harmonics is that their relative energy\nindex changes in a much narrower range and never falls below 40% even at the\ncycle minimum. This is due to active regions and nonglobal magnetic fields. It\nis possible that tesseral harmonics are formed in shallow subphotospheric\nlayers."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00185",
    "c_title":[
      "On the fixed-point proportion of self-similar groups"
    ],
    "c_abstract":[
      "We prove that super strongly fractal groups acting on regular rooted trees\nhave null fixed-point proportion. In particular, we show that the fixed-point\nproportion of an infinite family of iterated monodromy groups of exceptional\ncomplex polynomials have the same property. The proof uses the approach of Rafe\nJones in [15] based on martingales and a recent result of the first author on\nthe dynamics of self-similar groups [6]."
    ],
    "c_categories":[
      "math.GR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.18156",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b18"
    ],
    "b_title":[
      "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
    ],
    "b_abstract":[
      "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.00793",
    "c_title":[
      "Improvement of Jensen, Jensen-Steffensen's, and Jensen's functionals\n  related inequalities for various types of convexity"
    ],
    "c_abstract":[
      "In this paper we deal with improvement of Jensen, Jensen-Steffensen's and\nJensen's functionals related inequalities for uniformly convex, phi-convex and\nsuperquadratic functions."
    ],
    "c_categories":[
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in\n  Alzheimer's Disease",
    "a_abstract":"The rapid advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have shown great potential in medical diagnostics, particularly\nin radiology, where datasets such as X-rays are paired with human-generated\ndiagnostic reports. However, a significant research gap exists in the\nneuroimaging field, especially for conditions such as Alzheimer's disease, due\nto the lack of comprehensive diagnostic reports that can be utilized for model\nfine-tuning. This paper addresses this gap by generating synthetic diagnostic\nreports using GPT-4o-mini on structured data from the OASIS-4 dataset, which\ncomprises 663 patients. Using the synthetic reports as ground truth for\ntraining and validation, we then generated neurological reports directly from\nthe images in the dataset leveraging the pre-trained BiomedCLIP and T5 models.\nOur proposed method achieved a BLEU-4 score of 0.1827, ROUGE-L score of 0.3719,\nand METEOR score of 0.4163, revealing its potential in generating clinically\nrelevant and accurate diagnostic reports.",
    "explanation":"However, a significant research gap exists in the\nneuroimaging field, especially for conditions such as Alzheimer\u2019s\ndisease, due to the lack of comprehensive diagnostic reports that\ncan be utilized for model fine-tuning. This paper addresses this\ngap by generating synthetic diagnostic reports using GPT-4o-mini\non structured data from the OASIS-4 dataset, which comprises\n663 patients",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b5"
    ],
    "c_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "c_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15840",
    "c_title":[
      "Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents"
    ],
    "c_abstract":[
      "While Large Language Models (LLMs) can exhibit impressive proficiency in\nisolated, short-term tasks, they often fail to maintain coherent performance\nover longer time horizons. In this paper, we present Vending-Bench, a simulated\nenvironment designed to specifically test an LLM-based agent's ability to\nmanage a straightforward, long-running business scenario: operating a vending\nmachine. Agents must balance inventories, place orders, set prices, and handle\ndaily fees - tasks that are each simple but collectively, over long horizons\n(>20M tokens per run) stress an LLM's capacity for sustained, coherent\ndecision-making. Our experiments reveal high variance in performance across\nmultiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most\nruns and turn a profit, but all models have runs that derail, either through\nmisinterpreting delivery schedules, forgetting orders, or descending into\ntangential \"meltdown\" loops from which they rarely recover. We find no clear\ncorrelation between failures and the point at which the model's context window\nbecomes full, suggesting that these breakdowns do not stem from memory limits.\nApart from highlighting the high variance in performance over long time\nhorizons, Vending-Bench also tests models' ability to acquire capital, a\nnecessity in many hypothetical dangerous AI scenarios. We hope the benchmark\ncan help in preparing for the advent of stronger AI systems."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07374",
    "c_title":[
      "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!"
    ],
    "c_abstract":[
      "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06727",
    "c_title":[
      "Application of Artificial Intelligence (AI) in Civil Engineering"
    ],
    "c_abstract":[
      "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12130",
    "c_title":[
      "Scaling Autonomous Agents via Automatic Reward Modeling And Planning"
    ],
    "c_abstract":[
      "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of text-generation tasks. However, LLMs still struggle with problems\nrequiring multi-step decision-making and environmental feedback, such as online\nshopping, scientific reasoning, and mathematical problem-solving. Unlike pure\ntext data, collecting large-scale decision-making data is challenging.\nMoreover, many powerful LLMs are only accessible through APIs, which hinders\ntheir fine-tuning for agent tasks due to cost and complexity. To address LLM\nagents' limitations, we propose a framework that can automatically learn a\nreward model from the environment without human annotations. This model can be\nused to evaluate the action trajectories of LLM agents and provide heuristics\nfor task planning. Specifically, our approach involves employing one LLM-based\nagent to navigate an environment randomly, generating diverse action\ntrajectories. Subsequently, a separate LLM is leveraged to assign a task intent\nand synthesize a negative response alongside the correct response for each\ntrajectory. These triplets (task intent, positive response, and negative\nresponse) are then utilized as training data to optimize a reward model capable\nof scoring action trajectories. The effectiveness and generalizability of our\nframework are demonstrated through evaluations conducted on different agent\nbenchmarks. In conclusion, our proposed framework represents a significant\nadvancement in enhancing LLM agents' decision-making capabilities. By\nautomating the learning of reward models, we overcome the challenges of data\nscarcity and API limitations, potentially revolutionizing the application of\nLLMs in complex and interactive environments. This research paves the way for\nmore sophisticated AI agents capable of tackling a wide range of real-world\nproblems requiring multi-step decision-making."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16385",
    "c_title":[
      "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation"
    ],
    "c_abstract":[
      "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07664",
    "c_title":[
      "Classification of Gapped Domain Walls of Topological Orders in 2+1\n  dimensions: A Levin-Wen Model Realization"
    ],
    "c_abstract":[
      "This paper introduces a novel systematic construction of gapped domain walls\n(GDWs) within the Levin-Wen (LW) model, advancing our understanding of\ntopological phases. By gluing two LW models along their open sides in a\ncompatible way, we achieve a complete GDW classification by subsets of bulk\ninput data, encompassing $e$-$m$ exchanging GDWs. A generalized bimodule\nstructure is introduced to capture domain-wall excitations. Furthermore, we\ndemonstrate that folding along any GDW yields a gapped boundary (GB) described\nby a Frobenius algebra of the input UFC for the folded model, thus bridging GDW\nand GB classifications within a unified framework."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.02934",
    "c_title":[
      "Train on classical, deploy on quantum: scaling generative quantum\n  machine learning to a thousand qubits"
    ],
    "c_abstract":[
      "We propose an approach to generative quantum machine learning that overcomes\nthe fundamental scaling issues of variational quantum circuits. The core idea\nis to use a class of generative models based on instantaneous quantum\npolynomial circuits, which we show can be trained efficiently on classical\nhardware. Although training is classically efficient, sampling from these\ncircuits is widely believed to be classically hard, and so computational\nadvantages are possible when sampling from the trained model on quantum\nhardware. By combining our approach with a data-dependent parameter\ninitialisation strategy, we do not encounter issues of barren plateaus and\nsuccessfully circumvent the poor scaling of gradient estimation that plagues\ntraditional approaches to quantum circuit optimisation. We investigate and\nevaluate our approach on a number of real and synthetic datasets, training\nmodels with up to one thousand qubits and hundreds of thousands of parameters.\nWe find that the quantum models can successfully learn from high dimensional\ndata, and perform surprisingly well compared to simple energy-based classical\ngenerative models trained with a similar amount of hyperparameter optimisation.\nOverall, our work demonstrates that a path to scalable quantum generative\nmachine learning exists and can be investigated today at large scales."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04829",
    "c_title":[
      "Structures of various types of symmetry in the solar activity cycle"
    ],
    "c_abstract":[
      "The solar cycle is a complex phenomenon. To comprehensively understand it, we\nhave to study various tracers. The most important component of this complex is\nthe solar dynamo, which is understood as self-excitation of the solar magnetic\nfield in the form of traveling waves somewhere in the convection zone. Along\nwith the solar dynamo, the formation of the solar cycle involves other\nprocesses that are associated with the dynamo but are not its necessary part.\nWe give a review of such phenomena that have not yet been explained in terms of\ndynamo theory. We consider the manifestations of the solar cycle in harmonics\nof the solar large-scale surface magnetic field, including zonal, sectorial,\nand tesseral harmonics; analyze their contribution to magnetic energy; and\nidentify phases of the activity cycle using harmonics of different types of\nsymmetry. The universal magnetic scenario of a solar activity cycle does not\ndepend on its number and height. At the beginning of the cycle on the\nphotosphere, the zonal harmonics account for 37-42% of the total energy (not\n100%, as assumed in simplified descriptions). Sectorial harmonics do not\ndisappear at all but account for 5-10% of the total energy. At this stage, the\ngreatest energy (about 40%) is contained in the tesseral harmonics. As the\ncycle develops, the relative energy of zonal harmonics gradually decreases,\nreaching a minimum of 15-18% immediately before the onset of the sunspot\nmaximum. The relative energy of sectorial harmonics increases and reaches a\nmaximum (60-65%) somewhat later than the calendar date of the sunspot maximum.\nA particular feature of the tesseral harmonics is that their relative energy\nindex changes in a much narrower range and never falls below 40% even at the\ncycle minimum. This is due to active regions and nonglobal magnetic fields. It\nis possible that tesseral harmonics are formed in shallow subphotospheric\nlayers."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00185",
    "c_title":[
      "On the fixed-point proportion of self-similar groups"
    ],
    "c_abstract":[
      "We prove that super strongly fractal groups acting on regular rooted trees\nhave null fixed-point proportion. In particular, we show that the fixed-point\nproportion of an infinite family of iterated monodromy groups of exceptional\ncomplex polynomials have the same property. The proof uses the approach of Rafe\nJones in [15] based on martingales and a recent result of the first author on\nthe dynamics of self-similar groups [6]."
    ],
    "c_categories":[
      "math.GR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "2016 Alzheimer's disease facts and figures"
    ],
    "b_abstract":[
      "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.00793",
    "c_title":[
      "Improvement of Jensen, Jensen-Steffensen's, and Jensen's functionals\n  related inequalities for various types of convexity"
    ],
    "c_abstract":[
      "In this paper we deal with improvement of Jensen, Jensen-Steffensen's and\nJensen's functionals related inequalities for uniformly convex, phi-convex and\nsuperquadratic functions."
    ],
    "c_categories":[
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.21217",
    "c_title":[
      "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery"
    ],
    "c_abstract":[
      "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10904",
    "c_title":[
      "Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced\n  Physical Demand"
    ],
    "c_abstract":[
      "We present a hybrid brain-machine interface (BMI) that integrates\nsteady-state visually evoked potential (SSVEP)-based EEG and facial EMG to\nimprove multimodal control and mitigate fatigue in assistive applications.\nTraditional BMIs relying solely on EEG or EMG suffer from inherent limitations;\nEEG-based control requires sustained visual focus, leading to cognitive\nfatigue, while EMG-based control induces muscular fatigue over time. Our system\ndynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP\nsignals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize\ncontrol based on task demands. In a virtual turtle navigation task, the hybrid\nsystem achieved task completion times comparable to an EMG-only approach, while\n90% of users reported reduced or equal physical demand. These findings\ndemonstrate that multimodal BMI systems can enhance usability, reduce strain,\nand improve long-term adherence in assistive technologies."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.05645",
    "c_title":[
      "The Amplitude Modulation Structure of Japanese Infant- and\n  Child-Directed Speech: Longitudinal Data Reveal Universal Acoustic Physical\n  Structures Underpinning Moraic Timing"
    ],
    "c_abstract":[
      "Infant-directed speech (IDS) is highly rhythmic, and in European languages\nIDS is dominated by patterns of amplitude modulation (AM) at ~2Hz (reflecting\nprosody) and ~5Hz (reflecting individual syllables). The rhythm structure of\nspoken Japanese is thought to differ from European stress-timed and\nsyllable-timed languages, depending on moraic units. Morae comprise any onset\nphoneme and vowel phonemes within a syllable, PA-N-DA. Arguably, initial speech\nencoding via cortical tracking by infants is likely to utilize\nlanguage-universal physical acoustic structures in speech rather than\nlanguage-specific structures like morae, since the infant brain must be\nprepared to acquire any human language. Here a language-blind computational\nmodel of linguistic rhythm based on features of the amplitude envelope is used\nto compute these physical acoustic stimulus characteristics for Japanese. Using\n~18,000 samples of natural IDS and child-directed speech (CDS) recorded\nlongitudinally from 6 parents while speaking to their children over the ages\n0-5 years, we find that the temporal modulation patterns that characterise the\namplitude envelope of Japanese are highly similar to those found for\nstress-timed and syllable-timed European languages. However, the AM band\ncorresponding to the syllabic level in CDS\/IDS in European languages (~2-12Hz,\ntheta-rate cortical tracking) was elongated in Japanese (2.5-17Hz). Further,\nthe phase synchronization ratios between the two slowest AM bands were as\nlikely to be 1:3 as 1:2, which differs from European languages where 1:2 ratios\nare dominant. Accordingly, the language-universal amplitude-driven physical\nacoustic structures important for cortical speech tracking flexibly accommodate\nlanguage-specific differences in core rhythmic units."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11201",
    "c_title":[
      "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms"
    ],
    "c_abstract":[
      "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06286",
    "c_title":[
      "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision"
    ],
    "c_abstract":[
      "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05001",
    "c_title":[
      "40 Years of Interdisciplinary Research: Phases, Origins, and Key Turning\n  Points (1981-2020)"
    ],
    "c_abstract":[
      "This study examines the historical evolution of interdisciplinary research\n(IDR) over a 40-year period, focusing on its dynamic trends, phases, and key\nturning points. We apply time series analysis to identify critical years for\ninterdisciplinary citations (CYICs) and categorizes IDR into three distinct\nphases based on these trends: Period I (1981-2002), marked by sporadic and\nlimited interdisciplinary activity; Period II (2003-2016), characterized by the\nemergence of large-scale IDR led primarily by Medicine, with significant\nbreakthroughs in cloning and medical technology; and Period III (2017-present),\nwhere IDR became a widely adopted research paradigm. Our findings indicate that\nIDR has been predominantly concentrated within the Natural Sciences, with\nMedicine consistently at the forefront, and highlights increasing contributions\nfrom Engineering and Environmental disciplines as a new trend. These insights\nenhance the understanding of the evolution of IDR, its driving factors, and the\nshifts in the focus of interdisciplinary collaborations."
    ],
    "c_categories":[
      "cs.DL",
      "physics.soc-ph",
      "stat.AP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16308",
    "c_title":[
      "On the classification of CW complexes with prescribed links"
    ],
    "c_abstract":[
      "Reporting on a computer--assisted search for nonpositively curved CW\ncomplexes of intermediate rank conducted some years ago. Not intended for\npublication."
    ],
    "c_categories":[
      "math.GR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12748",
    "c_title":[
      "Arithmetic properties of generalized Delannoy polynomials and Schr\\\"oder\n  polynomials"
    ],
    "c_abstract":[
      "Let $n$ be any nonnegative integer and \\[\nD_n^{(h)}(x)=\\sum_{k=0}^{n}\\binom{n+k}{2k}^{h}\\binom{2k}{k}^{h}{x}^{k} \\text{\nand } S_{n}^{(h)}(x)=\\sum_{k=0}^{n}\\binom{n+k}{2k}^{h}C_{k}^{h}{x}^{k} \\] be\nthe generalized Delannoy polynomials and Schr\\\"oder polynomials respectively.\nHere $C_k$ is the Catalan number and $h$ is a positive integer. In this paper,\nwe prove that $$\\begin{align*} & \\frac{(2,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}k^a(k+1)^a(2k+1)D_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x],\\\\\n&\\frac{(2,hm-1,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}(-1)^{k}k^a(k+1)^a(2k+1)D_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x],\\\\\n&\\frac{(2,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}k^a(k+1)^a(2k+1)S_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x],\\\\\n&\\frac{(2,m-1,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}(-1)^{k}k^a(k+1)^a(2k+1)S_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x].\n\\end{align*}$$ Taking $a=1$ will confirm some of Z.-W. Sun's conjectures."
    ],
    "c_categories":[
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10743",
    "c_title":[
      "Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for\n  Bimanual Robotic Manipulation"
    ],
    "c_abstract":[
      "Despite the significant success of imitation learning in robotic\nmanipulation, its application to bimanual tasks remains highly challenging.\nExisting approaches mainly learn a policy to predict a distant next-best\nend-effector pose (NBP) and then compute the corresponding joint rotation\nangles for motion using inverse kinematics. However, they suffer from two\nimportant issues: (1) rarely considering the physical robotic structure, which\nmay cause self-collisions or interferences, and (2) overlooking the kinematics\nconstraint, which may result in the predicted poses not conforming to the\nactual limitations of the robot joints. In this paper, we propose Kinematics\nenhanced Spatial-TemporAl gRaph Diffuser (KStar Diffuser). Specifically, (1) to\nincorporate the physical robot structure information into action prediction,\nKStar Diffuser maintains a dynamic spatial-temporal graph according to the\nphysical bimanual joint motions at continuous timesteps. This dynamic graph\nserves as the robot-structure condition for denoising the actions; (2) to make\nthe NBP learning objective consistent with kinematics, we introduce the\ndifferentiable kinematics to provide the reference for optimizing KStar\nDiffuser. This module regularizes the policy to predict more reliable and\nkinematics-aware next end-effector poses. Experimental results show that our\nmethod effectively leverages the physical structural information and generates\nkinematics-aware actions in both simulation and real-world"
    ],
    "c_categories":[
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.07871",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
    ],
    "b_abstract":[
      "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05462",
    "c_title":[
      "Motion Planning of Nonholonomic Cooperative Mobile Manipulators"
    ],
    "c_abstract":[
      "We propose a real-time implementable motion planning technique for\ncooperative object transportation by nonholonomic mobile manipulator robots\n(MMRs) in an environment with static and dynamic obstacles. The proposed motion\nplanning technique works in two steps. A novel visibility vertices-based path\nplanning algorithm computes a global piece-wise linear path between the start\nand the goal location in the presence of static obstacles offline. It defines\nthe static obstacle free space around the path with a set of convex polygons\nfor the online motion planner. We employ a Nonliner Model Predictive Control\n(NMPC) based online motion planning technique for nonholonomic MMRs that\njointly plans for the mobile base and the manipulators arm. It efficiently\nutilizes the locomotion capability of the mobile base and the manipulation\ncapability of the arm. The motion planner plans feasible motion for the MMRs\nand generates trajectory for object transportation considering the kinodynamic\nconstraints and the static and dynamic obstacles. The efficiency of our\napproach is validated by numerical simulation and hardware experiments in\nvaried environments."
    ],
    "c_categories":[
      "cs.MA",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"LoRA-BERT: a Natural Language Processing Model for Robust and Accurate\n  Prediction of long non-coding RNAs",
    "a_abstract":"Long non-coding RNAs (lncRNAs) serve as crucial regulators in numerous\nbiological processes. Although they share sequence similarities with messenger\nRNAs (mRNAs), lncRNAs perform entirely different roles, providing new avenues\nfor biological research. The emergence of next-generation sequencing\ntechnologies has greatly advanced the detection and identification of lncRNA\ntranscripts and deep learning-based approaches have been introduced to classify\nlong non-coding RNAs (lncRNAs). These advanced methods have significantly\nenhanced the efficiency of identifying lncRNAs. However, many of these methods\nare devoid of robustness and accuracy due to the extended length of the\nsequences involved. To tackle this issue, we have introduced a novel\npre-trained bidirectional encoder representation called LoRA-BERT. LoRA-BERT is\ndesigned to capture the importance of nucleotide-level information during\nsequence classification, leading to more robust and satisfactory outcomes. In a\ncomprehensive comparison with commonly used sequence prediction tools, we have\ndemonstrated that LoRA-BERT outperforms them in terms of accuracy and\nefficiency. Our results indicate that, when utilizing the transformer model,\nLoRA-BERT achieves state-of-the-art performance in predicting both lncRNAs and\nmRNAs for human and mouse species. Through the utilization of LoRA-BERT, we\nacquire valuable insights into the traits of lncRNAs and mRNAs, offering the\npotential to aid in the comprehension and detection of diseases linked to\nlncRNAs in humans.",
    "explanation":"The emergence of next-\ngeneration sequencing technologies has greatly advanced the detection and identification of lncRNA\ntranscripts and deep learning-based approaches have been introduced to classify long non-coding\nRNAs (lncRNAs). ",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b20"
    ],
    "c_title":[
      "A primer on deep learning in genomics"
    ],
    "c_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17418",
    "c_title":[
      "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns"
    ],
    "c_abstract":[
      "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08363",
    "c_title":[
      "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry"
    ],
    "c_abstract":[
      "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04869",
    "c_title":[
      "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer"
    ],
    "c_abstract":[
      "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04822",
    "c_title":[
      "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)"
    ],
    "c_abstract":[
      "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11217",
    "c_title":[
      "CoverM: Read alignment statistics for metagenomics"
    ],
    "c_abstract":[
      "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces."
    ],
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05001",
    "c_title":[
      "40 Years of Interdisciplinary Research: Phases, Origins, and Key Turning\n  Points (1981-2020)"
    ],
    "c_abstract":[
      "This study examines the historical evolution of interdisciplinary research\n(IDR) over a 40-year period, focusing on its dynamic trends, phases, and key\nturning points. We apply time series analysis to identify critical years for\ninterdisciplinary citations (CYICs) and categorizes IDR into three distinct\nphases based on these trends: Period I (1981-2002), marked by sporadic and\nlimited interdisciplinary activity; Period II (2003-2016), characterized by the\nemergence of large-scale IDR led primarily by Medicine, with significant\nbreakthroughs in cloning and medical technology; and Period III (2017-present),\nwhere IDR became a widely adopted research paradigm. Our findings indicate that\nIDR has been predominantly concentrated within the Natural Sciences, with\nMedicine consistently at the forefront, and highlights increasing contributions\nfrom Engineering and Environmental disciplines as a new trend. These insights\nenhance the understanding of the evolution of IDR, its driving factors, and the\nshifts in the focus of interdisciplinary collaborations."
    ],
    "c_categories":[
      "cs.DL",
      "physics.soc-ph",
      "stat.AP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16308",
    "c_title":[
      "On the classification of CW complexes with prescribed links"
    ],
    "c_abstract":[
      "Reporting on a computer--assisted search for nonpositively curved CW\ncomplexes of intermediate rank conducted some years ago. Not intended for\npublication."
    ],
    "c_categories":[
      "math.GR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12748",
    "c_title":[
      "Arithmetic properties of generalized Delannoy polynomials and Schr\\\"oder\n  polynomials"
    ],
    "c_abstract":[
      "Let $n$ be any nonnegative integer and \\[\nD_n^{(h)}(x)=\\sum_{k=0}^{n}\\binom{n+k}{2k}^{h}\\binom{2k}{k}^{h}{x}^{k} \\text{\nand } S_{n}^{(h)}(x)=\\sum_{k=0}^{n}\\binom{n+k}{2k}^{h}C_{k}^{h}{x}^{k} \\] be\nthe generalized Delannoy polynomials and Schr\\\"oder polynomials respectively.\nHere $C_k$ is the Catalan number and $h$ is a positive integer. In this paper,\nwe prove that $$\\begin{align*} & \\frac{(2,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}k^a(k+1)^a(2k+1)D_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x],\\\\\n&\\frac{(2,hm-1,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}(-1)^{k}k^a(k+1)^a(2k+1)D_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x],\\\\\n&\\frac{(2,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}k^a(k+1)^a(2k+1)S_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x],\\\\\n&\\frac{(2,m-1,n)}{n(n+1)(n+2)}\n\\sum_{k=1}^{n}(-1)^{k}k^a(k+1)^a(2k+1)S_{k}^{(h)}(x)^{m}\\in\\mathbb{Z}[x].\n\\end{align*}$$ Taking $a=1$ will confirm some of Z.-W. Sun's conjectures."
    ],
    "c_categories":[
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10743",
    "c_title":[
      "Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for\n  Bimanual Robotic Manipulation"
    ],
    "c_abstract":[
      "Despite the significant success of imitation learning in robotic\nmanipulation, its application to bimanual tasks remains highly challenging.\nExisting approaches mainly learn a policy to predict a distant next-best\nend-effector pose (NBP) and then compute the corresponding joint rotation\nangles for motion using inverse kinematics. However, they suffer from two\nimportant issues: (1) rarely considering the physical robotic structure, which\nmay cause self-collisions or interferences, and (2) overlooking the kinematics\nconstraint, which may result in the predicted poses not conforming to the\nactual limitations of the robot joints. In this paper, we propose Kinematics\nenhanced Spatial-TemporAl gRaph Diffuser (KStar Diffuser). Specifically, (1) to\nincorporate the physical robot structure information into action prediction,\nKStar Diffuser maintains a dynamic spatial-temporal graph according to the\nphysical bimanual joint motions at continuous timesteps. This dynamic graph\nserves as the robot-structure condition for denoising the actions; (2) to make\nthe NBP learning objective consistent with kinematics, we introduce the\ndifferentiable kinematics to provide the reference for optimizing KStar\nDiffuser. This module regularizes the policy to predict more reliable and\nkinematics-aware next end-effector poses. Experimental results show that our\nmethod effectively leverages the physical structural information and generates\nkinematics-aware actions in both simulation and real-world"
    ],
    "c_categories":[
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
    ],
    "b_abstract":[
      "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05462",
    "c_title":[
      "Motion Planning of Nonholonomic Cooperative Mobile Manipulators"
    ],
    "c_abstract":[
      "We propose a real-time implementable motion planning technique for\ncooperative object transportation by nonholonomic mobile manipulator robots\n(MMRs) in an environment with static and dynamic obstacles. The proposed motion\nplanning technique works in two steps. A novel visibility vertices-based path\nplanning algorithm computes a global piece-wise linear path between the start\nand the goal location in the presence of static obstacles offline. It defines\nthe static obstacle free space around the path with a set of convex polygons\nfor the online motion planner. We employ a Nonliner Model Predictive Control\n(NMPC) based online motion planning technique for nonholonomic MMRs that\njointly plans for the mobile base and the manipulators arm. It efficiently\nutilizes the locomotion capability of the mobile base and the manipulation\ncapability of the arm. The motion planner plans feasible motion for the MMRs\nand generates trajectory for object transportation considering the kinodynamic\nconstraints and the static and dynamic obstacles. The efficiency of our\napproach is validated by numerical simulation and hardware experiments in\nvaried environments."
    ],
    "c_categories":[
      "cs.MA",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15840",
    "c_title":[
      "Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents"
    ],
    "c_abstract":[
      "While Large Language Models (LLMs) can exhibit impressive proficiency in\nisolated, short-term tasks, they often fail to maintain coherent performance\nover longer time horizons. In this paper, we present Vending-Bench, a simulated\nenvironment designed to specifically test an LLM-based agent's ability to\nmanage a straightforward, long-running business scenario: operating a vending\nmachine. Agents must balance inventories, place orders, set prices, and handle\ndaily fees - tasks that are each simple but collectively, over long horizons\n(>20M tokens per run) stress an LLM's capacity for sustained, coherent\ndecision-making. Our experiments reveal high variance in performance across\nmultiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most\nruns and turn a profit, but all models have runs that derail, either through\nmisinterpreting delivery schedules, forgetting orders, or descending into\ntangential \"meltdown\" loops from which they rarely recover. We find no clear\ncorrelation between failures and the point at which the model's context window\nbecomes full, suggesting that these breakdowns do not stem from memory limits.\nApart from highlighting the high variance in performance over long time\nhorizons, Vending-Bench also tests models' ability to acquire capital, a\nnecessity in many hypothetical dangerous AI scenarios. We hope the benchmark\ncan help in preparing for the advent of stronger AI systems."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07374",
    "c_title":[
      "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!"
    ],
    "c_abstract":[
      "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06727",
    "c_title":[
      "Application of Artificial Intelligence (AI) in Civil Engineering"
    ],
    "c_abstract":[
      "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12130",
    "c_title":[
      "Scaling Autonomous Agents via Automatic Reward Modeling And Planning"
    ],
    "c_abstract":[
      "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of text-generation tasks. However, LLMs still struggle with problems\nrequiring multi-step decision-making and environmental feedback, such as online\nshopping, scientific reasoning, and mathematical problem-solving. Unlike pure\ntext data, collecting large-scale decision-making data is challenging.\nMoreover, many powerful LLMs are only accessible through APIs, which hinders\ntheir fine-tuning for agent tasks due to cost and complexity. To address LLM\nagents' limitations, we propose a framework that can automatically learn a\nreward model from the environment without human annotations. This model can be\nused to evaluate the action trajectories of LLM agents and provide heuristics\nfor task planning. Specifically, our approach involves employing one LLM-based\nagent to navigate an environment randomly, generating diverse action\ntrajectories. Subsequently, a separate LLM is leveraged to assign a task intent\nand synthesize a negative response alongside the correct response for each\ntrajectory. These triplets (task intent, positive response, and negative\nresponse) are then utilized as training data to optimize a reward model capable\nof scoring action trajectories. The effectiveness and generalizability of our\nframework are demonstrated through evaluations conducted on different agent\nbenchmarks. In conclusion, our proposed framework represents a significant\nadvancement in enhancing LLM agents' decision-making capabilities. By\nautomating the learning of reward models, we overcome the challenges of data\nscarcity and API limitations, potentially revolutionizing the application of\nLLMs in complex and interactive environments. This research paves the way for\nmore sophisticated AI agents capable of tackling a wide range of real-world\nproblems requiring multi-step decision-making."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16385",
    "c_title":[
      "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation"
    ],
    "c_abstract":[
      "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.01893",
    "c_title":[
      "Early steep optical decay linked to reverse shock for GRB200131A"
    ],
    "c_abstract":[
      "We observed an optical afterglow of GRB 200131A obtaining the first\nphotometric point 63 s after the satellite trigger. This early observation\nshows a steep decay, suggesting either internal engine activity or a reverse\nshock. By fitting this data set, we show that the early data fit well as a\nreverse shock component of the GRB afterglow modeled as a thin shell expanding\ninto a constant density interstellar matter. The fitting also shows a good\nagreement with a catalogued Milky Way galactic extinction and leaves only\nlittle space for further extinction in the host galaxy. By judging several\nfactors we conclude that the most likely redshift of this GRB is 0.9 +\/- 0.1."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15523",
    "c_title":[
      "Gate Tunable Josephson Diode Effect in Josephson Junctions made from\n  InAs Nanosheets"
    ],
    "c_abstract":[
      "We report the observation of Josephson diode effect (JDE) in hybrid devices\nmade from semiconductor InAs nanosheets and superconductor Al contacts. By\napplying an in-plane magnetic field ($B_{\\mathrm{xy}}$), we detect\nnon-reciprocal superconducting switching current as well as non-reciprocal\nsuperconducting retrapping current. The strength of the JDE depends on the\nangle between the in-plane magnetic field and the bias current\n($I_{\\mathrm{b}}$), reaching its maximum when $B_{\\mathrm{xy}} \\perp\nI_{\\mathrm{b}}$ and dropping to nearly zero when $B_{\\mathrm{xy}}\\parallel\nI_{\\mathrm{b}}$. Additionally, the diode efficiency is tunable via an\nelectrostatic gate with a complete suppression at certain gate voltages. Our\nfindings indicate that the observed JDE in InAs nanosheet-based Josephson\njunctions most likely arises from the Rashba spin-orbit interaction (SOI) in\nthe nanosheets. Such gate-tunable JDE in Josephson junctions made from\nsemiconductor material with SOI is useful not only for constructing advanced\nsuperconducting electronics but also for detecting novel superconducting\nstates."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16074",
    "c_title":[
      "On the conjecture of Shang about free alternative algebras"
    ],
    "c_abstract":[
      "Kashuba and Mathieu proposed a conjecture on vanishing of some components of\nthe homology of certain Lie algebras, implying a description of the\n$GL_d$-module structure of the free $d$-generated Jordan algebra. Their\nconjecture relies on a functorial version of the Tits-Kantor-Koecher\nconstruction that builds Lie algebras out of Jordan algebras. Recently, Shang\nused a functorial construction of Allison, Benkart and Gao that builds Lie\nalgebras out of alternative algebras to propose another conjecture on vanishing\nof some components of the homology of certain Lie algebras, implying a\ndescription of the $GL_d$-module structure of the free $d$-generated\nalternative algebra. In this note, we explain why the conjecture of Shang is\nnot true."
    ],
    "c_categories":[
      "math.KT",
      "math.QA",
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.12268",
    "c_title":[
      "Practical scheme for efficient distillation of GHZ states"
    ],
    "c_abstract":[
      "We develop an efficient local operation and classical communication (LOCC)\nscheme for the distillation of Greenberger-Horne-Zeilinger (GHZ) states from\ntripartite systems subjected to both coherent and incoherent errors. The\nproposed method employs an iterative process that applies a postselection-based\nnon-linear transformation to increase the entanglement of 3-qubit states. In\ncontrast to traditional distillation protocols that require an exponential\nnumber of initial states as a resource, our method achieves subexponential\nconvergence towards a pure GHZ state. The proposed scheme is practical in the\nsense that it employs a small set of relatively simple unitary operations and\nprojective measurements in the computational basis. We systematically develop a\ndouble-iteration protocol by providing a mathematical framework for the\ntransformation processes involved, emphasizing the role of unitary operations\nin correcting arbitrary small errors in the initial states. Through analytical\nderivations and numerical simulations, we demonstrate the protocol's ability to\nprogressively eliminate noise and improve fidelity over subsequent iterations.\nSignificantly, our protocol not only corrects for small arbitrary distortions\nin the GHZ states but also maintains operational simplicity, making it feasible\nfor practical quantum computing applications."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08073",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "A primer on deep learning in genomics"
    ],
    "b_abstract":[
      "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11765",
    "c_title":[
      "On polycyclic linear and additive codes associated to a trinomial over a\n  finite chain ring"
    ],
    "c_abstract":[
      "In this paper, we investigate polycyclic codes associated with a trinomial of\narbitrary degree $n$ over a finite chain ring $ R.$ We extend the concepts of $\nn $-isometry and $ n $-equivalence known for constacyclic codes to this class\nof codes, providing a broader framework for their structural analysis. We\ndescribe the classes of $n$-equivalence and compute their number, significantly\nreducing the study of trinomial codes over $R$. Additionally, we examine the\nspecial case of trinomials of the form $ x^n - a_1x - a_0 \\in R[x] $ and\nanalyze their implications. Finally, we consider the extension of our results\nto certain trinomial additive codes over $ R.$"
    ],
    "c_categories":[
      "cs.IT",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"LightLLM: A Versatile Large Language Model for Predictive Light Sensing",
    "a_abstract":"We propose LightLLM, a model that fine tunes pre-trained large language\nmodels (LLMs) for light-based sensing tasks. It integrates a sensor data\nencoder to extract key features, a contextual prompt to provide environmental\ninformation, and a fusion layer to combine these inputs into a unified\nrepresentation. This combined input is then processed by the pre-trained LLM,\nwhich remains frozen while being fine-tuned through the addition of\nlightweight, trainable components, allowing the model to adapt to new tasks\nwithout altering its original parameters. This approach enables flexible\nadaptation of LLM to specialized light sensing tasks with minimal computational\noverhead and retraining effort. We have implemented LightLLM for three light\nsensing tasks: light-based localization, outdoor solar forecasting, and indoor\nsolar estimation. Using real-world experimental datasets, we demonstrate that\nLightLLM significantly outperforms state-of-the-art methods, achieving 4.4x\nimprovement in localization accuracy and 3.4x improvement in indoor solar\nestimation when tested in previously unseen environments. We further\ndemonstrate that LightLLM outperforms ChatGPT-4 with direct prompting,\nhighlighting the advantages of LightLLM's specialized architecture for sensor\ndata fusion with textual prompts.",
    "explanation":"We propose LightLLM, a model that fine tunes pre-trained\nlarge language models (LLMs) for light-based sensing tasks.",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b9"
    ],
    "c_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "c_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.04530",
    "c_title":[
      "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning"
    ],
    "c_abstract":[
      "Large Language Models (LLMs) excel in reasoning but remain constrained by\ntheir Chain-of-Thought (CoT) approach, which struggles with complex tasks\nrequiring more nuanced topological reasoning. We introduce SOLAR, Scalable\nOptimization of Large-scale Architecture for Reasoning, a framework that\ndynamically optimizes various reasoning topologies to enhance accuracy and\nefficiency.\n  Our Topological Annotation Generation (TAG) system automates topological\ndataset creation and segmentation, improving post-training and evaluation.\nAdditionally, we propose Topological-Scaling, a reward-driven framework that\naligns training and inference scaling, equipping LLMs with adaptive, task-aware\nreasoning.\n  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with\nTopological Tuning, +9% with Topological Reward, and +10.02% with Hybrid\nScaling. It also reduces response length by over 5% for complex problems,\nlowering inference latency.\n  To foster the reward system, we train a multi-task Topological Reward Model\n(M-TRM), which autonomously selects the best reasoning topology and answer in a\nsingle pass, eliminating the need for training and inference on multiple\nsingle-task TRMs (S-TRMs), thus reducing both training cost and inference\nlatency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,\nimproving accuracy by +10% and rank correlation by +9%.\n  To the best of our knowledge, SOLAR sets a new benchmark for scalable,\nhigh-precision LLM reasoning while introducing an automated annotation process\nand a dynamic reasoning topology competition mechanism."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.14540",
    "c_title":[
      "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning"
    ],
    "c_abstract":[
      "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.10071",
    "c_title":[
      "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM"
    ],
    "c_abstract":[
      "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval\/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.06577",
    "c_title":[
      "Transforming Social Science Research with Transfer Learning: Social\n  Science Survey Data Integration with AI"
    ],
    "c_abstract":[
      "Large-N nationally representative surveys, which have profoundly shaped\nAmerican politics scholarship, represent related but distinct domains -a key\ncondition for transfer learning applications. These surveys are related through\ntheir shared demographic, party identification, and ideological variables, yet\ndiffer in that individual surveys often lack specific policy preference\nquestions that researchers require. Our study introduces a novel application of\ntransfer learning (TL) to address these gaps, marking the first systematic use\nof TL paradigms in the context of survey data. Specifically, models pre-trained\non the Cooperative Election Study (CES) dataset are fine-tuned for use in the\nAmerican National Election Studies (ANES) dataset to predict policy questions\nbased on demographic variables. Even with a naive architecture, our transfer\nlearning approach achieves approximately 92 percentage accuracy in predicting\nmissing variables across surveys, demonstrating the robust potential of this\nmethod. Beyond this specific application, our paper argues that transfer\nlearning is a promising framework for maximizing the utility of existing survey\ndata. We contend that artificial intelligence, particularly transfer learning,\nopens new frontiers in social science methodology by enabling systematic\nknowledge transfer between well-administered surveys that share common\nvariables but differ in their outcomes of interest."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.06656",
    "c_title":[
      "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management"
    ],
    "c_abstract":[
      "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.05973",
    "c_title":[
      "On local smoothing estimates for wave equations"
    ],
    "c_abstract":[
      "We prove sharp local smoothing estimates for wave equations on compact\nRiemannian manifolds in 3+1 dimensions and obtained improved estimates in\nhigher dimensions. This is achieved by deriving local smoothing estimates for\ncertain Fourier integral operators. We also obtain improved local smoothing\nestimates for wave equations in Euclidean spaces."
    ],
    "c_categories":[
      "math.AP",
      "math.CA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.17245",
    "c_title":[
      "Extensions of traces for Sobolev mappings into manifolds at the endpoint\n  $p=1$"
    ],
    "c_abstract":[
      "We give direct proofs and constructions of the trace and extension theorems\nfor Sobolev mappings in $W^{1, 1} (M, N)$, where $M$ is Riemannian manifold\nwith compact boundary $\\partial M$ and $N$ is a complete Riemannian manifold.\nThe analysis is also applicable to halfspaces and strips. The extension is\nbased on a tiling the domain of the considered applications by suitably chosen\ndyadic cubes to construct the desired extension. Along the way, we obtain\nasymptotic characterizations of the $L^1$-energy of mappings."
    ],
    "c_categories":[
      "math.AP",
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.02851",
    "c_title":[
      "Exact Matching in Correlated Networks with Node Attributes for Improved\n  Community Recovery"
    ],
    "c_abstract":[
      "We study community detection in multiple networks whose nodes and edges are\njointly correlated. This setting arises naturally in applications such as\nsocial platforms, where a shared set of users may exhibit both correlated\nfriendship patterns and correlated attributes across different platforms.\nExtending the classical Stochastic Block Model (SBM) and its contextual\ncounterpart (CSBM), we introduce the correlated CSBM, which incorporates\nstructural and attribute correlations across graphs. To build intuition, we\nfirst analyze correlated Gaussian Mixture Models, wherein only correlated node\nattributes are available without edges, and identify the conditions under which\nan estimator minimizing the distance between attributes achieves exact matching\nof nodes across the two databases. For correlated CSBMs, we develop a two-step\nprocedure that first applies $k$-core matching to most nodes using edge\ninformation, then refines the matching for the remaining unmatched nodes by\nleveraging their attributes with a distance-based estimator. We identify the\nconditions under which the algorithm recovers the exact node correspondence,\nenabling us to merge the correlated edges and average the correlated attributes\nfor enhanced community detection. Crucially, by aligning and combining graphs,\nwe identify regimes in which community detection is impossible in a single\ngraph but becomes feasible when side information from correlated graphs is\nincorporated. Our results illustrate how the interplay between graph matching\nand community recovery can boost performance, broadening the scope of\nmulti-graph, attribute-based community detection."
    ],
    "c_categories":[
      "cs.IT",
      "cs.SI",
      "math.IT",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.16850",
    "c_title":[
      "Guaranteed upper bounds for iteration errors and modified Kacanov\n  schemes via discrete duality"
    ],
    "c_abstract":[
      "We apply duality theory to discretized convex minimization problems to obtain\ncomputable guaranteed upper bounds for the distance of given discrete functions\nand the exact discrete minimizer. Furthermore, we show that the discrete\nduality framework extends convergence results for the Kacanov scheme to a\nbroader class of problems."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Solar Cells for Indoor Applications: Progress and Development"
    ],
    "b_abstract":[
      "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
    ],
    "b_categories":[
      "astro-ph.SR"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.02733",
    "c_title":[
      "A maximum principle for the Coulomb gas: microscopic density bounds,\n  confinement estimates, and high temperature limits"
    ],
    "c_abstract":[
      "We introduce and prove a maximum principle for a natural quantity related to\nthe $k$-point correlation function of the classical one-component Coulomb gas.\nAs an application, we show that the gas is confined to the droplet by a\nwell-known effective potential in dimensions two and higher. We also prove new\nupper bounds for the particle density in the droplet that apply at any\ntemperature. In particular, we give the first controls on the microscopic point\nprocess for high temperature Coulomb gases beyond the mean-field regime,\nproving that their laws are uniformly tight in the particle number $N$ for any\ninverse temperatures $\\beta_N$. Furthermore, we prove that limit points are\nhomogeneous mixed Poisson point processes if $\\beta_N\\to 0$."
    ],
    "c_categories":[
      "math-ph",
      "math.MP",
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02976",
    "c_title":[
      "Spectroscopic Diagnosis of a B-Class Flare and an Associated Filament\n  Eruption"
    ],
    "c_abstract":[
      "The flare ribbon and an associated filament eruption are diagnosed using O iv\n1401.16 A, Si iv 1402.77 A, and Mg ii k 2796.35 A spectral lines provided by\nIRIS. The flare ribbons have downflow (redshifts) in all these lines, and this\nredshift decreases from the transition region to the chromosphere. While the\noverlapping region (flare-ribbon+filament rise\/eruption is dominated by\nupflows(blueshifts) in all three spectral lines. We found an extremely\nblueshifted Si iv profile (i.e., blueshift around -180 km\/s) in the overlapping\nregion. The mean non-thermal velocity (v_nt) in the flare ribbons is higher in\nO iv than Si iv. While, in the overlapping region, O iv have lower v_nt than Si\niv. Note that very high v_nt around 80 km\/s (in Si iv) exists in this weak\nB-class flare. The Mg ii k line widths are almost the same in the flare ribbon\nand overlapping region but, they are extremely broad than previously reported.\nWe found double peak profiles of Si iv and O iv in the overlapping region. Most\nprobably, one peak is due to downflow (flare ribbon) and another due to upflow\n(filament rise\/eruption). We report a high redshift of more than 150 km\/s in\nthe weak B-class flare. In some cases, both peaks show upflows which might be\nthe result of the superposition of two different sources, i.e., overlapping of\ntwo different velocity distributions in the line of sight."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.05076",
    "c_title":[
      "Temporal Variations in Asteroseismic Frequencies of KIC 6106415:\n  Insights into the Solar-Stellar Activity from GOLF and Kepler Observations"
    ],
    "c_abstract":[
      "The Global Oscillations at Low Frequencies instrument aboard the Solar and\nHeliospheric Observatory has provided over two decades of continuous,\nhigh-precision data, enabling detailed measurements of the Sun's oscillation\nfrequencies. These oscillations, analyzed through Doppler velocity shifts,\noffer invaluable insights into the Sun's internal structure and dynamics using\nthe methods of helioseismology. This methodology has been extended beyond the\nSun to the study of other stars, leveraging data from various space missions.\nNotably, NASA's Kepler mission, in operation from 2009 until 2018, observed\nover 500,000 stars, analyzing brightness variations over time and generating a\nvast database for asteroseismic studies. This investigation focuses on the\nsolar-type star KIC 6106415, comparing its oscillation frequencies with those\nderived from GOLF data. By analyzing frequency patterns and mode lifetimes, we\nexplore the similarities and differences in internal structures, stellar\nevolution, and magnetic activity cycles between KIC 6106415 and the Sun. Our\nanalysis reveals that KIC 6106415 exhibits starspot numbers similar to the Sun,\npeaking at an estimated 175, which is consistent with its faster rotation rate.\nThe data suggest that KIC 6106415 may have shorter magnetic activity cycles\nthan the Sun, reinforcing the established link between stellar rotation and\nmagnetic field generation in solar-type stars."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12107",
    "c_title":[
      "Spindown of massive main sequence stars in the Milky Way"
    ],
    "c_abstract":[
      "Context. We need to understand the spin evolution of massive stars to compute\ntheir internal rotationally induced mixing processes, isolate effects of close\nbinary evolution, and predict the rotation rates of white dwarfs, neutron stars\nand black holes.\n  Aims. We discuss the spindown of massive main sequence stars imposed by\nstellar winds.\n  Methods. We use detailed grids of single star evolutionary models to predict\nthe distribution of the surface rotational velocities of core-hydrogen burning\nGalactic massive stars as function of their mass and evolutionary state. We\nthen compare the spin properties of our synthetic populations with\nappropriately selected sub-samples of Galactic main sequence OB-type stars\nextracted from the IACOB survey.\n  Results. We find that below $\\sim 40 M_\\odot$, observations and models agree\nin finding that the surface rotational velocities of Galactic massive stars\nremain relatively constant during their main sequence evolution. The more\nmassive stars in the IACOB sample appear to spin down less than predicted,\nwhile our updated angular momentum loss prescription predicts an enhanced\nspindown. Furthermore, the observations show a population of fast rotators,\nwith $v \\sin I \\gtrsim 200$ km\/s persisting for all ages, which is not\nreproduced by our synthetic single star populations.\n  Conclusions. We conclude that the wind-induced spindown of massive main\nsequence stars is yet to be fully understood, and that close binary evolution\nmight significantly contribute to the fraction of rapid rotators in massive\nstars."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.14562",
    "c_title":[
      "MHD Modeling of the Near-Sun Evolution of Coronal Mass Ejection\n  Initiated from a Sheared Arcade"
    ],
    "c_abstract":[
      "Coronal mass ejections (CMEs) are phenomena in which the Sun suddenly\nreleases a mass of energy and magnetized plasma, potentially leading to adverse\nspace weather. Numerical simulation provides an important avenue for\ncomprehensively understanding the structure and mechanism of CMEs. Here we\npresent a global-corona MHD simulation of a CME originating from sheared\nmagnetic arcade and its interaction with the near-Sun solar wind. Our\nsimulation encompasses the pre-CME phase with gradual accumulation of free\nmagnetic energy (and building up of a current sheet within the sheared arcade)\nas driven by the photospheric shearing motion, the initiation of CME as\nmagnetic reconnection commences at the current sheet, and its subsequent\nevolution and propagation to around 0.1 AU. A twisted magnetic flux rope (MFR),\nas the main body of the CME, is created by the continuous reconnection during\nthe eruption. By interacting with the ambient field, the MFR experiences both\nrotation and deflection during the evolution. The CME exhibits a typical\nthree-part structure, namely a bright core, a dark cavity and a bright front.\nThe bright core is mainly located at the lower part of the MFR, where plasma is\nrapidly pumped in by the high-speed reconnection outflow. The dark cavity\ncontains both outer layer of the MFR and its overlying field that expands\nrapidly as the whole magnetic structure moves out. The bright front is formed\ndue to compression of plasma ahead of the fast-moving magnetic structure.\nFuture data-driven modeling of CME will be built upon this simulation with real\nobservations used for the bottom boundary conditions."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04854",
    "c_title":[
      "The First In-depth Photometric Study of the Four Delta Scuti Stars Using\n  TESS Data"
    ],
    "c_abstract":[
      "The first in-depth photometric study of four Delta Scuti stars was performed.\nWe used time series data from the Transiting Exoplanet Survey Satellite (TESS)\nthat is available in different sectors. According to the extracted maxima from\nTESS space-based observations, we calculated an ephemeris for each star. We\nestimated the physical parameters of the target stars based on the Gaia Data\nRelease 3 (DR3) parallax method. The results obtained for the surface gravity\nof the stars are consistent with the reports of the TESS Input Catalog and Gaia\nDR3. We estimated the pulsating constant based on the physical parameters and\nperiod of the stars. Therefore, we found that the stars 2MASS 15515693-7759002\nand 2MASS 07513202+0526526 belong to the fundamental, while 2MASS\n00044615+4936439 and 2MASS 10215638-3326137 relate to the first overtone. The\nFourier analysis using the Period04 program was done for each star. As we\nshowed in the Hertzsprung-Russell (H-R) diagram, the stars are located in the\ninstability strip of the Delta Scuti stars region. Four target stars were found\nto be of the low-amplitude Delta Scuti star type."
    ],
    "c_categories":[
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03137",
    "c_title":[
      "Positivity of line bundles on general blow ups of Hirzebruch surfaces"
    ],
    "c_abstract":[
      "We investigate various positivity properties of line bundles on general blow\nups of Hirzebruch surfaces motivated by \\cite{Han}, where the author has\nstudied general blow ups of $\\mathbb{P}^2$. For each of the properties:\nampleness, global generation, very ampleness, and $k$-very ampleness, we\nprovide several sufficient numerical conditions."
    ],
    "c_categories":[
      "math.AG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07887",
    "c_title":[
      "Blow-up of the one-dimensional wave equation with quadratic spatial\n  derivative nonlinearity"
    ],
    "c_abstract":[
      "We investigate the blow-up dynamics of smooth solutions to the\none-dimensional wave equation with a quadratic spatial derivative nonlinearity,\nmotivated by its applications in Effective Field Theory (EFT) in cosmology.\nDespite its relevance, explicit blow-up solutions for this equation have not\nbeen documented in the literature. In this work, we establish the non-existence\nof smooth, exact self-similar blow-up solutions and construct a five-parameter\nfamily of generalized self-similar solutions exhibiting logarithmic growth.\nMoreover, we prove the asymptotic stability of these blow-up solutions.\n  Our proof tackles several significant challenges, including the\nnon-self-adjoint nature of the linearized operator, the presence of unstable\neigenvalues, and, most notably, the treatment of non-compact perturbations. By\nsubstantially advancing Donninger's spectral-theoretic framework, we develop a\nrobust methodology that effectively handles non-compact perturbations. Key\ninnovations include the incorporation of the Lorentz transformation in\nself-similar variables, an adaptation of the functional framework in\n[Merle-Raphael-Rodnianski-Szeftel, Invent.Math., 2022], and a novel resolvent\nestimate. This approach is general and robust, allowing for straightforward\nextensions to higher dimensions and applications to a wide range of nonlinear\nequations."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.14698",
    "c_title":[
      "Hierarchical Count Echo State Network Models with Application to\n  Graduate Student Enrollments"
    ],
    "c_abstract":[
      "Poisson autoregressive count models have evolved into a time series staple\nfor correlated count data. This paper proposes an alternative to Poisson\nautoregressions: count echo state networks. Echo state networks can be\nstatistically analyzed in frequentist manners via optimizing penalized\nlikelihoods, or in Bayesian manners via MCMC sampling. This paper develops\nPoisson echo state techniques for count data and applies them to a massive\ncount data set containing the number of graduate students from 1,758 United\nStates universities during the years 1972-2021 inclusive. Negative binomial\nmodels are also implemented to better handle overdispersion in the counts.\nPerformance of the proposed models are compared via their forecasting\nperformance as judged by several methods. In the end, a hierarchical negative\nbinomial based echo state network is judged as the superior model."
    ],
    "c_categories":[
      "stat.AP",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08508",
    "c_title":[
      "Score-based 3D molecule generation with neural fields"
    ],
    "c_abstract":[
      "We introduce a new representation for 3D molecules based on their continuous\natomic density fields. Using this representation, we propose a new model based\non walk-jump sampling for unconditional 3D molecule generation in the\ncontinuous space using neural fields. Our model, FuncMol, encodes molecular\nfields into latent codes using a conditional neural field, samples noisy codes\nfrom a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these\nsamples in a single step (jump), and finally decodes them into molecular\nfields. FuncMol performs all-atom generation of 3D molecules without\nassumptions on the molecular structure and scales well with the size of\nmolecules, unlike most approaches. Our method achieves competitive results on\ndrug-like molecules and easily scales to macro-cyclic peptides, with at least\none order of magnitude faster sampling. The code is available at\nhttps:\/\/github.com\/prescient-design\/funcmol."
    ],
    "c_categories":[
      "cs.LG",
      "q-bio.BM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15211",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "b_abstract":[
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18794",
    "c_title":[
      "Survey and Improvement Strategies for Gene Prioritization with Large\n  Language Models"
    ],
    "c_abstract":[
      "Rare diseases are challenging to diagnose due to limited patient data and\ngenetic diversity. Despite advances in variant prioritization, many cases\nremain undiagnosed. While large language models (LLMs) have performed well in\nmedical exams, their effectiveness in diagnosing rare genetic diseases has not\nbeen assessed. To identify causal genes, we benchmarked various LLMs for gene\nprioritization. Using multi-agent and Human Phenotype Ontology (HPO)\nclassification, we categorized patients based on phenotypes and solvability\nlevels. As gene set size increased, LLM performance deteriorated, so we used a\ndivide-and-conquer strategy to break the task into smaller subsets. At\nbaseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking\ncausal genes correctly. The multi-agent and HPO approaches helped distinguish\nconfidently solved cases from challenging ones, highlighting the importance of\nknown gene-phenotype associations and phenotype specificity. We found that\ncases with specific phenotypes or clear associations were more accurately\nsolved. However, we observed biases toward well-studied genes and input order\nsensitivity, which hindered gene prioritization. Our divide-and-conquer\nstrategy improved accuracy by overcoming these biases. By utilizing HPO\nclassification, novel multi-agent techniques, and our LLM strategy, we improved\ncausal gene identification accuracy compared to our baseline evaluation. This\napproach streamlines rare disease diagnosis, facilitates reanalysis of unsolved\ncases, and accelerates gene discovery, supporting the development of targeted\ndiagnostics and therapies."
    ],
    "c_categories":[
      "cs.AI",
      "q-bio.GN"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"Scaling Particle Collision Data Analysis",
    "a_abstract":"For decades, researchers have developed task-specific models to address\nscientific challenges across diverse disciplines. Recently, large language\nmodels (LLMs) have shown enormous capabilities in handling general tasks;\nhowever, these models encounter difficulties in addressing real-world\nscientific problems, particularly in domains involving large-scale numerical\ndata analysis, such as experimental high energy physics. This limitation is\nprimarily due to BPE tokenization's inefficacy with numerical data. In this\npaper, we propose a task-agnostic architecture, BBT-Neutron, which employs a\nbinary tokenization method to facilitate pretraining on a mixture of textual\nand large-scale numerical experimental data. We demonstrate the application of\nBBT-Neutron to Jet Origin Identification (JoI), a critical categorization\nchallenge in high-energy physics that distinguishes jets originating from\nvarious quarks or gluons. Our results indicate that BBT-Neutron achieves\ncomparable performance to state-of-the-art task-specific JoI models.\nFurthermore, we examine the scaling behavior of BBT-Neutron's performance with\nincreasing data volume, suggesting the potential for BBT-Neutron to serve as a\nfoundational model for particle physics data analysis, with possible extensions\nto a broad spectrum of scientific computing applications for Big Science\nexperiments, industrial manufacturing and spacial computing. The project code\nis available at https:\/\/github.com\/supersymmetry-technologies\/bbt-neutron.",
    "explanation":"In this paper, we propose a task-agnostic architecture,\nBBT-Neutron, which employs a binary tokenization method to facilitate pre-\ntraining on a mixture of textual and large-scale numerical experimental data. We\ndemonstrate the application of BBT-Neutron to Jet Origin Identification (JoI),\na critical categorization challenge in high-energy physics that distinguishes jets\noriginating from various quarks or gluons",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b29"
    ],
    "c_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "c_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07049",
    "c_title":[
      "The timing and spectral properties of the 2022 outburst of SGR\n  J1935+2154 observed with NICER"
    ],
    "c_abstract":[
      "The magnetar SGR J1935+2154 entered a new active episode on October 10, 2022,\nwith X-ray bursts and enhanced persistent emission. At the tail of high burst\nrate interval, lasting several hours, radio bursts were detected, revealing the\nconnection between the X-ray activities and radio emissions. We analyzed\nobservations of SGR J1935+2154 for nearly three months, using data from Neutron\nStar Interior Composition Explorer (NICER). We report the timing and spectral\nresults following the onset of this outburst. In general, the X-ray flux of the\npersistent emission decays exponentially. While a flare is evident on the light\ncurve, a fast radio burst (FRB) was detected immediately following the peak of\nthis flare. We found a phase jump of pulse profile, with a deviation of\n$0.16\\pm0.03$ phase, which is related to the glitch. The spectra are well fit\nwith the combination of a blackbody and a power law model. The decay of the\noutburst is dominated by the drop of the non-thermal component, which also\nleads to the increase of thermal proportion. The photon index of the power law\nis inversely correlated with both the unabsorbed flux and the burst rate. We\nfind that unlike the large variety of the persistent emission around FRB\n221014, the X-ray properties are very stable when FRBs 221021 and 221201\nhappened. These results manifest the connection between glitch, phase jump,\nX-ray burst, and radio burst, crucial for studying the mutation in twisted\nmagnetic fields and constraining the trigger mechanism of radio bursts."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00904",
    "c_title":[
      "Multi-wavelength study of a hyperluminous X-ray source near NGC 6099: a\n  strong IMBH candidate"
    ],
    "c_abstract":[
      "We report on the intriguing properties of a variable X-ray source projected\nat the outskirts of the elliptical galaxy NGC 6099 ($d \\approx 139$ Mpc). If\ntruly located near NGC 6099, this is a hyperluminous X-ray source that reached\nan X-ray luminosity $L_{X} \\approx $ a few times $10^{42}$ erg s$^{-1}$ in 2012\nFebruary (XMM-Newton data), about 50 to 100 times brighter than in 2009 May\n(Chandra) and 2023 August (XMM-Newton). The X-ray spectrum was soft at all\nthree epochs, with a thermal component at $kT \\approx 0.2$ keV and a power-law\nphoton index $>3$. Such properties make it a strong candidate for an\nintermediate mass black hole (IMBH). We also discovered a point-like, blue\noptical counterpart ($m_{g,{Vega}}\\approx24.7$~mag,\n$M_{g,{Vega}}\\approx-11.2$~mag), from images taken by the Canada-France-Hawaii\nTelescope, and later confirmed with Hubble Space Telescope observations. The\noptical continuum can be modeled as stellar emission from a compact star\ncluster or an X-ray-irradiated accretion disk, consistent with the IMBH\nscenario. We discuss alternative explanations for the nature of this system. A\npossible scenario is tidal stripping of an orbiting star, with repeated X-ray\noutbursts every few years. An alternative possibility is that the thermal X-ray\nemission seen in 2009 was from shocked gas in the self-intersecting tidal\nstream during the rising phase of a tidal disruption event, while the 2012 and\n2023 emissions were from the fully-formed accretion disk."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11562",
    "c_title":[
      "Investigating the complex absorbers of Mrk 766 with XMM-Newton"
    ],
    "c_abstract":[
      "Aims. We examine the high energy resolution X-ray spectrum of the narrow-line\nSeyfert 1 galaxy Mrk 766 using 4 observations taken with XMM-Newton in 2005, to\ninvestigate the properties of the complex ionised absorber \/ emitter along the\nline of sight, as well as absorption by dust intrinsic to the source.\n  Methods. We make use of the high-energy resolution RGS spectrum to infer the\nproperties of the intervening matter. We also use the spectrum obtained by\nEPIC-pn and the photometric measurements of OM to obtain the spectral energy\ndistribution of the source, necessary for the photoionisation modelling of the\nionised outflow.\n  Results. The warm absorber in Mrk 766 consists of two phases of\nphotoionisation. In addition to these two warm absorber components with\n$\\log\\xi\\sim 2.15$ and $\\log\\xi\\sim -0.58$, we find evidence of absorption by a\ncollisionally ionised component ($T\\sim51$ eV). We discuss the implication of\nthis additional component in light of theoretical predictions. Moreover, we\ndetect signs of absorption by a dusty medium with $N_\\text{dust}\\sim 7.29\\times\n10^{16}$ cm$^{-2}$. Finally the relatively weak emission features in the\nspectrum seem to be unrelated to the absorbers and probably originated by an\nout-of sight-line ionised plasma."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17681",
    "c_title":[
      "Properties of 'Lite' Intermediate-Mass Black Hole Candidates in\n  LIGO-Virgo's Third Observing Run"
    ],
    "c_abstract":[
      "Over a hundred gravitational-wave (GW) detections and candidates have been\nreported from the first three observing runs of the Advanced LIGO-Virgo-KAGRA\n(LVK) detectors. Among these, the most intriguing events are binary black hole\nmergers that result in a 'lite' intermediate-mass black hole (IMBH) of\n${\\sim}10^2~\\mathrm{M}_\\odot$, such as GW170502 and GW190521. In this study, we\ninvestigate 11 GW candidates from LVK's Third Observing Run (April 2019-March\n2020) that have a total detector-frame masses in the lite IMBH range. Using the\nBayesian inference algorithm \\texttt{RIFT}, we systematically analyze these\ncandidates with three state-of-the-art waveform models that incorporate higher\nharmonics, which are crucial for resolving lite IMBHs in LVK data. For each\ncandidate, we infer the pre-merger and post-merger black hole masses in the\nsource frame, along with black hole spin projections across all three models.\nUnder the assumption that these are binary black hole mergers, our analysis\nfinds that 5 of them have a post-merger lite IMBH with masses ranging from\n$110\\sim 350~\\mathrm{M}_\\odot$ with over 90\\% confidence interval.\nAdditionally, we note that one of their pre-merger black holes is within the\npair-instability supernova mass gap ($60-120~\\mathrm{M}_\\odot$) with more than\n90\\% confidence interval, and additional two pre-merger black holes above the\nmass-gap. Furthermore, we report discrepancies among the three waveform models\nin their mass and spin inferences of lite IMBHs, with at least three GW\ncandidates showing deviations beyond accepted statistical limits. While the\nastrophysical certainty of these candidates cannot be established, our study\nprovides a foundation to probe the lite IMBH population that emerge within the\nlow-frequency noise spectrum of LVK detectors."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09998",
    "c_title":[
      "An in-depth study of Gamma rays from the Starburst Galaxy M 82 with\n  VERITAS"
    ],
    "c_abstract":[
      "Assuming Galactic cosmic rays originate in supernovae and the winds of\nmassive stars, starburst galaxies should produce very-high-energy (VHE; E$>$100\nGeV) gamma-ray emission via the interaction of their copious quantities of\ncosmic rays with the large reservoirs of dense gas within the galaxies. Such\nVHE emission was detected by VERITAS from the starburst galaxy M 82 in 2008-09.\nAn extensive, multi-year campaign followed these initial observations, yielding\na total of 254 h of good quality VERITAS data on M 82. Leveraging modern\nanalysis techniques and the larger exposure, these VERITAS data show a more\nstatistically significant VHE signal ($\\sim$6.5 standard deviations\n($\\sigma$)). The corresponding photon spectrum is well fit by a power law\n($\\Gamma = 2.3 \\pm 0.3_{stat} \\pm0.2_{sys}$) and the observed integral flux is\nF($>$450 GeV) = $(3.2 \\pm0.6_{stat} \\pm 0.6_{sys}) \\times\n10^{-13}~\\mathrm{cm^{-2}~s}^{-1}$, or $\\sim$0.4\\% of the Crab Nebula flux above\nthe same energy threshold. The improved VERITAS measurements, when combined\nwith various multi-wavelength data, enable modeling of the underlying emission\nand transport processes. A purely leptonic scenario is found to be a poor\nrepresentation of the gamma-ray spectral energy distribution (SED). A\nlepto-hadronic scenario with cosmic rays following a power-law spectrum in\nmomentum (index $s\\simeq 2.25$), and with significant bremsstrahlung below\n$1$~GeV, provides a good match to the observed SED. The synchrotron emission\nfrom the secondary electrons indicates that efficient non-radiative losses of\ncosmic-ray electrons may be related to advective escape from the starburst\ncore."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03137",
    "c_title":[
      "Positivity of line bundles on general blow ups of Hirzebruch surfaces"
    ],
    "c_abstract":[
      "We investigate various positivity properties of line bundles on general blow\nups of Hirzebruch surfaces motivated by \\cite{Han}, where the author has\nstudied general blow ups of $\\mathbb{P}^2$. For each of the properties:\nampleness, global generation, very ampleness, and $k$-very ampleness, we\nprovide several sufficient numerical conditions."
    ],
    "c_categories":[
      "math.AG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07887",
    "c_title":[
      "Blow-up of the one-dimensional wave equation with quadratic spatial\n  derivative nonlinearity"
    ],
    "c_abstract":[
      "We investigate the blow-up dynamics of smooth solutions to the\none-dimensional wave equation with a quadratic spatial derivative nonlinearity,\nmotivated by its applications in Effective Field Theory (EFT) in cosmology.\nDespite its relevance, explicit blow-up solutions for this equation have not\nbeen documented in the literature. In this work, we establish the non-existence\nof smooth, exact self-similar blow-up solutions and construct a five-parameter\nfamily of generalized self-similar solutions exhibiting logarithmic growth.\nMoreover, we prove the asymptotic stability of these blow-up solutions.\n  Our proof tackles several significant challenges, including the\nnon-self-adjoint nature of the linearized operator, the presence of unstable\neigenvalues, and, most notably, the treatment of non-compact perturbations. By\nsubstantially advancing Donninger's spectral-theoretic framework, we develop a\nrobust methodology that effectively handles non-compact perturbations. Key\ninnovations include the incorporation of the Lorentz transformation in\nself-similar variables, an adaptation of the functional framework in\n[Merle-Raphael-Rodnianski-Szeftel, Invent.Math., 2022], and a novel resolvent\nestimate. This approach is general and robust, allowing for straightforward\nextensions to higher dimensions and applications to a wide range of nonlinear\nequations."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.14698",
    "c_title":[
      "Hierarchical Count Echo State Network Models with Application to\n  Graduate Student Enrollments"
    ],
    "c_abstract":[
      "Poisson autoregressive count models have evolved into a time series staple\nfor correlated count data. This paper proposes an alternative to Poisson\nautoregressions: count echo state networks. Echo state networks can be\nstatistically analyzed in frequentist manners via optimizing penalized\nlikelihoods, or in Bayesian manners via MCMC sampling. This paper develops\nPoisson echo state techniques for count data and applies them to a massive\ncount data set containing the number of graduate students from 1,758 United\nStates universities during the years 1972-2021 inclusive. Negative binomial\nmodels are also implemented to better handle overdispersion in the counts.\nPerformance of the proposed models are compared via their forecasting\nperformance as judged by several methods. In the end, a hierarchical negative\nbinomial based echo state network is judged as the superior model."
    ],
    "c_categories":[
      "stat.AP",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08508",
    "c_title":[
      "Score-based 3D molecule generation with neural fields"
    ],
    "c_abstract":[
      "We introduce a new representation for 3D molecules based on their continuous\natomic density fields. Using this representation, we propose a new model based\non walk-jump sampling for unconditional 3D molecule generation in the\ncontinuous space using neural fields. Our model, FuncMol, encodes molecular\nfields into latent codes using a conditional neural field, samples noisy codes\nfrom a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these\nsamples in a single step (jump), and finally decodes them into molecular\nfields. FuncMol performs all-atom generation of 3D molecules without\nassumptions on the molecular structure and scales well with the size of\nmolecules, unlike most approaches. Our method achieves competitive results on\ndrug-like molecules and easily scales to macro-cyclic peptides, with at least\none order of magnitude faster sampling. The code is available at\nhttps:\/\/github.com\/prescient-design\/funcmol."
    ],
    "c_categories":[
      "cs.LG",
      "q-bio.BM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b23"
    ],
    "b_title":[
      "DARWIN Series: Domain Specific Large Language Models for Natural Science"
    ],
    "b_abstract":[
      "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18794",
    "c_title":[
      "Survey and Improvement Strategies for Gene Prioritization with Large\n  Language Models"
    ],
    "c_abstract":[
      "Rare diseases are challenging to diagnose due to limited patient data and\ngenetic diversity. Despite advances in variant prioritization, many cases\nremain undiagnosed. While large language models (LLMs) have performed well in\nmedical exams, their effectiveness in diagnosing rare genetic diseases has not\nbeen assessed. To identify causal genes, we benchmarked various LLMs for gene\nprioritization. Using multi-agent and Human Phenotype Ontology (HPO)\nclassification, we categorized patients based on phenotypes and solvability\nlevels. As gene set size increased, LLM performance deteriorated, so we used a\ndivide-and-conquer strategy to break the task into smaller subsets. At\nbaseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking\ncausal genes correctly. The multi-agent and HPO approaches helped distinguish\nconfidently solved cases from challenging ones, highlighting the importance of\nknown gene-phenotype associations and phenotype specificity. We found that\ncases with specific phenotypes or clear associations were more accurately\nsolved. However, we observed biases toward well-studied genes and input order\nsensitivity, which hindered gene prioritization. Our divide-and-conquer\nstrategy improved accuracy by overcoming these biases. By utilizing HPO\nclassification, novel multi-agent techniques, and our LLM strategy, we improved\ncausal gene identification accuracy compared to our baseline evaluation. This\napproach streamlines rare disease diagnosis, facilitates reanalysis of unsolved\ncases, and accelerates gene discovery, supporting the development of targeted\ndiagnostics and therapies."
    ],
    "c_categories":[
      "cs.AI",
      "q-bio.GN"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.01275",
    "c_title":[
      "Enhancing Non-English Capabilities of English-Centric Large Language\n  Models through Deep Supervision Fine-Tuning"
    ],
    "c_abstract":[
      "Large language models (LLMs) have demonstrated significant progress in\nmultilingual language understanding and generation. However, due to the\nimbalance in training data, their capabilities in non-English languages are\nlimited. Recent studies revealed the English-pivot multilingual mechanism of\nLLMs, where LLMs implicitly convert non-English queries into English ones at\nthe bottom layers and adopt English for thinking at the middle layers. However,\ndue to the absence of explicit supervision for cross-lingual alignment in the\nintermediate layers of LLMs, the internal representations during these stages\nmay become inaccurate. In this work, we introduce a deep supervision\nfine-tuning method (DFT) that incorporates additional supervision in the\ninternal layers of the model to guide its workflow. Specifically, we introduce\ntwo training objectives on different layers of LLMs: one at the bottom layers\nto constrain the conversion of the target language into English, and another at\nthe middle layers to constrain reasoning in English. To effectively achieve the\nguiding purpose, we designed two types of supervision signals: logits and\nfeature, which represent a stricter constraint and a relatively more relaxed\nguidance. Our method guides the model to not only consider the final generated\nresult when processing non-English inputs but also ensure the accuracy of\ninternal representations. We conducted extensive experiments on typical\nEnglish-centric large models, LLaMA-2 and Gemma-2, and the results on multiple\nmultilingual datasets show that our method significantly outperforms\ntraditional fine-tuning methods."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.14144",
    "c_title":[
      "UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent\n  approach for plain language adaptations of biomedical text"
    ],
    "c_abstract":[
      "This paper describes our submissions to the TREC 2024 PLABA track with the\naim to simplify biomedical abstracts for a K8-level audience (13-14 years old\nstudents). We tested three approaches using OpenAI's gpt-4o and gpt-4o-mini\nmodels: baseline prompt engineering, a two-AI agent approach, and fine-tuning.\nAdaptations were evaluated using qualitative metrics (5-point Likert scales for\nsimplicity, accuracy, completeness, and brevity) and quantitative readability\nscores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the\ntwo-agent approach and baseline prompt engineering with gpt-4o-mini models show\nsuperior qualitative performance, while fine-tuned models excelled in accuracy\nand completeness but were less simple. The evaluation results demonstrated that\nprompt engineering with gpt-4o-mini outperforms iterative improvement\nstrategies via two-agent approach as well as fine-tuning with gpt-4o. We intend\nto expand our investigation of the results and explore advanced evaluations."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.05268",
    "c_title":[
      "ZOGRASCOPE: A New Benchmark for Property Graphs"
    ],
    "c_abstract":[
      "Natural language interfaces to knowledge graphs have become increasingly\nimportant in recent years, enabling easy and efficient access to structured\ndata. In particular property graphs have seen growing adoption. However, these\nkind of graphs remain relatively underrepresented in research, which has\nfocused in large part on RDF-style graphs. As a matter of fact there is a lack\nof resources for evaluating systems on property graphs, with many existing\ndatasets featuring relatively simple queries. To address this gap, we introduce\nZOGRASCOPE, a benchmark designed specifically for the cypher query language.\nThe benchmark includes a diverse set of manually annotated queries of varying\ncomplexity. We complement this paper with a set of experiments that test the\nperformance of out-of-the-box LLMs of different sizes. Our experiments show\nthat semantic parsing over graphs is still a challenging open problem that can\nnot be solved by prompting LLMs alone."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.04611",
    "c_title":[
      "Towards Data-Efficient Language Models: A Child-Inspired Approach to\n  Language Learning"
    ],
    "c_abstract":[
      "In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.05926",
    "c_title":[
      "LLMs Reproduce Stereotypes of Sexual and Gender Minorities"
    ],
    "c_abstract":[
      "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case."
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.11917",
    "c_title":[
      "On the existence of twisted Shalika periods: the Archimedean case"
    ],
    "c_abstract":[
      "Let $\\K$ be an archimedean local field. We investigate the existence of the\ntwisted Shalika functionals on irreducible admissible smooth representations of\n$\\GL_{2n}(\\K)$ in terms of their L-parameters. As part of our proof, we\nestablish a Hochschild-Serre spectral sequence for nilpotent normal subgroups\nand a Kunneth formula in the framework of Schwartz homology. We also prove the\nanalogous result for twisted linear periods using theta correspondence. The\nexistence of twisted Shalika functionals on representations of\n$\\GL_{2n}^{+}(\\R)$ is also studied, which is of independent interest."
    ],
    "c_categories":[
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.11776",
    "c_title":[
      "Parabolic Dijkgraaf-Witten invariants of links in the $3$-sphere"
    ],
    "c_abstract":[
      "We define a new invariant of links in the $3$-sphere and call it the\nparabolic Dijkgraaf-Witten (DW) invariant. This invariant is a generalization\nof the reduced DW invariant derived by Karuo. In this paper, we compute the\ninvariant of several links over which double branched coverings are\nhomeomorphic to the lens spaces. Moreover, we introduce a procedure for\ncomputing partial information of the parabolic DW invariant using only link\ndiagrams."
    ],
    "c_categories":[
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.18505",
    "c_title":[
      "Error analysis for temporal second-order finite element approximations\n  of axisymmetric mean curvature flow of genus-1 surfaces"
    ],
    "c_abstract":[
      "Existing studies on the convergence of numerical methods for curvature flows\nprimarily focus on first-order temporal schemes. In this paper, we establish a\nnovel error analysis for parametric finite element approximations of genus-1\naxisymmetric mean curvature flow, formulated using two classical second-order\ntime-stepping methods: the Crank-Nicolson method and the BDF2 method. Our\nresults establish optimal error bounds in both the L^2-norm and H^1-norm, along\nwith a superconvergence result in the H^1-norm for each fully discrete\napproximation. Finally, we perform convergence experiments to validate the\ntheoretical findings and present numerical simulations for various genus-1\nsurfaces. Through a series of comparative experiments, we also demonstrate that\nthe methods proposed in this paper exhibit significant mesh advantages."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.10209",
    "c_title":[
      "Mutual Coupling in Holographic MIMO: Physical Modeling and\n  Information-Theoretic Analysis"
    ],
    "c_abstract":[
      "This paper presents a comprehensive framework for holographic multiantenna\ncommunication, a paradigm that integrates both wide apertures and closely\nspaced antennas relative to the wavelength. The presented framework is\nphysically grounded, enabling information-theoretic analyses that inherently\nincorporate correlation and mutual coupling among the antennas. This\nestablishes the combined effects of correlation and coupling on the\ninformation-theoretic performance limits across SNR levels. Additionally, it\nreveals that, by suitably selecting the individual antenna patterns, mutual\ncoupling can be harnessed to either reinforce or counter spatial correlations\nas appropriate for specific SNRs, thereby improving the performance."
    ],
    "c_categories":[
      "cs.IT",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00129",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b29"
    ],
    "b_title":[
      "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
    ],
    "b_abstract":[
      "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.09069",
    "c_title":[
      "Theoretical Guarantees for High Order Trajectory Refinement in\n  Generative Flows"
    ],
    "c_abstract":[
      "Flow matching has emerged as a powerful framework for generative modeling,\noffering computational advantages over diffusion models by leveraging\ndeterministic Ordinary Differential Equations (ODEs) instead of stochastic\ndynamics. While prior work established the worst case optimality of standard\nflow matching under Wasserstein distances, the theoretical guarantees for\nhigher-order flow matching - which incorporates acceleration terms to refine\nsample trajectories - remain unexplored. In this paper, we bridge this gap by\nproving that higher-order flow matching preserves worst case optimality as a\ndistribution estimator. We derive upper bounds on the estimation error for\nsecond-order flow matching, demonstrating that the convergence rates depend\npolynomially on the smoothness of the target distribution (quantified via Besov\nspaces) and key parameters of the ODE dynamics. Our analysis employs neural\nnetwork approximations with carefully controlled depth, width, and sparsity to\nbound acceleration errors across both small and large time intervals,\nultimately unifying these results into a general worst case optimal bound for\nall time steps."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"Effective Fine-Tuning of Vision-Language Models for Accurate Galaxy\n  Morphology Analysis",
    "a_abstract":"Galaxy morphology analysis involves classifying galaxies by their shapes and\nstructures. For this task, directly training domain-specific models on large,\nannotated astronomical datasets is effective but costly. In contrast,\nfine-tuning vision foundation models on a smaller set of astronomical images is\nmore resource-efficient but generally results in lower accuracy. To harness the\nbenefits of both approaches and address their shortcomings, we propose\nGalaxAlign, a novel method that fine-tunes pre-trained foundation models to\nachieve high accuracy on astronomical tasks. Specifically, our method extends a\ncontrastive learning architecture to align three types of data in fine-tuning:\n(1) a set of schematic symbols representing galaxy shapes and structures, (2)\ntextual labels of these symbols, and (3) galaxy images. This way, GalaxAlign\nnot only eliminates the need for expensive pretraining but also enhances the\neffectiveness of fine-tuning. Extensive experiments on galaxy classification\nand similarity search demonstrate that our method effectively fine-tunes\ngeneral pre-trained models for astronomical tasks by incorporating\ndomain-specific multi-modal knowledge.",
    "explanation":"To harness the benefits of\nboth approaches and address their shortcomings, we pro-\npose GalaxAlign, a novel method that fine-tunes pre-trained\nfoundation models to achieve high accuracy on astronom-\nical tasks.",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b4"
    ],
    "c_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "c_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.10568",
    "c_title":[
      "Autoregressive Image Generation with Randomized Parallel Decoding"
    ],
    "c_abstract":[
      "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.01754",
    "c_title":[
      "SDRT: Enhance Vision-Language Models by Self-Distillation with Diverse\n  Reasoning Traces"
    ],
    "c_abstract":[
      "Reasoning is increasingly crucial for various tasks. While chain-of-thought\nprompting enables large language models to leverage reasoning effectively,\nharnessing the reasoning capabilities of Vision-Language Models (VLMs) remains\nchallenging. To solve this problem, we propose a novel self-distillation\nframework that enhances the reasoning capabilities of the model. The proposed\nframework introduces several key innovations. We start by employing a prompt\nlibrary tailored to visual reasoning tasks to generate diverse in-context\nquestions and utilize a two-step reasoning procedure to derive reasoning-guided\nresponses. These responses are then used for self-distillation, enabling the\nmodel to internalize the reasoning process. Additionally, we improve the model\narchitecture with several innovative components, including an intervention\nadapter for efficient parameter updates, a cross-modal skip connection to\nfacilitate information exchange between modalities, and an ensemble learning\nalgorithm to integrate diverse reasoning from multiple in-context questions.\nExtensive experiments show that our method significantly improves the baseline\nperformance across five VQA datasets."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.10266",
    "c_title":[
      "MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object\n  Detection"
    ],
    "c_abstract":[
      "Radar and LiDAR have been widely used in autonomous driving as LiDAR provides\nrich structure information, and radar demonstrates high robustness under\nadverse weather. Recent studies highlight the effectiveness of fusing radar and\nLiDAR point clouds. However, challenges remain due to the modality misalignment\nand information loss during feature extractions. To address these issues, we\npropose a 4D radar-LiDAR framework to mutually enhance their representations.\nInitially, the indicative features from radar are utilized to guide both radar\nand LiDAR geometric feature learning. Subsequently, to mitigate their sparsity\ngap, the shape information from LiDAR is used to enrich radar BEV features.\nExtensive experiments on the View-of-Delft (VoD) dataset demonstrate our\napproach's superiority over existing methods, achieving the highest mAP of\n71.76% across the entire area and 86.36\\% within the driving corridor.\nEspecially for cars, we improve the AP by 4.17% and 4.20% due to the strong\nindicative features and symmetric shapes."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.18665",
    "c_title":[
      "Boosting Virtual Agent Learning and Reasoning: A Step-wise,\n  Multi-dimensional, and Generalist Reward Model with Benchmark"
    ],
    "c_abstract":[
      "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps:\/\/github.com\/Galery23\/Similar-v1."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.09279",
    "c_title":[
      "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption"
    ],
    "c_abstract":[
      "Video Detailed Captioning (VDC) is a crucial task for vision-language\nbridging, enabling fine-grained descriptions of complex video content. In this\npaper, we first comprehensively benchmark current state-of-the-art approaches\nand systematically identified two critical limitations: biased capability\ntowards specific captioning aspect and misalignment with human preferences. To\naddress these deficiencies, we propose Cockatiel, a novel three-stage training\npipeline that ensembles synthetic and human-aligned training for improving VDC\nperformance. In the first stage, we derive a scorer from a meticulously\nannotated dataset to select synthetic captions high-performing on certain\nfine-grained video-caption alignment and human-preferred while disregarding\nothers. Then, we train Cockatiel-13B, using this curated dataset to infuse it\nwith assembled model strengths and human preferences. Finally, we further\ndistill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive\nquantitative and qualitative experiments reflect the effectiveness of our\nmethod, as we not only set new state-of-the-art performance on VDCSCORE in a\ndimension-balanced way but also surpass leading alternatives on human\npreference by a large margin as depicted by the human evaluation results."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.11917",
    "c_title":[
      "On the existence of twisted Shalika periods: the Archimedean case"
    ],
    "c_abstract":[
      "Let $\\K$ be an archimedean local field. We investigate the existence of the\ntwisted Shalika functionals on irreducible admissible smooth representations of\n$\\GL_{2n}(\\K)$ in terms of their L-parameters. As part of our proof, we\nestablish a Hochschild-Serre spectral sequence for nilpotent normal subgroups\nand a Kunneth formula in the framework of Schwartz homology. We also prove the\nanalogous result for twisted linear periods using theta correspondence. The\nexistence of twisted Shalika functionals on representations of\n$\\GL_{2n}^{+}(\\R)$ is also studied, which is of independent interest."
    ],
    "c_categories":[
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.11776",
    "c_title":[
      "Parabolic Dijkgraaf-Witten invariants of links in the $3$-sphere"
    ],
    "c_abstract":[
      "We define a new invariant of links in the $3$-sphere and call it the\nparabolic Dijkgraaf-Witten (DW) invariant. This invariant is a generalization\nof the reduced DW invariant derived by Karuo. In this paper, we compute the\ninvariant of several links over which double branched coverings are\nhomeomorphic to the lens spaces. Moreover, we introduce a procedure for\ncomputing partial information of the parabolic DW invariant using only link\ndiagrams."
    ],
    "c_categories":[
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.18505",
    "c_title":[
      "Error analysis for temporal second-order finite element approximations\n  of axisymmetric mean curvature flow of genus-1 surfaces"
    ],
    "c_abstract":[
      "Existing studies on the convergence of numerical methods for curvature flows\nprimarily focus on first-order temporal schemes. In this paper, we establish a\nnovel error analysis for parametric finite element approximations of genus-1\naxisymmetric mean curvature flow, formulated using two classical second-order\ntime-stepping methods: the Crank-Nicolson method and the BDF2 method. Our\nresults establish optimal error bounds in both the L^2-norm and H^1-norm, along\nwith a superconvergence result in the H^1-norm for each fully discrete\napproximation. Finally, we perform convergence experiments to validate the\ntheoretical findings and present numerical simulations for various genus-1\nsurfaces. Through a series of comparative experiments, we also demonstrate that\nthe methods proposed in this paper exhibit significant mesh advantages."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.10209",
    "c_title":[
      "Mutual Coupling in Holographic MIMO: Physical Modeling and\n  Information-Theoretic Analysis"
    ],
    "c_abstract":[
      "This paper presents a comprehensive framework for holographic multiantenna\ncommunication, a paradigm that integrates both wide apertures and closely\nspaced antennas relative to the wavelength. The presented framework is\nphysically grounded, enabling information-theoretic analyses that inherently\nincorporate correlation and mutual coupling among the antennas. This\nestablishes the combined effects of correlation and coupling on the\ninformation-theoretic performance limits across SNR levels. Additionally, it\nreveals that, by suitably selecting the individual antenna patterns, mutual\ncoupling can be harnessed to either reinforce or counter spatial correlations\nas appropriate for specific SNRs, thereby improving the performance."
    ],
    "c_categories":[
      "cs.IT",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
    ],
    "b_abstract":[
      "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
    ],
    "b_categories":[
      "astro-ph.CO"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.09069",
    "c_title":[
      "Theoretical Guarantees for High Order Trajectory Refinement in\n  Generative Flows"
    ],
    "c_abstract":[
      "Flow matching has emerged as a powerful framework for generative modeling,\noffering computational advantages over diffusion models by leveraging\ndeterministic Ordinary Differential Equations (ODEs) instead of stochastic\ndynamics. While prior work established the worst case optimality of standard\nflow matching under Wasserstein distances, the theoretical guarantees for\nhigher-order flow matching - which incorporates acceleration terms to refine\nsample trajectories - remain unexplored. In this paper, we bridge this gap by\nproving that higher-order flow matching preserves worst case optimality as a\ndistribution estimator. We derive upper bounds on the estimation error for\nsecond-order flow matching, demonstrating that the convergence rates depend\npolynomially on the smoothness of the target distribution (quantified via Besov\nspaces) and key parameters of the ODE dynamics. Our analysis employs neural\nnetwork approximations with carefully controlled depth, width, and sparsity to\nbound acceleration errors across both small and large time intervals,\nultimately unifying these results into a general worst case optimal bound for\nall time steps."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10844",
    "c_title":[
      "Cosmic filament spin -- II: filament spin and its impact on galaxy\n  spin-filament alignment in a cosmological simulation"
    ],
    "c_abstract":[
      "Observational studies have reported that cosmic filaments on the megaparsec\nscale exhibit rotational motion. Subsequent simulation studies have shown\nqualitative agreement with these findings, but quantitative discrepancies\nremain due to differences in data and methods, which require verification. To\naddress this issue, we adopt the same methodology as used in the observations\nto identify filament spin from the galaxy distribution constructed from a\nhydrodynamic simulation. Using the same approach to measure filament spin, we\nfind that the simulation results closely match the observational findings, with\nonly minor discrepancies arising from slight differences in the fraction of\nfilaments classified as dynamically cold or hot based on their dynamic\ntemperature. Additionally, an analysis of how filament spin affects the galaxy\nspin-filament correlation shows that filaments with strong spin signals and\ndynamically cold have a greater impact on the galaxy spin-filament correlation\nthan those with weaker spin signals and dynamically hot filaments. These\nresults not only provide further evidence that cosmic filaments exhibit spin,\nbut also highlight the importance of this rotation in the acquisition of\nangular momentum by individual galaxies. Future studies exploring the influence\nof filament spin on galaxy spin may shed light on the physical origins of\nfilaments and the angular momentum of galaxies."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.15332",
    "c_title":[
      "Euclid Quick Data Release (Q1). The role of cosmic connectivity in\n  shaping galaxy clusters"
    ],
    "c_abstract":[
      "The matter distribution around galaxy clusters is distributed over several\nfilaments, reflecting their positions as nodes in the large-scale cosmic web.\nThe number of filaments connected to a cluster, namely its connectivity, is\nexpected to affect the physical properties of clusters. Using the first Euclid\ngalaxy catalogue from the Euclid Quick Release 1 (Q1), we investigate the\nconnectivity of galaxy clusters and how it correlates with their physical and\ngalaxy member properties. Around 220 clusters located within the three fields\nof Q1 (covering $\\sim 63 \\ \\text{deg}^2$), are analysed in the redshift range\n$0.2 < z < 0.7$. Due to the photometric redshift uncertainty, we reconstruct\nthe cosmic web skeleton, and measure cluster connectivity, in 2-D projected\nslices with a thickness of 170 comoving $h^{-1}.\\text{Mpc}$ and centred on each\ncluster redshift, by using two different filament finder algorithms on the most\nmassive galaxies ($M_*\\ > 10^{10.3} \\ M_\\odot$). In agreement with previous\nmeasurements, we recover the mass-connectivity relation independently of the\nfilament detection algorithm, showing that the most massive clusters are, on\naverage, connected to a larger number of cosmic filaments, consistent with\nhierarchical structure formation models. Furthermore, we explore possible\ncorrelations between connectivities and two cluster properties: the fraction of\nearly-type galaxies and the S\\'ersic index of galaxy members. Our result\nsuggests that the clusters populated by early-type galaxies exhibit higher\nconnectivity compared to clusters dominated by late-type galaxies. These\npreliminary investigations highlight our ability to quantify the impact of the\ncosmic web connectivity on cluster properties with Euclid."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18182",
    "c_title":[
      "Constraints on Baryon Density from the Effective Optical Depth of\n  High-Redshift Quasars"
    ],
    "c_abstract":[
      "We present constraints on the baryonic matter density parameter, $\\Omega_b$,\nwithin the framework of the $\\Lambda$CDM model. Our analysis utilizes\nobservational data on the effective optical depth from high-redshift quasars.\nTo parameterize the photoionization rate $\\Gamma_{-12}$, we employ a B\\'{e}zier\npolynomial. Additionally, we approximate the Hubble parameter at high redshifts\nas $H(z)\\approx 100h\\Omega_m^{1\/2} (1+z)^{3\/2}$ km s$^{-1}$ Mpc$^{-1}$.\nConfidence regions are obtained with $h=0.701\\pm0.013$ and $\\Omega_m = 0.315$,\noptimized by the Planck mission. The best-fit values are $\\Omega_b\n=0.043^{+0.005}_{-0.006}$ and $\\Omega_b = 0.045^{+0.004}_{-0.006}$,\ncorresponding to an old data set and a new data set, respectively. And we test\nthe non-parametric form of $\\Gamma_{-12}$, obtaining $\\Omega_b =\n0.048^{+0.001}_{-0.003}$. These results are consistent with the findings of\nPlanck at the 1 $\\sigma$ confidence level. Our findings underscore the\neffectiveness of quasar datasets in constraining $\\Omega_b$, eliminating the\nneed for independent photoionization rate data. This approach provides detailed\ncosmic information about baryon density and the photoionization history of the\nintergalactic medium."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15511",
    "c_title":[
      "Forecasting the performance of the Minimally Informed foreground\n  cleaning method for CMB polarization observations"
    ],
    "c_abstract":[
      "Astrophysical foreground substraction is crucial to retrieve the cosmic\nmicrowave background (CMB) polarization out of the observed data. Recent\nefforts have been carried out towards the development of a minimally informed\ncomponent separation method to handle a priori unknown foreground spectral\nenergy distributions (SEDs), while being able to estimate both cosmological,\nforeground, and potentially instrumental parameters, jointly. In this paper, we\ndevelop a semi-analytical performance forecasting framework for the minimally\ninformed method and we validate it by comparing its results against direct\nsampling of the harmonic-based likelihood and the pixel domain implementation\nMICMAC. We then use the forecasting tool to demonstrate the robustness of the\nbias correction procedure introduced in the minimally informed approach. We\nfind that a data-driven approach based on the currently available observational\ndata is enough to efficiently regularize the bias of the method."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07185",
    "c_title":[
      "Expansion-history preferences of DESI and external data"
    ],
    "c_abstract":[
      "We explore the origin of the preference of DESI Year-1 baryon acoustic\noscillation (BAO) measurements and external data from cosmic microwave\nbackground (CMB) and type Ia supernovae (SNIa) that dark energy behavior\ndeparts from that expected in the standard cosmological model with vacuum\nenergy ($\\Lambda$CDM). In our analysis, we allow a flexible scaling of the\nexpansion rate with redshift that nevertheless allows reasonably tight\nconstraints on the quantities of interest, and adopt and validate a simple yet\naccurate compression of the CMB data that allows us to constrain our\nphenomenological model of the expansion history. We find that data consistently\nshow a preference for a $\\sim$3-5% increase in the expansion rate at $z\\simeq\n0.5$ relative to that predicted by the standard $\\Lambda$CDM model, in\nexcellent agreement with results from the less flexible $(w_0, w_a)$\nparameterization which was used in previous analyses. Even though our model\nallows a departure from the best-fit $\\Lambda$CDM model at zero redshift, we\nfind no evidence for such a signal. We also find no evidence (at greater than\n1$\\sigma$ significance) for a departure of the expansion rate from the\n$\\Lambda$CDM predictions at higher redshifts for any of the data combinations\nthat we consider. Overall, our results strengthen the robustness of the\nfindings using the combination of DESI, CMB, and SNIa data to dark-energy\nmodeling assumptions."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.14798",
    "c_title":[
      "On the Osculating Spaces of Submanifolds in Euclidean Spaces"
    ],
    "c_abstract":[
      "This paper is a continuation of the papers [2,3,4,5,6]. In this paper the\nosculating spaces of arbitrary order of a manifold embedded in Euclidean space\nare considered. A better estimation of their dimensions as well as the\ndescription of its basis are given."
    ],
    "c_categories":[
      "math.GM"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04619",
    "c_title":[
      "Notes on Divisibility of Catalan Numbers"
    ],
    "c_abstract":[
      "We investigate the divisibility properties of \\sigma(C_n), the\nsum-of-divisors function applied to Catalan numbers, in relation to other\nnumber-theoretic functions. We establish conditions under which C_n has prime\nfactors of the form 6k-1, derive sufficient criteria for divisibility of\n\\sigma(C_n), and explore asymptotic estimates for the growth of \\sigma(C_n)\nusing de Bruijn's theorem. These results provide new insights into the\narithmetic structure of Catalan numbers."
    ],
    "c_categories":[
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00446",
    "c_title":[
      "On uniqueness of the equivariant smooth structure on a real moment-angle\n  manifold"
    ],
    "c_abstract":[
      "The paper is devoted to the well-known problem of smooth structures on\nmoment-angle manifolds. Each real or complex moment-angle manifold has an\nequivariant smooth structure given by an intersection of quadrics corresponding\nto a geometric realisation of a polytope. In 2006 F.Bosio and L.Meersseman\nproved that complex moment-angle manifolds of combinatorially equivalent simple\npolytopes are equivariantly diffeomorphic. Using arguments from calculus we\nderive from this result that real moment-angle manifolds of combinatorially\nequivalent simple polytopes are equivariantly diffeomorphic and the polytopes\nare diffeomorphic as manifolds with corners."
    ],
    "c_categories":[
      "math.AT",
      "math.DG",
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17895",
    "c_title":[
      "Betti elements and full atomic support in rings and monoids"
    ],
    "c_abstract":[
      "Several papers in the recent literature have studied factorization properties\nof affine monoids using the monoid's Betti elements. In this paper, we extend\nthis study to more general rings and monoids. We open by demonstrating the\nissues with computing the complete set of Betti elements of a general\ncommutative cancellative monoid, and as an example compute this set for an\nalgebraic number ring of class number two. We specialize our study to the case\nwhere the monoid has a single Betti element, before examining monoids with full\natomic support (that is, when each Betti element is divisible by every atom).\nFor such a monoid, we show that the catenary degree, tame degree, and omega\nvalue agree and can be computed using the monoid's set of Betti elements. We\nclose by considering Betti elements in block monoids, giving a \"Carlitz-like\"\ncharacterization of block monoids with full atomic support and proving that\nthese are precisely the block monoids having a unique Betti element."
    ],
    "c_categories":[
      "math.AC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19475",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "ImageNet: A large-scale hierarchical image database"
    ],
    "b_abstract":[
      "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02427",
    "c_title":[
      "Monge-Kantorovich quantiles and ranks for image data"
    ],
    "c_abstract":[
      "This paper defines quantiles, ranks and statistical depths for image data by\nleveraging ideas from measure transportation. The first step is to embed a\ndistribution of images in a tangent space, with the framework of linear optimal\ntransport. Therein, Monge-Kantorovich quantiles are shown to provide a\nmeaningful ordering of image data, with outward images having unusual shapes.\nNumerical experiments showcase the relevance of the proposed procedure, for\ndescriptive analysis, outlier detection or statistical testing."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"ChatBCI: A P300 Speller BCI Leveraging Large Language Models for\n  Improved Sentence Composition in Realistic Scenarios",
    "a_abstract":"P300 speller BCIs allow users to compose sentences by selecting target keys\non a GUI through the detection of P300 component in their EEG signals following\nvisual stimuli. Most P300 speller BCIs require users to spell words letter by\nletter, or the first few initial letters, resulting in high keystroke demands\nthat increase time, cognitive load, and fatigue. This highlights the need for\nmore efficient, user-friendly methods for faster sentence composition. In this\nwork, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot\nlearning capabilities of large language models (LLMs) to suggest words from\nuser-spelled initial letters or predict the subsequent word(s), reducing\nkeystrokes and accelerating sentence composition. ChatBCI retrieves word\nsuggestions through remote queries to the GPT-3.5 API. A new GUI, displaying\nGPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300\nclassification. Seven subjects completed two online spelling tasks: 1)\ncopy-spelling a self-composed sentence using ChatBCI, and 2) improvising a\nsentence using ChatBCI's word suggestions. Results demonstrate that in Task 1,\non average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time\nand keystrokes by 62.14% and 53.22%, respectively, and increasing information\ntransfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings\nand a record 8.53 characters\/min for typing speed. Overall, ChatBCI, by\nemploying remote LLM queries, enhances sentence composition in realistic\nscenarios, significantly outperforming traditional spellers without requiring\nlocal model training or storage. ChatBCI's (multi-) word predictions, combined\nwith its new GUI, pave the way for developing next-generation speller BCIs that\nare efficient and effective for real-time communication, especially for users\nwith communication and motor disabilities.",
    "explanation":"In this work, we introduce ChatBCI, a P300 speller BCI that leverages the zero-\nshot learning capabilities of large language models (LLMs) to suggest words from\nuser-spelled initial letters or predict the subsequent word(s), reducing keystrokes\nand accelerating sentence composition. ",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b17"
    ],
    "c_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "c_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10114",
    "c_title":[
      "Infrastructure for AI Agents"
    ],
    "c_abstract":[
      "Increasingly many AI systems can plan and execute interactions in open-ended\nenvironments, such as making phone calls or buying online goods. As developers\ngrow the space of tasks that such AI agents can accomplish, we will need tools\nboth to unlock their benefits and manage their risks. Current tools are largely\ninsufficient because they are not designed to shape how agents interact with\nexisting institutions (e.g., legal and economic systems) or actors (e.g.,\ndigital service providers, humans, other AI agents). For example, alignment\ntechniques by nature do not assure counterparties that some human will be held\naccountable when a user instructs an agent to perform an illegal action. To\nfill this gap, we propose the concept of agent infrastructure: technical\nsystems and shared protocols external to agents that are designed to mediate\nand influence their interactions with and impacts on their environments. Agent\ninfrastructure comprises both new tools and reconfigurations or extensions of\nexisting tools. For example, to facilitate accountability, protocols that tie\nusers to agents could build upon existing systems for user authentication, such\nas OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue\nthat agent infrastructure will be similarly indispensable to ecosystems of\nagents. We identify three functions for agent infrastructure: 1) attributing\nactions, properties, and other information to specific agents, their users, or\nother actors; 2) shaping agents' interactions; and 3) detecting and remedying\nharmful actions from agents. We propose infrastructure that could help achieve\neach function, explaining use cases, adoption, limitations, and open questions.\nMaking progress on agent infrastructure can prepare society for the adoption of\nmore advanced agents."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07350",
    "c_title":[
      "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination\n  in Multi-Agent Systems"
    ],
    "c_abstract":[
      "As scaling large language models faces prohibitive costs, multi-agent systems\nemerge as a promising alternative, though challenged by static knowledge\nassumptions and coordination inefficiencies. We introduces Knowledge-Aware\nBayesian Bandits (KABB), a novel framework that enhances multi-agent system\ncoordination through semantic understanding and dynamic adaptation. The\nframework features three key innovations: a three-dimensional knowledge\ndistance model for deep semantic understanding, a dual-adaptation mechanism for\ncontinuous expert optimization, and a knowledge-aware Thompson Sampling\nstrategy for efficient expert selection. Extensive evaluation demonstrates KABB\nachieves an optimal cost-performance balance, maintaining high performance\nwhile keeping computational demands relatively low in multi-agent coordination."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.06577",
    "c_title":[
      "Transforming Social Science Research with Transfer Learning: Social\n  Science Survey Data Integration with AI"
    ],
    "c_abstract":[
      "Large-N nationally representative surveys, which have profoundly shaped\nAmerican politics scholarship, represent related but distinct domains -a key\ncondition for transfer learning applications. These surveys are related through\ntheir shared demographic, party identification, and ideological variables, yet\ndiffer in that individual surveys often lack specific policy preference\nquestions that researchers require. Our study introduces a novel application of\ntransfer learning (TL) to address these gaps, marking the first systematic use\nof TL paradigms in the context of survey data. Specifically, models pre-trained\non the Cooperative Election Study (CES) dataset are fine-tuned for use in the\nAmerican National Election Studies (ANES) dataset to predict policy questions\nbased on demographic variables. Even with a naive architecture, our transfer\nlearning approach achieves approximately 92 percentage accuracy in predicting\nmissing variables across surveys, demonstrating the robust potential of this\nmethod. Beyond this specific application, our paper argues that transfer\nlearning is a promising framework for maximizing the utility of existing survey\ndata. We contend that artificial intelligence, particularly transfer learning,\nopens new frontiers in social science methodology by enabling systematic\nknowledge transfer between well-administered surveys that share common\nvariables but differ in their outcomes of interest."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12566",
    "c_title":[
      "Exploring the Impact of Personality Traits on LLM Bias and Toxicity"
    ],
    "c_abstract":[
      "With the different roles that AI is expected to play in human life, imbuing\nlarge language models (LLMs) with different personalities has attracted\nincreasing research interests. While the \"personification\" enhances human\nexperiences of interactivity and adaptability of LLMs, it gives rise to\ncritical concerns about content safety, particularly regarding bias, sentiment\nand toxicity of LLM generation. This study explores how assigning different\npersonality traits to LLMs affects the toxicity and biases of their outputs.\nLeveraging the widely accepted HEXACO personality framework developed in social\npsychology, we design experimentally sound prompts to test three LLMs'\nperformance on three toxic and bias benchmarks. The findings demonstrate the\nsensitivity of all three models to HEXACO personality traits and, more\nimportantly, a consistent variation in the biases, negative sentiment and\ntoxicity of their output. In particular, adjusting the levels of several\npersonality traits can effectively reduce bias and toxicity in model\nperformance, similar to humans' correlations between personality traits and\ntoxic behaviors. The findings highlight the additional need to examine content\nsafety besides the efficiency of training or fine-tuning methods for LLM\npersonification. They also suggest a potential for the adjustment of\npersonalities to be a simple and low-cost method to conduct controlled text\ngeneration."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09891",
    "c_title":[
      "Evolving Deeper LLM Thinking"
    ],
    "c_abstract":[
      "We explore an evolutionary search strategy for scaling inference time compute\nin Large Language Models. The proposed approach, Mind Evolution, uses a\nlanguage model to generate, recombine and refine candidate responses. The\nproposed approach avoids the need to formalize the underlying inference problem\nwhenever a solution evaluator is available. Controlling for inference cost, we\nfind that Mind Evolution significantly outperforms other inference strategies\nsuch as Best-of-N and Sequential Revision in natural language planning tasks.\nIn the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more\nthan 98% of the problem instances using Gemini 1.5 Pro without the use of a\nformal solver."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03255",
    "c_title":[
      "Temporal multilayer structures for designing higher-order transfer\n  functions using time-varying metamaterials"
    ],
    "c_abstract":[
      "Temporal metamaterials are artificial materials whose electromagnetic\nproperties change over time. In analogy with spatial media and metamaterials,\nwhere their properties change smoothly or abruptly over space, temporal\nmetamaterials can exhibit a smooth variation over time, realizing a temporal\nnon-homogeneous medium, or a stepwise transition, realizing the temporal\nversion of dielectric slabs or multilayer structures. In this Letter, we focus\nour attention on temporal multilayer structures, and we propose the synthesis\nof higher-order transfer functions by modeling the wave propagation through a\ngeneralized temporal multilayer structure, consisting of a cascade over time of\ndifferent media. The tailoring of the scattering response of temporal structure\nas a function of frequency is presented, deriving the corresponding scattering\ncoefficients for a properly designed set of medium properties, i.e.,\npermittivity and permeability, and application time, in analogy with what is\ntypically done in optical and electromagnetic spatial multilayered structures.\nThis allows us to design novel electromagnetic and optical devices with\nhigher-order transfer functions by exploiting the temporal dimension instead of\nthe spatial one."
    ],
    "c_categories":[
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15076",
    "c_title":[
      "Kink breathers on a traveling wave background in the defocusing modified\n  Korteweg--de Vries equation"
    ],
    "c_abstract":[
      "We characterize a general traveling periodic wave of the defocusing mKdV\n(modified Korteweg--de Vries) equation by using a quotient of products of\nJacobi's elliptic theta functions. Compared to the standing periodic wave of\nthe defocusing NLS (nonlinear Schr\\\"{o}dinger) equation, these solutions are\nspecial cases of Riemann's theta function of genus two. Based on our\ncharacterization, we derive a new two-parameter solution form which defines a\ngeneral three-parameter solution form with the scaling transformation.\nEigenfunctions of the Lax system for the general traveling periodic wave are\nalso characterized as quotients of products of Jacobi's theta functions. As the\nmain outcome of our analytical computations, we derive a new solution of the\ndefocusing mKdV equation which describes the kink breather propagating on a\ngeneral traveling wave background."
    ],
    "c_categories":[
      "math-ph",
      "math.AP",
      "math.CA",
      "math.MP",
      "nlin.PS",
      "nlin.SI"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.05590",
    "c_title":[
      "Parameter Estimation for Partially Observed Affine and Polynomial\n  Processes"
    ],
    "c_abstract":[
      "This paper is devoted to parameter estimation for partially observed\npolynomial state space models. This class includes discretely observed affine\nor more generally polynomial Markov processes. The polynomial structure allows\nfor the explicit computation of a Gaussian quasi-likelihood estimator and its\nasymptotic covariance matrix. We show consistency and asymptotic normality of\nthe estimating sequence and provide explicitly computable expressions for the\ncorresponding asymptotic covariance matrix."
    ],
    "c_categories":[
      "math.ST",
      "stat.TH"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.14082",
    "c_title":[
      "Superallowed $0^+ \\rightarrow 0^+$ $\\beta$ decay studies at GANIL and\n  upcoming opportunities with DESIR and S$^3$-LEB"
    ],
    "c_abstract":[
      "Corrected transition rates ($\\mathcal{F}t^{0^+ \\rightarrow 0^+}$) of $0^+\n\\rightarrow 0^+$ superallowed $\\beta$ decays currently give the most precise\nvalue of $V_{ud}$, the dominant term of the Cabibbo-Kobayashi-Maskawa (CKM)\nquark mixing matrix. By setting stringent constrains on the CKM unitarity,\nthese decays allow probing physics beyond the Standard Model in the electroweak\nsector. A recent global reevaluation of the $\\mathcal{F}t^{0^+ \\rightarrow\n0^+}$ values has indicated a violation of CKM unitarity prompting reassessment\nof the theoretical radiative and isospin symmetry breaking corrections applied\non the experimental transition rates $ft$. In this article we briefly discuss\nthis current situation and the experimental program at GANIL geared towards\nconstraining isospin symmetry breaking corrections. We conclude by presenting\nthe opportunities that will be available at DESIR and S$^3$-LEB, the upcoming\nlow-energy radioactive ion beam facilities at GANIL."
    ],
    "c_categories":[
      "hep-ph",
      "nucl-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Brain\u2013Computer Interface Spellers: A Review"
    ],
    "b_abstract":[
      "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15646",
    "c_title":[
      "Mathematical analysis of the gradients in deep learning"
    ],
    "c_abstract":[
      "Deep learning algorithms -- typically consisting of a class of deep\nartificial neural networks (ANNs) trained by a stochastic gradient descent\n(SGD) optimization method -- are nowadays an integral part in many areas of\nscience, industry, and also our day to day life. Roughly speaking, in their\nmost basic form, ANNs can be regarded as functions that consist of a series of\ncompositions of affine-linear functions with multidimensional versions of\nso-called activation functions. One of the most popular of such activation\nfunctions is the rectified linear unit (ReLU) function $\\mathbb{R} \\ni x\n\\mapsto \\max\\{ x, 0 \\} \\in \\mathbb{R}$. The ReLU function is, however, not\ndifferentiable and, typically, this lack of regularity transfers to the cost\nfunction of the supervised learning problem under consideration. Regardless of\nthis lack of differentiability issue, deep learning practioners apply SGD\nmethods based on suitably generalized gradients in standard deep learning\nlibraries like {\\sc TensorFlow} or {\\sc Pytorch}. In this work we reveal an\naccurate and concise mathematical description of such generalized gradients in\nthe training of deep fully-connected feedforward ANNs and we also study the\nresulting generalized gradient function analytically. Specifically, we provide\nan appropriate approximation procedure that uniquely describes the generalized\ngradient function, we prove that the generalized gradients are limiting\nFr\\'echet subgradients of the cost functional, and we conclude that the\ngeneralized gradients must coincide with the standard gradient of the cost\nfunctional on every open sets on which the cost functional is continuously\ndifferentiable."
    ],
    "c_categories":[
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09672",
    "c_title":[
      "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale"
    ],
    "c_abstract":[
      "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.14337",
    "c_title":[
      "Latent computing by biological neural networks: A dynamical systems\n  framework"
    ],
    "c_abstract":[
      "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13097",
    "c_title":[
      "Combined impact of grey and superficial white matter abnormalities:\n  implications for epilepsy surgery"
    ],
    "c_abstract":[
      "Drug-resistant focal epilepsy is associated with abnormalities in the brain\nin both grey matter (GM) and superficial white matter (SWM). However, it is\nunknown if both types of abnormalities are important in supporting seizures.\nHere, we test if surgical removal of GM and\/or SWM abnormalities relates to\npost-surgical seizure outcome in people with temporal lobe epilepsy (TLE).\n  We analyzed structural imaging data from 143 TLE patients (pre-op dMRI and\npre-op T1-weighted MRI) and 97 healthy controls. We calculated GM volume\nabnormalities and SWM mean diffusivity abnormalities and evaluated if their\nsurgical removal distinguished seizure outcome groups post-surgically.\n  At a group level, GM and SWM abnormalities were most common in the\nipsilateral temporal lobe and hippocampus in people with TLE. Analyzing both\nmodalities together, compared to in isolation, improved surgical outcome\ndiscrimination (GM AUC = 0.68, p < 0.01, WM AUC = 0.65, p < 0.01; Union AUC =\n0.72, p < 0.01, Concordance AUC = 0.64, p = 0.04). Additionally, 100% of people\nwho had all concordant abnormal regions resected had ILAE$_{1,2}$ outcomes.\n  These findings suggest that regions identified as abnormal from both\ndiffusion-weighted and T1-weighted MRIs are involved in the epileptogenic\nnetwork and that resection of both types of abnormalities may enhance the\nchances of living without disabling seizures."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11201",
    "c_title":[
      "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms"
    ],
    "c_abstract":[
      "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09984",
    "c_title":[
      "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach"
    ],
    "c_abstract":[
      "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05382",
    "c_title":[
      "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models"
    ],
    "c_abstract":[
      "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models."
    ],
    "c_categories":[
      "cs.AI",
      "hep-ph",
      "physics.comp-ph",
      "physics.data-an",
      "physics.hist-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.14957",
    "c_title":[
      "A new mass estimate method with hydrodynamical atmospheres for very\n  massive WNh stars"
    ],
    "c_abstract":[
      "Very massive stars with masses over 100 Msun are key objects in the Universe\nfor our understanding of chemical and energetic feedback in the Universe, but\ntheir evolution and fate are almost entirely determined by their wind mass\nloss. We aim to determine the mass of the most massive star known in the Local\nGroup R136a1. For this we compute the first hydrodynamically consistent\nnon-local thermodynamical equilibrium atmosphere models for both R136a1 (WN5h)\nas well as the binary system R144 (WN5\/6h+WN6\/7h) in the Tarantula nebula.\nUsing the Potsdam Wolf-Rayet code, we simultaneously empirically derive and\ntheoretically predict mass-loss rates and wind velocities. By fitting synthetic\nspectra derived from these models to multi-wavelength observations, we\nconstrain the stellar and wind properties of R144 and R136a1. We first\ndetermine the clumping stratification required by our hydro-models to fit the\nspectra of R144 by using the available dynamical mass estimates for the two\ncomponents. We then utilise this clumping stratification in hydrodynamic models\nof R136a1 and estimate a mass of $M_\\mathrm{Hydro}$ of 233 Msun. Remarkably,\nthe estimated mass is close to and entirely consistent with chemical\nhomogeneous mass relations. This present-day mass of 233 Msun provides a lower\nlimit to the initial stellar mass, that could be far higher due to previous\nwind mass loss."
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.16296",
    "c_title":[
      "Log-Concavity of the Grothendieck Classes of Banana Graphs and Clasped\n  Necklaces"
    ],
    "c_abstract":[
      "The Grothendieck classes of melonic graphs satisfy a recursive relation and\nmay be written as polynomials in the class of the moduli space\n$\\mathcal{M}_{0,4}$ with nonnegative integer coefficients, conjectured to be\nlog-concave. In this article, we investigate log-concavity and\nultra-log-concavity for the Grothendieck class of banana graphs and the three\nfamilies of polynomials involved in the recursive relation. We prove that all\nfour are log-concave, establishing the specific case of banana graphs for the\nlog-concavity conjecture. We additionally introduce the infinite family of\nclasped necklaces, melonic graphs obtained by replacing an edge of a $2$-banana\nwith a string of $m$-bananas. Using the recursive relation, we explicitly\ncompute the classes of clasped necklaces and prove that they too are\nlog-concave."
    ],
    "c_categories":[
      "math.AG",
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06095",
    "c_title":[
      "The Spectre of Underdetermination in Modern Cosmology"
    ],
    "c_abstract":[
      "The scientific status of physical cosmology has been the subject of\nphilosophical debate ever since detailed mathematical models of the Universe\nemerged from Einstein's general theory of relativity. Such debates revolve\naround whether and to what extent cosmology meets established demarcation\ncriteria for a discipline to be scientific, as well as determining how to best\ncharacterize cosmology as a science, given the unique challenges and\nlimitations faced by a discipline which aims to study the origin, composition,\nand fate of the Universe itself. The present article revisits, in light of the\ndramatic progress in cosmology in recent decades, an earlier debate held in the\n1950s between Herman Bondi and Gerald Whitrow regarding the scientific status\nof cosmology. We analyse cosmology's transition from an emerging science to a\ncornerstone of modern physics, highlighting its empirical successes in\nestablishing the $\\Lambda$-Cold Dark Matter ($\\Lambda$CDM) model and in its\ndelivery of various successful novel predictions. Despite this remarkable\nscientific success and progress, we argue that modern cosmology faces a further\nprofound challenge: the permanent underdetermination of the microphysical\nnature of its exotic energy components: inflation, dark matter, and dark\nenergy. Drawing historical parallels with the role of spectroscopy in revealing\nthe microphysical nature of atomic physics, we argue that the epistemic\nbarriers obstructing us from ascertaining the microphysical nature of these\nexotic energy components are significant, in turn casting doubt upon whether\ncosmology can ever transcend these particular epistemic challenges. We conclude\nby reflecting on the prospects for future breakthroughs and\/or non-empirical\narguments which could decide this issue conclusively."
    ],
    "c_categories":[
      "astro-ph.CO",
      "gr-qc",
      "hep-th",
      "physics.hist-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.15395",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
    ],
    "b_abstract":[
      "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02685",
    "c_title":[
      "TReND: Transformer derived features and Regularized NMF for neonatal\n  functional network Delineation"
    ],
    "c_abstract":[
      "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease."
    ],
    "c_categories":[
      "cs.CV",
      "eess.SP",
      "q-bio.NC",
      "q-bio.QM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"MatPilot: an LLM-enabled AI Materials Scientist under the Framework of\n  Human-Machine Collaboration",
    "a_abstract":"The rapid evolution of artificial intelligence, particularly large language\nmodels, presents unprecedented opportunities for materials science research. We\nproposed and developed an AI materials scientist named MatPilot, which has\nshown encouraging abilities in the discovery of new materials. The core\nstrength of MatPilot is its natural language interactive human-machine\ncollaboration, which augments the research capabilities of human scientist\nteams through a multi-agent system. MatPilot integrates unique cognitive\nabilities, extensive accumulated experience, and ongoing curiosity of\nhuman-beings with the AI agents' capabilities of advanced abstraction, complex\nknowledge storage and high-dimensional information processing. It could\ngenerate scientific hypotheses and experimental schemes, and employ predictive\nmodels and optimization algorithms to drive an automated experimental platform\nfor experiments. It turns out that our system demonstrates capabilities for\nefficient validation, continuous learning, and iterative optimization.",
    "explanation":"proposed and\ndeveloped an AI materials scientist named MatPilot, which has shown encouraging\nabilities in the discovery of new materials. ",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b4"
    ],
    "c_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "c_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.03274",
    "c_title":[
      "A Scalable Approach to Probabilistic Neuro-Symbolic Verification"
    ],
    "c_abstract":[
      "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.03505",
    "c_title":[
      "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems"
    ],
    "c_abstract":[
      "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.14491",
    "c_title":[
      "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk"
    ],
    "c_abstract":[
      "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.17821",
    "c_title":[
      "OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination"
    ],
    "c_abstract":[
      "AI agents hold the potential to transform everyday life by helping humans\nachieve their goals. To do this successfully, agents need to be able to\ncoordinate with novel partners without prior interaction, a setting known as\nzero-shot coordination (ZSC). Overcooked has become one of the most popular\nbenchmarks for evaluating coordination capabilities of AI agents and learning\nalgorithms. In this work, we investigate the origins of ZSC challenges in\nOvercooked. We introduce a state augmentation mechanism which mixes states that\nmight be encountered when paired with unknown partners into the training\ndistribution, reducing the out-of-distribution challenge associated with ZSC.\nWe show that independently trained agents under this algorithm coordinate\nsuccessfully in Overcooked. Our results suggest that ZSC failure can largely be\nattributed to poor state coverage under self-play rather than more\nsophisticated coordination challenges. The Overcooked environment is therefore\nnot suitable as a ZSC benchmark. To address these shortcomings, we introduce\nOvercookedV2, a new version of the benchmark, which includes asymmetric\ninformation and stochasticity, facilitating the creation of interesting ZSC\nscenarios. To validate OvercookedV2, we conduct experiments demonstrating that\nmere exhaustive state coverage is insufficient to coordinate well. Finally, we\nuse OvercookedV2 to build a new range of coordination challenges, including\nones that require test time protocol formation, and we demonstrate the need for\nnew coordination algorithms that can adapt online. We hope that OvercookedV2\nwill help benchmark the next generation of ZSC algorithms and advance\ncollaboration between AI agents and humans."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.06442",
    "c_title":[
      "ARES: Auxiliary Range Expansion for Outlier Synthesis"
    ],
    "c_abstract":[
      "Recent successes of artificial intelligence and deep learning often depend on\nthe well-collected training dataset which is assumed to have an identical\ndistribution with the test dataset. However, this assumption, which is called\nclosed-set learning, is hard to meet in realistic scenarios for deploying deep\nlearning models. As one of the solutions to mitigate this assumption, research\non out-of-distribution (OOD) detection has been actively explored in various\ndomains. In OOD detection, we assume that we are given the data of a new class\nthat was not seen in the training phase, i.e., outlier, at the evaluation\nphase. The ultimate goal of OOD detection is to detect and classify such unseen\noutlier data as a novel \"unknown\" class. Among various research branches for\nOOD detection, generating a virtual outlier during the training phase has been\nproposed. However, conventional generation-based methodologies utilize\nin-distribution training dataset to imitate outlier instances, which limits the\nquality of the synthesized virtual outlier instance itself. In this paper, we\npropose a novel methodology for OOD detection named Auxiliary Range Expansion\nfor Outlier Synthesis, or ARES. ARES models the region for generating\nout-of-distribution instances by escaping from the given in-distribution\nregion; instead of remaining near the boundary of in-distribution region.\nVarious stages consists ARES to ultimately generate valuable OOD-like virtual\ninstances. The energy score-based discriminator is then trained to effectively\nseparate in-distribution data and outlier data. Quantitative experiments on\nbroad settings show the improvement of performance by our method, and\nqualitative results provide logical explanations of the mechanism behind it."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17712",
    "c_title":[
      "Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis"
    ],
    "c_abstract":[
      "Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$."
    ],
    "c_categories":[
      "math.CA",
      "math.MG",
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.13448",
    "c_title":[
      "Criteria for asymptotic stability of eventually continuous Markov-Feller\n  semigroups"
    ],
    "c_abstract":[
      "In this paper, we establish three criteria for the asymptotic behavior of\nMarkov-Feller semigroups. First, we present a criterion for convergence in\ntotal variation to a unique invariant measure, requiring only $TV$-eventual\ncontinuity of the semigroup at a single point. Second, we propose two new\ncriteria for asymptotic stability that require eventual continuity at a single\npoint. This localized condition is more practical and easier to check. To\nillustrate the advantages of our framework, we provide an explicit example\nwhere verifying eventual continuity at a single point is straightforward,\nwhereas establishing the corresponding global property is challenging."
    ],
    "c_categories":[
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.03365",
    "c_title":[
      "Crystals and Double Quiver Algebras from Jeffrey-Kirwan Residues"
    ],
    "c_abstract":[
      "We construct statistical mechanical models of crystal melting describing the\nflavoured Witten indices of $\\mathcal{N}\\ge 2$ supersymmetric quiver gauge\ntheories. Our results can be derived from the Jeffrey-Kirwan (JK) residue\nformulas, and generalize the previous results for quivers corresponding to\ntoric Calabi-Yau threefolds and fourfolds to a large class of quivers\nsatisfying the no-overlap condition, including those corresponding to some\nnon-toric Calabi-Yau manifolds. We construct new quiver algebras which we call\nthe double quiver Yangians\/algebras, as well as their representations in terms\nof the aforementioned crystals. For theories with four supercharges, we compare\nthe double quiver algebras with the existing quiver Yangians\/BPS algebras,\nwhich we show can also be constructed from the JK residues. For theories with\ntwo supercharges, the double quiver algebras provide an algebraic description\nof the BPS states, including the information of the fixed points and their\nrelative coefficients in the full partition functions."
    ],
    "c_categories":[
      "hep-th",
      "math-ph",
      "math.AG",
      "math.CO",
      "math.MP"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.08092",
    "c_title":[
      "Chromatic Higher Semiadditivity by Height Induction"
    ],
    "c_abstract":[
      "We give a new proof of the $\\infty$-semiadditivity of $K(n)$-local spectra.\nThe proof proceeds by induction on the height via algebraic K-theory, utilizing\nrecent advances in chromatic homotopy theory and the redshift conjecture,\ninstead of using the Ravenel-Wilson computation of the Morava K-theory of\nEilenberg-MacLane spaces."
    ],
    "c_categories":[
      "math.AT",
      "math.KT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
    ],
    "b_abstract":[
      "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
    ],
    "b_categories":[
      "cond-mat.mtrl-sci"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.16210",
    "c_title":[
      "On the convergence of split exponential integrators for semilinear\n  parabolic problems"
    ],
    "c_abstract":[
      "Splitting the exponential-like $\\varphi$ functions, which typically appear in\nexponential integrators, is attractive in many situations since it can\ndramatically reduce the computational cost of the procedure. However, depending\non the employed splitting, this can result in order reduction. The aim of this\npaper is to analyze different such split approximations. We perform the\nanalysis for semilinear problems in the abstract framework of commuting\nsemigroups and derive error bounds that depend, in particular, on whether the\nvector (to which the $\\varphi$ functions are applied) satisfies appropriate\nboundary conditions. We then present the convergence analysis for two split\nversions of a second-order exponential Runge--Kutta integrator in the context\nof analytic semigroups, and show that one suffers from order reduction while\nthe other does not. Numerical results for semidiscretized parabolic PDEs\nconfirm the theoretical findings."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.02853",
    "c_title":[
      "BaTiO$_3$ -- SrTiO$_3$ composites: a microscopic study on paraelectric\n  cubic inclusions"
    ],
    "c_abstract":[
      "Composites of ferroelectric and paraelectric perovskites have attracted a lot\nof attention due to their application potential in energy storage as well as\nnovel computing and memory devices. So far the main focus of research has been\non superlattices and ferroelectric particles in a paraelectric matrix, while\nthe impact of paraelectric inclusions on the ferroelectric matrix is\nsurprisingly underrepresented. To close this gap in knowledge we perform\nmolecular dynamics simulations using an $ab\\ initio$ derived effective\nHamiltonian for BaTiO$_3$--SrTiO$_3$ and reveal the dependency of phase\nstability and phase transitions on the size and distances of paraelectric\ninclusions. We discuss how the combination of compressive strain and\ndepolarization fields at the SrTiO$_3$ interfaces induces large local\npolarization, complex domain structures and coexisting phases as well as\ndiffuse phase transitions and reduced coercive fields."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11025",
    "c_title":[
      "A large anomalous Hall effect and Weyl nodes in bulk FeNi$_3$: a density\n  functional theory study"
    ],
    "c_abstract":[
      "In this work, we report the study of electronic structure, magnetism, and the\nexistence of Weyl nodes in a pristine bulk FeNi$_{3}$, a member of Fe-Ni inver\nalloy compounds, known as good metal catalysts with high activity and stability\nfor water splitting for a very long time. Our observation of Weyl points in the\nbulk FeNi$_{3}$ may lead to a new technology to design highly efficient\ntopological catalysts. While the previous literature \\cite{PhysRev.97.304}\nmainly focused on the thermal and catalytic properties of FeNi$_{3}$ we report\nthe interplay of Fe $d$-Ni $d$ hybridization and spin-orbit coupling give rise\nto the ferromagnetic Weyl nodes in the bulk FeNi$_{3}$. Our study shows that\nthe ground state of the bulk FeNi$_{3}$ is a Weyl metal with a large number of\nWeyl nodes at the Fermi energy away from high-symmetry $k$-points. Furthermore,\nwe predict a large intrinsic anomalous Hall conductivity of about $10000~S\/m$\nat the ground state. In addition, we show the existence of Weyl nodes along the\nhigh symmetry $k$-points $~0.2eV$ above and $~0.05eV$ below self-consistent\nFermi level that may be achieved either by the electron or hole doping, or by\nexternal perturbation. In this article, FeNi$_{3}$ has been studied to explore\nthis scenario using first-principles density functional theory and subsequent\nWannier90-based tight-binding method. Furthermore, we report the existence of\ntwo types of Weyl cones, type-I and type-II, $~0.2eV$ above the Fermi level.\nOur report provides a realistic material to further explore the intrinsic\nproperties related to Weyl cones, and the spintronic applications."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09681",
    "c_title":[
      "Annealing-Induced Magnetic Modulation in Co- and Y-doped CeO2: Insights\n  from Experiments and DFT"
    ],
    "c_abstract":[
      "The potential applications of dilute magnetic oxides (DMOs) in magneto-optic\nand spintronic devices have attracted significant attention, although\nunderstanding their magnetic behavior is complex due to intricate interactions\nof intrinsic defects. The present study aims to investigate the effect of\ndifferent annealing environments on the magnetic properties of polycrystalline\ntransition metal cation (Co and Y) doped CeO2 DMO with a 5% doping\nconcentration of transition metal (TM). The objective is to investigate the\ndefect interactions within the lattice through a comprehensive investigation\ninvolving structural characterizations, magnetic measurements, and first\nprinciple calculations. The results show that the Ar\/H2 annealing environment\ninduced more oxygen vacancies than air-annealed samples. Consequently,\nfield-dependent magnetization measurements revealed above-room-temperature\nferromagnetism (RTFM) in both un-doped and TM-doped CeO2. The ferromagnetic\n(FM) properties of CeO2 resulted from carrier-trapped vacancy centers\nfacilitating exchange interactions between the spins of magnetic ions. The\nLangevin field profile indicated that TM-doped CeO2 formed more bound magnetic\npolarons (BMPs) during annealing in an Ar\/H2 environment, which contributed to\nthe enhanced ferromagnetism. Similarly, enhancement in the magnetic properties\nwith increasing oxygen vacancies is observed through first principle\ncalculations. This suggests the potential for optimizing the magnetic\nproperties of DMOs through controlled annealing processes."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15161",
    "c_title":[
      "Iron-Arsenide monolayer as an anode materials for Lithium-ion batteries:\n  A first-principles study"
    ],
    "c_abstract":[
      "This theoretical investigation delves into the structural, electronic, and\nelectrochemical properties of two hexagonal iron-arsenide monolayers, 1T-FeAs\nand 1H-FeAs, focusing on their potential as anode materials for Lithium-ion\nbatteries. Previous studies have highlighted the ferromagnetic nature of\n1T-FeAs at room temperature.Our calculations reveal that both phases exhibit\nmetallic behaviour with spin-polarized electronic band structures.\nElectrochemical studies show that the 1T-FeAs monolayer has better ionic\nconductivity for Li ions than the 1H-FeAs phase, attributed to a lower\nactivation barrier of 0.38 eV. This characteristic suggests a faster\ncharge\/discharge rate. Both FeAs phases exhibit comparable theoretical\ncapacities 374 mAh\/g, outperforming commercial graphite anodes. The average\nopen-circuit voltage for maximum Li atom adsorption is 0.61 V for 1H-FeAs and\n0.44 V for 1T-FeAs. The volume expansion over the maximum adsorption of Li\natoms on both phases is also remarkably less than the commercially used anode\nmaterial such as graphite. Further, the adsorption of Li atoms onto 1H-FeAs\ninduces a remarkable transition from ferromagnetism to anti-ferromagnetism,\nwith minimal impact on the electronic band structure. In contrast, the original\nstate of 1T-FeAs remains unaffected by Li adsorption. To summarize, the\npotential of both 1T-FeAs and 1H-FeAs monolayers as promising anode materials\nfor Lithium-ion batteries, offering valuable insights into their\nelectrochemical performance and phase transition behaviour upon Li adsorption."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07507",
    "c_title":[
      "The Role of 11B4C Interlayers in Enhancing Fe\/Si Multilayer Performance\n  for Polarized Neutron Mirrors"
    ],
    "c_abstract":[
      "This study investigates the effects of incorporating 11B4C interlayers into\nFe\/Si multilayers, with a focus on interface quality, reflectivity,\npolarization, and magnetic properties for polarized neutron optics. It is found\nthat the introduction of 1 {\\AA} and 2 {\\AA} 11B4C interlayers significantly\nimproves the interface sharpness, reducing interface width and preventing\nexcessive Si diffusion into the Fe layers. X-ray reflectivity and polarized\nneutron reflectivity measurements show enhanced reflectivity and polarization,\nwith a notable increase in polarization for 30 {\\AA} period multilayers. The\ninclusion of interlayers also helps prevent the formation of iron-silicides,\nimproving both the magnetic properties and neutron optical performance.\nHowever, the impact of interlayers is less pronounced in thicker-period\nmultilayers (100 {\\AA}), primarily due to the ratio between layer and interface\nwidths. These results suggest that 11B4C interlayers offer a promising route\nfor optimizing Fe\/Si multilayer performance in polarized neutron mirrors."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.13344",
    "c_title":[
      "K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug\n  Interaction Prediction"
    ],
    "c_abstract":[
      "Drug discovery is a complex and time-intensive process that requires\nidentifying and validating new therapeutic candidates. Computational approaches\nusing large-scale biomedical knowledge graphs (KGs) offer a promising solution\nto accelerate this process. However, extracting meaningful insights from\nlarge-scale KGs remains challenging due to the complexity of graph traversal.\nExisting subgraph-based methods are tailored to graph neural networks (GNNs),\nmaking them incompatible with other models, such as large language models\n(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,\ndiverse, and biologically meaningful paths from KGs. Integrating these paths\nenables LLMs and GNNs to effectively predict unobserved drug-drug and\ndrug-disease interactions. Unlike traditional path-ranking approaches, K-Paths\nretrieves and transforms paths into a structured format that LLMs can directly\nprocess, facilitating explainable reasoning. K-Paths employs a diversity-aware\nadaptation of Yen's algorithm to retrieve the K shortest loopless paths between\nentities in an interaction query, prioritizing biologically relevant and\ndiverse relationships. Our experiments on benchmark datasets show that K-Paths\nimproves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on\ndrug repurposing and 13.42 points on interaction severity prediction. We also\nshow that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,\nrespectively. K-Paths also improves the supervised training efficiency of\nEmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining\nstrong predictive performance. Beyond its scalability and efficiency, K-Paths\nuniquely bridges the gap between KGs and LLMs, providing explainable rationales\nfor predicted interactions. These capabilities show that K-Paths is a valuable\ntool for efficient data-driven drug discovery."
    ],
    "c_categories":[
      "cs.CL",
      "cs.LG",
      "q-bio.BM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17981",
    "c_title":[
      "Generating Correlation Matrices with Graph Structures Using Convex\n  Optimization"
    ],
    "c_abstract":[
      "This work deals with the generation of theoretical correlation matrices with\nspecific sparsity patterns, associated to graph structures. We present a novel\napproach based on convex optimization, offering greater flexibility compared to\nexisting techniques, notably by controlling the mean of the entry distribution\nin the generated correlation matrices. This allows for the generation of\ncorrelation matrices that better represent realistic data and can be used to\nbenchmark statistical methods for graph inference."
    ],
    "c_categories":[
      "eess.SP",
      "math.OC",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03929",
    "c_title":[
      "Kantorovich duality for optimal transport on completely regular\n  Hausdorff spaces"
    ],
    "c_abstract":[
      "We introduce a new intermediate optimization problem situated between\nKantorovich's primal and dual formulations. This new problem extends\nKantorovich's duality to separable Baire measures, which are strictly more\ngeneral than tight (or Radon) measures in completely regular Hausdorff spaces.\nIn the special case where the measures are Radon, our intermediate problem\naligns with the classical Kantorovich's primal problem. Existence of solutions\nfor all three formulations are also provided within this comprehensive\nframework."
    ],
    "c_categories":[
      "math.FA",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05677",
    "c_title":[
      "Single-Loop Variance-Reduced Stochastic Algorithm for Nonconvex-Concave\n  Minimax Optimization"
    ],
    "c_abstract":[
      "Nonconvex-concave (NC-C) finite-sum minimax problems have broad applications\nin decentralized optimization and various machine learning tasks. However, the\nnonsmooth nature of NC-C problems makes it challenging to design effective\nvariance reduction techniques. Existing vanilla stochastic algorithms using\nuniform samples for gradient estimation often exhibit slow convergence rates\nand require bounded variance assumptions. In this paper, we develop a novel\nprobabilistic variance reduction updating scheme and propose a single-loop\nalgorithm called the probabilistic variance-reduced smoothed gradient\ndescent-ascent (PVR-SGDA) algorithm. The proposed algorithm achieves an\niteration complexity of $O(\\epsilon^{-4})$, surpassing the best-known rates of\nstochastic algorithms for NC-C minimax problems and matching the performance of\nthe best deterministic algorithms in this context. Finally, we demonstrate the\neffectiveness of the proposed algorithm through numerical simulations."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.08063",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "A deep-learning approach to realizing functionality in nanoelectronic devices"
    ],
    "b_abstract":[
      "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07251",
    "c_title":[
      "Stochastic Epidemic Models with Partial Information"
    ],
    "c_abstract":[
      "Mathematical models of epidemics often use compartmental models dividing the\npopulation into several compartments. Based on a microscopic setting describing\nthe temporal evolution of the subpopulation sizes in the compartments by\nstochastic counting processes one can derive macroscopic models for large\npopulations describing the average behavior by associated ordinary differential\nequations such as the celebrated SIR model. Further, diffusion approximations\nallow to address fluctuations from the average and to describe the state\ndynamics also for smaller populations by stochastic differential equations.\n  In general, not all state variables are directly observable, and we face the\nso-called \"dark figure\" problem, which concerns, for example, the unknown\nnumber of asymptomatic and undetected infections. The present study addresses\nthis problem by developing stochastic epidemic models that incorporate partial\ninformation about the current state of the epidemic, also known as nowcast\nuncertainty. Examples include a simple extension of the SIR model, a model for\na disease with lifelong immunity after infection or vaccination, and a Covid-19\nmodel. For the latter, we propose a ``cascade state approach'' that allows to\nexploit the information contained in formally hidden compartments with\nobservable inflow but unobservable outflow. Furthermore, parameter estimation\nand calibration are performed using ridge regression for the Covid-19 model.\nThe results of the numerical simulations illustrate the theoretical findings."
    ],
    "c_categories":[
      "math.PR",
      "q-bio.PE"
    ],
    "c_fields":[
      "Quantitative Biology",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"A Knowledge-enhanced Pathology Vision-language Foundation Model for\n  Cancer Diagnosis",
    "a_abstract":"Deep learning has enabled the development of highly robust foundation models\nfor various pathological tasks across diverse diseases and patient cohorts.\nAmong these models, vision-language pre-training, which leverages large-scale\npaired data to align pathology image and text embedding spaces, and provides a\nnovel zero-shot paradigm for downstream tasks. However, existing models have\nbeen primarily data-driven and lack the incorporation of domain-specific\nknowledge, which limits their performance in cancer diagnosis, especially for\nrare tumor subtypes. To address this limitation, we establish a\nKnowledge-enhanced Pathology (KEEP) foundation model that harnesses disease\nknowledge to facilitate vision-language pre-training. Specifically, we first\nconstruct a disease knowledge graph (KG) that covers 11,454 human diseases with\n139,143 disease attributes, including synonyms, definitions, and hypernym\nrelations. We then systematically reorganize the millions of publicly available\nnoisy pathology image-text pairs, into 143K well-structured semantic groups\nlinked through the hierarchical relations of the disease KG. To derive more\nnuanced image and text representations, we propose a novel knowledge-enhanced\nvision-language pre-training approach that integrates disease knowledge into\nthe alignment within hierarchical semantic groups instead of unstructured\nimage-text pairs. Validated on 18 diverse benchmarks with more than 14,000\nwhole slide images (WSIs), KEEP achieves state-of-the-art performance in\nzero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP\ndemonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7\ncancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of\n0.456 in subtyping 30 rare brain cancers, indicating strong generalizability\nfor diagnosing rare tumors.",
    "explanation":"However, existing models have been primarily data-driven and\nlack the incorporation of domain-specific knowledge, which limits their performance in cancer diagnosis,\nespecially for rare tumor subtypes. To address this limitation, we establish a KnowledgE-Enhanced\nPathology (KEEP) foundation model that harnesses disease knowledge to facilitate vision-language\npre-training.",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b1"
    ],
    "c_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "c_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03223",
    "c_title":[
      "The relation between black hole spin, star formation rate, and black\n  hole mass for supermassive black holes"
    ],
    "c_abstract":[
      "Both theoretical models and observational evidence indicate that jets and\/or\noutflows driven by central active supermassive black holes exert a significant\nfeedback effect on the overall properties of their host galaxies. Theoretical\nmodels suggest that the spin of supermassive black holes drives relativistic\njets. Therefore, we investigate the relationship between black hole spin, star\nformation rate, and black hole mass using a sample of 48 low-redshift\nsupermassive black holes. By performing multiband fitting of spectral energy\ndistribution, we derive the star formation rates and stellar masses of the host\ngalaxies harbouring these supermassive black holes. Our main results are as\nfollows: (i) For black holes with masses \\(M_{\\rm BH} \\lesssim 10^{6.5}\nM_{\\odot}\\), the spin increases with increasing black hole mass, suggesting\nthat black hole growth is primarily driven by gas accretion, particularly in\nthe coherent gas accretion regime. Conversely, for black holes with masses\n\\(M_{\\rm BH} \\gtrsim 10^{7.5} M_{\\odot}\\), the spin decreases with increasing\nblack hole mass, indicating that growth occurs mainly through mergers, inducing\nchaotic accretion. (ii) At low star formation rates, black hole spin increases\nwith increasing star formation rates, consistent with gas accretion. However,\nat high star formation rates, black hole spin decreases with increasing star\nformation rates, suggesting black hole mergers. The value of the black hole\nspin may be used to diagnose the star formation rate of the host galaxies\nthrough active galactic nuclei activities. (iii) Our data and analysis confirm\nthe well-known relation between stellar mass and black hole mass, with the\nfitting function $\\log M_{\\rm BH}=0.57\\log M_{*}+1.94$."
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.10989",
    "c_title":[
      "Phase-sensitive Rydberg-atom interferometry with Floquet\n  electromagnetically induced transparency"
    ],
    "c_abstract":[
      "We design a phase-sensitive Rydberg-atom interferometry by implementing\nFloquet electromagnetically induced transparency (FEIT). The FEIT mixes the\nsidebands of a Rydberg state induced by a MHz radio frequency (RF) field and\nrecombines them into FEIT bands. The FEIT bands act as screens to present the\ninterference between paths with different phases, which are transitions between\nthe sidebands and excited states. This interferometry can measure the phase of\na MHz RF field without a local RF reference field. A phase reference is\nsupplied to the atoms via a periodic electrical signal in the FEIT. We\ndemonstrate that the MHz RF phase is measured over a full range of $2\\pi$, and\n$10^{-4}$ rad accuracy is achieved. Moreover, the RF amplitude can also be\nmeasured with higher accuracy than the traditional EIT-based scheme."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10335",
    "c_title":[
      "Studying number theory with deep learning: a case study with the\n  M\\\"obius and squarefree indicator functions"
    ],
    "c_abstract":[
      "Building on work of Charton, we train small transformer models to calculate\nthe M\\\"obius function $\\mu(n)$ and the squarefree indicator function\n$\\mu^2(n)$. The models attain nontrivial predictive power. We then iteratively\ntrain additional models to understand how the model functions, ultimately\nfinding a theoretical explanation."
    ],
    "c_categories":[
      "cs.LG",
      "math.NT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06567",
    "c_title":[
      "Membership Inference Risks in Quantized Models: A Theoretical and\n  Empirical Study"
    ],
    "c_abstract":[
      "Quantizing machine learning models has demonstrated its effectiveness in\nlowering memory and inference costs while maintaining performance levels\ncomparable to the original models. In this work, we investigate the impact of\nquantization procedures on the privacy of data-driven models, specifically\nfocusing on their vulnerability to membership inference attacks. We derive an\nasymptotic theoretical analysis of Membership Inference Security (MIS),\ncharacterizing the privacy implications of quantized algorithm weights against\nthe most powerful (and possibly unknown) attacks. Building on these theoretical\ninsights, we propose a novel methodology to empirically assess and rank the\nprivacy levels of various quantization procedures. Using synthetic datasets, we\ndemonstrate the effectiveness of our approach in assessing the MIS of different\nquantizers. Furthermore, we explore the trade-off between privacy and\nperformance using real-world data and models in the context of molecular\nmodeling."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
    ],
    "b_abstract":[
      "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07734",
    "c_title":[
      "Diffuselet method for three-dimensional turbulent mixing of a cloudy air\n  filament"
    ],
    "c_abstract":[
      "The mixing properties of vapor content, temperature and particle fields are\nof paramount importance in cloud turbulence as they pertain to essential\nprocesses, such as cloud water droplet evaporation and entrainment. Our study\nexamines the mixing of a single cloudy air (which implies droplet-laden)\nfilament with its clear air environment, a characteristic process at the cloud\nedge, in two ways. The first consists of 3-dimensional combined\nEuler-Lagrangian DNSs which describe the scalar supersaturation as an Eulerian\nfield and the individual cloud water droplets as an ensemble of Lagrangian\ntracers. The second way builds on the recently developed diffuselet method, a\nkinematic Lagrangian framework that decomposes a scalar filament into a\ncollection of small sections subject to deformation by local stirring and\ncross-sheet diffusion. The Schmidt number is Sc=0.7. The entrainment process\ncauses a deformation of the supersaturated cloud filament in combination with\ndiffusion until the system reaches a well-mixed equilibrium state with all\ndroplets evaporated. We compare the time dependence of the variance and\nprobability density function of the supersaturation field. For the initial\nperiod of the mixing process they agree very well; at later stages deviations\ncaused by non-zero mean of the conserved scalar are observed. For the cases\nincluding cloud water droplets, we investigate also the impact of droplet\nnumber density and condensation growth response. Turbulence causes deviations\nfrom the d2-law similar to recent experiments in sprays. A simulation at a Sc\nthat is by a factor of 100 larger than in clouds improves the agreement between\nsimulation and diffuselet method significantly. The latter result promotes the\ndiffuselet framework as an efficient parametrization for turbulent high-Sc\nmixing which can reduce the resolution efforts of the viscous-convective range\nof scalar turbulence."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03274",
    "c_title":[
      "A Scalable Approach to Probabilistic Neuro-Symbolic Verification"
    ],
    "c_abstract":[
      "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.03505",
    "c_title":[
      "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems"
    ],
    "c_abstract":[
      "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.14491",
    "c_title":[
      "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk"
    ],
    "c_abstract":[
      "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17821",
    "c_title":[
      "OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination"
    ],
    "c_abstract":[
      "AI agents hold the potential to transform everyday life by helping humans\nachieve their goals. To do this successfully, agents need to be able to\ncoordinate with novel partners without prior interaction, a setting known as\nzero-shot coordination (ZSC). Overcooked has become one of the most popular\nbenchmarks for evaluating coordination capabilities of AI agents and learning\nalgorithms. In this work, we investigate the origins of ZSC challenges in\nOvercooked. We introduce a state augmentation mechanism which mixes states that\nmight be encountered when paired with unknown partners into the training\ndistribution, reducing the out-of-distribution challenge associated with ZSC.\nWe show that independently trained agents under this algorithm coordinate\nsuccessfully in Overcooked. Our results suggest that ZSC failure can largely be\nattributed to poor state coverage under self-play rather than more\nsophisticated coordination challenges. The Overcooked environment is therefore\nnot suitable as a ZSC benchmark. To address these shortcomings, we introduce\nOvercookedV2, a new version of the benchmark, which includes asymmetric\ninformation and stochasticity, facilitating the creation of interesting ZSC\nscenarios. To validate OvercookedV2, we conduct experiments demonstrating that\nmere exhaustive state coverage is insufficient to coordinate well. Finally, we\nuse OvercookedV2 to build a new range of coordination challenges, including\nones that require test time protocol formation, and we demonstrate the need for\nnew coordination algorithms that can adapt online. We hope that OvercookedV2\nwill help benchmark the next generation of ZSC algorithms and advance\ncollaboration between AI agents and humans."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.06442",
    "c_title":[
      "ARES: Auxiliary Range Expansion for Outlier Synthesis"
    ],
    "c_abstract":[
      "Recent successes of artificial intelligence and deep learning often depend on\nthe well-collected training dataset which is assumed to have an identical\ndistribution with the test dataset. However, this assumption, which is called\nclosed-set learning, is hard to meet in realistic scenarios for deploying deep\nlearning models. As one of the solutions to mitigate this assumption, research\non out-of-distribution (OOD) detection has been actively explored in various\ndomains. In OOD detection, we assume that we are given the data of a new class\nthat was not seen in the training phase, i.e., outlier, at the evaluation\nphase. The ultimate goal of OOD detection is to detect and classify such unseen\noutlier data as a novel \"unknown\" class. Among various research branches for\nOOD detection, generating a virtual outlier during the training phase has been\nproposed. However, conventional generation-based methodologies utilize\nin-distribution training dataset to imitate outlier instances, which limits the\nquality of the synthesized virtual outlier instance itself. In this paper, we\npropose a novel methodology for OOD detection named Auxiliary Range Expansion\nfor Outlier Synthesis, or ARES. ARES models the region for generating\nout-of-distribution instances by escaping from the given in-distribution\nregion; instead of remaining near the boundary of in-distribution region.\nVarious stages consists ARES to ultimately generate valuable OOD-like virtual\ninstances. The energy score-based discriminator is then trained to effectively\nseparate in-distribution data and outlier data. Quantitative experiments on\nbroad settings show the improvement of performance by our method, and\nqualitative results provide logical explanations of the mechanism behind it."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00439",
    "c_title":[
      "Testing Multipole Moments of compact objects beyond Kerr paradigm"
    ],
    "c_abstract":[
      "Multipole moments are related to the physical properties of compact\ngravitating objects; therefore, understanding their structure is useful in\naccessing the nature of compact objects. We look into gravitational wave\nobservables for black holes with charge, black holes on the brane, black holes\nwith torsion, and regular black holes to see if and how they are correlated to\nthe black hole hairs, which are related to the multipole moments. We find that\nthe gravitational wave observables are indeed related to the hairs of\nnon-vacuum spacetimes (for instance, charge $Q$ in the case of Kerr-Newman\nblack holes). We also constrain the black hole hairs for change in\ngravitational wave phasing to see if the dependencies are significant and can\nbe observed. The results from the analysis imply that the charge $Q$ in\nKerr-Newman black holes should be detectable; thus, we provide a constraint to\n$Q^2\/M^2$ given the spin and mass ratio of an ideal EMRI system for which\nfuture detectors like LISA can detect the change in gravitational wave\nobservables. We also look into an analytical approach to find multipole moments\nof non-vacuum black hole spacetimes, mainly using the Improved Twist Vector\napproach for the Geroch-Hansen multipole moments and the Thorne formalism. The\nnecessary analytics are computed, and the multipole moments are obtained for\nvarious non-vacuum spacetimes. However, the multipole moments don't contain any\ninformation about the black hole hairs, and we have commented on this\nobservation in our paper."
    ],
    "c_categories":[
      "gr-qc",
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.13856",
    "c_title":[
      "Systolic $S^1$-index and characterization of non-smooth Zoll convex\n  bodies"
    ],
    "c_abstract":[
      "We define the systolic $S^1$-index of a convex body as the Fadell-Rabinowitz\nindex of the space of centralized generalized systoles associated with its\nboundary. We show that this index is a symplectic invariant. Using the systolic\n$S^1$-index, we propose a definition of generalized Zoll convex bodies and\nprove that this definition is equivalent to the usual one in the smooth\nsetting. Moreover, we show how generalized Zoll convex bodies can be\ncharacterized in terms of their Gutt-Hutchings capacities and we prove that the\nspace of generalized Zoll convex bodies is closed in the space of all convex\nbodies. As a corollary, we establish that if the interior of a convex body is\nsymplectomorphic to the interior of a ball, then such a convex body must be\ngeneralized Zoll, and in particular Zoll if its boundary is smooth. Finally, we\ndiscuss some examples."
    ],
    "c_categories":[
      "math.DG",
      "math.DS",
      "math.SG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08538",
    "c_title":[
      "Improved Calculation of Acoustic Deformation Potentials from First\n  Principles"
    ],
    "c_abstract":[
      "Using density functional theory (DFT) and density functional perturbation\ntheory (DFPT), the band structure, phonon dispersion and electron phonon\ncoupling matrix were calculated for silicon (Si), diamond and cubic boron\nnitride (cBN). From these, the acoustic deformation potential was calculated\nfor multiple angles between the electron and phonon wave vectors and analytic\nexpressions for the longitudinal and acoustic modes were fit to find an average\ndeformation potential. The ability to calculate the deformation potential from\nfirst principles allows for the scattering rates to be determined without the\nuse of lengthy empirical methods. For Si, the numerically calculated\ndeformation potentials are in excellent agreement with what is seen in the\nliterature. On the other hand, the deformation potentials calculated for\ndiamond were found to be larger than what has been seen previously, however\nprevious calculations of transport parameters in diamond report a large range\nof values for scattering parameters which may be due to assumptions made in\neach model. Excellent agreement was also seen between the value calculated for\ncBN and the literature, however there are no experimental results for cBN and\nso this value is compared against an estimate. This shows that scattering\nparameters can be calculated via first principles for materials with sparse\nexperimental data, which in turn allows for increased confidence in the output\nof charge transport simulations of new and emerging materials."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09990",
    "c_title":[
      "Entanglement witness for combined atom interferometer-mechanical\n  oscillator setup"
    ],
    "c_abstract":[
      "We investigate how to entangle an atom interferometer and a macroscopic\nmechanical oscillator in order to create non-classical states of the\noscillator. We propose an entanglement witness, from whose violation, the\ngeneration of entanglement can be determined. We do this for both the noiseless\ncase and when including thermal noise. Thermal noise can arise from two\nsources: the first being when the oscillator starts in a thermal state, and the\nsecond when a continuous thermal bath is in contact with the oscillator. We\nfind that for the appropriate oscillator quality factor $Q$, violation always\nexists for any value of magnetic coupling $\\lambda$ and thermal occupancy $\\bar\nn$. Cooling the oscillator to its ground state provides an $O(\\bar n)$\nimprovement in the EW violation than starting in a thermal state. However, this\nstill requires at least 10$^{5}$ measurements to be determined. We then\nconsider how to use magnetic interactions to realize this idea."
    ],
    "c_categories":[
      "physics.atom-ph",
      "physics.ins-det",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.13126",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
    ],
    "b_abstract":[
      "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.07421",
    "c_title":[
      "Hyperbolization and geometric decomposition of a class of 3-manifolds"
    ],
    "c_abstract":[
      "Thurston's triangulation conjecture asserts that every hyperbolic 3-manifold\nadmits a geometric triangulation into hyper-ideal hyperbolic tetrahedra. So\nfar, this conjecture had only been proven for a few special 3-manifolds. In\nthis article, we confirm this conjecture for a class of 3-manifolds. To be\nprecise, let $M$ be an oriented compact 3-manifold with boundary, no component\nof which is a 2-sphere, and $\\mathcal{T}$ is an ideal triangulation of $M$. If\n$\\mathcal{T}$ satisfies properly gluing condition, and the valence is at least\n6 at each ideal edge and 11 at each hyper-ideal edge, then $M$ admits an unique\ncomplete hyperbolic metric with totally geodesic boundary, so that\n$\\mathcal{T}$ is isotopic to a geometric ideal triangulation of $M$.\n  We use analytical tools such as combinatorial Ricci flow (CRF, abbr.) to\nderive the conclusions. There are intrinsic difficulties in dealing with CRF.\nFirst, the CRF may collapse in a finite time, second, most of the smooth\ncurvature flow methods are no longer applicable since there is no local\ncoordinates in $\\mathcal{T}$, and third, the evolution of CRF is affected by\ncertain combinatorial obstacles in addition to topology. To this end, we\nintroduce the ideas as ``extending CRF\", ``tetrahedral comparison principles\",\nand ``control CRF with edge valence\" to solve the above difficulties. In\naddition, the presence of torus boundary adds substantial difficulties in this\narticle, which we have solved by introducing the properly gluing conditions on\n$\\mathcal{T}$ and reducing the ECRF to a flow relatively easy to handle."
    ],
    "c_categories":[
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"Comprehensive Methodology for Sample Augmentation in EEG Biomarker\n  Studies for Alzheimers Risk Classification",
    "a_abstract":"Background: Dementia, marked by cognitive decline, is a global health\nchallenge. Alzheimer's disease (AD), the leading type, accounts for ~70% of\ncases. Electroencephalography (EEG) measures show promise in identifying AD\nrisk, but obtaining large samples for reliable comparisons is challenging.\nObjective: This study integrates signal processing, harmonization, and\nstatistical techniques to enhance sample size and improve AD risk\nclassification reliability. Methods: We used advanced EEG preprocessing,\nfeature extraction, harmonization, and propensity score matching (PSM) to\nbalance healthy non-carriers (HC) and asymptomatic E280A mutation carriers\n(ACr). Data from four databases were harmonized to adjust site effects while\npreserving covariates like age and sex. PSM ratios (2:1, 5:1, 10:1) were\napplied to assess sample size impact on model performance. The final dataset\nunderwent machine learning analysis with decision trees and cross-validation\nfor robust results. Results: Balancing sample sizes via PSM significantly\nimproved classification accuracy, ranging from 0.92 to 0.96 across ratios. This\napproach enabled precise risk identification even with limited samples.\nConclusion: Integrating data processing, harmonization, and balancing\ntechniques improves AD risk classification accuracy, offering potential for\nother neurodegenerative diseases.",
    "explanation":"This study implements a\ncomprehensive methodology that integrates signal processing, data harmonization, and\nstatistical techniques to increase sample size and improve the reliability of Alzheimer's disease\nrisk classification models.",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b5"
    ],
    "c_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "c_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.08070",
    "c_title":[
      "Normative Cerebral Perfusion Across the Lifespan"
    ],
    "c_abstract":[
      "Cerebral perfusion plays a crucial role in maintaining brain function and is\ntightly coupled with neuronal activity. While previous studies have examined\ncerebral perfusion trajectories across development and aging, precise\ncharacterization of its lifespan dynamics has been limited by small sample\nsizes and methodological inconsistencies. In this study, we construct the first\ncomprehensive normative model of cerebral perfusion across the human lifespan\n(birth to 85 years) using a large multi-site dataset of over 12,000\nhigh-quality arterial spin labeling (ASL) MRI scans. Leveraging generalized\nadditive models for location, scale, and shape (GAMLSS), we mapped nonlinear\ngrowth trajectories of cerebral perfusion at global, network, and regional\nlevels. We observed a rapid postnatal increase in cerebral perfusion, peaking\nat approximately 7.1 years, followed by a gradual decline into adulthood. Sex\ndifferences were evident, with distinct regional maturation patterns rather\nthan uniform differences across all brain regions. Beyond normative modeling,\nwe quantified individual deviations from expected CBF patterns in\nneurodegenerative and psychiatric conditions, identifying disease-specific\nperfusion abnormalities across four brain disorders. Using longitudinal data,\nwe established typical and atypical cerebral perfusion trajectories,\nhighlighting the prognostic value of perfusion-based biomarkers for detecting\ndisease progression. Our findings provide a robust normative framework for\ncerebral perfusion, facilitating precise characterization of brain health\nacross the lifespan and enhancing the early identification of neurovascular\ndysfunction in clinical populations."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09672",
    "c_title":[
      "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale"
    ],
    "c_abstract":[
      "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06286",
    "c_title":[
      "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision"
    ],
    "c_abstract":[
      "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.13661",
    "c_title":[
      "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study"
    ],
    "c_abstract":[
      "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.10098",
    "c_title":[
      "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning"
    ],
    "c_abstract":[
      "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies."
    ],
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00439",
    "c_title":[
      "Testing Multipole Moments of compact objects beyond Kerr paradigm"
    ],
    "c_abstract":[
      "Multipole moments are related to the physical properties of compact\ngravitating objects; therefore, understanding their structure is useful in\naccessing the nature of compact objects. We look into gravitational wave\nobservables for black holes with charge, black holes on the brane, black holes\nwith torsion, and regular black holes to see if and how they are correlated to\nthe black hole hairs, which are related to the multipole moments. We find that\nthe gravitational wave observables are indeed related to the hairs of\nnon-vacuum spacetimes (for instance, charge $Q$ in the case of Kerr-Newman\nblack holes). We also constrain the black hole hairs for change in\ngravitational wave phasing to see if the dependencies are significant and can\nbe observed. The results from the analysis imply that the charge $Q$ in\nKerr-Newman black holes should be detectable; thus, we provide a constraint to\n$Q^2\/M^2$ given the spin and mass ratio of an ideal EMRI system for which\nfuture detectors like LISA can detect the change in gravitational wave\nobservables. We also look into an analytical approach to find multipole moments\nof non-vacuum black hole spacetimes, mainly using the Improved Twist Vector\napproach for the Geroch-Hansen multipole moments and the Thorne formalism. The\nnecessary analytics are computed, and the multipole moments are obtained for\nvarious non-vacuum spacetimes. However, the multipole moments don't contain any\ninformation about the black hole hairs, and we have commented on this\nobservation in our paper."
    ],
    "c_categories":[
      "gr-qc",
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13856",
    "c_title":[
      "Systolic $S^1$-index and characterization of non-smooth Zoll convex\n  bodies"
    ],
    "c_abstract":[
      "We define the systolic $S^1$-index of a convex body as the Fadell-Rabinowitz\nindex of the space of centralized generalized systoles associated with its\nboundary. We show that this index is a symplectic invariant. Using the systolic\n$S^1$-index, we propose a definition of generalized Zoll convex bodies and\nprove that this definition is equivalent to the usual one in the smooth\nsetting. Moreover, we show how generalized Zoll convex bodies can be\ncharacterized in terms of their Gutt-Hutchings capacities and we prove that the\nspace of generalized Zoll convex bodies is closed in the space of all convex\nbodies. As a corollary, we establish that if the interior of a convex body is\nsymplectomorphic to the interior of a ball, then such a convex body must be\ngeneralized Zoll, and in particular Zoll if its boundary is smooth. Finally, we\ndiscuss some examples."
    ],
    "c_categories":[
      "math.DG",
      "math.DS",
      "math.SG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.08538",
    "c_title":[
      "Improved Calculation of Acoustic Deformation Potentials from First\n  Principles"
    ],
    "c_abstract":[
      "Using density functional theory (DFT) and density functional perturbation\ntheory (DFPT), the band structure, phonon dispersion and electron phonon\ncoupling matrix were calculated for silicon (Si), diamond and cubic boron\nnitride (cBN). From these, the acoustic deformation potential was calculated\nfor multiple angles between the electron and phonon wave vectors and analytic\nexpressions for the longitudinal and acoustic modes were fit to find an average\ndeformation potential. The ability to calculate the deformation potential from\nfirst principles allows for the scattering rates to be determined without the\nuse of lengthy empirical methods. For Si, the numerically calculated\ndeformation potentials are in excellent agreement with what is seen in the\nliterature. On the other hand, the deformation potentials calculated for\ndiamond were found to be larger than what has been seen previously, however\nprevious calculations of transport parameters in diamond report a large range\nof values for scattering parameters which may be due to assumptions made in\neach model. Excellent agreement was also seen between the value calculated for\ncBN and the literature, however there are no experimental results for cBN and\nso this value is compared against an estimate. This shows that scattering\nparameters can be calculated via first principles for materials with sparse\nexperimental data, which in turn allows for increased confidence in the output\nof charge transport simulations of new and emerging materials."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09990",
    "c_title":[
      "Entanglement witness for combined atom interferometer-mechanical\n  oscillator setup"
    ],
    "c_abstract":[
      "We investigate how to entangle an atom interferometer and a macroscopic\nmechanical oscillator in order to create non-classical states of the\noscillator. We propose an entanglement witness, from whose violation, the\ngeneration of entanglement can be determined. We do this for both the noiseless\ncase and when including thermal noise. Thermal noise can arise from two\nsources: the first being when the oscillator starts in a thermal state, and the\nsecond when a continuous thermal bath is in contact with the oscillator. We\nfind that for the appropriate oscillator quality factor $Q$, violation always\nexists for any value of magnetic coupling $\\lambda$ and thermal occupancy $\\bar\nn$. Cooling the oscillator to its ground state provides an $O(\\bar n)$\nimprovement in the EW violation than starting in a thermal state. However, this\nstill requires at least 10$^{5}$ measurements to be determined. We then\nconsider how to use magnetic interactions to realize this idea."
    ],
    "c_categories":[
      "physics.atom-ph",
      "physics.ins-det",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
    ],
    "b_abstract":[
      "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07421",
    "c_title":[
      "Hyperbolization and geometric decomposition of a class of 3-manifolds"
    ],
    "c_abstract":[
      "Thurston's triangulation conjecture asserts that every hyperbolic 3-manifold\nadmits a geometric triangulation into hyper-ideal hyperbolic tetrahedra. So\nfar, this conjecture had only been proven for a few special 3-manifolds. In\nthis article, we confirm this conjecture for a class of 3-manifolds. To be\nprecise, let $M$ be an oriented compact 3-manifold with boundary, no component\nof which is a 2-sphere, and $\\mathcal{T}$ is an ideal triangulation of $M$. If\n$\\mathcal{T}$ satisfies properly gluing condition, and the valence is at least\n6 at each ideal edge and 11 at each hyper-ideal edge, then $M$ admits an unique\ncomplete hyperbolic metric with totally geodesic boundary, so that\n$\\mathcal{T}$ is isotopic to a geometric ideal triangulation of $M$.\n  We use analytical tools such as combinatorial Ricci flow (CRF, abbr.) to\nderive the conclusions. There are intrinsic difficulties in dealing with CRF.\nFirst, the CRF may collapse in a finite time, second, most of the smooth\ncurvature flow methods are no longer applicable since there is no local\ncoordinates in $\\mathcal{T}$, and third, the evolution of CRF is affected by\ncertain combinatorial obstacles in addition to topology. To this end, we\nintroduce the ideas as ``extending CRF\", ``tetrahedral comparison principles\",\nand ``control CRF with edge valence\" to solve the above difficulties. In\naddition, the presence of torus boundary adds substantial difficulties in this\narticle, which we have solved by introducing the properly gluing conditions on\n$\\mathcal{T}$ and reducing the ECRF to a flow relatively easy to handle."
    ],
    "c_categories":[
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06727",
    "c_title":[
      "Application of Artificial Intelligence (AI) in Civil Engineering"
    ],
    "c_abstract":[
      "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.06713",
    "c_title":[
      "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation"
    ],
    "c_abstract":[
      "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps:\/\/github.com\/HKUDS\/MiniRAG."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.16242",
    "c_title":[
      "Reproducibility Study of Cooperation, Competition, and Maliciousness:\n  LLM-Stakeholders Interactive Negotiation"
    ],
    "c_abstract":[
      "This paper presents a reproducibility study and extension of \"Cooperation,\nCompetition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We\nvalidate the original findings using a range of open-weight models (1.5B-70B\nparameters) and GPT-4o Mini while introducing several novel contributions. We\nanalyze the Pareto front of the games, propose a communication-free baseline to\ntest whether successful negotiations are possible without agent interaction,\nevaluate recent small language models' performance, analyze structural\ninformation leakage in model responses, and implement an inequality metric to\nassess negotiation fairness. Our results demonstrate that smaller models (<10B\nparameters) struggle with format adherence and coherent responses, but larger\nopen-weight models can approach proprietary model performance. Additionally, in\nmany scenarios, single-agent approaches can achieve comparable results to\nmulti-agent negotiations, challenging assumptions about the necessity of agent\ncommunication to perform well on the benchmark. This work also provides\ninsights into the accessibility, fairness, environmental impact, and privacy\nconsiderations of LLM-based negotiation systems."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.09038",
    "c_title":[
      "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT"
    ],
    "c_abstract":[
      "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16938",
    "c_title":[
      "Interpretable Machine Learning for Oral Lesion Diagnosis through\n  Prototypical Instances Identification"
    ],
    "c_abstract":[
      "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05297",
    "c_title":[
      "The Role of Atmospheric Composition in Defining the Habitable Zone\n  Limits and Supporting E. coli Growth"
    ],
    "c_abstract":[
      "Studying exoplanet atmospheres is essential for assessing their potential to\nhost liquid water and their capacity to support life (their habitability). Each\natmosphere uniquely influences the likelihood of surface liquid water, defining\nthe habitable zone (HZ), the region around a star where liquid water can exist.\nHowever, being within the HZ does not guarantee habitability, as life requires\nmore than just liquid water. In this study, we adopted a two-pronged approach.\nFirst, we estimated the surface conditions of planets near the HZ's inner edge\nunder various atmospheric compositions. By utilizing a 3D climate model, we\nrefined the inner boundaries of the HZ for planets with atmospheres dominated\nby H2 and CO2 for the first time. Second, we investigated microbial survival in\nthese environments, conducting laboratory experiments on the growth and\nsurvival of E. coli K-12, focusing on the impact of different gas compositions.\nThis innovative combination of climate modeling and biological experiments\nbridges theoretical climate predictions with biological outcomes. Our findings\nindicate that atmospheric composition significantly affects bacterial growth\npatterns, highlighting the importance of considering diverse atmospheres in\nevaluating exoplanet habitability and advancing the search for life beyond\nEarth."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.20672",
    "c_title":[
      "Continuum Effects and the Trojan Horse Mechanism in Halo Nuclei-Induced\n  Reactions: Implications for Heavy Isotope Synthesis"
    ],
    "c_abstract":[
      "Nonelastic breakup (NEB) reactions induced by the halo nucleus $^{11}$Be on\n$^{64}$Zn at 28.7 MeV are investigated using the Ichimura-Austern-Vincent (IAV)\nmodel combined with the Continuum Discretized Coupled Channels (CDCC) method.\nNEB cross sections calculated with full CDCC wave functions (including\ncontinuum states), ground-state-only CDCC wave functions, and single-channel\ncalculations are compared. The results indicate that continuum effects are\nnegligible and that NEB cross sections are dominated by the ground-state\ncontribution. This validates the use of simpler models like the distorted wave\nBorn approximation for such reactions. Additionally, by varying the binding\nenergy in a toy model, the feasibility of using halo nuclei in the Trojan Horse\nMethod (THM) for synthesizing heavy isotopes is explored. It is demonstrated\nthat THM significantly enhances sub-barrier fusion cross sections due to the\nweak binding of halo nuclei, offering a promising approach for the synthesis of\nnew elements."
    ],
    "c_categories":[
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09670",
    "c_title":[
      "On the generalized eigenvalue problem in subspace-based excited state\n  methods for quantum computers"
    ],
    "c_abstract":[
      "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers."
    ],
    "c_categories":[
      "physics.chem-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12591",
    "c_title":[
      "Revealing Nanostructures in High-Entropy Alloys via Machine-Learning\n  Accelerated Scalable Monte Carlo Simulation"
    ],
    "c_abstract":[
      "Large-scale simulations are a powerful tool for understanding material\nbehavior and designing new materials. However, the computational limitations of\natomistic-scale simulations stemming from constraints in computational power,\nalgorithmic efficiency, and accuracy, prevent us from modeling large systems,\neven at ground state,(DFT)-level precision. Here, we introduce an efficient and\nScalable Monte Carlo (SMC) method that overcomes the parallelization\nbottlenecks inherent in conventional MC simulation algorithms, reducing the\ncomputational complexity of an MC sweep from cubic to linear. Leveraging the\nlarge degree of parallelization inherent in our method, we present a GPU\naccelerated implementation, SMC_GPU, which enables the simulation of atomistic\nsystems exceeding one billion atoms while maintaining the accuracy of density\nfunctional theory (DFT). Using this unprecedented capability, we perform\nlarge-scale thermodynamic simulations to investigate the microstructure\nevolution in the FeCoNiAlTi and MoNbTaW high entropy alloys (HEAs). Our results\nreveal a rich diversity of nanoscale phenomena, including short-range and\nlong-range order, nanoparticles, and nanophases. The size, composition, and\nmorphology of these nanostructures, which can comprise millions of atoms and\nthus present significant challenges for traditional methods, are analyzed with\nhigh-accuracy atomistic simulations for the first time, to the best of our\nknowledge. Our simulations produce thermodynamic properties and nanostructures\nthat align well with available theoretical and experimental data. More\nintriguingly, our results reveal that the intricate nanoscale interplay of\norder and disorder in HEA stems from the combined effects of chemical\ncomplexity and temperature. This work underscores the promising potential of\nlarge-scale MC simulation for HEAs.let alone at finite temperatures, with\ndensity functional theory"
    ],
    "c_categories":[
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17717",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
    ],
    "b_abstract":[
      "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19490",
    "c_title":[
      "Signatures of collective photon emission and ferroelectric ordering of\n  excitons near their Mott insulating state in a WSe$_2$\/WS$_2$ heterobilayer"
    ],
    "c_abstract":[
      "Spontaneous symmetry breaking, arising from the competition of interactions\nand quantum fluctuations, is fundamental to understanding ordered electronic\nphases. Although electrically neutral, optical excitations like excitons can\ninteract through their dipole moment, raising the possibility of optically\nactive ordered phases. The effects of spontaneous ordering on optical\nproperties remain largely unexplored. Recent observations of the excitonic Mott\ninsulating state in semiconducting moir\\'e crystals make them promising for\naddressing this question. Here, we present evidence for an in-plane\nferroelectric phase of dipolar moir\\'e excitons driven by strong\nexciton-exciton interactions. We discover a surprising speed-up of photon\nemission at late times and low densities in excitonic decay. This\ncounterintuitive behavior is attributed to collective radiance, linked to the\ntransition between disordered and symmetry-broken ferroelectric phases of\nmoir\\'e excitons. Our findings provide first evidence for strong dipolar\ninter-site interactions in moir\\'e lattices, demonstrate collective photon\nemission as a probe for moir\\'e quantum materials, and pave the way for\nexploring cooperative optical phenomena in strongly correlated systems."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.str-el",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"Maximizing the Impact of Deep Learning on Subseasonal-to-Seasonal\n  Climate Forecasting: The Essential Role of Optimization",
    "a_abstract":"Weather and climate forecasting is vital for sectors such as agriculture and\ndisaster management. Although numerical weather prediction (NWP) systems have\nadvanced, forecasting at the subseasonal-to-seasonal (S2S) scale, spanning 2 to\n6 weeks, remains challenging due to the chaotic and sparse atmospheric signals\nat this interval. Even state-of-the-art deep learning models struggle to\noutperform simple climatology models in this domain. This paper identifies that\noptimization, instead of network structure, could be the root cause of this\nperformance gap, and then we develop a novel multi-stage optimization strategy\nto close the gap. Extensive empirical studies demonstrate that our multi-stage\noptimization approach significantly improves key skill metrics, PCC and TCC,\nwhile utilizing the same backbone structure, surpassing the state-of-the-art\nNWP systems (ECMWF-S2S) by over \\textbf{19-91\\%}. Our research contests the\nrecent study that direct forecasting outperforms rolling forecasting for S2S\ntasks. Through theoretical analysis, we propose that the underperformance of\nrolling forecasting may arise from the accumulation of Jacobian matrix products\nduring training. Our multi-stage framework can be viewed as a form of teacher\nforcing to address this issue. Code is available at\n\\url{https:\/\/anonymous.4open.science\/r\/Baguan-S2S-23E7\/}",
    "explanation":"Weather and climate forecasting is vital for sectors such\nas agriculture and disaster management. Although numeri-\ncal weather prediction (NWP) systems have advanced, fore-\ncasting at the subseasonal-to-seasonal (S2S) scale, span-\nning 2 to 6 weeks, remains challenging due to the chaotic\nand sparse atmospheric signals at this interval.",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b21"
    ],
    "c_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "c_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "c_categories":[
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.01476",
    "c_title":[
      "Sparse identification of evolution equations via Bayesian model\n  selection"
    ],
    "c_abstract":[
      "The quantitative formulation of evolution equations is the backbone for\nprediction, control, and understanding of dynamical systems across diverse\nscientific fields. Besides deriving differential equations for dynamical\nsystems based on basic scientific reasoning or prior knowledge in recent times\na growing interest emerged to infer these equations purely from data. In this\narticle, we introduce a novel method for the sparse identification of nonlinear\ndynamical systems from observational data, based on the observation how the key\nchallenges of the quality of time derivatives and sampling rates influence this\nproblem. Our approach combines system identification based on thresholded least\nsquares minimization with additional error measures that account for both the\ndeviation between the model and the time derivative of the data, and the\nintegrated performance of the model in forecasting dynamics. Specifically, we\nintegrate a least squares error as well as the Wasserstein metric for estimated\nmodels and combine them within a Bayesian optimization framework to efficiently\ndetermine optimal hyperparameters for thresholding and weighting of the\ndifferent error norms. Additionally, we employ distinct regularization\nparameters for each differential equation in the system, enhancing the method's\nprecision and flexibility. We demonstrate the capabilities of our approach\nthrough applications to dynamical fMRI data and the prototypical example of a\nwake flow behind a cylinder. In the wake flow problem, our method identifies a\nsparse, accurate model that correctly captures transient dynamics, oscillation\nperiods, and phase information, outperforming existing methods. In the fMRI\nexample, we show how our approach extracts insights from a trained recurrent\nneural network, offering a novel avenue for explainable AI by inferring\ndifferential equations that capture potentially causal relationships."
    ],
    "c_categories":[
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06754",
    "c_title":[
      "Signatures of extreme events in the cumulative entropic spectrum"
    ],
    "c_abstract":[
      "In this study, the cumulative effect of the empirical probability\ndistribution of a random variable is identified as a factor that amplifies the\noccurrence of extreme events in datasets. To quantify this observation, a\ncorresponding information measure is introduced, drawing upon Shannon entropy\nfor joint probabilities. The proposed approach is validated using selected\nmarket data as case studies, encompassing various instances of extreme events.\nIn particular, the results indicate that the introduced cumulative measure\nexhibits distinctive signatures of such events, even when the data is\nrelatively noisy. These findings highlight the potential of the discussed\nconcept for developing a new class of related indicators or classifiers."
    ],
    "c_categories":[
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16814",
    "c_title":[
      "Dynamic Metadata Schemes in the Neutron and Photon Science Communities:\n  A Case Study of X-Ray Photon Correlation Spectroscopy"
    ],
    "c_abstract":[
      "Metadata is one of the most important aspects for advancing data management\npractices within all research communities. Definitions and schemes of metadata\nare inter alia of particular significance in the domain of neutron and photon\nscattering experiments covering a broad area of different scientific\ndisciplines. The demand of describing continuously evolving highly\nnonstandardized experiments, including the resulting processed and published\ndata, constitutes a considerable challenge for a static definition of metadata.\nHere, we present the concept of dynamic metadata for the neutron and photon\nscientific community, which enriches a static set of defined basic metadata. We\nexplore the idea of dynamic metadata with the help of the use case of X-ray\nPhoton Correlation Spectroscopy (XPCS), which is a synchrotron-based scattering\ntechnique that allows the investigation of nanoscale dynamic processes. It\nserves here as a demonstrator of how dynamic metadata can improve data\nacquisition, sharing, and analysis workflows. Our approach enables researchers\nto tailor metadata definitions dynamically and adapt them to the evolving\ndemands of describing data and results from a diverse set of experiments. We\ndemonstrate that dynamic metadata standards yield advantages that enhance data\nreproducibility, interoperability, and the dissemination of knowledge."
    ],
    "c_categories":[
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14861",
    "c_title":[
      "Maximum likelihood estimation of burst-merging kernels for bursty time\n  series"
    ],
    "c_abstract":[
      "Various time series in natural and social processes have been found to be\nbursty. Events in the time series rapidly occur within short time periods,\nforming bursts, which are alternated with long inactive periods. As the\ntimescale defining bursts increases, individual events are sequentially merged\nto become small bursts and then bigger ones, eventually leading to the single\nburst containing all events. Such a merging pattern has been depicted by a tree\nthat fully reveals the hierarchical structure of bursts, thus called a burst\ntree. The burst-tree structure can be simply characterized by a burst-merging\nkernel that dictates which bursts are merged together as the timescale\nincreases. In this work, we develop the maximum likelihood estimation method of\nthe burst-merging kernel from time series, which is successfully tested against\nthe time series generated using several model kernels. We also apply our method\nto some empirical time series from various backgrounds. Our method provides a\nuseful tool to precisely characterize the time series data, hence enabling to\nstudy their underlying mechanisms more accurately."
    ],
    "c_categories":[
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07666",
    "c_title":[
      "High-Performance Data Format for Scientific Data Storage and Analysis"
    ],
    "c_abstract":[
      "In this article, we present the High-Performance Output (HiPO) data format\ndeveloped at Jefferson Laboratory for storing and analyzing data from Nuclear\nPhysics experiments. The format was designed to efficiently store large amounts\nof experimental data, utilizing modern fast compression algorithms. The purpose\nof this development was to provide organized data in the output, facilitating\naccess to relevant information within the large data files. The HiPO data\nformat has features that are suited for storing raw detector data,\nreconstruction data, and the final physics analysis data efficiently,\neliminating the need to do data conversions through the lifecycle of\nexperimental data. The HiPO data format is implemented in C++ and JAVA, and\nprovides bindings to FORTRAN, Python, and Julia, providing users with the\nchoice of data analysis frameworks to use. In this paper, we will present the\ngeneral design and functionalities of the HiPO library and compare the\nperformance of the library with more established data formats used in data\nanalysis in High Energy and Nuclear Physics (such as ROOT and Parquete)."
    ],
    "c_categories":[
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.03041",
    "c_title":[
      "Group Shapley with Robust Significance Testing and Its Application to\n  Bond Recovery Rate Prediction"
    ],
    "c_abstract":[
      "We propose Group Shapley, a metric that extends the classical\nindividual-level Shapley value framework to evaluate the importance of feature\ngroups, addressing the structured nature of predictors commonly found in\nbusiness and economic data. More importantly, we develop a significance testing\nprocedure based on a three-cumulant chi-square approximation and establish the\nasymptotic properties of the test statistics for Group Shapley values. Our\napproach can effectively handle challenging scenarios, including sparse or\nskewed distributions and small sample sizes, outperforming alternative tests\nsuch as the Wald test. Simulations confirm that the proposed test maintains\nrobust empirical size and demonstrates enhanced power under diverse conditions.\nTo illustrate the method's practical relevance in advancing Explainable AI, we\napply our framework to bond recovery rate predictions using a global dataset\n(1996-2023) comprising 2,094 observations and 98 features, grouped into 16\nsubgroups and five broader categories: bond characteristics, firm fundamentals,\nindustry-specific factors, market-related variables, and macroeconomic\nindicators. Our results identify the market-related variables group as the most\ninfluential. Furthermore, Lorenz curves and Gini indices reveal that Group\nShapley assigns feature importance more equitably compared to individual\nShapley values."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.01198",
    "c_title":[
      "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study"
    ],
    "c_abstract":[
      "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.03900",
    "c_title":[
      "Realising VCD for untwisted automorphism groups of RAAGs"
    ],
    "c_abstract":[
      "The virtual cohomological dimension of $\\operatorname{Out}(F_n)$ is given\nprecisely by the dimension of the spine of Culler--Vogtmann Outer space.\nHowever, the dimension of the spine of untwisted Outer space for a general\nright-angled Artin group $A_\\Gamma$ does not necessarily match the virtual\ncohomological dimension of the untwisted subgroup $U(A_\\Gamma) \\leq\n\\operatorname{Out}(A_\\Gamma)$. Under certain graph-theoretic conditions, we\nperform an equivariant deformation retraction of this spine to produce a new\ncontractible cube complex upon which $U(A_\\Gamma)$ acts properly and\ncocompactly. Furthermore, we give conditions for when the dimension of this\ncomplex realises the virtual cohomological dimension of $U(A_\\Gamma)$."
    ],
    "c_categories":[
      "math.GR",
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13051",
    "c_title":[
      "Permutation Learning with Only N Parameters: From SoftSort to\n  Self-Organizing Gaussians"
    ],
    "c_abstract":[
      "Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical."
    ],
    "c_categories":[
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
    ],
    "b_abstract":[
      "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03333",
    "c_title":[
      "Causal drivers of dynamic networks"
    ],
    "c_abstract":[
      "Dynamic networks models describe temporal interactions between social actors,\nand as such have been used to describe financial fraudulent transactions,\ndispersion of destructive invasive species across the globe, and the spread of\nfake news. An important question in all of these examples is what are the\ncausal drivers underlying these processes. Current network models are\nexclusively descriptive and based on correlative structures.\n  In this paper we propose a causal extension of dynamic network modelling. In\nparticular, we prove that the causal model satisfies a set of population\nconditions that uniquely identifies the causal drivers. The empirical analogue\nof these conditions provide a consistent causal discovery algorithm, which\ndistinguishes it from other inferential approaches. Crucially, data from a\nsingle environment is sufficient. We apply the method in an analysis of bike\nsharing data in Washington D.C. in July 2023."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.07290",
    "c_title":[
      "Principles for Responsible AI Consciousness Research"
    ],
    "c_abstract":[
      "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.19629",
    "c_title":[
      "Agentic Mixture-of-Workflows for Multi-Modal Chemical Search"
    ],
    "c_abstract":[
      "The vast and complex materials design space demands innovative strategies to\nintegrate multidisciplinary scientific knowledge and optimize materials\ndiscovery. While large language models (LLMs) have demonstrated promising\nreasoning and automation capabilities across various domains, their application\nin materials science remains limited due to a lack of benchmarking standards\nand practical implementation frameworks. To address these challenges, we\nintroduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented\nGeneration (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic\nworkflows employing distinct CRAG strategies using open-source LLMs. Unlike\nprior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration\nagent, enabling direct evaluation of multiple LLMs across the same problem\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral\nretrieval. Our results demonstrate that CRAG-MoWs achieve performance\ncomparable to GPT-4o while being preferred more frequently in comparative\nevaluations, highlighting the advantage of structured retrieval and multi-agent\nsynthesis. By revealing performance variations across data types, CRAG-MoW\nprovides a scalable, interpretable, and benchmark-driven approach to optimizing\nAI architectures for materials discovery. These insights are pivotal in\naddressing fundamental gaps in benchmarking LLMs and autonomous AI agents for\nscientific applications."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.17297",
    "c_title":[
      "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts"
    ],
    "c_abstract":[
      "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.16169",
    "c_title":[
      "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations"
    ],
    "c_abstract":[
      "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at\n\\href{https:\/\/github.com\/lcy2723\/Robust-Rule-Induction}{https:\/\/github.com\/lcy2723\/Robust-Rule-Induction}."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.03137",
    "c_title":[
      "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver"
    ],
    "c_abstract":[
      "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.08849",
    "c_title":[
      "Local rigidity for symplectic billiards"
    ],
    "c_abstract":[
      "We show a local rigidity result for the integrability of symplectic\nbilliards. We prove that any domain which is close to an ellipse, and for which\nthe symplectic billiard map is rationally integrable must be an ellipse as\nwell. This is in spirit of the result of Avila, De Simoi, and Kaloshin for\nBirkhoff billiards."
    ],
    "c_categories":[
      "math.DS"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.08477",
    "c_title":[
      "Progressive hedging for multi-stage stochastic lot sizing problems with\n  setup carry-over under uncertain demand"
    ],
    "c_abstract":[
      "We investigate multi-stage demand uncertainty for the multi-item\nmulti-echelon capacitated lot sizing problem with setup carry-over. Considering\na multi-stage decision framework helps to quantify the benefits of being able\nto adapt decisions to newly available information. The drawback is that\nmulti-stage stochastic optimization approaches lead to very challenging\nformulations. This is because they usually rely on scenario tree\nrepresentations of the uncertainty, which grow exponentially in the number of\ndecision stages. Thus, even for a moderate number of decision stages it becomes\ndifficult to solve the problem by means of a compact optimization model. To\naddress this issue, we propose a progressive hedging algorithm and we\ninvestigate and tune the crucial penalty parameter that influences the\nconflicting goals of fast convergence and solution quality. While low penalty\nparameters usually lead to high quality solutions, this comes at the cost of\nslow convergence. To tackle this problem, we adapt metaheuristic adjustment\nstrategies to guide the algorithm towards a consensus more efficiently.\nFurthermore, we consider several options to compute the consensus solution.\nWhile averaging the subproblem decisions is a common choice, we also apply a\nmajority voting procedure. We test different algorithm configurations and\ncompare the results of progressive hedging to the solutions obtained by solving\na compact optimization model on well-known benchmark instances. For several\nproblem instances the progressive hedging algorithm converges to solutions\nwithin 1% of the cost of the compact model's solution, while requiring shorter\nruntimes."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.18575",
    "c_title":[
      "Colored Jones Polynomials and the Volume Conjecture"
    ],
    "c_abstract":[
      "Using the vertex model approach for braid representations, we compute\npolynomials for spin-1 placed on hyperbolic knots up to 15 crossings. These\npolynomials are referred to as 3-colored Jones polynomials or adjoint Jones\npolynomials. Training a subset of the data using a fully connected feedforward\nneural network, we predict the volume of the knot complement of hyperbolic\nknots from the adjoint Jones polynomial or its evaluations with 99.34%\naccuracy. A function of the adjoint Jones polynomial evaluated at the phase\n$q=e^{ 8 \\pi i \/ 15 }$ predicts the volume with nearly the same accuracy as the\nneural network. From an analysis of 2-colored and 3-colored Jones polynomials,\nwe conjecture the best phase for $n$-colored Jones polynomials, and use this\nhypothesis to motivate an improved statement of the volume conjecture. This is\ntested for knots for which closed form expressions for the $n$-colored Jones\npolynomial are known, and we show improved convergence to the volume."
    ],
    "c_categories":[
      "cs.LG",
      "hep-th",
      "math.GT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.04917",
    "c_title":[
      "Wave Decay with Singular Damping"
    ],
    "c_abstract":[
      "We consider the stabilization problem on a manifold with boundary for a wave\nequation with measure-valued linear damping. For a wide class of measures,\ncontaining Dirac masses on hypersurfaces as well as measures with fractal\nsupport, we establish an abstract energy decay result."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.16728",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b21"
    ],
    "b_title":[
      "Analysis methods for numerical weather prediction"
    ],
    "b_abstract":[
      "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
    ],
    "b_categories":[
      "physics.data-an"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.19292",
    "c_title":[
      "Optimal COVID-19 vaccine prioritization by age depends critically on\n  inter-group contacts and vaccination rates"
    ],
    "c_abstract":[
      "The limited availability of COVID-19 vaccines has prompted extensive research\non optimal vaccination strategies. Previous studies have considered various\nnon-pharmaceutical interventions, vaccine efficacy, and distribution\nstrategies. In this work, we address the combined effects of inter-group\ncontacts and vaccination rates under contact reduction, analyzing the Spanish\npopulation's demographic and age group contact patterns and incorporating\nreinfection dynamics. We conduct an exhaustive analysis, evaluating 362,880\npermutations of 9 age groups across 6 vaccination rates and two distinct,\nempirically quantified scenarios for social contacts. Our results show that at\nintermediate-to-high vaccination rates with unrestricted social contacts,\noptimal age-based vaccination strategies only slightly deviate from\nolder-to-younger prioritization, yielding marginal reductions in deaths and\ninfections. However, when significant reductions in social contacts are\nenforced -similar to the lockdowns in 2020-, there are substantial\nimprovements, particularly at moderate vaccination rates. These restrictions\nlead to a transition where infection propagation is halted, a scenario that\nbecame achievable during the pandemic with the observed vaccination rates. Our\nfindings emphasize the importance of combining appropriate social contact\nreductions with vaccination to optimize age-based vaccination strategies,\nunderscoring the complex, nonlinear dynamics involved in pandemic dynamics and\nthe necessity for tailored, context-specific interventions."
    ],
    "c_categories":[
      "nlin.AO",
      "physics.bio-ph",
      "physics.soc-ph",
      "q-bio.PE"
    ],
    "c_fields":[
      "Quantitative Biology",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"Weakly supervised deep learning model with size constraint for prostate\n  cancer detection in multiparametric MRI and generalization to unseen domains",
    "a_abstract":"Fully supervised deep models have shown promising performance for many\nmedical segmentation tasks. Still, the deployment of these tools in clinics is\nlimited by the very timeconsuming collection of manually expert-annotated data.\nMoreover, most of the state-ofthe-art models have been trained and validated on\nmoderately homogeneous datasets. It is known that deep learning methods are\noften greatly degraded by domain or label shifts and are yet to be built in\nsuch a way as to be robust to unseen data or label distributions. In the\nclinical setting, this problematic is particularly relevant as the deployment\ninstitutions may have different scanners or acquisition protocols than those\nfrom which the data has been collected to train the model. In this work, we\npropose to address these two challenges on the detection of clinically\nsignificant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the\nmethod proposed by (Kervadec et al., 2018), which introduces a size constaint\nloss to produce fine semantic cancer lesions segmentations from weak circle\nscribbles annotations. Performance of the model is based on two public (PI-CAI\nand Prostate158) and one private databases. First, we show that the model\nachieves on-par performance with strong fully supervised baseline models, both\non in-distribution validation data and unseen test images. Second, we observe a\nperformance decrease for both fully supervised and weakly supervised models\nwhen tested on unseen data domains. This confirms the crucial need for\nefficient domain adaptation methods if deep learning models are aimed to be\ndeployed in a clinical environment. Finally, we show that ensemble predictions\nfrom multiple trainings increase generalization performance.",
    "explanation":"n this work, we propose to address these two challenges\non the detection of clinically significant prostate cancer (csPCa) from bi-parametric MRI.\nWe evaluate the method proposed by (Kervadec et al., 2018), which introduces a size con-\nstaint loss to produce fine semantic cancer lesions segmentations from weak circle scribbles\nannotations",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b14"
    ],
    "c_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "c_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15384",
    "c_title":[
      "Quasi parton distributions of pions at large longitudinal momentum"
    ],
    "c_abstract":[
      "In this paper, we develop an approach to calculate the valence-quark quasi\nparton distribution amplitude (quasi-PDA) and quasi parton distribution\nfunction (quasi-PDF) for the pion with a large longitudinal momentum with the\nfunctional renormalization group (fRG). This is demonstrated in a low energy\neffective theory (LEFT) with four-quark scatterings. In the study of the\ncomplex structure of quasi-PDA, we introduce a deformed integration contour in\nthe calculations of quasi-PDA or quasi-PDF, which allows us to obtain correct\nintegrals for all momentum fractions. It is found that the pion light-front PDA\nextrapolated from quasi-PDA based on the large momentum effective theory\n(LaMET) in the LEFT is comparable with lattice QCD and Dyson-Schwinger\nequation. This work paves the way to study the PDA and PDF within the fRG\napproach to first-principles QCD."
    ],
    "c_categories":[
      "hep-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01671",
    "c_title":[
      "Diagnostic tools for exploring differences in distributional properties\n  between two samples: nonparametric approach"
    ],
    "c_abstract":[
      "This paper reconsiders the problem of testing the equality of two unspecified\ncontinuous distributions. The framework, which we propose, allows for readable\nand insightful data visualisation and helps to understand and quantify how two\ngroups of data differ. We consider a useful weighted rank empirical process on\n(0,1) and utilise a grid-based approach, based on diadic partitions of (0,1),\nto discretize the continuous process and construct local simultaneous\nacceptance regions. These regions help to identify statistically significant\ndeviations from the null model. In addition, the form of the process and its\ndicretization lead to a highly interpretable visualisation of distributional\ndifferences. We also introduce a new two-sample test, explicitly related to the\nvisualisation. Numerical studies show that the new test procedure performs very\nwell. We illustrate the use and diagnostic capabilities of our approach by an\napplication to a known set of neuroscience data."
    ],
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04390",
    "c_title":[
      "In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware\n  Knowledge Updates in LLMs"
    ],
    "c_abstract":[
      "Despite remarkable capabilities, large language models (LLMs) struggle to\ncontinually update their knowledge without catastrophic forgetting. In\ncontrast, humans effortlessly integrate new information, detect conflicts with\nexisting beliefs, and selectively update their mental models. This paper\nintroduces a cognitive-inspired investigation paradigm to study continual\nknowledge updating in LLMs. We implement two key components inspired by human\ncognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior\nto classify information as novel, familiar, or dissonant; and (2) Targeted\nNetwork Updates, which track neural activity to identify frequently used\n(stubborn) and rarely used (plastic) neurons. Through carefully designed\nexperiments in controlled settings, we uncover a number of empirical findings\ndemonstrating the potential of this approach. First, dissonance detection is\nfeasible using simple activation and gradient features, suggesting potential\nfor cognitive-inspired training. Second, we find that non-dissonant updates\nlargely preserve prior knowledge regardless of targeting strategy, revealing\ninherent robustness in LLM knowledge integration. Most critically, we discover\nthat dissonant updates prove catastrophically destructive to the model's\nknowledge base, indiscriminately affecting even information unrelated to the\ncurrent updates. This suggests fundamental limitations in how neural networks\nhandle contradictions and motivates the need for new approaches to knowledge\nupdating that better mirror human cognitive mechanisms."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.NC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01559",
    "c_title":[
      "Multiphysics simulations of microstructure influence on hysteresis and\n  eddy current losses of electrical steel"
    ],
    "c_abstract":[
      "Improving efficiency of electrical machines requires fundamental knowledge on\nthe mechanisms behind magnetic and eddy current losses of the magnetic core\nmaterials, with Fe-Si alloy as a prototype. These losses are intrinsically\ninfluenced by the microstructure of the materials. This necessitates\nphysics-based, microstructure-informed multiscale simulations. In the present\npaper, we utilised micromagnetic simulations and computational homogenization\nmethods to calculate the effective hysteresis and effective conductivities of\nFe-Si electrical steels. To demonstrate the methodology, binder-jet printed\nelectrical steel material samples with different microstructure were\ninvestigated. The microstructure samples were digitized based on both the\ndescriptor-based synthetic reconstruction and SEM-image-based digitization.\nMore samples were generated with varying microstructure features such as grain\nsize and grain boundary phases. The micromagnetic simulations were then\nperformed to investigate the magnetic hysteresis and hysteresis loss. The eddy\ncurrent loss was also evaluated by using the effective conductivity through\ncomputational homogenization. By performing parameter research on a series of\nsynthetic microstructures, effects of average grain size and grain boundary\n(GB) phase thickness on the hysteresis loss and eddy current loss were\nunveiled. An average grain size around 120 \\si{\\micro m} has the lowest\nhysteresis loss, although the eddy current loss increases with the grain size.\nIncreasing GB-phase thickness helps reduce both losses. Results indicate the\npotential to decrease loss of magnetic core materials by microstructure\noptimization."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "A review of artificial intelligence in prostate cancer detection on imaging"
    ],
    "b_abstract":[
      "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15859",
    "c_title":[
      "The Free Hamilton Algebra"
    ],
    "c_abstract":[
      "Over an arbitrary field $\\mathbb{F}$, let $p$ and $q$ be monic polynomials\nwith degree $2$ in $\\mathbb{F}[t]$. The free Hamilton algebra of the pair\n$(p,q)$ is the free noncommutative algebra in two generators $a$ and $b$\nsubject only to the relations $p(a)=0=q(b)$. Free Hamilton algebras are models\nof free products of two $2$-dimensional algebras over $\\mathbb{F}$. They can be\nviewed as the most elementary nontrivial noncommutative algebras over fields.\n  It has been recently observed that the free Hamilton algebra has surprising\nconnections with quaternion algebras. Here, we exploit these connections to\ninvestigate its zero divisors, group of units, maximal ideals,\nfinite-dimensional subalgebras, and its automorphism group."
    ],
    "c_categories":[
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04261",
    "c_title":[
      "VirtualXAI: A User-Centric Framework for Explainability Assessment\n  Leveraging GPT-Generated Personas"
    ],
    "c_abstract":[
      "In today's data-driven era, computational systems generate vast amounts of\ndata that drive the digital transformation of industries, where Artificial\nIntelligence (AI) plays a key role. Currently, the demand for eXplainable AI\n(XAI) has increased to enhance the interpretability, transparency, and\ntrustworthiness of AI models. However, evaluating XAI methods remains\nchallenging: existing evaluation frameworks typically focus on quantitative\nproperties such as fidelity, consistency, and stability without taking into\naccount qualitative characteristics such as satisfaction and interpretability.\nIn addition, practitioners face a lack of guidance in selecting appropriate\ndatasets, AI models, and XAI methods -a major hurdle in human-AI collaboration.\nTo address these gaps, we propose a framework that integrates quantitative\nbenchmarking with qualitative user assessments through virtual personas based\non the \"Anthology\" of backstories of the Large Language Model (LLM). Our\nframework also incorporates a content-based recommender system that leverages\ndataset-specific characteristics to match new input data with a repository of\nbenchmarked datasets. This yields an estimated XAI score and provides tailored\nrecommendations for both the optimal AI model and the XAI method for a given\nscenario."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.07540",
    "c_title":[
      "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol"
    ],
    "c_abstract":[
      "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09707",
    "c_title":[
      "The Goofus & Gallant Story Corpus for Practical Value Alignment"
    ],
    "c_abstract":[
      "Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.06561",
    "c_title":[
      "Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for\n  Mid-term Human Mobility Prediction"
    ],
    "c_abstract":[
      "Predicting individual mobility patterns is crucial across various\napplications. While current methods mainly focus on predicting the next\nlocation for personalized services like recommendations, they often fall short\nin supporting broader applications such as traffic management and epidemic\ncontrol, which require longer period forecasts of human mobility. This study\naddresses mid-term mobility prediction, aiming to capture daily travel patterns\nand forecast trajectories for the upcoming day or week. We propose a novel\nMulti-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to\nefficiently extract spatial and temporal information by decoupling daily\ntrajectories into distinct location-duration chains. Our approach employs a\nhierarchical encoder to model multi-scale temporal patterns, including daily\nrecurrence and weekly periodicity, and utilizes a transformer-based decoder to\nglobally attend to predicted information in the location or duration chain.\nAdditionally, we introduce a spatial heterogeneous graph learner to capture\nmulti-scale spatial relationships, enhancing semantic-rich representations.\nExtensive experiments, including statistical physics analysis, are conducted on\nlarge-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay\nArea, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to\nepidemic modeling in Boston, MSTDP significantly outperforms the\nbest-performing baseline, achieving a remarkable 62.8% reduction in MAE for\ncumulative new cases."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12917",
    "c_title":[
      "Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible"
    ],
    "c_abstract":[
      "The current Neuro-Symbolic (NeSy) Learning paradigm suffers from an\nover-reliance on labeled data. If we completely disregard labels, it leads to\nless symbol information, a larger solution space, and more shortcuts-issues\nthat current Nesy systems cannot resolve. This paper introduces a novel\nlearning paradigm, Verification Learning (VL), which addresses this challenge\nby transforming the label-based reasoning process in Nesy into a label-free\nverification process. VL achieves excellent learning results solely by relying\non unlabeled data and a function that verifies whether the current predictions\nconform to the rules. We formalize this problem as a Constraint Optimization\nProblem (COP) and propose a Dynamic combinatorial Sorting (DCS) algorithm that\naccelerates the solution by reducing verification attempts, effectively\nlowering computational costs to the level of a Constraint Satisfaction Problem\n(CSP). To further enhance performance, we introduce a prior alignment method to\naddress potential shortcuts. Our theoretical analysis points out which tasks in\nNesy systems can be completed without labels and explains why rules can replace\ninfinite labels, such as in addition, for some tasks, while for others, like\nSudoku, the rules have no effect. We validate the proposed framework through\nseveral fully unsupervised tasks including addition, sort, match, and chess,\neach showing significant performance and efficiency improvements."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.19359",
    "c_title":[
      "Mechanical Properties of the Meninges: Large Language Model Assisted\n  Systematic Review of over 25,000 Studies"
    ],
    "c_abstract":[
      "Accurate constitutive models and corresponding mechanical property values for\nthe meninges are important for predicting mechanical damage to brain tissue due\nto traumatic brain injury. The meninges are often oversimplified in current\nfinite element (FE) head models due to their complex anatomy and\nspatially-variant mechanical behavior. This study performed a systematic review\n(SR) on the mechanical properties of each individual layer of the meninges to\nobtain benchmark data for FE modeling and to identify gaps in the current\nliterature. Relevant studies were filtered through three stages: a broad\ninitial search filter, a large language model classifier, and manual\nverification by a human reviewer. Out of over 25,000 studies initially\nconsidered, this review ultimately included 47 studies on the dura mater, 8 on\nthe arachnoid mater, and 7 on the pia mater, representing the largest and most\ncomprehensive SR on the mechanical properties of the meninges. Each layer was\nfound to exhibit nonlinear rate dependence that varies with species, age,\nlocation, and orientation. This study revealed that the elastic modulus of pia\nmater most often used in simplified linear elastic FE models is likely\nunderestimated by an order of magnitude and fails to consider directional\ndependence. Future studies investigating the mechanical properties of the\nmeninges should focus on a wider range of loading rates as well as age effects\nfor the arachnoid mater and pia mater, as these features are relatively\nunderstudied and expected to affect the fidelity of FE predictions."
    ],
    "c_categories":[
      "cond-mat.soft",
      "physics.med-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10495",
    "c_title":[
      "Cohomology and deformations of nonabelian embedding tensors between Lie\n  triple systems"
    ],
    "c_abstract":[
      "In this paper, first we introduce the notion of nonabelian embedding tensors\nbetween Lie triple systems and show that nonabelian embedding tensors induce\nnaturally 3-Leibniz algebras. Next, we construct an $L_{\\infty}$-algebra whose\nMaurer-Cartan elements are nonabelian embedding tensors. Then, we have the\ntwisted $L_{\\infty}$-algebra that governs deformations of nonabelian embedding\ntensors. Following this, we establish the cohomology of a nonabelian embedding\ntensor between Lie triple systems and realize it as the cohomology of the\ndescendent 3-Leibniz algebra with coefficients in a suitable representation. As\napplications, we consider infinitesimal deformations of a nonabelian embedding\ntensor between Lie triple systems and demonstrate that they are governed by the\nabove-established cohomology. Furthermore, the notion of Nijenhuis elements\nassociated with a nonabelian embedding tensor is introduced to characterize\ntrivial infinitesimal deformations. Finally, we provide relationships between\nnonabelian embedding tensors on Lie algebras and associated Lie triple systems."
    ],
    "c_categories":[
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00867",
    "c_title":[
      "Partitions of an Eulerian Digraph into Circuits"
    ],
    "c_abstract":[
      "We investigate a cancellation property satisfied by a connected Eulerian\ndigraph $D$. Namely, unless $D$ is a single directed cycle, we have\n$\\sum_{k\\geq 1} (-1)^{k} f_k(D)=0$, where $f_k(D)$ is the number of partitions\nof Eulerian circuits of $D$ into $k$ circuits. This property is a consequence\nof the fact that the Martin polynomial of a digraph has no constant term. We\nprovide an alternative proof by employing Viennot's theory of Heaps of Pieces,\nand in particular, a bijection between closed trails of a digraph and heaps\nwith a unique maximal piece, which are also in bijection with unique sink\norientations of the intersection graphs $G_a$ of partitions $a$ of $E(D)$ into\ncycles. The argument considers the partition lattice of the edge set of a\ndigraph $D$, restricted to the join-semilattice $T(D)$ induced by elements\nwhose blocks are connected and Eulerian. The minimal elements of $T(D)$ are\nexactly the partitions of $D$ into cycles, and the up-set of a minimal element\n$a\\in T(D)$ is shown to be isomorphic to the bond lattice $L(G_a)$. Using tools\ndeveloped by Whitney and Rota, we perform M\\\"{o}bius inversion on $T(D)$ and\nobtain the claimed cancellation.\n  As a consequence of this alternative proof, we relate the Martin polynomial\nof a digraph directly to the chromatic polynomials of the intersection graphs\nof partitions of $D$ into cycles. Finally, we apply the cancellation property\nin order to deduce the classical Harary-Sachs Theorem for graphs of rank $2$\nfrom a hypergraph generalization thereof, remedying a gap in a previous proof\nof this."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.20424",
    "c_title":[
      "Theoretical study of Th III energy levels and transitions for\n  applications to kilonova spectra"
    ],
    "c_abstract":[
      "The neutron star merger is a promising site of heavy element production. By\nproducing heavy elements, the neutron star merger gives rise to a thermal\ntransient called a kilonova. Studying kilonova spectra enables us to quantify\nthe heavy element production. Among the heaviest elements, doubly ionized\nThorium (Th, Z=90) is one of the important candidates for producing detectable\nabsorption features in kilonova spectra. This paper investigates the atomic\nproperties of Th III to provide energy level and transition data. The\nmulticonfiguration Dirac-Hartree-Fock and relativistic configuration\ninteraction methods, which are implemented in the general-purpose relativistic\natomic structure package GRASP2018, are used to compute energy levels of the\n$\\mathrm{5f6d}$, $\\mathrm{6d^2}$, $\\mathrm{7s^2}$, $5\\mathrm{f^2}$,\n$\\mathrm{6d7s}$, $\\mathrm{5f7p}$ and $\\mathrm{5f7s}$ configurations and\nelectric dipole transitions between states of these configurations. The\naccuracy of energy levels is evaluated by comparing it with experimental data\nand with various theoretical methods. Our calculated energy levels are\nconsistent with the experimental results with a root mean square (RMS)\ndeviation of 436 cm$^{-1}$. The accuracy of transition data is investigated\nusing the quantitative and qualitative evaluation method.By performing\nradiative transfer simulations for kilonova spectra with our transition data,\nwe show that kilonova including Th with a mass fraction of $(3-10) \\times\n10^{-5}$ can produce Th III absorption features around 18,000 A."
    ],
    "c_categories":[
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02466",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
    ],
    "b_abstract":[
      "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05016",
    "c_title":[
      "Self-consistent Solutions of Evolving Nuclear Star Clusters with\n  Two-Dimensional Monte-Carlo Dynamical Simulations"
    ],
    "c_abstract":[
      "We recently developed a Monte-Carlo method (GNC) that can simulate the\ndynamical evolution of a nuclear stellar cluster (NSC) with a massive black\nhole (MBH), where the two-body relaxations can be solved by the Fokker-Planck\nequations in energy and angular momentum space. Here we make a major update of\nGNC~ by integrating stellar potential and adiabatic invariant theory, so that\nwe can study the self-consistent dynamics of NSCs with increasing mass of the\nMBH. We perform tests of the self-adaptation of cluster density due to MBH mass\ngrowth and Plummer core collapse, both finding consistent results with previous\nstudies, the latter having a core collapse time of $\\sim 17t_{\\rm rh}$ by GNC,\nwhere $t_{\\rm rh}$ is the time of half-mass relaxation. We use GNC~ to study\nthe cosmological evolution of the properties of NSC and the mass of MBH\nassuming that the mass growth of the MBH is due to loss-cone accretion of stars\n(e.g., tidal disruption of stars) and stellar black holes, and compare the\nsimulation results with the observations of NSCs in Milky-Way or near-by\ngalaxies. Such scenario is possible to produce MBHs with mass $10^5\\sim\n10^7\\,M_\\odot$ for NSCs with stellar mass of $10^6\\sim 10^9\\,M_\\odot$. In\nMilky-Way's NSC, to grow MBH up to $4\\times 10^6\\,M_\\odot$, its size needs to\nbe $\\sim 1.7$ times more compact in early universe than the current value. MBHs\nwith current masses $>6\\times 10^{7}\\,M_\\odot$ seem difficult to explain by\nloss-cone accretion alone, and thus may require other additional accretion\nchannels, such as gas accretion."
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"Divergent Domains, Convergent Grading: Enhancing Generalization in\n  Diabetic Retinopathy Grading",
    "a_abstract":"Diabetic Retinopathy (DR) constitutes 5% of global blindness cases. While\nnumerous deep learning approaches have sought to enhance traditional DR grading\nmethods, they often falter when confronted with new out-of-distribution data\nthereby impeding their widespread application. In this study, we introduce a\nnovel deep learning method for achieving domain generalization (DG) in DR\ngrading and make the following contributions. First, we propose a new way of\ngenerating image-to-image diagnostically relevant fundus augmentations\nconditioned on the grade of the original fundus image. These augmentations are\ntailored to emulate the types of shifts in DR datasets thus increase the\nmodel's robustness. Second, we address the limitations of the standard\nclassification loss in DG for DR fundus datasets by proposing a new DG-specific\nloss, domain alignment loss; which ensures that the feature vectors from all\ndomains corresponding to the same class converge onto the same manifold for\nbetter domain generalization. Third, we tackle the coupled problem of data\nimbalance across DR domains and classes by proposing to employ Focal loss which\nseamlessly integrates with our new alignment loss. Fourth, due to inevitable\nobserver variability in DR diagnosis that induces label noise, we propose\nleveraging self-supervised pretraining. This approach ensures that our DG model\nremains robust against early susceptibility to label noise, even when only a\nlimited dataset of non-DR fundus images is available for pretraining. Our\nmethod demonstrates significant improvements over the strong Empirical Risk\nMinimization baseline and other recently proposed state-of-the-art DG methods\nfor DR grading. Code is available at https:\/\/github.com\/sharonchokuwa\/dg-adr.",
    "explanation":"In this study, we introduce a novel deep learning\nmethod for achieving domain generalization (DG) in DR\ngrading and make the following contributions.",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b7"
    ],
    "c_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "c_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04686",
    "c_title":[
      "Learning Strategic Language Agents in the Werewolf Game with Iterative\n  Latent Space Policy Optimization"
    ],
    "c_abstract":[
      "Large language model (LLM)-based agents have recently shown impressive\nprogress in a variety of domains, including open-ended conversation and\nmulti-step decision-making. However, applying these agents to social deduction\ngames such as Werewolf, which requires both strategic decision-making and\nfree-form language interaction, remains non-trivial. Traditional methods based\non Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)\ntypically depend on a predefined action space, making them unsuitable for\nlanguage games with unconstrained text action space. Meanwhile, pure LLM-based\nagents often suffer from intrinsic biases and require prohibitively large\ndatasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),\nan iterative framework that addresses these challenges by first mapping\nfree-form text to a discrete latent space, where methods like CFR and RL can\nlearn strategic policy more effectively. We then translate the learned policy\nback into natural language dialogues, which are used to fine-tune an LLM via\nDirect Preference Optimization (DPO). By iteratively alternating between these\nstages, our LSPO agent progressively enhances both strategic reasoning and\nlanguage communication. Experiment results on the Werewolf game show that our\nmethod improves the agent's performance in each iteration and outperforms\nexisting Werewolf agents, underscoring its promise for free-form language\ndecision-making."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09545",
    "c_title":[
      "The Value of Goal Commitment in Planning"
    ],
    "c_abstract":[
      "In this paper, we revisit the concept of goal commitment from early planners\nin the presence of current forward chaining heuristic planners. We present a\ncompilation that extends the original planning task with commit actions that\nenforce the persistence of specific goals once achieved, thereby committing to\nthem in the search sub-tree. This approach imposes a specific goal achievement\norder in parts of the search tree, potentially introducing dead-end states.\nThis can reduce search effort if the goal achievement order is correct.\nOtherwise, the search algorithm can expand nodes in the open list where goals\ndo not persist. Experimental results demonstrate that the reformulated tasks\nsuit state-of-the-art agile planners, enabling them to find better"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07709",
    "c_title":[
      "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
    ],
    "c_abstract":[
      "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.03791",
    "c_title":[
      "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue"
    ],
    "c_abstract":[
      "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.13622",
    "c_title":[
      "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning"
    ],
    "c_abstract":[
      "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.19359",
    "c_title":[
      "Mechanical Properties of the Meninges: Large Language Model Assisted\n  Systematic Review of over 25,000 Studies"
    ],
    "c_abstract":[
      "Accurate constitutive models and corresponding mechanical property values for\nthe meninges are important for predicting mechanical damage to brain tissue due\nto traumatic brain injury. The meninges are often oversimplified in current\nfinite element (FE) head models due to their complex anatomy and\nspatially-variant mechanical behavior. This study performed a systematic review\n(SR) on the mechanical properties of each individual layer of the meninges to\nobtain benchmark data for FE modeling and to identify gaps in the current\nliterature. Relevant studies were filtered through three stages: a broad\ninitial search filter, a large language model classifier, and manual\nverification by a human reviewer. Out of over 25,000 studies initially\nconsidered, this review ultimately included 47 studies on the dura mater, 8 on\nthe arachnoid mater, and 7 on the pia mater, representing the largest and most\ncomprehensive SR on the mechanical properties of the meninges. Each layer was\nfound to exhibit nonlinear rate dependence that varies with species, age,\nlocation, and orientation. This study revealed that the elastic modulus of pia\nmater most often used in simplified linear elastic FE models is likely\nunderestimated by an order of magnitude and fails to consider directional\ndependence. Future studies investigating the mechanical properties of the\nmeninges should focus on a wider range of loading rates as well as age effects\nfor the arachnoid mater and pia mater, as these features are relatively\nunderstudied and expected to affect the fidelity of FE predictions."
    ],
    "c_categories":[
      "cond-mat.soft",
      "physics.med-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10495",
    "c_title":[
      "Cohomology and deformations of nonabelian embedding tensors between Lie\n  triple systems"
    ],
    "c_abstract":[
      "In this paper, first we introduce the notion of nonabelian embedding tensors\nbetween Lie triple systems and show that nonabelian embedding tensors induce\nnaturally 3-Leibniz algebras. Next, we construct an $L_{\\infty}$-algebra whose\nMaurer-Cartan elements are nonabelian embedding tensors. Then, we have the\ntwisted $L_{\\infty}$-algebra that governs deformations of nonabelian embedding\ntensors. Following this, we establish the cohomology of a nonabelian embedding\ntensor between Lie triple systems and realize it as the cohomology of the\ndescendent 3-Leibniz algebra with coefficients in a suitable representation. As\napplications, we consider infinitesimal deformations of a nonabelian embedding\ntensor between Lie triple systems and demonstrate that they are governed by the\nabove-established cohomology. Furthermore, the notion of Nijenhuis elements\nassociated with a nonabelian embedding tensor is introduced to characterize\ntrivial infinitesimal deformations. Finally, we provide relationships between\nnonabelian embedding tensors on Lie algebras and associated Lie triple systems."
    ],
    "c_categories":[
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00867",
    "c_title":[
      "Partitions of an Eulerian Digraph into Circuits"
    ],
    "c_abstract":[
      "We investigate a cancellation property satisfied by a connected Eulerian\ndigraph $D$. Namely, unless $D$ is a single directed cycle, we have\n$\\sum_{k\\geq 1} (-1)^{k} f_k(D)=0$, where $f_k(D)$ is the number of partitions\nof Eulerian circuits of $D$ into $k$ circuits. This property is a consequence\nof the fact that the Martin polynomial of a digraph has no constant term. We\nprovide an alternative proof by employing Viennot's theory of Heaps of Pieces,\nand in particular, a bijection between closed trails of a digraph and heaps\nwith a unique maximal piece, which are also in bijection with unique sink\norientations of the intersection graphs $G_a$ of partitions $a$ of $E(D)$ into\ncycles. The argument considers the partition lattice of the edge set of a\ndigraph $D$, restricted to the join-semilattice $T(D)$ induced by elements\nwhose blocks are connected and Eulerian. The minimal elements of $T(D)$ are\nexactly the partitions of $D$ into cycles, and the up-set of a minimal element\n$a\\in T(D)$ is shown to be isomorphic to the bond lattice $L(G_a)$. Using tools\ndeveloped by Whitney and Rota, we perform M\\\"{o}bius inversion on $T(D)$ and\nobtain the claimed cancellation.\n  As a consequence of this alternative proof, we relate the Martin polynomial\nof a digraph directly to the chromatic polynomials of the intersection graphs\nof partitions of $D$ into cycles. Finally, we apply the cancellation property\nin order to deduce the classical Harary-Sachs Theorem for graphs of rank $2$\nfrom a hypergraph generalization thereof, remedying a gap in a previous proof\nof this."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.20424",
    "c_title":[
      "Theoretical study of Th III energy levels and transitions for\n  applications to kilonova spectra"
    ],
    "c_abstract":[
      "The neutron star merger is a promising site of heavy element production. By\nproducing heavy elements, the neutron star merger gives rise to a thermal\ntransient called a kilonova. Studying kilonova spectra enables us to quantify\nthe heavy element production. Among the heaviest elements, doubly ionized\nThorium (Th, Z=90) is one of the important candidates for producing detectable\nabsorption features in kilonova spectra. This paper investigates the atomic\nproperties of Th III to provide energy level and transition data. The\nmulticonfiguration Dirac-Hartree-Fock and relativistic configuration\ninteraction methods, which are implemented in the general-purpose relativistic\natomic structure package GRASP2018, are used to compute energy levels of the\n$\\mathrm{5f6d}$, $\\mathrm{6d^2}$, $\\mathrm{7s^2}$, $5\\mathrm{f^2}$,\n$\\mathrm{6d7s}$, $\\mathrm{5f7p}$ and $\\mathrm{5f7s}$ configurations and\nelectric dipole transitions between states of these configurations. The\naccuracy of energy levels is evaluated by comparing it with experimental data\nand with various theoretical methods. Our calculated energy levels are\nconsistent with the experimental results with a root mean square (RMS)\ndeviation of 436 cm$^{-1}$. The accuracy of transition data is investigated\nusing the quantitative and qualitative evaluation method.By performing\nradiative transfer simulations for kilonova spectra with our transition data,\nwe show that kilonova including Th with a mass fraction of $(3-10) \\times\n10^{-5}$ can produce Th III absorption features around 18,000 A."
    ],
    "c_categories":[
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Medical diffusion on a budget: textual inversion for medical image generation"
    ],
    "b_abstract":[
      "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
    ],
    "b_categories":[
      "q-bio.OT"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05016",
    "c_title":[
      "Self-consistent Solutions of Evolving Nuclear Star Clusters with\n  Two-Dimensional Monte-Carlo Dynamical Simulations"
    ],
    "c_abstract":[
      "We recently developed a Monte-Carlo method (GNC) that can simulate the\ndynamical evolution of a nuclear stellar cluster (NSC) with a massive black\nhole (MBH), where the two-body relaxations can be solved by the Fokker-Planck\nequations in energy and angular momentum space. Here we make a major update of\nGNC~ by integrating stellar potential and adiabatic invariant theory, so that\nwe can study the self-consistent dynamics of NSCs with increasing mass of the\nMBH. We perform tests of the self-adaptation of cluster density due to MBH mass\ngrowth and Plummer core collapse, both finding consistent results with previous\nstudies, the latter having a core collapse time of $\\sim 17t_{\\rm rh}$ by GNC,\nwhere $t_{\\rm rh}$ is the time of half-mass relaxation. We use GNC~ to study\nthe cosmological evolution of the properties of NSC and the mass of MBH\nassuming that the mass growth of the MBH is due to loss-cone accretion of stars\n(e.g., tidal disruption of stars) and stellar black holes, and compare the\nsimulation results with the observations of NSCs in Milky-Way or near-by\ngalaxies. Such scenario is possible to produce MBHs with mass $10^5\\sim\n10^7\\,M_\\odot$ for NSCs with stellar mass of $10^6\\sim 10^9\\,M_\\odot$. In\nMilky-Way's NSC, to grow MBH up to $4\\times 10^6\\,M_\\odot$, its size needs to\nbe $\\sim 1.7$ times more compact in early universe than the current value. MBHs\nwith current masses $>6\\times 10^{7}\\,M_\\odot$ seem difficult to explain by\nloss-cone accretion alone, and thus may require other additional accretion\nchannels, such as gas accretion."
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.14929",
    "c_title":[
      "Cytogenetic, Hematobiochemical, and Histopathological Assessment of\n  Albino Rats (Rattus norvegicus) Fed on Gluten Extracts"
    ],
    "c_abstract":[
      "Background: Literature shows that most of the information on the toxicity of\ngluten is generated from survey and observational studies, resulting in\ninconsistent outcomes and a decrease in the acceptability of gluten-rich foods.\nTo determine gluten's safety, an in-depth in vitro and in vivo toxicological\nexamination is required. This enables scientists to come up with ameliorative\nstrategies if it turns out to have side effects, and consumers' trust can be\nrestored. Objectives: The objective of this study was to assess the toxicity of\ngluten extracts on albino rats (Rattus norvegicus). Materials and Methods:\nTwenty-four rats were randomly selected and divided into four groups, each\ncomprising six rats. Group 1 (control) rats were fed on pellet feeds and groups\n2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts,\nrespectively. The rats' body weights and reactions were observed for 90 days\nbefore blood samples were collected for hematobiochemical and micronucleus\ntests. Histopathological examinations of the liver and kidneys were also\nperformed. Results: There was no difference (P > 0.05) in body weight, blood\nglucose level, or micronuclei between the control and treated rats. The\nlymphocytes, alkaline phosphatase, alanine transaminase, total protein, and\ncalcium ions of the test rats were all significantly (P < 0.05) altered but\nremained within the normal ranges. Other hematobiochemical parameters,\nincluding packed cell volume, hemoglobin, white and red blood cells, aspartate\ntransaminase, albumin, sodium ions, potassium ions, chloride ions, and urea,\nrevealed no marked changes. The treated rats' livers and kidneys showed no\nhistopathological changes. Conclusion: Gluten had no adverse effects. However,\nit altered hematobiochemical parameters, particularly the lymphocytes, alkaline\nphosphatase, alanine transaminase, total protein, and calcium ions."
    ],
    "c_categories":[
      "q-bio.OT"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.14044",
    "c_title":[
      "Leveraging 13C NMR spectroscopic data derived from SMILES to predict the\n  functionality of small biomolecules by machine learning: a case study on\n  human Dopamine D1 receptor antagonists"
    ],
    "c_abstract":[
      "This study contributes to ongoing research which aims to predict small\nbiomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C\nNMR) spectrum data and machine learning (ML). The approach was demonstrated\nusing a bioassay on human dopamine D1 receptor antagonists. The Simplified\nMolecular Input Line Entry System (SMILES) notations of compounds in this\nbioassay were extracted and converted into spectroscopic data by software\ndesigned for this purpose. The resulting data was then used for ML with\nscikit-learn algorithms. The ML models were trained by 27,756 samples and\ntested by 5,466. From the estimators K-Nearest neighbor, Decision Tree\nClassifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost\nClassifier, and Support Vector Classifier, the last performed the best,\nachieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,\nand 0.749 cross-validation score with 0.005 standard deviation. The methodology\ncan be applied to predict any functionality of any compound when relevant data\nare available. It was hypothesized also that increasing the number of samples\nwould increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML\nmodel, the time- , and cost-efficient CID_SID ML model was developed. This\nmodel allows researchers who have developed a compound and obtained its PubChem\nCID and SID to check whether their compound is also a human dopamine D1\nreceptor antagonist based solely on the PubChem identifiers. The metrics of the\nCID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,\n79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard\ndeviation."
    ],
    "c_categories":[
      "q-bio.OT"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.20602",
    "c_title":[
      "Evidence for strong modality-dependence of chronotype assessment from\n  real world calendar app data"
    ],
    "c_abstract":[
      "Chronotypes allow for comparisons of one individual's daily rhythms to that\nof others and the environment. Mismatch between an individual's chronotype and\nthe timing constraints of their social environment create social jet lag, which\nis correlated with mental and physical health risks. The concept of chronotype\nimplicitly supposes that a single phase applies to an individual, whereas the\ncircadian rhythms of different internal systems entrain to or have their\noutputs masked by different environmental inputs. If the modern environment\ninterferes with internal synchrony or generates different masking for different\ninternal system's outputs, then real world data reflecting these outputs ought\nto reveal relatively low correlations, reflecting environmental interference.\nAt the other extreme, if there is no behavior or tissue specific interference,\nthen different internal system outputs should all be equally predicted from\ndata modalities capturing their different outputs. Here we explore multimodal\nbehavioral rhythm data from the Owaves calendaring app, focusing on behavioral\noutputs logged as: Sleep, Exercise, Eat, Work, Love, Play, Relax, Misc. We find\nthat individuals show daily rhythms within each behavior type from which\nchronotypes can be assigned, but that chronotypes derived from different\nbehaviors (or combinations of behaviors) lead to nearly independent sorting of\nindividual's by phase. This suggests that if real world data are used to assign\nan individuals chronotype, then that chronotype may be specific to the internal\nsystem the output of which is related to each specific data modality assessed.\nOur findings suggest that researchers cannot confidently assume to account for\noutputs from other internal systems in real world settings without additional\nobservations or controls."
    ],
    "c_categories":[
      "q-bio.OT"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03269",
    "c_title":[
      "Modeling and Optimization of Insulin Injection for Type-1 Diabetes\n  Mellitus Management"
    ],
    "c_abstract":[
      "Diabetes mellitus is a global health crisis characterized by poor blood sugar\nregulation, impacting millions of people worldwide and leading to severe\ncomplications and mortality. Although Type 1 Diabetes Mellitus (T1DM) has a\nlower number of cases compared to other forms of diabetes, it is often\ndiagnosed at a young age and requires lifelong exogenous insulin\nadministration. In this paper, we focus on understanding the interaction of\ninsulin and glucose molecules within the subcutaneous layer, which is crucial\nfor blood sugar control in T1DM patients. Specifically, we propose a\ncomprehensive model to characterize the insulin-glucose system within the\nsubcutaneous layer, incorporating a multicellular molecular communication\nsystem. We then divide the T1DM system into insulin and glucose subsystems and\nderive the end-to-end expression for insulin-glucose interaction in the\nsubcutaneous layer. We further validate the insulin-glucose interaction\nanalysis with an agent-based simulator. As effectively managing postprandial\nglucose levels is crucial for individuals with T1DM to safeguard their overall\nhealth and avert short-term and long-term complications, we also derive the\noptimal insulin administration time based on the derived glucose response via\nthe Lagrange multiplier and gradient descent ascent method. This allows us to\nexplore the impact of different types of insulin and dietary management on\nblood sugar levels. Simulation results confirm the correctness of our proposed\nmodel and the effectiveness of our optimized effective time window for\ninjecting insulin in individuals with T1DM."
    ],
    "c_categories":[
      "q-bio.OT"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.09392",
    "c_title":[
      "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling"
    ],
    "c_abstract":[
      "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes."
    ],
    "c_categories":[
      "q-bio.OT"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17508",
    "c_title":[
      "The growth of super-large pre-planetary pebbles to an impact erosion\n  limit"
    ],
    "c_abstract":[
      "Early dust evolution in protoplanetary disks is dominated by sticking\ncollisions. However, this initial phase of particle growth faces constraints -\nnotably from destructive encounters. To find the maximum particle size\nachievable, we studied collisional processes during a prolonged microgravity\nexperiment aboard a suborbital flight. Here, we specifically report an impact\nerosion limit. We observed individual basalt beads, each measuring 0.5 mm in\ndiameter, colliding with and either eroding or adhering to a cluster several\ncentimeters in size. This cluster, formed from tribocharged particles,\nsimulates an electrostatic growth phase that surpasses the classical bouncing\nbarrier. We find a threshold velocity of about 0.5 m\/s, distinguishing between\nadditive and erosive impacts of individual beads. Numerical simulations of\ngrain impacts into clusters, testing both low and high charge constituents\ncorroborate the experimental findings of surface erosion within the observed\nvelocity range. This specific velocity threshold suggests the potential\nformation of pebbles several centimeters in size within protoplanetary disks.\nSuch dimensions place these pebbles well into a regime where hydrodynamic\ninteraction might facilitate the formation of planetesimals."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16568",
    "c_title":[
      "Quantum lattice Boltzmann method for simulating nonlinear fluid dynamics"
    ],
    "c_abstract":[
      "Quantum computing holds great promise to accelerate scientific computations\nin fluid dynamics and other classical physical systems. While various quantum\nalgorithms have been proposed for linear flows, developing quantum algorithms\nfor nonlinear problems remains a significant challenge. We introduce a novel\nnode-level ensemble description of lattice gas for simulating nonlinear fluid\ndynamics on a quantum computer. This approach combines the advantages of the\nlattice Boltzmann method, which offers low-dimensional representation, and\nlattice gas cellular automata, which provide linear collision treatment.\nBuilding on this framework, we propose a quantum lattice Boltzmann method that\nrelies on linear operations with medium dimensionality. We validated the\nalgorithm through comprehensive simulations of benchmark cases, including\nvortex-pair merging and decaying turbulence on $2048^2$ computational grid\npoints. The results demonstrate remarkable agreement with direct numerical\nsimulation, effectively capturing the essential nonlinear mechanisms of fluid\ndynamics. This work offers valuable insights into developing quantum algorithms\nfor other nonlinear problems, and potentially advances the application of\nquantum computing across various transport phenomena in engineering."
    ],
    "c_categories":[
      "nlin.CG",
      "physics.comp-ph",
      "physics.flu-dyn",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09188",
    "c_title":[
      "Ultrafast Photoexcitation of Semiconducting Photocathode Materials"
    ],
    "c_abstract":[
      "Cs-based semiconductors like $\\mathrm{Cs_3Sb}$ and $\\mathrm{Cs_2Te}$ are\ncurrently used as photocathodes in particle accelerators. Their performance as\nelectron sources critically depends on their interaction with intense laser\nsources. In this work, we investigate from first principles the time-dependent\nresponse of $\\mathrm{Cs_3Sb}$ and $\\mathrm{Cs_2Te}$ to ultrafast pulses of\nvarying intensities, ranging from $1~\\mathrm{GW\/cm^2}$ to $1~\\mathrm{PW\/cm^2}$.\nNonlinear effects, including high harmonic generation, emerge starting from\n$100~\\mathrm{GW\/cm^2}$ in $\\mathrm{Cs_3Sb}$ and $200~\\mathrm{GW\/cm^2}$ in\n$\\mathrm{Cs_2Te}$. Above these intensities, the numbers of absorbed photons and\nexcited electrons saturate due to the depletion of one-photon absorption\nchannels, with renewed increases beyond $1~\\mathrm{TW\/cm^2}$ for\n$\\mathrm{Cs_3Sb}$ and $5~\\mathrm{TW\/cm^2}$ for $\\mathrm{Cs_2Te}$ where\nmulti-photon absorption appears. Finally, the analysis of the occupation\ndensity in $\\mathrm{Cs_3Sb}$ reveals the onset of tunnel ionization at\nintensities above $10~\\mathrm{TW\/cm^2}$. Our findings provide new insights into\nthe nonlinear optical properties of $\\mathrm{Cs_3Sb}$ and $\\mathrm{Cs_2Te}$,\ncontributing to the optimization of these materials for the development of\nnext-generation photoinjectors."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11336",
    "c_title":[
      "Probing invisible neutrino decay with the first six detection units of\n  KM3NeT\/ORCA"
    ],
    "c_abstract":[
      "In the era of precision measurements of neutrino oscillation parameters, it\nis necessary for experiments to disentangle discrepancies that may indicate\nphysics beyond the Standard Model in the neutrino sector. KM3NeT\/ORCA is a\nwater Cherenkov neutrino detector under construction and anchored at the bottom\nof the Mediterranean Sea. The detector is designed to study the oscillations of\natmospheric neutrinos and determine the neutrino mass ordering. This paper\nfocuses on the initial configuration of ORCA, referred to as ORCA6, which\ncomprises six out of the foreseen 115 detection units of photosensors. A\nhigh-purity neutrino sample was extracted during 2020 and 2021, corresponding\nto an exposure of 433 kton-years. This sample is analysed following a binned\nlog-likelihood approach to search for invisible neutrino decay, in a\nthree-flavour neutrino oscillation scenario, where the third neutrino mass\nstate $\\nu_3$ decays into an invisible state, e.g. a sterile neutrino. The\nresulting best fit of the invisible neutrino decay parameter is $\\alpha_3 =\n0.92^{+1.08}_{-0.57}\\times 10^{-4}~\\mathrm{eV^2}$, corresponding to a scenario\nwith $\\theta_{23}$ in the second octant and normal neutrino mass ordering. The\nresults are consistent with the Standard Model, within a $2.1\\,\\sigma$\ninterval."
    ],
    "c_categories":[
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02614",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
    ],
    "b_abstract":[
      "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15354",
    "c_title":[
      "Eigenfunctions with double exponential rate of localization"
    ],
    "c_abstract":[
      "We construct a real-valued solution to the eigenvalue problem\n$-\\text{div}(A\\nabla u)=\\lambda u$, $\\lambda>0,$ in the cylinder\n$\\mathbb{T}^2\\times \\mathbb{R}$ with a real, uniformly elliptic, and uniformly\n$C^1$ matrix $A$ such that $|u(x,y,t)|\\leq C e^{-c e^{c|t|}}$ for some $c,C>0$.\nWe also construct a complex-valued solution to the heat equation $u_t=\\Delta u\n+ B \\nabla u$ in a half-cylinder with continuous and uniformly bounded $B$,\nwhich also decays with double exponential speed. Related classical ideas, used\nin the construction of counterexamples to the unique continuation by Plis and\nMiller, are reviewed."
    ],
    "c_categories":[
      "math.AP",
      "math.CA",
      "math.SP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"Multi-Task Learning for Integrated Automated Contouring and Voxel-Based\n  Dose Prediction in Radiotherapy",
    "a_abstract":"Deep learning-based automated contouring and treatment planning has been\nproven to improve the efficiency and accuracy of radiotherapy. However,\nconventional radiotherapy treatment planning process has the automated\ncontouring and treatment planning as separate tasks. Moreover in deep learning\n(DL), the contouring and dose prediction tasks for automated treatment planning\nare done independently. In this study, we applied the multi-task learning (MTL)\napproach in order to seamlessly integrate automated contouring and voxel-based\ndose prediction tasks, as MTL can leverage common information between the two\ntasks and be able able to increase the efficiency of the automated tasks. We\ndeveloped our MTL framework using the two datasets: in-house prostate cancer\ndataset and the publicly available head and neck cancer dataset, OpenKBP.\nCompared to the sequential DL contouring and treatment planning tasks, our\nproposed method using MTL improved the mean absolute difference of dose volume\nhistogram metrics of prostate and head and neck sites by 19.82% and 16.33%,\nrespectively. Our MTL model for automated contouring and dose prediction tasks\ndemonstrated enhanced dose prediction performance while maintaining or\nsometimes even improving the contouring accuracy. Compared to the baseline\nautomated contouring model with the dice score coefficients of 0.818 for\nprostate and 0.674 for head and neck datasets, our MTL approach achieved\naverage scores of 0.824 and 0.716 for these datasets, respectively. Our study\nhighlights the potential of the proposed automated contouring and planning\nusing MTL to support the development of efficient and accurate automated\ntreatment planning for radiotherapy.",
    "explanation":"Deep learning-based automated contouring and treatment planning has\nbeen proven to improve the efficiency and accuracy of radiotherapy.",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b0"
    ],
    "c_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "c_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17508",
    "c_title":[
      "The growth of super-large pre-planetary pebbles to an impact erosion\n  limit"
    ],
    "c_abstract":[
      "Early dust evolution in protoplanetary disks is dominated by sticking\ncollisions. However, this initial phase of particle growth faces constraints -\nnotably from destructive encounters. To find the maximum particle size\nachievable, we studied collisional processes during a prolonged microgravity\nexperiment aboard a suborbital flight. Here, we specifically report an impact\nerosion limit. We observed individual basalt beads, each measuring 0.5 mm in\ndiameter, colliding with and either eroding or adhering to a cluster several\ncentimeters in size. This cluster, formed from tribocharged particles,\nsimulates an electrostatic growth phase that surpasses the classical bouncing\nbarrier. We find a threshold velocity of about 0.5 m\/s, distinguishing between\nadditive and erosive impacts of individual beads. Numerical simulations of\ngrain impacts into clusters, testing both low and high charge constituents\ncorroborate the experimental findings of surface erosion within the observed\nvelocity range. This specific velocity threshold suggests the potential\nformation of pebbles several centimeters in size within protoplanetary disks.\nSuch dimensions place these pebbles well into a regime where hydrodynamic\ninteraction might facilitate the formation of planetesimals."
    ],
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16568",
    "c_title":[
      "Quantum lattice Boltzmann method for simulating nonlinear fluid dynamics"
    ],
    "c_abstract":[
      "Quantum computing holds great promise to accelerate scientific computations\nin fluid dynamics and other classical physical systems. While various quantum\nalgorithms have been proposed for linear flows, developing quantum algorithms\nfor nonlinear problems remains a significant challenge. We introduce a novel\nnode-level ensemble description of lattice gas for simulating nonlinear fluid\ndynamics on a quantum computer. This approach combines the advantages of the\nlattice Boltzmann method, which offers low-dimensional representation, and\nlattice gas cellular automata, which provide linear collision treatment.\nBuilding on this framework, we propose a quantum lattice Boltzmann method that\nrelies on linear operations with medium dimensionality. We validated the\nalgorithm through comprehensive simulations of benchmark cases, including\nvortex-pair merging and decaying turbulence on $2048^2$ computational grid\npoints. The results demonstrate remarkable agreement with direct numerical\nsimulation, effectively capturing the essential nonlinear mechanisms of fluid\ndynamics. This work offers valuable insights into developing quantum algorithms\nfor other nonlinear problems, and potentially advances the application of\nquantum computing across various transport phenomena in engineering."
    ],
    "c_categories":[
      "nlin.CG",
      "physics.comp-ph",
      "physics.flu-dyn",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09188",
    "c_title":[
      "Ultrafast Photoexcitation of Semiconducting Photocathode Materials"
    ],
    "c_abstract":[
      "Cs-based semiconductors like $\\mathrm{Cs_3Sb}$ and $\\mathrm{Cs_2Te}$ are\ncurrently used as photocathodes in particle accelerators. Their performance as\nelectron sources critically depends on their interaction with intense laser\nsources. In this work, we investigate from first principles the time-dependent\nresponse of $\\mathrm{Cs_3Sb}$ and $\\mathrm{Cs_2Te}$ to ultrafast pulses of\nvarying intensities, ranging from $1~\\mathrm{GW\/cm^2}$ to $1~\\mathrm{PW\/cm^2}$.\nNonlinear effects, including high harmonic generation, emerge starting from\n$100~\\mathrm{GW\/cm^2}$ in $\\mathrm{Cs_3Sb}$ and $200~\\mathrm{GW\/cm^2}$ in\n$\\mathrm{Cs_2Te}$. Above these intensities, the numbers of absorbed photons and\nexcited electrons saturate due to the depletion of one-photon absorption\nchannels, with renewed increases beyond $1~\\mathrm{TW\/cm^2}$ for\n$\\mathrm{Cs_3Sb}$ and $5~\\mathrm{TW\/cm^2}$ for $\\mathrm{Cs_2Te}$ where\nmulti-photon absorption appears. Finally, the analysis of the occupation\ndensity in $\\mathrm{Cs_3Sb}$ reveals the onset of tunnel ionization at\nintensities above $10~\\mathrm{TW\/cm^2}$. Our findings provide new insights into\nthe nonlinear optical properties of $\\mathrm{Cs_3Sb}$ and $\\mathrm{Cs_2Te}$,\ncontributing to the optimization of these materials for the development of\nnext-generation photoinjectors."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11336",
    "c_title":[
      "Probing invisible neutrino decay with the first six detection units of\n  KM3NeT\/ORCA"
    ],
    "c_abstract":[
      "In the era of precision measurements of neutrino oscillation parameters, it\nis necessary for experiments to disentangle discrepancies that may indicate\nphysics beyond the Standard Model in the neutrino sector. KM3NeT\/ORCA is a\nwater Cherenkov neutrino detector under construction and anchored at the bottom\nof the Mediterranean Sea. The detector is designed to study the oscillations of\natmospheric neutrinos and determine the neutrino mass ordering. This paper\nfocuses on the initial configuration of ORCA, referred to as ORCA6, which\ncomprises six out of the foreseen 115 detection units of photosensors. A\nhigh-purity neutrino sample was extracted during 2020 and 2021, corresponding\nto an exposure of 433 kton-years. This sample is analysed following a binned\nlog-likelihood approach to search for invisible neutrino decay, in a\nthree-flavour neutrino oscillation scenario, where the third neutrino mass\nstate $\\nu_3$ decays into an invisible state, e.g. a sterile neutrino. The\nresulting best fit of the invisible neutrino decay parameter is $\\alpha_3 =\n0.92^{+1.08}_{-0.57}\\times 10^{-4}~\\mathrm{eV^2}$, corresponding to a scenario\nwith $\\theta_{23}$ in the second octant and normal neutrino mass ordering. The\nresults are consistent with the Standard Model, within a $2.1\\,\\sigma$\ninterval."
    ],
    "c_categories":[
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
    ],
    "b_abstract":[
      "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15354",
    "c_title":[
      "Eigenfunctions with double exponential rate of localization"
    ],
    "c_abstract":[
      "We construct a real-valued solution to the eigenvalue problem\n$-\\text{div}(A\\nabla u)=\\lambda u$, $\\lambda>0,$ in the cylinder\n$\\mathbb{T}^2\\times \\mathbb{R}$ with a real, uniformly elliptic, and uniformly\n$C^1$ matrix $A$ such that $|u(x,y,t)|\\leq C e^{-c e^{c|t|}}$ for some $c,C>0$.\nWe also construct a complex-valued solution to the heat equation $u_t=\\Delta u\n+ B \\nabla u$ in a half-cylinder with continuous and uniformly bounded $B$,\nwhich also decays with double exponential speed. Related classical ideas, used\nin the construction of counterexamples to the unique continuation by Plis and\nMiller, are reviewed."
    ],
    "c_categories":[
      "math.AP",
      "math.CA",
      "math.SP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04686",
    "c_title":[
      "Learning Strategic Language Agents in the Werewolf Game with Iterative\n  Latent Space Policy Optimization"
    ],
    "c_abstract":[
      "Large language model (LLM)-based agents have recently shown impressive\nprogress in a variety of domains, including open-ended conversation and\nmulti-step decision-making. However, applying these agents to social deduction\ngames such as Werewolf, which requires both strategic decision-making and\nfree-form language interaction, remains non-trivial. Traditional methods based\non Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)\ntypically depend on a predefined action space, making them unsuitable for\nlanguage games with unconstrained text action space. Meanwhile, pure LLM-based\nagents often suffer from intrinsic biases and require prohibitively large\ndatasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),\nan iterative framework that addresses these challenges by first mapping\nfree-form text to a discrete latent space, where methods like CFR and RL can\nlearn strategic policy more effectively. We then translate the learned policy\nback into natural language dialogues, which are used to fine-tune an LLM via\nDirect Preference Optimization (DPO). By iteratively alternating between these\nstages, our LSPO agent progressively enhances both strategic reasoning and\nlanguage communication. Experiment results on the Werewolf game show that our\nmethod improves the agent's performance in each iteration and outperforms\nexisting Werewolf agents, underscoring its promise for free-form language\ndecision-making."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09545",
    "c_title":[
      "The Value of Goal Commitment in Planning"
    ],
    "c_abstract":[
      "In this paper, we revisit the concept of goal commitment from early planners\nin the presence of current forward chaining heuristic planners. We present a\ncompilation that extends the original planning task with commit actions that\nenforce the persistence of specific goals once achieved, thereby committing to\nthem in the search sub-tree. This approach imposes a specific goal achievement\norder in parts of the search tree, potentially introducing dead-end states.\nThis can reduce search effort if the goal achievement order is correct.\nOtherwise, the search algorithm can expand nodes in the open list where goals\ndo not persist. Experimental results demonstrate that the reformulated tasks\nsuit state-of-the-art agile planners, enabling them to find better"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07709",
    "c_title":[
      "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
    ],
    "c_abstract":[
      "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.03791",
    "c_title":[
      "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue"
    ],
    "c_abstract":[
      "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.13622",
    "c_title":[
      "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning"
    ],
    "c_abstract":[
      "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03927",
    "c_title":[
      "Optimal Estimation of Temperature"
    ],
    "c_abstract":[
      "Over the past century, the Boltzmann entropy has been widely accepted as the\nstandard definition of entropy for an isolated system. However, it coexists\nwith controversial alternatives, such as the Gibbs entropy. These definitions,\nincluding the Boltzmann entropy, exhibit certain inconsistencies, both\nmathematically and thermodynamically. To address this challenge, we introduce\nthe estimation theory in statistical inference into the study of thermodynamics\nand statistical physics for finite-sized systems. By regarding the finite-sized\nsystem as a thermometer used to measure the temperature of the heat reservoir,\nwe show that optimal estimation of temperature yields the corresponding entropy\nformula for an isolated system. In the single-sample case, optimal estimation\nof inverse temperature (or temperature) corresponds to the Boltzmann entropy\n(or Gibbs entropy). These different definitions of entropy, rather than being\ncontradictory, apply to optimal estimation of different parameters.\nFurthermore, via the Laplace transform, we identify a complementarity between\nestimation of temperature and system's energy, a concept suggested by Niels\nBohr. We also correct the energy-temperature uncertainty relation, as expressed\nby the Cram\\'{e}r-Rao bound, in the large-$N$ limit. In the multiple-sample\ncase, we generalize the definitions of both Boltzmann entropy and Gibbs entropy\nto achieve optimal estimation of temperature, revealing the tight connection\nbetween statistical inference and Terrell Hill's nanothermodynamics."
    ],
    "c_categories":[
      "cond-mat.stat-mech"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17855",
    "c_title":[
      "A novel gradient-based method for decision trees optimizing arbitrary\n  differential loss functions"
    ],
    "c_abstract":[
      "There are many approaches for training decision trees. This work introduces a\nnovel gradient-based method for constructing decision trees that optimize\narbitrary differentiable loss functions, overcoming the limitations of\nheuristic splitting rules. Unlike traditional approaches that rely on heuristic\nsplitting rules, the proposed method refines predictions using the first and\nsecond derivatives of the loss function, enabling the optimization of complex\ntasks such as classification, regression, and survival analysis. We demonstrate\nthe method's applicability to classification, regression, and survival analysis\ntasks, including those with censored data. Numerical experiments on both real\nand synthetic datasets compare the proposed method with traditional decision\ntree algorithms, such as CART, Extremely Randomized Trees, and SurvTree. The\nimplementation of the method is publicly available, providing a practical tool\nfor researchers and practitioners. This work advances the field of decision\ntree-based modeling, offering a more flexible and accurate approach for\nhandling structured data and complex tasks. By leveraging gradient-based\noptimization, the proposed method bridges the gap between traditional decision\ntrees and modern machine learning techniques, paving the way for further\ninnovations in interpretable and high-performing models."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03025",
    "c_title":[
      "Optimal control of the fidelity coefficient in a Cahn-Hilliard image\n  inpainting model"
    ],
    "c_abstract":[
      "We consider an inpainting model proposed by A. Bertozzi et al., which is\nbased on a Cahn--Hilliard-type equation. This equation describes the evolution\nof an order parameter $u \\in [0,1]$ representing an approximation of the\noriginal image which occupies a bounded two-dimensional domain $\\Omega$. The\ngiven image $g$ is assumed to be damaged in a fixed subdomain $D \\subset\n\\Omega$ and the equation is characterized by a linear reaction term of the form\n$\\lambda (u - g)$. Here $\\lambda = \\lambda_0 \\chi_{\\Omega \\setminus D}$ is the\nso-called fidelity coefficient, $\\lambda_0$ being a strictly positive bounded\nfunction. The idea is that, given an initial image $u_0$, $u$ evolves towards\n$g$ and this process properly diffuses through the boundary of $D$ restoring\nthe damaged image, provided that $\\lambda_0$ is large enough. Here, we\nformulate an optimal control problem based on this fact, namely our cost\nfunctional accounts for the magnitude of $\\lambda_0$. Assuming a singular\npotential to assure that $u$ takes its values in $[0,1]$, we first analyse the\ncontrol-to-state operator and prove the existence of at least one optimal\ncontrol, establishing the validity of first-order optimality conditions. Then,\nunder suitable assumptions, we demonstrate second-order optimality conditions.\nAll these results depend on the existence and uniqueness of a strong solution,\nwhich we obtain thanks to the strict separation property from pure phases."
    ],
    "c_categories":[
      "math.AP",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15076",
    "c_title":[
      "Cryptanalysis via Machine Learning Based Information Theoretic Metrics"
    ],
    "c_abstract":[
      "The fields of machine learning (ML) and cryptanalysis share an interestingly\ncommon objective of creating a function, based on a given set of inputs and\noutputs. However, the approaches and methods in doing so vary vastly between\nthe two fields. In this paper, we explore integrating the knowledge from the ML\ndomain to provide empirical evaluations of cryptosystems. Particularly, we\nutilize information theoretic metrics to perform ML-based distribution\nestimation. We propose two novel applications of ML algorithms that can be\napplied in a known plaintext setting to perform cryptanalysis on any\ncryptosystem. We use mutual information neural estimation to calculate a\ncryptosystem's mutual information leakage, and a binary cross entropy\nclassification to model an indistinguishability under chosen plaintext attack\n(CPA). These algorithms can be readily applied in an audit setting to evaluate\nthe robustness of a cryptosystem and the results can provide a useful empirical\nbound. We evaluate the efficacy of our methodologies by empirically analyzing\nseveral encryption schemes. Furthermore, we extend the analysis to novel\nnetwork coding-based cryptosystems and provide other use cases for our\nalgorithms. We show that our classification model correctly identifies the\nencryption schemes that are not IND-CPA secure, such as DES, RSA, and AES ECB,\nwith high accuracy. It also identifies the faults in CPA-secure cryptosystems\nwith faulty parameters, such a reduced counter version of AES-CTR. We also\nconclude that with our algorithms, in most cases a smaller-sized neural network\nusing less computing power can identify vulnerabilities in cryptosystems,\nproviding a quick check of the sanity of the cryptosystem and help to decide\nwhether to spend more resources to deploy larger networks that are able to\nbreak the cryptosystem."
    ],
    "c_categories":[
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.18767",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
    ],
    "b_abstract":[
      "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.08127",
    "c_title":[
      "Experiment BEST-2 with 58Co neutrino source"
    ],
    "c_abstract":[
      "The article describes a new experiment with an artificial neutrino source\n58Co on a gallium target GGNT (SAGE). The goal of the experiment is to study\nthe gallium anomaly. The experiment makes it possible to find the parameters of\noscillation transitions of electron neutrinos to sterile states in a wide range\nof parameters. Including the parameter {\\Delta}m2, the experimental\ndetermination of which usually causes significant difficulties. An important\nfeature of the experiment is the possibility of identifying the dependence of\nthe gallium anomaly on the neutrino energy."
    ],
    "c_categories":[
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"EyeDiff: text-to-image diffusion model improves rare eye disease\n  diagnosis",
    "a_abstract":"The rising prevalence of vision-threatening retinal diseases poses a\nsignificant burden on the global healthcare systems. Deep learning (DL) offers\na promising solution for automatic disease screening but demands substantial\ndata. Collecting and labeling large volumes of ophthalmic images across various\nmodalities encounters several real-world challenges, especially for rare\ndiseases. Here, we introduce EyeDiff, a text-to-image model designed to\ngenerate multimodal ophthalmic images from natural language prompts and\nevaluate its applicability in diagnosing common and rare diseases. EyeDiff is\ntrained on eight large-scale datasets using the advanced latent diffusion\nmodel, covering 14 ophthalmic image modalities and over 80 ocular diseases, and\nis adapted to ten multi-country external datasets. The generated images\naccurately capture essential lesional characteristics, achieving high alignment\nwith text prompts as evaluated by objective metrics and human experts.\nFurthermore, integrating generated images significantly enhances the accuracy\nof detecting minority classes and rare eye diseases, surpassing traditional\noversampling methods in addressing data imbalance. EyeDiff effectively tackles\nthe issue of data imbalance and insufficiency typically encountered in rare\ndiseases and addresses the challenges of collecting large-scale annotated\nimages, offering a transformative solution to enhance the development of\nexpert-level diseases diagnosis models in ophthalmic field.",
    "explanation":"Here, we introduce EyeDiff, a text-to-image model designed to\ngenerate multimodal ophthalmic images from natural language prompts and evaluate its\napplicability in diagnosing common and rare diseases.",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b7"
    ],
    "c_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "c_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03661",
    "c_title":[
      "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues"
    ],
    "c_abstract":[
      "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12234",
    "c_title":[
      "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer"
    ],
    "c_abstract":[
      "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08533",
    "c_title":[
      "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant"
    ],
    "c_abstract":[
      "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04504",
    "c_title":[
      "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates"
    ],
    "c_abstract":[
      "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.03927",
    "c_title":[
      "Optimal Estimation of Temperature"
    ],
    "c_abstract":[
      "Over the past century, the Boltzmann entropy has been widely accepted as the\nstandard definition of entropy for an isolated system. However, it coexists\nwith controversial alternatives, such as the Gibbs entropy. These definitions,\nincluding the Boltzmann entropy, exhibit certain inconsistencies, both\nmathematically and thermodynamically. To address this challenge, we introduce\nthe estimation theory in statistical inference into the study of thermodynamics\nand statistical physics for finite-sized systems. By regarding the finite-sized\nsystem as a thermometer used to measure the temperature of the heat reservoir,\nwe show that optimal estimation of temperature yields the corresponding entropy\nformula for an isolated system. In the single-sample case, optimal estimation\nof inverse temperature (or temperature) corresponds to the Boltzmann entropy\n(or Gibbs entropy). These different definitions of entropy, rather than being\ncontradictory, apply to optimal estimation of different parameters.\nFurthermore, via the Laplace transform, we identify a complementarity between\nestimation of temperature and system's energy, a concept suggested by Niels\nBohr. We also correct the energy-temperature uncertainty relation, as expressed\nby the Cram\\'{e}r-Rao bound, in the large-$N$ limit. In the multiple-sample\ncase, we generalize the definitions of both Boltzmann entropy and Gibbs entropy\nto achieve optimal estimation of temperature, revealing the tight connection\nbetween statistical inference and Terrell Hill's nanothermodynamics."
    ],
    "c_categories":[
      "cond-mat.stat-mech"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17855",
    "c_title":[
      "A novel gradient-based method for decision trees optimizing arbitrary\n  differential loss functions"
    ],
    "c_abstract":[
      "There are many approaches for training decision trees. This work introduces a\nnovel gradient-based method for constructing decision trees that optimize\narbitrary differentiable loss functions, overcoming the limitations of\nheuristic splitting rules. Unlike traditional approaches that rely on heuristic\nsplitting rules, the proposed method refines predictions using the first and\nsecond derivatives of the loss function, enabling the optimization of complex\ntasks such as classification, regression, and survival analysis. We demonstrate\nthe method's applicability to classification, regression, and survival analysis\ntasks, including those with censored data. Numerical experiments on both real\nand synthetic datasets compare the proposed method with traditional decision\ntree algorithms, such as CART, Extremely Randomized Trees, and SurvTree. The\nimplementation of the method is publicly available, providing a practical tool\nfor researchers and practitioners. This work advances the field of decision\ntree-based modeling, offering a more flexible and accurate approach for\nhandling structured data and complex tasks. By leveraging gradient-based\noptimization, the proposed method bridges the gap between traditional decision\ntrees and modern machine learning techniques, paving the way for further\ninnovations in interpretable and high-performing models."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03025",
    "c_title":[
      "Optimal control of the fidelity coefficient in a Cahn-Hilliard image\n  inpainting model"
    ],
    "c_abstract":[
      "We consider an inpainting model proposed by A. Bertozzi et al., which is\nbased on a Cahn--Hilliard-type equation. This equation describes the evolution\nof an order parameter $u \\in [0,1]$ representing an approximation of the\noriginal image which occupies a bounded two-dimensional domain $\\Omega$. The\ngiven image $g$ is assumed to be damaged in a fixed subdomain $D \\subset\n\\Omega$ and the equation is characterized by a linear reaction term of the form\n$\\lambda (u - g)$. Here $\\lambda = \\lambda_0 \\chi_{\\Omega \\setminus D}$ is the\nso-called fidelity coefficient, $\\lambda_0$ being a strictly positive bounded\nfunction. The idea is that, given an initial image $u_0$, $u$ evolves towards\n$g$ and this process properly diffuses through the boundary of $D$ restoring\nthe damaged image, provided that $\\lambda_0$ is large enough. Here, we\nformulate an optimal control problem based on this fact, namely our cost\nfunctional accounts for the magnitude of $\\lambda_0$. Assuming a singular\npotential to assure that $u$ takes its values in $[0,1]$, we first analyse the\ncontrol-to-state operator and prove the existence of at least one optimal\ncontrol, establishing the validity of first-order optimality conditions. Then,\nunder suitable assumptions, we demonstrate second-order optimality conditions.\nAll these results depend on the existence and uniqueness of a strong solution,\nwhich we obtain thanks to the strict separation property from pure phases."
    ],
    "c_categories":[
      "math.AP",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15076",
    "c_title":[
      "Cryptanalysis via Machine Learning Based Information Theoretic Metrics"
    ],
    "c_abstract":[
      "The fields of machine learning (ML) and cryptanalysis share an interestingly\ncommon objective of creating a function, based on a given set of inputs and\noutputs. However, the approaches and methods in doing so vary vastly between\nthe two fields. In this paper, we explore integrating the knowledge from the ML\ndomain to provide empirical evaluations of cryptosystems. Particularly, we\nutilize information theoretic metrics to perform ML-based distribution\nestimation. We propose two novel applications of ML algorithms that can be\napplied in a known plaintext setting to perform cryptanalysis on any\ncryptosystem. We use mutual information neural estimation to calculate a\ncryptosystem's mutual information leakage, and a binary cross entropy\nclassification to model an indistinguishability under chosen plaintext attack\n(CPA). These algorithms can be readily applied in an audit setting to evaluate\nthe robustness of a cryptosystem and the results can provide a useful empirical\nbound. We evaluate the efficacy of our methodologies by empirically analyzing\nseveral encryption schemes. Furthermore, we extend the analysis to novel\nnetwork coding-based cryptosystems and provide other use cases for our\nalgorithms. We show that our classification model correctly identifies the\nencryption schemes that are not IND-CPA secure, such as DES, RSA, and AES ECB,\nwith high accuracy. It also identifies the faults in CPA-secure cryptosystems\nwith faulty parameters, such a reduced counter version of AES-CTR. We also\nconclude that with our algorithms, in most cases a smaller-sized neural network\nusing less computing power can identify vulnerabilities in cryptosystems,\nproviding a quick check of the sanity of the cryptosystem and help to decide\nwhether to spend more resources to deploy larger networks that are able to\nbreak the cryptosystem."
    ],
    "c_categories":[
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Artificial Intelligence for Pediatric Ophthalmology"
    ],
    "b_abstract":[
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08127",
    "c_title":[
      "Experiment BEST-2 with 58Co neutrino source"
    ],
    "c_abstract":[
      "The article describes a new experiment with an artificial neutrino source\n58Co on a gallium target GGNT (SAGE). The goal of the experiment is to study\nthe gallium anomaly. The experiment makes it possible to find the parameters of\noscillation transitions of electron neutrinos to sterile states in a wide range\nof parameters. Including the parameter {\\Delta}m2, the experimental\ndetermination of which usually causes significant difficulties. An important\nfeature of the experiment is the possibility of identifying the dependence of\nthe gallium anomaly on the neutrino energy."
    ],
    "c_categories":[
      "hep-ex"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.10197",
    "c_title":[
      "MathConstruct: Challenging LLM Reasoning with Constructive Proofs"
    ],
    "c_abstract":[
      "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.01642",
    "c_title":[
      "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph\n  Retrieval for LLM Reasoning"
    ],
    "c_abstract":[
      "Recent large language model (LLM) reasoning, despite its success, suffers\nfrom limited domain knowledge, susceptibility to hallucinations, and\nconstrained reasoning depth, particularly in small-scale models deployed in\nresource-constrained environments. This paper presents the first investigation\ninto integrating step-wise knowledge graph retrieval with step-wise reasoning\nto address these challenges, introducing a novel paradigm termed as\ngraph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to\nretrieve and process relevant mathematical knowledge in a step-wise manner,\nenhancing their problem-solving abilities without additional training. To this\nend, we propose KG-RAR, a framework centered on process-oriented knowledge\ngraph construction, a hierarchical retrieval strategy, and a universal\npost-retrieval processing and reward model (PRP-RM) that refines retrieved\ninformation and evaluates each reasoning step. Experiments on the Math500 and\nGSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging\nresults, achieving a 20.73\\% relative improvement with Llama-3B on Math500."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08011",
    "c_title":[
      "Training-Free Safe Denoisers for Safe Use of Diffusion Models"
    ],
    "c_abstract":[
      "There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.09242",
    "c_title":[
      "From large language models to multimodal AI: A scoping review on the\n  potential of generative AI in medicine"
    ],
    "c_abstract":[
      "Generative artificial intelligence (AI) models, such as diffusion models and\nOpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy\nand automating clinical workflows. The field has advanced rapidly, evolving\nfrom text-only large language models for tasks such as clinical documentation\nand decision support to multimodal AI systems capable of integrating diverse\ndata modalities, including imaging, text, and structured data, within a single\nmodel. The diverse landscape of these technologies, along with rising interest,\nhighlights the need for a comprehensive review of their applications and\npotential. This scoping review explores the evolution of multimodal AI,\nhighlighting its methods, applications, datasets, and evaluation in clinical\nsettings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed,\nIEEE Xplore, and Web of Science, prioritizing recent studies published up to\nthe end of 2024. After rigorous screening, 144 papers were included, revealing\nkey trends and challenges in this dynamic field. Our findings underscore a\nshift from unimodal to multimodal approaches, driving innovations in diagnostic\nsupport, medical report generation, drug discovery, and conversational AI.\nHowever, critical challenges remain, including the integration of heterogeneous\ndata types, improving model interpretability, addressing ethical concerns, and\nvalidating AI systems in real-world clinical settings. This review summarizes\nthe current state of the art, identifies critical gaps, and provides insights\nto guide the development of scalable, trustworthy, and clinically impactful\nmultimodal AI solutions in healthcare."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17726",
    "c_title":[
      "A Survey on Mathematical Reasoning and Optimization with Large Language\n  Models"
    ],
    "c_abstract":[
      "Mathematical reasoning and optimization are fundamental to artificial\nintelligence and computational problem-solving. Recent advancements in Large\nLanguage Models (LLMs) have significantly improved AI-driven mathematical\nreasoning, theorem proving, and optimization techniques. This survey explores\nthe evolution of mathematical problem-solving in AI, from early statistical\nlearning approaches to modern deep learning and transformer-based\nmethodologies. We review the capabilities of pretrained language models and\nLLMs in performing arithmetic operations, complex reasoning, theorem proving,\nand structured symbolic computation. A key focus is on how LLMs integrate with\noptimization and control frameworks, including mixed-integer programming,\nlinear quadratic control, and multi-agent optimization strategies. We examine\nhow LLMs assist in problem formulation, constraint generation, and heuristic\nsearch, bridging theoretical reasoning with practical applications. We also\ndiscuss enhancement techniques such as Chain-of-Thought reasoning, instruction\ntuning, and tool-augmented methods that improve LLM's problem-solving\nperformance. Despite their progress, LLMs face challenges in numerical\nprecision, logical consistency, and proof verification. Emerging trends such as\nhybrid neural-symbolic reasoning, structured prompt engineering, and multi-step\nself-correction aim to overcome these limitations. Future research should focus\non interpretability, integration with domain-specific solvers, and improving\nthe robustness of AI-driven decision-making. This survey offers a comprehensive\nreview of the current landscape and future directions of mathematical reasoning\nand optimization with LLMs, with applications across engineering, finance, and\nscientific research."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10540",
    "c_title":[
      "DPERC: Direct Parameter Estimation for Mixed Data"
    ],
    "c_abstract":[
      "The covariance matrix is a foundation in numerous statistical and\nmachine-learning applications such as Principle Component Analysis, Correlation\nHeatmap, etc. However, missing values within datasets present a formidable\nobstacle to accurately estimating this matrix. While imputation methods offer\none avenue for addressing this challenge, they often entail a trade-off between\ncomputational efficiency and estimation accuracy. Consequently, attention has\nshifted towards direct parameter estimation, given its precision and reduced\ncomputational burden. In this paper, we propose Direct Parameter Estimation for\nRandomly Missing Data with Categorical Features (DPERC), an efficient approach\nfor direct parameter estimation tailored to mixed data that contains missing\nvalues within continuous features. Our method is motivated by leveraging\ninformation from categorical features, which can significantly enhance\ncovariance matrix estimation for continuous features. Our approach effectively\nharnesses the information embedded within mixed data structures. Through\ncomprehensive evaluations of diverse datasets, we demonstrate the competitive\nperformance of DPERC compared to various contemporary techniques. In addition,\nwe also show by experiments that DPERC is a valuable tool for visualizing the\ncorrelation heatmap."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.14978",
    "c_title":[
      "Generalized Oxtoby subshifts and hyperfiniteness"
    ],
    "c_abstract":[
      "We show that there exists a class of symbolic subshifts which realizes all\nChoquet simplices as simplices of invariant measures and the conjugacy relation\non that class is hyperfinite."
    ],
    "c_categories":[
      "math.DS",
      "math.LO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.02689",
    "c_title":[
      "Interacting topological magnons in the Kitaev-Heisenberg honeycomb\n  ferromagnet with the Dzyaloshinskii-Moriya interaction"
    ],
    "c_abstract":[
      "The study of the Heisenberg-Kitaev honeycomb ferromagnets has recently drawn\nattention because of their rich topological properties. Topological phase\ntransitions may arise when there exist two or more distinct topological phases,\nand they are often revealed by a gap-closing phenomenon. In this work, we\ninvestigate the magnonic properties of honeycomb ferromagnets exhibiting Kitaev\nand DMI interactions in the presence of a Heisenberg exchange and\nmagnetocrystalline anisotropy exposed to a magnetic field. We employ the\nSelf-Consistent Renormalization (SCR) spin wave theory to investigate the\neffects of magnon-magnon interactions (MMIs) and thermal fluctuations on the\nproperties of magnons. Our findings demonstrate that the magnon system\nundergoes topological phase transitions driven by temperature and magnetic\nfields, which are attributed to MMIs. Specifically, as the temperature rises,\nthe magnon band gap at the Dirac points closes and reopens at the critical\ntemperature Tc , which is below the Curie temperature. By showing that the\nChern numbers of the magnonic bands are distinct above and below Tc , we\nconfirm that the gap-closing phenomenon is indeed a signature for the\ntopological phase transitions. Furthermore, our analysis indicates that the\nthermal Hall conductivity in the magnonic system exhibits a sign reversal at Tc\n, which can serve as an experimental probe of its topological nature."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11994",
    "c_title":[
      "A Bayesian Proportional Mean Model Using Panel Binary Data-An\n  Application to Health and Retirement Study"
    ],
    "c_abstract":[
      "In recurrent event studies, panel binary data arise when subjects are\nobserved at discrete time points and only the recurrent event status within\neach observation window is recorded. Such data frequently occur in longitudinal\nstudies due to recall difficulties or participants' privacy concerns during\nfollow-ups, necessitating rigorous statistical analysis. While frequentist\nmethods exist for handling such data, Bayesian approaches remain largely\nunexplored. This article proposes an efficient Bayesian proportional mean model\nfor analysing recurrent events using panel binary data. In addition to the\nestimation procedure, the article introduces techniques for model validation,\nselection, and Bayesian influence diagnostics. Simulation studies demonstrate\nthe method's effectiveness and robustness in different practical scenarios. The\nproposed approach is then applied to analyse the latest version of the Health\nand Retirement Study dataset, identifying key risk factors influencing doctor\nvisits among the elderly. The analysis is therefore capable of providing\nvaluable insights into healthcare utilisation patterns in ageing populations."
    ],
    "c_categories":[
      "stat.AP",
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.10004",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b7"
    ],
    "b_title":[
      "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
    ],
    "b_abstract":[
      "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04617",
    "c_title":[
      "Effects of Curved Superconducting Magnets on Beam Stability in a Compact\n  Ion Therapy Synchrotron"
    ],
    "c_abstract":[
      "Superconducting, curved magnets can reduce accelerator footprints by\nproducing strong fields (>3T) and reducing the total number of magnets through\ntheir capability for combined-function multipolar fields, making them an\nattractive choice for applications such as heavy ion therapy. There exists the\nproblem that the effect of strongly curved harmonics and fringe fields on\ncompact accelerator beam dynamics is not well represented: existing approaches\nuse integrated cylindrical multipoles to describe and model the fields for beam\ndynamics studies, which are invalid in curved coordinate systems and assume\nindividual errors cancel out over the full machine. In the modelling of these\nmachines, the effect of strongly curved harmonics and fringe fields on compact\naccelerator beam dynamics needs to properly included. An alternative approach\nmust be introduced for capturing off-axis fields in a strongly curved magnet,\nwhich may affect long-term beam stability in a compact accelerator. In this\narticle, we investigate the impacts of deploying a curved canted-cosine-theta\n(CCT) superconducting magnet in a compact medical synchrotron for the first\ntime. We develop a method to analyse and characterise the 3D curved fields of\nan electromagnetic model of a CCT developed for the main bending magnets of a\n27m circumference carbon ion therapy synchrotron, designed within the Heavy Ion\nTherapy Research Integration Plus European project, and the CERN Next Ion\nMedical Machine Study (NIMMS). The fields are modelled in the compact\nsynchrotron in MAD-X\/PTC to study their effects on beam dynamics and long-term\nbeam stability. The insights gained through the methods presented allow for the\noptimisation of both magnet and synchrotron designs, with the potential to\nimpact the operational performance of future ion therapy facilities."
    ],
    "c_categories":[
      "physics.acc-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"Scaling Laws with Hidden Structure",
    "a_abstract":"Statistical learning in high-dimensional spaces is challenging without a\nstrong underlying data structure. Recent advances with foundational models\nsuggest that text and image data contain such hidden structures, which help\nmitigate the curse of dimensionality. Inspired by results from nonparametric\nstatistics, we hypothesize that this phenomenon can be partially explained in\nterms of decomposition of complex tasks into simpler subtasks. In this paper,\nwe present a controlled experimental framework to test whether neural networks\ncan indeed exploit such ``hidden factorial structures.'' We find that they do\nleverage these latent patterns to learn discrete distributions more\nefficiently, and derive scaling laws linking model sizes, hidden\nfactorizations, and accuracy. We also study the interplay between our\nstructural assumptions and the models' capacity for generalization.",
    "explanation":"The key references I chose in Task 3 combined the concepts of Neural Networks with Discrete Distribution Theory to support this IDR paper. In the abstract, the following lines describe the integration of those selected references:\n\"In this paper, we present a controlled experimental framework to test whether neural networks can indeed exploit such \u201chidden factorial structures.\u201d\n\"We find that they do leverage these latent patterns to learn discrete distributions more efficiently. \"\n\n",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b11"
    ],
    "c_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "c_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "c_categories":[
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":true,
    "research_type":"basic"
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.05908",
    "c_title":[
      "MCMC for multi-modal distributions"
    ],
    "c_abstract":[
      "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces."
    ],
    "c_categories":[
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03969",
    "c_title":[
      "Spectrally Deconfounded Random Forests"
    ],
    "c_abstract":[
      "We introduce a modification of Random Forests to estimate functions when\nunobserved confounding variables are present. The technique is tailored for\nhigh-dimensional settings with many observed covariates. We use spectral\ndeconfounding techniques to minimize a deconfounded version of the least\nsquares objective, resulting in the Spectrally Deconfounded Random Forests\n(SDForests). We show how the omitted variable bias gets small given some\nassumptions. We compare the performance of SDForests to classical Random\nForests in a simulation study and a semi-synthetic setting using single-cell\ngene expression data. Empirical results suggest that SDForests outperform\nclassical Random Forests in estimating the direct regression function, even if\nthe theoretical assumptions, requiring linear and dense confounding, are not\nperfectly met, and that SDForests have comparable performance in the\nnon-confounded case."
    ],
    "c_categories":[
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16773",
    "c_title":[
      "Splitting Regularized Wasserstein Proximal Algorithms for Nonsmooth\n  Sampling Problems"
    ],
    "c_abstract":[
      "Sampling from nonsmooth target probability distributions is essential in\nvarious applications, including the Bayesian Lasso. We propose a\nsplitting-based sampling algorithm for the time-implicit discretization of the\nprobability flow for the Fokker-Planck equation, where the score function\ndefined as the gradient logarithm of the current probability density function,\nis approximated by the regularized Wasserstein proximal. When the prior\ndistribution is the Laplace prior, our algorithm is explicitly formulated as a\ndeterministic interacting particle system, incorporating softmax operators and\nshrinkage operations to efficiently compute the gradient drift vector field and\nthe score function. The proposed formulation introduces a particular class of\nattention layers in transformer structures, which can sample sparse target\ndistributions. We verify the convergence towards target distributions regarding\nR\\'enyi divergences under suitable conditions. Numerical experiments in\nhigh-dimensional nonsmooth sampling problems, such as sampling from mixed\nGaussian and Laplace distributions, logistic regressions, image restoration\nwith L1-TV regularization, and Bayesian neural networks, demonstrate the\nefficiency and robust performance of the proposed method."
    ],
    "c_categories":[
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04990",
    "c_title":[
      "Probabilistic Programming with Sufficient Statistics for faster Bayesian\n  Computation"
    ],
    "c_abstract":[
      "Probabilistic programming methods have revolutionised Bayesian inference,\nmaking it easier than ever for practitioners to perform\nMarkov-chain-Monte-Carlo sampling from non-conjugate posterior distributions.\nHere we focus on Stan, arguably the most used probabilistic programming tool\nfor Bayesian inference (Carpenter et al., 2017), and its interface with R via\nthe brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages.\nAlthough easy to implement, these tools can become computationally prohibitive\nwhen applied to datasets with many observations or models with numerous\nparameters. While the use of sufficient statistics is well-established in\ntheory, it has been surprisingly overlooked in state-of-the-art Stan software.\nWe show that when the likelihood can be written in terms of sufficient\nstatistics, considerable computational improvements can be made to current\nimplementations. We demonstrate how this approach provides accurate inference\nat a fraction of the time than state-of-the-art implementations for Gaussian\nlinear regression models with non-conjugate priors, hierarchical random effects\nmodels, and factor analysis models. Our results also show that moderate\ncomputational gains can be achieved even in models where the likelihood can\nonly be partially written in terms of sufficient statistics."
    ],
    "c_categories":[
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.20385",
    "c_title":[
      "rSPDE: tools for statistical modeling using fractional SPDEs"
    ],
    "c_abstract":[
      "The R software package rSPDE contains methods for approximating Gaussian\nrandom fields based on fractional-order stochastic partial differential\nequations (SPDEs). A common example of such fields are Whittle-Mat\\'ern fields\non bounded domains in $\\mathbb{R}^d$, manifolds, or metric graphs. The package\nalso implements various other models which are briefly introduced in this\narticle. Besides the approximation methods, the package contains methods for\nsimulation, prediction, and statistical inference for such models, as well as\ninterfaces to INLA, inlabru and MetricGraph. With these interfaces,\nfractional-order SPDEs can be used as model components in general latent\nGaussian models, for which full Bayesian inference can be performed, also for\nfractional models on metric graphs. This includes estimation of the smoothness\nparameter of the fields. This article describes the computational methods used\nin the package and summarizes the theoretical basis for these. The main\nfunctions of the package are introduced, and their usage is illustrated through\nvarious examples."
    ],
    "c_categories":[
      "stat.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17557",
    "c_title":[
      "Heuristic-Informed Mixture of Experts for Link Prediction in Multilayer\n  Networks"
    ],
    "c_abstract":[
      "Link prediction algorithms for multilayer networks are in principle required\nto effectively account for the entire layered structure while capturing the\nunique contexts offered by each layer. However, many existing approaches excel\nat predicting specific links in certain layers but struggle with others, as\nthey fail to effectively leverage the diverse information encoded across\ndifferent network layers. In this paper, we present MoE-ML-LP, the first\nMixture-of-Experts (MoE) framework specifically designed for multilayer link\nprediction. Building on top of multilayer heuristics for link prediction,\nMoE-ML-LP synthesizes the decisions taken by diverse experts, resulting in\nsignificantly enhanced predictive capabilities. Our extensive experimental\nevaluation on real-world and synthetic networks demonstrates that MoE-ML-LP\nconsistently outperforms several baselines and competing methods, achieving\nremarkable improvements of +60% in Mean Reciprocal Rank, +82% in Hits@1, +55%\nin Hits@5, and +41% in Hits@10. Furthermore, MoE-ML-LP features a modular\narchitecture that enables the seamless integration of newly developed experts\nwithout necessitating the re-training of the entire framework, fostering\nefficiency and scalability to new experts, paving the way for future\nadvancements in link prediction."
    ],
    "c_categories":[
      "cs.LG",
      "cs.SI",
      "physics.soc-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12933",
    "c_title":[
      "Electric Polarizability of Charged Kaons from Lattice QCD Four-Point\n  Functions"
    ],
    "c_abstract":[
      "We study the electric polarizability of a charged kaon from four-point\nfunctions in lattice QCD as an alternative to the background field method.\nLattice four-point correlation functions are constructed from quark and gluon\nfields to be used in Monte Carlo simulations. The elastic form factor (charge\nradius) is needed in the method which can be obtained from the same four-point\nfunctions at large current separations. Preliminary results from the connected\nquark-line diagrams are presented."
    ],
    "c_categories":[
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08368",
    "c_title":[
      "Antimatter Gravity Experiments, the Astronomical Challenges to\n  Lambda-CDM Cosmology and the Quantum Vacuum as a Possible Source of Gravity\n  in the Universe"
    ],
    "c_abstract":[
      "This review is motivated by the first result of the ALPHA-g experiment at\nCERN, which indicates that atoms and anti-atoms have different gravitational\ncharges; according to measurements, the gravitational acceleration of\nanti-atoms is only 0.75 of that of ordinary matter. If confirmed by more\nprecise measurements, this will greatly increase the plausibility of the\nemerging cosmological model, which is based on the working hypothesis that\nquantum vacuum fluctuations are virtual gravitational dipoles; a hypothesis\nthat opens up the possibility that the quantum vacuum is a major source of\ngravity in the universe (which could eventually eliminate the need for the\nhypothetical dark matter and dark energy). This laboratory challenge to general\nrelativity and Lambda-CDM cosmology is complemented by astronomical challenges\n(the Hubble tension, very fast initial growth of structures in the Universe,\ndark energy deviating from the cosmological constant...). The intriguing\nquestion is: do the antimatter gravity experiments at CERN and the recent\nastronomical observations point to the same (highly unexpected) new physics?\nWith this question in mind, we briefly review the antimatter gravity\nexperiments at CERN and elsewhere, together with the major astronomical\nchallenges and the emerging Quantum Vacuum cosmology, which seems to be\ncompatible with the ALPHA-g result and with the preliminary astronomical\nchallenges. The laboratory and astronomical challenges have suddenly taken us\ninto terra incognita, where we need absolutely unprecedented imagination and\nopen-minded thinking."
    ],
    "c_categories":[
      "physics.gen-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12339",
    "c_title":[
      "Orbitronics in Two-dimensional Materials"
    ],
    "c_abstract":[
      "Orbitronics explores the control and manipulation of electronic orbital\nangular momentum in solid-state systems, opening new pathways for information\nprocessing and storage. One significant advantage of orbitronics over\nspintronics is that it does not rely on spin-orbit coupling, thereby broadening\nthe range of non-magnetic materials that can be utilized for these\napplications. It also introduces new topological features related to electronic\norbital angular momentum, and clarifies some long-standing challenges in\nunderstanding experiments that rely on the conventional concept of valley\ntransport. This review highlights recent advances in orbitronics, particularly\nin relation to two-dimensional materials. We examine the fundamental principles\nunderlying the generation, transport, and dynamics of orbital angular momentum\nto illustrate how the unique properties of two-dimensional materials can\npromote orbitronic phenomena. We also outline potential future research\ndirections and address some outstanding questions in this field."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Learning Parities with Neural Networks"
    ],
    "b_abstract":[
      "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.00923",
    "c_title":[
      "Dark energy and cosmic acceleration"
    ],
    "c_abstract":[
      "The discovery that we live in an accelerating universe changed drastically\nthe paradigm of physics and introduced the concept of \\textit{dark energy}. In\nthis work, we present a brief historical description of the main events related\nto the discovery of cosmic acceleration and the basic elements of theoretical\nand observational aspects of dark energy. Regarding the historical perspective,\nwe outline some of the key milestones for tracing the journey from Einstein's\nproposal of the cosmological constant to the type Ia supernovae results.\nConversely, on the theoretical\/observational side, we begin by analyzing cosmic\nacceleration within the context of the standard cosmological model, i.e., in\nterms of the cosmological constant. In this case, we show how a positive\ncosmological constant drives accelerated expansion and discuss the main\nobservational aspects, such as updated results and current cosmological\ntensions. We also explore alternative descriptions of dark energy, encompassing\ndynamic and interacting dark energy models."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.12555",
    "c_title":[
      "Warm Starting of CMA-ES for Contextual Optimization Problems"
    ],
    "c_abstract":[
      "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.07000",
    "c_title":[
      "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization"
    ],
    "c_abstract":[
      "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.06813",
    "c_title":[
      "Pareto Optimization with Robust Evaluation for Noisy Subset Selection"
    ],
    "c_abstract":[
      "Subset selection is a fundamental problem in combinatorial optimization,\nwhich has a wide range of applications such as influence maximization and\nsparse regression. The goal is to select a subset of limited size from a ground\nset in order to maximize a given objective function. However, the evaluation of\nthe objective function in real-world scenarios is often noisy. Previous\nalgorithms, including the greedy algorithm and multi-objective evolutionary\nalgorithms POSS and PONSS, either struggle in noisy environments or consume\nexcessive computational resources. In this paper, we focus on the noisy subset\nselection problem with a cardinality constraint, where the evaluation of a\nsubset is noisy. We propose a novel approach based on Pareto Optimization with\nRobust Evaluation for noisy subset selection (PORE), which maximizes a robust\nevaluation function and minimizes the subset size simultaneously. PORE can\nefficiently identify well-structured solutions and handle computational\nresources, addressing the limitations observed in PONSS. Our experiments,\nconducted on real-world datasets for influence maximization and sparse\nregression, demonstrate that PORE significantly outperforms previous methods,\nincluding the classical greedy algorithm, POSS, and PONSS. Further validation\nthrough ablation studies confirms the effectiveness of our robust evaluation\nfunction."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.20381",
    "c_title":[
      "Waves and symbols in neuromorphic hardware: from analog signal\n  processing to digital computing on the same computational substrate"
    ],
    "c_abstract":[
      "Neural systems use the same underlying computational substrate to carry out\nanalog filtering and signal processing operations, as well as discrete symbol\nmanipulation and digital computation. Inspired by the computational principles\nof canonical cortical microcircuits, we propose a framework for using recurrent\nspiking neural networks to seamlessly and robustly switch between analog signal\nprocessing and categorical and discrete computation. We provide theoretical\nanalysis and practical neural network design tools to formally determine the\nconditions for inducing this switch. We demonstrate the robustness of this\nframework experimentally with hardware soft Winner-Take-All and mixed-feedback\nrecurrent spiking neural networks, implemented by appropriately configuring the\nanalog neuron and synapse circuits of a mixed-signal neuromorphic processor\nchip."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.09976",
    "c_title":[
      "Dendritic Localized Learning: Toward Biologically Plausible Algorithm"
    ],
    "c_abstract":[
      "Backpropagation is the foundational algorithm for training neural networks\nand a key driver of deep learning's success. However, its biological\nplausibility has been challenged due to three primary limitations: weight\nsymmetry, reliance on global error signals, and the dual-phase nature of\ntraining, as highlighted by the existing literature. Although various\nalternative learning approaches have been proposed to address these issues,\nmost either fail to satisfy all three criteria simultaneously or yield\nsuboptimal results. Inspired by the dynamics and plasticity of pyramidal\nneurons, we propose Dendritic Localized Learning (DLL), a novel learning\nalgorithm designed to overcome these challenges. Extensive empirical\nexperiments demonstrate that DLL satisfies all three criteria of biological\nplausibility while achieving state-of-the-art performance among algorithms that\nmeet these requirements. Furthermore, DLL exhibits strong generalization across\na range of architectures, including MLPs, CNNs, and RNNs. These results,\nbenchmarked against existing biologically plausible learning algorithms, offer\nvaluable empirical insights for future research. We hope this study can inspire\nthe development of new biologically plausible algorithms for training\nmultilayer networks and advancing progress in both neuroscience and machine\nlearning."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.14301",
    "c_title":[
      "A high-resolution discourse on seismic tomography"
    ],
    "c_abstract":[
      "Advances in data acquisition and numerical wave simulation have improved\ntomographic imaging techniques and results, but non-experts may find it\ndifficult to understand which model is best for their needs. This paper is\nintended for these users. We argue that our notion of best is influenced by the\nextent to which models satisfy our biases. We explain how the basic types of\nseismic waves see Earth structure, illustrate the essential strategy of seismic\ntomography, discuss advanced adaptations such as full-waveform inversion, and\nemphasize the artistic components of tomography. The compounding effect of a\nplethora of reasonable, yet subjective choices is a range of models that differ\nmore than their individual uncertainty analyses may suggest. Perhaps\ncounter-intuitively, we argue producing similar tomographic models should not\nbe the goal of seismic tomography. Instead, we promote a Community Monte Carlo\neffort to assemble a range of dissimilar models based on different modeling\napproaches and subjective choices, but which explain the seismic data. This\neffort could serve as input for geodynamic inferences with meaningful seismic\nuncertainties."
    ],
    "c_categories":[
      "physics.app-ph",
      "physics.geo-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.16654",
    "c_title":[
      "Local models and Bell inequalities for the minimal triangle network"
    ],
    "c_abstract":[
      "Nonlocal correlations created in networks with multiple independent sources\nenable surprising phenomena in quantum information and quantum foundations. The\npresence of independent sources, however, makes the analysis of network\nnonlocality challenging, and even in the simplest nontrivial scenarios a\ncomplete characterization is lacking. In this work we study one of the simplest\nof these scenarios, namely that of distributions invariant under permutations\nof parties in the minimal triangle network, which features no inputs and binary\noutcomes. We perform an exhaustive search for triangle-local models, and from\nit we infer analytic expressions for the boundaries of the set of distributions\nthat admit such models, which we conjecture to be all the tight Bell\ninequalities for the scenario. Armed with them and with improved outer\napproximations of the set, we provide new insights on the existence of a\nclassical-quantum gap in the triangle network with binary outcomes."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10786",
    "c_title":[
      "Epidemic-guided deep learning for spatiotemporal forecasting of\n  Tuberculosis outbreak"
    ],
    "c_abstract":[
      "Tuberculosis (TB) remains a formidable global health challenge, driven by\ncomplex spatiotemporal transmission dynamics and influenced by factors such as\npopulation mobility and behavioral changes. We propose an Epidemic-Guided Deep\nLearning (EGDL) approach that fuses mechanistic epidemiological principles with\nadvanced deep learning techniques to enhance early warning systems and\nintervention strategies for TB outbreaks. Our framework is built upon a\nnetworked Susceptible-Infectious-Recovered (SIR) model augmented with a\nsaturated incidence rate and graph Laplacian diffusion, capturing both\nlong-term transmission dynamics and region-specific population mobility\npatterns. Compartmental model parameters are rigorously estimated using\nBayesian inference via the Markov Chain Monte Carlo (MCMC) approach.\nTheoretical analysis leveraging the comparison principle and Green's formula\nestablishes global stability properties of the disease-free and endemic\nequilibria. Building on these epidemiological insights, we design two\nforecasting architectures, EGDL-Parallel and EGDL-Series, that integrate the\nmechanistic outputs of the networked SIR model within deep neural networks.\nThis integration mitigates the overfitting risks commonly encountered in\ndata-driven methods and filters out noise inherent in surveillance data,\nresulting in reliable forecasts of real-world epidemic trends. Experiments\nconducted on TB incidence data from 47 prefectures in Japan demonstrate that\nour approach delivers robust and accurate predictions across multiple time\nhorizons (short to medium-term forecasts). Additionally, incorporating\nuncertainty quantification through conformal prediction enhances the model's\npractical utility for guiding targeted public health interventions."
    ],
    "c_categories":[
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.16822",
    "c_title":[
      "Deciphering the $G$(3900) quantum numbers $J^{PC}$ in $e^+e^-$\n  collisions"
    ],
    "c_abstract":[
      "Inspired by the BESIII observation of exotic hadron $G$(3900) [PRL\n133(2024)081901], we use the parton and hadron cascade model PACIAE to simulate\nthe $G$(3900) production in $e^+e^-$ annihilations at $\\sqrt s$=4.95 GeV. The\n$G$(3900) candidates are recombined by Dynamically Constrained Phase-space\nCoalescence model using component mesons of $D\\bar D$ or $D\\bar D^*\/\\bar DD^*$\nin the PACIAE simulated final hadronic state. We then calculate the $G$(3900)\norbital angular momentum quantum number in its rest frame for the first time\nand perform the spectral classification for each of the above $G$(3900)\ncandidates. Our results confirm the BESIII observation of $G$(3900) in the\n$e^+e^-\\rightarrow D^0\\bar D^0\/ D^+D^-$ processes and suggest that the\n$G$(3900) composed of $D\\bar D$ is $D$-wave dominant with $J^{PC}=2^{++}$.\nMoreover, our results indicate that the $G$(3900) composed of $D\\bar D^*\/\\bar\nDD^*$ is $P$-wave dominant with $J^{PC}=$ $0^{-+}$, $1^{-+}$ or $2^{-+}$, which\nis consistent with the conclusion in PRL 133(2024)241903. Finally, significant\ndiscrepancies in the yields, the transverse momentum spectra and the rapidity\ndistributions among the $G$(3900) $S$-wave, $P$-wave, and $D$-wave states are\nobserved. These discrepancies are proposed as valuable criteria for deciphering\nthe $G$(3900) orbital angular momentum quantum number and $J^{PC}$."
    ],
    "c_categories":[
      "hep-ph",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01375",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "Testing conditional independence of discrete distributions"
    ],
    "b_abstract":[
      "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
    ],
    "b_categories":[
      "stat.CO"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10282",
    "c_title":[
      "The impact of medium-width bands on the selection, and subsequent\n  luminosity function measurements, of high-z galaxies"
    ],
    "c_abstract":[
      "New, ultra-deep medium-width photometric coverage with JWST's NIRCam\ninstrument provides the potential for much improved photo-z reliability at high\nredshifts. In this study, we conduct a systematic analysis of the JADES Origins\nField, which contains 14 broad- and medium-width near-infrared bands, to assess\nthe benefits of medium band photometry on high-z sample completeness and\ncontamination rates. Using imaging with depths of AB mag $29.8-30.35$, we\nconduct an experiment to observe how high-z selections differ when images are\nartificially degraded or bands are removed. In parallel, the same experiments\nare conducted on simulated catalogues from the JAGUAR semi-analytic model to\nexamine if the behaviour from observations can be replicated. We find sample\ncompleteness is high ($80\\%+$) and contamination low ($<4\\%$) when in the\n$10\\sigma+$ regime, even without the use of any medium-width bands. The\naddition of medium-width bands leads to notable increases in completeness\n($\\sim10\\%$) but multiple bands are required to improve contamination rates due\nto the small redshift ranges over which they probe strong emission lines.\nIncidents of Balmer-Lyman degeneracy increase in the $5-7\\sigma$ regime and\nthis can be replicated in both simulated catalogues and degraded real data. We\nmeasure the faint-end of the UV LF at $8.5<z<13.5$, finding high number\ndensities that agree with previous JWST observations. Overall, medium bands are\neffective at increasing completeness and reducing contamination, but investment\nin achieving at least comparable depths in the blue ($<1.5\\mu$m) as achieved in\nthe red is also found to be key to fully reducing contamination from high-z\nsamples."
    ],
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"RoBo6: Standardized MMT Light Curve Dataset for Rocket Body\n  Classification",
    "a_abstract":"Space debris presents a critical challenge for the sustainability of future\nspace missions, emphasizing the need for robust and standardized identification\nmethods. However, a comprehensive benchmark for rocket body classification\nremains absent. This paper addresses this gap by introducing the RoBo6 dataset\nfor rocket body classification based on light curves. The dataset, derived from\nthe Mini Mega Tortora database, includes light curves for six rocket body\nclasses: CZ-3B, Atlas 5 Centaur, Falcon 9, H-2A, Ariane 5, and Delta 4. With\n5,676 training and 1,404 test samples, it addresses data inconsistencies using\nresampling, normalization, and filtering techniques. Several machine learning\nmodels were evaluated, including CNN and transformer-based approaches, with\nAstroconformer reporting the best performance. The dataset establishes a common\nbenchmark for future comparisons and advancements in rocket body classification\ntasks.",
    "explanation":"The selected references in Task 3 were based from the following topics introduced in the abstract:\n\n\"Space debris presents a critical challenge for the sustainability of future space missions, emphasizing the need for robust and standardized identification methods.\"\n\"Several machine learning models were evaluated, including\nCNN and transformer-based approaches, with  Astroconformer reporting the best performance\"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b5"
    ],
    "c_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "c_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.19324",
    "c_title":[
      "Anisotropy signal of UHECRs from a structured magnetized Universe"
    ],
    "c_abstract":[
      "The surprising isotropy of the ultra-high-energy cosmic ray (UHECR) sky makes\nit difficult to identify their sources. Observables such as energy spectrum,\nmass composition and arrival directions are affected by interactions with\nbackground photon fields and by deflection in the extragalactic and galactic\nmagnetic fields (EGMF and GMF). In this work, we simulate the propagation of\nUHECRs with energy above 8 EeV in magnetized replicas of the local Universe,\nobtained from constrained simulations of the Large Scale Structure. We obtain\nthe real magnetic deflection in structured EGMF models with realistic\nthree-dimensional simulations. We investigate different scenarios for the UHECR\nsource distributions and densities. The effect of the GMF can be different\ndepending on the field model considered. In this work we consider the JF12\nmodel by mapping the arrival directions at the edge of the galaxy to those at\nEarth. We study the arrival direction distribution of the propagated UHECRs,\nand in particular their angular power spectrum, dipole and quadrupole moments.\nWe find that the properties of the source distribution affect the cosmic ray\nanisotropy more than the EGMF model considered. In particular, the low\nmultipole components depend on both the source distribution and the density. We\nalso find that it is difficult to simultaneously reproduce the observed dipole\nand quadrupole values above EeV. In general, we predict too large a quadrupole\nstrength, incompatible with observations."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.00735",
    "c_title":[
      "Role of gravitational decoupling on theoretical insights of relativistic\n  massive compact stars in the mass gap"
    ],
    "c_abstract":[
      "Advancements in theoretical simulations of mass gap objects, particularly\nthose resulting from neutron star mergers and massive pulsars, play a crucial\nrole in addressing the challenges of measuring neutron star radii. In the light\nof this, we have conducted a comprehensive investigation of compact objects\n(CSs), revealing that while the distribution of black hole masses varies based\non formation mechanisms, they frequently cluster around specific values. For\ninstance, the masses observed in GW190814 $(23.2^{+1.1}_{-1.0} \\, M_{\\odot})$\nand GW200210 $(24.1^{+7.5}_{-4.6} M_{\\odot})$ exemplify this clustering. We\nemployed the gravitational decoupling approach within the framework of standard\ngeneral relativity and thus focusing on the strange star model. This model\nhighlights the effects of deformation adjusted by the decoupling constant and\nthe bag function. By analyzing the mass-radius limits of mass gap objects from\nneutron star mergers and massive pulsars, we can effectively constrain the free\nparameters in our model, allowing us to predict the radii and moments of\ninertia for these objects. The mass-radius ($M-R$) and mass-inertia ($M-I$)\nprofiles demonstrate the robustness of our models. It is shown that as the\ndecoupling constant $\\beta$ increases from 0 to 0.1 and the bag constant\n$\\mathcal{B}_g$ decreases from 70 $MeV\/fm^3$ to 55 $MeV\/fm^3$, the maximum mass\nreaches $M_{max} = 2.87 \\, M_\\odot$ with a radius of 11.20 km. In contrast, for\n$\\beta = 0$, the maximum mass is $M_{max} = 2.48 \\, M_\\odot$ with a radius of\n10.69 km. Similarly, it has been exhibited that as $\\beta$ decreases to 0, the\nmaximum mass peaks at $M_{max} = 2.95 M_\\odot$ for $\\mathcal{B}_g = 55\nMeV\/fm^3$ with a radius of 11.32 km. These results not only exceed the observed\nmasses of CSs but also correlate with recent findings from gravitational wave\nevents like GW190814 and GW200210."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.11679",
    "c_title":[
      "Gravitational Wave Decoupling in Retrograde Circumbinary Disks"
    ],
    "c_abstract":[
      "We present a study of the late-time interaction between supermassive black\nhole binaries and retrograde circumbinary disks during the period of\ngravitational wave-driven inspiral. While mergers in prograde disks have\nreceived extensive study, retrograde disks offer distinct dynamics that could\npromote mergers and produce unique observational signatures. Through numerical\nsimulations, we explore the process of binary-disk decoupling, where the\nbinary's orbital decay rate is faster than the disk's viscous response rate. We\nfind the point of decoupling to be comparable in prograde and retrograde disks,\nsuggesting that any associated electromagnetic (EM) signatures will be produced\nat comparable times preceding merger. However, we find smaller central cavities\nfor retrograde disks, likely leading to higher-frequency EM emissions and\nshorter post-merger rebrightening timescales compared to their prograde\ncounterparts. Additionally, we identify quasi-periodic flaring due to\ninstabilities unique to low-viscosity retrograde disks, which may produce\ndistinctive EM signatures."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.15537",
    "c_title":[
      "Discovery of a new phase-transient cyclotron line in A0535+26:\n  Constraints on the accretion geometry"
    ],
    "c_abstract":[
      "In November 2020, A0535+26 underwent one of its brightest outbursts, reaching\nnearly 12 Crab in X-ray flux. Observed by \\textit{Insight-HXMT},\n\\textit{NuSTAR}, \\textit{NICER}, and \\textit{AstroSat}, this event provided\nvaluable insights into Be\/X-ray binaries. The pulse profiles evolved\nsignificantly with luminosity, transitioning from pencil-beam to fan-beam\ngeometries. A0535+26, known for its fundamental cyclotron line at $\\sim$44 keV,\nbecame only the second source to exhibit a negative correlation between\ncyclotron line energy and flux at high luminosities, with a plateau phase\npreceding the transition from positive to negative correlation. We report the\ndiscovery of a phase-transient low-energy cyclotron line, detected in a narrow\nphase range ($\\sim$16\\%) across all seven \\textit{NuSTAR} observations during\nthe rising, peak, and declining phases of the outburst. The new line exhibited\ndramatic variations with pulse phase and luminosity. We explain this behavior\nusing an accretion geometry where the accretion column sweeps across the line\nof sight."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.18686",
    "c_title":[
      "The BTSbot-nearby discovery of SN 2024jlf: rapid, autonomous follow-up\n  probes interaction in an 18.5 Mpc Type IIP supernova"
    ],
    "c_abstract":[
      "We present observations of the Type IIP supernova (SN) 2024jlf, including\nspectroscopy beginning just 0.7 days ($\\sim$17 hours) after first light. Rapid\nfollow-up was enabled by the new $\\texttt{BTSbot-nearby}$ program, which\ninvolves autonomously triggering target-of-opportunity requests for new\ntransients in Zwicky Transient Facility data that are coincident with nearby\n($D<60$ Mpc) galaxies and identified by the $\\texttt{BTSbot}$ machine learning\nmodel. Early photometry and non-detections shortly prior to first light show\nthat SN 2024jlf initially brightened by $>$4 mag\/day, quicker than $\\sim$90% of\nType II SNe. Early spectra reveal weak flash ionization features: narrow,\nshort-lived ($1.3 < \\tau ~\\mathrm{[d]} < 1.8$) emission lines of H$\\alpha$, He\nII, and C IV. Assuming a wind velocity of $v_w=50$ km s$^{-1}$, these\nproperties indicate that the red supergiant progenitor exhibited enhanced\nmass-loss in the last year before explosion. We constrain the mass-loss rate to\n$10^{-4} < \\dot{M}~\\mathrm{[M_\\odot~yr^{-1}]} < 10^{-3}$ by matching\nobservations to model grids from two independent radiative hydrodynamics codes.\n$\\texttt{BTSbot-nearby}$ automation minimizes spectroscopic follow-up latency,\nenabling the observation of ephemeral early-time phenomena exhibited by\ntransients."
    ],
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.15517",
    "c_title":[
      "Analysis of AI Effectiveness in Reducing Human Errors in Processing\n  Transportation Requests"
    ],
    "c_abstract":[
      "This article examines the characteristics of human errors in processing\ntransportation requests. The role of artificial intelligence (AI) in maritime\ntransportation is explored. The main methods and technologies used for\nautomating and optimizing the handling of transportation requests are analyzed,\nalong with their impact on reducing the number of errors. Examples of\nsuccessful AI implementation in large companies are provided, confirming the\npositive influence of these technologies on overall operational efficiency and\ncustomer service levels."
    ],
    "c_categories":[
      "cs.AI",
      "cs.HC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.19640",
    "c_title":[
      "Mass Shootings, Community Mobility, and the Relocation of Economic\n  Activity"
    ],
    "c_abstract":[
      "Using foot traffic data for over 150,000 points of interest (POIs) near the\nsites of 42 mass shootings (2018-2022, U.S.), we evaluate the spatial-temporal\nimpact of the tragic events on community mobility and relocation of economic\nactivities. Visits to nearby POIs decrease, while farther away POIs experience\nincreased foot traffic, implying that communities shift their activities away\nfrom the shooting sites. The impact is stronger when stronger trauma responses\nare expected. Our results suggest that mass shootings drive significant\ndisplacements of economic activities and can consequently lead to welfare\nlosses due to distortions in optimal choices of time and location."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.15175",
    "c_title":[
      "Complements of the point schemes of noncommutative projective lines"
    ],
    "c_abstract":[
      "Recently, Chan and Nyman constructed noncommutative projective lines via a\nnoncommutative symmetric algebra for a bimodule $V$ over a pair of fields.\nThese noncommutative projective lines of contain a canonical closed subscheme\n(the point scheme) determined by a normal family of elements in the\nnoncommutative symmetric algebra. We study the complement of this subscheme\nwhen $V$ is simple, the coordinate ring of which is obtained by inverting said\nnormal family. We show that this localised ring is a noncommutative Dedekind\ndomain of Gelfand-Kirillov dimension 1. Furthermore, the question of simplicity\nof these Dedekind domains is answered by a similar dichotomy to an analogous\nopen subscheme of the noncommutative quadrics of Artin, Tate and Van den Bergh."
    ],
    "c_categories":[
      "math.AG",
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.13467",
    "c_title":[
      "Multi-Level Attention and Contrastive Learning for Enhanced Text\n  Classification with an Optimized Transformer"
    ],
    "c_abstract":[
      "This paper studies a text classification algorithm based on an improved\nTransformer to improve the performance and efficiency of the model in text\nclassification tasks. Aiming at the shortcomings of the traditional Transformer\nmodel in capturing deep semantic relationships and optimizing computational\ncomplexity, this paper introduces a multi-level attention mechanism and a\ncontrastive learning strategy. The multi-level attention mechanism effectively\nmodels the global semantics and local features in the text by combining global\nattention with local attention; the contrastive learning strategy enhances the\nmodel's ability to distinguish between different categories by constructing\npositive and negative sample pairs while improving the classification effect.\nIn addition, in order to improve the training and inference efficiency of the\nmodel on large-scale text data, this paper designs a lightweight module to\noptimize the feature transformation process and reduce the computational cost.\nExperimental results on the dataset show that the improved Transformer model\noutperforms the comparative models such as BiLSTM, CNN, standard Transformer,\nand BERT in terms of classification accuracy, F1 score, and recall rate,\nshowing stronger semantic representation ability and generalization\nperformance. The method proposed in this paper provides a new idea for\nalgorithm optimization in the field of text classification and has good\napplication potential and practical value. Future work will focus on studying\nthe performance of this model in multi-category imbalanced datasets and\ncross-domain tasks and explore the integration wi"
    ],
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b10"
    ],
    "b_title":[
      "Attention Is All You Need"
    ],
    "b_abstract":[
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
    ],
    "b_categories":[
      "cond-mat.dis-nn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.09178",
    "c_title":[
      "Enhancing Graph Representation Learning with Localized Topological\n  Features"
    ],
    "c_abstract":[
      "Representation learning on graphs is a fundamental problem that can be\ncrucial in various tasks. Graph neural networks, the dominant approach for\ngraph representation learning, are limited in their representation power.\nTherefore, it can be beneficial to explicitly extract and incorporate\nhigh-order topological and geometric information into these models. In this\npaper, we propose a principled approach to extract the rich connectivity\ninformation of graphs based on the theory of persistent homology. Our method\nutilizes the topological features to enhance the representation learning of\ngraph neural networks and achieve state-of-the-art performance on various node\nclassification and link prediction benchmarks. We also explore the option of\nend-to-end learning of the topological features, i.e., treating topological\ncomputation as a differentiable operator during learning. Our theoretical\nanalysis and empirical study provide insights and potential guidelines for\nemploying topological features in graph learning tasks."
    ],
    "c_categories":[
      "cs.LG",
      "cs.SI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.16219",
    "c_title":[
      "Scaling of many-body localization transitions: Quantum dynamics in Fock\n  space and real space"
    ],
    "c_abstract":[
      "Many-body-localization (MBL) transitions are studied in a family of\nsingle-spin-flip spin-$\\frac12$ models, including the one-dimensional (1D)\nchain with nearest-neighbor interactions, the quantum dot (QD) model with\nall-to-all pair interactions, and the quantum random energy model (QREM). We\ninvestigate the generalized imbalance that characterizes propagation in Fock\nspace out of an initial basis state and, at the same time, can be efficiently\nprobed by real-space measurements. For all models considered, the average\nimbalance and its quantum and mesoscopic fluctuations provide excellent\nindicators for the position of the MBL transition $W_c(n)$, where $n$ is the\nnumber of spins. Combining these findings with earlier results on level\nstatistics, we determine phase diagrams of the MBL transitions in the $n$-$W$\nplane. Our results provide evidence for a direct transition between the ergodic\nand MBL phases for each of the models, without any intermediate phase. For QREM\nand QD model, $W_c(n)$ grows as a power law of $n$ (with logarithmic\ncorrections), in agreement with analytical predictions $W_c^{\\rm QREM}(n) \\sim\nn^{1\/2} \\ln n$ and $W_c^{\\rm QD}(n) \\gtrsim n^{3\/4} \\ln^{1\/2} n$. This growth\nis in stark contrast to the 1D model, where $W_c(n)$ is essentially independent\nof $n$, consistent with the analytic expectation $W_c^{\\rm 1D}(n\\to \\infty)=\n{\\rm const}$. We also determine the scaling of the transition width $\\Delta W\n(n) \/ W_c(n)$ and estimate the system size $n$ needed to study the asymptotic\nscaling behavior. While these values of $n$ are not accessible to exact\nsimulations on a classical computer, they are within the reach of quantum\nsimulators. Our results indicate feasibility of experimental studies of $n$-$W$\nphase diagrams and scaling properties of MBL transitions in models of 1D and QD\ntype and in their extensions to other spatial geometry or distance-dependent\ninteractions."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.18925",
    "c_title":[
      "Electrical conductivity of conductive films based on random metallic\n  nanowire networks"
    ],
    "c_abstract":[
      "Using computer simulation, we investigated the dependence of the electrical\nconductivity of random two-dimensional systems of straight nanowires on the\nmain parameters. Both the resistance of the conductors and the resistance of\nthe contacts between them were taken into account. The dependence of the\nresistance, $R$, between network nodes on the distance between nodes, $r$, is\n$R(r) = R_\\Box\/\\pi \\ln r + \\mathrm{const}$, where $R_\\Box$ is the sheet\nresistance."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.00241",
    "c_title":[
      "Accuracy and capacity of Modern Hopfield networks with synaptic noise"
    ],
    "c_abstract":[
      "We study the retrieval accuracy and capacity of modern Hopfield networks of\nwith two-state (Ising) spins interacting via modified Hebbian $n$-spin\ninteractions. In particular, we consider systems where the interactions deviate\nfrom the Hebb rule through additive or multiplicative noise or through clipping\nor deleting interactions. We find that the capacity scales as $N^{n-1}$ with\nthe number of spins $N$ in all cases, but with a prefactor reduced compared to\nthe Hebbian case. For $n=2$ our results agree with the previously known results\nfor the conventional $n = 2$ Hopfield network."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.07796",
    "c_title":[
      "Topological mechanical neural networks as classifiers through in situ\n  backpropagation learning"
    ],
    "c_abstract":[
      "Recently, a new frontier in computing has emerged with physical neural\nnetworks(PNNs) harnessing intrinsic physical processes for learning. Here, we\nexplore topological mechanical neural networks(TMNNs) inspired by the quantum\nspin Hall effect(QSHE) in topological metamaterials, for machine learning\nclassification tasks. TMNNs utilize pseudospin states and the robustness of the\nQSHE, making them damage-tolerant for binary classification. We first\ndemonstrate data clustering using untrained TMNNs. Then, for specific tasks, we\nderive an in situ backpropagation algorithm - a two-step, local-rule method\nthat updates TMNNs using only local information, enabling in situ physical\nlearning. TMNNs achieve high accuracy in classifications of Iris flowers,\nPenguins, and Seeds while maintaining robustness against bond pruning.\nFurthermore, we demonstrate parallel classification via frequency-division\nmultiplexing, assigning different tasks to distinct frequencies for enhanced\nefficiency. Our work introduces in situ backpropagation for wave-based\nmechanical neural networks and positions TMNNs as promising neuromorphic\ncomputing hardware for classification tasks."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.04443",
    "c_title":[
      "On the nature of the glass transition in metallic glasses after deep\n  relaxation"
    ],
    "c_abstract":[
      "We performed parallel study of calorimetric and high-frequency shear modulus\nbehavior of Zr-based metallic glasses after deep relaxation just below the\nglass transition. It is shown that deep relaxation results in the appearance of\na strong peak of the excess heat capacity while the shear modulus is moderately\naffected. A theory assuming high-frequency shear modulus to be a major physical\nparameter controlling glass relaxation is suggested. The energy barrier for\nthese rearrangements is proportional to the shear modulus while its magnitude,\nin turn, varies due to the changes in the defect concentration (diaelastic\neffect). Both dependences lead to the occurrence of heat effects. The excess\nheat capacity calculated using experimental shear modulus data demonstrates\nvery good agreement with the experimental calorimetric data for all states of\nglasses. It is argued that the glass transition behavior after deep relaxation\nof glass is close to a phase transition of the first kind."
    ],
    "c_categories":[
      "cond-mat.dis-nn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.16504",
    "c_title":[
      "Open-Source Tool for Evaluating Human-Generated vs. AI-Generated Medical\n  Notes Using the PDQI-9 Framework"
    ],
    "c_abstract":[
      "Background: The increasing use of artificial intelligence (AI) in healthcare\ndocumentation necessitates robust methods for evaluating the quality of\nAI-generated medical notes compared to those written by humans. This paper\nintroduces an open-source tool, the Human Notes Evaluator, designed to assess\nclinical note quality and differentiate between human and AI authorship.\nMethods: The Human Notes Evaluator is a Flask-based web application implemented\non Hugging Face Spaces. It employs the Physician Documentation Quality\nInstrument (PDQI-9), a validated 9-item rubric, to evaluate notes across\ndimensions such as accuracy, thoroughness, clarity, and more. The tool allows\nusers to upload clinical notes in CSV format and systematically score each note\nagainst the PDQI-9 criteria, as well as assess the perceived origin (human, AI,\nor undetermined). Results: The Human Notes Evaluator provides a user-friendly\ninterface for standardized note assessment. It outputs comprehensive results,\nincluding individual PDQI-9 scores for each criterion, origin assessments, and\noverall quality metrics. Exportable data facilitates comparative analyses\nbetween human and AI-generated notes, identification of quality trends, and\nareas for documentation improvement. The tool is available online at\nhttps:\/\/huggingface.co\/spaces\/iyadsultan\/human_evaluator . Discussion: This\nopen-source tool offers a valuable resource for researchers, healthcare\nprofessionals, and AI developers to rigorously evaluate and compare the quality\nof medical notes. By leveraging the PDQI-9 framework, it provides a structured\nand reliable approach to assess clinical documentation, contributing to the\nresponsible integration of AI in healthcare. The tool's availability on Hugging\nFace promotes accessibility and collaborative development in the field of\nAI-driven medical documentation."
    ],
    "c_categories":[
      "cs.HC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.01944",
    "c_title":[
      "Protecting DeFi Platforms against Non-Price Flash Loan Attacks"
    ],
    "c_abstract":[
      "Smart contracts in Decentralized Finance (DeFi) platforms are attractive\ntargets for attacks as their vulnerabilities can lead to massive amounts of\nfinancial losses. Flash loan attacks, in particular, pose a major threat to\nDeFi protocols that hold a Total Value Locked (TVL) exceeding \\$106 billion.\nThese attacks use the atomicity property of blockchains to drain funds from\nsmart contracts in a single transaction. While existing research primarily\nfocuses on price manipulation attacks, such as oracle manipulation, mitigating\nnon-price flash loan attacks that often exploit smart contracts' zero-day\nvulnerabilities remains largely unaddressed. These attacks are challenging to\ndetect because of their unique patterns, time sensitivity, and complexity. In\nthis paper, we present FlashGuard, a runtime detection and mitigation method\nfor non-price flash loan attacks. Our approach targets smart contract function\nsignatures to identify attacks in real-time and counterattack by disrupting the\nattack transaction atomicity by leveraging the short window when transactions\nare visible in the mempool but not yet confirmed. When FlashGuard detects an\nattack, it dispatches a stealthy dusting counterattack transaction to miners to\nchange the victim contract's state which disrupts the attack's atomicity and\nforces the attack transaction to revert. We evaluate our approach using 20\nhistorical attacks and several unseen attacks. FlashGuard achieves an average\nreal-time detection latency of 150.31ms, a detection accuracy of over 99.93\\%,\nand an average disruption time of 410.92ms. FlashGuard could have potentially\nrescued over \\$405.71 million in losses if it were deployed prior to these\nattack instances. FlashGuard demonstrates significant potential as a DeFi\nsecurity solution to mitigate and handle rising threats of non-price flash loan\nattacks."
    ],
    "c_categories":[
      "cs.CR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.02852",
    "c_title":[
      "Two-type continuous-state branching processes in varying environments"
    ],
    "c_abstract":[
      "A basic class of two-type continuous-state branching processes in varying\nenvironments are constructed by solving the backward equation determining the\ncumulant semigroup. The parameters of the process are allowed to be c\\`adl\\`ag\nin time and the difficulty brought about by the bottlenecks are overcome by\nintroducing a suitable moment condition."
    ],
    "c_categories":[
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.05588",
    "c_title":[
      "Enforcing Fundamental Relations via Adversarial Attacks on Input\n  Parameter Correlations"
    ],
    "c_abstract":[
      "Correlations between input parameters play a crucial role in many scientific\nclassification tasks, since these are often related to fundamental laws of\nnature. For example, in high energy physics, one of the common deep learning\nuse-cases is the classification of signal and background processes in particle\ncollisions. In many such cases, the fundamental principles of the correlations\nbetween observables are often better understood than the actual distributions\nof the observables themselves. In this work, we present a new adversarial\nattack algorithm called Random Distribution Shuffle Attack (RDSA), emphasizing\nthe correlations between observables in the network rather than individual\nfeature characteristics. Correct application of the proposed novel attack can\nresult in a significant improvement in classification performance -\nparticularly in the context of data augmentation - when using the generated\nadversaries within adversarial training. Given that correlations between input\nfeatures are also crucial in many other disciplines. We demonstrate the RDSA\neffectiveness on six classification tasks, including two particle collision\nchallenges (using CERN Open Data), hand-written digit recognition (MNIST784),\nhuman activity recognition (HAR), weather forecasting (Rain in Australia), and\nICU patient mortality (MIMIC-IV), demonstrating a general use case beyond\nfundamental physics for this new type of adversarial attack algorithms."
    ],
    "c_categories":[
      "cs.LG",
      "hep-ex"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00544",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b5"
    ],
    "b_title":[
      "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
    ],
    "b_abstract":[
      "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
    ],
    "b_categories":[
      "astro-ph.HE"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.10421",
    "c_title":[
      "Towards Constraint-Based Adaptive Hypergraph Learning for Solving\n  Vehicle Routing: An End-to-End Solution"
    ],
    "c_abstract":[
      "The application of learning based methods to vehicle routing problems has\nemerged as a pivotal area of research in combinatorial optimization. These\nproblems are characterized by vast solution spaces and intricate constraints,\nmaking traditional approaches such as exact mathematical models or heuristic\nmethods prone to high computational overhead or reliant on the design of\ncomplex heuristic operators to achieve optimal or near optimal solutions.\nMeanwhile, although some recent learning-based methods can produce good\nperformance for VRP with straightforward constraint scenarios, they often fail\nto effectively handle hard constraints that are common in practice. This study\nintroduces a novel end-to-end framework that combines constraint-oriented\nhypergraphs with reinforcement learning to address vehicle routing problems. A\ncentral innovation of this work is the development of a constraint-oriented\ndynamic hyperedge reconstruction strategy within an encoder, which\nsignificantly enhances hypergraph representation learning. Additionally, the\ndecoder leverages a double-pointer attention mechanism to iteratively generate\nsolutions. The proposed model is trained by incorporating asynchronous\nparameter updates informed by hypergraph constraints and optimizing a dual loss\nfunction comprising constraint loss and policy gradient loss. The experiment\nresults on benchmark datasets demonstrate that the proposed approach not only\neliminates the need for sophisticated heuristic operators but also achieves\nsubstantial improvements in solution quality."
    ],
    "c_categories":[
      "cs.LG",
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"Localized KBO with genetic dynamics for multi-modal optimization",
    "a_abstract":"In this paper, we introduce a novel approach to multi-modal optimization by\nenhancing the recently developed kinetic-based optimization (KBO) method with\ngenetic dynamics (GKBO). The proposed method targets objective functions with\nmultiple global minima, addressing a critical need in fields like engineering\ndesign, machine learning, and bioinformatics. By incorpo rating leader-follower\ndynamics and localized interactions, the algorithm efficiently navigates\nhigh-dimensional search spaces to detect multiple optimal solutions. After\nproviding a binary description, a mean-field approximation is derived, and\ndifferent numerical experiments are conducted to validate the results.",
    "explanation":"This IDR paper involves a combination of topics from different fields. In particular, I highlighted key references in Task 3 where topics in Genetics and the use of Data Structures and Algorithms come into play. Below are some sentences from the Abstract that reflect that:\n\n\"n this paper, we introduce a novel approach to multi-modal optimization by enhancing the recently developed kinetic-based optimization (KBO) method with genetic dynamics (GKBO)\"\n\" By incorporating leader-follower dynamics and localized interactions, the algorithm efficiently navigates high-dimensional search spaces to detect multiple optimal solution.\"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b25"
    ],
    "c_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "c_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "c_categories":[
      "cs.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"basic"
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.02415",
    "c_title":[
      "On the sensitivity of CDAWG-grammars"
    ],
    "c_abstract":[
      "The compact directed acyclic word graphs (CDAWG) [Blumer et al. 1987] of a\nstring is the minimal compact automaton that recognizes all the suffixes of the\nstring. CDAWGs are known to be useful for various string tasks including text\npattern searching, data compression, and pattern discovery. The CDAWG-grammar\n[Belazzougui & Cunial 2017] is a grammar-based text compression based on the\nCDAWG. In this paper, we prove that the CDAWG-grammar size $g$ can increase by\nat most an additive factor of $4e + 4$ than the original after any\nsingle-character edit operation is performed on the input string, where $e$\ndenotes the number of edges in the corresponding CDAWG before the edit."
    ],
    "c_categories":[
      "cs.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.18425",
    "c_title":[
      "Faster Construction of a Planar Distance Oracle with \\~{O}(1) Query Time"
    ],
    "c_abstract":[
      "We show how to preprocess a weighted undirected $n$-vertex planar graph in\n$\\tilde O(n^{4\/3})$ time, such that the distance between any pair of vertices\ncan then be reported in $\\tilde O(1)$ time. This improves the previous $\\tilde\nO(n^{3\/2})$ preprocessing time [JACM'23].\n  Our main technical contribution is a near optimal construction of\n\\emph{additively weighted Voronoi diagrams} in undirected planar graphs.\nNamely, given a planar graph $G$ and a face $f$, we show that one can\npreprocess $G$ in $\\tilde O(n)$ time such that given any weight assignment to\nthe vertices of $f$ one can construct the additively weighted Voronoi diagram\nof $f$ in near optimal $\\tilde O(|f|)$ time. This improves the $\\tilde\nO(\\sqrt{n |f|})$ construction time of [JACM'23]."
    ],
    "c_categories":[
      "cs.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.11999",
    "c_title":[
      "Algorithm Engineering of SSSP With Negative Edge Weights"
    ],
    "c_abstract":[
      "Computing shortest paths is one of the most fundamental algorithmic graph\nproblems. It is known since decades that this problem can be solved in\nnear-linear time if all weights are nonnegative. A recent break-through by\n[Bernstein, Nanongkai, Wulff-Nilsen '22] presented a randomized near-linear\ntime algorithm for this problem. A subsequent improvement in [Bringmann,\nCassis, Fischer '23] significantly reduced the number of logarithmic factors\nand thereby also simplified the algorithm. It is surprising and exciting that\nboth of these algorithms are combinatorial and do not contain any fundamental\nobstacles for being practical.\n  We launch the, to the best of our knowledge, first extensive investigation\ntowards a practical implementation of [Bringmann, Cassis, Fischer '23]. To this\nend, we give an accessible overview of the algorithm, discussing what adaptions\nare necessary to obtain a fast algorithm in practice. We manifest these\nadaptions in an efficient implementation. We test our implementation on a\nbenchmark data set that is adapted to be more difficult for our implementation\nin order to allow for a fair comparison. As in [Bringmann, Cassis, Fischer '23]\nas well as in our implementation there are multiple parameters to tune, we\nempirically evaluate their effect and thereby determine the best choices. Our\nimplementation is then extensively compared to one of the state-of-the-art\nalgorithms for this problem [Goldberg, Radzik '93]. On the hardest instance\ntype, we are faster by up to almost two orders of magnitude."
    ],
    "c_categories":[
      "cs.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.01163",
    "c_title":[
      "Minimum Riesz s-Energy Subset Selection in Ordered Point Sets via\n  Dynamic Programming"
    ],
    "c_abstract":[
      "We present a dynamic programming algorithm for selecting a representative\nsubset of size $k$ from a given set with $n$ points such that the Riesz\n$s$-energy is near minimized. While NP-hard in general dimensions, the\none-dimensional case can use the natural data ordering for efficient dynamic\nprogramming as an effective heuristic solution approach. This approach is then\nextended to problems related to two-dimensional Pareto front representations\narising in biobjective optimization problems. Under the assumption of sorted\n(or non-dominated) input, the method typically yields near-optimal solutions in\nmost cases. We also show that the approach avoids mistakes of greedy\nsubset-selection by means of example. However, as we demonstrate, there are\nexceptions where DP does not identify the global minimum; for example, in one\nof our examples, the DP solution slightly deviates from the configuration found\nby a brute-force search. This is because the DP scheme's recurrence is\napproximate. The total time complexity of our algorithm is shown to be $O(n^2\nk)$. We provide computational examples with discontinuous Pareto fronts and an\nopen-source Python implementation, demonstrating the approximate DP algorithm's\neffectiveness across various problems with large point sets."
    ],
    "c_categories":[
      "cs.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.14446",
    "c_title":[
      "LEIT-motifs: Scalable Motif Mining in Multidimensional Time Series"
    ],
    "c_abstract":[
      "Time series play a fundamental role in many domains, capturing a plethora of\ninformation about the underlying data-generating processes. When a process\ngenerates multiple synchronized signals we are faced with multidimensional time\nseries. In this context a fundamental problem is that of motif mining, where we\nseek patterns repeating twice with minor variations, spanning some of the\ndimensions. State of the art exact solutions for this problem run in time\nquadratic in the length of the input time series.\n  We provide a scalable method to find the top-k motifs in multidimensional\ntime series with probabilistic guarantees on the quality of the results. Our\nalgorithm runs in time subquadratic in the length of the input, and returns the\nexact solution with probability at least $1-\\delta$, where $\\delta$ is a\nuser-defined parameter. The algorithm is designed to be adaptive to the input\ndistribution, self-tuning its parameters while respecting user-defined limits\non the memory to use.\n  Our theoretical analysis is complemented by an extensive experimental\nevaluation, showing that our algorithm is orders of magnitude faster than the\nstate of the art."
    ],
    "c_categories":[
      "cs.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03196",
    "c_title":[
      "When Do Voters Stop Caring? Estimating the Shape of Voter Utility\n  Function"
    ],
    "c_abstract":[
      "In this paper, we address a longstanding puzzle over the functional form that\nbetter approximates voter's political utility. Though it has become the norm in\nthe literature to represent the voters' political utility with concave loss\nfunctions, for decades scholars have underscored this assumption's potential\nshortcomings. Yet there exists little to no evidence to support one functional\nform assumption over another. We fill this gap by first identifying electoral\nsettings where the different functional forms generate divergent predictions\nabout voter behavior. Then, we assess which functional form better matches\nobserved voter and abstention behavior using Cast Vote Record (CVR) data that\ncaptures the anonymized ballots of millions of voters in the 2020 U.S. general\nelection. Our findings indicate that concave loss functions fail to predict\nvoting and abstention behavior and it is the reverse S-shaped loss functions,\nsuch as the Gaussian function, that better match the observed voter behavior."
    ],
    "c_categories":[
      "econ.GN",
      "q-fin.EC"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.18646",
    "c_title":[
      "Spatial Analysis of Neuromuscular Junctions Activation in\n  Three-Dimensional Histology-based Muscle Reconstructions"
    ],
    "c_abstract":[
      "Histology has long been a foundational technique for studying anatomical\nstructures through tissue slicing. Advances in computational methods now enable\nthree dimensional (3D) reconstruction of organs from histology images,\nenhancing the analysis of structural and functional features. Here, we present\na novel multimodal computational method to reconstruct rodent muscles in 3D\nusing classical image processing and data analysis techniques, analyze their\nstructural features and correlate them to previously recorded\nelectrophysiological data. The algorithm analyzes spatial distribution patterns\nof features identified through histological staining, normalizing them across\nmultiple samples. Further, the algorithm successfully correlates spatial\npatterns with high density epimysial ElectroMyoGraphy (hdEMG) recordings,\nproviding a multimodal perspective on neuromuscular dynamics, linking spatial\nand electrophysiological information. The code was validated by looking at the\ndistribution of NeuroMuscular Junctions (NMJs) in naive soleus muscles and\ncompared the distributions and patterns observed with ones observed in previous\nliterature. Our results showed consistency with the expected results,\nvalidating our method for features and pattern recognition. The multimodal\naspect was shown in a naive soleus muscle, where a strong correlation was found\nbetween motor unit locations derived via hdEMG, and NMJ locations obtained from\nhistology, highlighting their spatial relationship. This multimodal analysis\ntool integrates 3D structural data with electrophysiological activity, opening\nnew avenues in muscle diagnostics, regenerative medicine, and personalized\ntherapies where spatial insights could one day predict electrophysiological\nbehavior or vice versa."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.06323",
    "c_title":[
      "Towards An Updated Simulation of the Booster Neutrino Beam"
    ],
    "c_abstract":[
      "Precise, accurate neutrino flux predictions for neutrino beam experiments are\ncrucial for physics results. Flux predictions for the Booster Neutrino Beam at\nFermilab were first published in 2009 by the MiniBooNE collaboration. It is no\nlonger possible to run the simulation used to create these predictions due to\noutdated software versions. In this exciting period for the Short-Baseline\nNeutrino Program at Fermilab, with both the far detector (ICARUS) and the near\ndetector (SBND) operating, an updated flux model for the BNB is necessary. For\nthe purpose of using a dynamic, up-to-date beam model, the SBN program has\ncreated a new simulation named G4BNB. This framework contains new features,\nsuch as a full neutrino ancestry to handle hadron production systematics with\nmore precision, and the inclusion of neutral mesons from the proton-beryllium\nscatter to allow the study of exotic BSM scenarios."
    ],
    "c_categories":[
      "hep-ex",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.13391",
    "c_title":[
      "Black holes inside cosmic voids"
    ],
    "c_abstract":[
      "This study examines the gravitational and thermodynamic properties of static,\nspherically symmetric black holes within cosmic voids -- vast underdense\nregions of the universe. By deriving a novel solution based on a universal\ndensity profile for voids, we analyze its spacetime structure, which reveals\ntwo horizons: One of the black hole and the other related to the de Sitter-like\nbehavior. As the void approaches a perfect vacuum, the black hole horizon\ndiminishes, tending to that of the Schwarzschild solution, while the outer\nhorizon increases. We also study the solution stability via sound speed of the\nfluid, as well as the thermodynamic properties, including Hawking temperature,\nevaporation time, entropy, and specific heat. Our results show that as the void\nempties, the Hawking temperature rises, shortening evaporation times. The\nentropy follows the area's law and specific heat exhibits a minimum for a given\nblack hole size, indicating a thermal transition and highlighting the role of\nvoids in the black hole evolution. These findings offer new insights into the\nrelationship between local gravitational collapse and large-scale cosmic\nstructure, enhancing our understanding of the black hole behavior in underdense\nenvironments. We also provide a glimpse of a potential thermodynamic\ninteraction between the event horizon and the cosmological horizon."
    ],
    "c_categories":[
      "gr-qc"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Kinetic description and convergence analysis of genetic algorithms for global optimization"
    ],
    "b_abstract":[
      "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
    ],
    "b_categories":[
      "math.OC"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.02456",
    "c_title":[
      "Inferring Galactic Parameters from Chemical Abundances with\n  Simulation-Based Inference"
    ],
    "c_abstract":[
      "Galactic chemical abundances provide crucial insights into fundamental\ngalactic parameters, such as the high-mass slope of the initial mass function\n(IMF) and the normalization of Type Ia supernova (SN Ia) rates. Constraining\nthese parameters is essential for advancing our understanding of stellar\nfeedback, metal enrichment, and galaxy formation processes. However,\ntraditional Bayesian inference techniques, such as Hamiltonian Monte Carlo\n(HMC), are computationally prohibitive when applied to large datasets of modern\nstellar surveys. We leverage simulation-based-inference (SBI) as a scalable,\nrobust, and efficient method for constraining galactic parameters from stellar\nchemical abundances and demonstrate its the advantages over HMC in terms of\nspeed, scalability, and robustness against model misspecifications. We combine\na Galactic Chemical Evolution (GCE) model, CHEMPY, with a neural network\nemulator and a Neural Posterior Estimator (NPE) to train our SBI pipeline. Mock\ndatasets are generated using CHEMPY, including scenarios with mismatched\nnucleosynthetic yields, with additional tests conducted on data from a\nsimulated Milky Way-like galaxy. SBI results are benchmarked against HMC-based\ninference, focusing on computational performance, accuracy, and resilience to\nsystematic discrepancies. SBI achieves a $\\sim75,600\\times$ speed-up compared\nto HMC, reducing inference runtime from $\\gtrsim42$ hours to mere seconds for\nthousands of stars. Inference on $1,000$ stars yields precise estimates for the\nIMF slope ($\\alpha_{\\rm IMF} = -2.298 \\pm 0.002$) and SN Ia normalization\n($\\log_{10}(N_{\\rm Ia}) = -2.885 \\pm 0.003$), deviating less than 0.05% from\nthe ground truth. SBI also demonstrates similar robustness to model\nmisspecification than HMC, recovering accurate parameters even with alternate\nyield tables or data from a cosmological simulation. (shortened...)"
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.IM",
      "physics.comp-ph",
      "physics.data-an",
      "physics.space-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15419",
    "c_title":[
      "A primal-dual interior point trust region method for\n  inequality-constrained optimization problems on Riemannian manifolds"
    ],
    "c_abstract":[
      "We consider Riemannian inequality-constrained optimization problems and\npropose a Riemannian primal-dual interior point trust region method (RIPTRM)\nfor solving them. We prove its global convergence to an approximate\nKarush-Kuhn-Tucker point and a second-order stationary point. We also establish\nthe local near-quadratic convergence. To the best of our knowledge, this is the\nfirst algorithm that incorporates the trust region strategy and has the\nsecond-order convergence property for optimization problems on Riemannian\nmanifolds with nonlinear inequality constraints. It is also the first\nRiemannian interior point method that possesses both global and local\nconvergence properties. We conduct numerical experiments in which we introduce\na truncated conjugate gradient method and an eigenvalue-based subsolver for\nRIPTRM to approximately and exactly solve the trust region subproblems,\nrespectively. Empirical results show that RIPTRMs find solutions with higher\naccuracy compared to an existing Riemannian interior point method and other\nalgorithms. Additionally, we observe that RIPTRM with the exact search\ndirection shows significantly promising performance in an instance where the\nHessian of the Lagrangian has a large negative eigenvalue."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10482",
    "c_title":[
      "A Rank-One-Update Method for the Training of Support Vector Machines"
    ],
    "c_abstract":[
      "This paper considers convex quadratic programs\n  associated with the training of support vector machines (SVM).\n  Exploiting the special structure of the SVM problem a new\n  type of active set method with long cycles and stable rank-one-updates\n  is proposed and tested (CMU: cycling method with updates).\n  The structure of the problem allows for a repeated simple increase\n  of the set of inactive constraints while controlling its size. This is\n  followed by minimization steps with cheap updates of a matrix factorization.\n  A widely used approach for solving SVM problems is the\n  alternating direction method SMO,\n  a method that is very efficient for low accuracy.\n  The new active set approach allows for higher accuracy\n  results at moderate computational cost. To relate both approaches,\n  the effect of the accuracy on the running time and on the\n  predictive quality of the SVM is compared with some numerical examples.\n  A surprising result of the numerical examples is that only a\n  very small number of cycles (each consisting of less than 2n\n  steps) was used for CMU."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01110",
    "c_title":[
      "Steepest Descent Algorithm for M-convex Function Minimization Using Long\n  Step Length"
    ],
    "c_abstract":[
      "We consider the minimization of an M-convex function, which is a discrete\nconvexity concept for functions on the integer lattice points. It is known that\na minimizer of an Mconvex function can be obtained by the steepest descent\nalgorithm. In this paper, we propose an effective use of long step length in\nthe steepest descent algorithm, aiming at the reduction in the running time. In\nparticular, we obtain an improved time bound by using long step length. We also\nconsider the constrained M-convex function minimization and show that long step\nlength can be applied to a variant of steepest descent algorithm as well."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01430",
    "c_title":[
      "A strong second-order sequential optimality condition for nonlinear\n  programming problems"
    ],
    "c_abstract":[
      "Most numerical methods developed for solving nonlinear programming problems\nare designed to find points that satisfy certain optimality conditions. While\nthe Karush-Kuhn-Tucker conditions are well-known, they become invalid when\nconstraint qualifications (CQ) are not met. Recent advances in sequential\noptimality conditions address this limitation in both first- and second-order\ncases, providing genuine optimality guarantees at local optima, even when CQs\ndo not hold. However, some second-order sequential optimality conditions still\nrequire some restrictive conditions on constraints in the recent literature. In\nthis paper, we propose a new strong second-order sequential optimality\ncondition without CQs. We also show that a penalty-type method and an augmented\nLagrangian method generate points satisfying these new optimality conditions."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10384",
    "c_title":[
      "Stochastic Gradient Descent for Constrained Optimization based on\n  Adaptive Relaxed Barrier Functions"
    ],
    "c_abstract":[
      "This paper presents a novel stochastic gradient descent algorithm for\nconstrained optimization. The proposed algorithm randomly samples constraints\nand components of the finite sum objective function and relies on a relaxed\nlogarithmic barrier function that is appropriately adapted in each optimization\niteration. For a strongly convex objective function and affine inequality\nconstraints, step-size rules and barrier adaptation rules are established that\nguarantee asymptotic convergence with probability one. The theoretical results\nin the paper are complemented by numerical studies which highlight potential\nadvantages of the proposed algorithm for optimization problems with a large\nnumber of constraints."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.18774",
    "c_title":[
      "(FAPP) Infinity Does Macroscopic Irreversibility From Microscopic\n  Reversibility"
    ],
    "c_abstract":[
      "Infinity is central to deriving macroscopic irreversibility from reversible\nmicroscopic laws across mathematics, theoretical computer science and physics.\nIn analysis, infinite processes -- such as Dedekind cuts and Cauchy sequences\n-- construct real numbers as equivalence classes of rational approximations,\nbridging discrete rationals to the continuous real line. In quantum mechanics,\ninfinite tensor products model nested measurements, where sectorization\npartitions the Hilbert space into equivalence classes, reconciling unitary\nevolution with wavefunction collapse. In statistical mechanics, macrostates\nemerge as equivalence classes of microstates sharing identical macroscopic\nproperties, providing the statistical basis for thermodynamic irreversibility\ndespite reversible dynamics. Equivalence relations formalize\nFor-All-Practical-Purposes (FAPP) indistinguishability, reflecting operational\nlimits on precision and observation. Together, these examples reveal a unified\nframework where infinity and equivalence underpin emergent macroscopic behavior\nfrom microscopic reversibility."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09670",
    "c_title":[
      "A Simplification Method for Inequality Constraints in Integer Binary\n  Encoding HOBO Formulations"
    ],
    "c_abstract":[
      "This study proposes a novel method for simplifying inequality constraints in\nHigher-Order Binary Optimization (HOBO) formulations. The proposed method\naddresses challenges associated with Quadratic Unconstrained Binary\nOptimization (QUBO) formulations, specifically the increased computational\ncomplexity and reduced solution accuracy caused by the introduction of slack\nvariables and the resulting growth in auxiliary qubits. By efficiently\nintegrating constraints, the method enhances the computational efficiency and\naccuracy of both quantum and classical solvers. The effectiveness of the\nproposed approach is demonstrated through numerical experiments applied to\ncombinatorial optimization problems. The results indicate that this method\nexpands the applicability of quantum algorithms to high-dimensional problems\nand improves the practicality of classical optimization solvers for\noptimization problems involving inequality constraints."
    ],
    "c_categories":[
      "math.OC",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.01388",
    "c_title":[
      "Quasinormal Modes and the Switchback Effect in Schwarzschild-de Sitter"
    ],
    "c_abstract":[
      "We study the causal structure of Schwarzschild-de Sitter (SdS), including\nshock wave perturbations, in $D>3$ using reflected null ray trajectories,\neither through the interior black hole or the exterior de Sitter region.\nSpecifically, we compute the quasinormal mode frequencies in the eikonal,\nhigh-frequency, limit, by identifying the `critical time', for arbitrary values\nof the black hole mass. We emphasize the important role of the static sphere\nproper time normalization and related boundary conditions. The computed\ncritical times indicate the presence of singularities in the late-time, large\nmass, scalar field correlator in SdS, which should be resolved by introducing\ncomplex geodesics consistent with interior black hole and exterior de Sitter\neffective thermofield double states. In addition we relate the critical time to\na diverging holographic complexity observable and compute the `switchback'\ndelay by adding a pair of shock wave perturbations for arbitrary values of the\nmass of the black hole."
    ],
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02787",
    "c_title":[
      "Super-enhanced Nuclear Fusion in Metal-like Systems and in Condensed\n  Plasmas of Supernova Progenitors"
    ],
    "c_abstract":[
      "In the scheme of chemonuclear reaction, bulk of itinerant s-electrons\nrevealing the thermodynamical liquid activity in metallic systems undergo\ncontact interaction with atomic nuclei and therein nucleons, inducing\ncontagiously thermodynamical liquid activity among the bulk of nuclei under the\nirreversible action of Nature towards the chemical potential minimum resulting\nin the united spontaneous, irreversible atomic and nuclear reactions. The\nchemonuclear reaction, moreover, is enhanced with astronomical figures besides\nthe enhancement of few particle processes (e.g., in the electron-screened\npycnonuclear fusion). In the systems of metal-like hydride\/deuteride and\nelectron donor mixtures, self-sustained H-H and D-D chemonuclear fusion may\ntake place. In these systems, however, some unforeseen phenomena are induced,\ne.g., radiation-less fusion and $\\gamma$-ray missing positron annihilation."
    ],
    "c_categories":[
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04840",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "Genetic Algorithms + Data Structures = Evolution Programs"
    ],
    "b_abstract":[
      "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
    ],
    "b_categories":[
      "cs.DS"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06693",
    "c_title":[
      "An Accurate Efficient Analytic Model of Fidelity under Depolarizing\n  Noise oriented to Large Scale Quantum System Design"
    ],
    "c_abstract":[
      "Fidelity is one of the most valuable and commonly used metrics for assessing\nthe performance of quantum circuits on error-prone quantum processors. Several\napproaches have been proposed to estimate circuit fidelity without the need of\nexecuting it on quantum hardware, but they often face limitations in\nscalability or accuracy. In this work, we present a comprehensive theoretical\nframework to predict the fidelity of quantum circuits under depolarizing noise.\nBuilding on theoretical results, we propose an efficient fidelity estimation\nalgorithm based on device calibration data. The method is thoroughly validated\nthrough simulation and execution on real hardware, demonstrating improved\naccuracy compared to state-of-the-art alternatives. The proposed approach\nprovides a scalable and practical tool for benchmarking quantum hardware,\ncomparing quantum software techniques such as compilation methods, obtaining\ncomputation bounds for quantum systems, and guiding hardware design decisions,\nmaking it a critical resource for the development and evaluation of quantum\ncomputing technologies."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"Automatic Skull Reconstruction by Deep Learnable Symmetry Enforcement",
    "a_abstract":"Every year, thousands of people suffer from skull damage and require\npersonalized implants to fill the cranial cavity. Unfortunately, the waiting\ntime for reconstruction surgery can extend to several weeks or even months,\nespecially in less developed countries. One factor contributing to the extended\nwaiting period is the intricate process of personalized implant modeling.\nCurrently, the preparation of these implants by experienced biomechanical\nexperts is both costly and time-consuming. Recent advances in artificial\nintelligence, especially in deep learning, offer promising potential for\nautomating the process. However, deep learning-based cranial reconstruction\nfaces several challenges: (i) the limited size of training datasets, (ii) the\nhigh resolution of the volumetric data, and (iii) significant data\nheterogeneity. In this work, we propose a novel approach to address these\nchallenges by enhancing the reconstruction through learnable symmetry\nenforcement. We demonstrate that it is possible to train a neural network\ndedicated to calculating skull symmetry, which can be utilized either as an\nadditional objective function during training or as a post-reconstruction\nobjective during the refinement step. We quantitatively evaluate the proposed\nmethod using open SkullBreak and SkullFix datasets, and qualitatively using\nreal clinical cases. The results indicate that the symmetry-preserving\nreconstruction network achieves considerably better outcomes compared to the\nbaseline (0.94\/0.94\/1.31 vs 0.84\/0.76\/2.43 in terms of DSC, bDSC, and HD95).\nMoreover, the results are comparable to the best-performing methods while\nrequiring significantly fewer computational resources (< 500 vs > 100,000 GPU\nhours). The proposed method is a considerable contribution to the field of\napplied artificial intelligence in medicine and is a step toward automatic\ncranial defect reconstruction in clinical practice.",
    "explanation":"This IDR paper is an applied research paper that focuses on combining Neural Networks to advance biomechanics. Below are some sentences in the abstract that reflect the references in Task 3:\n\n\"We demonstrate that it is possible to train a neural network dedicated to calculating skull symmetry,\"\n\n\"Every year, thousands of people suffer from skull damage and require personalized implants to fill the cranial cavity. Unfortunately, the waiting time for reconstruction surgery can extend to several weeks or even months, especially in less developed countries.\"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b35"
    ],
    "c_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "c_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.05824",
    "c_title":[
      "Aerial Reliable Collaborative Communications for Terrestrial Mobile\n  Users via Evolutionary Multi-Objective Deep Reinforcement Learning"
    ],
    "c_abstract":[
      "Unmanned aerial vehicles (UAVs) have emerged as the potential aerial base\nstations (BSs) to improve terrestrial communications. However, the limited\nonboard energy and antenna power of a UAV restrict its communication range and\ntransmission capability. To address these limitations, this work employs\ncollaborative beamforming through a UAV-enabled virtual antenna array to\nimprove transmission performance from the UAV to terrestrial mobile users,\nunder interference from non-associated BSs and dynamic channel conditions.\nSpecifically, we introduce a memory-based random walk model to more accurately\ndepict the mobility patterns of terrestrial mobile users. Following this, we\nformulate a multi-objective optimization problem (MOP) focused on maximizing\nthe transmission rate while minimizing the flight energy consumption of the UAV\nswarm. Given the NP-hard nature of the formulated MOP and the highly dynamic\nenvironment, we transform this problem into a multi-objective Markov decision\nprocess and propose an improved evolutionary multi-objective reinforcement\nlearning algorithm. Specifically, this algorithm introduces an evolutionary\nlearning approach to obtain the approximate Pareto set for the formulated MOP.\nMoreover, the algorithm incorporates a long short-term memory network and\nhyper-sphere-based task selection method to discern the movement patterns of\nterrestrial mobile users and improve the diversity of the obtained Pareto set.\nSimulation results demonstrate that the proposed method effectively generates a\ndiverse range of non-dominated policies and outperforms existing methods.\nAdditional simulations demonstrate the scalability and robustness of the\nproposed CB-based method under different system parameters and various\nunexpected circumstances."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03944",
    "c_title":[
      "A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for\n  Efficient Black-Box Neural Network Optimization"
    ],
    "c_abstract":[
      "Swarm intelligence optimization algorithms have gained significant attention\ndue to their ability to solve complex optimization problems. However, the\nefficiency of optimization in large-scale problems limits the use of related\nmethods. This paper presents a GPU-accelerated version of the Multi-Guiding\nSpark Fireworks Algorithm (MGFWA), which significantly improves the\ncomputational efficiency compared to its traditional CPU-based counterpart. We\nbenchmark the GPU-MGFWA on several neural network black-box optimization\nproblems and demonstrate its superior performance in terms of both speed and\nsolution quality. By leveraging the parallel processing power of modern GPUs,\nthe proposed GPU-MGFWA results in faster convergence and reduced computation\ntime for large-scale optimization tasks. The proposed implementation offers a\npromising approach to accelerate swarm intelligence algorithms, making them\nmore suitable for real-time applications and large-scale industrial problems.\nSource code is released at https:\/\/github.com\/mxxxr\/MGFWA."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09340",
    "c_title":[
      "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm"
    ],
    "c_abstract":[
      "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.10387",
    "c_title":[
      "Adding numbers with spiking neural circuits on neuromorphic hardware"
    ],
    "c_abstract":[
      "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07331",
    "c_title":[
      "Efficient Event-based Delay Learning in Spiking Neural Networks"
    ],
    "c_abstract":[
      "Spiking Neural Networks (SNNs) are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks. Spiking\nneurons are stateful and intrinsically recurrent, making them well-suited for\nspatio-temporal tasks. However, this intrinsic memory is limited by synaptic\nand membrane time constants. A powerful additional mechanism are delays. In\nthis paper, we propose a novel event-based training method for SNNs with\ndelays, grounded in the EventProp formalism and enabling the calculation of\nexact gradients with respect to weights and delays. Our method supports\nmultiple spikes per neuron and, to our best knowledge, is the first delay\nlearning algorithm to be applied to recurrent SNNs. We evaluate our method on a\nsimple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and\nSpiking Speech Commands datasets, demonstrating that our algorithm can optimize\ndelays from suboptimal initial conditions and enhance classification accuracy\ncompared to architectures without delays. Finally, we show that our approach\nuses less than half the memory of the current state-of-the-art delay-learning\nmethod and is up to 26x faster."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.01126",
    "c_title":[
      "Constrained multi-fidelity Bayesian optimization with automatic stop\n  condition"
    ],
    "c_abstract":[
      "Bayesian optimization (BO) is increasingly employed in critical applications\nto find the optimal design with minimal cost. While BO is known for its sample\nefficiency, relying solely on costly high-fidelity data can still result in\nhigh costs. This is especially the case in constrained search spaces where BO\nmust not only optimize but also ensure feasibility. A related issue in the BO\nliterature is the lack of a systematic stopping criterion. To solve these\nchallenges, we develop a constrained cost-aware multi-fidelity BO (CMFBO)\nframework whose goal is to minimize overall sampling costs by utilizing\ninexpensive low-fidelity sources while ensuring feasibility. In our case, the\nconstraints can change across the data sources and may be even black-box\nfunctions. We also introduce a systematic stopping criterion that addresses the\nlong-lasting issue associated with BO's convergence assessment. Our framework\nis publicly available on GitHub through the GP+ Python package and herein we\nvalidate it's efficacy on multiple benchmark problems."
    ],
    "c_categories":[
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.13886",
    "c_title":[
      "Contextuality and Chaos"
    ],
    "c_abstract":[
      "Classical chaos is marked by an extreme sensitivity to initial conditions,\nwhere infinitesimally close trajectories separate exponentially over time. In\nquantum mechanics, however, unitary evolution and the uncertainty principle\npreclude such behavior, necessitating alternative approaches to identifying\nchaos in quantum systems. One must therefore seek quantum features that can\nindicate the emergence of chaos in the classical limit. Here, we show that\ncontextuality, a quantum property that defies classical explanations, can serve\nas a signature of chaos. For a spin system undergoing chaotic dynamics, we\ndemonstrate that violations of Bell-type inequality can effectively\ndifferentiate regular and chaotic regions of the phase space, suggesting that\nthe nonclassicality of the system underpins signatures of chaos."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16617",
    "c_title":[
      "Concurrent Optimization of Satellite Phasing and Tasking for Cislunar\n  Space Situational Awareness"
    ],
    "c_abstract":[
      "Recently, renewed interest in cislunar space spurred by private and public\norganizations has driven research for future infrastructure in the region. As\nEarth-Moon traffic increases amidst a growing space economy, monitoring\narchitectures supporting this traffic must also develop. These are likely to be\nrealized as constellations of patrol satellites surveying traffic between the\nEarth and the Moon. This work investigates the concurrent optimization of\npatrol satellite phasing and tasking to provide information-maximal coverage of\ntraffic in periodic orbits."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.10244",
    "c_title":[
      "A Survey on Constructing Parseval Fusion Frames via Scaling Weights"
    ],
    "c_abstract":[
      "The construction of Parseval fusion frames is highly desirable in a wide\nrange of signal processing applications. In this paper, we study the problem of\nmodifying the weights of a fusion frame in order to generate a Parseval fusion\nframe. To this end, we extend the notion of the scalability to the fusion frame\nsetting. We then proceed to characterize scalable fusion Riesz bases and\n$1$-excess fusion frames. Furthermore, we provide the necessary and sufficient\nconditions for the scalability of certain $k$-excess fusion frames, $k\\geq 2$.\nFinally, we present several pertinent examples to confirm the obtained results."
    ],
    "c_categories":[
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
    ],
    "b_abstract":[
      "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.17533",
    "c_title":[
      "From Euler to AI: Unifying Formulas for Mathematical Constants"
    ],
    "c_abstract":[
      "The constant $\\pi$ has fascinated scholars for centuries, inspiring the\nderivation of countless formulas rooted in profound mathematical insight. This\nabundance of formulas raises a question: Are they interconnected, and can a\nunifying structure explain their relationships?\n  We propose a systematic methodology for discovering and proving formula\nequivalences, leveraging modern large language models, large-scale data\nprocessing, and novel mathematical algorithms. Analyzing 457,145 arXiv papers,\nover a third of the validated formulas for $\\pi$ were proven to be derivable\nfrom a single mathematical object - including formulas by Euler, Gauss, Lord\nBrouncker, and newer ones from algorithmic discoveries by the Ramanujan\nMachine.\n  Our approach extends to other constants, such as $e$, $\\zeta(3)$, and\nCatalan's constant, proving its broad applicability. This work represents a\nstep toward the automatic unification of mathematical knowledge, laying a\nfoundation for AI-driven discoveries of connections across scientific domains."
    ],
    "c_categories":[
      "cs.AI",
      "cs.CL",
      "math.HO",
      "math.NT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":[
      "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes"
    ],
    "c_abstract":[
      "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01319",
    "c_title":[
      "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability"
    ],
    "c_abstract":[
      "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18661",
    "c_title":[
      "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling"
    ],
    "c_abstract":[
      "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":[
      "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats"
    ],
    "c_abstract":[
      "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08533",
    "c_title":[
      "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant"
    ],
    "c_abstract":[
      "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03903",
    "c_title":[
      "Caribou -- A versatile data acquisition system for silicon pixel\n  detector prototyping"
    ],
    "c_abstract":[
      "Caribou is a versatile data acquisition system used in multiple collaborative\nframeworks (CERN EP R&D, DRD3, AIDAinnova, Tangerine) for laboratory and\ntest-beam qualification of novel silicon pixel detector prototypes. The system\nis built around a common hardware, firmware and software stack shared accross\ndifferent projects, thereby drastically reducing the development effort and\ncost. It consists of a custom Control and Readout (CaR) board and a commercial\nXilinx Zynq System-on-Chip (SoC) platform. The SoC platform runs a full Yocto\ndistribution integrating the custom software framework (Peary) and a custom\nFPGA firmware built within a common firmware infrastructure (Boreal). The CaR\nboard provides a hardware environment featuring various services such as\npowering, slow-control, and high-speed data links for the target detector\nprototype. Boreal and Peary, in turn, offer firmware and software architectures\nthat enable seamless integration of control and readout for new devices. While\nthe first version of the system used a SoC platform based on the ZC706\nevaluation board, migration to a Zynq UltraScale+ architecture is progressing\ntowards the support of the ZCU102 board and the ultimate objective of\nintegrating the SoC functionality directly into the CaR board, eliminating the\nneed for separate evaluation boards. This paper describes the Caribou system,\nfocusing on the latest project developments and showcasing progress and future\nplans across its hardware, firmware, and software components."
    ],
    "c_categories":[
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17554",
    "c_title":[
      "Information Theory for Expectation Measures"
    ],
    "c_abstract":[
      "Shannon based his information theory on the notion of probability measures as\nit we developed by Kolmogorov. In this paper we study some fundamental problems\nin information theory based on expectation measures. In the theory of\nexpectation measures it is natural to study data sets where no randomness is\npresent and it is also natural to study information theory for point processes\nas well as sampling where the sample size is not fixed. Expectation measures in\ncombination with Kraft's Inequality can be used to clarify in which cases\nprobability measures can be used to quantify randomness."
    ],
    "c_categories":[
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14394",
    "c_title":[
      "A new method to retrieve the star formation history from white dwarf\n  luminosity functions -- an application to the Gaia catalogue of nearby stars"
    ],
    "c_abstract":[
      "With the state-of-the-art Gaia astrometry, the number of confirmed white\ndwarfs has reached a few hundred thousand. We have reached the era where small\nfeatures in the white dwarf luminosity function (WDLF) of the solar\nneighbourhood can be resolved. We demonstrate how to apply Markov chain Monte\nCarlo sampling on a set of pre-computed partial-WDLFs to derive the star\nformation history of their progenitor stellar populations. We compare the\nresults against many well-accepted and established works using various types of\nstars, including white dwarfs, main sequence stars, sub-giants and the entire\nstellar population. We find convincing agreements among most of the methods,\nparticularly at the intermediate age of 0.1-9 Gyr."
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15234",
    "c_title":[
      "A Monte Carlo examination for the numerical values of universal\n  quantities in spatial dimension two"
    ],
    "c_abstract":[
      "By simulating a two-dimensional (2D) dimerized spin-1\/2 antiferromagnet with\nthe quantum Monte Carlo method, the numerical values of two universal\nquantities associated with the quantum critical regime (QCR), namely\n$S(\\pi,\\pi)\/\\left(\\chi_s T\\right)$ and $c\/\\left(T\\xi\\right)$, are determined.\nHere $S(\\pi,\\pi)$, $\\chi_s$, $c$, $\\xi,$ and $T$ are the staggered structure\nfactor, the staggered susceptibility, the spin-wave velocity, the correlation\nlength, and the temperature, respectively. For other QCR universal quantities,\nsuch as the Wilson ratio $W$ and $\\chi_u c^2\/T$ ($\\chi_u$ is the uniform\nsusceptibility), it is shown that the addition of higher order theoretical\ncontribution makes the agreement between the numerical and the analytic results\nworse. We find that the same scenario applies to $S(\\pi,\\pi)\/\\left(\\chi_s\nT\\right)$ and $c\/\\left(T\\xi\\right)$ as well. Specifically, our calculations\nlead to $S(\\pi,\\pi)\/\\left(\\chi_s T\\right)\\sim 1.073$ and\n$c\/\\left(T\\xi\\right)\\sim 0.963$ which are in better consistence with the\nleading theoretical predictions than those with the next-to-leading order\nterms. The presented outcome here as well as those in some relevant literature\nsuggest that it is desirable to conduct a refinement of the analytic\ncalculation to resolve the puzzle of why the inclusion of higher order terms\nleads to less accurate predictions for these universal quantities."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.17342",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b35"
    ],
    "b_title":[
      "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
    ],
    "b_abstract":[
      "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07692",
    "c_title":[
      "Generalized Euler numbers and ordered set partitions"
    ],
    "c_abstract":[
      "The Euler numbers have been widely studied. A signed version of the Euler\nnumbers of even subscript are given by the coefficients of the exponential\ngenerating function 1\/(1+x^2\/2!+x^4\/4!+...). Leeming and MacLeod introduced a\ngeneralization of the Euler numbers depending on an integer parameter d where\none takes the coefficients of the expansion of 1\/(1+x^d\/d!+x^{2d}\/(2d)!+...).\nThese numbers have been shown to have many interesting properties despite being\nmuch less studied. And the techniques used have been mainly algebraic. We\npropose a combinatorial model for them as signed sums over ordered partitions.\nWe show that this approach can be used to prove a number of old and new results\nincluding a recursion, integrality, and various congruences. Our methods\ninclude sign-reversing involutions and M\\\"obius inversion over partially\nordered sets."
    ],
    "c_categories":[
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"Equivariant Graph Attention Networks with Structural Motifs for\n  Predicting Cell Line-Specific Synergistic Drug Combinations",
    "a_abstract":"Cancer is the second leading cause of death, with chemotherapy as one of the\nprimary forms of treatment. As a result, researchers are turning to drug\ncombination therapy to decrease drug resistance and increase efficacy. Current\nmethods of drug combination screening, such as in vivo and in vitro, are\ninefficient due to stark time and monetary costs. In silico methods have become\nincreasingly important for screening drugs, but current methods are inaccurate\nand generalize poorly to unseen anticancer drugs. In this paper, I employ a\ngeometric deep-learning model utilizing a graph attention network that is\nequivariant to 3D rotations, translations, and reflections with structural\nmotifs. Additionally, the gene expression of cancer cell lines is utilized to\nclassify synergistic drug combinations specific to each cell line. I compared\nthe proposed geometric deep learning framework to current state-of-the-art\n(SOTA) methods, and the proposed model architecture achieved greater\nperformance on all 12 benchmark tasks performed on the DrugComb dataset.\nSpecifically, the proposed framework outperformed other SOTA methods by an\naccuracy difference greater than 28%. Based on these results, I believe that\nthe equivariant graph attention network's capability of learning geometric data\naccounts for the large performance improvements. The model's ability to\ngeneralize to foreign drugs is thought to be due to the structural motifs\nproviding a better representation of the molecule. Overall, I believe that the\nproposed equivariant geometric deep learning framework serves as an effective\ntool for virtually screening anticancer drug combinations for further\nvalidation in a wet lab environment. The code for this work is made available\nonline at: https:\/\/github.com\/WeToTheMoon\/EGAT_DrugSynergy.",
    "explanation":"From the key references cited in Task 3, below are some sentences that point to the significance of those references that show how this paper is an IDR.\n\n\"Additionally, the gene expression of cancer cell lines is utilized to classify synergistic drug combinations specificto each cell line. \"\n\"I compared the proposed geometric deep learning\nframework to current state-of-the-art (SOTA) methods\"\n\"Based on these results, I believe that the equivariant graph attention network\u2019s capability of learning geometric data accounts for the large performance improvements.\"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b16"
    ],
    "c_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "c_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09976",
    "c_title":[
      "Dendritic Localized Learning: Toward Biologically Plausible Algorithm"
    ],
    "c_abstract":[
      "Backpropagation is the foundational algorithm for training neural networks\nand a key driver of deep learning's success. However, its biological\nplausibility has been challenged due to three primary limitations: weight\nsymmetry, reliance on global error signals, and the dual-phase nature of\ntraining, as highlighted by the existing literature. Although various\nalternative learning approaches have been proposed to address these issues,\nmost either fail to satisfy all three criteria simultaneously or yield\nsuboptimal results. Inspired by the dynamics and plasticity of pyramidal\nneurons, we propose Dendritic Localized Learning (DLL), a novel learning\nalgorithm designed to overcome these challenges. Extensive empirical\nexperiments demonstrate that DLL satisfies all three criteria of biological\nplausibility while achieving state-of-the-art performance among algorithms that\nmeet these requirements. Furthermore, DLL exhibits strong generalization across\na range of architectures, including MLPs, CNNs, and RNNs. These results,\nbenchmarked against existing biologically plausible learning algorithms, offer\nvaluable empirical insights for future research. We hope this study can inspire\nthe development of new biologically plausible algorithms for training\nmultilayer networks and advancing progress in both neuroscience and machine\nlearning."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.05601",
    "c_title":[
      "Backpropagation through Soft Body: Investigating Information Processing\n  in Brain-Body Coupling Systems"
    ],
    "c_abstract":[
      "Animals achieve sophisticated behavioral control through dynamic coupling of\nthe brain, body, and environment. Accordingly, the co-design approach, in which\nboth the controllers and the physical properties are optimized simultaneously,\nhas been suggested for generating refined agents without designing each\ncomponent separately. In this study, we aim to reveal how the function of the\ninformation processing is distributed between brains and bodies while applying\nthe co-design approach. Using a framework called ``backpropagation through soft\nbody,\" we developed agents to perform specified tasks and analyzed their\nmechanisms. The tasks included classification and corresponding behavioral\nassociation, nonlinear dynamical system emulation, and autonomous behavioral\ngeneration. In each case, our analyses revealed reciprocal relationships\nbetween the brains and bodies. In addition, we show that optimized brain\nfunctionalities can be embedded into bodies using physical reservoir computing\ntechniques. Our results pave the way for efficient designs of brain--body\ncoupling systems."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16735",
    "c_title":[
      "Stochastic Population Update Provably Needs An Archive in Evolutionary\n  Multi-objective Optimization"
    ],
    "c_abstract":[
      "Evolutionary algorithms (EAs) have been widely applied to multi-objective\noptimization, due to their nature of population-based search. Population\nupdate, a key component in multi-objective EAs (MOEAs), is usually performed in\na greedy, deterministic manner. However, recent studies have questioned this\npractice and shown that stochastic population update (SPU), which allows\ninferior solutions have a chance to be preserved, can help MOEAs jump out of\nlocal optima more easily. While introducing randomness in the population update\nprocess boosts the exploration of MOEAs, there is a drawback that the\npopulation may not always preserve the very best solutions found, thus\nentailing a large population. Intuitively, a possible solution to this issue is\nto introduce an archive that stores the best solutions ever found. In this\npaper, we theoretically show that using an archive can allow a small population\nand accelerate the search of SPU-based MOEAs substantially. Specifically, we\nanalyze the expected running time of two well-established MOEAs, SMS-EMOA and\nNSGA-II, with SPU for solving a commonly studied bi-objective problem\nOneJumpZeroJump, and prove that using an archive can bring (even exponential)\nspeedups. The comparison between SMS-EMOA and NSGA-II also suggests that the\n$(\\mu+\\mu)$ update mode may be more suitable for SPU than the $(\\mu+1)$ update\nmode. Furthermore, our derived running time bounds for using SPU alone are\nsignificantly tighter than previously known ones. Our theoretical findings are\nalso empirically validated on a well-known practical problem, the\nmulti-objective traveling salesperson problem. We hope this work may provide\ntheoretical support to explore different ideas of designing algorithms in\nevolutionary multi-objective optimization."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15129",
    "c_title":[
      "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement\n  Learning"
    ],
    "c_abstract":[
      "Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising\napproach to overcoming the limitations of traditional reinforcement learning\n(RL) by integrating the Evolutionary Computation (EC) paradigm with RL.\nHowever, the population-based nature of EC significantly increases\ncomputational costs, thereby restricting the exploration of algorithmic design\nchoices and scalability in large-scale settings. To address this challenge, we\nintroduce $\\texttt{$\\textbf{EvoRL}$}$, the first end-to-end EvoRL framework\noptimized for GPU acceleration. The framework executes the entire training\npipeline on accelerators, including environment simulations and EC processes,\nleveraging hierarchical parallelism through vectorization and compilation\ntechniques to achieve superior speed and scalability. This design enables the\nefficient training of large populations on a single machine. In addition to its\nperformance-oriented design, $\\texttt{$\\textbf{EvoRL}$}$ offers a comprehensive\nplatform for EvoRL research, encompassing implementations of traditional RL\nalgorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g.,\nCMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL\n(e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's\nmodular architecture and user-friendly interface allow researchers to\nseamlessly integrate new components, customize algorithms, and conduct fair\nbenchmarking and ablation studies. The project is open-source and available at:\nhttps:\/\/github.com\/EMI-Group\/evorl."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07375",
    "c_title":[
      "A RankNet-Inspired Surrogate-Assisted Hybrid Metaheuristic for Expensive\n  Coverage Optimization"
    ],
    "c_abstract":[
      "Coverage optimization generally involves deploying a set of facilities to\nbest satisfy the demands of specified points, with broad applications in fields\nsuch as location science and sensor networks. Recent applications reveal that\nthe subset site selection coupled with continuous angular parameter\noptimization can be formulated as Mixed-Variable Optimization Problems (MVOPs).\nMeanwhile, high-fidelity discretization and visibility analysis significantly\nincrease computational cost and complexity, evolving the MVOP into an Expensive\nMixed-Variable Optimization Problem (EMVOP). While canonical Evolutionary\nAlgorithms have yielded promising results, their reliance on numerous fitness\nevaluations is too costly for our problem. Furthermore, most surrogate-assisted\nmethods face limitations due to their reliance on regression-based models. To\naddress these issues, we propose the RankNet-Inspired Surrogate-assisted Hybrid\nMetaheuristic (RI-SHM), an extension of our previous work. RI-SHM integrates\nthree key components: (1) a RankNet-based pairwise global surrogate that\ninnovatively predicts rankings between pairs of individuals, bypassing the\nchallenges of fitness estimation in discontinuous solution space; (2) a\nsurrogate-assisted local Estimation of Distribution Algorithm that enhances\nlocal exploitation and helps escape from local optima; and (3) a fitness\ndiversity-driven switching strategy that dynamically balances exploration and\nexploitation. Experiments demonstrate that our algorithm can effectively handle\nlarge-scale coverage optimization tasks of up to 300 dimensions and more than\n1,800 targets within desirable runtime. Compared to state-of-the-art algorithms\nfor EMVOPs, RI-SHM consistently outperforms them by up to 56.5$\\%$ across all\ntested instances."
    ],
    "c_categories":[
      "cs.NE"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03903",
    "c_title":[
      "Caribou -- A versatile data acquisition system for silicon pixel\n  detector prototyping"
    ],
    "c_abstract":[
      "Caribou is a versatile data acquisition system used in multiple collaborative\nframeworks (CERN EP R&D, DRD3, AIDAinnova, Tangerine) for laboratory and\ntest-beam qualification of novel silicon pixel detector prototypes. The system\nis built around a common hardware, firmware and software stack shared accross\ndifferent projects, thereby drastically reducing the development effort and\ncost. It consists of a custom Control and Readout (CaR) board and a commercial\nXilinx Zynq System-on-Chip (SoC) platform. The SoC platform runs a full Yocto\ndistribution integrating the custom software framework (Peary) and a custom\nFPGA firmware built within a common firmware infrastructure (Boreal). The CaR\nboard provides a hardware environment featuring various services such as\npowering, slow-control, and high-speed data links for the target detector\nprototype. Boreal and Peary, in turn, offer firmware and software architectures\nthat enable seamless integration of control and readout for new devices. While\nthe first version of the system used a SoC platform based on the ZC706\nevaluation board, migration to a Zynq UltraScale+ architecture is progressing\ntowards the support of the ZCU102 board and the ultimate objective of\nintegrating the SoC functionality directly into the CaR board, eliminating the\nneed for separate evaluation boards. This paper describes the Caribou system,\nfocusing on the latest project developments and showcasing progress and future\nplans across its hardware, firmware, and software components."
    ],
    "c_categories":[
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17554",
    "c_title":[
      "Information Theory for Expectation Measures"
    ],
    "c_abstract":[
      "Shannon based his information theory on the notion of probability measures as\nit we developed by Kolmogorov. In this paper we study some fundamental problems\nin information theory based on expectation measures. In the theory of\nexpectation measures it is natural to study data sets where no randomness is\npresent and it is also natural to study information theory for point processes\nas well as sampling where the sample size is not fixed. Expectation measures in\ncombination with Kraft's Inequality can be used to clarify in which cases\nprobability measures can be used to quantify randomness."
    ],
    "c_categories":[
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.14394",
    "c_title":[
      "A new method to retrieve the star formation history from white dwarf\n  luminosity functions -- an application to the Gaia catalogue of nearby stars"
    ],
    "c_abstract":[
      "With the state-of-the-art Gaia astrometry, the number of confirmed white\ndwarfs has reached a few hundred thousand. We have reached the era where small\nfeatures in the white dwarf luminosity function (WDLF) of the solar\nneighbourhood can be resolved. We demonstrate how to apply Markov chain Monte\nCarlo sampling on a set of pre-computed partial-WDLFs to derive the star\nformation history of their progenitor stellar populations. We compare the\nresults against many well-accepted and established works using various types of\nstars, including white dwarfs, main sequence stars, sub-giants and the entire\nstellar population. We find convincing agreements among most of the methods,\nparticularly at the intermediate age of 0.1-9 Gyr."
    ],
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15234",
    "c_title":[
      "A Monte Carlo examination for the numerical values of universal\n  quantities in spatial dimension two"
    ],
    "c_abstract":[
      "By simulating a two-dimensional (2D) dimerized spin-1\/2 antiferromagnet with\nthe quantum Monte Carlo method, the numerical values of two universal\nquantities associated with the quantum critical regime (QCR), namely\n$S(\\pi,\\pi)\/\\left(\\chi_s T\\right)$ and $c\/\\left(T\\xi\\right)$, are determined.\nHere $S(\\pi,\\pi)$, $\\chi_s$, $c$, $\\xi,$ and $T$ are the staggered structure\nfactor, the staggered susceptibility, the spin-wave velocity, the correlation\nlength, and the temperature, respectively. For other QCR universal quantities,\nsuch as the Wilson ratio $W$ and $\\chi_u c^2\/T$ ($\\chi_u$ is the uniform\nsusceptibility), it is shown that the addition of higher order theoretical\ncontribution makes the agreement between the numerical and the analytic results\nworse. We find that the same scenario applies to $S(\\pi,\\pi)\/\\left(\\chi_s\nT\\right)$ and $c\/\\left(T\\xi\\right)$ as well. Specifically, our calculations\nlead to $S(\\pi,\\pi)\/\\left(\\chi_s T\\right)\\sim 1.073$ and\n$c\/\\left(T\\xi\\right)\\sim 0.963$ which are in better consistence with the\nleading theoretical predictions than those with the next-to-leading order\nterms. The presented outcome here as well as those in some relevant literature\nsuggest that it is desirable to conduct a refinement of the analytic\ncalculation to resolve the puzzle of why the inclusion of higher order terms\nleads to less accurate predictions for these universal quantities."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Trends in Phase II Trials for Cancer Therapies"
    ],
    "b_abstract":[
      "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07692",
    "c_title":[
      "Generalized Euler numbers and ordered set partitions"
    ],
    "c_abstract":[
      "The Euler numbers have been widely studied. A signed version of the Euler\nnumbers of even subscript are given by the coefficients of the exponential\ngenerating function 1\/(1+x^2\/2!+x^4\/4!+...). Leeming and MacLeod introduced a\ngeneralization of the Euler numbers depending on an integer parameter d where\none takes the coefficients of the expansion of 1\/(1+x^d\/d!+x^{2d}\/(2d)!+...).\nThese numbers have been shown to have many interesting properties despite being\nmuch less studied. And the techniques used have been mainly algebraic. We\npropose a combinatorial model for them as signed sums over ordered partitions.\nWe show that this approach can be used to prove a number of old and new results\nincluding a recursion, integrality, and various congruences. Our methods\ninclude sign-reversing involutions and M\\\"obius inversion over partially\nordered sets."
    ],
    "c_categories":[
      "math.CO",
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":[
      "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model"
    ],
    "c_abstract":[
      "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":[
      "Multicellular self-organization in Escherichia coli"
    ],
    "c_abstract":[
      "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":[
      "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions"
    ],
    "c_abstract":[
      "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":[
      "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions"
    ],
    "c_abstract":[
      "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":[
      "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma"
    ],
    "c_abstract":[
      "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12517",
    "c_title":[
      "A Finite Element Implementation of the SRTD Algorithm for an Oldroyd\n  3-Parameter Viscoelastic Fluid Model"
    ],
    "c_abstract":[
      "In this paper, we discuss a finite element implementation of the SRTD\nalgorithm described by Girault and Scott for the steady-state case of a certain\n3-parameter subset of the Oldroyd models. We compare it to the well-known EVSS\nmethod, which, though originally described for the upper-convected Maxwell\nmodel, can easily accommodate the Oldroyd 3-parameter model. We obtain\nnumerical results for both methods on two benchmark problems: the lid-driven\ncavity problem and the journal-bearing, or eccentric rotating cylinders,\nproblem. We find that the resulting finite element implementation of SRTD is\nstable with respect to mesh refinement and is generally faster than EVSS,\nthough is not capable of reaching as high a Weissenberg number as EVSS."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07404",
    "c_title":[
      "Reconstruction of quantum states by applying an analytical optimization\n  model"
    ],
    "c_abstract":[
      "When working with quantum states, analysis of the final quantum state\ngenerated through probabilistic measurements is essential. This analysis is\ntypically conducted by constructing the density matrix from either partial or\nfull tomography measurements of the quantum state. While full tomography\nmeasurement offers the most accurate reconstruction of the density matrix,\nlimited measurements pose challenges for reconstruction algorithms, often\nresulting in non-physical density matrices with negative eigenvalues. This is\noften remedied using maximum likelihood estimators, which have a high computing\ntime or by other estimation methods that decrease the reconstructed fidelity.\nIn this study, we show that when restricting the measurement sample size,\nimprovement over existing algorithms can be achieved. Our findings underline\nthe multiplicity of solutions in the reconstruction problem, depending upon the\ngenerated state and measurement model utilized, thus motivating further\nresearch towards identifying optimal algorithms tailored to specific\nexperimental contexts."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18306",
    "c_title":[
      "The theory of one-relator groups: history and recent progress"
    ],
    "c_abstract":[
      "The theory of one-relator groups is now almost a century old. The authors\ntherefore feel that a comprehensive survey of this fascinating subject is in\norder, and this document is an attempt at precisely such a survey. This article\nis divided into two chapters, reflecting the two different phases in the story\nof one-relator groups. The first chapter, written by the second author, covers\nthe historical development of the theory roughly until the advent of geometric\ngroup theory. The second chapter, written by the first author, covers the\nrecent progress in the theory up until the present day. The two chapters can be\nread independently of one another and have minimal overlap."
    ],
    "c_categories":[
      "math.GR",
      "math.HO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10878",
    "c_title":[
      "The concept of a superconducting spin flipper - neutron decelerator for\n  a UCN source at a pulsed reactor"
    ],
    "c_abstract":[
      "The work is devoted to the development of a conceptual design for a gradient\nspin flipper - neutron decelerator, which is the main component of a designed\nUCN source for a pulsed reactor. In close cooperation between the JINR group\nand SuperOx, a preliminary design of a stationary gradient magnet for the\nadiabatic spin flipper has been developed. A thorough calculation of the\nmagnetic field configuration has been performed. The movement of neutrons in\nthe magnetic field generated by the designed magnetic system has been\nsimulated, and the deceleration time of neutrons in the spin flipper has been\nanalyzed. The results obtained give grounds for hope that the idea of creating\na UCN source based on pulsed accumulation in a trap using non-stationary\nneutron deceleration is feasible."
    ],
    "c_categories":[
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.04747",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "E(n) Equivariant Graph Neural Networks"
    ],
    "b_abstract":[
      "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
    ],
    "b_categories":[
      "cs.NE"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.16383",
    "c_title":[
      "Quantum Characterization, Verification, and Validation"
    ],
    "c_abstract":[
      "Quantum characterization, verification, and validation (QCVV) is a set of\ntechniques to probe, describe, and assess the behavior of quantum bits\n(qubits), quantum information-processing registers, and quantum computers. QCVV\nprotocols probe and describe the effects of unwanted decoherence so that it can\nbe eliminated or mitigated. They can be usefully divided into characterization\ntechniques that estimate predictive models for a device's behavior from data,\nand benchmarking techniques that assess overall performance of a device. In\nthis introductory article, we briefly summarize the history of QCVV, introduce\nthe mathematical models and metrics upon which it relies, and then summarize\nthe foundational fields of tomography, randomized benchmarking, and holistic\nbenchmarks. We conclude with brief descriptions of (and references to) advanced\ntopics including gate set tomography, phase estimation, Pauli noise learning,\ncharacterization of mid-circuit measurements and non-Markovianity, classical\nshadows, verification and certification, and logical qubit assessment."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"Improving speaker verification robustness with synthetic emotional\n  utterances",
    "a_abstract":"A speaker verification (SV) system offers an authentication service designed\nto confirm whether a given speech sample originates from a specific speaker.\nThis technology has paved the way for various personalized applications that\ncater to individual preferences. A noteworthy challenge faced by SV systems is\ntheir ability to perform consistently across a range of emotional spectra. Most\nexisting models exhibit high error rates when dealing with emotional utterances\ncompared to neutral ones. Consequently, this phenomenon often leads to missing\nout on speech of interest. This issue primarily stems from the limited\navailability of labeled emotional speech data, impeding the development of\nrobust speaker representations that encompass diverse emotional states.\n  To address this concern, we propose a novel approach employing the CycleGAN\nframework to serve as a data augmentation method. This technique synthesizes\nemotional speech segments for each specific speaker while preserving the unique\nvocal identity. Our experimental findings underscore the effectiveness of\nincorporating synthetic emotional data into the training process. The models\ntrained using this augmented dataset consistently outperform the baseline\nmodels on the task of verifying speakers in emotional speech scenarios,\nreducing equal error rate by as much as 3.64% relative.",
    "explanation":"A speaker verification (SV) system offers an authentication ser-\nvice designed to confirm whether a given speech sample orig-\ninates from a specific speaker. To address this concern, we propose a novel\napproach employing the CycleGAN framework to serve as a\ndata augmentation method. ",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b6"
    ],
    "c_title":[
      "Speaker Diarization with LSTM"
    ],
    "c_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.15762",
    "c_title":[
      "Dialogic Learning in Child-Robot Interaction: A Hybrid Approach to\n  Personalized Educational Content Generation"
    ],
    "c_abstract":[
      "Dialogic learning fosters motivation and deeper understanding in education\nthrough purposeful and structured dialogues. Foundational models offer a\ntransformative potential for child-robot interactions, enabling the design of\npersonalized, engaging, and scalable interactions. However, their integration\ninto educational contexts presents challenges in terms of ensuring\nage-appropriate and safe content and alignment with pedagogical goals. We\nintroduce a hybrid approach to designing personalized educational dialogues in\nchild-robot interactions. By combining rule-based systems with LLMs for\nselective offline content generation and human validation, the framework\nensures educational quality and developmental appropriateness. We illustrate\nthis approach through a project aimed at enhancing reading motivation, in which\na robot facilitated book-related dialogues."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.07540",
    "c_title":[
      "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol"
    ],
    "c_abstract":[
      "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.17025",
    "c_title":[
      "A Guide to Bayesian Networks Software Packages for Structure and\n  Parameter Learning -- 2025 Edition"
    ],
    "c_abstract":[
      "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.10038",
    "c_title":[
      "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI\n  Representation Learning"
    ],
    "c_abstract":[
      "POI representation learning plays a crucial role in handling tasks related to\nuser mobility data. Recent studies have shown that enriching POI\nrepresentations with multimodal information can significantly enhance their\ntask performance. Previously, the textual information incorporated into POI\nrepresentations typically involved only POI categories or check-in content,\nleading to relatively weak textual features in existing methods. In contrast,\nlarge language models (LLMs) trained on extensive text data have been found to\npossess rich textual knowledge. However leveraging such knowledge to enhance\nPOI representation learning presents two key challenges: first, how to extract\nPOI-related knowledge from LLMs effectively, and second, how to integrate the\nextracted information to enhance POI representations. To address these\nchallenges, we propose POI-Enhancer, a portable framework that leverages LLMs\nto improve POI representations produced by classic POI learning models. We\nfirst design three specialized prompts to extract semantic information from\nLLMs efficiently. Then, the Dual Feature Alignment module enhances the quality\nof the extracted information, while the Semantic Feature Fusion module\npreserves its integrity. The Cross Attention Fusion module then fully\nadaptively integrates such high-quality information into POI representations\nand Multi-View Contrastive Learning further injects human-understandable\nsemantic information into these representations. Extensive experiments on three\nreal-world datasets demonstrate the effectiveness of our framework, showing\nsignificant improvements across all baseline representations."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.01642",
    "c_title":[
      "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph\n  Retrieval for LLM Reasoning"
    ],
    "c_abstract":[
      "Recent large language model (LLM) reasoning, despite its success, suffers\nfrom limited domain knowledge, susceptibility to hallucinations, and\nconstrained reasoning depth, particularly in small-scale models deployed in\nresource-constrained environments. This paper presents the first investigation\ninto integrating step-wise knowledge graph retrieval with step-wise reasoning\nto address these challenges, introducing a novel paradigm termed as\ngraph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to\nretrieve and process relevant mathematical knowledge in a step-wise manner,\nenhancing their problem-solving abilities without additional training. To this\nend, we propose KG-RAR, a framework centered on process-oriented knowledge\ngraph construction, a hierarchical retrieval strategy, and a universal\npost-retrieval processing and reward model (PRP-RM) that refines retrieved\ninformation and evaluates each reasoning step. Experiments on the Math500 and\nGSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging\nresults, achieving a 20.73\\% relative improvement with Llama-3B on Math500."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.06796",
    "c_title":[
      "Conefield approach to identifying regions without flux surfaces for\n  magnetic fields"
    ],
    "c_abstract":[
      "The conefield variant of a Converse KAM method for 3D vector fields,\nidentifying regions through which no invariant 2-tori pass transverse to a\nspecified direction field, is tested on some helical perturbations of an\naxisymmetric magnetic field in toroidal geometry. This implementation computes\nbounds on the slopes of invariant tori of a given class and allows to apply a\nsubsidiary criterion to extend the non-existence region, saving significant\ncomputation time. The method finds regions corresponding to magnetic islands\nand chaos for the fieldline flow."
    ],
    "c_categories":[
      "math.DS",
      "physics.plasm-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.10833",
    "c_title":[
      "Universal Chern classes on the moduli of bundles"
    ],
    "c_abstract":[
      "The goal of this paper is to construct universal cohomology classes on the\nmoduli space of stable bundles over a curve when it is not a fine moduli space,\ni.e. when the rank and degree are not coprime. More precisely, we show that\ncertain Chern classes of the universal bundle on the product of the curve with\nthe moduli stack of bundles lift to the product of the curve with the moduli\nspace of stable bundles."
    ],
    "c_categories":[
      "math.AG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.17496",
    "c_title":[
      "The Akhiezer iteration and an inverse-free solver for Sylvester matrix\n  equations"
    ],
    "c_abstract":[
      "An inverse-free iterative method is developed for solving Sylvester matrix\nequations when the spectra of the coefficient matrices are on, or near, known\ndisjoint subintervals of the real axis. The method uses the recently-introduced\nAkhiezer iteration to address an equivalent problem of approximating the matrix\nsign function applied to a block matrix, resulting in a provable and computable\ngeometric rate of convergence. When the right-hand side matrix is low rank, the\nmethod requires only low-rank matrix-matrix products. Relative to existing\nstate-of-the-art approaches, the method presented here can be more efficient\nwhen the coefficient matrices are dense or otherwise costly to invert.\nApplications include solving partial differential equations and computing\nFr\\'echet derivatives."
    ],
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.19019",
    "c_title":[
      "Thermodynamics of Hamiltonian anyons with applications to quantum heat\n  engines"
    ],
    "c_abstract":[
      "The behavior of a collection of identical particles is intimately linked to\nthe symmetries of their wavefunction under particle exchange. Topological\nanyons, arising as quasiparticles in low-dimensional systems, interpolate\nbetween bosons and fermions, picking up a complex phase when exchanged. Recent\nresearch has demonstrated that similar statistical behavior can arise with\nmixtures of bosonic and fermionic pairs, offering theoretical and experimental\nsimplicity. We introduce an alternative implementation of such statistical\nanyons, based on promoting or suppressing the population of symmetric states\nvia a symmetry generating Hamiltonian. The scheme has numerous advantages:\nanyonic statistics emerge in a single particle pair, extending\nstraightforwardly to larger systems; the statistical properties can be\ndynamically adjusted; and the setup can be simulated efficiently. We show how\nexchange symmetry can be exploited to improve the performance of heat engines,\nand demonstrate a reversible work extraction cycle in which bosonization and\nfermionization replace compression and expansion strokes. Additionally, we\ninvestigate emergent thermal properties, including critical phenomena, in large\nstatistical anyon systems."
    ],
    "c_categories":[
      "cond-mat.stat-mech",
      "math-ph",
      "math.MP",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Human-Centric Interfaces for Ambient Intelligence"
    ],
    "b_abstract":[
      "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
    ],
    "b_categories":[
      "physics.app-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.09273",
    "c_title":[
      "Non-parametric estimation of net survival under dependence between death\n  causes"
    ],
    "c_abstract":[
      "Relative survival methodology deals with a competing risks survival model\nwhere the cause of death is unknown. This lack of information occurs regularly\nin population-based cancer studies. Non-parametric estimation of the net\nsurvival is possible through the Pohar Perme estimator. Derived similarly to\nKaplan-Meier, it nevertheless relies on an untestable independence assumption.\nWe propose here to relax this assumption and provide a generalized\nnon-parametric estimator that works for other dependence structures, by\nleveraging the underlying stochastic processes and martingales. We formally\nderive asymptotics of this estimator, providing variance estimation and\nlog-rank-type tests. Our approach provides a new perspective on the Pohar Perme\nestimator and the acceptability of the underlying independence assumption. We\nhighlight the impact of this dependence structure assumption on simulation\nstudies, and illustrate them through an application on registry data relative\nto colorectal cancer, before discussing potential extensions of our\nmethodology."
    ],
    "c_categories":[
      "math.ST",
      "stat.TH"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07723",
    "c_title":[
      "Total acoustic transmission between fluids using a solid material with\n  emphasis on the air-water interface"
    ],
    "c_abstract":[
      "Total acoustic transmission between water and air is modeled using a purely\nsolid interface comprising two elastic plates separated by periodically spaced\nribs. The frequency of full transmission depends only on, and is inversely\nproportional to, the areal density of the plate facing the air. Total\ntransmission also requires a specific dependence of the rib spacing on the\nbending stiffness of the two plates. These relations are the result of an\nexplicit analytical solution for the transmitted and reflected acoustic waves\ncombined with asymptotic approximations based on the small parameter defined by\nthe air-to-water impedance ratio. Surprisingly, the total transmission effect\nis almost independent of the angle of incidence, even though the transmission\nconditions are predicated on normal incidence. Parametric studies are performed\nto examine the effect on the frequency bandwidth and Q-factor of the acoustic\ntransmissivity. A lower bound for the Q-factor of $30.6$ is simply related to\nthe water-air impedance ratio."
    ],
    "c_categories":[
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.15063",
    "c_title":[
      "Transfer ABCD Matrix for Time-Varying Media and Time Crystals"
    ],
    "c_abstract":[
      "This paper introduces a formal definition of the transfer ABCD parameters in\ntime-varying electromagnetic systems. The formal definition comes after the\nrearrangement of the fields $D$ and $B$ at the inputs and outputs of the\ntemporal system based on the time-varying boundary conditions. Then, we derive\nthe ABCD parameters of a temporal transmission line, i.e., a temporal slab, and\ncompute the associated scattering parameters (reflection and transmission\ncoefficients). The results presented here open up an alternative way, based on\nnetwork theory, to analyze multilayer temporal configurations. Moreover, we\nshow that the ABCD parameters can be used to compute the dispersion diagram\n($\\omega$ vs $k$) of time crystals."
    ],
    "c_categories":[
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08874",
    "c_title":[
      "Benchmarking analytical electron ptychography methods for the low-dose\n  imaging of beam-sensitive materials"
    ],
    "c_abstract":[
      "This publication presents an investigation of the performance of different\nanalytical electron ptychography methods for low-dose imaging. In particular,\nbenchmarking is performed for two model-objects, monolayer MoS$_2$ and\napoferritin, by means of multislice simulations. Specific attention is given to\ncases where the individual diffraction patterns remain sparse. After a first\nrigorous introduction to the theoretical foundations of the methods, an\nimplementation based on the scan-frequency partitioning of calculation steps is\ndescribed, permitting a significant reduction of memory needs and high sampling\nflexibility. By analyzing the role of contrast transfer and illumination\nconditions, this work provides insights into the trade-off between resolution,\nsignal-to-noise ratio and probe focus, as is necessary for the optimization of\npractical experiments. Furthermore, important differences between the different\nmethods are demonstrated. Overall, the results obtained for the two\nmodel-objects demonstrate that analytical ptychography is an attractive option\nfor the low-dose imaging of beam-sensitive materials."
    ],
    "c_categories":[
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02117",
    "c_title":[
      "Analysis of random telegraph noise in resistive memories: The case of\n  unstable filaments"
    ],
    "c_abstract":[
      "Through Random Telegraph Noise (RTN) analysis, valuable information can be\nprovided about the role of defect traps in fine tuning and reading of the state\nof a nanoelectronic device. However, time domain analysis techniques exhibit\ntheir limitations in case where unstable RTN signals occur. These instabilities\nare a common issue in Multi-Level Cells (MLC) of resistive memories (ReRAM),\nwhen the tunning protocol fails to find a perfectly stable resistance state,\nwhich in turn brings fluctuations to the RTN signal especially in long time\nmeasurements and cause severe errors in the estimation of the distribution of\ntime constants of the observed telegraphic events, i.e., capture\/emission of\ncarriers from traps. In this work, we analyze the case of the unstable\nfilaments in silicon nitride-based ReRAM devices and propose an adaptive filter\nimplementing a moving-average detrending method in order to flatten unstable\nRTN signals and increase sufficiently the accuracy of the conducted\nmeasurements. The te and tc emission\/capture time constants of the traps,\nrespectively, are then calculated and a cross-validation through frequency\ndomain analysis (Lorentzian fitting) was performed proving that the proposed\nmethod is accurate."
    ],
    "c_categories":[
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08912",
    "c_title":[
      "High-Precision Fluidic Kirigami Metasurface for Ultrasonic Holographic\n  Lensing and Haptic Interfacing"
    ],
    "c_abstract":[
      "Morphing surfaces provide a versatile tool to advance the functionalities of\nhigh-performance aircraft, soft robots, biomedical devices, and human-machine\ninterfaces. However, achieving precise shape transformation and mechanical\nproperty control remains challenging due to nonlinearity, design constraints,\nand the difficulty of coordinating multiple constituent materials across a\ncontinuous surface. To this end, this study unveils that Kirigami art can\ninspire novel solutions. More specifically, the geometric principles of\nKirigami can be exploited to design and fabricate fluidic metasurfaces capable\nof highly accurate shape morphing and output force control, all via a single\npressure input. This study presents a systematic approach to designing Kirigami\nfor tuning the local deformation and force output through extensive nonlinear\nmodeling and experiment validation on two archetypal patterns: concentric\nsquare and circular cuts. The potentials of this approach are demonstrated via\ntwo multiphysics case studies: (1) an acoustic holography lens for ultrasonic\nwave steering, achieving highly accurate deformation control under a single\nglobal pressure input, and (2) a haptic device with a small volume, constant\ncontact area, and high-resolution output force. The fluidic Kirigami concept\nallows for simple yet effective adaptation to different shape and stiffness\nrequirements, paving the way for a new family of scalable and programmable\nmorphing surfaces."
    ],
    "c_categories":[
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02722",
    "c_title":[
      "N-player and mean field games among fund managers considering excess\n  logarithmic returns"
    ],
    "c_abstract":[
      "This paper studies the competition among multiple fund managers with relative\nperformance over the excess logarithmic return. Fund managers compete with each\nother and have expected utility or mean-variance criteria for excess\nlogarithmic return.\n  Each fund manager possesses a unique risky asset, and all fund managers can\nalso invest in a public risk-free asset and a public risk asset. We construct\nboth an $n$-player game and a mean field game (MFG) to address the competition\nproblem under these two criteria. We explicitly define and rigorously solve the\nequilibrium and mean field equilibrium (MFE) for each criteria. In the four\nmodels, the excess logarithmic return as the evaluation criterion of the fund\nleads to the { allocation fractions} being constant. The introduction of the\npublic risky asset yields different outcomes, with competition primarily\naffecting the investment in public assets, particularly evident in the MFG. We\ndemonstrate that the MFE of the MFG represents the limit of the $n$-player\ngame's equilibrium as the competitive scale $n$ approaches infinity. Finally,\nthe sensitivity analyses of the equilibrium are given."
    ],
    "c_categories":[
      "q-fin.PM"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12749",
    "c_title":[
      "Matrix phase-space representations in quantum optics"
    ],
    "c_abstract":[
      "We introduce matrix quantum phase-space distributions. These extend the idea\nof a quantum phase-space representation via projections onto a density matrix\nof global symmetry variables. The method is applied to verification of low-loss\nGaussian boson sampling (GBS) quantum computational advantage experiments with\nup to 10,000 modes, where classically generating photon-number counts is\nexponentially hard. We demonstrate improvements in sampling error by a factor\nof 1000 or more compared to unprojected methods, which are infeasible for such\ncases."
    ],
    "c_categories":[
      "math-ph",
      "math.MP",
      "physics.comp-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16299",
    "c_title":[
      "A calibration test for evaluating set-based epistemic uncertainty\n  representations"
    ],
    "c_abstract":[
      "The accurate representation of epistemic uncertainty is a challenging yet\nessential task in machine learning. A widely used representation corresponds to\nconvex sets of probabilistic predictors, also known as credal sets. One popular\nway of constructing these credal sets is via ensembling or specialized\nsupervised learning methods, where the epistemic uncertainty can be quantified\nthrough measures such as the set size or the disagreement among members. In\nprinciple, these sets should contain the true data-generating distribution. As\na necessary condition for this validity, we adopt the strongest notion of\ncalibration as a proxy. Concretely, we propose a novel statistical test to\ndetermine whether there is a convex combination of the set's predictions that\nis calibrated in distribution. In contrast to previous methods, our framework\nallows the convex combination to be instance dependent, recognizing that\ndifferent ensemble members may be better calibrated in different regions of the\ninput space. Moreover, we learn this combination via proper scoring rules,\nwhich inherently optimize for calibration. Building on differentiable,\nkernel-based estimators of calibration errors, we introduce a nonparametric\ntesting procedure and demonstrate the benefits of capturing instance-level\nvariability on of synthetic and real-world experiments."
    ],
    "c_categories":[
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.08367",
    "c_title":[
      "An equivariant Guillemin trace formula"
    ],
    "c_abstract":[
      "Guillemin's trace formula is an expression for the distributional trace of an\noperator defined by pulling back functions along a flow on a compact manifold.\nWe obtain an equivariant generalisation of this formula, for proper, cocompact\ngroup actions. This is motivated by the construction of an equivariant version\nof the Ruelle dynamical $\\zeta$-function in another paper by the same authors,\nwhich is based on the equivariant Guillemin trace formula. To obtain this\nformula, we first develop an equivariant version of the distributional trace\nthat appears in Guillemin's formula and other places."
    ],
    "c_categories":[
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00319",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "Speaker Diarization with LSTM"
    ],
    "b_abstract":[
      "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12610",
    "c_title":[
      "Eyring-Kramers Law for the Underdamped Langevin Process"
    ],
    "c_abstract":[
      "Consider the underdamped Langevin process $(q(t),p(t))_{t\\geq0}$ in\n$\\R^d\\times\\R^d$. We derive the low-temperature asymptotic of its\nmean-transition time between basins of attraction for a double-well potential.\nThis asymptotic is called Eyring-Kramers law and often relies in the literature\non Potential theory tools which are ill-defined for hypoelliptic processes like\nthe underdamped Langevin process. In this work, we implement a novel approach\nwhich circumvents the use of these traditional methods."
    ],
    "c_categories":[
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"Spatial Clustering of Molecular Localizations with Graph Neural Networks",
    "a_abstract":"Single-molecule localization microscopy generates point clouds corresponding\nto fluorophore localizations. Spatial cluster identification and analysis of\nthese point clouds are crucial for extracting insights about molecular\norganization. However, this task becomes challenging in the presence of\nlocalization noise, high point density, or complex biological structures. Here,\nwe introduce MIRO (Multimodal Integration through Relational Optimization), an\nalgorithm that uses recurrent graph neural networks to transform the point\nclouds in order to improve clustering efficiency when applying conventional\nclustering techniques. We show that MIRO supports simultaneous processing of\nclusters of different shapes and at multiple scales, demonstrating improved\nperformance across varied datasets. Our comprehensive evaluation demonstrates\nMIRO's transformative potential for single-molecule localization applications,\nshowcasing its capability to revolutionize cluster analysis and provide\naccurate, reliable details of molecular architecture. In addition, MIRO's\nrobust clustering capabilities hold promise for applications in various fields\nsuch as neuroscience, for the analysis of neural connectivity patterns, and\nenvironmental science, for studying spatial distributions of ecological data.",
    "explanation":"Single-molecule localization microscopy generates point clouds corresponding to\nfluorophore localizations.  Here, we introduce MIRO (Mul-\ntimodal Integration through Relational Optimization), an algorithm that uses\nrecurrent graph neural networks to transform the point clouds in order to improve\nclustering efficiency when applying conventional clustering techniques.",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b22"
    ],
    "c_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "c_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.20927",
    "c_title":[
      "Goal-Oriented Semantic Communication for Wireless Video Transmission via\n  Generative AI"
    ],
    "c_abstract":[
      "Efficient video transmission is essential for seamless communication and\ncollaboration within the visually-driven digital landscape. To achieve low\nlatency and high-quality video transmission over a bandwidth-constrained noisy\nwireless channel, we propose a stable diffusion (SD)-based goal-oriented\nsemantic communication (GSC) framework. In this framework, we first design a\nsemantic encoder that effectively identify the keyframes from video and extract\nthe relevant semantic information (SI) to reduce the transmission data size. We\nthen develop a semantic decoder to reconstruct the keyframes from the received\nSI and further generate the full video from the reconstructed keyframes using\nframe interpolation to ensure high-quality reconstruction. Recognizing the\nimpact of wireless channel noise on SI transmission, we also propose an\nSD-based denoiser for GSC (SD-GSC) condition on an instantaneous channel gain\nto remove the channel noise from the received noisy SI under a known channel.\nFor scenarios with an unknown channel, we further propose a parallel SD\ndenoiser for GSC (PSD-GSC) to jointly learn the distribution of channel gains\nand denoise the received SI. It is shown that, with the known channel, our\nproposed SD-GSC outperforms state-of-the-art ADJSCC, Latent-Diff DNSC, DeepWiVe\nand DVST, improving Peak Signal-to-Noise Ratio (PSNR) by 69%, 58%, 33% and 38%,\nreducing mean squared error (MSE) by 52%, 50%, 41% and 45%, and reducing\nFr\\'echet Video Distance (FVD) by 38%, 32%, 22% and 24%, respectively. With the\nunknown channel, our PSD-GSC achieves a 17% improvement in PSNR, a 29%\nreduction in MSE, and a 19% reduction in FVD compared to MMSE\nequalizer-enhanced SD-GSC. These significant performance improvements\ndemonstrate the robustness and superiority of our proposed methods in enhancing\nvideo transmission quality and efficiency under various channel conditions."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07127",
    "c_title":[
      "QoE-oriented Communication Service Provision for Annotation Rendering in\n  Mobile Augmented Reality"
    ],
    "c_abstract":[
      "As mobile augmented reality (MAR) continues to evolve, future 6G networks\nwill play a pivotal role in supporting immersive and personalized user\nexperiences. In this paper, we address the communication service provision\nproblem for annotation rendering in edge-assisted MAR, with the objective of\noptimizing spectrum resource utilization while ensuring the required quality of\nexperience (QoE) for MAR users. To overcome the challenges of user-specific\nuplink data traffic patterns and the complex operational mechanisms of\nannotation rendering, we propose a digital twin (DT)-based approach. We first\ndesign a DT specifically tailored for MAR applications to learn key annotation\nrendering mechanisms, enabling the network controller to access MAR\napplication-specific information. Then, we develop a DT based QoE modeling\napproach to capture the unique relationship between individual user QoE and\nspectrum resource demands. Finally, we propose a QoE-oriented resource\nallocation algorithm that decreases resource utilization compared to\nconventional net work slicing-based approaches. Simulation results demonstrate\nthat our DT-based approach outperforms benchmark approaches in the accuracy and\ngranularity of QoE modeling."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.01472",
    "c_title":[
      "Transferring between sparse and dense matching via probabilistic\n  reweighting"
    ],
    "c_abstract":[
      "Detector-based and detector-free matchers are only applicable within their\nrespective sparsity ranges. To improve adaptability of existing matchers, this\npaper introduces a novel probabilistic reweighting method. Our method is\napplicable to Transformer-based matching networks and adapts them to different\nsparsity levels without altering network parameters. The reweighting approach\nadjusts attention weights and matching scores using detection probabilities of\nfeatures. And we prove that the reweighted matching network is the asymptotic\nlimit of detector-based matching network. Furthermore, we propose a sparse\ntraining and pruning pipeline for detector-free networks based on reweighting.\nReweighted versions of SuperGlue, LightGlue, and LoFTR are implemented and\nevaluated across different levels of sparsity. Experiments show that the\nreweighting method improves pose accuracy of detector-based matchers on dense\nfeatures. And the performance of reweighted sparse LoFTR is comparable to\ndetector-based matchers, demonstrating good flexibility in balancing accuracy\nand computational complexity."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08675",
    "c_title":[
      "Improving Lesion Segmentation in Medical Images by Global and Regional\n  Feature Compensation"
    ],
    "c_abstract":[
      "Automated lesion segmentation of medical images has made tremendous\nimprovements in recent years due to deep learning advancements. However,\naccurately capturing fine-grained global and regional feature representations\nremains a challenge. Many existing methods obtain suboptimal performance on\ncomplex lesion segmentation due to information loss during typical downsampling\noperations and the insufficient capture of either regional or global features.\nTo address these issues, we propose the Global and Regional Compensation\nSegmentation Framework (GRCSF), which introduces two key innovations: the\nGlobal Compensation Unit (GCU) and the Region Compensation Unit (RCU). The\nproposed GCU addresses resolution loss in the U-shaped backbone by preserving\nglobal contextual features and fine-grained details during multiscale\ndownsampling. Meanwhile, the RCU introduces a self-supervised learning (SSL)\nresidual map generated by Masked Autoencoders (MAE), obtained as pixel-wise\ndifferences between reconstructed and original images, to highlight regions\nwith potential lesions. These SSL residual maps guide precise lesion\nlocalization and segmentation through a patch-based cross-attention mechanism\nthat integrates regional spatial and pixel-level features. Additionally, the\nRCU incorporates patch-level importance scoring to enhance feature fusion by\nleveraging global spatial information from the backbone. Experiments on two\npublicly available medical image segmentation datasets, including brain stroke\nlesion and coronary artery calcification datasets, demonstrate that our GRCSF\noutperforms state-of-the-art methods, confirming its effectiveness across\ndiverse lesion types and its potential as a generalizable lesion segmentation\nsolution."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16014",
    "c_title":[
      "Spatial-Angular Representation Learning for High-Fidelity Continuous\n  Super-Resolution in Diffusion MRI"
    ],
    "c_abstract":[
      "Diffusion magnetic resonance imaging (dMRI) often suffers from low spatial\nand angular resolution due to inherent limitations in imaging hardware and\nsystem noise, adversely affecting the accurate estimation of microstructural\nparameters with fine anatomical details. Deep learning-based super-resolution\ntechniques have shown promise in enhancing dMRI resolution without increasing\nacquisition time. However, most existing methods are confined to either spatial\nor angular super-resolution, limiting their effectiveness in capturing detailed\nmicrostructural features. Furthermore, traditional pixel-wise loss functions\nstruggle to recover intricate image details essential for high-resolution\nreconstruction. To address these challenges, we propose SARL-dMRI, a novel\nSpatial-Angular Representation Learning framework for high-fidelity, continuous\nsuper-resolution in dMRI. SARL-dMRI explores implicit neural representations\nand spherical harmonics to model continuous spatial and angular\nrepresentations, simultaneously enhancing both spatial and angular resolution\nwhile improving microstructural parameter estimation accuracy. To further\npreserve image fidelity, a data-fidelity module and wavelet-based frequency\nloss are introduced, ensuring the super-resolved images remain consistent with\nthe original input and retain fine details. Extensive experiments demonstrate\nthat, compared to five other state-of-the-art methods, our method significantly\nenhances dMRI data resolution, improves the accuracy of microstructural\nparameter estimation, and provides better generalization capabilities. It\nmaintains stable performance even under a 45$\\times$ downsampling factor."
    ],
    "c_categories":[
      "eess.IV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.21274",
    "c_title":[
      "BAnG: Bidirectional Anchored Generation for Conditional RNA Design"
    ],
    "c_abstract":[
      "Designing RNA molecules that interact with specific proteins is a critical\nchallenge in experimental and computational biology. Existing computational\napproaches require a substantial amount of experimentally determined RNA\nsequences for each specific protein or a detailed knowledge of RNA structure,\nrestricting their utility in practice. To address this limitation, we develop\nRNA-BAnG, a deep learning-based model designed to generate RNA sequences for\nprotein interactions without these requirements. Central to our approach is a\nnovel generative method, Bidirectional Anchored Generation (BAnG), which\nleverages the observation that protein-binding RNA sequences often contain\nfunctional binding motifs embedded within broader sequence contexts. We first\nvalidate our method on generic synthetic tasks involving similar localized\nmotifs to those appearing in RNAs, demonstrating its benefits over existing\ngenerative approaches. We then evaluate our model on biological sequences,\nshowing its effectiveness for conditional RNA sequence design given a binding\nprotein."
    ],
    "c_categories":[
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09216",
    "c_title":[
      "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal"
    ],
    "c_abstract":[
      "Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy\/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties."
    ],
    "c_categories":[
      "cond-mat.supr-con"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.09542",
    "c_title":[
      "Constant-Overhead Fault-Tolerant Bell-Pair Distillation using High-Rate\n  Codes"
    ],
    "c_abstract":[
      "We present a fault-tolerant Bell-pair distillation scheme achieving constant\noverhead through high-rate quantum low-density parity-check (qLDPC) codes. Our\napproach maintains a constant distillation rate equal to the code rate - as\nhigh as $1\/3$ in our implementations - while requiring no additional overhead\nbeyond the physical qubits of the code. Full circuit-level analysis\ndemonstrates fault-tolerance for input Bell pair infidelities below a threshold\n$\\sim 5\\%$, readily achievable with near-term capabilities. Unlike previous\nproposals, our scheme keeps the output Bell pairs encoded in qLDPC codes at\neach node, eliminating decoding overhead and enabling direct use in distributed\nquantum applications through recent advances in qLDPC computation. These\nresults establish qLDPC-based distillation as a practical route toward\nresource-efficient quantum networks and distributed quantum computing."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16110",
    "c_title":[
      "Compact implicit high resolution numerical method for solving transport\n  problems with sorption isotherms"
    ],
    "c_abstract":[
      "This study investigates numerical methods to solve nonlinear transport\nproblems characterized by various sorption isotherms with a focus on the\nFreundlich type of isotherms. We describe and compare second order accurate\nnumerical schemes, focusing on implicit methods, to effectively model transport\nphenomena without stability restriction on the choice of time steps.\nFurthermore, a high resolution form of the method is proposed that limits a\npriori the second order accurate scheme towards first order accuracy to keep\nthe values of numerical solutions in a physically acceptable range.\n  Through numerical experiments, we demonstrate the effectiveness of high\nresolution methods in minimizing oscillations near discontinuities, thereby\nenhancing solution plausibility. The observed convergence rates confirm that\nthe second order accurate schemes achieve expected accuracy for smooth\nsolutions and that they yield significant improvements when compared with the\nresults of the first order scheme. As the computational cost of the compact\nimplicit method seems to be comparable to similar explicit ones with a clear\nprofit of unconditional stability, this research provides a practical tool\ntoward numerical simulations of nonlinear transport phenomena applicable in\nvarious fields such as contaminant transport in porous media or column liquid\nchromatography."
    ],
    "c_categories":[
      "cs.NA",
      "math.AP",
      "math.NA"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
    ],
    "b_abstract":[
      "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04289",
    "c_title":[
      "Defect Phonon Renormalization during Nonradiative Multiphonon\n  Transitions in Semiconductors"
    ],
    "c_abstract":[
      "As a typical nonradiative multiphonon transition in semiconductors, carrier\ncapture at defects is critical to the performance of semiconductor devices. Its\ntransition rate is usually calculated using the equal-mode approximation, which\nassumes that phonon modes and frequencies remain unchanged before and after the\ntransition. Using the carbon substitutional defect ($\\text{C}_\\text{N}$) in GaN\nas a benchmark, here we demonstrate that the phonon renormalization can be\nsignificant during defect relaxation, which causes errors as large as orders of\nmagnitude in the approximation. To address this issue, we consider (i)\nDuschinsky matrix connecting the initial-state and final-state phonons, which\naccounts for the changes in phonon modes and frequencies; and (ii) the\noff-diagonal contributions in total transition matrix element, which\nincorporates the cross terms of electron-phonon interactions between different\nmodes. With this improvement, the calculated transition rates show agreements\nwith experimental results within an order of magnitude. We believe the present\nmethod makes one step forward for the accurate calculation of multiphonon\ntransition rate, especially in cases with large defect relaxations."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04258",
    "c_title":[
      "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors"
    ],
    "c_abstract":[
      "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04056",
    "c_title":[
      "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications"
    ],
    "c_abstract":[
      "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08115",
    "c_title":[
      "The standard coil or globule phases cannot describe the denatured state\n  of structured proteins and intrinsically disordered proteins"
    ],
    "c_abstract":[
      "The concepts of globule and random coil were developed to describe the phases\nof homopolymers and then used to characterize the denatured state of structured\ncytosolic proteins and intrinsically disordered proteins. Using multi-scale\nmolecular dynamics simulations, we were able to explore the conformational\nspace of the disordered conformations of both types of protein under biological\nconditions in an affordable amount of computational time. By studying the size\nof the protein and the density correlations in space, we conclude that the\nstandard phases of homopolymers and the tools to detect them cannot be applied\nstraightforwardly to proteins."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17007",
    "c_title":[
      "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching"
    ],
    "c_abstract":[
      "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.06107",
    "c_title":[
      "An Evaluation on the Role of Non-Coding RNA in HIV Transcription and\n  Latency: A Review"
    ],
    "c_abstract":[
      "The existence of latent cellular reservoirs is recognized as the major\nbarrier to an HIV cure. Reactivating and eliminating \"shock and kill\" or\npermanently silencing \"block and lock\" the latent HIV reservoir, as well as\ngene editing, remain promising approaches, but so far have proven to be only\npartially successful. Moreover, using latency reversing agents or \"block and\nlock\" drugs pose additional considerations, including the ability to cause\ncellular toxicity, a potential lack of specificity for HIV, or low potency when\neach agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long\nnon-coding RNAs (lncRNAs) are becoming increasingly recognized as important\nregulators of gene expression. RNA-based approaches for combatting HIV latency\nrepresent a promising strategy since both miRNAs and lncRNAs are more cell-type\nand tissue specific than protein coding genes. Thus, a higher specificity of\ntargeting the latent HIV reservoir with less overall cellular toxicity can\nlikely be achieved. In this review, we summarize current knowledge about HIV\ngene expression regulation by miRNAs and lncRNAs encoded in the human genome,\nas well as regulatory molecules encoded in the HIV genome. We discuss both the\ntranscriptional and post-transcriptional regulation of HIV gene expression to\nalign with the current definition of latency, and describe RNA molecules that\neither promote HIV latency or have anti-latency properties. Finally, we provide\nperspectives on using each class of RNAs as potential targets for combatting\nHIV latency, and describe the complexity of the interactions between different\nRNA molecules, their protein targets, and HIV."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18207",
    "c_title":[
      "On the modelling of polyatomic molecules in kinetic theory"
    ],
    "c_abstract":[
      "This communication is both a pedagogical note for understanding polyatomic\nmodelling in kinetic theory and a ''cheat sheet'' for a series of corresponding\nconcepts and formulas. We explain, detail and relate three possible approaches\nfor modelling the polyatomic internal structure, that are: the internal states\napproach, well suited for physical modelling and general proofs, the internal\nenergy levels approach, useful for analytic studies and corresponding to the\ncommon models of the literature, and the internal energy quantiles approach,\nless known while being a powerful tool for particle-based numerical simulations\nsuch as Direct Simulation Monte-Carlo (DSMC). This note may in particular be\nuseful in the study of non-polytropic gases."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.15244",
    "c_title":[
      "Controlled Diagonal Catalyst Improves the Efficiency of Quantum\n  Annealing"
    ],
    "c_abstract":[
      "Quantum annealing is a promising algorithm for solving combinatorial\noptimization problems. It searches for the ground state of the Ising model,\nwhich corresponds to the optimal solution of a given combinatorial optimization\nproblem. The guiding principle of quantum annealing is the adiabatic theorem in\nquantum mechanics, which guarantees that a system remains in the ground state\nof its Hamiltonian if the time evolution is sufficiently slow. According to the\nadiabatic theorem, the runtime required for quantum annealing to satisfy the\nadiabaticity scales is inverse to the square of the minimum energy gap between\nthe ground state and the first excited state during time evolution. As a\nresult, finding the ground state becomes significantly more difficult when the\nenergy gap is small, creating a major bottleneck in quantum annealing.\nExpanding the energy gap is one strategy to improve the performance of quantum\nannealing; however, its implementation in actual hardware remains challenging.\nThis study proposes a method for efficiently solving instances with small\nenergy gaps by introducing additional local terms to the Hamiltonian and\nexploiting the diabatic transition remaining in the small energy gap. The\nproposed method achieves an approximate square speed up in time-to-solution\ncompared to the conventional quantum annealing. In addition, we investigate the\ntransferability of the parameters obtained with the proposed method."
    ],
    "c_categories":[
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17388",
    "c_title":[
      "Coexistence of continuous-variable quantum key distribution and\n  classical data over 120-km fiber"
    ],
    "c_abstract":[
      "Integrating quantum key distribution (QKD) with classical data transmission\nover the same fiber is crucial for scalable quantum-secured communication.\nHowever, noise from classical channels limits QKD distance. We demonstrate the\nlongest-distance continuous-variable QKD (CV-QKD) over 120 km (20 dB loss)\ncoexisting with a fully populated coarse wavelength-division multiplexing\n(CWDM) system. Natural mode filtering of the local oscillator and phase noise\nmitigation enabled this without additional filtering or wavelength\nreallocation. Benchmarking against a commercial discrete-variable QKD system\nand considering finite-size effects confirms the feasibility of CV-QKD as a\nplug-and-play solution for typical 80--100 km long-haul optical networks. Our\nresults set a record distance for CV-QKD, showing its potential for\ncost-effective, large-scale deployment in existing network infrastructure."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.20792",
    "c_title":[
      "Cavity-Enhanced Rydberg Atomic Superheterodyne Receiver"
    ],
    "c_abstract":[
      "High-sensitivity measurements of the microwave electric field are important\nin applications of communication and metrology. \\replaced{The sensitivity of\ntraditional Rydberg superheterodyne receivers in free space is effectively\ndetermined by the signal-to-noise ratio (SNR), which is often considered\nequivalent to sensitivity in practical sensing applications.}{The sensitivity\nof the traditional Rydberg superheterodyne receivers in free space is limited\nby signal-to-noise contrast.} In this work, we demonstrate a cavity-enhanced\nreceiver, where an optical cavity significantly amplifies the interaction\nbetween the probe light and cesium atoms, which substantially improves the\nsignal-to-noise ratio via enhancing the expansion coefficient \\( \\kappa \\).\n\\added{Here, $\\kappa$ is the edge slope of the single peak obtained by fitting\nthe double-peak EIT-AT spectrum, characterizing the response of the probe light\nto the frequency detuning of the coupling laser.}The sensitivity is thus\nboosted by a factor of approximately 19 dB. This study highlights the pivotal\nrole of optical cavities in advancing Rydberg-based detection systems, offering\na promising approach for high-sensitivity microwave electric field\nmeasurements."
    ],
    "c_categories":[
      "physics.atom-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00173",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b22"
    ],
    "b_title":[
      "A framework for evaluating the performance of SMLM cluster analysis algorithms"
    ],
    "b_abstract":[
      "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
    ],
    "b_categories":[
      "eess.IV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17016",
    "c_title":[
      "Viscosity solution to complex Hessian quotient equations"
    ],
    "c_abstract":[
      "In this paper, we prove the existence of viscosity solutions to complex\nHessian equations on compact Hermitian manifolds, assuming the existence of a\nstrict subsolution in the viscosity sense. The results cover the complex\nHessian quotient equations. This generalized our previous results where the\nequation needs to satisfy a determinant domination condition."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"Unsupervised Learning Approach to Anomaly Detection in Gravitational\n  Wave Data",
    "a_abstract":"Gravitational waves (GW), predicted by Einstein's General Theory of\nRelativity, provide a powerful probe of astrophysical phenomena and fundamental\nphysics. In this work, we propose an unsupervised anomaly detection method\nusing variational autoencoders (VAEs) to analyze GW time-series data. By\ntraining on noise-only data, the VAE accurately reconstructs noise inputs while\nfailing to reconstruct anomalies, such as GW signals, which results in\nmeasurable spikes in the reconstruction error. The method was applied to data\nfrom the LIGO H1 and L1 detectors. Evaluation on testing datasets containing\nboth noise and GW events demonstrated reliable detection, achieving an area\nunder the ROC curve (AUC) of 0.89. This study introduces VAEs as a robust,\nunsupervised approach for identifying anomalies in GW data, which offers a\nscalable framework for detecting known and potentially new phenomena in\nphysics.",
    "explanation":"Gravitational waves (GW), predicted by Einstein\u2019s General Theory of Relativity, provide a pow-\nerful probe of astrophysical phenomena and fundamental physics. In this work, we propose an\nunsupervised anomaly detection method using variational autoencoders (VAEs) to analyze GW\ntime-series data. ",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b4"
    ],
    "c_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "c_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.06242",
    "c_title":[
      "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and\n  Top-k Selection"
    ],
    "c_abstract":[
      "We present a novel technique for constructing differentiable order-type\noperations, including soft ranking, soft top-k selection, and soft\npermutations. Our approach leverages an efficient closed-form formula for the\ninverse of the function LapSum, defined as the sum of Laplace distributions.\nThis formulation ensures low computational and memory complexity in selecting\nthe highest activations, enabling losses and gradients to be computed in\n$O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our\nmethod outperforms state-of-the-art techniques for high-dimensional vectors and\nlarge $k$ values. Furthermore, we provide efficient implementations for both\nCPU and CUDA environments, underscoring the practicality and scalability of our\nmethod for large-scale ranking and differentiable ordering problems."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.14427",
    "c_title":[
      "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making\n  in Virtual Escape Rooms"
    ],
    "c_abstract":[
      "Escape rooms present a unique cognitive challenge that demands\nexploration-driven planning: players should actively search their environment,\ncontinuously update their knowledge based on new discoveries, and connect\ndisparate clues to determine which elements are relevant to their objectives.\nMotivated by this, we introduce VisEscape, a benchmark of 20 virtual escape\nrooms specifically designed to evaluate AI models under these challenging\nconditions, where success depends not only on solving isolated puzzles but also\non iteratively constructing and refining spatial-temporal knowledge of a\ndynamically changing environment. On VisEscape, we observe that even\nstate-of-the-art multimodal models generally fail to escape the rooms, showing\nconsiderable variation in their levels of progress and trajectories. To address\nthis issue, we propose VisEscaper, which effectively integrates Memory,\nFeedback, and ReAct modules, demonstrating significant improvements by\nperforming 3.7 times more effectively and 4.9 times more efficiently on average\ncompared to baseline agents."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.19308",
    "c_title":[
      "Ontological analysis of proactive life event services"
    ],
    "c_abstract":[
      "Life event service is a direct digital public service provided jointly by\nseveral governmental institutions so that a person can fulfill all the\nobligations and use all the rights that arise due to a particular event or\nsituation in personal life. Life event service consolidates several public\nservices related to the same life event into one service for the service\nconsumer. This paper presents an ontological analysis of life event services,\nwhich is based on the works by Guarino, Guizzardi, Nardi, Wagner, and others.\nThe purpose of the ontological analysis is to understand the meanings of life\nevent, proactive public service based on life event, and other related notions.\nThis kind of ontological analysis is crucial because for implementing the\nhardware and software architectures of e-government and digital public\nservices, it is essential to agree upon the precise meanings of the underlying\nterms."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.15580",
    "c_title":[
      "How Well Can AI Build SD Models?"
    ],
    "c_abstract":[
      "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.14264",
    "c_title":[
      "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics"
    ],
    "c_abstract":[
      "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.18396",
    "c_title":[
      "Finiteness properties of generalized Montr\\'eal functors with\n  applications to mod $p$ representations of $\\mathrm{GL}_n(\\mathbb{Q}_p)$"
    ],
    "c_abstract":[
      "The second named author previously constructed a functor\n$\\mathbb{V}^\\vee\\circ D^\\vee_\\Delta$ from the category of smooth $p$-power\ntorsion representations of $\\mathrm{GL}_n(\\mathbb{Q}_p)$ to the category of\ninductive limits of continuous representations on finite $p$-primary abelian\ngroups of the direct product $G_{\\mathbb{Q}_p,\\Delta}\\times\n\\mathbb{Q}_p^\\times$ of $(n-1)$ copies of the absolute Galois group of\n$\\mathbb{Q}_p$ and one copy of the multiplicative group $\\mathbb{Q}_p^\\times$.\nIn the present work we show that this functor attaches finite dimensional\nrepresentations on the Galois side to smooth $p$-power torsion representations\nof finite length on the automorphic side. This has some implications on the\nfiniteness properties of Breuil's functor, too. Moreover, $\\mathbb{V}^\\vee\\circ\nD^\\vee_\\Delta$ produces irreducible representations of\n$G_{\\mathbb{Q}_p,\\Delta}\\times \\mathbb{Q}_p^\\times$ when applied to irreducible\nobjects on the automorphic side and detects isomorphisms unless it vanishes.\nFurther, we determine the kernel of $D^\\vee_\\Delta$ when restricted to\nsuccessive extensions of subquotients of principal series. We use this to\ncharacterize representations that are parabolically induced from the product of\na torus and $\\mathrm{GL}_2(\\mathbb{Q}_p)$. Finally, we formulate a conjecture\nand prove partial results on the essential image."
    ],
    "c_categories":[
      "math.NT",
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.05989",
    "c_title":[
      "Learning about passivity from data"
    ],
    "c_abstract":[
      "This paper presents a data-driven methodology to estimate the storage\nfunction of a passive system. The methodology consists in parametrizing the\nstorage function with a dictionary then running a linear program. Results on a\nbenchmark are presented to illustrate its properties, including its robustness\nto noise. Various uses of the storage function that do not require knowledge of\na model are also discussed."
    ],
    "c_categories":[
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.16859",
    "c_title":[
      "Kato-Milne cohomology group over rational function fields in\n  characteristic 2, I"
    ],
    "c_abstract":[
      "Let F be a field of characteristic 2. In this paper we determine the\nKato-Milne cohomology of the rational function field F(x) in one variable x.\nThis will be done by proving an analogue of the Milnor exact sequence [4] in\nthe setting of Kato-Milne cohomology. As an application, we answer the open\ncase of the norm theorem for Kato-Milne cohomology that concerns separable\nirreducible polynomials in many variables. This completes a result of Mukhija\n[17, Theorem A.3] that gives the norm theorem for inseparable polynomials."
    ],
    "c_categories":[
      "math.AC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.02900",
    "c_title":[
      "A Note on the Convergence of Muon and Further"
    ],
    "c_abstract":[
      "In this note, we inspect the convergence of a new optimizer for pretraining\nLLMs, namely the Muon optimizer. Such an optimizer is closely related to a\nspecialized steepest descent method where the update direction is the minimizer\nof the quadratic approximation of the objective function under spectral norm.\nWe provide the convergence analysis on both versions of the optimizer and\ndiscuss its implications."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    ],
    "b_abstract":[
      "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
    ],
    "b_categories":[
      "physics.gen-ph"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.16223",
    "c_title":[
      "Statistical Inference for Low-Rank Tensor Models"
    ],
    "c_abstract":[
      "Statistical inference for tensors has emerged as a critical challenge in\nanalyzing high-dimensional data in modern data science. This paper introduces a\nunified framework for inferring general and low-Tucker-rank linear functionals\nof low-Tucker-rank signal tensors for several low-rank tensor models. Our\nmethodology tackles two primary goals: achieving asymptotic normality and\nconstructing minimax-optimal confidence intervals. By leveraging a debiasing\nstrategy and projecting onto the tangent space of the low-Tucker-rank manifold,\nwe enable inference for general and structured linear functionals, extending\nfar beyond the scope of traditional entrywise inference. Specifically, in the\nlow-Tucker-rank tensor regression or PCA model, we establish the computational\nand statistical efficiency of our approach, achieving near-optimal sample size\nrequirements (in regression model) and signal-to-noise ratio (SNR) conditions\n(in PCA model) for general linear functionals without requiring sparsity in the\nloading tensor. Our framework also attains both computationally and\nstatistically optimal sample size and SNR thresholds for low-Tucker-rank linear\nfunctionals. Numerical experiments validate our theoretical results, showcasing\nthe framework's utility in diverse applications. This work addresses\nsignificant methodological gaps in statistical inference, advancing tensor\nanalysis for complex and high-dimensional data environments."
    ],
    "c_categories":[
      "math.ST",
      "stat.ME",
      "stat.ML",
      "stat.TH"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04045",
    "c_title":[
      "A new type of Multiverse, G\\\"odel theorems and the nonstandard logic of\n  classical, quantum mechanics and quantum gravity"
    ],
    "c_abstract":[
      "The problem is posed of establishing a possible relationship between a new\ntype of Multi-verse representation, G\\\"odel undecidability theorems and the\nlogic of classical, quantum mechanics and quantum gravity. For this purpose\nexample cases of multi-verses are first discussed in the context of\nnon-relativistic classical, quantum mechanics and quantum gravity. As a result,\nit is confirmed that thanks to G\\\"odel theorems non-relativistic classical and\nquantum mechanics, as well as quantum gravity theory are incomplete. As a\nconsequence, they necessarily admit undecidable logical propositions and\ntherefore obey a three-way boolean logical, i.e., a propositional logic with\nthe three different logical truth values true, false and undecidable."
    ],
    "c_categories":[
      "physics.gen-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.15539",
    "c_title":[
      "Pinched Multi Affine Geometry and Confinement: Describing the Yang-Mills\n  Mass Gap"
    ],
    "c_abstract":[
      "We introduce a multi affine geometric framework in which spacetime curvature\nrelaxes non-instantaneously, subject to a fundamental Planck-scale limit on\nvolumetric contraction. This pinched geometry is shown to localize high-energy\ndistributions, leading to effective constraints on curvature that manifest as a\ndiscrete energy gap. Our analysis explores how this limiting curvature\ndispersion rate not only yields an intuitive explanation of the Yang-Mills mass\ngap by enforcing a finite tension between non-Abelian color sources. In\nparallel, we connect these results to an information-geometric viewpoint,\ndemonstrating how the Fisher-Rao curvature quantifies localization pinning in\nboth classical and quantum settings. The resulting picture suggests that\nquantized excitations and confinement emerge naturally once one accounts for a\nmaximum relaxation speed of curved manifolds. We conclude by outlining how\nthese ideas could be tested through lattice gauge theory comparisons and by\nexamining low-energy glueball spectra, shedding light on a potential geometric\nunification of gravitational and quantum phenomena."
    ],
    "c_categories":[
      "physics.gen-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15818",
    "c_title":[
      "Comment on work of Umrigar and Anderson \"Energy needed to propel a tiny\n  spacecraft to Proxima Centauri, and, an unstated assumption in Einstein's\n  1905 paper\" (arXiv:2502.04331v1)"
    ],
    "c_abstract":[
      "In a recent paper by Umrigar and Anderson, the authors make an important\nstatement: it is possible to misunderstand the behavior of physical objects\n(parts of a spacecraft) if that object has reached a velocity comparable to the\nspeed of light. Achieving such a velocity for a spacecraft is critical for\ninterstellar travel, since only traveling at such speeds makes travel to\ndistant stars possible within a human lifetime. Last years, several ideas have\nbeen proposed to achieve relativistic velocities for a spacecraft. The authors\nof these ideas understand that this task is very difficult due to technological\nobstacles. However, when describing these projects, the authors omit mentioning\npossible obstacles of physical origin. The paper by Umrigar and Anderson is the\nfirst to consider physical limitations on construction of future spacecraft. In\nthis Comment, it will be shown that, according to Umrigar and Anderson, not\nonly technological obstacles will arise in the project of interstellar flights,\nbut also the physical limitations of a spacecraft moving at relativistic\nvelocities may become more serious problems for the implementation of such a\nproject."
    ],
    "c_categories":[
      "physics.gen-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15950",
    "c_title":[
      "Thermodynamic-Complexity Duality: Embedding Computational Hardness as a\n  Thermodynamic Coordinate"
    ],
    "c_abstract":[
      "We propose a duality between thermodynamics and computational complexity,\nelevating the difficulty of a computational task to the status of a\nthermodynamic variable. By introducing a complexity measure C as a novel\ncoordinate, we formulate an extended first law, dU = T dS - p dV + ... + lambda\ndC, capturing energy costs beyond classical bit erasures. This perspective\nunifies ideas from Landauer's principle with the combinatorial overhead of hard\n(e.g., NP-complete) problems, suggesting that algorithmic intractability can\nmanifest as an additional contribution to thermodynamic potentials. We outline\nhow this \"complexity potential\" might produce phase-transition-like signatures\nin spin glasses, random constraint satisfaction, or advanced computing hardware\nnear minimal dissipation. We also discuss parallels with previous\ngeometry-information dualities, emphasize the role of complexity in shaping\nenergy landscapes, and propose experimental avenues (in reversible computing or\nspin-glass setups) to detect subtle thermodynamic signatures of computational\nhardness. This framework opens a route for systematically incorporating\ncomplexity constraints into physical modeling, offering a novel link between\nthe fundamental cost of computation and thermodynamic laws."
    ],
    "c_categories":[
      "physics.gen-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.15841",
    "c_title":[
      "Vacuum permittivity and gravitational refractive index revisited"
    ],
    "c_abstract":[
      "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other."
    ],
    "c_categories":[
      "physics.gen-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06701",
    "c_title":[
      "Sequential Portfolio Selection under Latent Side Information-Dependence\n  Structure: Optimality and Universal Learning Algorithms"
    ],
    "c_abstract":[
      "This paper investigates the investment problem of constructing an optimal\nno-short sequential portfolio strategy in a market with a latent dependence\nstructure between asset prices and partly unobservable side information, which\nis often high-dimensional. The results demonstrate that a dynamic strategy,\nwhich forms a portfolio based on perfect knowledge of the dependence structure\nand full market information over time, may not grow at a higher rate infinitely\noften than a constant strategy, which remains invariant over time.\nSpecifically, if the market is stationary, implying that the dependence\nstructure is statistically stable, the growth rate of an optimal dynamic\nstrategy, utilizing the maximum capacity of the entire market information,\nalmost surely decays over time into an equilibrium state, asymptotically\nconverging to the growth rate of a constant strategy.\n  Technically, this work reassesses the common belief that a constant strategy\nonly attains the optimal limiting growth rate of dynamic strategies when the\nmarket process is identically and independently distributed. By analyzing the\ndynamic log-optimal portfolio strategy as the optimal benchmark in a stationary\nmarket with side information, we show that a random optimal constant strategy\nalmost surely exists, even when a limiting growth rate for the dynamic strategy\ndoes not. Consequently, two approaches to learning algorithms for portfolio\nconstruction are discussed, demonstrating the safety of removing side\ninformation from the learning process while still guaranteeing an asymptotic\ngrowth rate comparable to that of the optimal dynamic strategy."
    ],
    "c_categories":[
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.PR",
      "q-fin.MF",
      "q-fin.PM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics",
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17927",
    "c_title":[
      "Engineering of Anyons on M5-Probes via Flux Quantization"
    ],
    "c_abstract":[
      "These extended lecture notes survey a novel derivation of anyonic topological\norder (as seen in fractional quantum Hall systems) on single magnetized\nM5-branes probing Seifert orbi-singularities (\"geometric engineering\" of\nanyons), which we motivate from fundamental open problems in the field of\nquantum computing.\n  The rigorous construction is non-Lagrangian and non-perturbative, based on\npreviously neglected global completion of the M5-brane's tensor field by\nflux-quantization consistent with its non-linear self-duality and its twisting\nby the bulk C-field. This exists only in little-studied non-abelian generalized\ncohomology theories, notably in a twisted equivariant (and \"twistorial\") form\nof unstable Cohomotopy (\"Hypothesis H\").\n  As a result, topological quantum observables form Pontrjagin homology\nalgebras of mapping spaces from the orbi-fixed worldvolume into a classifying\n2-sphere. Remarkably, results from algebraic topology imply from this the\nquantum observables and modular functor of abelian Chern-Simons theory, as well\nas braid group actions on defect anyons of the kind envisioned as hardware for\ntopologically protected quantum gates."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "hep-th",
      "math-ph",
      "math.AT",
      "math.MP",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.17214",
    "c_title":[
      "The Plateau Problem of Michell Trusses and Orthogonality in Springs"
    ],
    "c_abstract":[
      "Given finitely many pointed forces in the plane. Suppose that these forces\nsum up to zero and their net torques also sum up to zero. One can show that\nthere exists a system of springs whose boundary forces exactly counter-balance\nthese pointed forces. We will generalize to higher dimensions using the Cauchy\nstress tensor for elastic materials.\n  Given a system of springs, we can multiply the length of each spring with its\ncorresponding spring constant and then sum these products up. The result is\ncalled the total mass of the system. We are interested in the Plateau problem\nof the existence of the minimal spring system given a boundary condition.\n  This minimization problem was first introduced in 1904 by A. Michell. He\nshowed that a minimizer could smear out. The Michell Truss became known in\nmechanical engineering. It raised attention in optimal design, such as\nminimizing costs in building bridges. In 1960s and 1970s, the problem was\ndeveloped using PDE and convex analysis by introducing an equivalent dual\nmaximization problem. In 2008, Bouchitt\\'{e}, Gangbo, and Sppecher introduced\nlines of principal actions to generalize Hencky-Prandtle net to higher\ndimensional duality and proved that the minimizer can be found provided that it\nexists. In the unpublished notes of Gangbo, he also showed that if springs of\nthe same kind are optimal.\n  In this paper, we are going to solve the Plateau problem using two different\ntools in GMT: first, a minimizer can be viewed as a flat chain complex; second,\na minimizer can also be viewed as a current. At the end, we are going to show\none progress in discovering the topological properties of minimizers:\ncompressed and stretched springs must be perpendicular to each other at\nnon-boundary points.\n  I appreciate my advisor Prof. Robert Hardt for communicating with me\nregularly on this problem."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.10426",
    "c_title":[
      "Cubic norm pairs and hermitian cubic norm structures"
    ],
    "c_abstract":[
      "We generalize cubic norm structures to cubic norm pairs and extend hermitian\ncubic norm structures to arbitrary commutative unital rings. For the associated\n``skew dimension one structurable algebra\" of these pairs, we construct a\ncorresponding Lie algebra and a group of automorphisms of the Lie algebra.\nUsing the structure of this automorphism group, we also prove that each\nhermitian cubic norm structure induces an operator Kantor pair."
    ],
    "c_categories":[
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.19450",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Auto-Encoding Variational Bayes"
    ],
    "b_abstract":[
      "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02957",
    "c_title":[
      "Abscence of embedded spectrum for nonlinear Schr\\\"odinger equations\n  linearized around one dimensional ground states"
    ],
    "c_abstract":[
      "We consider the nonlinear Schr\\\"odinger equation in dimension one for a\ngeneric nonlinearity. We show that ground states do not have embedded\neigenvalues in the essential spectrum of their linearized operators."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"Neural Vector Tomography for Reconstructing a Magnetization Vector Field",
    "a_abstract":"Discretized techniques for vector tomographic reconstructions are prone to\nproducing artifacts in the reconstructions. The quality of these\nreconstructions may further deteriorate as the amount of noise increases. In\nthis work, we instead model the underlying vector fields using smooth neural\nfields. Owing to the fact that the activation functions in the neural network\nmay be chosen to be smooth and the domain is no longer pixelated, the model\nresults in high-quality reconstructions, even under presence of noise. In the\ncase where we have underlying global continuous symmetry, we find that the\nneural network substantially improves the accuracy of the reconstruction over\nthe existing techniques.",
    "explanation":" In this work, we instead model the underlying\nvector fields using smooth neural fields",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b15"
    ],
    "c_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "c_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.19140",
    "c_title":[
      "Transformation trees -- documentation of multimodal image registration"
    ],
    "c_abstract":[
      "Multimodal image registration plays a key role in creating digital patient\nmodels by combining data from different imaging techniques into a single\ncoordinate system. This process often involves multiple sequential and\ninterconnected transformations, which must be well-documented to ensure\ntransparency and reproducibility. In this paper, we propose the use of\ntransformation trees as a method for structured recording and management of\nthese transformations. This approach has been implemented in the dpVision\nsoftware and uses a dedicated .dpw file format to store hierarchical\nrelationships between images, transformations, and motion data. Transformation\ntrees allow precise tracking of all image processing steps, reduce the need to\nstore multiple copies of the same data, and enable the indirect registration of\nimages that do not share common reference points. This improves the\nreproducibility of the analyses and facilitates later processing and\nintegration of images from different sources. The practical application of this\nmethod is demonstrated with examples from orthodontics, including the\nintegration of 3D face scans, intraoral scans, and CBCT images, as well as the\ndocumentation of mandibular motion. Beyond orthodontics, this method can be\napplied in other fields that require systematic management of image\nregistration processes, such as maxillofacial surgery, oncology, and\nbiomechanical analysis. Maintaining long-term data consistency is essential for\nboth scientific research and clinical practice. It enables easier comparison of\nresults in longitudinal studies, improves retrospective analysis, and supports\nthe development of artificial intelligence algorithms by providing standardized\nand well-documented datasets. The proposed approach enhances data organization,\nallows for efficient analysis, and facilitates the reuse of information in\nfuture studies and diagnostic procedures."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.11892",
    "c_title":[
      "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal\n  Representation Learning"
    ],
    "c_abstract":[
      "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https:\/\/taco-group.github.io\/DecAlign and the\ncode is available at https:\/\/github.com\/taco-group\/DecAlign."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.06027",
    "c_title":[
      "Geometric-Based Nail Segmentation for Clinical Measurements"
    ],
    "c_abstract":[
      "A robust segmentation method that can be used to perform measurements on\ntoenails is presented. The proposed method is used as the first step in a\nclinical trial to objectively quantify the incidence of a particular pathology.\nFor such an assessment, it is necessary to distinguish a nail, which locally\nappears to be similar to the skin. Many algorithms have been used, each of\nwhich leverages different aspects of toenail appearance. We used the Hough\ntransform to locate the tip of the toe and estimate the nail location and size.\nSubsequently, we classified the super-pixels of the image based on their\ngeometric and photometric information. Thereafter, the watershed transform\ndelineated the border of the nail. The method was validated using a 348-image\nmedical dataset, achieving an accuracy of 0.993 and an F-measure of 0.925. The\nproposed method is considerably robust across samples, with respect to factors\nsuch as nail shape, skin pigmentation, illumination conditions, and appearance\nof large regions affected by a medical condition"
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.12875",
    "c_title":[
      "An interpretable approach to automating the assessment of biofouling in\n  video footage"
    ],
    "c_abstract":[
      "Biofouling$\\unicode{x2013}$communities of organisms that grow on hard\nsurfaces immersed in water$\\unicode{x2013}$provides a pathway for the spread of\ninvasive marine species and diseases. To address this risk, international\nvessels are increasingly being obligated to provide evidence of their\nbiofouling management practices. Verification that these activities are\neffective requires underwater inspections, using divers or underwater remotely\noperated vehicles (ROVs), and the collection and analysis of large amounts of\nimagery and footage. Automated assessment using computer vision techniques can\nsignificantly streamline this process, and this work shows how this challenge\ncan be addressed efficiently and effectively using the interpretable Component\nFeatures (ComFe) approach with a DINOv2 Vision Transformer (ViT) foundation\nmodel. ComFe is able to obtain improved performance in comparison to previous\nnon-interpretable Convolutional Neural Network (CNN) methods, with\nsignificantly fewer weights and greater transparency$\\unicode{x2013}$through\nidentifying which regions of the image contribute to the classification, and\nwhich images in the training data lead to that conclusion. All code, data and\nmodel weights are publicly released."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.08580",
    "c_title":[
      "Ultrasound Image Generation using Latent Diffusion Models"
    ],
    "c_abstract":[
      "Diffusion models for image generation have been a subject of increasing\ninterest due to their ability to generate diverse, high-quality images. Image\ngeneration has immense potential in medical imaging because open-source medical\nimages are difficult to obtain compared to natural images, especially for rare\nconditions. The generated images can be used later to train classification and\nsegmentation models. In this paper, we propose simulating realistic ultrasound\n(US) images by successive fine-tuning of large diffusion models on different\npublicly available databases. To do so, we fine-tuned Stable Diffusion, a\nstate-of-the-art latent diffusion model, on BUSI (Breast US Images) an\nultrasound breast image dataset. We successfully generated high-quality US\nimages of the breast using simple prompts that specify the organ and pathology,\nwhich appeared realistic to three experienced US scientists and a US\nradiologist. Additionally, we provided user control by conditioning the model\nwith segmentations through ControlNet. We will release the source code at\nhttp:\/\/code.sonography.ai\/ to allow fast US image generation to the scientific\ncommunity."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.06701",
    "c_title":[
      "Sequential Portfolio Selection under Latent Side Information-Dependence\n  Structure: Optimality and Universal Learning Algorithms"
    ],
    "c_abstract":[
      "This paper investigates the investment problem of constructing an optimal\nno-short sequential portfolio strategy in a market with a latent dependence\nstructure between asset prices and partly unobservable side information, which\nis often high-dimensional. The results demonstrate that a dynamic strategy,\nwhich forms a portfolio based on perfect knowledge of the dependence structure\nand full market information over time, may not grow at a higher rate infinitely\noften than a constant strategy, which remains invariant over time.\nSpecifically, if the market is stationary, implying that the dependence\nstructure is statistically stable, the growth rate of an optimal dynamic\nstrategy, utilizing the maximum capacity of the entire market information,\nalmost surely decays over time into an equilibrium state, asymptotically\nconverging to the growth rate of a constant strategy.\n  Technically, this work reassesses the common belief that a constant strategy\nonly attains the optimal limiting growth rate of dynamic strategies when the\nmarket process is identically and independently distributed. By analyzing the\ndynamic log-optimal portfolio strategy as the optimal benchmark in a stationary\nmarket with side information, we show that a random optimal constant strategy\nalmost surely exists, even when a limiting growth rate for the dynamic strategy\ndoes not. Consequently, two approaches to learning algorithms for portfolio\nconstruction are discussed, demonstrating the safety of removing side\ninformation from the learning process while still guaranteeing an asymptotic\ngrowth rate comparable to that of the optimal dynamic strategy."
    ],
    "c_categories":[
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.PR",
      "q-fin.MF",
      "q-fin.PM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics",
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17927",
    "c_title":[
      "Engineering of Anyons on M5-Probes via Flux Quantization"
    ],
    "c_abstract":[
      "These extended lecture notes survey a novel derivation of anyonic topological\norder (as seen in fractional quantum Hall systems) on single magnetized\nM5-branes probing Seifert orbi-singularities (\"geometric engineering\" of\nanyons), which we motivate from fundamental open problems in the field of\nquantum computing.\n  The rigorous construction is non-Lagrangian and non-perturbative, based on\npreviously neglected global completion of the M5-brane's tensor field by\nflux-quantization consistent with its non-linear self-duality and its twisting\nby the bulk C-field. This exists only in little-studied non-abelian generalized\ncohomology theories, notably in a twisted equivariant (and \"twistorial\") form\nof unstable Cohomotopy (\"Hypothesis H\").\n  As a result, topological quantum observables form Pontrjagin homology\nalgebras of mapping spaces from the orbi-fixed worldvolume into a classifying\n2-sphere. Remarkably, results from algebraic topology imply from this the\nquantum observables and modular functor of abelian Chern-Simons theory, as well\nas braid group actions on defect anyons of the kind envisioned as hardware for\ntopologically protected quantum gates."
    ],
    "c_categories":[
      "cond-mat.str-el",
      "hep-th",
      "math-ph",
      "math.AT",
      "math.MP",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17214",
    "c_title":[
      "The Plateau Problem of Michell Trusses and Orthogonality in Springs"
    ],
    "c_abstract":[
      "Given finitely many pointed forces in the plane. Suppose that these forces\nsum up to zero and their net torques also sum up to zero. One can show that\nthere exists a system of springs whose boundary forces exactly counter-balance\nthese pointed forces. We will generalize to higher dimensions using the Cauchy\nstress tensor for elastic materials.\n  Given a system of springs, we can multiply the length of each spring with its\ncorresponding spring constant and then sum these products up. The result is\ncalled the total mass of the system. We are interested in the Plateau problem\nof the existence of the minimal spring system given a boundary condition.\n  This minimization problem was first introduced in 1904 by A. Michell. He\nshowed that a minimizer could smear out. The Michell Truss became known in\nmechanical engineering. It raised attention in optimal design, such as\nminimizing costs in building bridges. In 1960s and 1970s, the problem was\ndeveloped using PDE and convex analysis by introducing an equivalent dual\nmaximization problem. In 2008, Bouchitt\\'{e}, Gangbo, and Sppecher introduced\nlines of principal actions to generalize Hencky-Prandtle net to higher\ndimensional duality and proved that the minimizer can be found provided that it\nexists. In the unpublished notes of Gangbo, he also showed that if springs of\nthe same kind are optimal.\n  In this paper, we are going to solve the Plateau problem using two different\ntools in GMT: first, a minimizer can be viewed as a flat chain complex; second,\na minimizer can also be viewed as a current. At the end, we are going to show\none progress in discovering the topological properties of minimizers:\ncompressed and stretched springs must be perpendicular to each other at\nnon-boundary points.\n  I appreciate my advisor Prof. Robert Hardt for communicating with me\nregularly on this problem."
    ],
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.10426",
    "c_title":[
      "Cubic norm pairs and hermitian cubic norm structures"
    ],
    "c_abstract":[
      "We generalize cubic norm structures to cubic norm pairs and extend hermitian\ncubic norm structures to arbitrary commutative unital rings. For the associated\n``skew dimension one structurable algebra\" of these pairs, we construct a\ncorresponding Lie algebra and a group of automorphisms of the Lie algebra.\nUsing the structure of this automorphism group, we also prove that each\nhermitian cubic norm structure induces an operator Kantor pair."
    ],
    "c_categories":[
      "math.RA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Three-dimensional nanomagnetism"
    ],
    "b_abstract":[
      "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
    ],
    "b_categories":[
      "cond-mat.mes-hall"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.02957",
    "c_title":[
      "Abscence of embedded spectrum for nonlinear Schr\\\"odinger equations\n  linearized around one dimensional ground states"
    ],
    "c_abstract":[
      "We consider the nonlinear Schr\\\"odinger equation in dimension one for a\ngeneric nonlinearity. We show that ground states do not have embedded\neigenvalues in the essential spectrum of their linearized operators."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12671",
    "c_title":[
      "Stability of Majorana modes in Coulomb-disordered topological insulator\n  nanowires"
    ],
    "c_abstract":[
      "We evaluate theoretically the possibility to realize Majorana zero modes in\nhybrid devices made from topological-insulator (TI) nanowires proximity-coupled\nto a superconductor. Such systems have been suggested as building blocks of\nfuture topological quantum computers, as they have been predicted to realize\nMajorana zero modes protected by large gaps. A main obstacle is, however, the\npresence of a relatively large density of charged impurities, $n_\\text{imp}\\sim\n10^{19}$cm$^{-3}$. Based on extensive numerical simulations, we show that the\nproximity to the superconductor leads to an efficient screening of the disorder\npotential. By analyzing the Majorana splitting energy, the size of the Andreev\ngap and the localization of edge modes, we show that robust Majorana modes can\nbe realized for realistic levels of impurity concentrations and wire radii."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.14567",
    "c_title":[
      "Semiconducting and superconducting properties of 2D hexagonal materials"
    ],
    "c_abstract":[
      "The beginning of high interest in two-dimensional (2D) crystals is marked by\nthe synthesis of graphene, which constitutes exemplary monolayer material. This\nis due to the multiple extraordinary properties of graphene, particularly in\nthe field of quantum electronic phenomena. However, there are electronic\nfeatures that are notably missing in this material due to the inherent nature\nof its charge carriers. Of particular importance is that pristine graphene does\nnot exhibit semiconducting or superconducting properties, preventing related\napplications. Certain modifications to graphene or even synthesis of sibling\nmaterials is needed to arrive with semiconducting and superconducting 2D\nhexagonal materials. Here, the representative examples of such materials are\ndiscussed in detail along with their expected properties. Special attention is\ngiven to the unique semiconducting and superconducting phenomena found in these\nmaterials e.g. the non-adiabatic superconductivity, spin- and valley-dependent\nconductivity or the bulk-like Schottky-type potential barriers. The discussion\nis supplemented with some pertinent conclusions and perspectives for future\nwork."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13141",
    "c_title":[
      "Gate electrode-induced nonreciprocal resistance in topological\n  insulators"
    ],
    "c_abstract":[
      "A common method of controlling the chemical potential in topological\ninsulators is applying a gate electrode. Simultaneously applying high\nsource-drain bias currents can lead to parasitic effects in such devices. We\nderive that these parasitic effects lead to a gradient in the Hall effect along\nthe current lead of a Hall bar. Consequently, nonreciprocal effects in both\nlongitudinal and Hall voltages appear upon reversing the bias. These effects\nscale similarly to the magnetochiral anisotropy, requiring detailed analysis to\nmake a distinction. Experimentally we show that nonreciprocal effects can\nappear in materials where magnetochiral anisotropy is not expected while a top\ngate is present. Without gate electrode, this nonreciprocal effect is found to\nbe absent. These results show the importance of considering and, if possible,\nexcluding gate electrode-induced effects when searching for nonreciprocal\nresistance intrinsic to a material."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17875",
    "c_title":[
      "Heavy fermions, mass renormalization and local moments in magic-angle\n  twisted bilayer graphene via planar tunneling spectroscopy"
    ],
    "c_abstract":[
      "Topological heavy fermion models[1-5] describe the flat bands in magic-angle\ntwisted bilayer graphene (MATBG) as arising from the hybridization between\nlocalized flat-band orbitals (f-electrons) and nearly-free conduction bands\n(c-electrons). The interplay between these f-electrons and c-electrons is\ntheorized to give rise to emergent phenomena, including unconventional\nsuperconductivity[6-8], non-Fermi liquid behavior[9-11], and topologically\nnontrivial phases[12-14]. However, the fundamental properties of f- and\nc-electrons, such as their respective heavy and light effective mass and their\nproperties under strain, need experimental verification. Here we report on the\nelectronic inverse compressibility, effective mass, and entropy of MATBG,\nobtained from planar tunneling spectroscopy. Our results include the\nobservation of electron mass renormalization, found to be consistent with the\ntopological heavy fermion model prediction of heavy charge-one excitations away\nfrom integer fillings. Importantly, we present entropic evidence for 4-fold and\n8-fold degenerate isospin local moment states emerging at temperatures of 10K\nand 20K, respectively, consistent with the entropy of 8 heavy-fermions flavors\nenergetically split by the sample strain."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15433",
    "c_title":[
      "The Connection between Spin Wave Polarization and Dissipation"
    ],
    "c_abstract":[
      "This study establishes a fundamental connection between the dissipation and\npolarization of spin waves, which are often treated as independent phenomena.\nThrough theoretical analysis and numerical validation, we demonstrate that\nwithin the linearized spin wave regime, a spin wave mode's dissipation rate,\ndefined as the ratio of linewidth to the resonance frequency, exceeds Gilbert\ndamping by a factor given by its spatially averaged polarization. This average\nis governed by a non-positive definite weight, whose magnitude depends on the\nmagnon density of the local excitation, while its sign is dictated by the local\npolarization handedness. Remarkably, this universal connection applies across\ndiverse magnetic interactions and textures, offering crucial insights into spin\nwave dynamics and dissipation."
    ],
    "c_categories":[
      "cond-mat.mes-hall"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13894",
    "c_title":[
      "Bayesian high-dimensional biological pathway-guided mediation analysis\n  with application to metabolomics"
    ],
    "c_abstract":[
      "With advances in high-resolution mass spectrometry technologies, metabolomics\ndata are increasingly used to investigate biological mechanisms underlying\nassociations between exposures and health outcomes in clinical and\nepidemiological studies. Mediation analysis is a powerful framework for\ninvestigating a hypothesized causal chain and when applied to metabolomics\ndata, a large number of correlated metabolites belonging to interconnected\nmetabolic pathways need to be considered as mediators. To identify metabolic\npathways as active mediators, existing approaches typically focus on first\nidentifying individual metabolites as active mediators, followed by post-hoc\nmetabolic pathway determination. These multi-stage procedures make statistical\ninference challenging. We propose a Bayesian biological pathway-guided\nmediation analysis that aims to jointly analyze all metabolites together,\nidentify metabolic pathways directly, and estimate metabolic pathway-specific\nindirect effects. This is accomplished by incorporating existing biological\nknowledge of metabolic pathways to account for correlations among mediators,\nalong with variable selection and dimension reduction techniques. Advantages of\nthe proposed method is demonstrated in extensive simulation studies with\nreal-word metabolic pathway structure. We apply the proposed method to two\nstudies examining the role of metabolism in mediating (1) the effect of\nRoux-en-Y gastric bypass on glycemic control, and (2) the effect of prenatal\nexposure to per- and polyfluoroalkyl substances (PFAS) on gestational age at\nbirth. Our analyses confirm metabolic pathways previously identified and\nprovide additional uncertainty quantification for the mediation effects."
    ],
    "c_categories":[
      "stat.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16263",
    "c_title":[
      "PLS-based approach for fair representation learning"
    ],
    "c_abstract":[
      "We revisit the problem of fair representation learning by proposing Fair\nPartial Least Squares (PLS) components. PLS is widely used in statistics to\nefficiently reduce the dimension of the data by providing representation\ntailored for the prediction. We propose a novel method to incorporate fairness\nconstraints in the construction of PLS components. This new algorithm provides\na feasible way to construct such features both in the linear and the non linear\ncase using kernel embeddings. The efficiency of our method is evaluated on\ndifferent datasets, and we prove its superiority with respect to standard fair\nPCA method."
    ],
    "c_categories":[
      "cs.CY",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19248",
    "c_title":[
      "On the sharp quantitative stability of critical points of the\n  Hardy-Littlewood-Sobolev inequality in $\\mathbb{R}^{n}$ with $n\\geq3$"
    ],
    "c_abstract":[
      "Assume $n\\geq3$ and $u\\in \\dot{H}^1(\\mathbb{R}^n)$. Recently, Piccione, Yang\nand Zhao \\cite{Piccione-Yang-Zhao} established a nonlocal version of Struwe's\ndecomposition in \\cite{Struwe-1984}, i.e., if $u\\geq 0$ and\n$\\Gamma(u):=\\left\\|\\Delta\nu+D_{n,\\alpha}\\int_{\\mathbb{R}^{n}}\\frac{|u|^{p_{\\alpha}}(y)\n}{|x-y|^{\\alpha}}\\mathrm{d}y |u|^{p_{\\alpha}-2} u\\right\\|_{H^{-1}} \\rightarrow\n0$, then $dist(u,\\mathcal{T})\\to 0$, where $dist(u,\\mathcal{T})$ denotes the\n$\\dot{H}^1(\\mathbb{R}^n)$-distance of $u$ from the manifold of sums of Talenti\nbubbles. In this paper, we establish the nonlocal version of the quantitative\nestimates of Struwe's decomposition in Ciraolo, Figalli and Maggi \\cite{CFM}\nfor one bubble and $n\\geq3$, Figalli and Glaudo \\cite{Figalli-Glaudo2020} for\n$3\\leq n\\leq5$ and Deng, Sun and Wei \\cite{DSW} for $n\\geq6$ and two or more\nbubbles. We prove that for $n\\geq 3$, $\\alpha<n$ and $0<\\alpha\\leq 4$, \\[dist\n(u,\\mathcal{T})\\leq C\\begin{cases} \\Gamma(u)\\left|\\log\n\\Gamma(u)\\right|^{\\frac{1}{2}}\\quad&\\text{if } \\,\\, n=6 \\,\\, \\text{and} \\,\\,\n\\alpha=4, \\\\ \\Gamma(u) \\quad&\\text{for any other cases.}\\end{cases}\\]\nFurthermore, we show that this inequality is sharp for $N=6$ and $\\alpha=4$."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10190",
    "c_title":[
      "The multifractal nature of a parametrized family of von Koch functions"
    ],
    "c_abstract":[
      "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem."
    ],
    "c_categories":[
      "math.CA",
      "math.DS",
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.09927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
    ],
    "b_abstract":[
      "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.09423",
    "c_title":[
      "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction"
    ],
    "c_abstract":[
      "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"Modeling Continuous Spatial-temporal Dynamics of Turbulent Flow with\n  Test-time Refinement",
    "a_abstract":"The precise simulation of turbulent flows holds immense significance across\nvarious scientific and engineering domains, including climate science,\nfreshwater science, and energy-efficient manufacturing. Within the realm of\nsimulating turbulent flows, large eddy simulation (LES) has emerged as a\nprevalent alternative to direct numerical simulation (DNS), offering\ncomputational efficiency. However, LES cannot accurately capture the full\nspectrum of turbulent transport scales and is present only at a lower spatial\nresolution. Reconstructing high-fidelity DNS data from the lower-resolution LES\ndata is essential for numerous applications, but it poses significant\nchallenges to existing super-resolution techniques, primarily due to the\ncomplex spatio-temporal nature of turbulent flows. This paper proposes a novel\nflow reconstruction approach that leverages physical knowledge to model flow\ndynamics. Different from traditional super-resolution techniques, the proposed\napproach uses LES data only in the testing phase through a degradation-based\nrefinement approach to enforce physical constraints and mitigate cumulative\nreconstruction errors over time. Furthermore, a feature sampling strategy is\ndeveloped to enable flow data reconstruction across different resolutions. The\nresults on two distinct sets of turbulent flow data indicate the effectiveness\nof the proposed method in reconstructing high-resolution DNS data, preserving\nthe inherent physical attributes of flow transport, and achieving DNS\nreconstruction at different resolutions.",
    "explanation":"his paper proposes a novel flow recon-\nstruction approach that leverages physical knowledge to\nmodel flow dynamics.",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":[
      "b40"
    ],
    "c_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "c_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.17361",
    "c_title":[
      "A Closer Look at TabPFN v2: Strength, Limitation, and Extension"
    ],
    "c_abstract":[
      "Tabular datasets are inherently heterogeneous, posing significant challenges\nfor developing pre-trained foundation models. The recently introduced\ntransformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves\nunprecedented in-context learning accuracy across multiple tabular datasets,\nmarking a pivotal advancement in tabular foundation models. In this paper, we\ncomprehensively evaluate TabPFN v2 on over 300 datasets, confirming its\nexceptional generalization capabilities on small- to medium-scale tasks. Our\nanalysis identifies randomized feature tokens as a key factor behind TabPFN\nv2's success, as they unify heterogeneous datasets into a fixed-dimensional\nrepresentation, enabling more effective training and inference. To further\nunderstand TabPFN v2's predictions, we propose a leave-one-fold-out approach,\ntransforming TabPFN v2 into a feature extractor and revealing its capability to\nsimplify data distributions and boost accuracy. Lastly, to address TabPFN v2's\nlimitations in high-dimensional, large-scale, and many-category tasks, we\nintroduce a divide-and-conquer mechanism inspired by Chain-of-Thought\nprompting, enabling scalable inference. By uncovering the mechanisms behind\nTabPFN v2's success and introducing strategies to expand its applicability,\nthis study provides key insights into the future of tabular foundation models."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.03331",
    "c_title":[
      "Leap: Inductive Link Prediction via Learnable TopologyAugmentation"
    ],
    "c_abstract":[
      "Link prediction is a crucial task in many downstream applications of graph\nmachine learning. To this end, Graph Neural Network (GNN) is a widely used\ntechnique for link prediction, mainly in transductive settings, where the goal\nis to predict missing links between existing nodes. However, many real-life\napplications require an inductive setting that accommodates for new nodes,\ncoming into an existing graph. Thus, recently inductive link prediction has\nattracted considerable attention, and a multi-layer perceptron (MLP) is the\npopular choice of most studies to learn node representations. However, these\napproaches have limited expressivity and do not fully capture the graph's\nstructural signal. Therefore, in this work we propose LEAP, an inductive link\nprediction method based on LEArnable toPology augmentation. Unlike previous\nmethods, LEAP models the inductive bias from both the structure and node\nfeatures, and hence is more expressive. To the best of our knowledge, this is\nthe first attempt to provide structural contexts for new nodes via learnable\naugmentation in inductive settings. Extensive experiments on seven real-world\nhomogeneous and heterogeneous graphs demonstrates that LEAP significantly\nsurpasses SOTA methods. The improvements are up to 22\\% and 17\\% in terms of\nAUC and average precision, respectively. The code and datasets are available on\nGitHub (https:\/\/github.com\/AhmedESamy\/LEAP\/)"
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.14136",
    "c_title":[
      "Saliency Maps are Ambiguous: Analysis of Logical Relations on First and\n  Second Order Attributions"
    ],
    "c_abstract":[
      "Recent work uncovered potential flaws in \\eg attribution or heatmap based\nsaliency methods. A typical flaw is a confirmations bias, where the scores are\ncompared to human expectation. Since measuring the quality of saliency methods\nis hard due to missing ground truth model reasoning, finding general\nlimitations is also hard. This is further complicated, because masking-based\nevaluation on complex data can easily introduce a bias, as most methods cannot\nfully ignore inputs. In this work, we extend our previous analysis on the\nlogical dataset framework ANDOR, where we showed that all analysed saliency\nmethods fail to grasp all needed classification information for all possible\nscenarios. Specifically, this paper extends our previous work using analysis on\nmore datasets, in order to better understand in which scenarios the saliency\nmethods fail. Further, we apply the Global Coherence Representation as an\nadditional evaluation method in order to enable actual input omission."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.17896",
    "c_title":[
      "Explainable Machine Learning: An Illustration of Kolmogorov-Arnold\n  Network Model for Airfoil Lift Prediction"
    ],
    "c_abstract":[
      "Data science has emerged as fourth paradigm of scientific exploration.\nHowever many machine learning models operate as black boxes offering limited\ninsight into the reasoning behind their predictions. This lack of transparency\nis one of the drawbacks to generate new knowledge from data. Recently\nKolmogorov-Arnold Network or KAN has been proposed as an alternative model\nwhich embeds explainable AI. This study demonstrates the potential of KAN for\nnew scientific exploration. KAN along with five other popular supervised\nmachine learning models are applied to the well-known problem of airfoil lift\nprediction in aerospace engineering. Standard data generated from an earlier\nstudy on 2900 different airfoils is used. KAN performed the best with an R2\nscore of 96.17 percent on the test data, surpassing both the baseline model and\nMulti Layer Perceptron. Explainability of KAN is shown by pruning and\nsymbolizing the model resulting in an equation for coefficient of lift in terms\nof input variables. The explainable information retrieved from KAN model is\nfound to be consistent with the known physics of lift generation by airfoil\nthus demonstrating its potential to aid in scientific exploration."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.18235",
    "c_title":[
      "Enhance GNNs with Reliable Confidence Estimation via Adversarial\n  Calibration Learning"
    ],
    "c_abstract":[
      "Despite their impressive predictive performance, GNNs often exhibit poor\nconfidence calibration, i.e., their predicted confidence scores do not\naccurately reflect true correctness likelihood. This issue raises concerns\nabout their reliability in high-stakes domains such as fraud detection, and\nrisk assessment, where well-calibrated predictions are essential for\ndecision-making. To ensure trustworthy predictions, several GNN calibration\nmethods are proposed. Though they can improve global calibration, our\nexperiments reveal that they often fail to generalize across different node\ngroups, leading to inaccurate confidence in node groups with different degree\nlevels, classes, and local structures. In certain cases, they even degrade\ncalibration compared to the original uncalibrated GNN. To address this\nchallenge, we propose a novel AdvCali framework that adaptively enhances\ncalibration across different node groups. Our method leverages adversarial\ntraining to automatically identify mis-calibrated node groups and applies a\ndifferentiable Group Expected Calibration Error (ECE) loss term to refine\nconfidence estimation within these groups. This allows the model to dynamically\nadjust its calibration strategy without relying on dataset-specific prior\nknowledge about miscalibrated subgroups. Extensive experiments on real-world\ndatasets demonstrate that our approach not only improves global calibration but\nalso significantly enhances calibration within groups defined by feature\nsimilarity, topology, and connectivity, outperforming previous methods and\ndemonstrating its effectiveness in practical scenarios."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.13894",
    "c_title":[
      "Bayesian high-dimensional biological pathway-guided mediation analysis\n  with application to metabolomics"
    ],
    "c_abstract":[
      "With advances in high-resolution mass spectrometry technologies, metabolomics\ndata are increasingly used to investigate biological mechanisms underlying\nassociations between exposures and health outcomes in clinical and\nepidemiological studies. Mediation analysis is a powerful framework for\ninvestigating a hypothesized causal chain and when applied to metabolomics\ndata, a large number of correlated metabolites belonging to interconnected\nmetabolic pathways need to be considered as mediators. To identify metabolic\npathways as active mediators, existing approaches typically focus on first\nidentifying individual metabolites as active mediators, followed by post-hoc\nmetabolic pathway determination. These multi-stage procedures make statistical\ninference challenging. We propose a Bayesian biological pathway-guided\nmediation analysis that aims to jointly analyze all metabolites together,\nidentify metabolic pathways directly, and estimate metabolic pathway-specific\nindirect effects. This is accomplished by incorporating existing biological\nknowledge of metabolic pathways to account for correlations among mediators,\nalong with variable selection and dimension reduction techniques. Advantages of\nthe proposed method is demonstrated in extensive simulation studies with\nreal-word metabolic pathway structure. We apply the proposed method to two\nstudies examining the role of metabolism in mediating (1) the effect of\nRoux-en-Y gastric bypass on glycemic control, and (2) the effect of prenatal\nexposure to per- and polyfluoroalkyl substances (PFAS) on gestational age at\nbirth. Our analyses confirm metabolic pathways previously identified and\nprovide additional uncertainty quantification for the mediation effects."
    ],
    "c_categories":[
      "stat.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.16263",
    "c_title":[
      "PLS-based approach for fair representation learning"
    ],
    "c_abstract":[
      "We revisit the problem of fair representation learning by proposing Fair\nPartial Least Squares (PLS) components. PLS is widely used in statistics to\nefficiently reduce the dimension of the data by providing representation\ntailored for the prediction. We propose a novel method to incorporate fairness\nconstraints in the construction of PLS components. This new algorithm provides\na feasible way to construct such features both in the linear and the non linear\ncase using kernel embeddings. The efficiency of our method is evaluated on\ndifferent datasets, and we prove its superiority with respect to standard fair\nPCA method."
    ],
    "c_categories":[
      "cs.CY",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2501.19248",
    "c_title":[
      "On the sharp quantitative stability of critical points of the\n  Hardy-Littlewood-Sobolev inequality in $\\mathbb{R}^{n}$ with $n\\geq3$"
    ],
    "c_abstract":[
      "Assume $n\\geq3$ and $u\\in \\dot{H}^1(\\mathbb{R}^n)$. Recently, Piccione, Yang\nand Zhao \\cite{Piccione-Yang-Zhao} established a nonlocal version of Struwe's\ndecomposition in \\cite{Struwe-1984}, i.e., if $u\\geq 0$ and\n$\\Gamma(u):=\\left\\|\\Delta\nu+D_{n,\\alpha}\\int_{\\mathbb{R}^{n}}\\frac{|u|^{p_{\\alpha}}(y)\n}{|x-y|^{\\alpha}}\\mathrm{d}y |u|^{p_{\\alpha}-2} u\\right\\|_{H^{-1}} \\rightarrow\n0$, then $dist(u,\\mathcal{T})\\to 0$, where $dist(u,\\mathcal{T})$ denotes the\n$\\dot{H}^1(\\mathbb{R}^n)$-distance of $u$ from the manifold of sums of Talenti\nbubbles. In this paper, we establish the nonlocal version of the quantitative\nestimates of Struwe's decomposition in Ciraolo, Figalli and Maggi \\cite{CFM}\nfor one bubble and $n\\geq3$, Figalli and Glaudo \\cite{Figalli-Glaudo2020} for\n$3\\leq n\\leq5$ and Deng, Sun and Wei \\cite{DSW} for $n\\geq6$ and two or more\nbubbles. We prove that for $n\\geq 3$, $\\alpha<n$ and $0<\\alpha\\leq 4$, \\[dist\n(u,\\mathcal{T})\\leq C\\begin{cases} \\Gamma(u)\\left|\\log\n\\Gamma(u)\\right|^{\\frac{1}{2}}\\quad&\\text{if } \\,\\, n=6 \\,\\, \\text{and} \\,\\,\n\\alpha=4, \\\\ \\Gamma(u) \\quad&\\text{for any other cases.}\\end{cases}\\]\nFurthermore, we show that this inequality is sharp for $N=6$ and $\\alpha=4$."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2503.10190",
    "c_title":[
      "The multifractal nature of a parametrized family of von Koch functions"
    ],
    "c_abstract":[
      "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem."
    ],
    "c_categories":[
      "math.CA",
      "math.DS",
      "math.FA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Model-free simulations of turbulent reactive flows"
    ],
    "b_abstract":[
      "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
    ],
    "b_categories":[
      "physics.flu-dyn"
    ],
    "b_fields":[
      "Physics"
    ],
    "c_id":"2502.09423",
    "c_title":[
      "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction"
    ],
    "c_abstract":[
      "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13966",
    "c_title":[
      "High-efficient machine learning projection method for incompressible\n  Navier-Stokes equations"
    ],
    "c_abstract":[
      "This study proposes a high-efficient machine learning (ML) projection method\nusing forward-generated data for incompressible Navier-Stokes equations. A\nPoisson neural network (Poisson-NN) embedded method and a wavelet transform\nconvolutional neural network multigrid (WTCNN-MG) method are proposed,\nintegrated into the projection method framework in patchwork and overall\ndifferentiable manners with MG method, respectively. The solution of the\npressure Poisson equation split from the Navier-Stokes equations is first\ngenerated either following a random field (e.g. Gaussian random field, GRF,\ncomputational complexity O(NlogN), N is the number of spatial points) or\nphysical laws (e.g. a kind of spectra, computational complexity O(NM), M is the\nnumber of modes), then the source terms, boundary conditions and initial\nconditions are constructed via balance of equations, avoiding the difficulties\nof obtaining high-fidelity training datasets. The feasibility of generated data\nfor training Poisson-NN and WTCNN as well as the acceleration performances of\nthe Poisson-NN embedded method and WTCNN-MG method are validated. The results\nindicate that even without any DNS data, the generated data can train these two\nmodels with excellent generalization and accuracy. The data following physical\nlaws can significantly improve the high-frequency approximation, convergence\nrate, generalization and accuracy than that generated following GRF. The ML\nprojection method offers significant improvements in computational efficiency.\nParticularly, the Poisson-NN embedded method achieves an average speed-up of\n5.83 times over the traditional MG method, while the WTCNN-MG method offers an\neven greater average speed-up of 7.03 times, demonstrating impressive\nacceleration performance."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11309",
    "c_title":[
      "Wake transition and aerodynamics of a dragonfly-inspired airfoil"
    ],
    "c_abstract":[
      "We investigate the dynamics and the stability of the incompressible flow past\na corrugated dragonfly-inspired airfoil in the two-dimensional (2D) $\\alpha-Re$\nparameter space, where $\\alpha$ is the angle of attack and $Re$ is the Reynolds\nnumber. The angle of attack is varied between $-5^\\circ \\le \\alpha \\le\n10^\\circ$, and $Re$ (based on the free-stream velocity and the airfoil chord)\nis increased up to $Re=6000$. The study relies on linear stability analyses and\nthree-dimensional (3D) nonlinear direct numerical simulations. For all $\\alpha$\nthe primary instability consists of a Hopf bifurcation towards a periodic\nregime. The linear stability analysis reveals that two distinct modes drive the\nflow bifurcation for positive and negative $\\alpha$, being characterised by a\ndifferent frequency and a distinct triggering mechanism. The critical $Re$\ndecreases as $|\\alpha|$ increases, and scales as a power law for large\npositive\/negative $\\alpha$. At intermediate $Re$, different limit cycles arise\ndepending on $\\alpha$, each one characterised by a distinctive vortex\ninteraction, leading thus to secondary instabilities of different nature. For\nintermediate positive\/negative $\\alpha$ vortices are shed from both the\ntop\/bottom leading- and trailing-edge shear layers, and the two phenomena are\nfrequency locked. By means of Floquet stability analysis, we show that the\nsecondary instability consists of a 2D subharmonic bifurcation for large\nnegative $\\alpha$, of a 2D Neimark--Sacker bifurcation for small negative\n$\\alpha$, of a 3D pitchfork bifurcation for small positive $\\alpha$, and of a\n3D subharmonic bifurcation for large positive $\\alpha$. The aerodynamic\nperformance of the dragonfly-inspired airfoil is discussed in relation to the\ndifferent flow regimes emerging in the $\\alpha-Re$ space of parameters."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08401",
    "c_title":[
      "Mean resolvent analysis of periodic flows"
    ],
    "c_abstract":[
      "The mean resolvent operator predicts, in the frequency domain, the mean\nlinear response to forcing, and, as such, it provides the optimal LTI\napproximation of the input-output dynamics of flows in the statistically steady\nregime (Leclercq & Sipp 2023). In this paper, we aim at providing numerical\nframeworks to extract optimal forcings and responses of the mean resolvent,\nalso known as mean resolvent modes. For periodic flows, we rewrite the mean\nresolvent operator in terms of a harmonic resolvent operator (Wereley & Hall\n1990; Padovan & Rowley 2022) to obtain reference mean resolvent modes.\nSuccessively, we propose a projection algorithm approximating those modes\nwithin a subspace of mean-flow resolvent modes. The projected problem is\ndirectly solved in the frequency domain, but we also discuss a time-stepper\nversion that can bypass the explicit construction of the operator without\nrecurring to direct-adjoint looping. We evaluate the algorithms on an\nincompressible axisymmetric laminar jet periodically forced at the inlet. For a\nweakly unsteady case, the mean-flow resolvent correctly approximates the main\nreceptivity peak of the mean resolvent, but completely fails to capture a\nsecondary receptivity peak. For a strongly unsteady case, even the main\nreceptivity peak of the mean resolvent is incorrectly captured by the mean-flow\nresolvent. Although the present algorithms are currently restricted to periodic\nflows, input projection may be a key ingredient to extend mean resolvent\nanalysis to more complex statistically steady flows."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04847",
    "c_title":[
      "Using Diffusion Models for Reducing Spatiotemporal Errors of Deep\n  Learning Based Urban Microclimate Predictions at Post-Processing Stage"
    ],
    "c_abstract":[
      "Computational fluid dynamics (CFD) is a powerful tool for modeling turbulent\nflow and is commonly used for urban microclimate simulations. However,\ntraditional CFD methods are computationally intensive, requiring substantial\nhardware resources for high-fidelity simulations. Deep learning (DL) models are\nbecoming popular as efficient alternatives as they require less computational\nresources to model complex non-linear interactions in fluid flow simulations. A\nmajor drawback of DL models is that they are prone to error accumulation in\nlong-term temporal predictions, often compromising their accuracy and\nreliability. To address this shortcoming, this study investigates the use of a\ndenoising diffusion probabilistic model (DDPM) as a novel post-processing\ntechnique to mitigate error propagation in DL models' sequential predictions.\nTo address this, we employ convolutional autoencoder (CAE) and U-Net\narchitectures to predict airflow dynamics around a cubic structure. The DDPM is\nthen applied to the models' predictions, refining the reconstructed flow fields\nto better align with high-fidelity statistical results obtained from large-eddy\nsimulations. Results demonstrate that, although deep learning models provide\nsignificant computational advantages over traditional numerical solvers, they\nare susceptible to error accumulation in sequential predictions; however,\nutilizing DDPM as a post-processing step enhances the accuracy of DL models by\nup to 65% while maintaining a 3 times speedup compared to traditional numerical\nsolvers. These findings highlight the potential of integrating denoising\ndiffusion probabilistic models as a transformative approach to improving the\nreliability and accuracy of deep learning-based urban microclimate simulations,\npaving the way for more efficient and scalable fluid dynamics modeling."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03844",
    "c_title":[
      "Direct numerical simulation benchmarks for the prediction of boundary\n  layer bypass transition in the narrow sense"
    ],
    "c_abstract":[
      "We report a comprehensive set of direct numerical simulation benchmarks of\nbypass transition in the narrow sense with inlet freestream turbulent intensity\nlevels of 0.75%, 1.5%, 2.25%, 3.0%, and 6.0%, respectively. Detailed\ndescriptions of length scales and the rate of viscous dissipation are provided.\nWe ask two key physical questions. First, how do the decay rates and length\nscales of freestream turbulence over a transitional and turbulent boundary\nlayer compare to those in spatially developing isotropic turbulence without the\nwall? Second, what bypass mechanisms drive turbulent spot inception at the\nintermediate rage of freestream turbulence intensity level? We find that the\nboundary-layer freestream turbulence decay and length scales evolve similarly\nto their spatially developing isotropic turbulence flow without the wall\ncounterparts. We also present evidence of the coexistence of two turbulent spot\ninception mechanisms at the inlet FST level of 2.25%: the long low-speed streak\nprimary and secondary instabilities (only in lower inlet FST levels) and the\nself-amplifying process of oblique vortex filaments interacting with a\nDelta-shaped low-speed patch underneath (prevailing only in higher inlet FST\nlevels)."
    ],
    "c_categories":[
      "physics.flu-dyn"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16928",
    "c_title":[
      "On modular representations of C-recursive integer sequences"
    ],
    "c_abstract":[
      "Prunescu and Sauras-Altuzarra showed that all C-recursive sequences of\nnatural numbers have an arithmetic div-mod representation that can be derived\nfrom their generating function. This representation consists of computing the\nquotient of two exponential polynomials and taking the remainder of the result\nmodulo a third exponential polynomial, and works for all integers $n \\geq 1$.\nUsing a different approach, Prunescu proved the existence of two other\nrepresentations, one of which is the mod-mod representation, consisting of two\nsuccessive remainder computations. This result has two weaknesses: (i) the\nrepresentation works only ultimately, and (ii) a correction term must be added\nto the first exponential polynomial. We show that a mod-mod representation\nwithout inner correction term holds for all integers $n \\geq 1$. This follows\ndirectly from the div-mod representation by an arithmetic short-cut from\noutside."
    ],
    "c_categories":[
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02285",
    "c_title":[
      "Minimizing Age of Detection for a Markov Source over a Lossy Channel"
    ],
    "c_abstract":[
      "Monitoring a process\/phenomenon of specific interest is prevalent in\nCyber-Physical Systems (CPS), remote healthcare, smart buildings, intelligent\ntransport, industry 4.0, etc. A key building block of the monitoring system is\na sensor sampling the process and communicating the status updates to a monitor\nfor detecting events of interest. Measuring the freshness of the status updates\nis essential for the timely detection of events, and it has received\nsignificant research interest in recent times. In this paper, we propose a new\nfreshness metric, Age of Detection (AoD), for monitoring the state transitions\nof a Discrete Time Markov Chain (DTMC) source over a lossy wireless channel. We\nconsider the pull model where the sensor samples DTMC state whenever the\nmonitor requests a status update. We formulate a Constrained Markov Decision\nProblem (CMDP) for optimising the AoD subject to a constraint on the average\nsampling frequency and solve it using the Lagrangian MDP formulation and\nRelative Value Iteration (RVI) algorithm. Our numerical results show\ninteresting trade-offs between AoD, sampling frequency, and transmission\nsuccess probability. Further, the AoD minimizing policy provides a lower\nestimation error than the Age of Information (AoI) minimizing policy, thus\ndemonstrating the utility of AoD for monitoring DTMC sources."
    ],
    "c_categories":[
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.09047",
    "c_title":[
      "Optimal Algorithms in Linear Regression under Covariate Shift: On the\n  Importance of Precondition"
    ],
    "c_abstract":[
      "A common pursuit in modern statistical learning is to attain satisfactory\ngeneralization out of the source data distribution (OOD). In theory, the\nchallenge remains unsolved even under the canonical setting of covariate shift\nfor the linear model. This paper studies the foundational (high-dimensional)\nlinear regression where the ground truth variables are confined to an\nellipse-shape constraint and addresses two fundamental questions in this\nregime: (i) given the target covariate matrix, what is the min-max\n\\emph{optimal} algorithm under covariate shift? (ii) for what kinds of target\nclasses, the commonly-used SGD-type algorithms achieve optimality? Our analysis\nstarts with establishing a tight lower generalization bound via a Bayesian\nCramer-Rao inequality. For (i), we prove that the optimal estimator can be\nsimply a certain linear transformation of the best estimator for the source\ndistribution. Given the source and target matrices, we show that the\ntransformation can be efficiently computed via a convex program. The min-max\noptimal analysis for SGD leverages the idea that we recognize both the\naccumulated updates of the applied algorithms and the ideal transformation as\npreconditions on the learning variables. We provide sufficient conditions when\nSGD with its acceleration variants attain optimality."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16268",
    "c_title":[
      "Structural stability of boundary layers in the entire subsonic regime"
    ],
    "c_abstract":[
      "Despite the physical importance, there are limited mathematical theories for\nthe compressible Navier-Stokes equations with strong boundary layers. This is\nmainly due to the absence of a stream function structure, unlike the\nextensively studied incompressible fluid dynamics in two dimensions. This paper\naims to establish the structural stability of boundary layer profiles in the\nform of shear flow for the two-dimensional steady compressible Navier-Stokes\nequations. Our estimates are uniform across the entire subsonic regime, where\nthe Mach number $m\\in (0,1)$. As a byproduct, we provide the first result\nconcerning the low Mach number limit in the presence of Prandtl boundary\nlayers. The proof relies on the quasi-compressible-Stokes iteration introduced\nin [38], along with a subtle analysis of the interplay between density and\nvelocity variables in different frequency regimes, and the identification of\ncancellations in higher-order estimates."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.19927",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b40"
    ],
    "b_title":[
      "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
    ],
    "b_abstract":[
      "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.20461",
    "c_title":[
      "A Polynomial Ramsey Statement for Bounded VC-dimension"
    ],
    "c_abstract":[
      "A theorem by Ding, Oporowski, Oxley, and Vertigan states that every\nsufficiently large bipartite graph without twins contains a matching,\nco-matching, or half-graph of any given size as an induced subgraph. We prove\nthat this Ramsey statement has polynomial dependency assuming bounded\nVC-dimension of the initial graph, using the recent verification of the\nErd\\H{o}s-Hajnal property for graphs of bounded VC-dimension. Since the theorem\nof Ding et al. plays a role in (finite) model theory, which studies even more\nrestricted structures, we also comment on further refinements of the theorem\nwithin this context."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"High-dimensional Statistics Applications to Batch Effects in\n  Metabolomics",
    "a_abstract":"Batch effects are inevitable in large-scale metabolomics. Prior to formal\ndata analysis, batch effect correction (BEC) is applied to prevent from\nobscuring biological variations, and batch effect evaluation (BEE) is used for\ncorrection assessment. However, existing BEE algorithms neglect covariances\nbetween the variables, and existing BEC algorithms might fail to adequately\ncorrect the covariances. Therefore, we resort to recent advancements in\nhigh-dimensional statistics, and respectively propose \"quality control-based\nsimultaneous tests (QC-ST)\" and \"covariance correction (CoCo)\". Validated by\nthe simulation data, QC-ST can simultaneously detect the statistical\nsignificance of QC samples' mean vectors and covariance matrices across\ndifferent batches, and has a satisfactory statistical performance in empirical\nsizes, empirical powers, and computational speed. Then, we apply four QC-based\nBEC algorithms to two large cohort datasets, and find that extreme gradient\nboost (XGBoost) performs best in relative standard deviation (RSD) and\ndispersion-ratio (D-ratio). After prepositive BEC, if QC-ST still suggests that\nbatch effects between some two batches are significant, CoCo should be\nimplemented. And after CoCo (if necessary), the four metrics (i.e., RSD,\nD-ratio, classification performance, and QC-ST) might be further improved. In\nsummary, under the guidance of QC-ST, we can develop a matching strategy to\nintegrate multiple BEC algorithms more rationally and flexibly, and minimize\nbatch effects for reliable biological conclusions.",
    "explanation":"Batch effects are inevitable in large-scale metabolomics. Prior to formal data analysis, batch effect correction (BEC) is applied to prevent from obscuring biological variations, and batch effect evaluation (BEE) is used for correction assessment.we apply four QC-based BEC algorithms to two large cohort datasets, and find that extreme gradient boost (XGBoost) performs best in relative standard deviation (RSD) and dispersion-ratio (D-ratio).",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b16"
    ],
    "c_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "c_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07141",
    "c_title":[
      "Small steps no more: Global convergence of stochastic gradient bandits\n  for arbitrary learning rates"
    ],
    "c_abstract":[
      "We provide a new understanding of the stochastic gradient bandit algorithm by\nshowing that it converges to a globally optimal policy almost surely using\n\\emph{any} constant learning rate. This result demonstrates that the stochastic\ngradient algorithm continues to balance exploration and exploitation\nappropriately even in scenarios where standard smoothness and noise control\nassumptions break down. The proofs are based on novel findings about action\nsampling rates and the relationship between cumulative progress and noise, and\nextend the current understanding of how simple stochastic gradient methods\nbehave in bandit settings."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.06999",
    "c_title":[
      "Outsourced diffusion sampling: Efficient posterior inference in latent\n  spaces of generative models"
    ],
    "c_abstract":[
      "Any well-behaved generative model over a variable $\\mathbf{x}$ can be\nexpressed as a deterministic transformation of an exogenous ('outsourced')\nGaussian noise variable $\\mathbf{z}$: $\\mathbf{x}=f_\\theta(\\mathbf{z})$. In\nsuch a model (e.g., a VAE, GAN, or continuous-time flow-based model), sampling\nof the target variable $\\mathbf{x} \\sim p_\\theta(\\mathbf{x})$ is\nstraightforward, but sampling from a posterior distribution of the form\n$p(\\mathbf{x}\\mid\\mathbf{y}) \\propto\np_\\theta(\\mathbf{x})r(\\mathbf{x},\\mathbf{y})$, where $r$ is a constraint\nfunction depending on an auxiliary variable $\\mathbf{y}$, is generally\nintractable. We propose to amortize the cost of sampling from such posterior\ndistributions with diffusion models that sample a distribution in the noise\nspace ($\\mathbf{z}$). These diffusion samplers are trained by reinforcement\nlearning algorithms to enforce that the transformed samples\n$f_\\theta(\\mathbf{z})$ are distributed according to the posterior in the data\nspace ($\\mathbf{x}$). For many models and constraints of interest, the\nposterior in the noise space is smoother than the posterior in the data space,\nmaking it more amenable to such amortized inference. Our method enables\nconditional sampling under unconditional GAN, (H)VAE, and flow-based priors,\ncomparing favorably both with current amortized and non-amortized inference\nmethods. We demonstrate the proposed outsourced diffusion sampling in several\nexperiments with large pretrained prior models: conditional image generation,\nreinforcement learning with human feedback, and protein structure generation."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.00963",
    "c_title":[
      "PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs"
    ],
    "c_abstract":[
      "While recent AI-for-math has made strides in pure mathematics, areas of\napplied mathematics, particularly PDEs, remain underexplored despite their\nsignificant real-world applications. We present PDE-Controller, a framework\nthat enables large language models (LLMs) to control systems governed by\npartial differential equations (PDEs). Our approach enables LLMs to transform\ninformal natural language instructions into formal specifications, and then\nexecute reasoning and planning steps to improve the utility of PDE control. We\nbuild a holistic solution comprising datasets (both human-written cases and 2\nmillion synthetic samples), math-reasoning models, and novel evaluation\nmetrics, all of which require significant effort. Our PDE-Controller\nsignificantly outperforms prompting the latest open-source and GPT models in\nreasoning, autoformalization, and program synthesis, achieving up to a 62%\nimprovement in utility gain for PDE control. By bridging the gap between\nlanguage generation and PDE systems, we demonstrate the potential of LLMs in\naddressing complex scientific and engineering challenges. We will release all\ndata, model checkpoints, and code at https:\/\/pde-controller.github.io\/."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.01765",
    "c_title":[
      "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation"
    ],
    "c_abstract":[
      "As advancements in large language models (LLMs) continue and the demand for\npersonalized models increases, parameter-efficient fine-tuning (PEFT) methods\n(e.g., LoRA) will become essential due to their efficiency in reducing\ncomputation costs. However, recent studies have raised alarming concerns that\nLoRA fine-tuning could potentially compromise the safety alignment in LLMs,\nposing significant risks for the model owner. In this paper, we first\ninvestigate the underlying mechanism by analyzing the changes in safety\nalignment related features before and after fine-tuning. Then, we propose a\nfixed safety module calculated by safety data and a task-specific\ninitialization for trainable parameters in low-rank adaptations, termed\nSafety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA\nmethods and their variants, SaLoRA enables targeted modifications to LLMs\nwithout disrupting their original alignments. Our experiments show that SaLoRA\noutperforms various adapters-based approaches across various evaluation metrics\nin different fine-tuning tasks."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04687",
    "c_title":[
      "Compositional World Knowledge leads to High Utility Synthetic data"
    ],
    "c_abstract":[
      "Machine learning systems struggle with robustness, under subpopulation\nshifts. This problem becomes especially pronounced in scenarios where only a\nsubset of attribute combinations is observed during training -a severe form of\nsubpopulation shift, referred as compositional shift. To address this problem,\nwe ask the following question: Can we improve the robustness by training on\nsynthetic data, spanning all possible attribute combinations? We first show\nthat training of conditional diffusion models on limited data lead to incorrect\nunderlying distribution. Therefore, synthetic data sampled from such models\nwill result in unfaithful samples and does not lead to improve performance of\ndownstream machine learning systems. To address this problem, we propose CoInD\nto reflect the compositional nature of the world by enforcing conditional\nindependence through minimizing Fisher's divergence between joint and marginal\ndistributions. We demonstrate that synthetic data generated by CoInD is\nfaithful and this translates to state-of-the-art worst-group accuracy on\ncompositional shift tasks on CelebA."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07856",
    "c_title":[
      "Dynamic loan portfolio management in a three time step model"
    ],
    "c_abstract":[
      "This paper studies the bank dynamic decision problem in the intermediate time\nstep for a discrete-time setup. We have considered a three-time-step model.\nInitially, the banks raise money through debt and equity and invest in\ndifferent types of loans. It liquidates its assets and raises new funds at the\nintermediate-time step to meet the short-term debt holders claim. Further, it\nhas to meet specific capital requirements given by the regulators. In this\nwork, we have theoretically studied the effect of raising new equity and debt.\nWe show that in some cases, raising equity and debt may increase the return on\nequity, and in some cases, it may decrease the return on equity. We have\ndiscussed several cases and given a bound on the capital that can be raised. We\nhave added an equity holders constraint, which ensures the return on equity and\ndesists the bank from defaulting at the final time point."
    ],
    "c_categories":[
      "q-fin.RM"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04366",
    "c_title":[
      "Twisted heterobilayer photonic crystal based on stacking and selective\n  etching of 2D materials"
    ],
    "c_abstract":[
      "Nanophotonic devices with moir\\'e superlattice is currently attracting broad\ninterest due to the unique periodicity and high efficiency control of photons.\nTill now, experimental investigations mainly focus on the single layer device,\ni.e., two or more layers of photonic crystal patterns are merged and etched in\na single layer of material. By comparison, twisted photonic crystal with\nmultilayer materials raises challenges in the nanofabrication technology,\nbecause the growth of upper layer material usually requires a smooth bottom\nlayer without nanostructures. Hereby, we fabricate twisted heterobilayer\nphotonic crystal in the graphite\/Si$_3$N$_4$ heterostructure. We use dry\ntransfer method to stack the graphite on top of bottom Si$_3$N$_4$ with\npre-etched photonic crystal patterns. Selective dry etching recipes are used to\netch two photonic crystal layers individually, which improves the quality and\naccuracy in alignment. The cavity photonic mode at the visible wavelength $\\sim\n700$ nm arsing from the moir\\'e site is clearly observed in experiment. These\nresults reveal the experimental diagram of heterobilayer nanophotonic devices\nand open the way to design flexibility and control of photons in new degrees of\nfreedom."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.02903",
    "c_title":[
      "Asymmetric Cross-Correlation in Multivariate Spatial Stochastic\n  Processes: A Primer"
    ],
    "c_abstract":[
      "Multivariate spatial phenomena are ubiquitous, spanning domains such as\nclimate, pandemics, air quality, and social economy. Cross-correlation between\ndifferent quantities of interest at different locations is asymmetric in\ngeneral. This paper provides the visualization, structure, and properties of\nasymmetric cross-correlation as well as symmetric auto-correlation. It reviews\nmainstream multivariate spatial models and analyzes their capability to\naccommodate asymmetric cross-correlation. It also illustrates the difference in\nmodel accuracy with and without asymmetric accommodation using a 1D simulated\nexample."
    ],
    "c_categories":[
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00869",
    "c_title":[
      "Higher Differentiability of Minimizers for Non-Autonomous Orthotropic\n  Functionals"
    ],
    "c_abstract":[
      "We establish the higher differentiability for the minimizers of the following\nnon-autonomous integral functionals \\begin{equation*}\n  \\mathcal{F}(u,\\Omega):= \\, \\int_\\Omega \\sum_{i=1}^{n} \\, a_i(x) \\lvert\nu_{x_i} \\rvert^{p_i} dx, \\end{equation*} with exponents $p_i \\geq 2$ and with\ncoefficients $a_i(x)$ that satisfy a suitable Sobolev regularity. The main\nresult is obtained, as usual, by imposing a gap bound on the exponents $p_i$,\nwhich depends on the dimension and on the degree of regularity of the\ncoefficients $a_i(x)$"
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
    ],
    "b_abstract":[
      "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.06240",
    "c_title":[
      "The Convergence of Dynamic Routing between Capsules"
    ],
    "c_abstract":[
      "Capsule networks(CapsNet) are recently proposed neural network models with\nnew processing layers, specifically for entity representation and discovery of\nimages. It is well known that CapsNet have some advantages over traditional\nneural networks, especially in generalization capability. At the same time,\nsome studies report negative experimental results. The causes of this\ncontradiction have not been thoroughly analyzed. The preliminary experimental\nresults show that the behavior of routing algorithms does not always produce\ngood results as expected, and in most cases, different routing algorithms do\nnot change the classification results, but simply polarize the link strength,\nespecially when they continue to repeat without stopping. To realize the true\npotential of the CapsNet, deep mathematical analysis of the routing algorithms\nis crucial. In this paper, we will give the objective function that is\nminimized by the dynamic routing algorithm, which is a concave function. The\ndynamic routing algorithm can be regarded as nonlinear gradient method to\nsolving an optimization algorithm under linear constraints, and its convergence\ncan be strictly proved mathematically. Furthermore, the mathematically rigorous\nproof of the convergence is given for this class of iterative routing\nprocedures. We analyze the relation between the objective function and the\nconstraints solved by the dynamic routing algorithm in detail, and perform the\ncorresponding routing experiment to analyze the effect of our convergence\nproof."
    ],
    "c_categories":[
      "cs.LG",
      "math.OC"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02971",
    "c_title":[
      "Bowel Incision Closure with a Semi-Automated Robot-Assisted Laser Tissue\n  Soldering System"
    ],
    "c_abstract":[
      "Traditional methods for closing gastrointestinal (GI) surgery incisions, like\nsuturing and stapling, present significant challenges, including potentially\nlife-threatening leaks. These techniques, especially in robot-assisted\nminimally invasive surgery (RAMIS), require advanced manual skills. While their\nrepetitive and time-consuming nature makes them suitable candidates for\nautomation, the automation process is complicated by the need for extensive\ncontact with the tissue. Addressing this, we demonstrate a semi-autonomous\ncontactless surgical procedure using our novel Robot-assisted Laser Tissue\nSoldering (RLTS) system on a live porcine bowel. Towards this in-vivo\ndemonstration, we optimized soldering protocols and system parameters in\nex-vivo experiments on porcine bowels and a porcine cadaver. To assess the RLTS\nsystem performance, we compared the pressure at which the anastomosis leaked\nbetween our robotic soldering and manual suturing. With the best setup, we\nadvanced to an in-vivo Heineke Mikulicz closure on small bowel incision in live\npigs and evaluated their healing for two weeks. All pigs successfully\ncompleting the procedure (N=5) survived without leaks and the histology\nindicated mucosal regeneration and fibrous tissue adhesion. This marks the\nfirst in-vivo semi-automated contactless incision closure, paving the way for\nautomating GI surgery incision closure which has the potential to become an\nalternative to traditional methods."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13142",
    "c_title":[
      "Computational modelling of biological systems now and then: revisiting\n  tools and visions from the beginning of the century"
    ],
    "c_abstract":[
      "Since the turn of the millennium, computational modelling of biological\nsystems has evolved remarkably and sees matured use spanning basic and clinical\nresearch. While the topic of the peri-millennial debate about the virtues and\nlimitations of 'reductionism and integrationism' seems less controversial\ntoday, a new apparent dichotomy dominates discussions: mechanistic vs.\ndata-driven modelling. In light of this distinction, we provide an overview of\nrecent achievements and new challenges with a focus on the cardiovascular\nsystem. Attention has shifted from generating a universal model of the human to\neither models of individual humans (digital twins) or entire cohorts of models\nrepresentative of clinical populations to enable in silico clinical trials.\nDisease-specific parameterisation, inter-individual and intra-individual\nvariability, uncertainty quantification as well as interoperable, standardised,\nand quality-controlled data are important issues today, which call for open\ntools, data and metadata standards, as well as strong community interactions.\nThe quantitative, biophysical, and highly controlled approach provided by in\nsilico methods has become an integral part of physiological and medical\nresearch. In silico methods have the potential to accelerate future progress\nalso in the fields of integrated multi-physics modelling, multi-scale models,\nvirtual cohort studies, and machine learning beyond what is feasible today. In\nfact, mechanistic and data-driven modelling can complement each other\nsynergistically and fuel tomorrow's artificial intelligence applications to\nfurther our understanding of physiology and disease mechanisms, to generate new\nhypotheses and assess their plausibility, and thus to contribute to the\nevolution of preventive, diagnostic, and therapeutic approaches."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.16214",
    "c_title":[
      "Targeting Neurodegeneration: Three Machine Learning Methods for G9a\n  Inhibitors Discovery Using PubChem and Scikit-learn"
    ],
    "c_abstract":[
      "In light of the increasing interest in G9a's role in neuroscience, three\nmachine learning (ML) models, that are time efficient and cost effective, were\ndeveloped to support researchers in this area. The models are based on data\nprovided by PubChem and performed by algorithms interpreted by the scikit-learn\nPython-based ML library. The first ML model aimed to predict the efficacy\nmagnitude of active G9a inhibitors. The ML models were trained with 3,112 and\ntested with 778 samples. The Gradient Boosting Regressor perform the best,\nachieving 17.81% means relative error (MRE), 21.48% mean absolute error (MAE),\n27.39% root mean squared error (RMSE) and 0.02 coefficient of determination\n(R2) error. The goal of the second ML model called a CID_SID ML model, utilised\nPubChem identifiers to predict the G9a inhibition probability of a small\nbiomolecule that has been primarily designed for different purposes. The ML\nmodels were trained with 58,552 samples and tested with 14,000. The most\nsuitable classifier for this case study was the Extreme Gradient Boosting\nClassifier, which obtained 78.1% accuracy, 84.3% precision,69.1% recall, 75.9%\nF1-score and 8.1% Receiver-operating characteristic (ROC). The third ML model\nbased on the Random Forest Classifier algorithm led to the generation of a list\nof descending-ordered functional groups based on their importance to the G9a\ninhibition. The model was trained with 19,455 samples and tested with 14,100.\nThe probability of this rank was 70% accuracy."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08747",
    "c_title":[
      "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica"
    ],
    "c_abstract":[
      "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13627",
    "c_title":[
      "Optimal compound downselection to promote diversity and parallel\n  chemistry"
    ],
    "c_abstract":[
      "Early stage drug discovery and molecular design projects often follow\niterative design-make-test cycles. The selection of which compounds to\nsynthesize from all possible candidate compounds is a complex decision inherent\nto these design cycles that must weigh multiple factors. We build upon the\nalgorithmic downselection framework SPARROW that considers synthetic cost,\nsynthetic feasibility, and compound utility, extending it to address additional\ncritical factors related to the risk of synthesis failure, molecular diversity,\nand parallel chemistry capabilities. These design considerations further align\nalgorithmic compound selection with the true complexity of this decision-making\nprocess, allowing SPARROW to capture a broader set of principles typically\nreliant on expert chemist intuition. The application of these formulations to\nan exemplary case study highlights SPARROW's ability to promote the selection\nof diverse batches of compounds whose syntheses are amenable to parallel\nchemistry."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.09956",
    "c_title":[
      "On the local well-posedness of fractionally dissipated primitive\n  equations with transport noise"
    ],
    "c_abstract":[
      "We investigate the three-dimensional fractionally dissipated primitive\nequations with transport noise, focusing on subcritical and critical\ndissipation regimes characterized by $ (-\\Delta)^{s\/2} $ with $ s \\in (1,2)$\nand $s = 1$, respectively. For $\\sigma>3$, we establish the local existence of\nunique pathwise solutions in Sobolev space $H^\\sigma$. This result applies to\narbitrary initial data in the subcritical case ($s \\in(1,2)$), and to small\ninitial data in the critical case ($s=1$). The analysis is particularly\nchallenging due to the loss of horizontal derivatives in the nonlinear terms\nand the lack of full dissipation. To address these challenges, we develop novel\ncommutator estimates involving the hydrostatic Leray projection."
    ],
    "c_categories":[
      "math.AP",
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18116",
    "c_title":[
      "DeepFRC: An End-to-End Deep Learning Model for Functional Registration\n  and Classification"
    ],
    "c_abstract":[
      "Functional data analysis (FDA) is essential for analyzing continuous,\nhigh-dimensional data, yet existing methods often decouple functional\nregistration and classification, limiting their efficiency and performance. We\npresent DeepFRC, an end-to-end deep learning framework that unifies these tasks\nwithin a single model. Our approach incorporates an alignment module that\nlearns time warping functions via elastic function registration and a learnable\nbasis representation module for dimensionality reduction on aligned data. This\nintegration enhances both alignment accuracy and predictive performance.\nTheoretical analysis establishes that DeepFRC achieves low misalignment and\ngeneralization error, while simulations elucidate the progression of\nregistration, reconstruction, and classification during training. Experiments\non real-world datasets demonstrate that DeepFRC consistently outperforms\nstate-of-the-art methods, particularly in addressing complex registration\nchallenges. Code is available at: https:\/\/github.com\/Drivergo-93589\/DeepFRC."
    ],
    "c_categories":[
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08674",
    "c_title":[
      "Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields"
    ],
    "c_abstract":[
      "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps:\/\/tkreiman.github.io\/projects\/mlff_distribution_shifts\/."
    ],
    "c_categories":[
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Quantitative Biology",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.13961",
    "c_title":[
      "The Computational Advantage of Depth: Learning High-Dimensional\n  Hierarchical Functions with Gradient Descent"
    ],
    "c_abstract":[
      "Understanding the advantages of deep neural networks trained by gradient\ndescent (GD) compared to shallow models remains an open theoretical challenge.\nWhile the study of multi-index models with Gaussian data in high dimensions has\nprovided analytical insights into the benefits of GD-trained neural networks\nover kernels, the role of depth in improving sample complexity and\ngeneralization in GD-trained networks remains poorly understood. In this paper,\nwe introduce a class of target functions (single and multi-index Gaussian\nhierarchical targets) that incorporate a hierarchy of latent subspace\ndimensionalities. This framework enables us to analytically study the learning\ndynamics and generalization performance of deep networks compared to shallow\nones in the high-dimensional limit. Specifically, our main theorem shows that\nfeature learning with GD reduces the effective dimensionality, transforming a\nhigh-dimensional problem into a sequence of lower-dimensional ones. This\nenables learning the target function with drastically less samples than with\nshallow networks. While the results are proven in a controlled training\nsetting, we also discuss more common training procedures and argue that they\nlearn through the same mechanisms. These findings open the way to further\nquantitative studies of the crucial role of depth in learning hierarchical\nstructures with deep networks."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.10196",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "XGBoost: A Scalable Tree Boosting System"
    ],
    "b_abstract":[
      "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04175",
    "c_title":[
      "Unbounded integral Hankel operators"
    ],
    "c_abstract":[
      "For a wide class of unbounded integral Hankel operators on the positive\nhalf-line, we prove essential self-adjointness on the set of smooth compactly\nsupported functions."
    ],
    "c_categories":[
      "math.SP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"Meta-learning Loss Functions of Parametric Partial Differential\n  Equations Using Physics-Informed Neural Networks",
    "a_abstract":"This paper proposes a new way to learn Physics-Informed Neural Network loss\nfunctions using Generalized Additive Models. We apply our method by\nmeta-learning parametric partial differential equations, PDEs, on Burger's and\n2D Heat Equations. The goal is to learn a new loss function for each parametric\nPDE using meta-learning. The derived loss function replaces the traditional\ndata loss, allowing us to learn each parametric PDE more efficiently, improving\nthe meta-learner's performance and convergence.",
    "explanation":"This paper proposes a new way to learn Physics-Informed Neural Network loss functions using Generalized Additive Models.We apply our method by meta-learning parametric partial differential equations, PDEs, on Burger\u2019s and 2D Heat Equations.",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b17"
    ],
    "c_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "c_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.07709",
    "c_title":[
      "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
    ],
    "c_abstract":[
      "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.06395",
    "c_title":[
      "Causal Discovery and Inference towards Urban Elements and Associated\n  Factors"
    ],
    "c_abstract":[
      "To uncover the city's fundamental functioning mechanisms, it is important to\nacquire a deep understanding of complicated relationships among citizens,\nlocation, and mobility behaviors. Previous research studies have applied direct\ncorrelation analysis to investigate such relationships. Nevertheless, due to\nthe ubiquitous confounding effects, empirical correlation analysis may not\naccurately reflect underlying causal relationships among basic urban elements.\nIn this paper, we propose a novel urban causal computing framework to\ncomprehensively explore causalities and confounding effects among a variety of\nfactors across different types of urban elements. In particular, we design a\nreinforcement learning algorithm to discover the potential causal graph, which\ndepicts the causal relations between urban factors. The causal graph further\nserves as the guidance for estimating causal effects between pair-wise urban\nfactors by propensity score matching. After removing the confounding effects\nfrom correlations, we leverage significance levels of causal effects in\ndownstream urban mobility prediction tasks. Experimental studies on open-source\nurban datasets show that the discovered causal graph demonstrates a\nhierarchical structure, where citizens affect locations, and they both cause\nchanges in urban mobility behaviors. Experimental results in urban mobility\nprediction tasks further show that the proposed method can effectively reduce\nconfounding effects and enhance performance of urban computing tasks."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.20231",
    "c_title":[
      "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions"
    ],
    "c_abstract":[
      "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.19308",
    "c_title":[
      "WOFOSTGym: A Crop Simulator for Learning Annual and Perennial Crop\n  Management Strategies"
    ],
    "c_abstract":[
      "We introduce WOFOSTGym, a novel crop simulation environment designed to train\nreinforcement learning (RL) agents to optimize agromanagement decisions for\nannual and perennial crops in single and multi-farm settings. Effective crop\nmanagement requires optimizing yield and economic returns while minimizing\nenvironmental impact, a complex sequential decision-making problem well suited\nfor RL. However, the lack of simulators for perennial crops in multi-farm\ncontexts has hindered RL applications in this domain. Existing crop simulators\nalso do not support multiple annual crops. WOFOSTGym addresses these gaps by\nsupporting 23 annual crops and two perennial crops, enabling RL agents to learn\ndiverse agromanagement strategies in multi-year, multi-crop, and multi-farm\nsettings. Our simulator offers a suite of challenging tasks for learning under\npartial observability, non-Markovian dynamics, and delayed feedback.\nWOFOSTGym's standard RL interface allows researchers without agricultural\nexpertise to explore a wide range of agromanagement problems. Our experiments\ndemonstrate the learned behaviors across various crop varieties and soil types,\nhighlighting WOFOSTGym's potential for advancing RL-driven decision support in\nagriculture."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.00870",
    "c_title":[
      "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In\n  Open Domains"
    ],
    "c_abstract":[
      "We explore neuro-symbolic approaches to generalize actionable knowledge,\nenabling embodied agents to tackle complex tasks more effectively in\nopen-domain environments. A key challenge for embodied agents is the\ngeneralization of knowledge across diverse environments and situations, as\nlimited experiences often confine them to their prior knowledge. To address\nthis issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual\nlearner that emulates the hypothetico-deductive model by continually\nformulating and validating knowledge from limited experiences through the\ncombined use of Large Language Models (LLMs) and symbolic tools. Specifically,\nwe devise a contrastive generality improvement scheme within NeSyC, which\niteratively generates hypotheses using LLMs and conducts contrastive validation\nvia symbolic tools. This scheme reinforces the justification for admissible\nactions while minimizing the inference of inadmissible ones. Additionally, we\nincorporate a memory-based monitoring scheme that efficiently detects action\nerrors and triggers the knowledge refinement process across domains.\nExperiments conducted on diverse embodied task benchmarks-including ALFWorld,\nVirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate\nthat NeSyC is highly effective in solving complex embodied tasks across a range\nof open-domain environments."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.09401",
    "c_title":[
      "Entanglement behavior and localization properties in monitored fermion\n  systems"
    ],
    "c_abstract":[
      "We study the asymptotic bipartite entanglement in various integrable and\nnonintegrable models of monitored fermions. We find that, for the integrable\ncases, the entanglement versus the system size is well fitted, over more than\none order of magnitude, by a function interpolating between a linear and a\npower-law behavior. Up to the sizes we are able to reach, a logarithmic growth\nof the entanglement can be also captured by the same fit with a very small\npower-law exponent. We thus propose a characterization of the various\nentanglement phases using the fitting parameters. For the nonintegrable cases,\nas the staggered t-V and the Sachdev-Ye-Kitaev (SYK) models, the numerics\nprevents us from spanning different orders of magnitude in the size, therefore\nwe fit the asymptotic entanglement versus the measurement strength and then\nlook at the scaling with the size of the fitting parameters. We find two\ndifferent behaviors: for the SYK we observe a volume-law growth, while for the\nt-V model some traces of an entanglement transition emerge. In the latter\nmodels, we study the localization properties in the Hilbert space through the\ninverse participation ratio, finding an anomalous delocalization with no\nrelation with the entanglement properties. Finally, we show that our function\nfits very well the fermionic logarithmic negativity of a quadratic model in\nladder geometry with stroboscopic projective measurements."
    ],
    "c_categories":[
      "cond-mat.other",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.12442",
    "c_title":[
      "Spectral efficiency for mmWave downlink with beam misalignment in urban\n  macro scenario"
    ],
    "c_abstract":[
      "In this paper, we analyze the spectral efficiency for millimeter wave\ndownlink with beam misalignment in urban macro scenario. For this purpose, we\nuse a new approach based on the modified Shannon formula, which considers the\npropagation environment and antenna system coefficients. These factors are\ndetermined based on a multi-ellipsoidal propagation model. The obtained results\nshow that under non-line-of-sight conditions, the appropriate selection of the\nantenna beam orientation may increase the spectral efficiency in relation to\nthe direct line to a user."
    ],
    "c_categories":[
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.01099",
    "c_title":[
      "Bound states of 2+1 fermionic trimers on lattice at strong couplings"
    ],
    "c_abstract":[
      "In this paper, we investigate the bound states of $2+1$ fermionic trimers on\na three-dimensional lattice at strong coupling. Specifically, we analyze the\ndiscrete spectrum of the associated three-body discrete Schr\\\"odinger operator\n$H_{\\gamma,\\lambda}(K),$ focusing on energies below the continuum and within\nits gap. Depending on the quasi-momentum $K,$ we show that if the mass ratio\n$\\gamma>0$ between the identical fermions and the third particle is below a\ncertain threshold, the operator lacks a discrete spectrum below the essential\nspectrum for sufficiently large coupling $\\lambda>0.$ Conversely, if $\\gamma$\nexceeds this threshold, $H_{\\gamma,\\lambda}(K)$ admits at least one eigenvalue\nbelow the essential spectrum. Similar phenomena are observed in the\nneighborhood of the two-particle branch of the essential spectrum, which\nresides within the gap and grows sublinearly as $\\lambda\\to+\\infty.$ For $K=0,$\nthe mass ratio thresholds are explicitly calculated and it turns out that, for\ncertain intermediate mass ratios and large couplings, bound states emerge\nwithin the gap, although ground states are absent."
    ],
    "c_categories":[
      "math-ph",
      "math.FA",
      "math.MP",
      "math.SP"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03076",
    "c_title":[
      "The Eigenstate Thermalization Hypothesis in a Quantum Point Contact\n  Geometry"
    ],
    "c_abstract":[
      "It is known that the long-range quantum entanglement exhibited in free\nfermion systems is sufficient to \"thermalize\" a small subsystem in that the\nsubsystem reduced density matrix computed from a typical excited eigenstate of\nthe combined system is approximately thermal. Remarkably, fermions without any\ninteractions are thus thought to satisfy the Eigenstate Thermalization\nHypothesis (ETH). We explore this hypothesis when the fermion subsystem is only\nminimally coupled to a quantum reservoir (in the form of another fermion\nsystem) through a quantum point contact (QPC). The entanglement entropy of two\n2-d free fermion systems connected by one or more quantum point contacts (QPC)\nis examined at finite energy and in the ground state. When the combined system\nis in a typical excited state, it is shown that the entanglement entropy of a\nsubsystem connected by a small number of QPCs is sub-extensive, scaling as the\nlinear size of the subsystem ($L_A$). For sufficiently low energies ($E$) and\nsmall subsystems, it is demonstrated numerically that the entanglement entropy\n$S_A \\sim L_A E$, what one would expect for the thermodynamics of a\none-dimensional system. In this limit, we suggest that the entropy carried by\neach additional QPC is quantized using the one-dimensional finite\nsize\/temperature conformal scaling: $\\Delta S_A = \\alpha\n\\log{(1\/E)\\sinh{(L_AE)}}$. The sub-extensive entropy in the case of a small\nnumber of QPCs should be contrasted with the expectation for both classical,\nergodic systems and quantum chaotic systems wherein a restricted geometry might\naffect the equilibrium relaxation times, but not the equilibrium properties\nthemselves, such as extensive entropy and heat capacity."
    ],
    "c_categories":[
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "The Burgers equation"
    ],
    "b_abstract":[
      "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
    ],
    "b_categories":[
      "math.AP"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.17751",
    "c_title":[
      "Looking at infrared background radiation anisotropies with Spitzer:\n  large scale anisotropies and their implications"
    ],
    "c_abstract":[
      "We use Spitzer\/IRAC deep exposure data covering two significantly larger than\nbefore sky areas to construct maps suitable for evaluating source-subtracted\nfluctuations in the cosmic infrared background (CIB). The maps are constructed\nusing the self-calibration methodology eliminating artifacts to sufficient\naccuracy and subset maps are selected in each area containing approximately\nuniform exposures. These maps are clipped and removed of known sources and then\nFourier transformed to probe the CIB anisotropies to new larger scales. The\npower spectrum of the resultant CIB anisotropies is measured from the data to\n>1 degree revealing the component well above that from remaining known galaxies\non scales >1 arcmin. The fluctuations are demonstrated to be free of Galactic\nand Solar System foreground contributions out to the largest scales measured.\nWe discuss the proposed theories for the origin of the excess CIB anisotropies\nin light of the new data. Out of these, the model where the CIB fluctuation\nexcess originates from the granulation power due to LIGO-observed primordial\nblack holes as dark matter appears most successful in accounting for all\nobservations related to the measured CIB power amplitude and spatial structure,\nincluding the measured coherence between the CIB and unresolved cosmic X-ray\nbackground (CXB). Finally we point out the use of the data to probe the CIB-CXB\ncross-power to new scales and higher accuracy. We also discuss the synergy of\nthese data with future CIB programs at shorter near-IR wavelengths with deep\nwide surveys and sub-arcsecond angular resolution as provided by Euclid and\nRoman space missions."
    ],
    "c_categories":[
      "astro-ph.CO"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.02049",
    "c_title":[
      "Normalized solutions to focusing Sobolev critical biharmonic\n  Schr\\\"{o}dinger equation with mixed dispersion"
    ],
    "c_abstract":[
      "This paper is concerned with the following focusing biharmonic\nSchr\\\"{o}dinger equation with mixed dispersion and Sobolev critical growth: $$\n\\begin{cases}\n  {\\Delta}^2u-\\Delta u-\\lambda u-\\mu|u|^{p-2}u-|u|^{4^*-2}u=0\\ \\ \\mbox{in}\\\n\\mathbb{R}^N, \\\\[0.1cm]\n  \\int_{\\mathbb{R}^N} u^2 dx = c, \\end{cases} $$ where $N \\geq 5$, $\\mu,c>0$,\n$2<p<4^*:=\\frac{2N}{N-4}$ and $\\lambda \\in \\mathbb{R}$ is a Lagrange\nmultiplier. For this problem, under the $L^2$-subcritical perturbation\n($2<p<2+\\frac{8}{N}$), we derive the existence and multiplicity of normalized\nsolutions via the truncation technique, concentration-compactness principle and\nthe genus theory presented by C.O. Alves et al. (Arxiv, (2021), doi:\n2103.07940v2). Compared to the results of C.O. Alves et al. we obtain a more\ngeneral result after removing the further assumptions given in (3.2) of their\npaper. In the case of $L^2$-supercritical perturbation ($2+\\frac{8}{N}<p<4^*$),\nwe explore the existence results of normalized solutions by applying the\nconstrained variational methods and the mountain pass theorem. Moreover, we\npropose a novel method to address the effects of the dispersion term $\\Delta\nu$. This approach allows us to extend the recent results obtained by X. Chang\net al. (Arxiv, (2023), doi: 2305.00327v1) to the mixed dispersion situation."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13632",
    "c_title":[
      "Steady 3d Euler flows via a topology-preserving convex integration\n  scheme"
    ],
    "c_abstract":[
      "Given any smooth solenoidal vector field $v_0$ on $\\mathbf T^3$, we show the\nexistence of infinitely many H\\\"older-continuous steady Euler flows $v$ with\nthe same topology as $v_0$, in certain weak sense. In particular, we show that\n$v$ possesses a unique flow of the highest H\\\"older regularity, which is\nconjugate to the flow of $v_0$ via a volume-preserving H\\\"older homeomorphism\nof $\\mathbf T^3$. This result extends to the case of Euler equations on\ntoroidal domains, which has applications to the study of plasmas. The proof\nrelies on a novel convex integration scheme incorporating the key idea that the\nvelocity field of the subsolutions must remain diffeomorphic to $v_0$ at each\niteration step."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02742",
    "c_title":[
      "Potential-based versus non potential-based cohesive models accounting\n  for loading and unloading with application to sliding elastic laminates"
    ],
    "c_abstract":[
      "A rigorous unified perspective of cohesive zone models is presented,\nincluding and comparing potential-based and non potential-based formulations,\nand encompassing known examples studied in literature. The main novelty of the\nwork consists in the natural inclusion of loading and unloading effects in a\ngeneral mixed-mode framework, incorporated through an intrinsic construction of\nenergy densities or tensions. The proposed mathematical investigation\nidentifies and proves the limitations of variational models with respect to\nnon-variational ones, the latter yielding a feasible description of real\ninstances in all relevant situations and regimes. This validates existing\nempirical and numerical observations. An application to a mechanical process of\ntwo elastic laminates sliding one on each other along their cohesive interface\nis finally analyzed, and existence results in both potential-based and non\npotential-based versions are obtained, extending previous contributions."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15132",
    "c_title":[
      "Zero modes and Dirac-(logarithmic) Sobolev-type inequalities"
    ],
    "c_abstract":[
      "We study the decay rate of the zero modes of the Dirac operator with a\nmatrix-valued potential that is considered here without any regularity\nassumptions, compared to the existing literature. For the Dirac operator and\nfor Clifford-valued functions we prove the $L^p$-$L^2$ Dirac Sobolev inequality\nwith explicit constant, as well as the $L^p$-$L^q$ Dirac-Sobolev inequalities.\nWe prove its logarithmic counterpart for $q=2$, extending it to its Gaussian\nversion of Gross, as well as show Nash and Poincar\\'e inequalities in this\nsetting, with explicit values for constants."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16505",
    "c_title":[
      "Asymptotic behavior for the Brezis-Nirenberg problem. The subcritical\n  perturbation case"
    ],
    "c_abstract":[
      "In this paper, we are concerned with the well-known Brezis-Nirenberg problem\n\\begin{equation*} \\begin{cases} -\\Delta u= u^{2^*-1}+\\varepsilon u^{q-1},\\quad\nu>0, &{\\text{in}~\\Omega},\\\\ \\quad \\ \\ u=0, &{\\text{on}~\\partial \\Omega},\n\\end{cases} \\end{equation*} where $\\Omega\\subset \\mathbb R^N$ with $N\\ge 3$ is\na bounded domain, $q\\in(2,2^*)$ and $2^*=\\frac{2N}{N-2}$ denotes the critical\nSobolev exponent. It is well-known (H. Br\\'{e}zis and L. Nirenberg, \\newblock\n{\\em Comm. Pure Appl. Math.}, 36(4):437--477, 1983) that the above problem\nadmits a positive least energy solution for all $\\varepsilon >0$ and\n$q>\\max\\{2,\\frac{4}{N-2}\\}$. In the present paper, we first analyze the\nasymptotic behavior of the positive least energy solution as $\\varepsilon\\to 0$\nand establish a sharp asymptotic characterisation of the profile and blow-up\nrate of the least energy solution. Then, we prove the uniqueness and\nnondegeneracy of the least energy solution under some mild assumptions on\ndomain $\\Omega$. The main results in this paper can be viewed as a\ngeneralization of the results for $q=2$ previously established in the\nliterature. But the situation is quite different from the case $q=2$, and the\nblow-up rate not only heavily depends on the space dimension $N$ and the\ngeometry of the domain $\\Omega$, but also depends on the exponent\n$q\\in(\\max\\{2,\\frac{4}{N-2}\\}, 2^*)$ in a non-trivial way."
    ],
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17696",
    "c_title":[
      "Moir\\'e Superradiance in Cavity Quantum Electrodynamics with Quantum\n  Atom Gas"
    ],
    "c_abstract":[
      "As a novel platform for exploring exotic quantum phenomena, the moir\\'e\nlattice has garnered significant interest in solid-state physics, photonics,\nand cold atom physics. While moir\\'e lattices in two- and three-dimensional\nsystems have been proposed for neutral cold atoms, the simpler one-dimensional\nmoir\\'e effect remains largely unexplored. We present a scheme demonstrating\nmoir\\'e effects in a one-dimensional cold atom-cavity coupling system, which\nresembles a generalized open Dicke model exhibiting superradiant phase\ntransitions. We reveal a strong link between the phase transition critical\npoint and the one-dimensional moir\\'e parameter. Additionally, we derive the\ncavity field spectrum, connected to the dynamical structure factor, and\nshowcase controlled atomic diffusion. This work provides a new route for\ntesting one-dimensional moir\\'e effects with cold atoms and open new\npossibility of moir\\'e metrology."
    ],
    "c_categories":[
      "cond-mat.quant-gas",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.12083",
    "c_title":[
      "Does a Fine-Tuned Universe Tell Us Anything About God?"
    ],
    "c_abstract":[
      "The apparent fine-tuning of several fundamental parameters that determine the\nproperties of our Universe and make it hospitable to life is sometimes used as\nan argument for God from design. I review the concept of cosmic fine-tuning and\ncritically examine the claim that God is its most probable cause. While not\ndefinitively repudiating this claim, I argue that it is potentially in tension\nwith the more apophatic approach to God found in the Abrahamic traditions. I\nthen offer a metaphysical analysis of the contingency of fine-tuning that\nsituates it within the classical analogy of being that points to the Divinity."
    ],
    "c_categories":[
      "physics.hist-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18494",
    "c_title":[
      "Lagrangians and Newtonian analogues for Biological Systems"
    ],
    "c_abstract":[
      "This study investigates the potential for biological systems to be governed\nby a variational principle, suggesting that such systems may evolve to minimize\nor optimize specific quantities. To explore this idea, we focus on identifying\nLagrange functions that can effectively model the dynamics of selected\npopulation systems. These functions provide a deeper understanding of\npopulation evolution by framing their behavior in terms of energy-like\nvariables. We present an algorithm for generating Lagrangian functions\napplicable to a family of population dynamics models and demonstrate the\nequivalence between two-dimensional population models and a one-dimensional\nNewtonian mechanical analog. Furthermore, we explore the existence of\nconservation laws for these models, utilizing Noether's theorems to investigate\ntheir implications."
    ],
    "c_categories":[
      "q-bio.PE"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11250",
    "c_title":[
      "CRPS-Based Targeted Sequential Design with Application in Chemical Space"
    ],
    "c_abstract":[
      "Sequential design of real and computer experiments via Gaussian Process (GP)\nmodels has proven useful for parsimonious, goal-oriented data acquisition\npurposes. In this work, we focus on acquisition strategies for a GP model that\nneeds to be accurate within a predefined range of the response of interest.\nSuch an approach is useful in various fields including synthetic chemistry,\nwhere finding molecules with particular properties is essential for developing\nuseful materials and effective medications. GP modeling and sequential design\nof experiments have been successfully applied to a plethora of domains,\nincluding molecule research. Our main contribution here is to use the\nthreshold-weighted Continuous Ranked Probability Score (CRPS) as a basic\nbuilding block for acquisition functions employed within sequential design. We\nstudy pointwise and integral criteria relying on two different weighting\nmeasures and benchmark them against competitors, demonstrating improved\nperformance with respect to considered goals. The resulting acquisition\nstrategies are applicable to a wide range of fields and pave the way to further\ndeveloping sequential design relying on scoring rules."
    ],
    "c_categories":[
      "cs.LG",
      "stat.AP",
      "stat.CO",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00225",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b17"
    ],
    "b_title":[
      "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
    ],
    "b_abstract":[
      "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05737",
    "c_title":[
      "An MLE analysis on the relationship between the initial-state\n  granularity and final-state flow factorization"
    ],
    "c_abstract":[
      "In this study, we employ the maximum likelihood estimator (MLE) to\ninvestigate the relationship between initial-state fluctuations and final-state\nanisotropies in relativistic heavy-ion collisions. The granularity of the\ninitial state, reflecting fluctuations in the initial conditions (IC), is\nmodeled using a peripheral tube model. Besides differential flow, our analysis\nfocuses on a class of more sensitive observables known as flow factorization.\nSpecifically, we evaluate these observables using MLE, an asymptotically normal\nand unbiased tool in standard statistical inference. Our findings show that the\nresulting differential flow remains essentially unchanged for different IC\ndefined by the peripheral tube model. The resulting harmonic coefficients\nobtained using MLE and multi-particle cumulants are found to be consistent.\nHowever, the calculated flow factorizations show significant variations\ndepending on both the IC and the estimators, which is attributed to their\nsensitivity to initial-state fluctuations. Thus, we argue that MLE offers a\ncompelling alternative to standard methods such as multi-particle correlators,\nparticularly for sensitive observables constructed from higher moments of the\nazimuthal distribution."
    ],
    "c_categories":[
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"Comparative Study of Probabilistic Atlas and Deep Learning Approaches\n  for Automatic Brain Tissue Segmentation from MRI Using N4 Bias Field\n  Correction and Anisotropic Diffusion Pre-processing Techniques",
    "a_abstract":"Automatic brain tissue segmentation from Magnetic Resonance Imaging (MRI)\nimages is vital for accurate diagnosis and further analysis in medical imaging.\nDespite advancements in segmentation techniques, a comprehensive comparison\nbetween traditional statistical methods and modern deep learning approaches\nusing pre-processing techniques like N4 Bias Field Correction and Anisotropic\nDiffusion remains underexplored. This study provides a comparative analysis of\nvarious segmentation models, including Probabilistic ATLAS, U-Net, nnU-Net, and\nLinkNet, enhanced with these pre-processing techniques to segment brain tissues\n(white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF)) on the\nInternet Brain Segmentation Repository (IBSR18) dataset. Our results\ndemonstrate that the 3D nnU-Net model outperforms others, achieving the highest\nmean Dice Coefficient score (0.937 +- 0.012), while the 2D nnU-Net model\nrecorded the lowest mean Hausdorff Distance (5.005 +- 0.343 mm) and the lowest\nmean Absolute Volumetric Difference (3.695 +- 2.931 mm) across five unseen test\nsamples. The findings highlight the superiority of nnU-Net models in brain\ntissue segmentation, particularly when combined with N4 Bias Field Correction\nand Anisotropic Diffusion pre-processing techniques. Our implemented code can\nbe accessed via GitHub.",
    "explanation":"Automatic brain tissue segmentation from Magnetic Resonance Imaging (MRI) images is vital for accurate diagnosis and further analysis in medical imaging. \n\nThis study provides a comparative analysis of various\nsegmentation models, including Probabilistic ATLAS, U-Net, nnU-Net, and LinkNet, enhanced with these preprocessing techniques to segment brain tissue",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b11"
    ],
    "c_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "c_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.14102",
    "c_title":[
      "Explainable Distributed Constraint Optimization Problems"
    ],
    "c_abstract":[
      "The Distributed Constraint Optimization Problem (DCOP) formulation is a\npowerful tool to model cooperative multi-agent problems that need to be solved\ndistributively. A core assumption of existing approaches is that DCOP solutions\ncan be easily understood, accepted, and adopted, which may not hold, as\nevidenced by the large body of literature on Explainable AI. In this paper, we\npropose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include\nits solution and a contrastive query for that solution. We formally define some\nkey properties that contrastive explanations must satisfy for them to be\nconsidered as valid solutions to X-DCOPs as well as theoretical results on the\nexistence of such valid explanations. To solve X-DCOPs, we propose a\ndistributed framework as well as several optimizations and suboptimal variants\nto find valid explanations. We also include a human user study that showed that\nusers, not surprisingly, prefer shorter explanations over longer ones. Our\nempirical evaluations showed that our approach can scale to large problems, and\nthe different variants provide different options for trading off explanation\nlengths for smaller runtimes. Thus, our model and algorithmic contributions\nextend the state of the art by reducing the barrier for users to understand\nDCOP solutions, facilitating their adoption in more real-world applications."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08011",
    "c_title":[
      "Training-Free Safe Denoisers for Safe Use of Diffusion Models"
    ],
    "c_abstract":[
      "There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.08324",
    "c_title":[
      "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations"
    ],
    "c_abstract":[
      "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19902",
    "c_title":[
      "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy"
    ],
    "c_abstract":[
      "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15268",
    "c_title":[
      "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?"
    ],
    "c_abstract":[
      "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02360",
    "c_title":[
      "Injectivity of polynomials over finite discrete dynamical systems"
    ],
    "c_abstract":[
      "The analysis of observable phenomena (for instance, in biology or physics)\nallows the detection of dynamical behaviors and, conversely, starting from a\ndesired behavior allows the design of objects exhibiting that behavior in\nengineering. The decomposition of dynamics into simpler subsystems allows us to\nsimplify this analysis (or design). Here we focus on an algebraic approach to\ndecomposition, based on alternative and synchronous execution as the sum and\nproduct operations; this gives rise to polynomial equations (with a constant\nside). In this article we focus on univariate, injective polynomials, giving a\ncharacterization in terms of the form of their coefficients and a\npolynomial-time algorithm for solving the associated equations."
    ],
    "c_categories":[
      "cs.DM",
      "math.DS"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08537",
    "c_title":[
      "Broken symmetries associated with a Kagome chiral charge order"
    ],
    "c_abstract":[
      "Chirality or handedness manifests in all fields of science, ranging from cell\nbiology, molecular interaction, and catalysis to different branches of physics.\nIn condensed matter physics, chirality is intrinsic to enigmatic quantum\nphases, such as chiral charge density waves and chiral superconductivity. Here,\nthe underlying chiral response is subtle and leads to broken symmetries in the\nground state. Detection of subtle broken symmetries is the key to understand\nthese quantum states but they are extremely challenging to expose leading to\ndebate and controversy. Here, using second-order optical response, we uncover\nthe broken symmetries of a chiral charge density wave in the Kagome lattice\nKV3Sb5, revealing the relevant broken symmetries of its charge order. KV3Sb5\nundergoes a phase transition to a charge-ordered state at low temperatures. Our\npolarization-dependent mid-infrared photocurrent microscopy reveals an\nintrinsic, longitudinal helicity-dependent photocurrent associated with the\ncharge order. Our measurements, supported by our theoretical analysis, provide\ndirect evidence for broken inversion and mirror symmetries at the charge order\ntransition, indicating a chiral charge ordered state. On the other hand, we do\nnot observe a circular photogalvanic effect along the direction perpendicular\nto that of the incident light, imposing stringent constraints on the rotational\nand point group symmetries of the charge order. Our study not only visualizes\nthe chiral nature of the Kagome charge order revealing its broken symmetries,\nbut also highlights the nonlinear photogalvanic effect as a sensitive probe for\ndetecting subtle symmetry breakings."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cond-mat.str-el",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04692",
    "c_title":[
      "A study of the frequency and characteristics of stellar companions and\n  Jupiter-like planets in nearby open clusters"
    ],
    "c_abstract":[
      "Observations of companions of solar-type stars in nearby young moving groups\n(NYMGs) show that they split into two groups: stellar and brown dwarf\ncompanions (mass ratio $q>0.05$) and Jupiter-like (JL) planets ($q<0.02$). The\nfrequency of JL planets in NYMGs appears to be higher than that obtained from\nradial velocity (RV) surveys. We extended the search for companions to three\nnearby clusters of intermediate age: Hyades, Coma Berenices, and Ursa Major.\nThey are older and formed in more massive events than the NYMGs. The sample of\nhost stars is complete for the core of the clusters, while we considered only a\nfraction of the tidal tails. We used the same methods considered for the\nmembers of NYMGs. We obtained a fairly complete sample of stellar companions\nand detected six massive JL planets. We found a lower frequency of equal-mass\ncompanions than in the NYMGs; this might be related to how binaries form in\nthese environments. We also observed a concentration of stellar binaries in the\ncores of Ursa Major and Coma Berenices; we attribute this to the selective loss\nof low-mass systems. The observed scarcity of wide companions in Hyades can be\ndue to the destruction of binaries in close encounters. The frequency of JL\nplanets is lower than in the NYMGs but similar to that obtained from RV\nsurveys. This extends the correlation with age and mass previously found for\nNYMGs. Results of this study alone do not indicate whether age or mass are the\nfactors driving the observed correlation. A comparison of the frequencies of\nfree-floating planets from microlenses and in young associations favours mass\nas the main driving parameter. Once the initial cluster mass function is\nconsidered, the frequency of JL planets in NYMGs is consistent with the results\nobtained using RVs."
    ],
    "c_categories":[
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.17962",
    "c_title":[
      "Nonlinear Domain Engineering for Quantum Technologies"
    ],
    "c_abstract":[
      "The continuously growing effort towards developing real-world quantum\ntechnological applications has come to demand an increasing amount of\nflexibility from its respective platforms. This review presents a highly\nadaptable engineering technique for photonic quantum technologies based on the\nartificial structuring of the material nonlinearity. This technique, while, in\na simple form, already featured across the full breadth of photonic quantum\ntechnologies, has undergone significant development over the last decade, now\nfeaturing advanced, aperiodic designs. This review gives an introduction to the\nthree-wave-mixing processes lying at the core of this approach, and\nillustrates, on basis of the underlying quantum-mechanical description, how\nthey can artificially be manipulated to engineer the corresponding photon\ncharacteristics. It then describes how this technique can be employed to\nrealize a number of very different objectives which are expected to find\napplication across the full range of photonic quantum technologies, and\npresents a summary of the research done towards these ends to date."
    ],
    "c_categories":[
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "MRI segmentation of the human brain: challenges, methods, and applications"
    ],
    "b_abstract":[
      "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
    ],
    "b_categories":[
      "q-bio.QM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04262",
    "c_title":[
      "Efficient Randomized Experiments Using Foundation Models"
    ],
    "c_abstract":[
      "Randomized experiments are the preferred approach for evaluating the effects\nof interventions, but they are costly and often yield estimates with\nsubstantial uncertainty. On the other hand, in silico experiments leveraging\nfoundation models offer a cost-effective alternative that can potentially\nattain higher statistical precision. However, the benefits of in silico\nexperiments come with a significant risk: statistical inferences are not valid\nif the models fail to accurately predict experimental responses to\ninterventions. In this paper, we propose a novel approach that integrates the\npredictions from multiple foundation models with experimental data while\npreserving valid statistical inference. Our estimator is consistent and\nasymptotically normal, with asymptotic variance no larger than the standard\nestimator based on experimental data alone. Importantly, these statistical\nproperties hold even when model predictions are arbitrarily biased. Empirical\nresults across several randomized experiments show that our estimator offers\nsubstantial precision gains, equivalent to a reduction of up to 20% in the\nsample size needed to match the same precision as the standard estimator based\non experimental data alone."
    ],
    "c_categories":[
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04147",
    "c_title":[
      "A Framework for Building Enviromics Matrices in Mixed Models"
    ],
    "c_abstract":[
      "This study introduces a framework for constructing enviromics matrices in\nmixed models to integrate genetic and environmental data to enhance phenotypic\npredictions in plant breeding. Enviromics utilizes diverse data sources, such\nas climate and soil, to characterize genotype-by-environment (GxE)\ninteractions. The approach employs block-diagonal structures in the design\nmatrix to incorporate random effects from genetic and envirotypic covariates\nacross trials. The covariance structure is modeled using the Kronecker product\nof the genetic relationship matrix and an identity matrix representing\nenvirotypic effects, capturing genetic and environmental variability. This dual\nrepresentation enables more accurate crop performance predictions across\nenvironments, improving selection strategies in breeding programs. The\nframework is compatible with existing mixed model software, including rrBLUP\nand BGLR, and can be extended for more complex interactions. By combining\ngenetic relationships and environmental influences, this approach offers a\npowerful tool for advancing GxE studies and accelerating the development of\nimproved crop varieties."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06548",
    "c_title":[
      "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)"
    ],
    "c_abstract":[
      "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07606",
    "c_title":[
      "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions"
    ],
    "c_abstract":[
      "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13821",
    "c_title":[
      "Redefining Influenza Transmission Seasonality Using the Novel\n  Seasonality Index"
    ],
    "c_abstract":[
      "The impact of climate conditions on influenza epidemiology has mostly been\nstudied by addressing a singular aspect of transmission and a climate variable\ncorrelating to it. As climate change unfolds at an unprecedented rate, we\nurgently need new multidisciplinary approaches that can embrace complexity of\ndisease transmission in the fast-changing environment and help us better\nunderstand the implications for health. In this study, we have implemented a\nnovel seasonality index to capture a vast network of climate, infectious, and\nsocio-behavioural mechanisms influencing a seasonal influenza epidemic. We\nhypothesize that intricate, region-specific behavioural patterns are cross\nregulating the influenza spreading and dynamics of epidemics with changes in\nmeteorological conditions within a specific season. To better understand the\nphenomena, we analysed weekly surveillance data from temperate European\ncountries and redefined seasonal transitions using the seasonality index. This\napproach allowed us to characterize influenza seasonality more accurately in\nrelation to specific atmospheric conditions. Key findings include: i) a strong\ncorrelation between influenza infection rates and the seasonality index across\ndifferent climate zones and social groups, and ii) a high linear correlation\nbetween winter duration, determined by the seasonality index, and the time\nscale of low-frequency peaks in the infection rates power spectral density."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.01198",
    "c_title":[
      "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study"
    ],
    "c_abstract":[
      "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population."
    ],
    "c_categories":[
      "q-bio.QM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18926",
    "c_title":[
      "Excited-state magnetic properties of carbon-like $\\text{Ca}^{14+}$"
    ],
    "c_abstract":[
      "We measured the $g$-factor of the excited state $^3\\text{P}_1$ in\n$\\text{Ca}^{14+}$ ion to be $g = 1.499032(6)$ with a relative uncertainty of\n$4\\times10^{-6}$. The magnetic field magnitude is derived from the Zeeman\nsplitting of a $\\text{Be}^+$ ion, co-trapped in the same linear Paul trap as\nthe highly charged $\\text{Ca}^{14+}$ ion. Furthermore, we experimentally\ndetermined the second-order Zeeman coefficient $C_2$ of the $^3\\text{P}_0$ -\n$^3\\text{P}_1$ clock transition. For the $m_J=0\\rightarrow m_{J'}=0$\ntransition, we obtain $C_2 = 0.39\\pm0.04\\text{HzmT}^{-2}$, which is to our\nknowledge the smallest reported for any atomic transition to date. This\nconfirms the predicted low sensitivity of highly charged ions to higher-order\nZeeman effects, making them ideal candidates for high-precision optical clocks.\nComparison of the experimental results with our state-of-the art electronic\nstructure calculations shows good agreement, and demonstrates the significance\nof the frequency-dependent Breit contribution, negative energy states and QED\neffects on magnetic moments."
    ],
    "c_categories":[
      "physics.atom-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14712",
    "c_title":[
      "Distribution and Purification of Entanglement States in Quantum Networks"
    ],
    "c_abstract":[
      "We consider problems of distributing high-fidelity entangled states across\nnodes of a quantum network. We consider a repeater-based network architecture\nwith entanglement swapping (fusion) operations for generating long-distance\nentanglements, and purification operations that produce high-fidelity states\nfrom several lower-fidelity states. The contributions of this paper are\ntwo-fold: First, while there have been several works on fidelity-aware routing\nand incorporating purification into routing for generating EPs, this paper\npresents the first algorithms for optimal solutions to the high-fidelity EP\ndistribution problem. We provide a dynamic programming algorithm for generating\nthe optimal tree of operations to produce a high-fidelity EP, and an LP-based\nalgorithm for generating an optimal collection of trees. Second, following the\nEP algorithms, this paper presents the first algorithms for the high-fidelity\nGHZ-state distribution problem and characterizes its optimality. We evaluate\nour techniques via simulations over NetSquid, a quantum network simulator."
    ],
    "c_categories":[
      "cs.NI",
      "quant-ph"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.10336",
    "c_title":[
      "Eigenspectra of Minimally Doubled Fermions"
    ],
    "c_abstract":[
      "In this work, we explored the eigenspectra of minimally doubled fermions, in\nboth Karsten-Wilczek and Borici-Creutz realizations. We generated 4-dim $SU(3)$\ngauge fields with a definite topological charge and calculated the chiralities\nof the eigenmodes for KW and BC fermions. We used the spectral flow of the\neigenvalues for this purpose and demonstrated the Index theorem."
    ],
    "c_categories":[
      "hep-lat"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16103",
    "c_title":[
      "$d$-Wave Polarization-Spin Locking in Two-Dimensional Altermagnets"
    ],
    "c_abstract":[
      "We report the emergence of an uncharted phenomenon, termed $d$-wave\npolarization-spin locking (PSL), in two-dimensional (2D) altermagnets. This\nphenomenon arises from nontrivial Berry connections, resulting in perpendicular\nelectronic polarizations in the spin-up and spin-down channels.\nSymmetry-protected $d$-wave PSL occurs exclusively in $d$-wave altermagnets\nwith tetragonal layer groups. To identify 2D altermagnets capable of exhibiting\nthis phenomenon, we propose a symmetry-eigenvalue-based criterion, and a rapid\nmethod by observing the spin-momentum locking. Using first-principles\ncalculations, monolayer Cr$_2$X$_2$O (X = Se, Te) characterizes promising\ncandidates for $d$-wave PSL, driven by the unusual charge order in these\nmonolayers. This unique polarization-spin interplay leads to spin-up and\nspin-down electrons accumulating at orthogonal edges, enabling potential\napplications as spin filters or splitters in spintronics. Furthermore, $d$-wave\nPSL introduces an unexpected spin-driven ferroelectricity in conventional\nantiferromagnets. Such magnetoelectric coupling positions $d$-wave PSL as an\nideal platform for fast antiferromagnetic memory devices. Our findings not only\nexpand the landscape of altermagnets, complementing conventional collinear\nferromagnets and antiferromagnets, but also highlight tantalizing\nfunctionalities in altermagnetic materials, potentially revolutionizing\ninformation technology."
    ],
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05456",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b11"
    ],
    "b_title":[
      "U-net: Convolutional networks for biomedical image segmentation"
    ],
    "b_abstract":[
      "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02683",
    "c_title":[
      "The subpath number of cactus graphs"
    ],
    "c_abstract":[
      "The subpath number of a graph G is defined as the total number of subpaths in\nG, and it is closely related to the number of subtrees, a well-studied topic in\ngraph theory. This paper is a continuation of our previous paper [5], where we\ninvestigated the subpath number and identified extremal graphs within the\nclasses of trees, unicyclic graphs, bipartite graphs, and cycle chains. Here,\nwe focus on the subpath number of cactus graphs and characterize all maximal\nand minimal cacti with n vertices and k cycles. We prove that maximal cacti are\ncycle chains in which all interior cycles are triangles, while the two\nend-cycles differ in length by at most one. In contrast, minimal cacti consist\nof k triangles, all sharing a common vertex, with the remaining vertices\nforming a tree attached to this joint vertex. By comparing extremal cacti with\nrespect to the subpath number to those that are extremal for the subtree number\nand the Wiener index, we demonstrate that the subpath number does not correlate\nwith either of these quantities, as their corresponding extremal graphs differ."
    ],
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  }
]