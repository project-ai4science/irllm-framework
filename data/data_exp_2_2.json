[
  {
    "id":"2411.01019",
    "a_title":"A lightweight Convolutional Neural Network based on U shape structure\n  and Attention Mechanism for Anterior Mediastinum Segmentation",
    "a_abstract":"To automatically detect Anterior Mediastinum Lesions (AMLs) in the Anterior\nMediastinum (AM), the primary requirement will be an automatic segmentation\nmodel specifically designed for the AM. The prevalence of AML is extremely low,\nmaking it challenging to conduct screening research similar to lung cancer\nscreening. Retrospectively reviewing chest CT scans over a specific period to\ninvestigate the prevalence of AML requires substantial time. Therefore,\ndeveloping an Artificial Intelligence (AI) model to find location of AM helps\nradiologist to enhance their ability to manage workloads and improve diagnostic\naccuracy for AMLs. In this paper, we introduce a U-shaped structure network to\nsegment AM. Two attention mechanisms were used for maintaining long-range\ndependencies and localization. In order to have the potential of Multi-Head\nSelf-Attention (MHSA) and a lightweight network, we designed a parallel MHSA\nnamed Wide-MHSA (W-MHSA). Maintaining long-range dependencies is crucial for\nsegmentation when we upsample feature maps. Therefore, we designed a Dilated\nDepth-Wise Parallel Path connection (DDWPP) for this purpose. In order to\ndesign a lightweight architecture, we introduced an expanding convolution block\nand combine it with the proposed W-MHSA for feature extraction in the encoder\npart of the proposed U-shaped network. The proposed network was trained on 2775\nAM cases, which obtained an average Dice Similarity Coefficient (DSC) of\n87.83%, mean Intersection over Union (IoU) of 79.16%, and Sensitivity of\n89.60%. Our proposed architecture exhibited superior segmentation performance\ncompared to the most advanced segmentation networks, such as Trans Unet,\nAttention Unet, Res Unet, and Res Unet++.",
    "explanation":"It's leveraging AI in another diffrent domain, namely medical science, to detect Anterior Mediastinum Lesions.",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b6",
      "b1"
    ],
    "c_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "c_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "c_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":"Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
    "c_abstract":"Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":"Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
    "c_abstract":"Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":"Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
    "c_abstract":"Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":"Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
    "c_abstract":"Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":"A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
    "c_abstract":"Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03567",
    "c_title":"Visual tests using several safe confidence intervals",
    "c_abstract":"We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
    "c_categories":[
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16204",
    "c_title":"Gamma-ray burst plateaus within the framework of black hole spin-down",
    "c_abstract":"In this manuscript, we investigate observational correlations between the\nproperties of gamma-ray bursts (GRBs) across the gamma, X-ray, and optical\nbands during the prompt and plateau phases of their light curves (LCs). Our\nanalysis includes all GRBs with known redshifts detected by the Neil Gehrels\n{\\it Swift} Observatory ({\\it Swift}) and the {\\it Fermi} Gamma-ray Space\nTelescope ({\\it Fermi}), as well as ground-based optical telescopes. We\nidentify a tight correlation with the $R^2$ coefficient of $\\sim 0.89$ for the\nthree-dimensional Dainotti relation between the luminosity at the end of the\nplateau, its duration measured by {\\it Swift}, and the peak luminosity measured\nby {\\it Fermi} in the 10-1000 keV band. When accounting for redshift evolution,\nwe achieve very small intrinsic scatter $\\sigma_{int}=0.25\\pm0.04$ ($\\sim 43\\%$\nreduction compared to the previous results). Additionally, we explore\ncorrelations involving the optical luminosity at the end of the plateau,\nyielding promising results. We investigate the clustering of different classes\nof GRBs in the investigated parameter space and discuss its impact on the\ncorrelations. Finally, we discuss the theory supporting the evidence of the\nplateau emission. We present a new paradigm for the GRB plateau: energy\nextraction from a quickly rotating black hole via spin-down by a magnetically\narrested disk. We compare this model with observations and explain multiple\nobserved features. We predict the plateau luminosity - time anti-correlation\nand discuss the cosmological evolution within this proposed model. Furthermore,\nwithin this new model, we discuss the possible physical origin of the\nclustering of long and short GRBs in the parameter space of plateau luminosity\n- time - prompt luminosity.",
    "c_categories":[
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01726",
    "c_title":"Chaotic Behavior of Trapped Cosmic Rays",
    "c_abstract":"Recent experimental results on the arrival direction of high-energy cosmic\nrays have motivated studies to understand their propagating environment. The\nobserved anisotropy is shaped by interstellar and local magnetic fields. In\ncoherent magnetic structures, such as the heliosphere, or due to\nmagnetohydrodynamic turbulence, magnetic mirroring can temporarily trap\nparticles, leading to chaotic behavior. In this work, we develop a new method\nto characterize cosmic rays' chaotic behavior in magnetic systems using\nfinite-time Lyapunov exponents. This quantity determines the degree of chaos\nand adapts to transitory behavior. We study particle trajectories in an\naxial-symmetric magnetic bottle to highlight mirroring effects. By introducing\ntime-dependent magnetic perturbations, we study how temporal variations affect\nchaotic behavior. We tailor our model to the heliosphere; however, it can\nrepresent diverse magnetic configurations exhibiting mirroring phenomena. Our\nresults have three key implications. (1)Theoretical: We find a correlation\nbetween the finite-time Lyapunov exponent and the particle escape time from the\nsystem, which follows a power law that persists even under additional\nperturbations. This power law may reveal intrinsic system characteristics,\noffering insight into propagation dynamics beyond simple diffusion.\n(2)Simulation: Chaotic effects play a role in cosmic ray simulations and can\ninfluence the resulting anisotropy maps. (3)Observational: Arrival maps display\nareas where the chaotic properties vary significantly; these changes can be the\nbasis for time variability in the anisotropy maps. This work lays the framework\nfor studying the effects of magnetic mirroring of cosmic rays within the\nheliosphere and the role of temporal variability in the observed anisotropy.",
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08526",
    "c_title":"Towards a noncommutative theory of Cowen-Douglas class of noncommuting\n  operators",
    "c_abstract":"The classical Cowen-Douglas class of (commuting tuples of) operators\npossessing an open set of (joint) eigenvalues of finite constant multiplicity\nwas introduced by Cowen and Douglas, generalizing the backward shifts. Their\nunitary equivalence classes are determined by the equivalence classes of\ncertain hermitian holomorphic vector bundles associated with them on this set.\n  This article develops a free noncommutative analogue of Cowen-Douglas theory\nto explore the notion of vector bundles in the setting of free noncommutative\nfunction theory. We define the noncommutative Cowen-Douglas class using\nmatricial joint eigenvalues, as envisioned by Taylor, and show via the\nTaylor-Taylor series that the associated joint eigenspaces naturally form such\na vector bundle, what we call a noncommutative hermitian holomorphic vector\nbundle.\n  A key result is that the unitary equivalence class of a tuple in this class\nis completely determined by the equivalence class of its associated\nnoncommutative vector bundle. This work lays the groundwork of the\nnoncommutative hermitian geometry, which investigates noncommutative analogues\nof complex manifolds, vector bundles, and hermitian metrics by drawing on ideas\nfrom both complex hermitian geometry and operator theory.\n  We also examine noncommutative reproducing kernel Hilbert space models and\nintroduce the noncommutative Gleason problem, showing that elements of the\nnoncommutative Cowen-Douglas class are essentially (up to unitary equivalence)\nadjoints of left multiplication operators by noncommuting independent variables\nin a noncommutative reproducing kernel Hilbert space.",
    "c_categories":[
      "math.DG",
      "math.FA",
      "math.OA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b15"
    ],
    "b_title":[
      "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
    ],
    "b_abstract":[
      "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.10257",
    "c_title":"Orthogonal projections of hypercubes",
    "c_abstract":"Projections of hypercubes have been applied to visualize high-dimensional\nbinary state spaces in various scientific fields. Conventional methods for\nprojecting hypercubes, however, face practical difficulties. Manual methods\nrequire nontrivial adjustments of the projection basis, while\noptimization-based algorithms limit the interpretability and reproducibility of\nthe resulting plots. These limitations motivate us to explore theoretically\nanalyzable projection algorithms such as principal component analysis (PCA).\nHere, we investigate the mathematical properties of PCA-projected hypercubes.\nOur numerical and analytical results show that PCA effectively captures\npolarized distributions within the hypercubic state space. This property\nenables the assessment of the asymptotic distribution of projected vertices and\nerror bounds, which characterize the performance of PCA in the projected space.\nWe demonstrate the application of PCA to visualize the hypercubic energy\nlandscapes of Ising spin systems. By adding projected hypercubic edges, these\nvisualizations reveal pathways of correlated spin flips. Our work provides a\nbetter understanding of how PCA discovers hidden patterns in high-dimensional\nbinary data.",
    "c_categories":[
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "physics.data-an"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07709",
    "c_title":"MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
    "c_abstract":"Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08177",
    "c_title":"SycEval: Evaluating LLM Sycophancy",
    "c_abstract":"Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15726",
    "c_title":"Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
    "c_abstract":"The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.10768",
    "c_title":"MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
    "c_abstract":"Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.07487",
    "c_title":"Data and System Perspectives of Sustainable Artificial Intelligence",
    "c_abstract":"Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04143",
    "c_title":"Linear Optimization for the Perfect Meal: A Data-Driven Approach to\n  Optimising the Perfect Meal Using Gurobi",
    "c_abstract":"This study aims to optimize meal planning for nutritional health and cost\nefficiency using linear programming. Linear optimization provides an effective\nframework for addressing the problem of an optimal diet, as the composition of\nfood can be naturally modeled as a linearly additive system. Leveraging a\ncomprehensive nutrition dataset, our model minimizes meal costs while meeting\nspecific nutritional requirements. We explore additional complexities, such as\nfractional weights and nutrient ratio constraints, enhancing the robustness of\nthe solution. Case studies address common nutritional challenges, providing\ntailored diet plans. The significance lies in aiding individuals to form\nbalanced, cost-effective dietary schedules, considering fitness goals and\ncaloric needs. This research contributes to efficient, sustainable, and\ntime-sensitive meal planning, emphasizing the intersection of nutrition,\noptimization, and real-world applicability.",
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.05435",
    "c_title":"The excenters of bicentric polygons are concyclic",
    "c_abstract":"We show that the centers of the excircles of a bicentric polygon $B$ are\nconcyclic on a circle $E$. The center of the circumscribed circle $K$ of $B$ is\nthe midpoint of the center of $E$ and the center of the inscribed circle $C$ of\n$B$. The radius of $E$ is given by a simple formula in terms of the radii of\n$C$ and $K$ and the distance between their centers.",
    "c_categories":[
      "math.MG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17712",
    "c_title":"Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis",
    "c_abstract":"Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$.",
    "c_categories":[
      "math.CA",
      "math.MG",
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09727",
    "c_title":"Convergence of a Deep BSDE solver with jumps",
    "c_abstract":"We study the error arising in the numerical approximation of FBSDEs and\nrelated PIDEs by means of a deep learning-based method. Our results focus on\ndecoupled FBSDEs with jumps and extend the seminal work of HAn and Long (2020)\nanalyzing the numerical error of the deep BSDE solver proposed in E et al.\n(2017). We provide a priori and a posteriori error estimates for the finite and\ninfinite activity case.",
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC",
      "math.PR",
      "q-fin.CP",
      "q-fin.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Economics and Quantitative Finance",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.01019",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6",
      "b1"
    ],
    "b_title":[
      "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
      "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
    ],
    "b_abstract":[
      "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
      "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
    ],
    "b_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.14295",
    "c_title":"Comparative analysis of two episodes of strongly geoeffective CME events\n  in November and December 2023",
    "c_abstract":"In autumn 2023, a series of closely timed eruptive events were observed\nremotely and measured in situ. We studied analogous solar events, where several\nCMEs were launched partly from the same (active) regions near a CH. These\nevents occurred in two episodes, separated by a full solar rotation, covering\nOctober 31-November 3 and November 27-28, 2023. Both episodes are linked to\nstrong geomagnetic storms on November 4-5 and December 1-2, 2023. We aim to\nunderstand the complexity of these events and how the global magnetic field,\nsolar wind conditions, and structural interactions relate to the observed\ngeomagnetic effects. Using the GCS 3D reconstruction method, we derived each\nCME's motion direction and speed. These results were input into the DBM with\nenhanced latitudinal information (3D DBM), aiding in connecting in-situ\nmeasurements with solar surface structures for integrated interpretation. The\nfirst episode caused SAR arcs, with a three-step Dst index drop to -163 nT on\nNovember 5, 2023. Two CME-related shocks arrived close in time, separated by a\nSBC, followed by a short-duration flux rope-like structure. The second episode\nsaw auroral lights and a two-step Dst index drop to -108 nT on December 1,\n2023. A shock from one CME interacted with the magnetic structure of a\npreceding CME, again combined with an SBC. A clear flux rope structure from the\nshock-producing CME was detected. Both events showed distinct magnetic field\n'ripples' and fluctuations in density and temperature following the SBC. This\nstudy compares two episodes of multiple eruptive events in November and\nDecember 2023. Interacting CME structures and SBC-related magnetic modulations\ncontributed to the stronger geomagnetic impacts, particularly in the November\n4-5, 2023 event. The highly tilted heliospheric current sheet may have further\ninfluenced the CMEs' impact at Earth.",
    "c_categories":[
      "astro-ph.SR",
      "physics.space-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"BarcodeMamba: State Space Models for Biodiversity Analysis",
    "a_abstract":"DNA barcodes are crucial in biodiversity analysis for building automatic\nidentification systems that recognize known species and discover unseen\nspecies. Unlike human genome modeling, barcode-based invertebrate\nidentification poses challenges in the vast diversity of species and taxonomic\ncomplexity. Among Transformer-based foundation models, BarcodeBERT excelled in\nspecies-level identification of invertebrates, highlighting the effectiveness\nof self-supervised pretraining on barcode-specific datasets. Recently,\nstructured state space models (SSMs) have emerged, with a time complexity that\nscales sub-quadratically with the context length. SSMs provide an efficient\nparameterization of sequence modeling relative to attention-based\narchitectures. Given the success of Mamba and Mamba-2 in natural language, we\ndesigned BarcodeMamba, a performant and efficient foundation model for DNA\nbarcodes in biodiversity analysis. We conducted a comprehensive ablation study\non the impacts of self-supervised training and tokenization methods, and\ncompared both versions of Mamba layers in terms of expressiveness and their\ncapacity to identify \"unseen\" species held back from training. Our study shows\nthat BarcodeMamba has better performance than BarcodeBERT even when using only\n8.3% as many parameters, and improves accuracy to 99.2% on species-level\naccuracy in linear probing without fine-tuning for \"seen\" species. In our\nscaling study, BarcodeMamba with 63.6% of BarcodeBERT's parameters achieved\n70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing for unseen\nspecies. The code repository to reproduce our experiments is available at\nhttps:\/\/github.com\/bioscan-ml\/BarcodeMamba.",
    "explanation":"The paper talks about the use of BarcodeMamba for better scores in DNA barcode analysis of genomes.",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b0"
    ],
    "c_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "c_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15985",
    "c_title":"Exploring the Reliability of Self-explanation and its Relationship with\n  Classification in Language Model-driven Financial Analysis",
    "c_abstract":"Language models (LMs) have exhibited exceptional versatility in reasoning and\nin-depth financial analysis through their proprietary information processing\ncapabilities. Previous research focused on evaluating classification\nperformance while often overlooking explainability or pre-conceived that\nrefined explanation corresponds to higher classification accuracy. Using a\npublic dataset in finance domain, we quantitatively evaluated self-explanations\nby LMs, focusing on their factuality and causality. We identified the\nstatistically significant relationship between the accuracy of classifications\nand the factuality or causality of self-explanations. Our study built an\nempirical foundation for approximating classification confidence through\nself-explanations and for optimizing classification via proprietary reasoning.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06479",
    "c_title":"ExKG-LLM: Leveraging Large Language Models for Automated Expansion of\n  Cognitive Neuroscience Knowledge Graphs",
    "c_abstract":"The paper introduces ExKG-LLM, a framework designed to automate the expansion\nof cognitive neuroscience knowledge graphs (CNKG) using large language models\n(LLMs). It addresses limitations in existing tools by enhancing accuracy,\ncompleteness, and usefulness in CNKG. The framework leverages a large dataset\nof scientific papers and clinical reports, applying state-of-the-art LLMs to\nextract, optimize, and integrate new entities and relationships. Evaluation\nmetrics include precision, recall, and graph density. Results show significant\nimprovements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score\n(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density\nslightly decreased, reflecting a broader but more fragmented structure.\nEngagement rates rose by 20%, while CNKG diameter increased to 15, indicating a\nmore distributed structure. Time complexity improved to O(n log n), but space\ncomplexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates\npotential for enhancing knowledge generation, semantic search, and clinical\ndecision-making in cognitive neuroscience, adaptable to broader scientific\nfields.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11951",
    "c_title":"SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
    "c_abstract":"Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04512",
    "c_title":"Safety is Essential for Responsible Open-Ended Systems",
    "c_abstract":"AI advancements have been significantly driven by a combination of foundation\nmodels and curiosity-driven learning aimed at increasing capability and\nadaptability. A growing area of interest within this field is Open-Endedness -\nthe ability of AI systems to continuously and autonomously generate novel and\ndiverse artifacts or solutions. This has become relevant for accelerating\nscientific discovery and enabling continual adaptation in AI agents. This\nposition paper argues that the inherently dynamic and self-propagating nature\nof Open-Ended AI introduces significant, underexplored risks, including\nchallenges in maintaining alignment, predictability, and control. This paper\nsystematically examines these challenges, proposes mitigation strategies, and\ncalls for action for different stakeholders to support the safe, responsible\nand successful development of Open-Ended AI.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07374",
    "c_title":"LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
    "c_abstract":"Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04143",
    "c_title":"Linear Optimization for the Perfect Meal: A Data-Driven Approach to\n  Optimising the Perfect Meal Using Gurobi",
    "c_abstract":"This study aims to optimize meal planning for nutritional health and cost\nefficiency using linear programming. Linear optimization provides an effective\nframework for addressing the problem of an optimal diet, as the composition of\nfood can be naturally modeled as a linearly additive system. Leveraging a\ncomprehensive nutrition dataset, our model minimizes meal costs while meeting\nspecific nutritional requirements. We explore additional complexities, such as\nfractional weights and nutrient ratio constraints, enhancing the robustness of\nthe solution. Case studies address common nutritional challenges, providing\ntailored diet plans. The significance lies in aiding individuals to form\nbalanced, cost-effective dietary schedules, considering fitness goals and\ncaloric needs. This research contributes to efficient, sustainable, and\ntime-sensitive meal planning, emphasizing the intersection of nutrition,\noptimization, and real-world applicability.",
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.05435",
    "c_title":"The excenters of bicentric polygons are concyclic",
    "c_abstract":"We show that the centers of the excircles of a bicentric polygon $B$ are\nconcyclic on a circle $E$. The center of the circumscribed circle $K$ of $B$ is\nthe midpoint of the center of $E$ and the center of the inscribed circle $C$ of\n$B$. The radius of $E$ is given by a simple formula in terms of the radii of\n$C$ and $K$ and the distance between their centers.",
    "c_categories":[
      "math.MG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17712",
    "c_title":"Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis",
    "c_abstract":"Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$.",
    "c_categories":[
      "math.CA",
      "math.MG",
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09727",
    "c_title":"Convergence of a Deep BSDE solver with jumps",
    "c_abstract":"We study the error arising in the numerical approximation of FBSDEs and\nrelated PIDEs by means of a deep learning-based method. Our results focus on\ndecoupled FBSDEs with jumps and extend the seminal work of HAn and Long (2020)\nanalyzing the numerical error of the deep BSDE solver proposed in E et al.\n(2017). We provide a priori and a posteriori error estimates for the finite and\ninfinite activity case.",
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC",
      "math.PR",
      "q-fin.CP",
      "q-fin.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Economics and Quantitative Finance",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "Biological identifications through DNA barcodes"
    ],
    "b_abstract":[
      "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.14295",
    "c_title":"Comparative analysis of two episodes of strongly geoeffective CME events\n  in November and December 2023",
    "c_abstract":"In autumn 2023, a series of closely timed eruptive events were observed\nremotely and measured in situ. We studied analogous solar events, where several\nCMEs were launched partly from the same (active) regions near a CH. These\nevents occurred in two episodes, separated by a full solar rotation, covering\nOctober 31-November 3 and November 27-28, 2023. Both episodes are linked to\nstrong geomagnetic storms on November 4-5 and December 1-2, 2023. We aim to\nunderstand the complexity of these events and how the global magnetic field,\nsolar wind conditions, and structural interactions relate to the observed\ngeomagnetic effects. Using the GCS 3D reconstruction method, we derived each\nCME's motion direction and speed. These results were input into the DBM with\nenhanced latitudinal information (3D DBM), aiding in connecting in-situ\nmeasurements with solar surface structures for integrated interpretation. The\nfirst episode caused SAR arcs, with a three-step Dst index drop to -163 nT on\nNovember 5, 2023. Two CME-related shocks arrived close in time, separated by a\nSBC, followed by a short-duration flux rope-like structure. The second episode\nsaw auroral lights and a two-step Dst index drop to -108 nT on December 1,\n2023. A shock from one CME interacted with the magnetic structure of a\npreceding CME, again combined with an SBC. A clear flux rope structure from the\nshock-producing CME was detected. Both events showed distinct magnetic field\n'ripples' and fluctuations in density and temperature following the SBC. This\nstudy compares two episodes of multiple eruptive events in November and\nDecember 2023. Interacting CME structures and SBC-related magnetic modulations\ncontributed to the stronger geomagnetic impacts, particularly in the November\n4-5, 2023 event. The highly tilted heliospheric current sheet may have further\ninfluenced the CMEs' impact at Earth.",
    "c_categories":[
      "astro-ph.SR",
      "physics.space-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04869",
    "c_title":"Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
    "c_abstract":"The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.21125",
    "c_title":"Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
    "c_abstract":"Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18758",
    "c_title":"Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
    "c_abstract":"Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17418",
    "c_title":"Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
    "c_abstract":"As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11217",
    "c_title":"CoverM: Read alignment statistics for metagenomics",
    "c_abstract":"Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.02557",
    "c_title":"Coalgebras, bialgebras and Rota-Baxter algebras from shuffles of rooted\n  forests",
    "c_abstract":"We construct and study new generalisations to rooted trees and forests of\nsome properties of shuffles of words. First, we build a coproduct on rooted\ntrees which, together with their shuffle, endow them with bialgebra structure.\nWe then caracterize the coproduct dual to the shuffle product of rooted forests\nand build a product on rooted trees to obtain the bialgebra dual to the shuffle\nbialgebra. We then characterize and enumerate primitive trees for the dual\ncoproduct. Finally, using modified shuffles of rooted forests, we prove a\nproperty in the category of Rota-Baxter algebras.",
    "c_categories":[
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.07804",
    "c_title":"Achievable Rate Regions for Multi-terminal Quantum Channels via Coset\n  Codes",
    "c_abstract":"We undertake a Shannon theoretic study of the problem of communicating\nclassical information over (i) a $3-$user quantum interference channel (QIC)\nand (ii) a $3-$user quantum broadcast channel (QBC). Our focus is on\ncharacterizing inner bounds. In our previous work, we had demonstrated that\ncoding strategies based on coset codes can yield strictly larger inner bounds.\nAdopting the powerful technique of \\textit{tilting}, \\textit{smoothing} and\n\\textit{augmentation} discovered by Sen recently, and combining with our coset\ncode strategy we derive a new inner bound to the classical-quantum capacity\nregion of both the $3-$user QIC and $3-$user QBC. The derived inner bound\nsubsumes all current known bounds.",
    "c_categories":[
      "cs.IT",
      "math.IT",
      "quant-ph"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.20851",
    "c_title":"Can Bohmian mechanics be considered complete?",
    "c_abstract":"In this work celebrating the centenary of quantum mechanics, we review the\nprinciples of de Broglie Bohm theory, also known as pilot-wave theory and\nBohmian mechanics. We assess the most common reading of it (the Nomological\ninterpretation based on the notion of primitive ontology in tridimensional\nspace) and defend instead a more causal and pluralistic approach, drawing on\nclassical analogies with optics and hydrodynamics. Within this framework, we\nreview some of the approaches exploiting mechanical analogies to overcome the\nlimitations of current Bohmian theory and perhaps quantum mechanics itself.",
    "c_categories":[
      "physics.hist-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13062",
    "c_title":"The outer structure of old star clusters in the Small Magellanic Cloud",
    "c_abstract":"We report results on the internal dynamical evolution of old star clusters\nlocated in the outer regions of the Small Magellanic Cloud (SMC). Because the\nSMC has been imprinted with evidence of tidal interaction with the Large\nMagellanic Cloud (LMC), we investigated at what extend such an interaction has\nproduced extra tidal structures or excess of stars beyond the clusters' tidal\nradii. For that purpose, we used the Survey of the Magellanic Stellar History\n(SMASH) DR2 data sets to build number density radial profiles of suitable star\nclusters, and derived their structural and internal dynamics parameters. The\nanalysed stellar density profiles do not show any evidence of tidal effects\ncaused by the LMC. On the contrary, the Jacobi volume of the selected SMC star\nclusters would seem underfilled, with a clear trend toward a smaller percentage\nof underfilled volume as their deprojected distance to the SMC centre\nincreases. Moreover, the internal dynamical evolution of SMC star clusters\nwould seem to be influenced by the SMC gravitational field, being star clusters\nlocated closer to the SMC centre in a more advanced evolutionary stage. We\ncompared the internal dynamical evolution of SMC old star clusters with those\nof LMC and Milky Way globular clusters, and found that Milky Way globular\nclusters have dynamical evolutionary paths similar to LMC\/SMC old star clusters\nlocated closer to their respective galaxy's centres. Finally, we speculate with\nthe possibility that globular clusters belonging to Magellanic Clouds like-mass\ngalaxies have lived a couple of times their median relaxation times.",
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.11084",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "BarcodeBERT: Transformers for Biodiversity Analysis"
    ],
    "b_abstract":[
      "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01759",
    "c_title":"Spinning generalizations of Majumdar-Papapetrou multi-black hole\n  spacetimes: light rings, lensing and shadows",
    "c_abstract":"A generalization of the Majumdar-Papapetrou multi-black hole spacetime was\nrecently constructed by Teo and Wan [1], describing charged and spinning\n(extremal) balanced black holes in asymptotically flat spacetime. We explore\nthe dynamics of null geodesics on this geometry, focusing on the two-center\nsolution. Using the topological charge formalism, we show that various light\nring arrangements arise from different choices of individual angular momenta:\nlight rings with opposite topological charges can merge and annihilate each\nother, resulting in configurations with a total of 4, 6, or 8 light rings.\nUsing backward ray-tracing, we obtained the shadow and lensing of these\nspacetimes. The former, in particular, closely resembles those for the\ndouble-Kerr metric.",
    "c_categories":[
      "gr-qc"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"Beyond Monte Carlo: Harnessing Diffusion Models to Simulate Financial\n  Market Dynamics",
    "a_abstract":"We propose a highly efficient and accurate methodology for generating\nsynthetic financial market data using a diffusion model approach. The synthetic\ndata produced by our methodology align closely with observed market data in\nseveral key aspects: (i) they pass the two-sample Cramer - von Mises test for\nportfolios of assets, and (ii) Q - Q plots demonstrate consistency across\nquantiles, including in the tails, between observed and generated market data.\nMoreover, the covariance matrices derived from a large set of synthetic market\ndata exhibit significantly lower condition numbers compared to the estimated\ncovariance matrices of the observed data. This property makes them suitable for\nuse as regularized versions of the latter. For model training, we develop an\nefficient and fast algorithm based on numerical integration rather than Monte\nCarlo simulations. The methodology is tested on a large set of equity data.",
    "explanation":"The paper proposes the use of a new method using diffusion model generative methodology to produce synthetic market scenarios.",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":[
      "b24"
    ],
    "c_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "c_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.12054",
    "c_title":"PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "c_abstract":"Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:\/dxzxy12138.github.io\/PhysReason.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.09054",
    "c_title":"Cost-Saving LLM Cascades with Early Abstention",
    "c_abstract":"LLM cascades are based on the idea that processing all queries with the\nlargest and most expensive LLMs is inefficient. Instead, cascades deploy small\nLLMs to answer the majority of queries, limiting the use of large and expensive\nLLMs to only the most difficult queries. This approach can significantly reduce\ncosts without impacting performance. However, risk-sensitive domains such as\nfinance or medicine place an additional premium on avoiding model errors.\nRecognizing that even the most expensive models may make mistakes, applications\nin these domains benefit from allowing LLM systems to completely abstain from\nanswering a query when the chance of making a mistake is significant. However,\ngiving a cascade the ability to abstain poses an immediate design question for\nLLM cascades: should abstention only be allowed at the final model or also at\nearlier models? Since the error patterns of small and large models are\ncorrelated, the latter strategy may further reduce inference costs by letting\ninexpensive models anticipate abstention decisions by expensive models, thereby\nobviating the need to run the expensive models. We investigate the benefits of\n\"early abstention\" in LLM cascades and find that it reduces the overall test\nloss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA,\nTruthfulQA, and XSum). These gains result from a more effective use of\nabstention, which trades a 4.1% average increase in the overall abstention rate\nfor a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings\ndemonstrate that it is possible to leverage correlations between the error\npatterns of different language models to drive performance improvements for LLM\nsystems with abstention.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.13879",
    "c_title":"Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment",
    "c_abstract":"The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.11926",
    "c_title":"Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
    "c_abstract":"Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.17201",
    "c_title":"Smart Cubing for Graph Search: A Comparative Study",
    "c_abstract":"Parallel solving via cube-and-conquer is a key method for scaling SAT solvers\nto hard instances. While cube-and-conquer has proven successful for pure SAT\nproblems, notably the Pythagorean triples conjecture, its application to SAT\nsolvers extended with propagators presents unique challenges, as these\npropagators learn constraints dynamically during the search.\n  We study this problem using SAT Modulo Symmetries (SMS) as our primary test\ncase, where a symmetry-breaking propagator reduces the search space by learning\nconstraints that eliminate isomorphic graphs. Through extensive experimentation\ncomprising over 10,000 CPU hours, we systematically evaluate different\ncube-and-conquer variants on three well-studied combinatorial problems. Our\nmethodology combines prerun phases to collect learned constraints, various\ncubing strategies, and parameter tuning via algorithm configuration and\nLLM-generated design suggestions.\n  The comprehensive empirical evaluation provides new insights into effective\ncubing strategies for propagator-based SAT solving, with our best method\nachieving speedups of 2-3x from improved cubing and parameter tuning, providing\nan additional 1.5-2x improvement on harder instances.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.20365",
    "c_title":"Half-Metallic Fe\/MgO Superlattice: An Ideal Candidate for Magnetic\n  Tunnel Junction Electrodes",
    "c_abstract":"Magnetic Tunnel Junction (MTJ) based Spin-Transfer Torque Magnetic Random\nAccess Memory (STT-MRAM) is poised to replace embedded Flash for advanced\napplications such as automotive microcontroller units. To achieve deeper\ntechnological adoption, MTJ needs to exhibit three key features: low\nmagnetization (Ms), high perpendicular magnetic anisotropy (PMA) and high\ntunnel magnetoresistance (TMR). Here, we theoretically show that when Fe\/MgO\nmultilayers are inserted into the fixed and free layers of the MTJ, these three\nconditions are simultaneously met. As the number of Fe\/MgO multilayers in MTJ\nelectrodes is increased, we find that the electron transport evolves from\ndirect barrier tunneling of majority spin states to the resonant tunneling of\nminority spin states. Remarkably, the projected density of states (PDOS) of\nFe\/MgO superlattice at the MgO tunnel barrier exhibits half-metallicity near\nthe Fermi Energy, where the minority states exist while the majority states are\ngapped out, resulting in astronomically high TMR.",
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cond-mat.other",
      "physics.app-ph",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.17865",
    "c_title":"Understanding Inverse Reinforcement Learning under Overparameterization:\n  Non-Asymptotic Analysis and Global Optimality",
    "c_abstract":"The goal of the Inverse reinforcement learning (IRL) task is to identify the\nunderlying reward function and the corresponding optimal policy from a set of\nexpert demonstrations. While most IRL algorithms' theoretical guarantees rely\non a linear reward structure, we aim to extend the theoretical understanding of\nIRL to scenarios where the reward function is parameterized by neural networks.\nMeanwhile, conventional IRL algorithms usually adopt a nested structure,\nleading to computational inefficiency, especially in high-dimensional settings.\nTo address this problem, we propose the first two-timescale single-loop IRL\nalgorithm under neural network parameterized reward and provide a\nnon-asymptotic convergence analysis under overparameterization. Although prior\noptimality results for linear rewards do not apply, we show that our algorithm\ncan identify the globally optimal reward and policy under certain neural\nnetwork structures. This is the first IRL algorithm with a non-asymptotic\nconvergence guarantee that provably achieves global optimality in neural\nnetwork settings.",
    "c_categories":[
      "cs.LG",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2501.17618",
    "c_title":"Periodic Boundary Conditions for Bosonic Path Integral Molecular\n  Dynamics",
    "c_abstract":"We develop an algorithm for bosonic path integral molecular dynamics (PIMD)\nsimulations with periodic boundary conditions (PBC) that scales quadratically\nwith the number of particles. Path integral methods are a powerful tool to\nsimulate bosonic condensed phases, which exhibit fundamental physical phenomena\nsuch as Bose--Einstein condensation and superfluidity. Recently, we developed a\nquadratic scaling algorithm for bosonic PIMD, but employed an ad hoc treatment\nof PBC. Here we rigorously enforce PBC in bosonic PIMD. It requires summing\nover the spring energies of all periodic images in the partition function, and\na naive implementation scales exponentially with the system size. We present an\nalgorithm for bosonic PIMD simulations of periodic systems that scales only\nquadratically. We benchmark our implementation on the free Bose gas and a model\nsystem of cold atoms in optical lattices. We also study an approximate\ntreatment of PBC based on the minimum-image convention, and derive a numerical\ncriterion to determine when it is valid.",
    "c_categories":[
      "cond-mat.quant-gas",
      "cond-mat.stat-mech",
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2503.02244",
    "c_title":"Integrated Communication and Learned Recognizer with Customized RIS\n  Phases and Sensing Durations",
    "c_abstract":"Future wireless communication networks are expected to be smarter and more\naware of their surroundings, enabling a wide range of context-aware\napplications. Reconfigurable intelligent surfaces (RISs) are set to play a\ncritical role in supporting various sensing tasks, such as target recognition.\nHowever, current methods typically use RIS configurations optimized once and\napplied over fixed sensing durations, limiting their ability to adapt to\ndifferent targets and reducing sensing accuracy. To overcome these limitations,\nthis study proposes an advanced wireless communication system that multiplexes\ndownlink signals for environmental sensing and introduces an intelligent\nrecognizer powered by deep learning techniques. Specifically, we design a novel\nneural network based on the long short-term memory architecture and the\nphysical channel model. This network iteratively captures and fuses information\nfrom previous measurements, adaptively customizing RIS phases to gather the\nmost relevant information for the recognition task at subsequent moments. These\nconfigurations are dynamically adjusted according to scene, task, target, and\nquantization priors. Furthermore, the recognizer includes a decision-making\nmodule that dynamically allocates different sensing durations, determining\nwhether to continue or terminate the sensing process based on the collected\nmeasurements. This approach maximizes resource utilization efficiency.\nSimulation results demonstrate that the proposed method significantly\noutperforms state-of-the-art techniques while minimizing the impact on\ncommunication performance, even when sensing and communication occur\nsimultaneously. Part of the source code for this paper can be accessed at\nhttps:\/\/github.com\/kiwi1944\/CRISense.",
    "c_categories":[
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
    ],
    "b_abstract":[
      "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
    ],
    "b_categories":[
      "q-fin.GN"
    ],
    "b_fields":[
      "Economics and Quantitative Finance"
    ],
    "c_id":"2502.13607",
    "c_title":"Environmental Influences on Collaboration Network Evolution: A\n  Historical Analysis",
    "c_abstract":"We analysed two large collaboration networks -- the Microsoft Academic Graph\n(1800-2020) and Internet Movie Database (1900-2020) -- to quantify network\nresponses to major historical events. Our analysis revealed four properties of\nnetwork-environment interaction. First, historical events can influence network\nevolution, with effects persisting far longer than previously recognised; the\nacademic network showed 45\\% declines during World Wars and 90\\% growth during\nLa Belle Epoque. Second, node and edge processes exhibited different\nenvironmental sensitivities; while node addition\/removal tracked historical\nevents, edge formation maintained stable statistical properties even during\nmajor disruptions. Third, different collaboration networks showed distinct\nresponse patterns; academic networks displayed sharp disruptions and rapid\nrecoveries, while entertainment networks showed gradual changes and greater\nresilience. Fourth, both networks developed increasing resilience. Our results\nprovide new insights for modelling network evolution and managing collaborative\nsystems during periods of external disruption.",
    "c_categories":[
      "cs.SI",
      "physics.soc-ph"
    ],
    "c_fields":[
      "Physics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.08875",
    "c_title":"Utilizing Pre-trained and Large Language Models for 10-K Items\n  Segmentation",
    "c_abstract":"Extracting specific items from 10-K reports remains challenging due to\nvariations in document formats and item presentation. Traditional rule-based\nitem segmentation approaches often yield suboptimal results. This study\nintroduces two advanced item segmentation methods leveraging language models:\n(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize\nGPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings\nwith a Bi-LSTM model in a hierarchical structure to overcome context window\nconstraints. Trained and evaluated on 3,737 annotated 10-K reports,\nBERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),\nconditional random field (0.9818), and rule-based methods (0.9048) for core\nitems (1, 1A, 3, and 7). These approaches enhance item segmentation\nperformance, improving text analytics in accounting and finance. BERT4ItemSeg\noffers satisfactory item segmentation performance, while GPT4ItemSeg can easily\nadapt to regulatory changes. Together, they offer practical benefits for\nresearchers and practitioners, enabling reliable empirical studies and\nautomated 10-K item segmentation functionality.",
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12010",
    "c_title":"The role of FDI along transitional dynamics of the host country in an\n  endogenous growth model",
    "c_abstract":"We investigate the role of foreign direct investment (FDI) in the\ntransitional dynamics of host countries by using an optimal growth model. FDI\nmay be beneficial for the host country because local people can work for\nmultinational firms to get a favorable salary. However, if the host country\nonly focuses on FDI, it may face a middle-income trap. We show that if the host\ncountry invests in research and development, its economy may have sustained\ngrowth. Moreover, in this case, FDI helps the host country only at the first\nstages of its development process.",
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15557",
    "c_title":"Preventing Household Bankruptcy: The One-Third Rule in Financial\n  Planning with Mathematical Validation and Game-Theoretic Insights",
    "c_abstract":"This paper analyzes the 1\/3 Financial Rule, a method of allocating income\nequally among debt repayment, savings, and living expenses. Through\nmathematical modeling, game theory, behavioral finance, and technological\nanalysis, we examine the rule's potential for supporting household financial\nstability and reducing bankruptcy risk. The research develops theoretical\nfoundations using utility maximization theory, demonstrating how equal\nallocation emerges as a solution under standard economic assumptions. The\ngame-theoretic analysis explores the rule's effectiveness across different\nhousehold structures, revealing potential strategic advantages in financial\ndecision-making. We investigate psychological factors influencing financial\nchoices, including cognitive biases and neurobiological mechanisms that impact\neconomic behavior. Technological approaches, such as AI-driven personalization,\nblockchain tracking, and smart contract applications, are examined for their\npotential to support financial planning. Empirical validation using U.S. Census\ndata and longitudinal studies assesses the rule's performance across various\nhousehold types. Stress testing under different economic conditions provides\ninsights into its adaptability and resilience. The research integrates\nmathematical analysis with behavioral insights and technological perspectives\nto develop a comprehensive approach to household financial management.",
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10644",
    "c_title":"Combined climate stress testing of supply-chain networks and the\n  financial system with nation-wide firm-level emission estimates",
    "c_abstract":"On the way towards carbon neutrality, climate stress testing provides\nestimates for the physical and transition risks that climate change poses to\nthe economy and the financial system. Missing firm-level CO2 emissions data\nseverely impedes the assessment of transition risks originating from carbon\npricing. Based on the individual emissions of all Hungarian firms (410,523), as\nestimated from their fossil fuel purchases, we conduct a stress test of both\nactual and hypothetical carbon pricing policies. Using a simple 1:1 economic\nABM and introducing the new carbon-to-profit ratio, we identify firms that\nbecome unprofitable and default, and estimate the respective loan write-offs.\nWe find that 45% of all companies are directly exposed to carbon pricing. At a\nprice of 45 EUR\/t, direct economic losses of 1.3% of total sales and bank\nequity losses of 1.2% are expected. Secondary default cascades in supply chain\nnetworks could increase these losses by 300% to 4000%, depending on firms'\nability to substitute essential inputs. To reduce transition risks, firms\nshould reduce their dependence on essential inputs from supply chains with high\nCO2 exposure. We discuss the implications of different policy implementations\non these transition risks.",
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17778",
    "c_title":"Heterogeneity of household stock portfolios in a national market",
    "c_abstract":"We study the long term dynamics of the stock portfolios owned by single\nFinnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001\nand 2021. Using the Herfindahl-Hirschman index as a measure of concentration\nfor the composition of stock portfolios, we investigate the concentration of\nFinnish household portfolios both at the level of each individual household and\ntracking the time evolution of an aggregated Finnish household portfolio. We\nalso consider aggregated portfolios of two other macro categories of investors\none comprising Finnish institutional investors and the other comprising foreign\ninvestors. Different macro categories of investors present a different degree\nof concentration of aggregated stock portfolios with highest concentration\nobserved for foreign investors. For individual Finnish retail investors,\nportfolio concentration estimated by the Herfindahl-Hirschman index presents\nhigh values for more than half of the total number of retail investors. In\nspite of the observation that retail stock portfolios are often composed by\njust a few stocks, the concentration of the aggregated stock portfolio for\nFinnish retail investors has a portfolio concentration comparable with the one\nof Finnish institutional investors. Within retail investors, stock portfolios\nof women present a similar pattern of portfolios of men but with a systematic\nhigher level of concentration observed for women both at individual and at\naggregated level.",
    "c_categories":[
      "q-fin.GN"
    ],
    "c_fields":[
      "Economics and Quantitative Finance"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09386",
    "c_title":"Optimal control of fractional Poisson equation from non-local to local",
    "c_abstract":"In this article, the limiting behavior of the solution $\\bar u_s$ of the\noptimal control problem subjected to the fractional Poisson equation\n$$(-\\Delta)^s u_s(x)=f_s(x), \\quad x\\in \\Omega$$ defined on domain $\\Omega$\nbounded by smooth boundary with zero exterior boundary conditions $u_s(x)\\equiv\n0, \\quad x \\in \\Omega^c $ is established. We will prove that $\\lim_{s\\to 1^-}\n\\bar u_s= \\bar u$, where $\\bar u$ is a solution of the optimal control problem\nsubjected to classical Poisson equation $-\\Delta u(x)=f(x), \\quad x \\in \\Omega$\nand $u(x)=0, \\quad x\\in \\partial \\Omega.$",
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16649",
    "c_title":"Attractors for Singular-Degenerate Porous Medium Type Equations Arising\n  in Models for Biofilm Growth",
    "c_abstract":"We investigate the long-time behaviour of solutions of a class of\nsingular-degenerate porous medium type equations in bounded Lipschitz domains\nwith mixed Dirichlet-Neumann boundary conditions. The existence of global\nattractors is shown under very general assumptions. Assuming, in addition, that\nsolutions are globally H\\\"older continuous and the reaction terms satisfy a\nsuitable sign condition in the vicinity of the degeneracy, we also prove the\nexistence of an exponential attractor, which, in turn, yields the finite\nfractal dimension of the global attractor. Moreover, we extend the results for\nscalar equations to systems where the degenerate equation is coupled to a\nsemilinear reaction-diffusion equation. The study of such systems is motivated\nby models for biofilm growth.",
    "c_categories":[
      "math.AP",
      "math.DS"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.21176",
    "c_title":"A non-loxodromic Morse element in a Morse local-to-global group",
    "c_abstract":"We use small-cancellation techniques to construct a Morse local-to-global\ngroup G with an infinite-order Morse element that is not loxodromic in any\naction of G on a hyperbolic space. In particular, the element cannot be WPD.",
    "c_categories":[
      "math.GR",
      "math.GT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.08078",
    "c_title":"The chemisorption thermodynamics of O$_2$ and H$_2$O on AFM UO$_2$\n  surfaces unraveled by DFT+U-D3 study",
    "c_abstract":"Unraveling the adsorption mechanism and thermodynamics of O$_2$ and H$_2$O on\nuranium dioxide surfaces is critical for the nuclear fuel storage and uranium\ncorrosion. Based on the first-principles DFT+U-D3 calculations, we carefully\ntest the effect of antiferromagnetic order arrangements on the thermodynamic\nstability of UO$_2$ surfaces and propose the 1k AFM surface computational\nmodel. The chemisorption states of O$_2$ and H$_2$O on UO$_2$ (111) surface,\nsuggested by previous experiments, are accurately calculated for the first\ntime. The adsorption properties of O$_2$ and H$_2$O on UO$_2$(111) and (110)\nsurfaces are discussed in detail to reveal the different interaction\nmechanisms. Combined with ab initio atomistic thermodynamics method, we\nsystematically calculate the chemisorption phase diagram and isotherm of O$_2$\nand H$_2$O on UO$_2$ surfaces. Due to the different intermolecular\ninteractions, the monolayer and multilayer adsorption models are identified for\nO$_2$ and H$_2$O, respectively. This study has comprehensively revealed the\ndifferent adsorption mechanisms of O$_2$ and H$_2$O on UO$_2$ surfaces,\nbridging the electronic structure calculations to the interpretation of\nexperimental results and providing a solid foundation for future theoretical\nstudies of uranium corrosion mechanism in humid air.",
    "c_categories":[
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2412.00036",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Quant GANs: deep generation of financial time series"
    ],
    "b_abstract":[
      "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03004",
    "c_title":"Large $N$ Vertex Algebras via Deligne Category",
    "c_abstract":"In this paper, we propose a new construction of vertex algebras using the\nDeligne category. This approach provides a rigorous framework for defining the\nso-called large $N$ vertex algebra, which has appeared in recent physics\nliteratures. We first define the notion of a vertex algebra in a symmetric\nmonoidal category and extend familiar constructions in ordinary vertex algebras\nto this broader categorical context. As an application, we consider a\n$\\beta\\gamma$ vertex algebra in the Deligne category and construct the large N\nvertex algebra from it. We study some simple properties of this vertex algebra\nand analyze a certain vertex Poisson algebra limit.",
    "c_categories":[
      "hep-th",
      "math-ph",
      "math.MP",
      "math.QA"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"Adding Error Bars to Evals: A Statistical Approach to Language Model\n  Evaluations",
    "a_abstract":"Evaluations are critical for understanding the capabilities of large language\nmodels (LLMs). Fundamentally, evaluations are experiments; but the literature\non evaluations has largely ignored the literature from other sciences on\nexperiment analysis and planning. This article shows researchers with some\ntraining in statistics how to think about and analyze data from language model\nevaluations. Conceptualizing evaluation questions as having been drawn from an\nunseen super-population, we present formulas for analyzing evaluation data,\nmeasuring differences between two models, and planning an evaluation\nexperiment. We make a number of specific recommendations for running language\nmodel evaluations and reporting experiment results in a way that minimizes\nstatistical noise and maximizes informativeness.",
    "explanation":"This is an interdisciplinary work because it combines two different subjects: LLM and statistics. This suggests that we could also consider the error bars in the evaluations. ",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":[
      "b6"
    ],
    "c_title":[
      "The Llama 3 Herd of Models"
    ],
    "c_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.07919",
    "c_title":"Large Language Model Interface for Home Energy Management Systems",
    "c_abstract":"Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and\/or few-shot prompting.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.02976",
    "c_title":"Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
    "c_abstract":"Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.13388",
    "c_title":"Reflection of Episodes: Learning to Play Game from Expert and Self\n  Experiences",
    "c_abstract":"StarCraft II is a complex and dynamic real-time strategy (RTS) game\nenvironment, which is very suitable for artificial intelligence and\nreinforcement learning research. To address the problem of Large Language\nModel(LLM) learning in complex environments through self-reflection, we propose\na Reflection of Episodes(ROE) framework based on expert experience and\nself-experience. This framework first obtains key information in the game\nthrough a keyframe selection method, then makes decisions based on expert\nexperience and self-experience. After a game is completed, it reflects on the\nprevious experience to obtain new self-experience. Finally, in the experiment,\nour method beat the robot under the Very Hard difficulty in TextStarCraft II.\nWe analyze the data of the LLM in the process of the game in detail, verified\nits effectiveness.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.13389",
    "c_title":"Reasoning with Reinforced Functional Token Tuning",
    "c_abstract":"In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel\nreinforced fine-tuning framework that empowers Large Language Models (LLMs)\nwith self-play learn-to-reason capabilities. Unlike prior prompt-driven\nreasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g.,\n<analyze>, <verify>, <refine>) directly into the model vocabulary, enabling\nchain-of-thought construction with diverse human-like reasoning behaviors.\nSpecifically, RFTT comprises two phases: (1) supervised fine-tuning performs\nprompt-driven tree search to obtain self-generated training data annotated with\nfunctional tokens, which warms up the model to learn these tokens for\nreasoning; and (2) online reinforcement learning further allows the model to\nexplore different reasoning pathways through functional token sampling without\nrelying on prompts, thereby facilitating effective self-improvement for\nfunctional reasoning. Extensive experiments demonstrate the superiority of the\nproposed RFTT on mathematical benchmarks, significantly boosting\nQwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to\n60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently\nimproves with more search rollouts at inference time. Our code is available at\nhttps:\/\/github.com\/sastpg\/RFTT.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2503.04291",
    "c_title":"MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs",
    "c_abstract":"We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.01092",
    "c_title":"Machine Learning-Driven Insights into Excitonic Effects in 2D Materials",
    "c_abstract":"Understanding excitonic effects in two-dimensional (2D) materials is critical\nfor advancing their potential in next-generation electronic and photonic\ndevices. In this study, we introduce a machine learning (ML)-based framework to\npredict exciton binding energies in 2D materials, offering a computationally\nefficient alternative to traditional methods such as many-body perturbation\ntheory (GW) and the Bethe-Salpeter equation. Leveraging data from the\nComputational 2D Materials Database (C2DB), our ML models establish connections\nbetween cheaply available material descriptors and complex excitonic\nproperties, significantly accelerating the screening process for materials with\npronounced excitonic effects. Additionally, Bayesian optimization with Gaussian\nprocess regression was employed to efficiently filter materials with largest\nexciton binding energies, further enhancing the discovery process. Although\ndeveloped for 2D systems, this approach is versatile and can be extended to\nthree-dimensional materials, broadening its applicability in materials\ndiscovery.",
    "c_categories":[
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2501.03756",
    "c_title":"Conditions for radiative zones in the molecular hydrogen envelope of\n  Jupiter and Saturn: The role of alkali metals",
    "c_abstract":"Interior models of gas giants in the Solar System traditionally assume a\nfully convective molecular hydrogen envelope. However, recent observations from\nthe Juno mission suggest a possible depletion of alkali metals in Jupiter's\nmolecular hydrogen envelope, indicating that a stable radiative layer could\nexist at the kilobar level. Recent studies propose that deep stable layers help\nreconcile various Jupiter observations, including its atmospheric water and CO\nabundances and the depth of its zonal winds. However, opacity tables used to\ninfer stable layers are often outdated and incomplete, leaving the precise\nmolecular hydrogen envelope composition required for a deep radiative zone\nuncertain. In this paper, we determine atmospheric compositions that can lead\nto the formation of a radiative zone at the kilobar level in Jupiter and Saturn\ntoday. We computed radiative opacity tables covering pressures up to $10^5$\nbar, including the most abundant molecules present in the gas giants of the\nSolar System, as well as contributions from free electrons, metal hydrides,\noxides, and atomic species, using the most up-to-date line lists published in\nthe literature. These tables were used to calculate Rosseland-mean opacities\nfor the molecular hydrogen envelopes of Jupiter and Saturn, which were then\ncompared to the critical mean opacity required to maintain convection. We find\nthat the presence of a radiative zone is controlled by the existence of K, Na,\nand NaH in the atmosphere of Jupiter and Saturn. For Jupiter, the elemental\nabundance of K and Na must be less than $\\sim 10^{-3}$ times solar to form a\nradiative zone. In contrast, for Saturn, the required abundance for K and Na is\nbelow $\\sim 10^{-4}$ times solar.",
    "c_categories":[
      "astro-ph.EP"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.10604",
    "c_title":"Shape Changes of Liquid Crystal Elastomers Swollen by Low Molecular\n  Weight Liquid Crystal Drops",
    "c_abstract":"An elastomer swelling actuator deforms by absorbing a fluid, thus generating\nmechanical movement. We show that depositing small droplets of low molecular\nweight liquid crystal on liquid crystal elastomer (LCE) films leads to shape\nchanges and bending actuation. It is found that the radially symmetric LCE\ndirector alignments provide radially symmetric hat shapes, while swelling LCEs\nwith uniform director structure leads to arch shapes. Hybrid samples (different\ndirector alignments on two sides) lead to more complicated bent shapes. All the\nobserved shapes can be explained by the diffusion that mainly progresses along\nthe direction normal to the director of the LCE. The swelling induced bending\nforce is elevating the top of the swollen LCE up to a factor of 30, providing a\npowerful and long-lasting actuation. These observations may lead to\napplications in various fields, like sealants, soft robotics and biomedical\ndevices.",
    "c_categories":[
      "cond-mat.soft"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.13123",
    "c_title":"No Evidence of Asymmetrically Enhanced Star Formation in Infalling\n  Galaxies in UNIONS",
    "c_abstract":"Ram pressure stripping is a well-known environmental quenching mechanism that\nremoves gas from galaxies infalling into groups and clusters. In some extreme\nexamples of ram pressure stripping, galaxies with extended gas tails show\nevidence of enhanced star formation prior to quenching. In this work we use a\nsample of 5277 local satellite galaxies in which a stripped tail of gas has not\nnecessarily been observed, to quantify the strength of ram pressure-enhanced\nstar formation and compare these results to a control sample of 8360 field\ngalaxies. We use u-band imaging from the Ultraviolet-Near Infrared Northern\nSurvey (UNIONS) as a star formation tracer and several metrics to quantify star\nformation asymmetry. We compare these results to environmental properties of\nthe galaxy, such as their time since infall and host halo mass, to constrain\nthe degree of ram pressure enhanced star formation as a function of\nenvironment. We find no significant differences between the satellite and the\nfield samples. We further restrict our sample to galaxies which we most expect\nto be experiencing significant ram pressure but find no strong evidence of\nthese galaxies having systematically enhanced star formation. Finally, we\ninvestigate the properties of the most asymmetric galaxies in our sample and\nagain find no strong evidence of ram pressure-induced star formation\nenhancement. We conclude that any star formation enhancement must be small for\ninfalling galaxies, suggesting that this effect is either uncommon or\nshort-lived.",
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b14"
    ],
    "b_title":[
      "Quantifying Variance in Evaluation Benchmarks"
    ],
    "b_abstract":[
      "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
    ],
    "b_categories":[
      "stat.ME"
    ],
    "b_fields":[
      "Mathematics and Statistics"
    ],
    "c_id":"2502.17585",
    "c_title":"Synergizing Deep Learning and Full-Waveform Inversion: Bridging\n  Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging",
    "c_abstract":"This review explores the integration of deep learning (DL) with full-waveform\ninversion (FWI) for enhanced seismic imaging and subsurface characterization.\nIt covers FWI and DL fundamentals, geophysical applications (velocity\nestimation, deconvolution, tomography), and challenges (model complexity, data\nquality). The review also outlines future research directions, including\nhybrid, generative, and physics-informed models for improved accuracy,\nefficiency, and reliability in subsurface property estimation. The synergy\nbetween DL and FWI has the potential to transform geophysics, providing new\ninsights into Earth's subsurface.",
    "c_categories":[
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "physics.geo-ph"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.04093",
    "c_title":"Evaluating and Testing for Actionable Treatment Effect Heterogeneity",
    "c_abstract":"Developing tools for estimating heterogeneous treatment effects (HTE) and\nindividualized treatment effects has been an area of active research in recent\nyears. While these tools have proven to be useful in many contexts, a concern\nwhen deploying such methods is the degree to which incorporating HTE into a\nprediction model provides an advantage over predictive methods which do not\nallow for variation in treatment effect across individuals. To address this\nconcern, we propose a procedure which evaluates the extent to which an HTE\nmodel provides a predictive advantage. Specifically, our procedure targets the\ngain in predictive performance from using a flexible predictive model\nincorporating HTE versus an alternative model which is similar to the\nHTE-utilizing model except that it is constrained to not allow variation in\ntreatment effect. By drawing upon recent work in using nested cross-validation\ntechniques for prediction error inference, we generate confidence intervals for\nthis measure of gain in predictive performance which allows one to directly\ncalculate the level at which one is confident of a substantial HTE-modeling\ngain in prediction -- a quantity which we refer to as the h-value. Our\nprocedure is generic and can be directly used to assess the benefit of modeling\nHTE for any method that incorporates treatment effect variation.",
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10266",
    "c_title":"Comparative analysis and practical applications of cubic transmutations\n  for the Pareto distribution",
    "c_abstract":"Transmutation is a technique for extending classical probability\ndistributions in order to give them more flexibility. In this paper, we are\ninterested in cubic transmutations of the Pareto distribution. We establish a\ngeneral formula that unifies existing cubic transmutations of the Pareto\ndistribution and facilitates the derivation of new cubic transmutations that\nhave not yet been explored in the literature. We also derive general formulas\nfor the related mathematical properties. Finally, we perform a comparative\nanalysis of the six transmutations existing in the literature using real-world\ndata. The results obtained confirm the flexibility and effectiveness of cubic\ntransmutations in modeling various types of data.",
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07790",
    "c_title":"Sampling from Density power divergence-based Generalized posterior\n  distribution via Stochastic optimization",
    "c_abstract":"Robust Bayesian inference using density power divergence (DPD) has emerged as\na promising approach for handling outliers in statistical estimation. While the\nDPD-based posterior offers theoretical guarantees for robustness, its practical\nimplementation faces significant computational challenges, particularly for\ngeneral parametric models with intractable integral terms. These challenges\nbecome especially pronounced in high-dimensional settings where traditional\nnumerical integration methods prove inadequate and computationally expensive.\nWe propose a novel sampling methodology that addresses these limitations by\nintegrating the loss-likelihood bootstrap with a stochastic gradient descent\nalgorithm specifically designed for DPD-based estimation. Our approach enables\nefficient and scalable sampling from DPD-based posteriors for a broad class of\nparametric models, including those with intractable integrals, and we further\nextend it to accommodate generalized linear models. Through comprehensive\nsimulation studies, we demonstrate that our method efficiently samples from\nDPD-based posteriors, offering superior computational scalability compared to\nconventional methods, particularly in high-dimensional settings. The results\nalso highlight its ability to handle complex parametric models with intractable\nintegral terms.",
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08093",
    "c_title":"A note on local parameter orthogonality for multivariate data and the\n  Whittle algorithm for multivariate autoregressive models",
    "c_abstract":"This article extends the Cox--Reid local parameter orthogonality to a\nmultivariate setting, gives an affirmative reply to one of Cox and Reid's\nquestions, and shows that the extension can lead to efficient computational\nalgorithms with the celebrated Whittle algorithm for multivariate\nautoregressive modeling as a showcase.",
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16261",
    "c_title":"A step-by-step guide to generalized estimating equations using SPSS in\n  the field of dentistry",
    "c_abstract":"The Generalized Estimating Equations (GEE) approach is a widely used\nstatistical method for analyzing longitudinal data and clustered data in clinic\nstudies. In dentistry, due to multiple outcomes obtained from one patient, the\noutcomes produced from individual patients are correlated with one another.\nThis study focuses on the basic ideas of GEE and introduces the types of\ncovariance matrix and working correlation matrix. The quasi-likelihood\ninformation criterion(QIC) and quasi-likelihood information criterion\napproximation(QICu) were used to select the best working matrix and the best\nfitting model for the correlated outcomes.",
    "c_categories":[
      "stat.ME"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.00210",
    "c_title":"Semilinear wave equations on extremal Reissner-Nordstr\\\"om black holes\n  revisited",
    "c_abstract":"We revisit global existence and decay for small-data solutions of semilinear\nwave equations on extremal Reissner-Nordstr\\\"om black hole backgrounds\nsatisfying the classical null condition, a problem which was previously\naddressed by the first author in joint work with Aretakis and Gajic (Ann. of\nPDE, 2020). In this paper, we develop a new approach based on propagating a\nsignificantly weaker set of estimates, which allows for a simpler and more\nstreamlined proof. Our proof does not require tracking sharp estimates for the\nsolution in the near-horizon region, which means that it is compatible with,\nbut does not imply, the non-decay and growth hierarchy of derivatives of the\nsolution along the event horizon expected from the Aretakis instability. In\nparticular, this approach is in principle compatible with other settings where\nstronger horizon instabilities are expected, such as nonlinear charged scalar\nfields on extremal Reissner-Nordstr\\\"om, or nonlinear waves on extremal Kerr.\nWe also sketch how our proof applies to semilinear problems on spacetimes\nsettling down to extremal Reissner-Nordstr\\\"om, such as those constructed in\nour joint work with Kehle (arXiv:2410.16234, 2024).",
    "c_categories":[
      "gr-qc",
      "math-ph",
      "math.AP",
      "math.DG",
      "math.MP"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17546",
    "c_title":"Communities in the Kuramoto Model: Dynamics and Detection via Path\n  Signatures",
    "c_abstract":"The behavior of multivariate dynamical processes is often governed by\nunderlying structural connections that relate the components of the system. For\nexample, brain activity which is often measured via time series is determined\nby an underlying structural graph, where nodes represent neurons or brain\nregions and edges represent cortical connectivity. Existing methods for\ninferring structural connections from observed dynamics, such as\ncorrelation-based or spectral techniques, may fail to fully capture complex\nrelationships in high-dimensional time series in an interpretable way. Here, we\npropose the use of path signatures a mathematical framework that encodes\ngeometric and temporal properties of continuous paths to address this problem.\nPath signatures provide a reparametrization-invariant characterization of\ndynamical data and, in particular, can be used to compute the lead matrix which\nreveals lead-lag phenomena. We showcase our approach on time series from\ncoupled oscillators in the Kuramoto model defined on a stochastic block model\ngraph, termed the Kuramoto stochastic block model (KSBM). Using mean-field\ntheory and Gaussian approximations, we analytically derive reduced models of\nKSBM dynamics in different temporal regimes and theoretically characterize the\nlead matrix in these settings. Leveraging these insights, we propose a novel\nsignature-based community detection algorithm, achieving exact recovery of\nstructural communities from observed time series in multiple KSBM instances.\nOur results demonstrate that path signatures provide a novel perspective on\nanalyzing complex neural data and other high-dimensional systems, explicitly\nexploiting temporal functional relationships to infer underlying structure.",
    "c_categories":[
      "cond-mat.dis-nn",
      "cs.LG",
      "nlin.AO",
      "q-bio.NC",
      "q-bio.QM",
      "stat.ML"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics",
      "Quantitative Biology",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05778",
    "c_title":"The Warm-Hot Disk-Halo Interface Below the Perseus Spiral Arm",
    "c_abstract":"The Milky Way's disk-halo interface mediates energy and mass exchange between\nthe interstellar thin disk and the halo. In the first detailed study of the\nPerseus arm's disk-halo interface, we combine HST\/STIS and COS absorption\nspectra toward 6 stars and 23 AGNs projected behind a narrow section (95 degree\n< l <145 degree, -46 degree < b <0 degree), providing a unique dataset that\nbridges the disk and its extended vertical structure in these directions. We\ndetect S II, Si IV, and C IV absorption, along with HI 21 cm emission, within\n-70 pc to -3.3 kpc from the mid-plane. The arm's southern vertical structure\nexhibits complexity beyond simple exponential scaling: HI and S II column\ndensities sharply decline with height up to 1.5 kpc before flattening, while\nhigh ion (Si IV and C IV) column densities remain relatively constant. In this\nregion, where warm neutral medium (WNM) dominates, S II and the high ions show\nsimilar kinematics, and we find a remarkably uniform CIV\/SiIV ratio (<C IV\/Si\nIV> = 2.5 pm 0.5) within -0.9 to -3.25 kpc. Both the kinematic correspondence\nand high-ion ratio are consistent with the high ions probing turbulent mixing\nlayers at the interfaces between warm\/cool and hot gas phases. AGN sightlines\nreveal minimal circumgalactic medium (CGM) contribution in the low-velocity gas\nat |v_{LSR}|< 100 km\/s, suggesting the observed properties may be attributed to\nprevious fountain activity.",
    "c_categories":[
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14602",
    "c_title":"Yang-Lee Zeros of 2D Nearest-Neighbor Antiferromagnetic Ising Models: A\n  Numerical Linked Cluster Expansion Study",
    "c_abstract":"We study Yang-Lee zeros in the thermodynamic limit of the 2D nearest-neighbor\nantiferromagnetic Ising model on square and triangular lattices. We employ the\nNumerical Linked Cluster Expansion (NLCE) equipped with Exact Enumeration (EE)\nof the partition function to estimate the Laplacian of the free energy, which\nis proportional to the zeros density. Using a modified NLCE, where the\nexpansion can be carried directly on the Yang-Lee zeros of the involved\nclusters, we estimate the density of Yang-Lee zeros in the thermodynamic limit.\nNLCE gives significantly more zeros than EE in the complex field plane\nproviding more insights on how the root curves look in the thermodynamic limit.\nFor the square lattice at $T \\ll T_c$, the results suggest that two vertical\nlines at $\\pm h_c(T)$ in the complex field plane (i.e two concentric circles in\nthe complex fugacity plane) are the thermodynamic root curves. A similar\npicture is expected for the triangular lattice for phase transitions at large\nvalues of magnetic field while further study is needed for phase transitions at\nsmaller values of magnetic field. The convergence of the NLCE and (EE)\ncalculations of the partition function to the thermodynamic limit is studied in\nboth lattices and the temperature-field phase diagram is obtained from Yang-Lee\nzeros using both methods. This NLCE-based approach will facilitate the study of\ndifferent types of phase transitions using Yang-Lee zeros in future research.",
    "c_categories":[
      "cond-mat.stat-mech",
      "math-ph",
      "math.MP",
      "physics.comp-ph"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00640",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b6"
    ],
    "b_title":[
      "The Llama 3 Herd of Models"
    ],
    "b_abstract":[
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08396",
    "c_title":"Study of 14.1 MeV Neutron Moderation in Beryllium",
    "c_abstract":"This study investigates the moderation of 14.1 MeV neutrons in a natural\nberyllium moderator arranged in a spherical geometry. The neutron interactions\nand moderation efficiency were analyzed using Monte Carlo simulations with the\nGEANT4 toolkit. Various sphere radii were tested to determine the optimal\nmoderator thickness for neutron thermalization.",
    "c_categories":[
      "nucl-ex",
      "nucl-th",
      "physics.comp-ph",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"Tumor Location-weighted MRI-Report Contrastive Learning: A Framework for\n  Improving the Explainability of Pediatric Brain Tumor Diagnosis",
    "a_abstract":"Despite the promising performance of convolutional neural networks (CNNs) in\nbrain tumor diagnosis from magnetic resonance imaging (MRI), their integration\ninto the clinical workflow has been limited. That is mainly due to the fact\nthat the features contributing to a model's prediction are unclear to\nradiologists and hence, clinically irrelevant, i.e., lack of explainability. As\nthe invaluable sources of radiologists' knowledge and expertise, radiology\nreports can be integrated with MRI in a contrastive learning (CL) framework,\nenabling learning from image-report associations, to improve CNN\nexplainability. In this work, we train a multimodal CL architecture on 3D brain\nMRI scans and radiology reports to learn informative MRI representations.\nFurthermore, we integrate tumor location, salient to several brain tumor\nanalysis tasks, into this framework to improve its generalizability. We then\napply the learnt image representations to improve explainability and\nperformance of genetic marker classification of pediatric Low-grade Glioma, the\nmost prevalent brain tumor in children, as a downstream task. Our results\nindicate a Dice score of 31.1% between the model's attention maps and manual\ntumor segmentation (as an explainability measure) with test classification\nperformance of 87.7%, significantly outperforming the baselines. These\nenhancements can build trust in our model among radiologists, facilitating its\nintegration into clinical practices for more efficient tumor diagnosis.",
    "explanation":"The article presents a research involving the use of Computer Science methods to treat problems in Medicine. In this case, applying the use of Convolutional Neural Networks (CNN) for the recognition of pediatric tumors in medical images and Contrastive Learning (CL) to improve the explainability of the model.",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b0"
    ],
    "c_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "c_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06617",
    "c_title":"Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D\n  Gaussian Modeling",
    "c_abstract":"Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution\n(HR) images from low-resolution (LR) inputs with arbitrary upsampling factors\nusing a single model, addressing the limitations of traditional SR methods\nconstrained to fixed-scale factors (\\textit{e.g.}, $\\times$ 2). Recent advances\nleveraging implicit neural representation (INR) have achieved great progress by\nmodeling coordinate-to-pixel mappings. However, the efficiency of these methods\nmay suffer from repeated upsampling and decoding, while their reconstruction\nfidelity and quality are constrained by the intrinsic representational\nlimitations of coordinate-based functions. To address these challenges, we\npropose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which\nexplicitly reconstructs 2D continuous HR signals from LR images using Gaussian\nSplatting. This approach eliminates the need for time-consuming upsampling and\ndecoding, enabling extremely fast arbitrary-scale super-resolution. Once the\nGaussian field is built in a single pass, ContinuousSR can perform\narbitrary-scale rendering in just 1ms per scale. Our method introduces several\nkey innovations. Through statistical ana",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.10403",
    "c_title":"Hyper3D: Efficient 3D Representation via Hybrid Triplane and Octree\n  Feature for Enhanced 3D Shape Variational Auto-Encoders",
    "c_abstract":"Recent 3D content generation pipelines often leverage Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations,\nfacilitating diffusion-based generation. Efficiently compressing 3D shapes\nwhile preserving intricate geometric details remains a key challenge. Existing\n3D shape VAEs often employ uniform point sampling and 1D\/2D latent\nrepresentations, such as vector sets or triplanes, leading to significant\ngeometric detail loss due to inadequate surface coverage and the absence of\nexplicit 3D representations in the latent space. Although recent work explores\n3D latent representations, their large scale hinders high-resolution encoding\nand efficient training. Given these challenges, we introduce Hyper3D, which\nenhances VAE reconstruction through efficient 3D representation that integrates\nhybrid triplane and octree features. First, we adopt an octree-based feature\nrepresentation to embed mesh information into the network, mitigating the\nlimitations of uniform point sampling in capturing geometric distributions\nalong the mesh surface. Furthermore, we propose a hybrid latent space\nrepresentation that integrates a high-resolution triplane with a low-resolution\n3D grid. This design not only compensates for the lack of explicit 3D\nrepresentations but also leverages a triplane to preserve high-resolution\ndetails. Experimental results demonstrate that Hyper3D outperforms traditional\nrepresentations by reconstructing 3D shapes with higher fidelity and finer\ndetails, making it well-suited for 3D generation pipelines.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.06976",
    "c_title":"Task-Specific Knowledge Distillation from the Vision Foundation Model\n  for Enhanced Medical Image Segmentation",
    "c_abstract":"Large-scale pre-trained models, such as Vision Foundation Models (VFMs), have\ndemonstrated impressive performance across various downstream tasks by\ntransferring generalized knowledge, especially when target data is limited.\nHowever, their high computational cost and the domain gap between natural and\nmedical images limit their practical application in medical segmentation tasks.\nMotivated by this, we pose the following important question: \"How can we\neffectively utilize the knowledge of large pre-trained VFMs to train a small,\ntask-specific model for medical image segmentation when training data is\nlimited?\" To address this problem, we propose a novel and generalizable\ntask-specific knowledge distillation framework. Our method fine-tunes the VFM\non the target segmentation task to capture task-specific features before\ndistilling the knowledge to smaller models, leveraging Low-Rank Adaptation\n(LoRA) to reduce the computational cost of fine-tuning. Additionally, we\nincorporate synthetic data generated by diffusion models to augment the\ntransfer set, enhancing model performance in data-limited scenarios.\nExperimental results across five medical image datasets demonstrate that our\nmethod consistently outperforms task-agnostic knowledge distillation and\nself-supervised pretraining approaches like MoCo v3 and Masked Autoencoders\n(MAE). For example, on the KidneyUS dataset, our method achieved a 28% higher\nDice score than task-agnostic KD using 80 labeled samples for fine-tuning. On\nthe CHAOS dataset, it achieved an 11% improvement over MAE with 100 labeled\nsamples. These results underscore the potential of task-specific knowledge\ndistillation to train accurate, efficient models for medical image segmentation\nin data-constrained settings.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.00591",
    "c_title":"AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware\n  Preference Alignment of Multi-modal Large Language Models",
    "c_abstract":"Visual layouts are essential in graphic design fields such as advertising,\nposters, and web interfaces. The application of generative models for\ncontent-aware layout generation has recently gained traction. However, these\nmodels fail to understand the contextual aesthetic requirements of layout\ndesign and do not align with human-like preferences, primarily treating it as a\nprediction task without considering the final rendered output. To overcome\nthese problems, we offer Aesthetic-Aware Preference Alignment(AAPA), a novel\ntechnique to train a Multi-modal Large Language Model (MLLM) for layout\nprediction that uses MLLM's aesthetic preferences for Direct Preference\nOptimization over graphic layouts. We propose a data filtering protocol\nutilizing our layout-quality heuristics for AAPA to ensure training happens on\nhigh-quality layouts. Additionally, we introduce a novel evaluation metric that\nuses another MLLM to compute the win rate of the generated layout against the\nground-truth layout based on aesthetics criteria. We also demonstrate the\napplicability of AAPA for MLLMs of varying scales (1B to 8B parameters) and LLM\nfamilies (Qwen, Phi, InternLM). By conducting thorough qualitative and\nquantitative analyses, we verify the efficacy of our approach on two\nchallenging benchmarks - Crello and Webui, showcasing 17%, and 16 improvement\nover current State-of-The-Art methods, thereby highlighting the potential of\nMLLMs in aesthetic-aware layout generation.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04207",
    "c_title":"Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior",
    "c_abstract":"Can our brain signals faithfully reflect the original visual stimuli, even\nincluding high-frequency details? Although human perceptual and cognitive\ncapacities enable us to process and remember visual information, these\nabilities are constrained by several factors, such as limited attentional\nresources and the finite capacity of visual memory. When visual stimuli are\nprocessed by human visual system into brain signals, some information is\ninevitably lost, leading to a discrepancy known as the \\textbf{System GAP}.\nAdditionally, perceptual and cognitive dynamics, along with technical noise in\nsignal acquisition, degrade the fidelity of brain signals relative to the\nvisual stimuli, known as the \\textbf{Random GAP}. When encoded brain\nrepresentations are directly aligned with the corresponding pretrained image\nfeatures, the System GAP and Random GAP between paired data challenge the\nmodel, requiring it to bridge these gaps. However, in the context of limited\npaired data, these gaps are difficult for the model to learn, leading to\noverfitting and poor generalization to new data. To address these GAPs, we\npropose a simple yet effective approach called the \\textbf{Uncertainty-aware\nBlur Prior (UBP)}. It estimates the uncertainty within the paired data,\nreflecting the mismatch between brain signals and visual stimuli. Based on this\nuncertainty, UBP dynamically blurs the high-frequency details of the original\nimages, reducing the impact of the mismatch and improving alignment. Our method\nachieves a top-1 accuracy of \\textbf{50.9\\%} and a top-5 accuracy of\n\\textbf{79.7\\%} on the zero-shot brain-to-image retrieval task, surpassing\nprevious state-of-the-art methods by margins of \\textbf{13.7\\%} and\n\\textbf{9.8\\%}, respectively. Code is available at\n\\href{https:\/\/github.com\/HaitaoWuTJU\/Uncertainty-aware-Blur-Prior}{GitHub}.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.12037",
    "c_title":"Information geometry of tempered stable processes",
    "c_abstract":"We find information geometry of tempered stable processes. Starting with the\nderivation of $\\alpha$-divergence between two tempered stable processes, Fisher\ninformation matrices of tempered stable processes and $\\alpha$-connections of\ntheir statistical manifolds are obtained. Additionally, we also provide\nstatistical applications for the information geometry of tempered stable\nprocesses. Various tempered stable processes such as generalized tempered\nstable processes, classical tempered stable processes, and rapidly decreasing\ntempered stable processes are given as examples.",
    "c_categories":[
      "cs.IT",
      "math.DG",
      "math.IT",
      "math.PR"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.21296",
    "c_title":"Oscillatory finite-time singularities in rockbursts",
    "c_abstract":"Forecasting violent rockbursts remains a formidable challenge due to\nsignificant uncertainties involved. One major uncertainty arises from the\nintermittency of rock failure processes, typically characterised by a series of\nprogressively shorter quiescent phases punctuated by sudden accelerations,\nrather than a smooth continuous progression towards the final breakdown. This\nnon-monotonic evolution of rock mass deformation complicates rockburst\nprediction, challenging conventional time-to-failure models that often assume a\nsmooth power law accelerating behaviour. Here, we introduce a generalised\ntime-to-failure model called log-periodic power law singularity (LPPLS) model\nto effectively capture the intermittent dynamics of damage and rupture\nprocesses in rock leading up to violent rockbursts. We perform parametric and\nnonparametric tests on 11 historical rockburst events at three underground\nmines, documenting empirical evidence and providing theoretical arguments to\ndemonstrate the significance of log-periodic oscillatory power law finite-time\nsingularities. Log-periodicity in these rockburst events is likely driven by\nthe interaction of subparallel propagating cracks, the diffusion of\nstress-triggering processes, or the interplay between stress drop and stress\ncorrosion. Our results and insights obtained have significant implications for\nnot only understanding but also forecasting rockbursts, as recognising and\ncharacterising log-periodicity can help transform intermittency from\ntraditionally perceived noise into valuable predictive information.",
    "c_categories":[
      "physics.geo-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.05717",
    "c_title":"A computational model for crack-tip fields in a 3-D porous elastic solid\n  with material moduli dependent on density",
    "c_abstract":"A mathematical model for crack-tip fields is proposed in this paper for the\nresponse of a three-dimensional (3-D) porous elastic solid whose material\nmoduli are dependent on the density. Such a description wherein the generalized\nLam\\`e coefficients are nonlinear functions of material stiffness is more\nrealistic because most engineering materials are porous, and their material\nproperties depend on porosity and density. The governing boundary value problem\nfor the static equilibrium state in a 3-D, homogeneous, isotropic material is\nobtained as a second-order, quasilinear partial-differential-equation system\nwith a classical traction-free crack-surface boundary condition. The numerical\nsolution is obtained from a continuous trilinear Galerkin-type finite element\ndiscretization. A Picard-type linearization is utilized to handle the\nnonlinearities in the discrete problem. The proposed model can describe the\nstate of stress and strain in various materials, including recovering the\nclassical singularities in the linearized model. The role of \\textit{tensile\nstress}, \\textit{stress intensity factor} (SIF), and \\textit{strain energy\ndensity} are examined. The results indicate that the maximum values of all\nthese quantities occur directly before the crack-tip, consistent with the\nobservation made in the canonical problem for the linearized elastic fracture\nmechanics. One can use the same classical local fracture criterion, like the\nmaximum of SIF, to study crack tips' quasi-static and dynamic evolution within\nthe framework described in this article.",
    "c_categories":[
      "cs.NA",
      "math.NA"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19561",
    "c_title":"Quantum Level-Crossing Induced by Anisotropy in Spin-1 Heisenberg\n  Dimers: Applications to Quantum Stirling Engines",
    "c_abstract":"This work explores the thermodynamic performance of a quantum Stirling heat\nengine implemented with an anisotropic spin-1 Heisenberg dimer as the working\nmedium. Using the Hamiltonian of the system, we analyze the interplay of\nanisotropy, magnetic field, and exchange interactions and their influence on\nthe energy spectrum and the quantum level crossing. Our results reveal that\ndouble-degenerate point (DDP) and a triple-degenerate point (TDP) play pivotal\nroles in shaping the operational regimes and efficiency of the quantum Stirling\nengine. At those points, the Carnot efficiency reaches higher work output and\nenhanced stability, making it a robust candidate for optimal thermodynamic\nperformance. These findings highlight the potential of anisotropic spin systems\nas viable platforms for quantum heat engines and contribute to advancing the\nfield of quantum thermodynamics.",
    "c_categories":[
      "cond-mat.stat-mech"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b9"
    ],
    "b_title":[
      "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
    ],
    "b_abstract":[
      "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
    ],
    "b_categories":[
      "q-bio.NC"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09256",
    "c_title":"Block Designs and K-Geodetic Graphs: A Survey",
    "c_abstract":"K-geodetic graphs (K capital) are defined as graphs in which each pair of\nnonadjacent vertices has at most K paths of minimum length between them. A\nK-geodetic graph is geodetic if K=1, bigeodetic if K=2 and trigeodetic if K=3.\nK-geodetic graphs are applied effectively to the solution of several practical\nproblems in distinct areas of computer science, hence the importance of their\nstudy. Four problems are central to the study of K-geodetic graphs, namely,\ncharacterization, construction, enumeration and classification. The problems of\nfinding the general classification of K-geodetic graphs for each of their\nclasses K=1,2,3 are open. The present paper is a survey dedicated to the\nconstruction of K-geodetic graphs for K=1,2,3 using balanced incomplete block\ndesigns (BIBDs). To this purpose, we use block designs as combinatorial\nstructures defined in terms of completely predetermined parameters, which is\nessential for the easy construction of the K-geodetic graphs described in this\nsurvey.",
    "c_categories":[
      "cs.DM",
      "math.CO"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13628",
    "c_title":"Language modulates vision: Evidence from neural networks and human\n  brain-lesion models",
    "c_abstract":"Comparing information structures in between deep neural networks (DNNs) and\nthe human brain has become a key method for exploring their similarities and\ndifferences. Recent research has shown better alignment of vision-language DNN\nmodels, such as CLIP, with the activity of the human ventral occipitotemporal\ncortex (VOTC) than earlier vision models, supporting the idea that language\nmodulates human visual perception. However, interpreting the results from such\ncomparisons is inherently limited due to the \"black box\" nature of DNNs. To\naddress this, we combined model-brain fitness analyses with human brain lesion\ndata to examine how disrupting the communication pathway between the visual and\nlanguage systems causally affects the ability of vision-language DNNs to\nexplain the activity of the VOTC. Across four diverse datasets, CLIP\nconsistently outperformed both label-supervised (ResNet) and unsupervised\n(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,\naligning with the human language network. Analyses of the data of 33 stroke\npatients revealed that reduced white matter integrity between the VOTC and the\nlanguage region in the left angular gyrus was correlated with decreased CLIP\nperformance and increased MoCo performance, indicating a dynamic influence of\nlanguage processing on the activity of the VOTC. These findings support the\nintegration of language modulation in neurocognitive models of human vision,\nreinforcing concepts from vision-language DNN models. The sensitivity of\nmodel-brain similarity to specific brain lesions demonstrates that leveraging\nmanipulation of the human brain is a promising framework for evaluating and\ndeveloping brain-like computer models.",
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.00869",
    "c_title":"Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
    "c_abstract":"By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.02402",
    "c_title":"Asynchronous Hebbian\/anti-Hebbian networks",
    "c_abstract":"Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.06374",
    "c_title":"On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
    "c_abstract":"One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.16557",
    "c_title":"Predictability of temporal network dynamics in normal ageing and brain\n  pathology",
    "c_abstract":"Spontaneous brain activity generically displays transient spatiotemporal\ncoherent structures, which can selectively be affected in various neurological\nand psychiatric pathologies. Here we model the full brain's\nelectroencephalographic activity as a high-dimensional functional network\nperforming a trajectory in a latent graph phase space. This approach allows us\nto investigate the orbital stability of brain's activity and in particular its\nshort-term predictability. We do this by constructing a non-parametric\nstatistic quantifying the expansion of initially close functional network\ntrajectories. We apply the method to cohorts of healthy ageing individuals, and\npatients previously diagnosed with Parkinson's or Alzheimer's disease. Results\nnot only characterise brain dynamics from a new angle, but further show that\nfunctional network predictability varies in a marked scale-dependent way across\nhealthy controls and patient groups. The path towards both pathologies is\nmarkedly different. Furthermore, healthy ageing's predictability appears to\nstrongly differ from that of Parkinson's disease, but much less from that of\npatients with Alzheimer's disease.",
    "c_categories":[
      "q-bio.NC"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03840",
    "c_title":"Beyond uniqueness: Relaxation calculus of junction conditions for\n  coercive Hamilton-Jacobi equations",
    "c_abstract":"A junction is a particular network given by the collection of $N\\ge 1$ half\nlines $[0,+\\infty)$ glued together at the origin. On such a junction, we\nconsider evolutive Hamilton-Jacobi equations with $N$ coercive Hamiltonians.\nFurthermore,we consider a general desired junction condition at the origin,\ngiven by some monotone function $F_0:\\R^N\\to \\R$.There is existence and\nuniqueness of solutions which only satisfy weakly the junction condition (at\nthe origin, they satisfy either the desired junction condition or the PDE).We\nshow that those solutions satisfy strongly a relaxed junction condition $\\frak\nR F_0$ (that we can recognize as an effective junction condition). It is\nremarkable that this relaxed condition can be computed in three different but\nequivalent ways: 1) using viscosity inequalities, 2) using Godunov fluxes, 3)\nusing Riemann problems.Our result goes beyond uniqueness theory, in the\nfollowing sense: solutions to two different desired junction conditions $F_0$\nand $F_1$ do coincide if $\\frak R F_0=\\frak R F_1$.",
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17616",
    "c_title":"Asymptotics of $L^r$ extremal polynomials for ${0<r\\leq\\infty}$ on\n  $C^{1+}$ Jordan regions",
    "c_abstract":"We study strong asymptotics of $L^r$-extremal polynomials for measures\nsupported on Jordan regions with $C^{1+}$ boundary for $0<r<\\infty$. Using the\nresults for $r=2$, we derive asymptotics of weighted Chebyshev and residual\npolynomials for upper-semicontinuous weights supported on a $C^{1+}$ Jordan\nregion corresponding to $r=\\infty$. As an application, we show how strong\nasymptotics for extremal polynomials in the Ahlfors problem on a $C^{1+}$\nJordan region can be obtained from that for the weighted residual polynomials.\nBased on the results we pose a conjecture for asymptotics of weighted Chebyshev\nand residual polynomials for a $C^{1+}$ arc.",
    "c_categories":[
      "math.CA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11606",
    "c_title":"Black Holes Rule Out Heavy Tachyons",
    "c_abstract":"We present direct observational constraints on tachyons; particles with group\nvelocity greater than $c$ in vacuum in a Lorentz invariant theory. Since\ntachyons may have no direct couplings to Standard Model particles, the most\nrobust and model independent constraints come from gravitational effects,\nespecially black holes. We compute the Hawking radiation of tachyons from black\nholes, finding it to be significantly enhanced in the presence of heavy\ntachyons. For a black hole of mass $M$ and tachyons of mass $m$ with $g$\ndegrees of freedom, the black hole lifetime is found to be $t_{bh} \\approx 192\n\\pi \\hbar M\/(g c^2 m^2)$ (or doubled for fermions). This implies that the\nobservation of black holes of a few solar masses, with lifetime of several\nbillion years, rules out tachyons of mass $m > 3 \\times 10^9$ GeV. This means\nthere cannot exist any tachyons associated with unification scales or quantum\ngravity. So while there already exists theoretical reasons to be skeptical of\ntachyons, our work provides a complementary direct observational constraint.",
    "c_categories":[
      "astro-ph.CO",
      "gr-qc",
      "hep-ph",
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.03060",
    "c_title":"Predicting Electromagnetically Induced Transparency based Cold Atomic\n  Engines using Deep Learning",
    "c_abstract":"We develop an artificial neural network model to predict quantum heat engines\nworking within the experimentally realized framework of electromagnetically\ninduced transparency. We specifically focus on {\\Lambda}-type alkali-based cold\natomic systems. This network allows us to analyze all the alkali atom-based\nengines' performance. High performance engines are predicted and analyzed based\non three figures of merit output, radiation temperature, work and ergotropy.\nContrary to traditional notion, the algorithm reveal the limitations of output\nradiation temperature as a stand alone metric for enhanced engine performance.\nIn high output temperature regime, Cs based engine with a higher output\ntemperature than Rb based engine is characterized by lower work and ergotropy.\nThis is found to be true for different atomic engines with common predicted\nstates in both high and low output temperature regimes. Additionally, the\nergotropy is found to exhibit a saturating exponential dependency on the\ncontrol Rabi frequency.",
    "c_categories":[
      "physics.atom-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00609",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
    ],
    "b_abstract":[
      "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03362",
    "c_title":"Radiation Testing of New Readout Electronics for the CMS ECAL Barrel",
    "c_abstract":"In preparation of the operation of the CMS electromagnetic calorimeter (ECAL)\nbarrel at the High Luminosity Large Hadron Collider (HL-LHC) the entire\non-detector electronics will be replaced. The new readout electronics comprises\n12240 very front end (VFE), 2448 front end (FE) and low voltage regulator (LVR)\ncards arranged into readout towers (RTs) of five VFEs, one FE and one LVR card.\nThe results of testing one RT of final prototype cards at CERNs CHARM mixed\nfield facility and PSIs proton irradiation facility are presented. They\ndemonstrate the proper functioning of the new electronics in the expected\nradiation conditions.",
    "c_categories":[
      "hep-ex",
      "physics.app-ph",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading\n  with Cataract",
    "a_abstract":"Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a\ncommon complication of diabetes. As two different imaging tools for DR grading,\ncolor fundus photography (CFP) and infrared fundus photography (IFP) are\nhighly-correlated and complementary in clinical applications. To the best of\nour knowledge, this is the first study that explores a novel multi-modal deep\nlearning framework to fuse the information from CFP and IFP towards more\naccurate DR grading. Specifically, we construct a dual-stream architecture\nCross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus\nimage modalities. In particular, a meticulously engineered Cross-Fundus\nAttention (CFA) module is introduced to capture the correspondence between CFP\nand IFP images. Moreover, we adopt both the single-modality and multi-modality\nsupervisions to maximize the overall performance for DR grading. Extensive\nexperiments on a clinical dataset consisting of 1,713 pairs of multi-modal\nfundus images demonstrate the superiority of our proposed method. Our code will\nbe released for public access.",
    "explanation":"The work combines transformers with two distinct methods that evaluate the quality of retinopathy",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b2"
    ],
    "c_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "c_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.02232",
    "c_title":"Distillation-Enhanced Physical Adversarial Attacks",
    "c_abstract":"The study of physical adversarial patches is crucial for identifying\nvulnerabilities in AI-based recognition systems and developing more robust deep\nlearning models. While recent research has focused on improving patch\nstealthiness for greater practical applicability, achieving an effective\nbalance between stealth and attack performance remains a significant challenge.\nTo address this issue, we propose a novel physical adversarial attack method\nthat leverages knowledge distillation. Specifically, we first define a stealthy\ncolor space tailored to the target environment to ensure smooth blending. Then,\nwe optimize an adversarial patch in an unconstrained color space, which serves\nas the 'teacher' patch. Finally, we use an adversarial knowledge distillation\nmodule to transfer the teacher patch's knowledge to the 'student' patch,\nguiding the optimization of the stealthy patch. Experimental results show that\nour approach improves attack performance by 20%, while maintaining stealth,\nhighlighting its practical value.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.10713",
    "c_title":"Improving action segmentation via explicit similarity measurement",
    "c_abstract":"Existing supervised action segmentation methods depend on the quality of\nframe-wise classification using attention mechanisms or temporal convolutions\nto capture temporal dependencies. Even boundary detection-based methods\nprimarily depend on the accuracy of an initial frame-wise classification, which\ncan overlook precise identification of segments and boundaries in case of\nlow-quality prediction. To address this problem, this paper proposes ASESM\n(Action Segmentation via Explicit Similarity Measurement) to enhance the\nsegmentation accuracy by incorporating explicit similarity evaluation across\nframes and predictions. Our supervised learning architecture uses frame-level\nmulti-resolution features as input to multiple Transformer encoders. The\nresulting multiple frame-wise predictions are used for similarity voting to\nobtain high quality initial prediction. We apply a newly proposed boundary\ncorrection algorithm that operates based on feature similarity between\nconsecutive frames to adjust the boundary locations iteratively through the\nlearning process. The corrected prediction is then further refined through\nmultiple stages of temporal convolutions. As post-processing, we optionally\napply boundary correction again followed by a segment smoothing method that\nremoves outlier classes within segments using similarity measurement between\nconsecutive predictions. Additionally, we propose a fully unsupervised boundary\ndetection-correction algorithm that identifies segment boundaries based solely\non feature similarity without any training. Experiments on 50Salads, GTEA, and\nBreakfast datasets show the effectiveness of both the supervised and\nunsupervised algorithms. Code and models are made available on Github.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.12898",
    "c_title":"DocTTT: Test-Time Training for Handwritten Document Recognition Using\n  Meta-Auxiliary Learning",
    "c_abstract":"Despite recent significant advancements in Handwritten Document Recognition\n(HDR), the efficient and accurate recognition of text against complex\nbackgrounds, diverse handwriting styles, and varying document layouts remains a\npractical challenge. Moreover, this issue is seldom addressed in academic\nresearch, particularly in scenarios with minimal annotated data available. In\nthis paper, we introduce the DocTTT framework to address these challenges. The\nkey innovation of our approach is that it uses test-time training to adapt the\nmodel to each specific input during testing. We propose a novel Meta-Auxiliary\nlearning approach that combines Meta-learning and self-supervised Masked\nAutoencoder~(MAE). During testing, we adapt the visual representation\nparameters using a self-supervised MAE loss. During training, we learn the\nmodel parameters using a meta-learning framework, so that the model parameters\nare learned to adapt to a new input effectively. Experimental results show that\nour proposed method significantly outperforms existing state-of-the-art\napproaches on benchmark datasets.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.17753",
    "c_title":"Task Graph Maximum Likelihood Estimation for Procedural Activity\n  Understanding in Egocentric Videos",
    "c_abstract":"We introduce a gradient-based approach for learning task graphs from\nprocedural activities, improving over hand-crafted methods. Our method directly\noptimizes edge weights via maximum likelihood, enabling integration into neural\narchitectures. We validate our approach on CaptainCook4D, EgoPER, and\nEgoProceL, achieving +14.5%, +10.2%, and +13.6% F1-score improvements. Our\nfeature-based approach for predicting task graphs from textual\/video embeddings\ndemonstrates emerging video understanding abilities. We also achieved top\nperformance on the procedure understanding benchmark on Ego-Exo4D and\nsignificantly improved online mistake detection (+19.8% on Assembly101-O, +6.4%\non EPIC-Tent-O). Code:\nhttps:\/\/github.com\/fpv-iplab\/Differentiable-Task-Graph-Learning.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12539",
    "c_title":"BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature\n  Analysis",
    "c_abstract":"3D semantic segmentation plays a fundamental and crucial role to understand\n3D scenes. While contemporary state-of-the-art techniques predominantly\nconcentrate on elevating the overall performance of 3D semantic segmentation\nbased on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave\nthe exploration of challenging regions for segmentation mostly neglected. In\nthis paper, we revisit 3D semantic segmentation through a more granular lens,\nshedding light on subtle complexities that are typically overshadowed by\nbroader performance metrics. Concretely, we have delineated 3D semantic\nsegmentation errors into four comprehensive categories as well as corresponding\nevaluation metrics tailored to each. Building upon this categorical framework,\nwe introduce an innovative 3D semantic segmentation network called BFANet that\nincorporates detailed analysis of semantic boundary features. First, we design\nthe boundary-semantic module to decouple point cloud features into semantic and\nboundary features, and fuse their query queue to enhance semantic features with\nattention. Second, we introduce a more concise and accelerated boundary\npseudo-label calculation algorithm, which is 3.9 times faster than the\nstate-of-the-art, offering compatibility with data augmentation and enabling\nefficient computation in training. Extensive experiments on benchmark data\nindicate the superiority of our BFANet model, confirming the significance of\nemphasizing the four uniquely designed metrics. Code is available at\nhttps:\/\/github.com\/weiguangzhao\/BFANet.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03840",
    "c_title":"Beyond uniqueness: Relaxation calculus of junction conditions for\n  coercive Hamilton-Jacobi equations",
    "c_abstract":"A junction is a particular network given by the collection of $N\\ge 1$ half\nlines $[0,+\\infty)$ glued together at the origin. On such a junction, we\nconsider evolutive Hamilton-Jacobi equations with $N$ coercive Hamiltonians.\nFurthermore,we consider a general desired junction condition at the origin,\ngiven by some monotone function $F_0:\\R^N\\to \\R$.There is existence and\nuniqueness of solutions which only satisfy weakly the junction condition (at\nthe origin, they satisfy either the desired junction condition or the PDE).We\nshow that those solutions satisfy strongly a relaxed junction condition $\\frak\nR F_0$ (that we can recognize as an effective junction condition). It is\nremarkable that this relaxed condition can be computed in three different but\nequivalent ways: 1) using viscosity inequalities, 2) using Godunov fluxes, 3)\nusing Riemann problems.Our result goes beyond uniqueness theory, in the\nfollowing sense: solutions to two different desired junction conditions $F_0$\nand $F_1$ do coincide if $\\frak R F_0=\\frak R F_1$.",
    "c_categories":[
      "math.AP"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.17616",
    "c_title":"Asymptotics of $L^r$ extremal polynomials for ${0<r\\leq\\infty}$ on\n  $C^{1+}$ Jordan regions",
    "c_abstract":"We study strong asymptotics of $L^r$-extremal polynomials for measures\nsupported on Jordan regions with $C^{1+}$ boundary for $0<r<\\infty$. Using the\nresults for $r=2$, we derive asymptotics of weighted Chebyshev and residual\npolynomials for upper-semicontinuous weights supported on a $C^{1+}$ Jordan\nregion corresponding to $r=\\infty$. As an application, we show how strong\nasymptotics for extremal polynomials in the Ahlfors problem on a $C^{1+}$\nJordan region can be obtained from that for the weighted residual polynomials.\nBased on the results we pose a conjecture for asymptotics of weighted Chebyshev\nand residual polynomials for a $C^{1+}$ arc.",
    "c_categories":[
      "math.CA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11606",
    "c_title":"Black Holes Rule Out Heavy Tachyons",
    "c_abstract":"We present direct observational constraints on tachyons; particles with group\nvelocity greater than $c$ in vacuum in a Lorentz invariant theory. Since\ntachyons may have no direct couplings to Standard Model particles, the most\nrobust and model independent constraints come from gravitational effects,\nespecially black holes. We compute the Hawking radiation of tachyons from black\nholes, finding it to be significantly enhanced in the presence of heavy\ntachyons. For a black hole of mass $M$ and tachyons of mass $m$ with $g$\ndegrees of freedom, the black hole lifetime is found to be $t_{bh} \\approx 192\n\\pi \\hbar M\/(g c^2 m^2)$ (or doubled for fermions). This implies that the\nobservation of black holes of a few solar masses, with lifetime of several\nbillion years, rules out tachyons of mass $m > 3 \\times 10^9$ GeV. This means\nthere cannot exist any tachyons associated with unification scales or quantum\ngravity. So while there already exists theoretical reasons to be skeptical of\ntachyons, our work provides a complementary direct observational constraint.",
    "c_categories":[
      "astro-ph.CO",
      "gr-qc",
      "hep-ph",
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03060",
    "c_title":"Predicting Electromagnetically Induced Transparency based Cold Atomic\n  Engines using Deep Learning",
    "c_abstract":"We develop an artificial neural network model to predict quantum heat engines\nworking within the experimentally realized framework of electromagnetically\ninduced transparency. We specifically focus on {\\Lambda}-type alkali-based cold\natomic systems. This network allows us to analyze all the alkali atom-based\nengines' performance. High performance engines are predicted and analyzed based\non three figures of merit output, radiation temperature, work and ergotropy.\nContrary to traditional notion, the algorithm reveal the limitations of output\nradiation temperature as a stand alone metric for enhanced engine performance.\nIn high output temperature regime, Cs based engine with a higher output\ntemperature than Rb based engine is characterized by lower work and ergotropy.\nThis is found to be true for different atomic engines with common predicted\nstates in both high and low output temperature regimes. Additionally, the\nergotropy is found to exhibit a saturating exponential dependency on the\ncontrol Rabi frequency.",
    "c_categories":[
      "physics.atom-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b1"
    ],
    "b_title":[
      "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
    ],
    "b_abstract":[
      "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.03362",
    "c_title":"Radiation Testing of New Readout Electronics for the CMS ECAL Barrel",
    "c_abstract":"In preparation of the operation of the CMS electromagnetic calorimeter (ECAL)\nbarrel at the High Luminosity Large Hadron Collider (HL-LHC) the entire\non-detector electronics will be replaced. The new readout electronics comprises\n12240 very front end (VFE), 2448 front end (FE) and low voltage regulator (LVR)\ncards arranged into readout towers (RTs) of five VFEs, one FE and one LVR card.\nThe results of testing one RT of final prototype cards at CERNs CHARM mixed\nfield facility and PSIs proton irradiation facility are presented. They\ndemonstrate the proper functioning of the new electronics in the expected\nradiation conditions.",
    "c_categories":[
      "hep-ex",
      "physics.app-ph",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18661",
    "c_title":"Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
    "c_abstract":"Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13516",
    "c_title":"A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
    "c_abstract":"The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":"Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
    "c_abstract":"Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12234",
    "c_title":"Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
    "c_abstract":"ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01319",
    "c_title":"Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
    "c_abstract":"Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02927",
    "c_title":"The THESAN-ZOOM project: Long-term imprints of external reionization on\n  galaxy evolution",
    "c_abstract":"We investigate the impact of ionizing external ultraviolet (UV) radiation on\nlow-mass haloes ($M_{h}<10^{10}M_\\odot$) at high redshift using $1140M_\\odot$\nbaryonic resolution zoom-in simulations of seven regions from the THESAN-ZOOM\nproject. We compare three simulation sets that differ in the treatment of\nexternal UV radiation: one employing a uniform UV background initiated at\nz=10.6 in addition to radiation transport for local sources, another with the\nsame background starting at z=5.5, and the default configuration in which the\nlarge-scale radiation field from the parent THESAN-1 simulation box acts as a\nboundary condition. The multi-phase interstellar medium (ISM) model, combined\nwith its high mass resolution, allows us to resolve all star-forming haloes and\ncapture the back-reaction of ionizing radiation on galaxy properties during the\nepoch of reionization. When present, external UV radiation efficiently unbinds\ngas in haloes with masses below $10^9M_\\odot$ and suppresses subsequent star\nformation. As a result, in simulations with early reionization, minihaloes fail\nto form stars from pristine gas, leading to reduced metal enrichment of gas\nlater accreted by more massive haloes. Consequently, haloes with masses below\n$10^{10}M_\\odot$ at all simulated epochs (z>3) exhibit lower metallicities and\naltered metallicity distributions. The more accurate and realistic shielding\nfrom external UV radiation, achieved through self-consistent radiative\ntransfer, permits the existence of a cold but low-density gas phase down to\nz=3. These findings highlight the importance of capturing a patchy reionization\nhistory in high-resolution simulations targeting high-redshift galaxy\nformation. We conclude that at minimum, a semi-numerical model that\nincorporates spatially inhomogeneous reionization and a non-uniform metallicity\nfloor is necessary to accurately emulate metal enrichment in minihaloes.",
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.08509",
    "c_title":"DISTINGUISH Workflow: A New Paradigm of Dynamic Well Placement Using\n  Generative Machine Learning",
    "c_abstract":"The real-time process of directional changes while drilling, known as\ngeosteering, is crucial for hydrocarbon extraction and emerging directional\ndrilling applications such as geothermal energy, civil infrastructure, and CO2\nstorage. The geo-energy industry seeks an automatic geosteering workflow that\ncontinually updates the subsurface uncertainties and captures the latest\ngeological understanding given the most recent observations in real-time.\n  We propose \"DISTINGUISH\": a real-time, AI-driven workflow designed to\ntransform geosteering by integrating Generative Adversarial Networks (GANs) for\ngeological parameterization, ensemble methods for model updating, and global\ndiscrete dynamic programming (DDP) optimization for complex decision-making\nduring directional drilling operations. The DISTINGUISH framework relies on\noffline training of a GAN model to reproduce relevant geology realizations and\na Forward Neural Network (FNN) to model Logging-While-Drilling (LWD) tools'\nresponse for a given geomodel.\n  This paper introduces a first-of-its-kind workflow that progressively reduces\nGAN-geomodel uncertainty around and ahead of the drilling bit and adjusts the\nwell plan accordingly. The workflow automatically integrates real-time LWD data\nwith a DDP-based decision support system, enhancing predictive models of\ngeology ahead of drilling and leading to better steering decisions. We present\na simple yet representative benchmark case and document the performance target\nachieved by the DISTINGUISH workflow prototype. This benchmark will be a\nfoundation for future methodological advancements and workflow refinements.",
    "c_categories":[
      "cs.LG",
      "math.OC",
      "physics.geo-ph",
      "stat.AP"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.09605",
    "c_title":"Modular reduction of complex representations of finite reductive groups",
    "c_abstract":"Given a complex representation of a finite group, Brauer and Nesbitt defined\nin 1941 its reduction mod p, obtaining a representation over the algebraic\nclosure of $\\mathbb{F}_p$. In 2021, Lusztig studied the characters obtained by\nreducing mod p an irreducible unipotent representation of a finite reductive\ngroup over $\\mathbb{F}_p$. He gave a conjectural formula for this character as\na linear combination of terms which had no explicit definition and were only\nknown in some small-rank examples. In this paper we provide an explicit formula\nfor these terms and prove Lusztig's conjecture, giving a formula for the\nreduction mod p of any unipotent representation of $G(\\mathbb{F}_q)$ for q a\npower of p. We also propose a conjecture linking this construction to the full\nexceptional collection in the derived category of coherent sheaves on a partial\nflag variety constructed recently by Samokhin and van der Kallen.",
    "c_categories":[
      "math.AG",
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03358",
    "c_title":"Three-loop chiral effective potential in the Wess-Zumino model",
    "c_abstract":"We calculate the three-loop contribution to the chiral effective potential in\nthe massless Wess-Zumino model. It is shown that while the non-renormalisation\ntheorem forbids divergent contributions to the chiral potential, in the\nmassless case the finite corrections survive. There are only three three-loop\nsupergraphs that give rise to a superfield effective action in the pure chiral\nsector. Two of them are UV finite while the third requires one-loop counterterm\ncorresponding to the chiral field renormalisation.",
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00726",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.04237",
    "c_title":"Casimir force inadequacy in explaining a strong attractive force in a\n  micrometer-sized narrow-gap re-entrant cavity",
    "c_abstract":"Pate et al. \\cite{pate} investigated a macroscopic opto-mechanical system\nwith a narrow-gap re-entrant cavity coupled to a SiN membrane resonator coated\nwith Au or Nb. They observed a significant increase in the membrane's effective\nspring constant $k_{\\rm eff}$ for sub-2-micron gaps $x$. This increase scales\nroughly with $x^{-4}$, suggesting an attractive force pulling the membrane\ntowards the re-entrant Al post, with an $x^{-3}$ dependence. Attributing this\nforce solely to the thermal Casimir effect is challenged by our detailed\ncalculations (presented below). These calculations reveal that the Casimir\nforce, at the investigated gap sizes, is orders of magnitude weaker than the\nobserved force. This significant discrepancy necessitates an alternative\nexplanation for the observed attraction.",
    "c_categories":[
      "physics.app-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"Designing a Light-based Communication System with a Biomolecular\n  Receiver",
    "a_abstract":"Biological systems transduce signals from their surroundings in numerous\nways. This paper introduces a communication system using the light-gated ion\nchannel Channelrhodopsin-2 (ChR2), which causes an ion current to flow in\nresponse to light. Our design includes a ChR2-based receiver along with\nencoding, modulation techniques and detection. Analyzing the resulting\ncommunication system, we discuss the effect of different parameters on the\nperformance of the system. Finally, we discuss its potential design in the\ncontext of bio-engineering and light-based communication and show that the data\nrate scales up with the number of receptors, indicating that high-speed\ncommunication may be possible.",
    "explanation":"The paper is interdisciplinary because it aims to use channelrhodopsin-2 (ChR2), a biomolecule, as a receiver to design a light-based communication system, which is a work related to engineering.",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b2",
      "b0"
    ],
    "c_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "c_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "c_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03076",
    "c_title":"Perfect matching of reactive loads through complex frequencies: from\n  circuital analysis to experiments",
    "c_abstract":"The experimental evidence of purely reactive loads impedance matching is here\nprovided by exploiting the special scattering response under complex\nexcitations. The study starts with a theoretical analysis of the reflection\nproperties of an arbitrary reactive load and identifies the proper excitation\nable to transform the purely reactive load into a virtual resistive load during\nthe time the signal is applied. To minimize reflections between the load and\nthe transmission line, the excitation must have a complex frequency, leading to\na propagating signal with a tailored temporal envelope. The aim of this work is\nto design and, for the first time,experimentally demonstrate this anomalous\nscattering behavior in microwave circuits, showing that the time-modulated\nsignals can be exploited as a new degree of freedom for achieving impedance\nmatching without introducing neither a matching network nor resistive elements,\nthat are typically used for ensuring power dissipation and, thus, zero\nreflection. The proposed matching strategy does not alter the reactive load\nthat is still lossless, enabling an anomalous termination condition where the\nenergy is not dissipated nor reflected, but indefinitely accumulated in the\nreactive load. The stored energy leaks out the load as soon as the applied\nsignal changes or stops.",
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.13488",
    "c_title":"Integrated 6G TN and NTN Localization: Challenges, Opportunities, and\n  Advancements",
    "c_abstract":"The rapid evolution of cellular networks has introduced groundbreaking\ntechnologies, including large and distributed antenna arrays and reconfigurable\nintelligent surfaces in terrestrial networks (TNs), as well as aerial and\nspace-based nodes in non-terrestrial networks (NTNs). These advancements enable\napplications beyond traditional communication, such as high-precision\nlocalization and sensing. While integrating TN and NTN enablers will lead to\nunparalleled opportunities for seamless global localization, such integration\nattempts are expected to face several challenges. To understand these\nopportunities and challenges, we first examine the distinctive characteristics\nof the key 6G enablers, evaluating their roles in localization from both\ntechnical and practical perspectives. Next, to identify developments driving\nTN-NTN localization, we review the latest standardization and industrial\ninnovation progress. Finally, we discuss the opportunities and challenges of\nTN-NTN integration, illustrating its potential through two numerical case\nstudies.",
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03378",
    "c_title":"Placement, Orientation, and Resource Allocation Optimization for\n  Cell-Free OIRS-aided OWC Network",
    "c_abstract":"The emergence of optical intelligent reflecting surface (OIRS) technologies\nmarks a milestone in optical wireless communication (OWC) systems, enabling\nenhanced control over light propagation in indoor environments. This capability\nallows for the customization of channel conditions to achieve specific\nperformance goals. This paper presents an enhancement in downlink cell-free OWC\nnetworks through the integration of OIRS. The key focus is on fine-tuning\ncrucial parameters, including transmit power, receiver orientations, OIRS\nelements allocation, and strategic placement. In particular, a multi-objective\noptimization problem (MOOP) aimed at simultaneously improving the network's\nspectral efficiency (SE) and energy efficiency (EE) while adhering to the\nnetwork's quality of service (QoS) constraints is formulated. The problem is\nsolved by employing the $\\epsilon$-constraint method to convert the MOOP into a\nsingle-objective optimization problem and solving it through successive convex\napproximation. Simulation results show the significant impact of OIRS on SE and\nEE, confirming its effectiveness in improving OWC network performance.",
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.12004",
    "c_title":"WiFi-Diffusion: Achieving Fine-Grained WiFi Radio Map Estimation With\n  Ultra-Low Sampling Rate by Diffusion Models",
    "c_abstract":"Fine-grained radio map presents communication parameters of interest, e.g.,\nreceived signal strength, at every point across a large geographical region. It\ncan be leveraged to improve the efficiency of spectrum utilization for a large\narea, particularly critical for the unlicensed WiFi spectrum. The problem of\nfine-grained radio map estimation is to utilize radio samples collected by\nsparsely distributed sensors to infer the map. This problem is challenging due\nto the ultra-low sampling rate, where the number of available samples is far\nless than the fine-grained resolution required for radio map estimation. We\npropose WiFi-Diffusion -- a novel generative framework for achieving\nfine-grained WiFi radio map estimation using diffusion models. WiFi-Diffusion\nemploys the creative power of generative AI to address the ultra-low sampling\nrate challenge and consists of three blocks: 1) a boost block, using prior\ninformation such as the layout of obstacles to optimize the diffusion model; 2)\na generation block, leveraging the diffusion model to generate a candidate set\nof radio maps; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best radio map from the candidate set. Extensive\nsimulations demonstrate that 1) the fine-grained radio map generated by\nWiFi-Diffusion is ten times better than those produced by state-of-the-art\n(SOTA) when they use the same ultra-low sampling rate; and 2) WiFi-Diffusion\nachieves comparable fine-grained radio map quality with only one-fifth of the\nsampling rate required by SOTA.",
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.05817",
    "c_title":"RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT",
    "c_abstract":"This paper evaluates the performance of reconfigurable intelligent surface\n(RIS) optimization algorithms, which utilize channel estimation methods, in ray\ntracing (RT) simulations within urban digital twin environments. Beyond\nSionna's native capabilities, we implement and benchmark additional RIS\noptimization algorithms based on channel estimation, enabling an evaluation of\nRIS strategies under various deployment conditions. Coverage maps for\nRIS-assisted communication systems are generated through the integration of\nSionna's RT simulations. Moreover, real-world experimentation underscores the\nnecessity of validating algorithms in near-realistic simulation environments,\nas minor variations in measurement setups can significantly affect performance.",
    "c_categories":[
      "eess.SP"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.02927",
    "c_title":"The THESAN-ZOOM project: Long-term imprints of external reionization on\n  galaxy evolution",
    "c_abstract":"We investigate the impact of ionizing external ultraviolet (UV) radiation on\nlow-mass haloes ($M_{h}<10^{10}M_\\odot$) at high redshift using $1140M_\\odot$\nbaryonic resolution zoom-in simulations of seven regions from the THESAN-ZOOM\nproject. We compare three simulation sets that differ in the treatment of\nexternal UV radiation: one employing a uniform UV background initiated at\nz=10.6 in addition to radiation transport for local sources, another with the\nsame background starting at z=5.5, and the default configuration in which the\nlarge-scale radiation field from the parent THESAN-1 simulation box acts as a\nboundary condition. The multi-phase interstellar medium (ISM) model, combined\nwith its high mass resolution, allows us to resolve all star-forming haloes and\ncapture the back-reaction of ionizing radiation on galaxy properties during the\nepoch of reionization. When present, external UV radiation efficiently unbinds\ngas in haloes with masses below $10^9M_\\odot$ and suppresses subsequent star\nformation. As a result, in simulations with early reionization, minihaloes fail\nto form stars from pristine gas, leading to reduced metal enrichment of gas\nlater accreted by more massive haloes. Consequently, haloes with masses below\n$10^{10}M_\\odot$ at all simulated epochs (z>3) exhibit lower metallicities and\naltered metallicity distributions. The more accurate and realistic shielding\nfrom external UV radiation, achieved through self-consistent radiative\ntransfer, permits the existence of a cold but low-density gas phase down to\nz=3. These findings highlight the importance of capturing a patchy reionization\nhistory in high-resolution simulations targeting high-redshift galaxy\nformation. We conclude that at minimum, a semi-numerical model that\nincorporates spatially inhomogeneous reionization and a non-uniform metallicity\nfloor is necessary to accurately emulate metal enrichment in minihaloes.",
    "c_categories":[
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.08509",
    "c_title":"DISTINGUISH Workflow: A New Paradigm of Dynamic Well Placement Using\n  Generative Machine Learning",
    "c_abstract":"The real-time process of directional changes while drilling, known as\ngeosteering, is crucial for hydrocarbon extraction and emerging directional\ndrilling applications such as geothermal energy, civil infrastructure, and CO2\nstorage. The geo-energy industry seeks an automatic geosteering workflow that\ncontinually updates the subsurface uncertainties and captures the latest\ngeological understanding given the most recent observations in real-time.\n  We propose \"DISTINGUISH\": a real-time, AI-driven workflow designed to\ntransform geosteering by integrating Generative Adversarial Networks (GANs) for\ngeological parameterization, ensemble methods for model updating, and global\ndiscrete dynamic programming (DDP) optimization for complex decision-making\nduring directional drilling operations. The DISTINGUISH framework relies on\noffline training of a GAN model to reproduce relevant geology realizations and\na Forward Neural Network (FNN) to model Logging-While-Drilling (LWD) tools'\nresponse for a given geomodel.\n  This paper introduces a first-of-its-kind workflow that progressively reduces\nGAN-geomodel uncertainty around and ahead of the drilling bit and adjusts the\nwell plan accordingly. The workflow automatically integrates real-time LWD data\nwith a DDP-based decision support system, enhancing predictive models of\ngeology ahead of drilling and leading to better steering decisions. We present\na simple yet representative benchmark case and document the performance target\nachieved by the DISTINGUISH workflow prototype. This benchmark will be a\nfoundation for future methodological advancements and workflow refinements.",
    "c_categories":[
      "cs.LG",
      "math.OC",
      "physics.geo-ph",
      "stat.AP"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.09605",
    "c_title":"Modular reduction of complex representations of finite reductive groups",
    "c_abstract":"Given a complex representation of a finite group, Brauer and Nesbitt defined\nin 1941 its reduction mod p, obtaining a representation over the algebraic\nclosure of $\\mathbb{F}_p$. In 2021, Lusztig studied the characters obtained by\nreducing mod p an irreducible unipotent representation of a finite reductive\ngroup over $\\mathbb{F}_p$. He gave a conjectural formula for this character as\na linear combination of terms which had no explicit definition and were only\nknown in some small-rank examples. In this paper we provide an explicit formula\nfor these terms and prove Lusztig's conjecture, giving a formula for the\nreduction mod p of any unipotent representation of $G(\\mathbb{F}_q)$ for q a\npower of p. We also propose a conjecture linking this construction to the full\nexceptional collection in the derived category of coherent sheaves on a partial\nflag variety constructed recently by Samokhin and van der Kallen.",
    "c_categories":[
      "math.AG",
      "math.RT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.03358",
    "c_title":"Three-loop chiral effective potential in the Wess-Zumino model",
    "c_abstract":"We calculate the three-loop contribution to the chiral effective potential in\nthe massless Wess-Zumino model. It is shown that while the non-renormalisation\ntheorem forbids divergent contributions to the chiral potential, in the\nmassless case the finite corrections survive. There are only three three-loop\nsupergraphs that give rise to a superfield effective action in the pure chiral\nsector. Two of them are UV finite while the third requires one-loop counterterm\ncorresponding to the chiral field renormalisation.",
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
    ],
    "b_abstract":[
      "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04237",
    "c_title":"Casimir force inadequacy in explaining a strong attractive force in a\n  micrometer-sized narrow-gap re-entrant cavity",
    "c_abstract":"Pate et al. \\cite{pate} investigated a macroscopic opto-mechanical system\nwith a narrow-gap re-entrant cavity coupled to a SiN membrane resonator coated\nwith Au or Nb. They observed a significant increase in the membrane's effective\nspring constant $k_{\\rm eff}$ for sub-2-micron gaps $x$. This increase scales\nroughly with $x^{-4}$, suggesting an attractive force pulling the membrane\ntowards the re-entrant Al post, with an $x^{-3}$ dependence. Attributing this\nforce solely to the thermal Casimir effect is challenged by our detailed\ncalculations (presented below). These calculations reveal that the Casimir\nforce, at the investigated gap sizes, is orders of magnitude weaker than the\nobserved force. This significant discrepancy necessitates an alternative\nexplanation for the observed attraction.",
    "c_categories":[
      "physics.app-ph",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09979",
    "c_title":"Silicon is the next frontier in plant synthetic biology",
    "c_abstract":"Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00165",
    "c_title":"Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
    "c_abstract":"AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17007",
    "c_title":"RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
    "c_abstract":"Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.04200",
    "c_title":"DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
    "c_abstract":"Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17449",
    "c_title":"Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
    "c_abstract":"In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07037",
    "c_title":"The integer group determinants for $GA(1,q)$",
    "c_abstract":"We show that the integer group determinants for the general affine group of\ndegree one, $GA(1,q)$ with $q=p^k$ a prime power, take the form $D=AB^{q-1},$\nwhere $A$ is a $\\mathbb Z_{q-1}$ integer group determinant and $B\\equiv A \\bmod\nq$. This generalizes the result for $k=1$.\n  When $2^k-1$ is a Mersenne prime we show that this condition is both\nnecessary and sufficient for $GA(1,2^k).$ The same is true for $GA(1,9)$ and\n$GA(1,27)$.",
    "c_categories":[
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12456",
    "c_title":"Nonlinear Principal Component Analysis with Random Bernoulli Features\n  for Process Monitoring",
    "c_abstract":"The process generates substantial amounts of data with highly complex\nstructures, leading to the development of numerous nonlinear statistical\nmethods. However, most of these methods rely on computations involving\nlarge-scale dense kernel matrices. This dependence poses significant challenges\nin meeting the high computational demands and real-time responsiveness required\nby online monitoring systems. To alleviate the computational burden of dense\nlarge-scale matrix multiplication, we incorporate the bootstrap sampling\nconcept into random feature mapping and propose a novel random Bernoulli\nprincipal component analysis method to efficiently capture nonlinear patterns\nin the process. We derive a convergence bound for the kernel matrix\napproximation constructed using random Bernoulli features, ensuring theoretical\nrobustness. Subsequently, we design four fast process monitoring methods based\non random Bernoulli principal component analysis to extend its nonlinear\ncapabilities for handling diverse fault scenarios. Finally, numerical\nexperiments and real-world data analyses are conducted to evaluate the\nperformance of the proposed methods. Results demonstrate that the proposed\nmethods offer excellent scalability and reduced computational complexity,\nachieving substantial cost savings with minimal performance loss compared to\ntraditional kernel-based approaches.",
    "c_categories":[
      "cs.LG",
      "stat.AP",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17861",
    "c_title":"Scalable, universal and conformal direct electrodes microprinting for\n  high-performance van der Waals-integrated two-dimensional electronics and\n  flexible applications",
    "c_abstract":"Two-dimensional (2D) materials with extraordinary electrical properties, hold\npromising for large-scale, flexible electronics. However, their device\nperformance could be hindered due to the excessive defects introduced via\ntraditional electrode integration processes. Transfer printing techniques have\nbeen developed for van der Waals contacts integration, while existing\ntechniques encounter limitations in achieving conformal electrode transfer and\ncompatibility with flexible devices. Here we introduce a highly conformal\nmicroprinting technique utilizing polypropylene carbonate (PPC)\/Polyvinyl\nalcohol (PVA) copolymer, which enables successful transfer of wafer-scale,\nmicropatterned electrodes onto diverse substrates, including those with complex\ngeometries. This technique, implemented with 2D transition metal\ndichalcogenides (TMDCs), yields 2D field-effect transistors with near-ideal\nohmic contacts, and a record-high carrier mobility up to 334 cm2 V-1 s-1 for a\nWSe2 device. Furthermore, we fabricated transistor arrays on MoS2 thin film,\nwhich show uniform device performance. We also present the flexible MoS2\ntransistors that not only achieve a high electron mobility of up to 111 cm2 V-1\ns-1 but also exhibit outstanding mechanical robustness. Our findings represent\na significant leap forward in the fabrication of flexible 2D electronics,\npaving the way for numerous emerging technologies.",
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.19589",
    "c_title":"Anomalous spin-optical helical effect in Ti-based kagome metal",
    "c_abstract":"The kagome lattice stands as a rich platform for hosting a wide array of\ncorrelated quantum phenomena, ranging from charge density waves and\nsuperconductivity to electron nematicity and loop current states. Direct\ndetection of loop currents in kagome systems has remained a formidable\nchallenge due to their intricate spatial arrangements and the weak magnetic\nfield signatures they produce. This has left their existence and underlying\nmechanisms a topic of intense debate. In this work, we uncover a hallmark\nreconcilable with loop currents: spin handedness-selective signals that surpass\nconventional dichroic, spin, and spin-dichroic responses. We observe this\nphenomenon in the kagome metal CsTi$_3$Bi$_5$ and we call it the anomalous\nspin-optical helical effect. This effect arises from the coupling of light' s\nhelicity with spin-orbital electron correlations, providing a groundbreaking\nmethod to visualize loop currents in quantum materials. Our discovery not only\nenriches the debate surrounding loop currents but also paves the way for new\nstrategies to exploit the electronic phases of quantum materials via\nlight-matter interaction.",
    "c_categories":[
      "cond-mat.str-el"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.05236",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b2",
      "b0"
    ],
    "b_title":[
      "Shannon capacity of signal transduction for multiple independent receptors",
      "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
    ],
    "b_abstract":[
      "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
      "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
    ],
    "b_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16880",
    "c_title":"A failed wind candidate in NGC 3783 from the 2001 year campaign with\n  Chandra\/HETGS",
    "c_abstract":"We reanalyze the Chandra\/HETGS observations of NGC 3783 from the campaign in\nthe year 2001, identifying significant spectral variations in the Fe unresolved\ntransition array (UTA) over timescales of weeks to months. These changes\ncorrelate with a $1.4-2$ fold increase in the ionizing continuum and exceed $10\n\\, \\sigma$ significance. The variations primarily originate from a\nlow-ionization state ($\\rm log \\xi = 1.65$) component of the warm absorber.\nTime-dependent photoionization modelling confirms the sensitivity of this\nlow-ionization component to continuum variations within the Fe UTA band. Local\nfitting indicates a lower density limit of $>10^{12.3} \\, \\rm m^{-3}$ at $3 \\,\n\\sigma$ statistical uncertainty, with the component located within $0.27 \\, \\rm\npc$. Our findings suggest that this low-ionization component is a potential\nfailed wind candidate.",
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"Artificial Intelligence-Enhanced Couinaud Segmentation for Precision\n  Liver Cancer Therapy",
    "a_abstract":"Precision therapy for liver cancer necessitates accurately delineating liver\nsub-regions to protect healthy tissue while targeting tumors, which is\nessential for reducing recurrence and improving survival rates. However, the\nsegmentation of hepatic segments, known as Couinaud segmentation, is\nchallenging due to indistinct sub-region boundaries and the need for extensive\nannotated datasets. This study introduces LiverFormer, a novel Couinaud\nsegmentation model that effectively integrates global context with low-level\nlocal features based on a 3D hybrid CNN-Transformer architecture. Additionally,\na registration-based data augmentation strategy is equipped to enhance the\nsegmentation performance with limited labeled data. Evaluated on CT images from\n123 patients, LiverFormer demonstrated high accuracy and strong concordance\nwith expert annotations across various metrics, allowing for enhanced treatment\nplanning for surgery and radiation therapy. It has great potential to reduces\ncomplications and minimizes potential damages to surrounding tissue, leading to\nimproved outcomes for patients undergoing complex liver cancer treatments.",
    "explanation":"The paper presents a tool that uses Convolutional Neural Networks (CNN) and Transformers, technologies from Computer Science, to improve the accuracy and efficiency of Couinaud segmentation in liver cancer treatment, a challenge in the field of Medicine.",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b13"
    ],
    "c_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "c_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":"Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
    "c_abstract":"Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17455",
    "c_title":"Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
    "c_abstract":"Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12234",
    "c_title":"Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
    "c_abstract":"ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":"Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
    "c_abstract":"Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13516",
    "c_title":"A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
    "c_abstract":"The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.07037",
    "c_title":"The integer group determinants for $GA(1,q)$",
    "c_abstract":"We show that the integer group determinants for the general affine group of\ndegree one, $GA(1,q)$ with $q=p^k$ a prime power, take the form $D=AB^{q-1},$\nwhere $A$ is a $\\mathbb Z_{q-1}$ integer group determinant and $B\\equiv A \\bmod\nq$. This generalizes the result for $k=1$.\n  When $2^k-1$ is a Mersenne prime we show that this condition is both\nnecessary and sufficient for $GA(1,2^k).$ The same is true for $GA(1,9)$ and\n$GA(1,27)$.",
    "c_categories":[
      "math.NT"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.12456",
    "c_title":"Nonlinear Principal Component Analysis with Random Bernoulli Features\n  for Process Monitoring",
    "c_abstract":"The process generates substantial amounts of data with highly complex\nstructures, leading to the development of numerous nonlinear statistical\nmethods. However, most of these methods rely on computations involving\nlarge-scale dense kernel matrices. This dependence poses significant challenges\nin meeting the high computational demands and real-time responsiveness required\nby online monitoring systems. To alleviate the computational burden of dense\nlarge-scale matrix multiplication, we incorporate the bootstrap sampling\nconcept into random feature mapping and propose a novel random Bernoulli\nprincipal component analysis method to efficiently capture nonlinear patterns\nin the process. We derive a convergence bound for the kernel matrix\napproximation constructed using random Bernoulli features, ensuring theoretical\nrobustness. Subsequently, we design four fast process monitoring methods based\non random Bernoulli principal component analysis to extend its nonlinear\ncapabilities for handling diverse fault scenarios. Finally, numerical\nexperiments and real-world data analyses are conducted to evaluate the\nperformance of the proposed methods. Results demonstrate that the proposed\nmethods offer excellent scalability and reduced computational complexity,\nachieving substantial cost savings with minimal performance loss compared to\ntraditional kernel-based approaches.",
    "c_categories":[
      "cs.LG",
      "stat.AP",
      "stat.ME",
      "stat.ML"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.17861",
    "c_title":"Scalable, universal and conformal direct electrodes microprinting for\n  high-performance van der Waals-integrated two-dimensional electronics and\n  flexible applications",
    "c_abstract":"Two-dimensional (2D) materials with extraordinary electrical properties, hold\npromising for large-scale, flexible electronics. However, their device\nperformance could be hindered due to the excessive defects introduced via\ntraditional electrode integration processes. Transfer printing techniques have\nbeen developed for van der Waals contacts integration, while existing\ntechniques encounter limitations in achieving conformal electrode transfer and\ncompatibility with flexible devices. Here we introduce a highly conformal\nmicroprinting technique utilizing polypropylene carbonate (PPC)\/Polyvinyl\nalcohol (PVA) copolymer, which enables successful transfer of wafer-scale,\nmicropatterned electrodes onto diverse substrates, including those with complex\ngeometries. This technique, implemented with 2D transition metal\ndichalcogenides (TMDCs), yields 2D field-effect transistors with near-ideal\nohmic contacts, and a record-high carrier mobility up to 334 cm2 V-1 s-1 for a\nWSe2 device. Furthermore, we fabricated transistor arrays on MoS2 thin film,\nwhich show uniform device performance. We also present the flexible MoS2\ntransistors that not only achieve a high electron mobility of up to 111 cm2 V-1\ns-1 but also exhibit outstanding mechanical robustness. Our findings represent\na significant leap forward in the fabrication of flexible 2D electronics,\npaving the way for numerous emerging technologies.",
    "c_categories":[
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.19589",
    "c_title":"Anomalous spin-optical helical effect in Ti-based kagome metal",
    "c_abstract":"The kagome lattice stands as a rich platform for hosting a wide array of\ncorrelated quantum phenomena, ranging from charge density waves and\nsuperconductivity to electron nematicity and loop current states. Direct\ndetection of loop currents in kagome systems has remained a formidable\nchallenge due to their intricate spatial arrangements and the weak magnetic\nfield signatures they produce. This has left their existence and underlying\nmechanisms a topic of intense debate. In this work, we uncover a hallmark\nreconcilable with loop currents: spin handedness-selective signals that surpass\nconventional dichroic, spin, and spin-dichroic responses. We observe this\nphenomenon in the kagome metal CsTi$_3$Bi$_5$ and we call it the anomalous\nspin-optical helical effect. This effect arises from the coupling of light' s\nhelicity with spin-orbital electron correlations, providing a groundbreaking\nmethod to visualize loop currents in quantum materials. Our discovery not only\nenriches the debate surrounding loop currents but also paves the way for new\nstrategies to exploit the electronic phases of quantum materials via\nlight-matter interaction.",
    "c_categories":[
      "cond-mat.str-el"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b36",
      "b33"
    ],
    "b_title":[
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
    ],
    "b_abstract":[
      "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
      "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
    ],
    "b_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.16880",
    "c_title":"A failed wind candidate in NGC 3783 from the 2001 year campaign with\n  Chandra\/HETGS",
    "c_abstract":"We reanalyze the Chandra\/HETGS observations of NGC 3783 from the campaign in\nthe year 2001, identifying significant spectral variations in the Fe unresolved\ntransition array (UTA) over timescales of weeks to months. These changes\ncorrelate with a $1.4-2$ fold increase in the ionizing continuum and exceed $10\n\\, \\sigma$ significance. The variations primarily originate from a\nlow-ionization state ($\\rm log \\xi = 1.65$) component of the warm absorber.\nTime-dependent photoionization modelling confirms the sensitivity of this\nlow-ionization component to continuum variations within the Fe UTA band. Local\nfitting indicates a lower density limit of $>10^{12.3} \\, \\rm m^{-3}$ at $3 \\,\n\\sigma$ statistical uncertainty, with the component located within $0.27 \\, \\rm\npc$. Our findings suggest that this low-ionization component is a potential\nfailed wind candidate.",
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11919",
    "c_title":"k-fold Subsampling based Sequential Backward Feature Elimination",
    "c_abstract":"We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.04592",
    "c_title":"A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
    "c_abstract":"Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps:\/\/github.com\/mrazhou\/BRSIC.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.01725",
    "c_title":"HarmonySet: A Comprehensive Dataset for Understanding Video-Music\n  Semantic Alignment and Temporal Synchronization",
    "c_abstract":"This paper introduces HarmonySet, a comprehensive dataset designed to advance\nvideo-music understanding. HarmonySet consists of 48,328 diverse video-music\npairs, annotated with detailed information on rhythmic synchronization,\nemotional alignment, thematic coherence, and cultural relevance. We propose a\nmulti-step human-machine collaborative framework for efficient annotation,\ncombining human insights with machine-generated descriptions to identify key\ntransitions and assess alignment across multiple dimensions. Additionally, we\nintroduce a novel evaluation framework with tasks and metrics to assess the\nmulti-dimensional alignment of video and music, including rhythm, emotion,\ntheme, and cultural context. Our extensive experiments demonstrate that\nHarmonySet, along with the proposed evaluation framework, significantly\nimproves the ability of multimodal models to capture and analyze the intricate\nrelationships between video and music.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16873",
    "c_title":"Classifier-guided CLIP Distillation for Unsupervised Multi-label\n  Classification",
    "c_abstract":"Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https:\/\/github.com\/k0u-id\/CCD.",
    "c_categories":[
      "cs.AI",
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08560",
    "c_title":"Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion",
    "c_abstract":"The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https:\/\/github.com\/LemuelPuglisi\/BrLP.",
    "c_categories":[
      "cs.AI",
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.10820",
    "c_title":"Self-calibration in two dimensional aperture mask optical\n  interferometry: a new method for wavefront sensing",
    "c_abstract":"We present a new methodology in optical aperture masking interferometry\ninvolving Fourier analysis of the complex visibilities (real and imaginary, or\namplitude and phase), derived from the interferometric images. The analysis\nincludes use of a non-redundant aperture mask, and self-calibration of the\nhole-based complex voltage gains to correct for non-uniform illumination and\nphase fluctuations across the mask. The technique is demonstrated using the\nSynchrotron Radiation Interferometry (SRI) facility at the ALBA synchrotron\nlight source. Application of the technique results in a joint derivation of the\nGaussian shape of the electron beam, and of the hole-based voltage gain\namplitude and phase distribution over the mask area. The gain phases are\nlinearly related to the photon path-lengths through the optical system for the\nray to each hole, and hence represent an accurate wavefront sensor (WFS)\ndetermining the path-length distortions across the wavefront. Wavefront sensing\nis a vital technology in fields ranging from adaptive optics to metrology to\noptometry. We calculate that the rms precision of this WFS method is better\nthan 1~nm per frame in the current experiment. We compare the technique to the\nstandard Shack-Hartman WFS. We also show that a structure-agnostic imaging and\ndeconvolution process can be used with the visibility data to determine the\nbeam shape without assuming a Gaussian model, and hence the technique is\ngeneralizable to more complex source morphologies.",
    "c_categories":[
      "astro-ph.IM",
      "physics.acc-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.03014",
    "c_title":"Chiral dynamics: Quo vadis?",
    "c_abstract":"I review the status of chiral dynamics. Topics include pion-pion scattering,\ndynamically generated states in the hadron spectrum and the emergence of\ntwo-pole structures, chiral symmetry in nuclear physics and chiral dynamics in\nthe Big Bang.",
    "c_categories":[
      "hep-ex",
      "hep-lat",
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04629",
    "c_title":"Characterizations of Variational Convexity and Tilt Stability via\n  Quadratic Bundles",
    "c_abstract":"In this paper, we establish characterizations of variational $s$-convexity\nand tilt stability for prox-regular functions in the absence of subdifferential\ncontinuity via quadratic bundles, a kind of primal-dual generalized\nsecond-order derivatives recently introduced by Rockafellar. Deriving such\ncharacterizations in the effective pointbased form requires a certain revision\nof quadratic bundles investigated below. Our device is based on the notion of\ngeneralized twice differentiability and its novel characterization via\nclassical twice differentiability of the associated Moreau envelopes combined\nwith various limiting procedures for functions and sets.",
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.13251",
    "c_title":"Groupoids, equivalence bibundles and bimodules for noncommutative\n  solenoids",
    "c_abstract":"Let $p$ be a prime number and $\\mathcal{S}_p$ the $p$-solenoid. For\n$\\alpha\\in \\mathbb{R}\\times \\mathbb{Q}_p$ we consider in this paper a naturally\nassociated action groupoid $S_\\alpha:=\\mathbb{Z} [1\/p]\\ltimes_\\alpha\n\\mathcal{S}_p \\rightrightarrows \\mathcal{S}_p$ whose $C^*-$algebra is a model\nfor the noncommutative solenoid $\\mathcal{A}_\\alpha^\\mathscr{S}$ studied by\nLatremoli\\`ere and Packer. Following the geometric ideas of Connes and Rieffel\nto describe the Morita equivalences of noncommutative torus using the Kronecker\nfoliation on the torus, we give an explicit description of the\ngeometric\/topologic equivalence bibundle for groupoids $S_\\alpha$ and $S_\\beta$\nwhenever $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ are in the same orbit\nof the $GL_2(\\mathbb{Z}[1\/p])$ action by linear fractional transformations. As\na corollary, for $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ as above we\nget an explicit description of the imprimitivity bimodules for the associated\nnoncommutative solenoids.",
    "c_categories":[
      "math.DS",
      "math.OA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.02815",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b13"
    ],
    "b_title":[
      "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
    ],
    "b_abstract":[
      "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.09303",
    "c_title":"Development of a Test System for Data Links of the ATLAS Inner Tracker\n  (ITk) Upgrade Silicon Pixel Detector",
    "c_abstract":"This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb\/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
    "c_categories":[
      "hep-ex",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"Automated Classification of Cell Shapes: A Comparative Evaluation of\n  Shape Descriptors",
    "a_abstract":"This study addresses the challenge of classifying cell shapes from noisy\ncontours, such as those obtained through cell instance segmentation of\nhistological images. We assess the performance of various features for shape\nclassification, including Elliptical Fourier Descriptors, curvature features,\nand lower dimensional representations. Using an annotated synthetic dataset of\nnoisy contours, we identify the most suitable shape descriptors and apply them\nto a set of real images for qualitative analysis. Our aim is to provide a\ncomprehensive evaluation of descriptors for classifying cell shapes, which can\nsupport cell type identification and tissue characterization-critical tasks in\nboth biological research and histopathological assessments.",
    "explanation":"This study addresses the challenge of classifying cell shapes from noisy contours, such as those obtained through cell instance segmentation of histological images.\n\nOur aim is to provide a comprehensive evaluation of descriptors for classifying cell shapes, which can support cell type identification and tissue characterization\u2014critical tasks in both biological research and histopathological assessments.\n",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b4"
    ],
    "c_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "c_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01376",
    "c_title":"Pushing the boundaries of Structure-Based Drug Design through\n  Collaboration with Large Language Models",
    "c_abstract":"Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.04200",
    "c_title":"DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
    "c_abstract":"Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03606",
    "c_title":"Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
    "c_abstract":"Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.06371",
    "c_title":"COLOR: A compositional linear operation-based representation of protein\n  sequences for identification of monomer contributions to properties",
    "c_abstract":"The properties of biological materials like proteins and nucleic acids are\nlargely determined by their primary sequence. While certain segments in the\nsequence strongly influence specific functions, identifying these segments, or\nso-called motifs, is challenging due to the complexity of sequential data.\nWhile deep learning (DL) models can accurately capture sequence-property\nrelationships, the degree of nonlinearity in these models limits the assessment\nof monomer contributions to a property - a critical step in identifying key\nmotifs. Recent advances in explainable AI (XAI) offer attention and\ngradient-based methods for estimating monomeric contributions. However, these\nmethods are primarily applied to classification tasks, such as binding site\nidentification, where they achieve limited accuracy (40-45%) and rely on\nqualitative evaluations. To address these limitations, we introduce a DL model\nwith interpretable steps, enabling direct tracing of monomeric contributions.\nWe also propose a metric ($\\mathcal{I}$), inspired by the masking technique in\nthe field of image analysis and natural language processing, for quantitative\nanalysis on datasets mainly containing distinct properties of anti-cancer\npeptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits\n22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that\nsignificantly destabilize ACPs, and identifies motifs in AMPs that are 50% more\neffective in converting non-AMPs to AMPs. These findings highlight the\npotential of our model in guiding mutation strategies for designing\nprotein-based biomaterials.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.11326",
    "c_title":"Deep Learning of Proteins with Local and Global Regions of Disorder",
    "c_abstract":"Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
    "c_categories":[
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.10820",
    "c_title":"Self-calibration in two dimensional aperture mask optical\n  interferometry: a new method for wavefront sensing",
    "c_abstract":"We present a new methodology in optical aperture masking interferometry\ninvolving Fourier analysis of the complex visibilities (real and imaginary, or\namplitude and phase), derived from the interferometric images. The analysis\nincludes use of a non-redundant aperture mask, and self-calibration of the\nhole-based complex voltage gains to correct for non-uniform illumination and\nphase fluctuations across the mask. The technique is demonstrated using the\nSynchrotron Radiation Interferometry (SRI) facility at the ALBA synchrotron\nlight source. Application of the technique results in a joint derivation of the\nGaussian shape of the electron beam, and of the hole-based voltage gain\namplitude and phase distribution over the mask area. The gain phases are\nlinearly related to the photon path-lengths through the optical system for the\nray to each hole, and hence represent an accurate wavefront sensor (WFS)\ndetermining the path-length distortions across the wavefront. Wavefront sensing\nis a vital technology in fields ranging from adaptive optics to metrology to\noptometry. We calculate that the rms precision of this WFS method is better\nthan 1~nm per frame in the current experiment. We compare the technique to the\nstandard Shack-Hartman WFS. We also show that a structure-agnostic imaging and\ndeconvolution process can be used with the visibility data to determine the\nbeam shape without assuming a Gaussian model, and hence the technique is\ngeneralizable to more complex source morphologies.",
    "c_categories":[
      "astro-ph.IM",
      "physics.acc-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.03014",
    "c_title":"Chiral dynamics: Quo vadis?",
    "c_abstract":"I review the status of chiral dynamics. Topics include pion-pion scattering,\ndynamically generated states in the hadron spectrum and the emergence of\ntwo-pole structures, chiral symmetry in nuclear physics and chiral dynamics in\nthe Big Bang.",
    "c_categories":[
      "hep-ex",
      "hep-lat",
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04629",
    "c_title":"Characterizations of Variational Convexity and Tilt Stability via\n  Quadratic Bundles",
    "c_abstract":"In this paper, we establish characterizations of variational $s$-convexity\nand tilt stability for prox-regular functions in the absence of subdifferential\ncontinuity via quadratic bundles, a kind of primal-dual generalized\nsecond-order derivatives recently introduced by Rockafellar. Deriving such\ncharacterizations in the effective pointbased form requires a certain revision\nof quadratic bundles investigated below. Our device is based on the notion of\ngeneralized twice differentiability and its novel characterization via\nclassical twice differentiability of the associated Moreau envelopes combined\nwith various limiting procedures for functions and sets.",
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.13251",
    "c_title":"Groupoids, equivalence bibundles and bimodules for noncommutative\n  solenoids",
    "c_abstract":"Let $p$ be a prime number and $\\mathcal{S}_p$ the $p$-solenoid. For\n$\\alpha\\in \\mathbb{R}\\times \\mathbb{Q}_p$ we consider in this paper a naturally\nassociated action groupoid $S_\\alpha:=\\mathbb{Z} [1\/p]\\ltimes_\\alpha\n\\mathcal{S}_p \\rightrightarrows \\mathcal{S}_p$ whose $C^*-$algebra is a model\nfor the noncommutative solenoid $\\mathcal{A}_\\alpha^\\mathscr{S}$ studied by\nLatremoli\\`ere and Packer. Following the geometric ideas of Connes and Rieffel\nto describe the Morita equivalences of noncommutative torus using the Kronecker\nfoliation on the torus, we give an explicit description of the\ngeometric\/topologic equivalence bibundle for groupoids $S_\\alpha$ and $S_\\beta$\nwhenever $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ are in the same orbit\nof the $GL_2(\\mathbb{Z}[1\/p])$ action by linear fractional transformations. As\na corollary, for $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ as above we\nget an explicit description of the imprimitivity bimodules for the associated\nnoncommutative solenoids.",
    "c_categories":[
      "math.DS",
      "math.OA"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b24"
    ],
    "b_title":[
      "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
    ],
    "b_abstract":[
      "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
    ],
    "b_categories":[
      "cs.AI"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.09303",
    "c_title":"Development of a Test System for Data Links of the ATLAS Inner Tracker\n  (ITk) Upgrade Silicon Pixel Detector",
    "c_abstract":"This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb\/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
    "c_categories":[
      "hep-ex",
      "physics.ins-det"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.03274",
    "c_title":"A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
    "c_abstract":"Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.18202",
    "c_title":"On Scaling Neurosymbolic Programming through Guided Logical Inference",
    "c_abstract":"Probabilistic neurosymbolic learning seeks to integrate neural networks with\nsymbolic programming. Many state-of-the-art systems rely on a reduction to the\nProbabilistic Weighted Model Counting Problem (PWMC), which requires computing\na Boolean formula called the logical provenance.However, PWMC is \\\\#P-hard, and\nthe number of clauses in the logical provenance formula can grow exponentially,\ncreating a major bottleneck that significantly limits the applicability of PNL\nsolutions in practice.We propose a new approach centered around an exact\nalgorithm DPNL, that enables bypassing the computation of the logical\nprovenance.The DPNL approach relies on the principles of an oracle and a\nrecursive DPLL-like decomposition in order to guide and speed up logical\ninference.Furthermore, we show that this approach can be adapted for\napproximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees,\ncalled ApproxDPNL.Experiments show significant performance gains.DPNL enables\nscaling exact inference further, resulting in more accurate models.Further,\nApproxDPNL shows potential for advancing the scalability of neurosymbolic\nprogramming by incorporating approximations even further, while simultaneously\nensuring guarantees for the reasoning process.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15840",
    "c_title":"Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents",
    "c_abstract":"While Large Language Models (LLMs) can exhibit impressive proficiency in\nisolated, short-term tasks, they often fail to maintain coherent performance\nover longer time horizons. In this paper, we present Vending-Bench, a simulated\nenvironment designed to specifically test an LLM-based agent's ability to\nmanage a straightforward, long-running business scenario: operating a vending\nmachine. Agents must balance inventories, place orders, set prices, and handle\ndaily fees - tasks that are each simple but collectively, over long horizons\n(>20M tokens per run) stress an LLM's capacity for sustained, coherent\ndecision-making. Our experiments reveal high variance in performance across\nmultiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most\nruns and turn a profit, but all models have runs that derail, either through\nmisinterpreting delivery schedules, forgetting orders, or descending into\ntangential \"meltdown\" loops from which they rarely recover. We find no clear\ncorrelation between failures and the point at which the model's context window\nbecomes full, suggesting that these breakdowns do not stem from memory limits.\nApart from highlighting the high variance in performance over long time\nhorizons, Vending-Bench also tests models' ability to acquire capital, a\nnecessity in many hypothetical dangerous AI scenarios. We hope the benchmark\ncan help in preparing for the advent of stronger AI systems.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.11312",
    "c_title":"AI Generations: From AI 1.0 to AI 4.0",
    "c_abstract":"This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.10620",
    "c_title":"ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
    "c_abstract":"Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
    "c_categories":[
      "cs.AI"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19192",
    "c_title":"Embodying mechano-fluidic memory in soft machines to program behaviors\n  upon interactions",
    "c_abstract":"Soft machines display shape adaptation to external circumstances due to their\nintrinsic compliance. To achieve increasingly more responsive behaviors upon\ninteractions without relying on centralized computation, embodying memory\ndirectly in the machines' structure is crucial. Here, we harness the\nbistability of elastic shells to alter the fluidic properties of an enclosed\ncavity, thereby switching between stable frequency states of a locomoting\nself-oscillating machine. To program these memory states upon interactions, we\ndevelop fluidic circuits surrounding the bistable shell, with soft tubes that\nkink and unkink when externally touched. We implement circuits for both\nlong-term and short-term memory in a soft machine that switches behaviors in\nresponse to a human user and that autonomously changes direction after\ndetecting a wall. By harnessing only geometry and elasticity, embodying memory\nallows physical structures without a central brain to exhibit autonomous feats\nthat are typically reserved for computer-based robotic systems.",
    "c_categories":[
      "cond-mat.soft",
      "cs.RO",
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02545",
    "c_title":"Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
    "c_abstract":"We consider the problem of how many samples from a Gaussian multi-index model\nare required to weakly reconstruct the relevant index subspace. Despite its\nincreasing popularity as a testbed for investigating the computational\ncomplexity of neural networks, results beyond the single-index setting remain\nelusive. In this work, we introduce spectral algorithms based on the\nlinearization of a message passing scheme tailored to this problem. Our main\ncontribution is to show that the proposed methods achieve the optimal\nreconstruction threshold. Leveraging a high-dimensional characterization of the\nalgorithms, we show that above the critical threshold the leading eigenvector\ncorrelates with the relevant index subspace, a phenomenon reminiscent of the\nBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix\ntheory. Supported by numerical experiments and a rigorous theoretical\nframework, our work bridges critical gaps in the computational limits of weak\nlearnability in multi-index model.",
    "c_categories":[
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "c_fields":[
      "Physics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15494",
    "c_title":"Three flavor QCD phase transition with M\\\"obius domain wall fermions",
    "c_abstract":"We present an updated study of the $N_f=3$ QCD phase transition using\nM\\\"{o}bius domain wall fermions. Simulations were performed on $N_t=12$\nlattices with aspect ratios ranging from 2 to 4 for various quark masses, at a\nlattice spacing of $a=0.1361(20)$ fm, corresponding to a temperature of 121(2)\nMeV. To clarify the nature of the phase transition, a large-volume lattice,\n$48^3 \\times 12\\times 16$, was added to analyze the volume dependence of\ndisconnected chiral susceptibility. By examining the chiral condensate,\ndisconnected chiral susceptibility, and Binder cumulant, and incorporating\nresults from $24^3 \\times 12 \\times 16$ and $36^3 \\times 12 \\times 16$ lattices\nreported in earlier studies, we observe that the transition is consistent with\na crossover at a quark mass of approximately $m_f^{\\mathrm{\\overline {MS}}}(2\\,\n\\mathrm{GeV}) \\sim 4$ MeV at this temperature. Furthermore, we discuss the\neffects of residual chiral symmetry breaking on the chiral condensate and\ndisconnected chiral susceptibility for different sizes in the 5th direction.",
    "c_categories":[
      "hep-lat",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04337",
    "c_title":"Diagrammatic Multiplet-Sum Method (MSM) Density-Functional Theory (DFT):\n  II. Completion of the Two-Orbital Two-Electron Model (TOTEM) with an\n  Application to the Avoided Crossing in Lithium Hydride (LiH)",
    "c_abstract":"The Ziegler-Rauk-Baerends multiplet sum method (MSM) assumes that\ndensity-functional theory (DFT) provides a good description of states dominated\nby a single determinant. It then uses symmetry to add static correlation to\nDFT. In our previous article (Article I) [J. Chem. Phys. 159, 244306 (2023)],\nwe introduced diagrammatic MSM-DFT as a tool to aid in extending MSM-DFT to\ninclude the nondynamic correlation needed for making and breaking bonds even in\nthe absence of symmetry. An attractive feature of this approach is that no\nfunctional-dependent parameters need to be introduced, though choices are\nneeded in making correspondances between wave function theory (WFT) and MSM-DFT\ndiagrams. The preliminary examples in Article I used the two-orbital\ntwo-electron model (TOTEM) less completely than could have been the case.\nDiagrammatic MSM-DFT is extended here to treat the full TOTEM and it is shown\nthat the unsymmetric lithium hydride (LiH) molecule dissociates into neutral\natoms when diagrammatic MSM-DFT techniques are used to introduce a proper\ndescription of the avoided crossing between ionic bonding and covalent bonding\nstates.The method is tested for Hartree-Fock and for three functionals (LDA,\nPW91, and B3LYP). All the functionals yield similar results as should be\nexpected for a properly-formulated parameter-free theory. Agreement with\navailable estimates show that the magnitude of the coupling element introduced\nhere is excellent. However more work will be needed to obtain quantitative\nagreement between our diagrammatic MSM-DFT ground-state potential energy curve\nand that found from high-quality ab initio calculations",
    "c_categories":[
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00561",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b4"
    ],
    "b_title":[
      "What is a cell type, really? The quest to categorize life's myriad forms."
    ],
    "b_abstract":[
      "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
    ],
    "b_categories":[
      "q-bio.BM"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.01596",
    "c_title":"More on unconstrained descriptions of Higher Spin Massless Particles",
    "c_abstract":"Here we suggest a new local action describing arbitrary integer spin-$s$\nmassless particles in terms of only two symmetric fields $\\varphi $ and\n$\\alpha$ of rank-$s$ and $(s-3)$ respectively. It is an unconstrained version\nof the Fronsdal theory where the double traceless constraint on the physical\nfield is evaded via a rank-$(s-4)$ Weyl like symmetry. The constrained higher\nspin diffeomorphism is enlarged to full diffeomorphism via the Stueckelberg\nfield $\\alpha$ through an appropriate field redefinition. After a partial gauge\nfixing where the Weyl symmetry is broken while preserving diffeomorphisms, the\nfield equations reproduce, for arbitrary integer spin-$s$, diffeomorphism\ninvariant equations of motion previously obtained via a truncation of the\nspectrum of the open bosonic string field theory in the tensionless limit. In\nthe $s=4$ case we show that the functional integration over $\\alpha$ leads to a\nunique non local Weyl and diffeomorphism invariant action given only in terms\nof the physical field $\\varphi$ whose spectrum is confirmed via an analysis of\nthe analytic structure of the spin-4 propagator for which we introduce a\ncomplete basis of projection and transition non local differential operators.\nWe also show that the elimination of $\\alpha$ after the Weyl gauge fixing leads\nto a non local diffeomorphism invariant action previously obtained in the\nliterature.",
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum\n  annotations",
    "a_abstract":"In drug discovery, accurate lung tumor segmentation is an important step for\nassessing tumor size and its progression using \\textit{in-vivo} imaging such as\nMRI. While deep learning models have been developed to automate this process,\nthe focus has predominantly been on human subjects, neglecting the pivotal role\nof animal models in pre-clinical drug development. In this work, we focus on\noptimizing lung tumor segmentation in mice. First, we demonstrate that the\nnnU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Most\nimportantly, we achieve better results with nnU-Net 3D models than 2D models,\nindicating the importance of spatial context for segmentation tasks in MRI mice\nscans. This study demonstrates the importance of 3D input over 2D input images\nfor lung tumor segmentation in MRI scans. Finally, we outperform the prior\nstate-of-the-art approach that involves the combined segmentation of lungs and\ntumors within the lungs. Our work achieves comparable results using only lung\ntumor annotations requiring fewer annotations, saving time and annotation\nefforts. This work\n(https:\/\/anonymous.4open.science\/r\/lung-tumour-mice-mri-64BB) is an important\nstep in automating pre-clinical animal studies to quantify the efficacy of\nexperimental drugs, particularly in assessing tumor changes.",
    "explanation":"In this work, we focus on optimizing lung tumor segmen-\ntation in mice. First, we demonstrate that the nnU-Net model outper-\nforms the U-Net, U-Net3+, and DeepMeta models.",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b12"
    ],
    "c_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "c_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.11249",
    "c_title":"Enhancing SAR Object Detection with Self-Supervised Pre-training on\n  Masked Auto-Encoders",
    "c_abstract":"Supervised fine-tuning methods (SFT) perform great efficiency on artificial\nintelligence interpretation in SAR images, leveraging the powerful\nrepresentation knowledge from pre-training models. Due to the lack of\ndomain-specific pre-trained backbones in SAR images, the traditional strategies\nare loading the foundation pre-train models of natural scenes such as ImageNet,\nwhose characteristics of images are extremely different from SAR images. This\nmay hinder the model performance on downstream tasks when adopting SFT on\nsmall-scale annotated SAR data. In this paper, an self-supervised learning\n(SSL) method of masked image modeling based on Masked Auto-Encoders (MAE) is\nproposed to learn feature representations of SAR images during the pre-training\nprocess and benefit the object detection task in SAR images of SFT. The\nevaluation experiments on the large-scale SAR object detection benchmark named\nSARDet-100k verify that the proposed method captures proper latent\nrepresentations of SAR images and improves the model generalization in\ndownstream tasks by converting the pre-trained domain from natural scenes to\nSAR images through SSL. The proposed method achieves an improvement of 1.3 mAP\non the SARDet-100k benchmark compared to only the SFT strategies.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.07331",
    "c_title":"ERANet: Edge Replacement Augmentation for Semi-Supervised Meniscus\n  Segmentation with Prototype Consistency Alignment and Conditional\n  Self-Training",
    "c_abstract":"Manual segmentation is labor-intensive, and automatic segmentation remains\nchallenging due to the inherent variability in meniscal morphology, partial\nvolume effects, and low contrast between the meniscus and surrounding tissues.\nTo address these challenges, we propose ERANet, an innovative semi-supervised\nframework for meniscus segmentation that effectively leverages both labeled and\nunlabeled images through advanced augmentation and learning strategies. ERANet\nintegrates three key components: edge replacement augmentation (ERA), prototype\nconsistency alignment (PCA), and a conditional self-training (CST) strategy\nwithin a mean teacher architecture. ERA introduces anatomically relevant\nperturbations by simulating meniscal variations, ensuring that augmentations\nalign with the structural context. PCA enhances segmentation performance by\naligning intra-class features and promoting compact, discriminative feature\nrepresentations, particularly in scenarios with limited labeled data. CST\nimproves segmentation robustness by iteratively refining pseudo-labels and\nmitigating the impact of label noise during training. Together, these\ninnovations establish ERANet as a robust and scalable solution for meniscus\nsegmentation, effectively addressing key barriers to practical implementation.\nWe validated ERANet comprehensively on 3D Double Echo Steady State (DESS) and\n3D Fast\/Turbo Spin Echo (FSE\/TSE) MRI sequences. The results demonstrate the\nsuperior performance of ERANet compared to state-of-the-art methods. The\nproposed framework achieves reliable and accurate segmentation of meniscus\nstructures, even when trained on minimal labeled data. Extensive ablation\nstudies further highlight the synergistic contributions of ERA, PCA, and CST,\nsolidifying ERANet as a transformative solution for semi-supervised meniscus\nsegmentation in medical imaging.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.16429",
    "c_title":"Sonata: Self-Supervised Learning of Reliable Point Representations",
    "c_abstract":"In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.00795",
    "c_title":"Multimodal Large Models Are Effective Action Anticipators",
    "c_abstract":"The task of long-term action anticipation demands solutions that can\neffectively model temporal dynamics over extended periods while deeply\nunderstanding the inherent semantics of actions. Traditional approaches, which\nprimarily rely on recurrent units or Transformer layers to capture long-term\ndependencies, often fall short in addressing these challenges. Large Language\nModels (LLMs), with their robust sequential modeling capabilities and extensive\ncommonsense knowledge, present new opportunities for long-term action\nanticipation. In this work, we introduce the ActionLLM framework, a novel\napproach that treats video sequences as successive tokens, leveraging LLMs to\nanticipate future actions. Our baseline model simplifies the LLM architecture\nby setting future tokens, incorporating an action tuning module, and reducing\nthe textual decoder layer to a linear layer, enabling straightforward action\nprediction without the need for complex instructions or redundant descriptions.\nTo further harness the commonsense reasoning of LLMs, we predict action\ncategories for observed frames and use sequential textual clues to guide\nsemantic understanding. In addition, we introduce a Cross-Modality Interaction\nBlock, designed to explore the specificity within each modality and capture\ninteractions between vision and textual modalities, thereby enhancing\nmultimodal tuning. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of the proposed ActionLLM framework, encouraging a promising\ndirection to explore LLMs in the context of action anticipation. Code is\navailable at https:\/\/github.com\/2tianyao1\/ActionLLM.git.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.15986",
    "c_title":"Improved Partial Differential Equation and Fast Approximation Algorithm\n  for Hazy\/Underwater\/Dust Storm Image Enhancement",
    "c_abstract":"This paper presents an improved and modified partial differential equation\n(PDE)-based de-hazing algorithm. The proposed method combines logarithmic image\nprocessing models in a PDE formulation refined with linear filter-based\noperators in either spatial or frequency domain. Additionally, a fast,\nsimplified de-hazing function approximation of the hazy image formation model\nis developed in combination with fuzzy homomorphic refinement. The proposed\nalgorithm solves the problem of image darkening and over-enhancement of edges\nin addition to enhancement of dark image regions encountered in previous\nformulations. This is in addition to avoiding enhancement of sky regions in\nde-hazed images while avoiding halo effect. Furthermore, the proposed algorithm\nis utilized for underwater and dust storm image enhancement with the\nincorporation of a modified global contrast enhancement algorithm. Experimental\ncomparisons indicate that the proposed approach surpasses a majority of the\nalgorithms from the literature based on quantitative image quality metrics.",
    "c_categories":[
      "cs.CV"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.19192",
    "c_title":"Embodying mechano-fluidic memory in soft machines to program behaviors\n  upon interactions",
    "c_abstract":"Soft machines display shape adaptation to external circumstances due to their\nintrinsic compliance. To achieve increasingly more responsive behaviors upon\ninteractions without relying on centralized computation, embodying memory\ndirectly in the machines' structure is crucial. Here, we harness the\nbistability of elastic shells to alter the fluidic properties of an enclosed\ncavity, thereby switching between stable frequency states of a locomoting\nself-oscillating machine. To program these memory states upon interactions, we\ndevelop fluidic circuits surrounding the bistable shell, with soft tubes that\nkink and unkink when externally touched. We implement circuits for both\nlong-term and short-term memory in a soft machine that switches behaviors in\nresponse to a human user and that autonomously changes direction after\ndetecting a wall. By harnessing only geometry and elasticity, embodying memory\nallows physical structures without a central brain to exhibit autonomous feats\nthat are typically reserved for computer-based robotic systems.",
    "c_categories":[
      "cond-mat.soft",
      "cs.RO",
      "physics.app-ph"
    ],
    "c_fields":[
      "Physics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.02545",
    "c_title":"Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
    "c_abstract":"We consider the problem of how many samples from a Gaussian multi-index model\nare required to weakly reconstruct the relevant index subspace. Despite its\nincreasing popularity as a testbed for investigating the computational\ncomplexity of neural networks, results beyond the single-index setting remain\nelusive. In this work, we introduce spectral algorithms based on the\nlinearization of a message passing scheme tailored to this problem. Our main\ncontribution is to show that the proposed methods achieve the optimal\nreconstruction threshold. Leveraging a high-dimensional characterization of the\nalgorithms, we show that above the critical threshold the leading eigenvector\ncorrelates with the relevant index subspace, a phenomenon reminiscent of the\nBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix\ntheory. Supported by numerical experiments and a rigorous theoretical\nframework, our work bridges critical gaps in the computational limits of weak\nlearnability in multi-index model.",
    "c_categories":[
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "c_fields":[
      "Physics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.15494",
    "c_title":"Three flavor QCD phase transition with M\\\"obius domain wall fermions",
    "c_abstract":"We present an updated study of the $N_f=3$ QCD phase transition using\nM\\\"{o}bius domain wall fermions. Simulations were performed on $N_t=12$\nlattices with aspect ratios ranging from 2 to 4 for various quark masses, at a\nlattice spacing of $a=0.1361(20)$ fm, corresponding to a temperature of 121(2)\nMeV. To clarify the nature of the phase transition, a large-volume lattice,\n$48^3 \\times 12\\times 16$, was added to analyze the volume dependence of\ndisconnected chiral susceptibility. By examining the chiral condensate,\ndisconnected chiral susceptibility, and Binder cumulant, and incorporating\nresults from $24^3 \\times 12 \\times 16$ and $36^3 \\times 12 \\times 16$ lattices\nreported in earlier studies, we observe that the transition is consistent with\na crossover at a quark mass of approximately $m_f^{\\mathrm{\\overline {MS}}}(2\\,\n\\mathrm{GeV}) \\sim 4$ MeV at this temperature. Furthermore, we discuss the\neffects of residual chiral symmetry breaking on the chiral condensate and\ndisconnected chiral susceptibility for different sizes in the 5th direction.",
    "c_categories":[
      "hep-lat",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.04337",
    "c_title":"Diagrammatic Multiplet-Sum Method (MSM) Density-Functional Theory (DFT):\n  II. Completion of the Two-Orbital Two-Electron Model (TOTEM) with an\n  Application to the Avoided Crossing in Lithium Hydride (LiH)",
    "c_abstract":"The Ziegler-Rauk-Baerends multiplet sum method (MSM) assumes that\ndensity-functional theory (DFT) provides a good description of states dominated\nby a single determinant. It then uses symmetry to add static correlation to\nDFT. In our previous article (Article I) [J. Chem. Phys. 159, 244306 (2023)],\nwe introduced diagrammatic MSM-DFT as a tool to aid in extending MSM-DFT to\ninclude the nondynamic correlation needed for making and breaking bonds even in\nthe absence of symmetry. An attractive feature of this approach is that no\nfunctional-dependent parameters need to be introduced, though choices are\nneeded in making correspondances between wave function theory (WFT) and MSM-DFT\ndiagrams. The preliminary examples in Article I used the two-orbital\ntwo-electron model (TOTEM) less completely than could have been the case.\nDiagrammatic MSM-DFT is extended here to treat the full TOTEM and it is shown\nthat the unsymmetric lithium hydride (LiH) molecule dissociates into neutral\natoms when diagrammatic MSM-DFT techniques are used to introduce a proper\ndescription of the avoided crossing between ionic bonding and covalent bonding\nstates.The method is tested for Hartree-Fock and for three functionals (LDA,\nPW91, and B3LYP). All the functionals yield similar results as should be\nexpected for a properly-formulated parameter-free theory. Agreement with\navailable estimates show that the magnitude of the coupling element introduced\nhere is excellent. However more work will be needed to obtain quantitative\nagreement between our diagrammatic MSM-DFT ground-state potential energy curve\nand that found from high-quality ab initio calculations",
    "c_categories":[
      "physics.chem-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b16"
    ],
    "b_title":[
      "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
    ],
    "b_abstract":[
      "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
    ],
    "b_categories":[
      "q-bio.TO"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.01596",
    "c_title":"More on unconstrained descriptions of Higher Spin Massless Particles",
    "c_abstract":"Here we suggest a new local action describing arbitrary integer spin-$s$\nmassless particles in terms of only two symmetric fields $\\varphi $ and\n$\\alpha$ of rank-$s$ and $(s-3)$ respectively. It is an unconstrained version\nof the Fronsdal theory where the double traceless constraint on the physical\nfield is evaded via a rank-$(s-4)$ Weyl like symmetry. The constrained higher\nspin diffeomorphism is enlarged to full diffeomorphism via the Stueckelberg\nfield $\\alpha$ through an appropriate field redefinition. After a partial gauge\nfixing where the Weyl symmetry is broken while preserving diffeomorphisms, the\nfield equations reproduce, for arbitrary integer spin-$s$, diffeomorphism\ninvariant equations of motion previously obtained via a truncation of the\nspectrum of the open bosonic string field theory in the tensionless limit. In\nthe $s=4$ case we show that the functional integration over $\\alpha$ leads to a\nunique non local Weyl and diffeomorphism invariant action given only in terms\nof the physical field $\\varphi$ whose spectrum is confirmed via an analysis of\nthe analytic structure of the spin-4 propagator for which we introduce a\ncomplete basis of projection and transition non local differential operators.\nWe also show that the elimination of $\\alpha$ after the Weyl gauge fixing leads\nto a non local diffeomorphism invariant action previously obtained in the\nliterature.",
    "c_categories":[
      "hep-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.00197",
    "c_title":"Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
    "c_abstract":"Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.01319",
    "c_title":"Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
    "c_abstract":"Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18661",
    "c_title":"Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
    "c_abstract":"Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.03661",
    "c_title":"Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
    "c_abstract":"While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.19343",
    "c_title":"Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
    "c_abstract":"Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
    "c_categories":[
      "q-bio.TO"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11748",
    "c_title":"Direct experimental observation of total absorption and loss\n  compensation using sound waves with complex frequencies",
    "c_abstract":"In this study, we experimentally investigate the application of a transient\nsignal with complex frequencies to the absorption and transmission of sound\nwaves. Indeed, the emission of a wave with an exponentially varying amplitude\nin time is analogous, in the frequency domain, to a monochromatic wave with\nspatial gain or loss. Our results show that by exciting a non-critically\ncoupled Helmholtz resonator with a wave having a growing amplitude, total\nabsorption can still be achieved. Furthermore, the lossy propagation of a\ntraveling wave in a duct is also studied, and it is shown that the losses\nembedded in the complex wavenumber can be compensated by using a transient\nsignal with decreasing amplitude. These results confirm the potential of\ncomplex frequency excitation for exploring new means of manipulating sound\nwaves to mimic gain and loss.",
    "c_categories":[
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02931",
    "c_title":"Relativistic Navier-Stokes description of the quark-gluon plasma radial\n  flow",
    "c_abstract":"The relativistic viscous hydrodynamic description of the quark-gluon plasma\nby M\\\"uller-Israel-Stewart formulations has been very successful, but despite\nthis success, these theories present limitations regarding well-posedness and\ncausality. In recent years, a well-behaved version of the relativistic\nNavier-Stokes equations has been formulated, appearing as a promising\nalternative in which those limitations are absent. Using this novel theory, we\nperform numerical simulations of a quark-gluon plasma fluid that we use to\ndescribe experimental data on the transverse momentum distribution of hadrons\nfrom central Pb-Pb collisions measured at the LHC.",
    "c_categories":[
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12172",
    "c_title":"A quadratic BSDE approach to normalization for the finite volume 2D\n  sine-Gordon model in the finite ultraviolet regime",
    "c_abstract":"This paper is devoted to a new construction of the two-dimensional\nsine-Gordon model on bounded domains by a novel normalization technique in the\nfinite ultraviolet regime. Our methodology involves a family of backward\nstochastic differential equations (BSDEs for short) driven by a cylindrical\nWiener process, whose generators are purely quadratic functions of the second\nunknown variable. The terminal conditions of the quadratic BSDEs are uniformly\nbounded and converge in probability to the real part of imaginary\nmultiplicative chaos tested against an arbitrarily given test function, which\nhelps us describe our sine-Gordon measure through some delicate estimates\nconcerning bounded mean oscillation martingales. As the ultraviolet cutoffs are\nvanishing, the quadratic BSDEs converge to a quadratic BSDE that completely\ncharacterizes the absolute continuity of our sine-Gordon measure with respect\nto the law of Gaussian free fields. Our approach can also be used effectively\nto establish the connection between our sine-Gordon measure and the scaling\nlimit of correlation functions of the critical planar XOR-Ising model and to\nprove the weak convergence of the normalized charge distributions of\ntwo-dimensional log-gases.",
    "c_categories":[
      "math-ph",
      "math.MP",
      "math.PR"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14437",
    "c_title":"Functional classification of metabolic networks",
    "c_abstract":"Chemical reaction networks underpin biological and physical phenomena across\nscales, from microbial interactions to planetary atmosphere dynamics. Bacterial\ncommunities exhibit complex competitive interactions for resources, human\norgans and tissues demonstrate specialized biochemical functions, and planetary\natmospheres are capable of displaying diverse organic and inorganic chemical\nprocesses. Despite their complexities, comparing these networks methodically\nremains a challenge due to the vast underlying degrees of freedom. In\nbiological systems, comparative genomics has been pivotal in tracing\nevolutionary trajectories and classifying organisms via DNA sequences. However,\npurely genomic classifications often fail to capture functional roles within\necological systems. Metabolic changes driven by nutrient availability highlight\nthe need for classification schemes that integrate metabolic information. Here\nwe introduce and apply a computational framework for a classification scheme of\norganisms that compares matrix representations of chemical reaction networks\nusing the Grassmann distance, corresponding to measuring distances between the\nfundamental subspaces of stoichiometric matrices. Applying this framework to\nhuman gut microbiome data confirms that metabolic distances are distinct from\nphylogenetic distances, underscoring the limitations of genetic information in\nmetabolic classification. Importantly, our analysis of metabolic distances\nreveals functional groups of organisms enriched or depleted in specific\nmetabolic processes and shows robustness to metabolically silent genetic\nperturbations. The generalizability of metabolic Grassmann distances is\nillustrated by application to chemical reaction networks in human tissue and\nplanetary atmospheres, highlighting its potential for advancing functional\ncomparisons across diverse chemical reaction systems.",
    "c_categories":[
      "physics.bio-ph",
      "q-bio.MN"
    ],
    "c_fields":[
      "Physics",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.00922",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b12"
    ],
    "b_title":[
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
    ],
    "b_abstract":[
      "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
    ],
    "b_categories":[
      "cs.CV"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17960",
    "c_title":"Comparison of near-field light intensities: plasmon nanofocusing vs\n  localized plasmon resonance",
    "c_abstract":"The localized surface plasmon resonance of metallic nanostructures produces\nstrongly localized and enhanced near-field light, significantly contributing to\nnanophotonics research and applications. Plasmon nanofocusing represents\nanother method for generating near-field light through the propagation and\ncondensation of plasmons on tapered plasmonic structures. In both methods, the\nintensity of near-field light is a critical aspect for many applications. In\nthis study, we numerically inspect and compare the intensities of near-field\nlight generated by either localized plasmon resonance or plasmon nanofocusing.\nTo account for the light-induced changes in the optical properties of plasmonic\nstructures, which in turn influence the near-field light intensity, we couple\nelectromagnetic and thermal calculations to consider in a fully self-consistent\nmanner the effects of the incident light and the light-induced temperature rise\nwithin the metal. A gold nanorod and a cone were adopted for exciting the\nlocalized plasmon resonance and plasmon nanofocusing, respectively. We find\nthat plasmon nanofocusing generates approximately 1.5 times as strong\nnear-field light as localized plasmon resonance. Our research provides a\nnecessary foundation for generating near-field light, which is crucial for\nadvancing the applications of near-field optics.",
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"Exploring the Potentials and Challenges of Using Large Language Models\n  for the Analysis of Transcriptional Regulation of Long Non-coding RNAs",
    "a_abstract":"Research on long non-coding RNAs (lncRNAs) has garnered significant attention\ndue to their critical roles in gene regulation and disease mechanisms. However,\nthe complexity and diversity of lncRNA sequences, along with the limited\nknowledge of their functional mechanisms and the regulation of their\nexpressions, pose significant challenges to lncRNA studies. Given the\ntremendous success of large language models (LLMs) in capturing complex\ndependencies in sequential data, this study aims to systematically explore the\npotential and limitations of LLMs in the sequence analysis related to the\ntranscriptional regulation of lncRNA genes. Our extensive experiments\ndemonstrated promising performance of fine-tuned genome foundation models on\nprogressively complex tasks. Furthermore, we conducted an insightful analysis\nof the critical impact of task complexity, model selection, data quality, and\nbiological interpretability for the studies of the regulation of lncRNA gene\nexpression.",
    "explanation":"Given the tremendous success of large language mod-\nels (LLMs) in capturing complex dependencies in sequential data, this study aims to systematically explore the potential and limitations of LLMs in the sequence analysis related to the transcriptional regulation of lncRNA genes. ",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":[
      "b20"
    ],
    "c_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "c_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17738",
    "c_title":"Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
    "c_abstract":"Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.03001",
    "c_title":"Multicellular self-organization in Escherichia coli",
    "c_abstract":"Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.01834",
    "c_title":"Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
    "c_abstract":"Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.05039",
    "c_title":"A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
    "c_abstract":"Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18947",
    "c_title":"Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
    "c_abstract":"A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
    "c_categories":[
      "q-bio.CB"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.11748",
    "c_title":"Direct experimental observation of total absorption and loss\n  compensation using sound waves with complex frequencies",
    "c_abstract":"In this study, we experimentally investigate the application of a transient\nsignal with complex frequencies to the absorption and transmission of sound\nwaves. Indeed, the emission of a wave with an exponentially varying amplitude\nin time is analogous, in the frequency domain, to a monochromatic wave with\nspatial gain or loss. Our results show that by exciting a non-critically\ncoupled Helmholtz resonator with a wave having a growing amplitude, total\nabsorption can still be achieved. Furthermore, the lossy propagation of a\ntraveling wave in a duct is also studied, and it is shown that the losses\nembedded in the complex wavenumber can be compensated by using a transient\nsignal with decreasing amplitude. These results confirm the potential of\ncomplex frequency excitation for exploring new means of manipulating sound\nwaves to mimic gain and loss.",
    "c_categories":[
      "physics.app-ph",
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.02931",
    "c_title":"Relativistic Navier-Stokes description of the quark-gluon plasma radial\n  flow",
    "c_abstract":"The relativistic viscous hydrodynamic description of the quark-gluon plasma\nby M\\\"uller-Israel-Stewart formulations has been very successful, but despite\nthis success, these theories present limitations regarding well-posedness and\ncausality. In recent years, a well-behaved version of the relativistic\nNavier-Stokes equations has been formulated, appearing as a promising\nalternative in which those limitations are absent. Using this novel theory, we\nperform numerical simulations of a quark-gluon plasma fluid that we use to\ndescribe experimental data on the transverse momentum distribution of hadrons\nfrom central Pb-Pb collisions measured at the LHC.",
    "c_categories":[
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.12172",
    "c_title":"A quadratic BSDE approach to normalization for the finite volume 2D\n  sine-Gordon model in the finite ultraviolet regime",
    "c_abstract":"This paper is devoted to a new construction of the two-dimensional\nsine-Gordon model on bounded domains by a novel normalization technique in the\nfinite ultraviolet regime. Our methodology involves a family of backward\nstochastic differential equations (BSDEs for short) driven by a cylindrical\nWiener process, whose generators are purely quadratic functions of the second\nunknown variable. The terminal conditions of the quadratic BSDEs are uniformly\nbounded and converge in probability to the real part of imaginary\nmultiplicative chaos tested against an arbitrarily given test function, which\nhelps us describe our sine-Gordon measure through some delicate estimates\nconcerning bounded mean oscillation martingales. As the ultraviolet cutoffs are\nvanishing, the quadratic BSDEs converge to a quadratic BSDE that completely\ncharacterizes the absolute continuity of our sine-Gordon measure with respect\nto the law of Gaussian free fields. Our approach can also be used effectively\nto establish the connection between our sine-Gordon measure and the scaling\nlimit of correlation functions of the critical planar XOR-Ising model and to\nprove the weak convergence of the normalized charge distributions of\ntwo-dimensional log-gases.",
    "c_categories":[
      "math-ph",
      "math.MP",
      "math.PR"
    ],
    "c_fields":[
      "Physics",
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.14437",
    "c_title":"Functional classification of metabolic networks",
    "c_abstract":"Chemical reaction networks underpin biological and physical phenomena across\nscales, from microbial interactions to planetary atmosphere dynamics. Bacterial\ncommunities exhibit complex competitive interactions for resources, human\norgans and tissues demonstrate specialized biochemical functions, and planetary\natmospheres are capable of displaying diverse organic and inorganic chemical\nprocesses. Despite their complexities, comparing these networks methodically\nremains a challenge due to the vast underlying degrees of freedom. In\nbiological systems, comparative genomics has been pivotal in tracing\nevolutionary trajectories and classifying organisms via DNA sequences. However,\npurely genomic classifications often fail to capture functional roles within\necological systems. Metabolic changes driven by nutrient availability highlight\nthe need for classification schemes that integrate metabolic information. Here\nwe introduce and apply a computational framework for a classification scheme of\norganisms that compares matrix representations of chemical reaction networks\nusing the Grassmann distance, corresponding to measuring distances between the\nfundamental subspaces of stoichiometric matrices. Applying this framework to\nhuman gut microbiome data confirms that metabolic distances are distinct from\nphylogenetic distances, underscoring the limitations of genetic information in\nmetabolic classification. Importantly, our analysis of metabolic distances\nreveals functional groups of organisms enriched or depleted in specific\nmetabolic processes and shows robustness to metabolically silent genetic\nperturbations. The generalizability of metabolic Grassmann distances is\nillustrated by application to chemical reaction networks in human tissue and\nplanetary atmospheres, highlighting its potential for advancing functional\ncomparisons across diverse chemical reaction systems.",
    "c_categories":[
      "physics.bio-ph",
      "q-bio.MN"
    ],
    "c_fields":[
      "Physics",
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b3"
    ],
    "b_title":[
      "Language Models are Few-Shot Learners"
    ],
    "b_abstract":[
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
    ],
    "b_categories":[
      "cs.CL"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.17960",
    "c_title":"Comparison of near-field light intensities: plasmon nanofocusing vs\n  localized plasmon resonance",
    "c_abstract":"The localized surface plasmon resonance of metallic nanostructures produces\nstrongly localized and enhanced near-field light, significantly contributing to\nnanophotonics research and applications. Plasmon nanofocusing represents\nanother method for generating near-field light through the propagation and\ncondensation of plasmons on tapered plasmonic structures. In both methods, the\nintensity of near-field light is a critical aspect for many applications. In\nthis study, we numerically inspect and compare the intensities of near-field\nlight generated by either localized plasmon resonance or plasmon nanofocusing.\nTo account for the light-induced changes in the optical properties of plasmonic\nstructures, which in turn influence the near-field light intensity, we couple\nelectromagnetic and thermal calculations to consider in a fully self-consistent\nmanner the effects of the incident light and the light-induced temperature rise\nwithin the metal. A gold nanorod and a cone were adopted for exciting the\nlocalized plasmon resonance and plasmon nanofocusing, respectively. We find\nthat plasmon nanofocusing generates approximately 1.5 times as strong\nnear-field light as localized plasmon resonance. Our research provides a\nnecessary foundation for generating near-field light, which is crucial for\nadvancing the applications of near-field optics.",
    "c_categories":[
      "physics.optics"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.18360",
    "c_title":"J&H: Evaluating the Robustness of Large Language Models Under\n  Knowledge-Injection Attacks in Legal Domain",
    "c_abstract":"As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.",
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.13628",
    "c_title":"Non-Euclidean Hierarchical Representational Learning using Hyperbolic\n  Graph Neural Networks for Environmental Claim Detection",
    "c_abstract":"Transformer-based models dominate NLP tasks like sentiment analysis, machine\ntranslation, and claim verification. However, their massive computational\ndemands and lack of interpretability pose challenges for real-world\napplications requiring efficiency and transparency. In this work, we explore\nGraph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as\nlightweight yet effective alternatives for Environmental Claim Detection,\nreframing it as a graph classification problem. We construct dependency parsing\ngraphs to explicitly model syntactic structures, using simple word embeddings\n(word2vec) for node features with dependency relations encoded as edge\nfeatures. Our results demonstrate that these graph-based models achieve\ncomparable or superior performance to state-of-the-art transformers while using\n30x fewer parameters. This efficiency highlights the potential of structured,\ninterpretable, and computationally efficient graph-based approaches.",
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.15664",
    "c_title":"Enhancing Pancreatic Cancer Staging with Large Language Models: The Role\n  of Retrieval-Augmented Generation",
    "c_abstract":"Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+\/RAG+ (NotebookLM\nwith REK), REK+\/RAG- (Gemini 2.0 Flash with REK), and REK-\/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+\/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+\/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+\/RAG- (38%) and REK-\/RAG- (35%). For TNM classification, REK+\/RAG+ attained\n80% accuracy, exceeding REK+\/RAG- (55%) and REK-\/RAG- (50%). Additionally,\nREK+\/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.",
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.16865",
    "c_title":"JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience",
    "c_abstract":"Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com\/Zzoay\/JRE-L.",
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.04295",
    "c_title":"Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
    "c_abstract":"Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps:\/\/github.com\/HenryLau7\/CFPO.",
    "c_categories":[
      "cs.CL"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.05313",
    "c_title":"Anomalous suppression of large-scale density fluctuations in classical\n  and quantum spin liquids",
    "c_abstract":"Classical spin liquids (CSLs) are intriguing states of matter that do not\nexhibit long-range magnetic order and are characterized by an extensive\nground-state degeneracy. Adding quantum fluctuations, which induce dynamics\nbetween these different classical ground states, can give rise to quantum spin\nliquids (QSLs). QSLs are highly entangled quantum phases of matter\ncharacterized by fascinating emergent properties, such as fractionalized\nexcitations and topological order. One such exotic quantum liquid is the\n$\\mathbb{Z}_2$ QSL, which can be regarded as a resonating valence bond (RVB)\nstate formed from superpositions of dimer coverings of an underlying lattice.\nIn this work, we unveil a \\textit{hidden} large-scale structural property of\narchetypal CSLs and QSLs known as hyperuniformity, i.e., normalized\ninfinite-wavelength density fluctuations are completely suppressed in these\nsystems. In particular, we first demonstrate that classical ensembles of\nclose-packed dimers and their corresponding quantum RVB states are perfectly\nhyperuniform in general. Subsequently, we focus on a ruby-lattice spin liquid\nthat was recently realized in a Rydberg-atom quantum simulator, and show that\nthe QSL remains effectively hyperuniform even in the presence of a finite\ndensity of spinon and vison excitations, as long as the dimer constraint is\nstill largely preserved. Moreover, we demonstrate that metrics based on the\nframework of hyperuniformity can be used to distinguish the QSL from other\nproximate quantum phases. These metrics can help identify potential QSL\ncandidates, which can then be further analyzed using more advanced,\ncomputationally-intensive quantum numerics to confirm their status as true\nQSLs.",
    "c_categories":[
      "cond-mat.soft",
      "cond-mat.str-el",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11155",
    "c_title":"Surface brightness-color relations for red giant branch stars using\n  asteroseismic radii and Gaia distances (the ARD method)",
    "c_abstract":"Aims. Asteroseismic radius and Gaia distance (ARD) method has been proposed\nto establish the SBCRs for late-type stars. Methods. We select Kepler RGB stars\nwith high-precision asteroseismic radii (uncertainties < 1%) and cross-match\nthem with 2MASS, APASS, and Gaia to obtain Johnson-B, Johnson-V, G, J, H, and\nKs-band photometric data. After applying selection criteria, we obtain 626 RGB\nstars to build the SBCR. Among these, 100 RGBs are used as independent\nvalidation for the distance, and the remaining samples are used to fit the\nSBCR. Results. First, using 526 targets with asteroseismic radii and Gaia\ndistances, nine SBCRs are proposed based on 2MASS (J, H, Ks), APASS (Johnson-B,\nJohnson-V), and Gaia (G) photometry. The average rms scatter in these relations\nis 0.075 mag, which corresponds to an uncertainty of approximately 3.5% in\ndistance. These relations are further validated using 100 independent samples\nwith Gaia distances, showing no bias, with a dispersion of approximately 3%.\nCompared to interferometric measurements, a systematic underestimation of 2.3%\nwas observed, and the discrepancy decreases as the angular diameter increases.\nAdditionally, the distances of eclipsing binaries in the Large Magellanic Cloud\nand Small Magellanic Cloud obtained using our SBCRs are generally consistent\nwith those measured in the literature, with a dispersion of 1% and a slight\noverestimation of 1% to 2.5%. Conclusions. The ARD method capitalizes on two\nkey advantages for precise stellar distance determination: a statistically\nrobust sample of homogeneous RGB stars with low observational costs, and\nindependent distance verification through Gaia data. Such SBCRs can be further\ncalibrated and expanded more efficiently and effectively.",
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.08061",
    "c_title":"On Fenchel c-conjugate dual problems for DC optimization: characterizing\n  weak, strong and stable strong duality",
    "c_abstract":"In this paper we present two Fenchel-type dual problems for a DC (difference\nof convex functions) optimization primal one. They have been built by means of\nthe c-conjugation scheme, a pattern of conjugation which has been shown to be\nsuitable for evenly convex functions. We study characterizations of weak,\nstrong and stable strong duality for both pairs of primal-dual problems. We\nalso give conditions which relate the existence of strong and stable strong\nduality for both pairs.",
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09790",
    "c_title":"Generation of entanglement and non-stationary states via competing\n  coherent and incoherent bosonic hopping",
    "c_abstract":"Incoherent stochastic processes added to unitary dynamics are typically\ndeemed detrimental since they are expected to diminish quantum features such as\nsuperposition and entanglement. Instead of exhibiting energy-conserving\npersistent coherent motion, the dynamics of such open systems feature, in most\ncases, a steady state, which is approached in the long-time limit from all\ninitial conditions. This can, in fact, be advantageous as it offers a mechanism\nfor the creation of robust quantum correlations on demand without the need for\nfine-tuning. Here, we show this for a system consisting of two coherently\ncoupled bosonic modes, which is a paradigmatic scenario for the realization of\nquantum resources such as squeezed entangled states. Rather counterintuitively,\nthe mere addition of incoherent hopping, which results in a statistical\ncoupling between the bosonic modes, leads to steady states with robust quantum\nentanglement and enables the emergence of persistent coherent non-stationary\nbehavior.",
    "c_categories":[
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.03522",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b20"
    ],
    "b_title":[
      "Long non-coding RNAs: definitions, functions, challenges and recommendations"
    ],
    "b_abstract":[
      "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
    ],
    "b_categories":[
      "q-bio.CB"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.14764",
    "c_title":"A shape-optimization approach for inverse diffusion problems using a\n  single boundary measurement",
    "c_abstract":"This paper explores the reconstruction of a space-dependent parameter in\ninverse diffusion problems, proposing a shape-optimization-based approach. The\nmain objective is to recover the absorption coefficient from a single boundary\nmeasurement. While conventional gradient-based methods rely on the Fr\\'{e}chet\nderivative of a cost functional with respect to the unknown parameter, we also\nutilize its shape derivative with respect to the unknown boundary interface for\nrecovery. This non-conventional approach addresses the problem of parameter\nrecovery from a single measurement, which represents the key innovation of this\nwork. Numerical experiments confirm the effectiveness of the proposed method,\neven for intricate and non-convex boundary interfaces.",
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"White-Box Diffusion Transformer for single-cell RNA-seq generation",
    "a_abstract":"As a powerful tool for characterizing cellular subpopulations and cellular\nheterogeneity, single cell RNA sequencing (scRNA-seq) technology offers\nadvantages of high throughput and multidimensional analysis. However, the\nprocess of data acquisition is often constrained by high cost and limited\nsample availability. To overcome these limitations, we propose a hybrid model\nbased on Diffusion model and White-Box transformer that aims to generate\nsynthetic and biologically plausible scRNA-seq data. Diffusion model\nprogressively introduce noise into the data and then recover the original data\nthrough a denoising process, a forward and reverse process that is particularly\nsuitable for generating complex data distributions. White-Box transformer is a\ndeep learning architecture that emphasizes mathematical interpretability. By\nminimizing the encoding rate of the data and maximizing the sparsity of the\nrepresentation, it not only reduces the computational burden, but also provides\nclear insight into underlying structure. Our White-Box Diffusion Transformer\ncombines the generative capabilities of Diffusion model with the mathematical\ninterpretability of White-Box transformer. Through experiments using six\ndifferent single-cell RNA-Seq datasets, we visualize both generated and real\ndata using t-SNE dimensionality reduction technique, as well as quantify\nsimilarity between generated and real data using various metrics to demonstrate\ncomparable performance of White-Box Diffusion Transformer and Diffusion\nTransformer in generating scRNA-seq data alongside significant improvements in\ntraining efficiency and resource utilization. Our code is available at\nhttps:\/\/github.com\/lingximamo\/White-Box-Diffusion-Transformer",
    "explanation":"As a powerful tool for characterizing cellular subpopulations and cellular heterogeneity, single cell\nRNA sequencing (scRNA-seq) technology offers advantages of high throughput and multidimensional\nanalysis. However, the process of data acquisition is often constrained by high cost and limited\nsample availability. To overcome these limitations, we propose a model based on Diffusion model\nand White-Box transformer that aims to generate synthetic and biologically plausible scRNA-seq\ndata.",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":[
      "b25"
    ],
    "c_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "c_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":true,
    "research_type":"applied"
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.19351",
    "c_title":"Neural Implicit Solution Formula for Efficiently Solving Hamilton-Jacobi\n  Equations",
    "c_abstract":"This paper presents an implicit solution formula for the Hamilton-Jacobi\npartial differential equation (HJ PDE). The formula is derived using the method\nof characteristics and is shown to coincide with the Hopf and Lax formulas in\nthe case where either the Hamiltonian or the initial function is convex. It\nprovides a simple and efficient numerical approach for computing the viscosity\nsolution of HJ PDEs, bypassing the need for the Legendre transform of the\nHamiltonian or the initial condition, and the explicit computation of\nindividual characteristic trajectories. A deep learning-based methodology is\nproposed to learn this implicit solution formula, leveraging the mesh-free\nnature of deep learning to ensure scalability for high-dimensional problems.\nBuilding upon this framework, an algorithm is developed that approximates the\ncharacteristic curves piecewise linearly for state-dependent Hamiltonians.\nExtensive experimental results demonstrate that the proposed method delivers\nhighly accurate solutions, even for nonconvex Hamiltonians, and exhibits\nremarkable scalability, achieving computational efficiency for problems up to\n40 dimensions.",
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.18580",
    "c_title":"Node Classification and Search on the Rubik's Cube Graph with GNNs",
    "c_abstract":"This study focuses on the application of deep geometric models to solve the\n3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation and\ndefining distance as the model's optimization objective. The distance\napproximation task is reformulated as a node classification problem,\neffectively addressed using Graph Neural Networks (GNNs). After training the\nmodel on a random subgraph, the predicted classes are used to construct a\nheuristic for $A^*$ search. We conclude with experiments comparing our\nheuristic to that of the DeepCubeA model.",
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.08593",
    "c_title":"Toward Universal Laws of Outlier Propagation",
    "c_abstract":"We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.",
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.17115",
    "c_title":"Evidence on the Regularisation Properties of Maximum-Entropy\n  Reinforcement Learning",
    "c_abstract":"The generalisation and robustness properties of policies learnt through\nMaximum-Entropy Reinforcement Learning are investigated on chaotic dynamical\nsystems with Gaussian noise on the observable. First, the robustness under\nnoise contamination of the agent's observation of entropy regularised policies\nis observed. Second, notions of statistical learning theory, such as complexity\nmeasures on the learnt model, are borrowed to explain and predict the\nphenomenon. Results show the existence of a relationship between\nentropy-regularised policy optimisation and robustness to noise, which can be\ndescribed by the chosen complexity measures.",
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.01488",
    "c_title":"InversionGNN: A Dual Path Network for Multi-Property Molecular\n  Optimization",
    "c_abstract":"Exploring chemical space to find novel molecules that simultaneously satisfy\nmultiple properties is crucial in drug discovery. However, existing methods\noften struggle with trading off multiple properties due to the conflicting or\ncorrelated nature of chemical properties. To tackle this issue, we introduce\nInversionGNN framework, an effective yet sample-efficient dual-path graph\nneural network (GNN) for multi-objective drug discovery. In the direct\nprediction path of InversionGNN, we train the model for multi-property\nprediction to acquire knowledge of the optimal combination of functional\ngroups. Then the learned chemical knowledge helps the inversion generation path\nto generate molecules with required properties. In order to decode the complex\nknowledge of multiple properties in the inversion path, we propose a\ngradient-based Pareto search method to balance conflicting properties and\ngenerate Pareto optimal molecules. Additionally, InversionGNN is able to search\nthe full Pareto front approximately in discrete chemical space. Comprehensive\nexperimental evaluations show that InversionGNN is both effective and\nsample-efficient in various discrete multi-objective settings including drug\ndiscovery.",
    "c_categories":[
      "cs.LG"
    ],
    "c_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2502.05313",
    "c_title":"Anomalous suppression of large-scale density fluctuations in classical\n  and quantum spin liquids",
    "c_abstract":"Classical spin liquids (CSLs) are intriguing states of matter that do not\nexhibit long-range magnetic order and are characterized by an extensive\nground-state degeneracy. Adding quantum fluctuations, which induce dynamics\nbetween these different classical ground states, can give rise to quantum spin\nliquids (QSLs). QSLs are highly entangled quantum phases of matter\ncharacterized by fascinating emergent properties, such as fractionalized\nexcitations and topological order. One such exotic quantum liquid is the\n$\\mathbb{Z}_2$ QSL, which can be regarded as a resonating valence bond (RVB)\nstate formed from superpositions of dimer coverings of an underlying lattice.\nIn this work, we unveil a \\textit{hidden} large-scale structural property of\narchetypal CSLs and QSLs known as hyperuniformity, i.e., normalized\ninfinite-wavelength density fluctuations are completely suppressed in these\nsystems. In particular, we first demonstrate that classical ensembles of\nclose-packed dimers and their corresponding quantum RVB states are perfectly\nhyperuniform in general. Subsequently, we focus on a ruby-lattice spin liquid\nthat was recently realized in a Rydberg-atom quantum simulator, and show that\nthe QSL remains effectively hyperuniform even in the presence of a finite\ndensity of spinon and vison excitations, as long as the dimer constraint is\nstill largely preserved. Moreover, we demonstrate that metrics based on the\nframework of hyperuniformity can be used to distinguish the QSL from other\nproximate quantum phases. These metrics can help identify potential QSL\ncandidates, which can then be further analyzed using more advanced,\ncomputationally-intensive quantum numerics to confirm their status as true\nQSLs.",
    "c_categories":[
      "cond-mat.soft",
      "cond-mat.str-el",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.11155",
    "c_title":"Surface brightness-color relations for red giant branch stars using\n  asteroseismic radii and Gaia distances (the ARD method)",
    "c_abstract":"Aims. Asteroseismic radius and Gaia distance (ARD) method has been proposed\nto establish the SBCRs for late-type stars. Methods. We select Kepler RGB stars\nwith high-precision asteroseismic radii (uncertainties < 1%) and cross-match\nthem with 2MASS, APASS, and Gaia to obtain Johnson-B, Johnson-V, G, J, H, and\nKs-band photometric data. After applying selection criteria, we obtain 626 RGB\nstars to build the SBCR. Among these, 100 RGBs are used as independent\nvalidation for the distance, and the remaining samples are used to fit the\nSBCR. Results. First, using 526 targets with asteroseismic radii and Gaia\ndistances, nine SBCRs are proposed based on 2MASS (J, H, Ks), APASS (Johnson-B,\nJohnson-V), and Gaia (G) photometry. The average rms scatter in these relations\nis 0.075 mag, which corresponds to an uncertainty of approximately 3.5% in\ndistance. These relations are further validated using 100 independent samples\nwith Gaia distances, showing no bias, with a dispersion of approximately 3%.\nCompared to interferometric measurements, a systematic underestimation of 2.3%\nwas observed, and the discrepancy decreases as the angular diameter increases.\nAdditionally, the distances of eclipsing binaries in the Large Magellanic Cloud\nand Small Magellanic Cloud obtained using our SBCRs are generally consistent\nwith those measured in the literature, with a dispersion of 1% and a slight\noverestimation of 1% to 2.5%. Conclusions. The ARD method capitalizes on two\nkey advantages for precise stellar distance determination: a statistically\nrobust sample of homogeneous RGB stars with low observational costs, and\nindependent distance verification through Gaia data. Such SBCRs can be further\ncalibrated and expanded more efficiently and effectively.",
    "c_categories":[
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.08061",
    "c_title":"On Fenchel c-conjugate dual problems for DC optimization: characterizing\n  weak, strong and stable strong duality",
    "c_abstract":"In this paper we present two Fenchel-type dual problems for a DC (difference\nof convex functions) optimization primal one. They have been built by means of\nthe c-conjugation scheme, a pattern of conjugation which has been shown to be\nsuitable for evenly convex functions. We study characterizations of weak,\nstrong and stable strong duality for both pairs of primal-dual problems. We\nalso give conditions which relate the existence of strong and stable strong\nduality for both pairs.",
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2501.09790",
    "c_title":"Generation of entanglement and non-stationary states via competing\n  coherent and incoherent bosonic hopping",
    "c_abstract":"Incoherent stochastic processes added to unitary dynamics are typically\ndeemed detrimental since they are expected to diminish quantum features such as\nsuperposition and entanglement. Instead of exhibiting energy-conserving\npersistent coherent motion, the dynamics of such open systems feature, in most\ncases, a steady state, which is approached in the long-time limit from all\ninitial conditions. This can, in fact, be advantageous as it offers a mechanism\nfor the creation of robust quantum correlations on demand without the need for\nfine-tuning. Here, we show this for a system consisting of two coherently\ncoupled bosonic modes, which is a paradigmatic scenario for the realization of\nquantum resources such as squeezed entangled states. Rather counterintuitively,\nthe mere addition of incoherent hopping, which results in a statistical\ncoupling between the bosonic modes, leads to steady states with robust quantum\nentanglement and enables the emergence of persistent coherent non-stationary\nbehavior.",
    "c_categories":[
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "c_fields":[
      "Physics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b0"
    ],
    "b_title":[
      "RNA-Seq: a revolutionary tool for transcriptomics"
    ],
    "b_abstract":[
      "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
    ],
    "b_categories":[
      "q-bio.GN"
    ],
    "b_fields":[
      "Quantitative Biology"
    ],
    "c_id":"2503.14764",
    "c_title":"A shape-optimization approach for inverse diffusion problems using a\n  single boundary measurement",
    "c_abstract":"This paper explores the reconstruction of a space-dependent parameter in\ninverse diffusion problems, proposing a shape-optimization-based approach. The\nmain objective is to recover the absorption coefficient from a single boundary\nmeasurement. While conventional gradient-based methods rely on the Fr\\'{e}chet\nderivative of a cost functional with respect to the unknown parameter, we also\nutilize its shape derivative with respect to the unknown boundary interface for\nrecovery. This non-conventional approach addresses the problem of parameter\nrecovery from a single measurement, which represents the key innovation of this\nwork. Numerical experiments confirm the effectiveness of the proposed method,\neven for intricate and non-convex boundary interfaces.",
    "c_categories":[
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.04822",
    "c_title":"Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
    "c_abstract":"Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.08363",
    "c_title":"TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
    "c_abstract":"Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.11831",
    "c_title":"Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
    "c_abstract":"The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.18758",
    "c_title":"Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
    "c_abstract":"Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.15472",
    "c_title":"GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
    "c_abstract":"Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
    "c_categories":[
      "q-bio.GN"
    ],
    "c_fields":[
      "Quantitative Biology"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.02441",
    "c_title":"A Statistical Hypothesis Testing Framework for Data Misappropriation\n  Detection in Large Language Models",
    "c_abstract":"Large Language Models (LLMs) are rapidly gaining enormous popularity in\nrecent years. However, the training of LLMs has raised significant privacy and\nlegal concerns, particularly regarding the inclusion of copyrighted materials\nin their training data without proper attribution or licensing, which falls\nunder the broader issue of data misappropriation. In this article, we focus on\na specific problem of data misappropriation detection, namely, to determine\nwhether a given LLM has incorporated data generated by another LLM. To address\nthis issue, we propose embedding watermarks into the copyrighted training data\nand formulating the detection of data misappropriation as a hypothesis testing\nproblem. We develop a general statistical testing framework, construct a\npivotal statistic, determine the optimal rejection threshold, and explicitly\ncontrol the type I and type II errors. Furthermore, we establish the asymptotic\noptimality properties of the proposed tests, and demonstrate its empirical\neffectiveness through intensive numerical experiments.",
    "c_categories":[
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "c_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2503.18023",
    "c_title":"Regularization of ML models for Earth systems by using longer model\n  timesteps",
    "c_abstract":"Regularization is a technique to improve generalization of machine learning\n(ML) models. A common form of regularization in the ML literature is to train\non data where similar inputs map to different outputs. This improves\ngeneralization by preventing ML models from becoming overconfident in their\npredictions. This paper shows how using longer timesteps when modelling chaotic\nEarth systems naturally leads to more of this regularization. We show this in\ntwo domains. We explain how using longer model timesteps can improve results\nand demonstrate that increased regularization is one of the causes. We explain\nwhy longer model timesteps lead to improved regularization in these systems and\npresent a procedure to pick the model timestep. We also carry out a\nbenchmarking exercise on ORAS5 ocean reanalysis data to show that a longer\nmodel timestep (28 days) than is typically used gives realistic simulations. We\nsuggest that there will be many opportunities to use this type of\nregularization in Earth system problems because the Earth system is chaotic and\nthe regularization is so easy to implement.",
    "c_categories":[
      "cs.LG",
      "nlin.CD"
    ],
    "c_fields":[
      "Physics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.13835",
    "c_title":"A Globally Convergent Method for Computing B-stationary Points of\n  Mathematical Programs with Equilibrium Constraints",
    "c_abstract":"This paper introduces a method that globally converges to B-stationary points\nof mathematical programs with equilibrium constraints (MPECs) in a finite\nnumber of iterations. B-stationarity is necessary for optimality and means that\nno feasible first-order direction improves the objective. Given a feasible\npoint of an MPEC, B-stationarity can be certified by solving a linear program\nwith equilibrium constraints (LPEC) constructed at this point. The proposed\nmethod solves a sequence of LPECs, which either certify B-stationarity or\nprovide an active-set estimate for the complementarity constraints, and\nnonlinear programs (NLPs) - referred to as branch NLPs (BNLPs) - obtained by\nfixing the active set in the MPEC. A BNLP is more regular than the MPEC, easier\nto solve, and with the correct active set, its solution coincides with the\nsolution of the MPEC. The method has two phases: the first phase identifies a\nfeasible BNLP or certifies local infeasibility, and the second phase solves a\nfinite sequence of BNLPs until a B-stationary point of the MPEC is found. The\npaper provides a detailed convergence analysis and discusses implementation\ndetails. In addition, extensive numerical experiments and an open-source\nsoftware implementation are provided. The experiments demonstrate that the\nproposed method is more robust and faster than relaxation-based methods, while\nalso providing a certificate of B-stationarity without requiring the usual\ntoo-restrictive assumptions.",
    "c_categories":[
      "math.OC"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2501.18439",
    "c_title":"MolGraph-xLSTM: A graph-based dual-level xLSTM framework with multi-head\n  mixture-of-experts for enhanced molecular representation and interpretability",
    "c_abstract":"Predicting molecular properties is essential for drug discovery, and\ncomputational methods can greatly enhance this process. Molecular graphs have\nbecome a focus for representation learning, with Graph Neural Networks (GNNs)\nwidely used. However, GNNs often struggle with capturing long-range\ndependencies. To address this, we propose MolGraph-xLSTM, a novel graph-based\nxLSTM model that enhances feature extraction and effectively models molecule\nlong-range interactions.\n  Our approach processes molecular graphs at two scales: atom-level and\nmotif-level. For atom-level graphs, a GNN-based xLSTM framework with jumping\nknowledge extracts local features and aggregates multilayer information to\ncapture both local and global patterns effectively. Motif-level graphs provide\ncomplementary structural information for a broader molecular view. Embeddings\nfrom both scales are refined via a multi-head mixture of experts (MHMoE),\nfurther enhancing expressiveness and performance.\n  We validate MolGraph-xLSTM on 10 molecular property prediction datasets,\ncovering both classification and regression tasks. Our model demonstrates\nconsistent performance across all datasets, with improvements of up to 7.03% on\nthe BBBP dataset for classification and 7.54% on the ESOL dataset for\nregression compared to baselines. On average, MolGraph-xLSTM achieves an AUROC\nimprovement of 3.18\\% for classification tasks and an RMSE reduction of 3.83\\%\nacross regression datasets compared to the baseline methods. These results\nconfirm the effectiveness of our model, offering a promising solution for\nmolecular representation learning for drug discovery.",
    "c_categories":[
      "cs.LG",
      "q-bio.BM"
    ],
    "c_fields":[
      "Quantitative Biology",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "y_true":false,
    "research_type":null
  },
  {
    "id":"2411.06785",
    "a_title":"",
    "a_abstract":"",
    "explanation":"",
    "b_id":[
      "b25"
    ],
    "b_title":[
      "White-Box Transformers via Sparse Rate Reduction"
    ],
    "b_abstract":[
      "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
    ],
    "b_categories":[
      "cs.LG"
    ],
    "b_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "c_id":"2502.07756",
    "c_title":"Nonabelian Yang-Mills-Higgs and Plateau's problem in codimension three",
    "c_abstract":"We investigate the asymptotic behavior of the\n$\\mathrm{SU}(2)$-Yang-Mills-Higgs energy $E(\\Phi,A)=\\int_M|d_A\\Phi|^2+|F_A|^2$\nin the large mass limit, proving convergence to the codimension-three area\nfunctional in the sense of De Giorgi's $\\Gamma$-convergence. More precisely,\nfor a compact manifold with boundary $M$ and any family of pairs\n$\\Phi_m\\in\\Omega^0(M;\\mathfrak{su}(2))$ and $A_m\\in\n\\Omega^1(M;\\mathfrak{su}(2))$ indexed by a mass parameter $m\\to\\infty$,\nsatisfying $$E(\\Phi_m,A_m)\\leq\nCm\\quad\\text{and}\\quad\\lim_{m\\to\\infty}\\frac{1}{m}\\int_M(m-|\\Phi_m|)^2=0,$$ we\nprove that the $(n-3)$-currents dual to $\\frac{1}{2\\pi\nm}\\mathrm{tr}(d_{A_m}\\Phi_m\\wedge F_{A_m})$ converge subsequentially to a\nrelative integral $(n-3)$-cycle $T$ of mass \\begin{equation}\n  \\mathbb{M}(T)\\leq \\liminf_{m\\to\\infty}\\frac{1}{4\\pi m}E(\\Phi_m,A_m),\n\\end{equation} and show conversely that any integral $(n-3)$-current $T$ with\n$[T]=0\\in H_{n-3}(M,\\partial M;\\mathbb{Z})$ admits such an approximation, with\nequality in the above inequality. In the special case of pairs $(\\Phi_m,A_m)$\nsatisfying the generalized monopole equation $*d_{A_m}\\Phi_m=F_{A_m}\\wedge\n\\Theta$ for a calibration form $\\Theta\\in \\Omega^{n-3}(M)$, we deduce that the\nlimit $\\nu=\\lim_{m\\to\\infty}\\frac{1}{2\\pi m}|d_{A_m}\\Phi_m|^2$ of the Dirichlet\nenergy measures satisfies $\\nu\\leq |T|$, with equality if and only if $T$ is\ncalibrated by $\\Theta$, giving evidence for predictions of Donaldson-Segal in\nthe settings of $G_2$-manifolds and Calabi-Yau $3$-folds.",
    "c_categories":[
      "math.AP",
      "math.DG"
    ],
    "c_fields":[
      "Mathematics and Statistics"
    ],
    "y_true":false,
    "research_type":null
  }
]