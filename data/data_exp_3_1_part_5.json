[
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Distributed Sky Imaging Radiometry and Tomography",
    "start_abstract":"The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data.",
    "start_categories":[
      "astro-ph.EP"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Variable Imaging Projection Cloud Scattering Tomography"
      ],
      "abstract":[
        "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Codes with symmetric distances",
        "RobotIQ: Empowering Mobile Robots with Human-Level Planning for\n  Real-World Execution",
        "UniDemoir\\'e: Towards Universal Image Demoir\\'eing with Data Generation\n  and Synthesis",
        "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching",
        "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Recovering Partially Corrupted Major Objects through Tri-modality Based\n  Image Completion",
        "Designing VR Simulation System for Clinical Communication Training with\n  LLMs-Based Embodied Conversational Agents",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "On the approaching geodesics property",
        "Multilevel Generative Samplers for Investigating Critical Phenomena",
        "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
        "Multi-View Depth Consistent Image Generation Using Generative AI Models:\n  Application on Architectural Design of University Buildings",
        "Universal Chern classes on the moduli of bundles",
        "Deep ARTMAP: Generalized Hierarchical Learning with Adaptive Resonance\n  Theory",
        "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
        "GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals",
        "A Simple Aerial Detection Baseline of Multimodal Language Models",
        "PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant",
        "FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs",
        "Canonical forms of oriented matroids",
        "XTS mode revisited: high hopes for key scopes?",
        "Efficient Coordination and Synchronization of Multi-Robot Systems Under\n  Recurring Linear Temporal Logic",
        "An Efficient Row-Based Sparse Fine-Tuning",
        "Maximal rigid modules over a gentle algebra and applications to higher\n  Auslander-Reiten theory",
        "Mechanics on flag manifolds",
        "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models",
        "Prosperity: Accelerating Spiking Neural Networks via Product Sparsity"
      ],
      "abstract":[
        "For a code $C$ in a space with maximal distance $n$, we say that $C$ has\nsymmetric distances if its distance set $S(C)$ is symmetric with respect to $n\n\/ 2$. In this paper, we prove that if $C$ is a binary code with length $2n$,\nconstant weight $n$ and symmetric distances, then \\[\n  |C| \\leq \\binom{2 n - 1}{|S(C)|}. \\] This result can be interpreted using the\nlanguage of Johnson association schemes. More generally, we give a framework to\nstudy codes with symmetric distances in Q-bipartite Q-polynomial association\nschemes, and provide upper bounds for such codes. Moreover, we use number\ntheoretic techniques to determine when the equality holds.",
        "This paper introduces RobotIQ, a framework that empowers mobile robots with\nhuman-level planning capabilities, enabling seamless communication via natural\nlanguage instructions through any Large Language Model. The proposed framework\nis designed in the ROS architecture and aims to bridge the gap between humans\nand robots, enabling robots to comprehend and execute user-expressed text or\nvoice commands. Our research encompasses a wide spectrum of robotic tasks,\nranging from fundamental logical, mathematical, and learning reasoning for\ntransferring knowledge in domains like navigation, manipulation, and object\nlocalization, enabling the application of learned behaviors from simulated\nenvironments to real-world operations. All encapsulated within a modular\ncrafted robot library suite of API-wise control functions, RobotIQ offers a\nfully functional AI-ROS-based toolset that allows researchers to design and\ndevelop their own robotic actions tailored to specific applications and robot\nconfigurations. The effectiveness of the proposed system was tested and\nvalidated both in simulated and real-world experiments focusing on a home\nservice scenario that included an assistive application designed for elderly\npeople. RobotIQ with an open-source, easy-to-use, and adaptable robotic library\nsuite for any robot can be found at https:\/\/github.com\/emmarapt\/RobotIQ.",
        "Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.",
        "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.",
        "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https:\/\/research.zenseact.com\/publications\/gasp\/.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Diffusion models have become widely adopted in image completion tasks, with\ntext prompts commonly employed to ensure semantic coherence by providing\nhigh-level guidance. However, a persistent challenge arises when an object is\npartially obscured in the damaged region, yet its remaining parts are still\nvisible in the background. While text prompts offer semantic direction, they\noften fail to precisely recover fine-grained structural details, such as the\nobject's overall posture, ensuring alignment with the visible object\ninformation in the background. This limitation stems from the inability of text\nprompts to provide pixel-level specificity. To address this, we propose\nsupplementing text-based guidance with a novel visual aid: a casual sketch,\nwhich can be roughly drawn by anyone based on visible object parts. This sketch\nsupplies critical structural cues, enabling the generative model to produce an\nobject structure that seamlessly integrates with the existing background. We\nintroduce the Visual Sketch Self-Aware (VSSA) model, which integrates the\ncasual sketch into each iterative step of the diffusion process, offering\ndistinct advantages for partially corrupted scenarios. By blending\nsketch-derived features with those of the corrupted image, and leveraging text\nprompt guidance, the VSSA assists the diffusion model in generating images that\npreserve both the intended object semantics and structural consistency across\nthe restored objects and original regions. To support this research, we created\ntwo datasets, CUB-sketch and MSCOCO-sketch, each combining images, sketches,\nand text. Extensive qualitative and quantitative experiments demonstrate that\nour approach outperforms several state-of-the-art methods.",
        "VR simulation in Health Professions (HP) education demonstrates huge\npotential, but fixed learning content with little customization limits its\napplication beyond lab environments. To address these limitations in the\ncontext of VR for patient communication training, we conducted a user-centered\nstudy involving semi-structured interviews with advanced HP students to\nunderstand their challenges in clinical communication training and perceptions\nof VR-based solutions. From this, we derived design insights emphasizing the\nimportance of realistic scenarios, simple interactions, and unpredictable\ndialogues. Building on these insights, we developed the Virtual AI Patient\nSimulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and\nEmbodied Conversational Agents (ECAs), supporting dynamic and customizable\npatient interactions for immersive learning. We also provided an example of how\nclinical professors could use user-friendly design forms to create personalized\nscenarios that align with course objectives in VAPS and discuss future\nimplications of integrating AI-driven technologies into VR education.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We survey some recent results and open questions on the approaching geodesics\nproperty and its application to the study of the Gromov and horofunction\ncompactifications of a proper geodesic Gromov metric space. We obtain results\non the dynamics of isometries and we exhibit an example of a Gromov hyperbolic\ndomain of $\\mathbb{C}$ which does not satisfy the approaching geodesic\nproperty.",
        "Investigating critical phenomena or phase transitions is of high interest in\nphysics and chemistry, for which Monte Carlo (MC) simulations, a crucial tool\nfor numerically analyzing macroscopic properties of given systems, are often\nhindered by an emerging divergence of correlation length -- known as scale\ninvariance at criticality (SIC) in the renormalization group theory. SIC causes\nthe system to behave the same at any length scale, from which many existing\nsampling methods suffer: long-range correlations cause critical slowing down in\nMarkov chain Monte Carlo (MCMC), and require intractably large receptive fields\nfor generative samplers. In this paper, we propose a Renormalization-informed\nGenerative Critical Sampler (RiGCS) -- a novel sampler specialized for\nnear-critical systems, where SIC is leveraged as an advantage rather than a\nnuisance. Specifically, RiGCS builds on MultiLevel Monte Carlo (MLMC) with Heat\nBath (HB) algorithms, which perform ancestral sampling from low-resolution to\nhigh-resolution lattice configurations with site-wise-independent conditional\nHB sampling. Although MLMC-HB is highly efficient under exact SIC, it suffers\nfrom a low acceptance rate under slight SIC violation. Notably, SIC violation\nalways occurs in finite-size systems, and may induce long-range and\nhigher-order interactions in the renormalized distributions, which are not\nconsidered by independent HB samplers. RiGCS enhances MLMC-HB by replacing a\npart of the conditional HB sampler with generative models that capture those\nresidual interactions and improve the sampling efficiency. Our experiments show\nthat the effective sample size of RiGCS is a few orders of magnitude higher\nthan state-of-the-art generative model baselines in sampling configurations for\n128x128 two-dimensional Ising systems.",
        "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
        "In the early stages of architectural design, shoebox models are typically\nused as a simplified representation of building structures but require\nextensive operations to transform them into detailed designs. Generative\nartificial intelligence (AI) provides a promising solution to automate this\ntransformation, but ensuring multi-view consistency remains a significant\nchallenge. To solve this issue, we propose a novel three-stage consistent image\ngeneration framework using generative AI models to generate architectural\ndesigns from shoebox model representations. The proposed method enhances\nstate-of-the-art image generation diffusion models to generate multi-view\nconsistent architectural images. We employ ControlNet as the backbone and\noptimize it to accommodate multi-view inputs of architectural shoebox models\ncaptured from predefined perspectives. To ensure stylistic and structural\nconsistency across multi-view images, we propose an image space loss module\nthat incorporates style loss, structural loss and angle alignment loss. We then\nuse depth estimation method to extract depth maps from the generated multi-view\nimages. Finally, we use the paired data of the architectural images and depth\nmaps as inputs to improve the multi-view consistency via the depth-aware 3D\nattention module. Experimental results demonstrate that the proposed framework\ncan generate multi-view architectural images with consistent style and\nstructural coherence from shoebox model inputs.",
        "The goal of this paper is to construct universal cohomology classes on the\nmoduli space of stable bundles over a curve when it is not a fine moduli space,\ni.e. when the rank and degree are not coprime. More precisely, we show that\ncertain Chern classes of the universal bundle on the product of the curve with\nthe moduli stack of bundles lift to the product of the curve with the moduli\nspace of stable bundles.",
        "This paper presents Deep ARTMAP, a novel extension of the ARTMAP architecture\nthat generalizes the self-consistent modular ART (SMART) architecture to enable\nhierarchical learning (supervised and unsupervised) across arbitrary\ntransformations of data. The Deep ARTMAP framework operates as a divisive\nclustering mechanism, supporting an arbitrary number of modules with\ncustomizable granularity within each module. Inter-ART modules regulate the\nclustering at each layer, permitting unsupervised learning while enforcing a\none-to-many mapping from clusters in one layer to the next. While Deep ARTMAP\nreduces to both ARTMAP and SMART in particular configurations, it offers\nsignificantly enhanced flexibility, accommodating a broader range of data\ntransformations and learning modalities.",
        "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.",
        "This study introduces a novel application of a Generative Pre-trained\nTransformer (GPT) model tailored for photoplethysmography (PPG) signals,\nserving as a foundation model for various downstream tasks. Adapting the\nstandard GPT architecture to suit the continuous characteristics of PPG\nsignals, our approach demonstrates promising results. Our models are\npre-trained on our extensive dataset that contains more than 200 million 30s\nPPG samples. We explored different supervised fine-tuning techniques to adapt\nour model to downstream tasks, resulting in performance comparable to or\nsurpassing current state-of-the-art (SOTA) methods in tasks like atrial\nfibrillation detection. A standout feature of our GPT model is its inherent\ncapability to perform generative tasks such as signal denoising effectively,\nwithout the need for further fine-tuning. This success is attributed to the\ngenerative nature of the GPT framework.",
        "The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps:\/\/github.com\/Li-Qingyun\/mllm-mmrotate.",
        "In the paper, we introduce a paper reading assistant, PaperHelper, a potent\ntool designed to enhance the capabilities of researchers in efficiently\nbrowsing and understanding scientific literature. Utilizing the\nRetrieval-Augmented Generation (RAG) framework, PaperHelper effectively\nminimizes hallucinations commonly encountered in large language models (LLMs),\noptimizing the extraction of accurate, high-quality knowledge. The\nimplementation of advanced technologies such as RAFT and RAG Fusion\nsignificantly boosts the performance, accuracy, and reliability of the\nLLMs-based literature review process. Additionally, PaperHelper features a\nuser-friendly interface that facilitates the batch downloading of documents and\nuses the Mermaid format to illustrate structural relationships between\ndocuments. Experimental results demonstrate that PaperHelper, based on a\nfine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8\nseconds, outperforming the basic RAG model by 7\\% in F1 Score.",
        "FormalSpecCpp is a dataset designed to fill the gap in standardized\nbenchmarks for verifying formal specifications in C++ programs. To the best of\nour knowledge, this is the first comprehensive collection of C++ programs with\nwell-defined preconditions and postconditions. It provides a structured\nbenchmark for evaluating specification inference tools and testing theaccuracy\nof generated specifications. Researchers and developers can use this dataset to\nbenchmark specification inference tools,fine-tune Large Language Models (LLMs)\nfor automated specification generation, and analyze the role of formal\nspecifications in improving program verification and automated testing. By\nmaking this dataset publicly available, we aim to advance research in program\nverification, specification inference, and AI-assisted software development.\nThe dataset and the code are available at\nhttps:\/\/github.com\/MadhuNimmo\/FormalSpecCpp.",
        "Positive geometries are semialgebraic sets equipped with a canonical\ndifferential form whose residues mirror the boundary structure of the geometry.\nEvery full-dimensional projective polytope is a positive geometry. Motivated by\nthe canonical forms of polytopes, we construct a canonical form for any tope of\nan oriented matroid, inside the Orlik--Solomon algebra of the underlying\nmatroid. Using these canonical forms, we construct bases for the Orlik--Solomon\nalgebra of a matroid, and for the Aomoto cohomology. These bases of canonical\nforms are a foundational input in the theory of matroid amplitudes introduced\nby the second author.",
        "This paper concisely summarizes the XTS block encryption mode for storage\nsector-based encryption applications and clarifies its limitations. In\nparticular, we aim to provide a unified basis for much needed discussions about\nthe newly proposed key scope change to the IEEE 1619 standard.",
        "We consider multi-robot systems under recurring tasks formalized as linear\ntemporal logic (LTL) specifications. To solve the planning problem efficiently,\nwe propose a bottom-up approach combining offline plan synthesis with online\ncoordination, dynamically adjusting plans via real-time communication. To\naddress action delays, we introduce a synchronization mechanism ensuring\ncoordinated task execution, leading to a multi-agent coordination and\nsynchronization framework that is adaptable to a wide range of multi-robot\napplications. The software package is developed in Python and ROS2 for broad\ndeployment. We validate our findings through lab experiments involving nine\nrobots showing enhanced adaptability compared to previous methods.\nAdditionally, we conduct simulations with up to ninety agents to demonstrate\nthe reduced computational complexity and the scalability features of our work.",
        "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning (SFT)\nand Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SFT framework, based on ideas from neural network pruning. At\na high level, we first identify \"important\" neurons\/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Using experiments on common language tasks, we\ndemonstrate that our method significantly improves the memory efficiency of SFT\nwithout increasing training time complexity and implementation complexity,\nwhile achieving accuracy comparable to state-of-the-art methods such as LoRA\nand its variants.",
        "We construct a bijective correspondence between the set of rigid modules over\na gentle algebra and the set of admissible arc systems on the associated\ncoordinated-marked surface. In particular, a maximal rigid module aligns with\nan equivalence class of admissible $5$-partial triangulations, which is an\n(admissible) set of simple arcs dissecting the surface into $s$-gons with\n$3\\leqslant s\\leqslant 5$. Furthermore, the rank of the maximal rigid module is\nequal to the rank of the algebra plus the number of internal $4$-gons and\n$5$-gons in the associated $5$-partial triangulation.\n  Subsequently, these results facilitate an exploration of the higher\nAuslander-Reiten theory for gentle algebras with global dimension $n$. The\n$\\tau_m$-closures of injective modules are realized as admissible\n$(m+2)$-partial triangulations, where $\\tau_m$ are higher Auslander-Reiten\ntranslations with $2\\leqslant m \\leqslant n$. Finally, we provide a complete\nclassification of gentle algebras that are $\\tau_n$-finite or $n$-complete\nintroduced by Iyama [I11].",
        "We study the connection between $\\mathrm{SU}(n)$ spin chains and\none-dimensional sigma models on flag manifolds. Using this connection, we\ncalculate the spectrum of the Laplace-Beltrami operator and geodesics for a\nparticular class of metrics on $\\mathbb{CP}^1$ and $\\mathcal{F}_3$, which is a\nmanifold of complete flags in $\\mathbb{C}^3$.",
        "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition\/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
        "Spiking Neural Networks (SNNs) are highly efficient due to their spike-based\nactivation, which inherently produces bit-sparse computation patterns. Existing\nhardware implementations of SNNs leverage this sparsity pattern to avoid\nwasteful zero-value computations, yet this approach fails to fully capitalize\non the potential efficiency of SNNs. This study introduces a novel sparsity\nparadigm called Product Sparsity, which leverages combinatorial similarities\nwithin matrix multiplication operations to reuse the inner product result and\nreduce redundant computations. Product Sparsity significantly enhances sparsity\nin SNNs without compromising the original computation results compared to\ntraditional bit sparsity methods. For instance, in the SpikeBERT SNN model,\nProduct Sparsity achieves a density of only $1.23\\%$ and reduces computation by\n$11\\times$, compared to bit sparsity, which has a density of $13.19\\%$. To\nefficiently implement Product Sparsity, we propose Prosperity, an architecture\nthat addresses the challenges of identifying and eliminating redundant\ncomputations in real-time. Compared to prior SNN accelerator PTB and the A100\nGPU, Prosperity achieves an average speedup of $7.4\\times$ and $1.8\\times$,\nrespectively, along with energy efficiency improvements of $8.0\\times$ and\n$193\\times$, respectively. The code for Prosperity is available at\nhttps:\/\/github.com\/dubcyfor3\/Prosperity."
      ]
    }
  },
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Variable Imaging Projection Cloud Scattering Tomography",
    "start_abstract":"Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Distributed Sky Imaging Radiometry and Tomography"
      ],
      "abstract":[
        "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
      ],
      "categories":[
        "astro-ph.EP"
      ]
    },
    "list":{
      "title":[
        "Signless Laplacian State Transfer on Vertex Complemented Coronae",
        "Testing and Combining Transient Spectral Classification Tools on\n  4MOST-like Blended Spectra",
        "Highly Entangled Magnetodielectric and Magnetostriction effects, and\n  Spin-Phonon coupling in the Antiferromagnetic Ni$_2$ScSbO$_6$",
        "Sky localization of gravitational waves from eccentric binaries",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "J-braid groups are torus necklace groups",
        "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
        "Revealing higher-order neural representations with generative artificial\n  intelligence",
        "Are compact open-charm tetraquarks consistent with recent lattice\n  results?",
        "Partial Condition Numbers for Double Saddle Point Problems",
        "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish",
        "Regularity for free boundary surfaces minimizing degenerate area\n  functionals",
        "A parameterization method for quasi-periodic systems with noise:\n  computation of random invariant tori",
        "Really perverse periodic solutions of the planar N-body problem",
        "Topological superconductivity in hourglass Dirac chain metals (Ti,\n  Hf)IrGe",
        "High-Dimensional Bayesian Optimization Using Both Random and Supervised\n  Embeddings",
        "Smooth Calabi-Yau varieties with large index and Betti numbers",
        "On 1-11-representability and multi-1-11-representability of graphs",
        "The Jordan decomposition and Kaplansky's second test problem for\n  Hermitian holomorphic vector bundles",
        "Detection of [C\\,{\\sc i}] Emission in Nebular Spectra of a Peculiar Type\n  Ia Supernova 2022pul",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "ALMAGAL III. Compact source catalog: Fragmentation statistics and\n  physical evolution of the core population",
        "On the harmonic generalized Cauchy-Kovalevskaya extension and its\n  connection with the Fueter-Sce theorem",
        "ADAGE: A generic two-layer framework for adaptive agent based modelling",
        "Overfitting Regimes of Nadaraya-Watson Interpolators",
        "The Paradox of Intervention: Resilience in Adaptive Multi-Role\n  Coordination Networks",
        "Motivic stable stems and Galois approximations of cellular motivic\n  categories",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "Approximating the Total Variation Distance between Gaussians"
      ],
      "abstract":[
        "Given a graph $G$ with vertex set $V(G)=\\{v_1,v_2,\\ldots,v_{n_1}\\}$ and a\ngraph $H$ of order $n_2$, the vertex complemented corona, denoted by\n$G\\tilde{\\circ}{H}$, is the graph produced by copying $H$ $n_1$ times, with the\n$i$-th copy of $H$ corresponding to the vertex $v_i$, and then adding edges\nbetween any vertex in $V(G)\\setminus\\{v_{i}\\}$ and any vertex of the $i$-th\ncopy of $H$. The present article deals with quantum state transfer of vertex\ncomplemented coronae concerning signless Laplacian matrix. Our research\ninvestigates conditions in which signless Laplacian perfect state transfer\nexists or not on vertex complemented coronae. Additionally, we also provide\nsome mild conditions for the class of graphs under consideration that allow\nsignless Laplacian pretty good state transfer.",
        "With the 4-meter Multi-Object Spectroscopic Telescope (4MOST) expected to\nprovide an influx of transient spectra when it begins observations in early\n2026 we consider the potential for real-time classification of these spectra.\nWe investigate three extant spectroscopic transient classifiers: the Deep\nAutomated Supernova and Host classifier (DASH), Next Generation SuperFit (NGSF)\nand SuperNova IDentification (SNID), with a focus on comparing the efficiency\nand purity of the transient samples they produce. We discuss our method for\nsimulating realistic, 4MOST-like, host-galaxy contaminated spectra and\ndetermining quality cuts for each classifier used to ensure pure SN Ia samples\nwhile maintaining efficient classification in other transient classes. We\ninvestigate the classifiers individually and in combinations. We find that a\ncombination of DASH and NGSF can produce a SN Ia sample with a purity of 99.9%\nwhile successfully classifying 70% of SNe Ia. However, it struggles to classify\nnon-SN Ia transients. We investigate photometric cuts to transient magnitude\nand transient flux fraction, finding that both can be used to improve transient\nclassification efficiencies by 7--25% depending on the transient subclass.\nFinally, we present an example classification plan for live classification and\nthe predicted purities and efficiencies across five transient classes: Ia, Ibc,\nII, superluminous and non-supernova transients.",
        "Magnetic systems with noncentrosymmetric crystal structures are renowned for\ntheir complex magnetic ordering and diverse and fascinating physical\nproperties. In this report, we provide a comprehensive study of the chiral\nmagnetic system Ni$_2$ScSbO$_6$, which exhibits a robust incommensurate\nlong-range antiferromagnetic spin ordering at a temperature of $T_N = 62$~K, as\nrevealed by bulk magnetization, specific heat, and neutron diffraction studies.\nThis magnetic ordering triggers a series of intriguing phenomena, including\nprominent magnetodielectric coupling manifested by a dielectric peak at $T_N$,\nsignificant spin-phonon coupling resulting in strong phonon renormalization\ncharacterized by anomalous softening of various Raman modes, and a remarkable\nvolume magnetostriction effect probed by high-resolution synchrotron X-ray\ndiffraction. These phenomena are intricately interlinked, positioning the\npresent system as a rare and interesting material.",
        "We demonstrate that the orbital eccentricity in compact binary mergers can be\nused to improve their sky localization using gravitational wave observations.\nExisting algorithms that conduct the localizations are not optimized for\neccentric sources. We use a semi-Bayesian technique to carry out localizations\nof simulated sources recovered using a matched-filter search. Through these\nsimulations, we find that if a non-negligible eccentricity is obtained during\nthe detection, an eccentricity-optimized algorithm can significantly improve\nthe localization areas compared to the existing methods. We also lay out the\nfoundation for an eccentric early-warning system using the matched-filter\nsearch. The potential impact on the early-warning localization is investigated.\nWe indicate a few possible cases of improvements while accounting for\neccentricity toward any detectable eccentric neutron star binaries in the\nforthcoming observing scenarios of ground-based detectors. Improved\nlocalizations can be useful in effectually utilizing the capabilities of the\nfollow-up facilities.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "We construct a family of links we call torus necklaces for which the link\ngroups are precisely the braid groups of generalised $J$-reflection groups.\nMoreover, this correspondence exhibits the meridians of the aforementioned link\ngroups as braid reflections. In particular, this construction generalises to\nall irreducible rank two complex reflection groups a well-known correspondence\nbetween some rank two complex braid groups and some torus knot groups. In\naddition, as abstract groups, we show that the family of link groups associated\nto Seifert links coincides with the family of circular groups. This shows that\nevery time a link group has a non-trivial center, it is a Garside group.",
        "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
        "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
        "We argue that the hypothesis that positive-parity charm meson resonances\nexhibit a compact tetraquark structure has some clear tension with recent\nlattice results for the $S$-wave $\\pi D$ system for an SU(3) flavor symmetric\nsetting. In particular, we show that such a diquark--anti-diquark tetraquark\nscenario would call for the presence of a state in the flavor\n$[{\\mathbf{\\overline{15}}}]$ representation, not seen in the lattice analysis.\nMoreover, we show that analogous lattice data in the axial-vector channel are\neven more sensitive to the internal structure of these very interesting states.",
        "This paper presents a unified framework for investigating the partial\ncondition number (CN) of the solution of double saddle point problems (DSPPs)\nand provides closed-form expressions for it. This unified framework encompasses\nthe well-known partial normwise CN (NCN), partial mixed CN (MCN) and partial\ncomponentwise CN (CCN) as special cases. Furthermore, we derive sharp upper\nbounds for the partial NCN, MCN and CCN, which are computationally efficient\nand free of expensive Kronecker products. By applying perturbations that\npreserve the structure of the block matrices of the DSPPs, we analyze the\nstructured partial NCN, MCN and CCN when the block matrices exhibit linear\nstructures. By leveraging the relationship between DSPP and equality\nconstrained indefinite least squares (EILS) problems, we recover the partial\nCNs for the EILS problem. Numerical results confirm the sharpness of the\nderived upper bounds and demonstrate their effectiveness in estimating the\npartial CNs.",
        "Data-driven benchmarks have led to significant progress in key scientific\nmodeling domains including weather and structural biology. Here, we introduce\nthe Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on\nthe problem of predicting cellular-resolution neural activity throughout an\nentire vertebrate brain. The benchmark is based on a novel dataset containing\n4d light-sheet microscopy recordings of over 70,000 neurons in a larval\nzebrafish brain, along with motion stabilized and voxel-level cell\nsegmentations of these data that facilitate development of a variety of\nforecasting methods. Initial results from a selection of time series and\nvolumetric video modeling approaches achieve better performance than naive\nbaseline methods, but also show room for further improvement. The specific\nbrain used in the activity recording is also undergoing synaptic-level\nanatomical mapping, which will enable future integration of detailed structural\ninformation into forecasting methods.",
        "We establish an epsilon-regularity theorem at points in the free boundary of\nalmost-minimizers of the energy\n$\\mathrm{Per}_{w}(E)=\\int_{\\partial^*E}w\\,\\mathrm{d} {\\mathscr{H}}^{n-1}$,\nwhere $w$ is a weight asymptotic to $d(\\cdot,\\mathbb{R}^n\\setminus\\Omega)^a$\nnear $\\partial\\Omega$ and $a>0$.\n  This implies that the boundaries of almost-minimizers are\n$C^{1,\\gamma_0}$-surfaces that touch $\\partial \\Omega$ orthogonally, up to a\nSingular Set $\\mathrm{Sing}(\\partial E)$ whose Hausdorff dimension satisfies\nthe bound\n  $d_{\\mathscr{H}}(\\mathrm{Sing}(\\partial E)) \\leq n +a -(5+\\sqrt{8})$.",
        "This work is devoted to studying normally hyperbolic invariant manifolds\n(NHIMs) for a class of quasi-periodically forced systems subject to additional\nstochastic noise. These systems can be understood as skew-product systems. The\nexistence of NHIMs is established by developing a parameterization method in\nrandom settings and applying the Implicit Function Theorem in appropriate\nBanach spaces. Based on this, we propose a numerical algorithm to compute the\nstatistics of NHIMs and Lyapunov exponents.",
        "Examples are given of solutions of the planar N-body problem which remain the\nsame for at least two systems of masses with the same sum and same center of\nmass. The least value of N achieved up to now with this property is 474, a\nnumber which had been announced in the first author's thesis.",
        "Realizing topological superconductivity in stoichiometric materials is a key\nchallenge in condensed matter physics. Here, we report the discovery of ternary\ngermanide superconductors, $M$IrGe ($M$ = Ti, Hf), as prime candidates for\ntopological superconductivity, predicted to exhibit nonsymmorphic\nsymmetry-protected hourglass Dirac chains. Using comprehensive thermodynamic\nand muon-spin rotation\/relaxation ($\\mu$SR) measurements, we establish these\nmaterials as conventional bulk type-II superconductors with transition\ntemperatures of 2.24(5) K for TiIrGe and 5.64(4) K for HfIrGe, featuring a full\ngap and preserved time-reversal symmetry. First-principles calculations reveal\nstriking topological features in $M$IrGe, including hourglass-shaped bulk\ndispersions and a Dirac chain -- a ring of fourfold-degenerate Dirac points\nprotected by nonsymmorphic symmetry. Each Dirac point corresponds to the neck\nof the hourglass dispersion, while the Dirac chain gives rise to drumhead-like\nsurface states near the Fermi level. Additionally, nontrivial $\\mathbb{Z}_2$\ntopology leads to isolated Dirac surface states with helical spin textures that\ndisperse across the Fermi level, forming an ideal platform for\nproximity-induced topological superconductivity. The coexistence of\nconventional bulk superconductivity, symmetry-protected hourglass topology, and\nhelical spin-textured surface states establishes $M$IrGe as a rare and robust\nplatform to realize topological superconductivity, opening new avenues for\nnext-generation quantum technologies.",
        "Bayesian optimization (BO) is one of the most powerful strategies to solve\ncomputationally expensive-to-evaluate blackbox optimization problems. However,\nBO methods are conventionally used for optimization problems of small dimension\nbecause of the curse of dimensionality. In this paper, a high-dimensionnal\noptimization method incorporating linear embedding subspaces of small dimension\nis proposed to efficiently perform the optimization. An adaptive learning\nstrategy for these linear embeddings is carried out in conjunction with the\noptimization. The resulting BO method, named efficient global optimization\ncoupled with random and supervised embedding (EGORSE), combines in an adaptive\nway both random and supervised linear embeddings. EGORSE has been compared to\nstate-of-the-art algorithms and tested on academic examples with a number of\ndesign variables ranging from 10 to 600. The obtained results show the high\npotential of EGORSE to solve high-dimensional blackbox optimization problems,\nin terms of both CPU time and the limited number of calls to the expensive\nblackbox simulation.",
        "A normal variety $X$ is called Calabi-Yau if $K_X \\sim_{\\mathbb Q} 0$. The\nindex of $X$ is the smallest positive integer $m$ so that $m K_X \\sim 0$. We\nconstruct smooth, projective Calabi-Yau varieties in every dimension with\ndoubly exponentially growing index, which we conjecture to be maximal in every\ndimension. We also construct smooth, projective Calabi-Yau varieties with\nextreme topological invariants; namely, their Euler characteristics and the\nsums of their Betti numbers grow doubly exponentially. These are conjecturally\nextremal in every dimension. The varieties we construct are known in small\ndimensions but we believe them to be new in general. This work builds off of\nthe singular Calabi-Yau varieties found by Esser, Totaro, and Wang in\narXiv:2209.04597.",
        "Jeff Remmel introduced the concept of a $k$-11-representable graph in 2017.\nThis concept was first explored by Cheon et al. in 2019, who considered it as a\nnatural extension of word-representable graphs, which are exactly\n0-11-representable graphs. A graph $G$ is $k$-11-representable if it can be\nrepresented by a word $w$ such that for any edge (resp., non-edge) $xy$ in $G$\nthe subsequence of $w$ formed by $x$ and $y$ contains at most $k$ (resp., at\nleast $k+1$) pairs of consecutive equal letters. A remarkable result of Cheon\nat al. is that any graph is 2-11-representable, while it is still unknown\nwhether every graph is 1-11-representable. Cheon et al. showed that the class\nof 1-11-representable graphs is strictly larger than that of word-representable\ngraphs, and they introduced a useful toolbox to study 1-11-representable\ngraphs, which was extended by additional powerful tools suggested by Futorny et\nal. in 2024.\n  In this paper, we prove that all graphs on at most 8 vertices are\n1-11-representable hence extending the known fact that all graphs on at most 7\nvertices are 1-11-representable. Also, we discuss applications of our main\nresult in the study of multi-1-11-representation of graphs we introduce in this\npaper analogously to the notion of multi-word-representation of graphs\nsuggested by Kenkireth and Malhotra in 2023.",
        "In 1954, I. Kaplansky proposed three test problems for deciding the strength\nof structural understanding of a class of mathematical objects in his treatise\n\"Infinite abelian groups\", which can be formulated for very general\nmathematical systems. In this paper, we focus on Kaplansky's second test\nproblem in a context of complex geometry. Let $H^2_{\\beta}$ be a weighted Hardy\nspace. The Cowen-Douglas operator theory tells us that each\n$h\\in\\textrm{Hol}(\\overline{\\mathbb{D}})$ induces a Hermitian holomorphic\nvector bundle on $H^2_{\\beta}$, denoted by $E_{h(S_\\beta)}(\\Omega)$, where\n$\\Omega$ is a domain. We show that the vector bundle $E_{h(S_\\beta)}$ is a\npush-forwards Hermitian holomorphic vector bundle and study the similarity\ndeformation problems. Our main theorem is that if $H^2_{\\beta}$ is a weighted\nHardy space of polynomial growth, then for any $f\\in\n\\textrm{Hol}(\\overline{\\mathbb{D}})$, there exists a unique positive integer\n$m$ and an function $h\\in\\textrm{Hol}(\\overline{\\mathbb{D}})$ inducing an\nindecomposable vector bundle $E_{h(S_{\\beta})}$, such that $E_{f(S_\\beta)}$ is\nsimilar to $\\bigoplus_1^m E_{h(S_\\beta)}$, where $h$ is unique in the sense of\nanalytic automorphism group action. That could be seemed as a Jordan\ndecomposition theorem for the push-forwards Hermitian holomorphic vector\nbundles. Furthermore, we give the similarity classification of those\npush-forwards Hermitian holomorphic vector bundles induced by analytic\nfunctions, and give an affirmative answer to Kaplansky's second test problem\nfor those objects. We also give an affirmative answer to the geometric version\nand generalized version of a problem proposed by R. Douglas in 2007, and obtain\nthe $K_0$-group of the commutant algebra of a multiplication operator on a\nweighted Hardy space of polynomial growth. In addition, we give an example to\nshow the setting of polynomial growth condition is necessary.",
        "SN~2022pul gains special attention due to its possible origin of a\nsuper-Chandarsekhar-mass white dwarf explosion (or called a 03fg-like type Ia\nsupernova), which shows prominent [O\\,{\\sc i}], [Ne\\,{\\sc i}], and [Ca\\,{\\sc\nii}] lines in its late-time spectra taken at $\\sim+$300 days after the peak\nbrightness. In this paper, we present new optical observations for this\npeculiar object, extending up to over 500 days after the peak brightness. In\nparticular, in the $t\\approx+515$ days spectrum, we identified for the first\ntime the presence of narrow emission from [C\\,{\\sc i}] $\\lambda\\lambda9824,\n9850$, which appears asymmetric and quite similar to the accompanied [O\\,{\\sc\ni}] $\\lambda6300$ line in strength and profile. Based on the violent merger\nmodel that accounts well for previous observations but leaves little carbon in\nthe center of the ejecta, this carbon line can be reproduced by increasing the\ndegree of clumping in the ejecta and setting the carbon mass the same as that\nof oxygen ($\\sim$0.06 $M_{\\odot}$) in the innermost region ($\\lesssim 2000$ km\ns$^{-1}$). In principle, the central carbon could come from the secondary white\ndwarf (WD) if it is ignited when hit by the shockwave of the explosion of the\nprimary WD and explodes as a Ca-rich supernova, whereas pure deflagration of a\nsuper-Chandarsekhar-mass WD can account for such unburnt carbon more naturally.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "The mechanisms behind the fragmentation of high-mass dense clumps into\ncompact star-forming cores are fundamental topics in current astrophysical\nresearch. The ALMAGAL survey provides the opportunity to study this process at\nan unprecedented level of detail and statistical significance, featuring\nhigh-angular resolution $1.38$ mm ALMA observations of $1013$ massive dense\nclumps at various Galactic locations. These clumps cover a wide range of\ndistances, masses, surface densities, and evolutionary stages. Here, we present\nthe catalog of compact sources obtained with the CuTEx algorithm from continuum\nimages of the full ALMAGAL clump sample combining ACA-$7$m and $12$m ALMA\narrays, reaching a uniform high median spatial resolution of $\\sim1400$ au. We\ndiscuss the fragmentation properties and the estimated physical parameters of\nthe core population. The ALMAGAL compact source catalog includes $6348$ cores\ndetected in $844$ clumps ($83\\%$ of the total), with a number of cores per\nclump between $1$ and $49$ (median of $5$). The estimated core diameters are\nmostly within $\\sim800-3000$ au (median of $1700$ au). We obtained core masses\nfrom $0.002$ to $345\\,\\mathrm{M_{\\odot}}$. We evaluated the variation in the\ncore mass function (CMF) with evolution as traced by the clump $L\/M$, finding a\nclear, robust shift and change in slope among CMFs within subsamples at\ndifferent stages. This finding suggests that the CMF shape is not constant\nthroughout the star formation process, but rather it builds (and flattens) with\nevolution, with higher core masses reached at later stages. We found that all\ncores within a clump grow in mass on average with evolution, and the number of\ncores increases with the core masses. Our results favor a clump-fed scenario\nfor high-mass star formation, in which cores form as low-mass seeds, and then\ngain mass while further fragmentation occurs in the clump.",
        "One of the primary objectives of this paper is to establish a generalized\nCauchy-Kovalevskaya extension for axially harmonic functions. We demonstrate\nthat the result can be expressed as a power series involving Bessel-type\nfunctions of specific differential operators acting on two initial functions.\nAdditionally, we analyze the decomposition of the harmonic CK extension in\nterms of integrals over the sphere $ \\mathbb{S}^{m-1} $ involving functions of\nplane wave type.\n  Another key goal of this paper is to explore the relationship between the\nharmonic Cauchy-Kovalevskaya extension and the Fueter-Sce theorem. The\nFueter-Sce theorem outlines a two-step process for constructing axially\nmonogenic functions in $ \\mathbb{R}^{m+1}$ starting from holomorphic functions\nin one complex variable. The first step generates the class of slice monogenic\nfunctions, while the second step produces axially monogenic functions by\napplying the pointwise differential operator $\n\\Delta_{\\mathbb{R}{^{m+1}}}^{\\frac{m-1}{2}} $ with $m$ being odd, known as the\nFueter-Sce map, to a slice monogenic function.\n  By suitably factorizing the Fueter-Sce map, we introduce the set of axially\nharmonic functions, which serves as an intermediate class between slice\nmonogenic and axially monogenic functions. In this paper, we establish a\nconnection between the harmonic CK extension and the factorization of the\nFueter-Sce map. This connection leads to a new notion of harmonic polynomials,\nwhich we show to form a basis for the Riesz potential. Finally, we also\nconstruct a basis for the space of axially harmonic functions.",
        "Agent-based models (ABMs) are valuable for modelling complex, potentially\nout-of-equilibria scenarios. However, ABMs have long suffered from the Lucas\ncritique, stating that agent behaviour should adapt to environmental changes.\nFurthermore, the environment itself often adapts to these behavioural changes,\ncreating a complex bi-level adaptation problem. Recent progress integrating\nmulti-agent reinforcement learning into ABMs introduces adaptive agent\nbehaviour, beginning to address the first part of this critique, however, the\napproaches are still relatively ad hoc, lacking a general formulation, and\nfurthermore, do not tackle the second aspect of simultaneously adapting\nenvironmental level characteristics in addition to the agent behaviours. In\nthis work, we develop a generic two-layer framework for ADaptive AGEnt based\nmodelling (ADAGE) for addressing these problems. This framework formalises the\nbi-level problem as a Stackelberg game with conditional behavioural policies,\nproviding a consolidated framework for adaptive agent-based modelling based on\nsolving a coupled set of non-linear equations. We demonstrate how this generic\napproach encapsulates several common (previously viewed as distinct) ABM tasks,\nsuch as policy design, calibration, scenario generation, and robust behavioural\nlearning under one unified framework. We provide example simulations on\nmultiple complex economic and financial environments, showing the strength of\nthe novel framework under these canonical settings, addressing long-standing\ncritiques of traditional ABMs.",
        "In recent years, there has been much interest in understanding the\ngeneralization behavior of interpolating predictors, which overfit on noisy\ntraining data. Whereas standard analyses are concerned with whether a method is\nconsistent or not, recent observations have shown that even inconsistent\npredictors can generalize well. In this work, we revisit the classic\ninterpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method),\nand study its generalization capabilities through this modern viewpoint. In\nparticular, by varying a single bandwidth-like hyperparameter, we prove the\nexistence of multiple overfitting behaviors, ranging non-monotonically from\ncatastrophic, through benign, to tempered. Our results highlight how even\nclassical interpolating methods can exhibit intricate generalization behaviors.\nNumerical experiments complement our theory, demonstrating the same phenomena.",
        "Complex adaptive networks exhibit remarkable resilience, driven by the\ndynamic interplay of structure (interactions) and function (state). While\nstatic-network analyses offer valuable insights, understanding how structure\nand function co-evolve under external interventions is critical for explaining\nsystem-level adaptation. Using a unique dataset of clandestine criminal\nnetworks, we combine empirical observations with computational modeling to test\nthe impact of various interventions on network adaptation. Our analysis\nexamines how networks with specialized roles adapt and form emergent structures\nto optimize cost-benefit trade-offs. We find that emergent sparsely connected\nnetworks exhibit greater resilience, revealing a security-efficiency trade-off.\nNotably, interventions can trigger a \"criminal opacity amplification\" effect,\nwhere criminal activity increases despite reduced network visibility. While\nnode isolation fragments networks, it strengthens remaining active ties. In\ncontrast, deactivating nodes (analogous to social reintegration) can\nunintentionally boost criminal coordination, increasing activity or\nconnectivity. Failed interventions often lead to temporary functional surges\nbefore reverting to baseline. Surprisingly, stimulating connectivity\ndestabilizes networks. Effective interventions require precise calibration to\nnode roles, connection types, and external conditions. These findings challenge\nconventional assumptions about connectivity and intervention efficacy in\ncomplex adaptive systems across diverse domains.",
        "We reconstruct (appropriately completed) categories of cellular motivic\nspectra over fields of small cohomological dimension in terms of only their\nabsolute Galois groups. As our main application, we determine the motivic\nstable stems (away from the characteristic) of almost all fields.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "The total variation distance is a metric of central importance in statistics\nand probability theory. However, somewhat surprisingly, questions about\ncomputing it algorithmically appear not to have been systematically studied\nuntil very recently. In this paper, we contribute to this line of work by\nstudying this question in the important special case of multivariate Gaussians.\nMore formally, we consider the problem of approximating the total variation\ndistance between two multivariate Gaussians to within an $\\epsilon$-relative\nerror. Previous works achieved a fixed constant relative error approximation\nvia closed-form formulas. In this work, we give algorithms that given any two\n$n$-dimensional Gaussians $D_1,D_2$, and any error bound $\\epsilon > 0$,\napproximate the total variation distance $D := d_{TV}(D_1,D_2)$ to\n$\\epsilon$-relative accuracy in $\\text{poly}(n,\\frac{1}{\\epsilon},\\log\n\\frac{1}{D})$ operations. The main technical tool in our work is a reduction\nthat helps us extend the recent progress on computing the TV-distance between\ndiscrete random variables to our continuous setting."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Physics guided deep learning for generative design of crystal materials with symmetry constraints",
    "start_abstract":"Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability.",
    "start_categories":[
      "physics.comp-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
      ],
      "abstract":[
        "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Sovereign Debt Default and Climate Risk",
        "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with\n  Generative Flow Networks",
        "Mathematical modelling and homogenization of thin fiber-reinforced\n  hydrogels",
        "Training Dynamics of In-Context Learning in Linear Attention",
        "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$",
        "Surface Diagrams for Frobenius Algebras and Frobenius-Schur Indicators\n  in Grothendieck-Verdier Categories",
        "Technical Note: Targeted Maximum Likelihood Estimator for an ATE\n  Standardized for New Target Population",
        "Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition",
        "SoK: A Review of Cross-Chain Bridge Hacks in 2023",
        "O-RIS-ing: Evaluating RIS-Assisted NextG Open RAN",
        "Algebraization of rigid analytic varieties and formal schemes via\n  perfect complexes",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Enhancing Large Language Model Efficiencyvia Symbolic Compression: A\n  Formal Approach Towards Interpretability",
        "Improved quasi-invariance result for the periodic Benjamin-Ono-BBM\n  equation",
        "The dynamics of meaning through time: Assessment of Large Language\n  Models",
        "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label\n  Recognition in Vision-Language Models",
        "Cluster weighted models for functional data",
        "Quantization dimension for a generalized inhomogeneous bi-Lipschitz\n  iterated function system",
        "Preventing Rogue Agents Improves Multi-Agent Collaboration",
        "Smell of Source: Learning-Based Odor Source Localization with Molecular\n  Communication",
        "EvoP: Robust LLM Inference via Evolutionary Pruning",
        "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian",
        "Modeling ice cliff stability using a new Mohr-Coulomb-based phase field\n  fracture model",
        "A Novel Approach to Network Traffic Analysis: the HERA tool",
        "Modern Models, Medieval Texts: A POS Tagging Study of Old Occitan",
        "C2D-ISR: Optimizing Attention-based Image Super-resolution from\n  Continuous to Discrete Scales",
        "Supervised Quadratic Feature Analysis: An Information Geometry Approach\n  to Dimensionality Reduction",
        "A note on the physical interpretation of neural PDE's",
        "Voting or Consensus? Decision-Making in Multi-Agent Debate"
      ],
      "abstract":[
        "We explore the interplay between sovereign debt default\/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
        "Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.",
        "This work considers simultaneous homogenization dimension reduction of a\nporoelastic model for thin fiber-reinforced hydrogels. The analysed medium is\ndefined as a two-component system consisting of a continuous fiber framework\nwith hydrogel inclusions arranged periodically throughout. The fibers are\nassumed to operate under quasi-stationary linear elasticity, whereas the\nhydrogel's hydromechanical behavior is represented using Biot's linear\nporoelasticity model. The asymptotic limit of the coupled system is established\nwhen the periodicity and thickness parameters are of the same order and tend to\nzero simultaneously, utilizing the re-scaling unfolding operator. It is\ndemonstrated that the limit displacement exhibits Kirchhoff-Love-type behavior\nthrough Griso's decomposition of plate displacements. Towards the end, a unique\nsolution for the macroscopic problem has been demonstrated.",
        "While attention-based models have demonstrated the remarkable ability of\nin-context learning, the theoretical understanding of how these models acquired\nthis ability through gradient descent training is still preliminary. Towards\nanswering this question, we study the gradient descent dynamics of multi-head\nlinear self-attention trained for in-context linear regression. We examine two\nparametrizations of linear self-attention: one with the key and query weights\nmerged as a single matrix (common in theoretical studies), and one with\nseparate key and query matrices (closer to practical settings). For the merged\nparametrization, we show the training dynamics has two fixed points and the\nloss trajectory exhibits a single, abrupt drop. We derive an analytical\ntime-course solution for a certain class of datasets and initialization. For\nthe separate parametrization, we show the training dynamics has exponentially\nmany fixed points and the loss exhibits saddle-to-saddle dynamics, which we\nreduce to scalar ordinary differential equations. During training, the model\nimplements principal component regression in context with the number of\nprincipal components increasing over training time. Overall, we characterize\nhow in-context learning abilities evolve during gradient descent training of\nlinear attention, revealing dynamics of abrupt acquisition versus progressive\nimprovements in models with different parametrizations.",
        "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions.",
        "Grothendieck-Verdier categories (also known as $\\ast$-autonomous categories)\ngeneralize rigid monoidal categories, with notable representation-theoretic\nexamples including categories of bimodules, modules over Hopf algebroids, and\nmodules over vertex operator algebras.\n  In this paper, we develop a surface-diagrammatic calculus for\nGrothendieck-Verdier categories, extending the string-diagrammatic calculus of\nJoyal and Street for rigid monoidal categories into a third dimension. This\nextension naturally arises from the non-invertibility of coherence data in\nGrothendieck-Verdier categories.\n  We show that key properties of Frobenius algebras in rigid monoidal\ncategories carry over to the Grothendieck-Verdier setting. Moreover, we\nintroduce higher Frobenius-Schur indicators for suitably finite $k$-linear\npivotal Grothendieck-Verdier categories and prove their invariance under\npivotal Frobenius linearly distributive equivalences.\n  The proofs are carried out using the surface-diagrammatic calculus. To\nfacilitate the verification of some of our results, we provide auxiliary files\nfor the graphical proof assistant homotopy.io.",
        "In this technical note we present a targeted maximum likelihood estimator\n(TMLE) for a previously studied target parameter that aims to transport an\naverage treatment effect (ATE) on a clinical outcome in a source population to\nwhat the ATE would have been in another target population. It is assumed that\none only observes baseline covariates in the target population, while we assume\nthat one can learn the conditional treatment effect on the outcome of interest\nin the source population. We also allow that one might observe only a subset of\nthe covariates in the target population while all covariates are measured in\nthe source population. We consider the case that the outcome is a clinical\noutcome at some future time point that is subject to missingness, or that our\noutcome of interest is a time to event that is subject to right-censoring. We\nderive the canonical gradients and present the corresponding TMLEs for these\ntwo cases.",
        "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
        "Blockchain technology has revolutionized industries by enabling secure and\ndecentralized transactions. However, the isolated nature of blockchain\necosystems hinders the seamless transfer of digital assets across different\nchains. Cross-chain bridges have emerged as vital web3 infrastructure to\naddress this challenge by facilitating interoperability between distinct\nblockchains. Cross-chain bridges remain vulnerable to various attacks despite\nsophisticated designs and security measures. The industry has experienced a\nsurge in bridge attacks, resulting in significant financial losses. The largest\nhack impacted Axie Infinity Ronin Bridge, with a loss of almost \\$600 million\nUSD. This paper analyzes recent cross-chain bridge hacks in 2022 and 2023 and\nexamines the exploited vulnerabilities. By understanding the attack nature and\nunderlying weaknesses, the paper aims to enhance bridge security and propose\npotential countermeasures. The findings contribute to developing industry-wide\nstandards for bridge security and operational resilience. Addressing the\nvulnerabilities and weaknesses exploited in recent cross-chain bridge hacks\nfosters trust and confidence in cross-chain interoperability.",
        "Reconfigurable Intelligent Surfaces (RISs) pose as a transformative\ntechnology to revolutionize the cellular architecture of Next Generation\n(NextG) Radio Access Networks (RANs). Previous studies have demonstrated the\ncapabilities of RISs in optimizing wireless propagation, achieving high\nspectral efficiency, and improving resource utilization. At the same time, the\ntransition to softwarized, disaggregated, and virtualized architectures, such\nas those being standardized by the O-RAN ALLIANCE, enables the vision of a\nreconfigurable Open RAN. In this work, we aim to integrate these technologies\nby studying how different resource allocation policies enhance the performance\nof RIS-assisted Open RANs. We perform a comparative analysis among various\nnetwork configurations and show how proper network optimization can enhance the\nperformance across the Enhanced Mobile Broadband (eMBB) and Ultra Reliable and\nLow Latency Communications (URLLC) network slices, achieving up to ~34%\nthroughput improvement. Furthermore, leveraging the capabilities of OpenRAN\nGym, we deploy an xApp on Colosseum, the world's largest wireless system\nemulator with hardware-in-the-loop, to control the Base Station (BS)'s\nscheduling policy. Experimental results demonstrate that RIS-assisted\ntopologies achieve high resource efficiency and low latency, regardless of the\nBS's scheduling policy.",
        "In this paper, we extend a theorem of To\\\"en and Vaqui\\'e to the\nnon-Archimedean and formal settings. More precisely, we prove that a smooth and\nproper rigid analytic variety is algebraizable if and only if its category of\nperfect complexes is smooth and proper. As a corollary, we deduce an analogous\nstatement for formal schemes and demonstrate that, in general, the bounded\nderived category of coherent sheaves on a formal scheme is not smooth.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "Large language models (LLMs) face significant token efficiency bottlenecks in\ncode generation and logical reasoning tasks, a challenge that directly impacts\ninference cost and model interpretability. This paper proposes a formal\nframework based on symbolic compression,integrating combinatory logic,\ninformation-theoretic optimal encoding, and context-aware inference techniques\nto achieve a step-change improvement in token efficiency while preserving\nsemantic integrity. We establish a mathematical framework within a functional\nprogramming paradigm, derive the quantitative relationship between symbolic\ndensity and model interpretability, and propose a differentiable compression\nfactor metric to evaluate encoding efficiency. Furthermore, we leverage\nparameter-efficient fine-tuning (PEFT) techniques to achieve a low-cost\napplication of the GAEL language. Experimental results show that this method\nachieves a 78.3% token compression rate in code generation tasks while\nimproving logical traceability by 62% through structural explicitness. This\nresearch provides new theoretical tools for efficient inference in LLMs and\nopens a symbolic path for modelinterpretability research.",
        "We extend recent results of Genovese-Luca-Tzvetkov (2022) regarding the\nquasi-invariance of Gaussian measures under the flow of the periodic\nBenjamin-Ono-BBM (BO-BBM) equation to the full range where BO-BBM is globally\nwell-posed. The main difficulty is due to the critical nature of the dispersion\nwhich we overcome by combining the approach of Coe-Tolomeo (2024) with an\niteration argument due to Forlano-Tolomeo (2024) to obtain long-time higher\nintegrability bounds on the transported density.",
        "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities.",
        "Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs)\nfaces significant challenges without training data, model tuning, or\narchitectural modifications. Existing approaches require prompt tuning or\narchitectural adaptations, limiting zero-shot applicability. Our work proposes\na novel solution treating VLMs as black boxes, leveraging scores without\ntraining data or ground truth. Using large language model insights on object\nco-occurrence, we introduce compound prompts grounded in realistic object\ncombinations. Analysis of these prompt scores reveals VLM biases and\n``AND''\/``OR'' signal ambiguities, notably that maximum compound scores are\nsurprisingly suboptimal compared to second-highest scores. We address these\nthrough a debiasing and score-fusion algorithm that corrects image bias and\nclarifies VLM response behaviors. Our method enhances other zero-shot\napproaches, consistently improving their results. Experiments show superior\nmean Average Precision (mAP) compared to methods requiring training data,\nachieved through refined object ranking for robust zero-shot MLR.",
        "We propose a method, funWeightClust, based on a family of parsimonious models\nfor clustering heterogeneous functional linear regression data. These models\nextend cluster weighted models to functional data, and they allow for\nmultivariate functional responses and predictors. The proposed methodology\nfollows the approach used by the the functional high dimensional data\nclustering (funHDDC) method. We construct an expectation maximization (EM)\nalgorithm for parameter estimation. Using simulated and benchmark data we show\nthat funWeightClust outperforms funHDDC and several two-steps clustering\nmethods. We also use funWeightClust to analyze traffic patterns in Edmonton,\nCanada.",
        "For a given $r\\in (0, +\\infty)$, the quantization dimension of order $r$, if\nit exists, denoted by $D_r(\\mu)$, of a Borel probability measure $\\mu$ on\n${\\mathbb R}^d$ represents the speed how fast the $n$th quantization error of\norder $r$ approaches to zero as the number of elements $n$ in an optimal set of\n$n$-means for $\\mu$ tends to infinity. If $D_r(\\mu)$ does not exists, we call\n$\\underline D_r(\\mu)$ and $\\overline D_r(\\mu)$, the lower and upper\nquantization dimensions of $\\mu$ of order $r$. In this paper, we estimate the\nquantization dimension of condensation measures associated with condensation\nsystems $(\\{f_i\\}_{i=1}^N, (p_i)_{i=0}^N, \\nu)$, where the mappings $f_i$ are\nbi-Lipschitz and the measure $\\nu$ is an image measure of an ergodic measure\nwith bounded distortion supported on a conformal set. In addition, we determine\nthe optimal quantization for an infinite discrete distribution, and give an\nexample which shows that the quantization dimension of a Borel probability\nmeasure can be positive with zero quantization coefficient.",
        "Multi-agent systems, where specialized agents collaborate to solve a shared\ntask hold great potential, from increased modularity to simulating complex\nenvironments. However, they also have a major caveat -- a single agent can\ncause the entire system to fail. Consider a simple game where the knowledge to\nsolve the task is distributed between agents, which share information in a\ncommunication channel. At each round, any of the agents can terminate the game\nand make the final prediction, even if they are uncertain about the outcome of\ntheir action. Detection of such rogue agents $\\textit{before they act}$ may\nprevent the system's failure. In this work, we propose to $\\textit{monitor}$\nagents during action prediction and $\\textit{intervene}$ when a future error is\nlikely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent\ncollaboration environment that allows modular control over task complexity and\ncommunication structure. Experiments on two variants of WhoDunitEnv and the\nGovSim environment for resource sustainability show that our approach leads to\nsubstantial performance gains up to 17.4% and 20%, respectively. Moreover, a\nthorough analysis shows that our monitors successfully identify critical points\nof agent confusion and our interventions effectively stop agent errors from\npropagating.",
        "Odor source localization is a fundamental challenge in molecular\ncommunication, environmental monitoring, disaster response, industrial safety,\nand robotics. In this study, we investigate three major approaches: Bayesian\nfiltering, machine learning (ML) models, and physics-informed neural networks\n(PINNs) with the aim of odor source localization in a single-source,\nsingle-molecule case. By considering the source-sensor architecture as a\ntransmitter-receiver model we explore source localization under the scope of\nmolecular communication. Synthetic datasets are generated using a 2D\nadvection-diffusion PDE solver to evaluate each method under varying\nconditions, including sensor noise and sparse measurements. Our experiments\ndemonstrate that \\textbf{Physics-Informed Neural Networks (PINNs)} achieve the\nlowest localization error of \\(\\mathbf{0.89 \\times 10^{-6}}\\) m, outperforming\n\\textbf{machine learning (ML) inversion} (\\(\\mathbf{1.48 \\times 10^{-6}}\\) m)\nand \\textbf{Kalman filtering} (\\(\\mathbf{1.62 \\times 10^{-6}}\\) m). The\n\\textbf{reinforcement learning (RL)} approach, while achieving a localization\nerror of \\(\\mathbf{3.01 \\times 10^{-6}}\\) m, offers an inference time of\n\\(\\mathbf{0.147}\\) s, highlighting the trade-off between accuracy and\ncomputational efficiency among different methodologies.",
        "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing\nstructured pruning methods address this issue by removing redundant structures\n(e.g., elements, channels, layers) from the model. However, these methods\nemploy a heuristic pruning strategy, which leads to suboptimal performance.\nBesides, they also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting structured pruning techniques, EvoP achieves the best performance\nwhile maintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications.",
        "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps:\/\/huggingface.co\/datasets\/teias-ai\/percul",
        "Iceberg calving at glacier termini results in mass loss from ice sheets, but\nthe associated fracture mechanics is often poorly represented using simplistic\n(empirical or elementary mechanics-based) failure criteria. Here, we propose an\nadvanced Mohr-Coulomb failure criterion that drives cracking based on the\nvisco-elastic stress state in ice. This criterion is implemented in a phase\nfield fracture framework, and finite element simulations are conducted to\ndetermine the critical conditions that can trigger ice cliff collapse. Results\ndemonstrate that fast-moving glaciers with negligible basal friction are prone\nto tensile failure causing crevasse propagation far away from the ice front;\nwhilst slow-moving glaciers with significant basal friction are likely to\nexhibit shear failure near the ice front. Results also indicate that seawater\npressure plays a major role in modulating cliff failure. For land terminating\nglaciers, full thickness cliff failure is observed if the glacier exceeds a\ncritical height, dependent on cohesive strength $\\tau_\\mathrm{c}$ ($H \\approx\n120\\;\\text{m}$ for $\\tau_\\mathrm{c}=0.5\\;\\text{MPa}$). For marine-terminating\nglaciers, ice cliff failure occurs if a critical glacier free-board\n($H-h_\\mathrm{w}$) is exceeded, with ice slumping only observed above the\nocean-water height; for $\\tau_\\mathrm{c} = 0.5\\;\\text{MPa}$, the\nmodel-predicted critical free-board is $H-h_\\mathrm{w} \\approx 215\\;\\text{m}$,\nwhich is in good agreement with field observations. While the critical\nfree-board height is larger than that predicted by some previous models, we\ncannot conclude that marine ice cliff instability is less likely because we do\nnot include other failure processes such as hydrofracture of basal crevasses\nand plastic necking.",
        "Cybersecurity threats highlight the need for robust network intrusion\ndetection systems to identify malicious behaviour. These systems rely heavily\non large datasets to train machine learning models capable of detecting\npatterns and predicting threats. In the past two decades, researchers have\nproduced a multitude of datasets, however, some widely utilised recent datasets\ngenerated with CICFlowMeter contain inaccuracies. These result in flow\ngeneration and feature extraction inconsistencies, leading to skewed results\nand reduced system effectiveness. Other tools in this context lack ease of use,\ncustomizable feature sets, and flow labelling options. In this work, we\nintroduce HERA, a new open-source tool that generates flow files and labelled\nor unlabelled datasets with user-defined features. Validated and tested with\nthe UNSW-NB15 dataset, HERA demonstrated accurate flow and label generation.",
        "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing, yet their effectiveness in handling historical\nlanguages remains largely unexplored. This study examines the performance of\nopen-source LLMs in part-of-speech (POS) tagging for Old Occitan, a historical\nlanguage characterized by non-standardized orthography and significant\ndiachronic variation. Through comparative analysis of two distinct\ncorpora-hagiographical and medical texts-we evaluate how current models handle\nthe inherent challenges of processing a low-resource historical language. Our\nfindings demonstrate critical limitations in LLM performance when confronted\nwith extreme orthographic and syntactic variability. We provide detailed error\nanalysis and specific recommendations for improving model performance in\nhistorical language processing. This research advances our understanding of LLM\ncapabilities in challenging linguistic contexts while offering practical\ninsights for both computational linguistics and historical language studies.",
        "In recent years, attention mechanisms have been exploited in single image\nsuper-resolution (SISR), achieving impressive reconstruction results. However,\nthese advancements are still limited by the reliance on simple training\nstrategies and network architectures designed for discrete up-sampling scales,\nwhich hinder the model's ability to effectively capture information across\nmultiple scales. To address these limitations, we propose a novel framework,\n\\textbf{C2D-ISR}, for optimizing attention-based image super-resolution models\nfrom both performance and complexity perspectives. Our approach is based on a\ntwo-stage training methodology and a hierarchical encoding mechanism. The new\ntraining methodology involves continuous-scale training for discrete scale\nmodels, enabling the learning of inter-scale correlations and multi-scale\nfeature representation. In addition, we generalize the hierarchical encoding\nmechanism with existing attention-based network structures, which can achieve\nimproved spatial feature fusion, cross-scale information aggregation, and more\nimportantly, much faster inference. We have evaluated the C2D-ISR framework\nbased on three efficient attention-based backbones, SwinIR-L, SRFormer-L and\nMambaIRv2-L, and demonstrated significant improvements over the other existing\noptimization framework, HiT, in terms of super-resolution performance (up to\n0.2dB) and computational complexity reduction (up to 11%). The source code will\nbe made publicly available at www.github.com.",
        "Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Despite\nthe availability of methods for learning complex non-linear features (e.g. Deep\nLearning), there is an enduring demand for dimensionality reduction methods\nthat learn linear features due to their interpretability, low computational\ncost, and broad applicability. However, there is a gap between methods that\noptimize linear separability (e.g. LDA), and more flexible but computationally\nexpensive methods that optimize over arbitrary class boundaries (e.g.\nmetric-learning methods). Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a dimensionality reduction method for learning linear features\nthat maximize the differences between class-conditional first- and second-order\nstatistics, which allow for quadratic discrimination. SQFA exploits the\ninformation geometry of second-order statistics in the symmetric positive\ndefinite manifold. We show that SQFA features support quadratic\ndiscriminability in real-world problems. We also provide a theoretical link,\nbased on information geometry, between SQFA and the Quadratic Discriminant\nAnalysis (QDA) classifier.",
        "We highlight a formal and substantial analogy between Machine Learning (ML)\nalgorithms and discrete dynamical systems (DDS) in relaxation form. The analogy\noffers a transparent interpretation of the weights in terms of physical\ninformation-propagation processes and identifies the model function of the\nforward ML step with the local attractor of the corresponding discrete\ndynamics. Besides improving the explainability of current ML applications, this\nanalogy may also facilitate the development of a new class ML algorithms with a\nreduced number of weights.",
        "Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. Among them, the decision-making protocol stands out.\nSystematic comparison of decision protocols is difficult because studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making addresses the challenges of different tasks. This\nwork systematically evaluates the impact of seven decision protocols (e.g.,\nmajority voting, unanimity consensus). We change only one variable at a time\n(i.e., decision protocol) to analyze how different methods affect the\ncollaboration between agents and test different protocols on knowledge (MMLU,\nMMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks over the other\ndecision protocol. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduces it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks",
    "start_abstract":"Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org .",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
      ],
      "abstract":[
        "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
      ],
      "categories":[
        "physics.comp-ph"
      ]
    },
    "list":{
      "title":[
        "Inductive methods for counting number fields",
        "Strengthening the No-Go Theorem for QRNGs",
        "Social Influence Distorts Ratings in Online Interfaces",
        "Bounds for quasimodes with polynomially narrow bandwidth on surfaces of\n  revolution",
        "Semiclassical scar on tori in high dimension",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Some limit theorems for locally stationary Hawkes processes",
        "Energy burdens of carbon lock-in in household heating transitions",
        "Perturbations of a minimal surface with triple junctions in\n  $\\mathbb{R}^2 \\times \\mathbb{S}^1$",
        "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
        "Theoretical and Experimental Investigations of High-Performance\n  Sr2CoNbO6-delta Double Perovskite for IT-SOFC Cathode Applications",
        "Exploring quasar evolution with proximate molecular absorbers: Insights\n  from the kinematics of highly ionized nitrogen",
        "Any function I can actually write down is measurable, right?",
        "SyNPar: Synthetic Null Data Parallelism for High-Power False Discovery\n  Rate Control in High-Dimensional Variable Selection",
        "Strategizing with AI: Insights from a Beauty Contest Experiment",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "Gravitomagnetic tidal response of relativistic stars in partially\n  screened scalar-tensor theories",
        "Poisson Hail on a Wireless Ground",
        "Clifford-Dressed Variational Principles for Precise Loschmidt Echoes",
        "A new combinatorial interpretation of partial sums of $m$-step Fibonacci\n  numbers",
        "Search for charge-parity violation in semileptonically tagged $D^{0} \\to\n  K^{+} \\pi^{-}$ decays",
        "Fractional Brownian motion with mean-density interaction",
        "On the Precise Asymptotics of Universal Inference",
        "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space",
        "Effect of transverse momentum conservation and flow on symmetric\n  cumulants $sc_{2,3} \\left \\{ 4 \\right \\}$ and $sc_{2,3,4} \\left \\{ 6 \\right\n  \\}$",
        "Efficient, Fast, and Fair Voting Through Dynamic Resource Allocation in\n  a Secure Election Physical Intranet",
        "JWST photometry and astrometry of 47 Tucanae. Discontinuity in the\n  stellar sequence at the star\/brown dwarf transition",
        "Octagonal tilings with three prototiles",
        "Deterministic generation of non-classical mechanical states in cavity\n  optomechanics via reinforcement learning"
      ],
      "abstract":[
        "We give a new method for counting extensions of a number field asymptotically\nby discriminant, which we employ to prove many new cases of Malle's Conjecture\nand counterexamples to Malle's Conjecture. We consider families of extensions\nwhose Galois closure is a fixed permutation group $G$. Our method relies on\nhaving asymptotic counts for $T$-extensions for some normal subgroup $T$ of\n$G$, uniform bounds for the number of such $T$-extensions, and possibly weak\nbounds on the asymptotic number of $G\/T$-extensions. However, we do not require\nthat most $T$-extensions of a $G\/T$-extension are $G$-extensions. Our new\nresults use $T$ either abelian or $S_3^m$, though our framework is general.",
        "Quantum random numbers are essential for security against quantum algorithms.\nRandomness as a beacon is a service being provided for companies and\ngovernments to upgrade their security standards from RSA to PQC - QKD or\nPQC-RSA protocols. Both security mechanisms assume trust in the service\nprovider unless one aims for device-independent protocols. How does an entity\nensure that the beacon service has a quantum signature other than relying on\nfaith? Specifically, given a bit-stream, can a user verify a quantum signature\nin it? Researchers claim this is indecipherable and have stated a no-go theorem\nfor post-processed bit-streams. In this article, we corroborate the results of\nthe no-go theorem while discussing its nuances using two different random\nnumber generators and four test methods. These include the NIST statistical\ntest suite and machine learning algorithms that strengthen the theorem. This\nwork is relevant for companies and governments using QRNG OpenAPI to enhance\nsecurity against quantum threats.",
        "Theoretical work on sequential choice and large-scale experiments in online\nranking and voting systems has demonstrated that social influence can have a\ndrastic impact on social and technological systems. Yet, the effect of social\ninfluence on online rating systems remains understudied and the few existing\ncontributions suggest that online ratings would self-correct given enough\nusers. Here, we propose a new framework for studying the effect of social\ninfluence on online ratings. We start from the assumption that people are\ninfluenced linearly by the observed average rating, but postulate that their\npropensity to be influenced varies. When the weight people assign to the\nobserved average depends only on their own latent rating, the resulting system\nis linear, but the long-term rating may substantially deviate from the true\nmean rating. When the weight people put on the observed average depends on both\ntheir own latent rating and the observed average rating, the resulting system\nis non-linear, and may support multiple equilibria, suggesting that ratings\nmight be path-dependent and deviations dramatic. Our results highlight\npotential limitations in crowdsourced information aggregation and can inform\nthe design of more robust online rating systems.",
        "Given a compact surface of revolution with Laplace-beltrami operator\n$\\Delta$, we consider the spectral projector $P_{\\lambda,\\delta}$ on a\npolynomially narrow frequency interval $[\\lambda-\\delta,\\lambda + \\delta]$,\nwhich is associated to the self-adjoint operator $\\sqrt{-\\Delta}$. For a large\nclass of surfaces of revolution, and after excluding small disks around the\npoles, we prove that the $L^2 \\to L^{\\infty}$ norm of $P_{\\lambda,\\delta}$ is\nof order $\\lambda^{\\frac{1}{2}} \\delta^{\\frac{1}{2}}$ up to $\\delta \\geq\n\\lambda^{-\\frac{1}{32}}$. We adapt the microlocal approach introduced by Sogge\nfor the case $\\delta = 1$, by using the Quantum Completely Integrable structure\nof surfaces of revolution introduced by Colin de Verdi\\`ere. This reduces the\nanalysis to a number of estimates of explicit oscillatory integrals, for which\nwe introduce new quantitative tools.",
        "We show that the eigenfunctions of the self-adjoint elliptic $h-$differential\noperator $P_{h}(t)$ exhibits semiclassical scar phenomena on the\n$d-$dimensional torus, under the $\\sigma$-Bruno-R\\\"{u}ssmann condition, instead\nof the Diophantine one. Its equivalence is described as: for almost all\nperturbed Hamiltonian's KAM Lagrangian tori $\\Lambda_{\\omega}$, there exists a\nsemiclassical measure with positive mass on $\\Lambda_{\\omega}$. The premise is\nthat we can obatain a family of quasimodes for the $h-$differential operator\n$P_{h}(t)$ in the semiclassical limit as $h\\rightarrow0$, under the\n$\\sigma$-Bruno-R\\\"{u}ssmann condition.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "We prove a law of large numbers and functional central limit theorem for a\nclass of multivariate Hawkes processes with time-dependent reproduction rate.\nWe address the difficulties induced by the use of non-convolutive Volterra\nprocesses by recombining classical martingale methods introduced in Bacry et\nal. [3] with novel ideas proposed by Kwan et al. [19]. The asymptotic theory we\nobtain yields useful applications in financial statistics. As an illustration,\nwe derive closed-form expressions for price distortions under liquidity\nconstraints.",
        "Heating electrification presents opportunities and challenges for energy\naffordability. Without careful planning and policy, the costs of natural gas\nservice will be borne by a shrinking customer base, driving up expenses for\nthose who are left behind. This affordability issue is worsened by new fossil\nfuel investments, which risk locking communities into carbon-intensive\ninfrastructure. Here, we introduce a framework to quantify the distributional\neffects of natural gas phasedown on energy affordability, integrating detailed\nhousehold data with utility financial and planning documents. Applying our\nframework first to Massachusetts and then nationwide, we show that vulnerable\ncommunities face disproportionate affordability risks in building energy\ntransitions. Households that do not electrify may bear up to 50% higher energy\ncosts over the next decade. Targeted electrification may help to alleviate\nimmediate energy burdens, but household heating transitions will ultimately\nrequire coordinated, neighborhood-scale strategies that consider the high fixed\ncosts of legacy infrastructure.",
        "We construct stationary perturbations of a specific minimal surface with a\ncircle of triple junctions in $\\mathbb{R}^2 \\times \\mathbb{S}^1$, that satisfy\ngiven boundary data.",
        "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
        "Enhancing the transport of oxygen anions in the cathode while maintaining\nsurface stability is essential for improving the performance of\nintermediate-temperature solid oxide fuel cells (IT-SOFCs). This study\ninvestigates a novel cathode material candidate, Sr2CoNbO6-delta (SCNO), using\ndensity functional theory, molecular dynamics, and experimental\ncharacterization. The redox active Co cation at B-site and less reducible Nb\ncation at the B'-site together enhance both surface stability and\nelectrocatalytic performance. SCNO is observed to have a higher concentration\nof oxygen vacancies and increased oxygen diffusivity on the surface. The\nsurface stability of SCNO is further improved when simulated under compressive\nstrain due to the GDC electrolyte substrate. These findings offer new insights\ninto controlling Sr segregation in SCNO, contributing to a better understanding\nof its enhanced oxygen reduction reaction (ORR) activity and high surface\nstability. Subsequently, SCNO was synthesized to evaluate its potential as a\ncathode material in SOFCs. To assess its performance, symmetric cells with\nuniform dense thin films of varying thicknesses (40 and 80 nm) were fabricated\nusing the pulsed laser deposition technique. Electrochemical impedance\nspectroscopy and distributed relaxation time analysis indicate that bulk oxygen\nion diffusion is a limiting factor for the ORR in SCNO. The polarization\nresistance for the 40 and 80 nm dense thin film symmetric cells ranged between\n0.329 - 0.241 ohm cm2 and 1.095 - 0.438 ohm cm2, respectively, within the\ntemperature range of 773 - 973 K in an air atmosphere. The full cell\nconfiguration of NiO-GDC|GDC|SCNO demonstrated a significantly high peak power\ndensity of 0.633 W\/cm2 at 973 K. This theory-guided design and experimental\nstudy suggest that SCNO is a promising candidate for IT-SOFC cathode materials.",
        "We investigate the presence and kinematics of NV absorption proximate to high\nredshift quasars selected upon the presence of strong $H_{2}$ and HI absorption\nat the quasar redshift. Our spectroscopic observations with X-shooter at the\nVLT reveal a 70% detection rate of NV (9 of 13 quasars with 2.5 < z < 3.3),\nremarkably higher than the 10% detection rate in intervening DLA systems and\nthe 30% rate observed within a few thousand km\/s of the source in the general\nquasar population. While many NV components lie within the velocity range of\nthe neutral gas, the kinematic profiles of high-ionization species appear\ndecoupled from those of low-ionization species, with the former extending over\nmuch larger velocity ranges, particularly towards bluer velocities. We also\nobserve significant variations in the NV\/SiIV, which we attribute to varying\nionization conditions, with a velocity-dependent trend: blueshifted NV\ncomponents systematically exhibit higher ionization parameters compared to\nthose near the quasar's systemic redshift. Furthermore, the most redshifted\nsystems relative to the quasar show no evidence of NV absorption. The results\nsuggest that proximate $H_{2}$ absorption systems select critical stages of\nquasar evolution, during which the quasar remains embedded in a rich molecular\nenvironment. Redshifted systems trace infalling gas, potentially associated\nwith mergers, preceding the onset of outflows. Such outflows may reach or even\ncarry out neutral and molecular gas.This latter stage would correspond to\nproximate $H_{2}$ systems located around or blueshifted relative to the\nquasar's systemic z. Finally, the only case in our sample featuring highly\nblueshifted neutral gas shows no evidence of an association with the quasar.Our\nfindings highlight the need to account for the ionization state when defining a\nvelocity threshold to distinguish quasar-associated systems from intervening.",
        "In this expository paper aimed at a general mathematical audience, we discuss\nhow to combine certain classic theorems of set-theoretic inner model theory and\neffective descriptive set theory with work on Hilbert's tenth problem and\nuniversal Diophantine equations to produce the following surprising result:\nThere is a specific polynomial $p(x,y,z,n,k_1,\\dots,k_{70})$ of degree $7$ with\ninteger coefficients such that it is independent of $\\mathsf{ZFC}$ (and much\nstronger theories) whether the function $$f(x) = \\inf_{y \\in \\mathbb{R}}\\sup_{z\n\\in \\mathbb{R}}\\inf_{n \\in \\mathbb{N}}\\sup_{\\bar{k} \\in\n\\mathbb{N}^{70}}p(x,y,z,n,\\bar{k})$$ is Lebesgue measurable. We also give\nsimilarly defined $g(x,y)$ with the property that the statement \"$x \\mapsto\ng(x,r)$ is measurable for every $r \\in \\mathbb{R}$\" has large cardinal\nconsistency strength (and in particular implies the consistency of\n$\\mathsf{ZFC}$) and $h(m,x,y,z)$ such that $h(1,x,y,z),\\dots,h(16,x,y,z)$ can\nconsistently be the indicator functions of a Banach$\\unicode{x2013}$Tarski\nparadoxical decomposition of the sphere.\n  Finally, we discuss some situations in which measurability of analogously\ndefined functions can be concluded by inspection, which touches on\nmodel-theoretic o-minimality and the fact that sufficiently strong large\ncardinal hypotheses (such as Vop\\v{e}nka's principle and much weaker\nassumptions) imply that all 'reasonably definable' functions (including the\nabove $f(x)$, $g(x,y)$, and $h(m,x,y,z)$) are universally measurable.",
        "Balancing false discovery rate (FDR) and statistical power to ensure reliable\ndiscoveries is a key challenge in high-dimensional variable selection. Although\nseveral FDR control methods have been proposed, most involve perturbing the\noriginal data, either by concatenating knockoff variables or splitting the data\ninto two halves, both of which can lead to a loss of power. In this paper, we\nintroduce a novel approach called Synthetic Null Parallelism (SyNPar), which\ncontrols the FDR in high-dimensional variable selection while preserving the\noriginal data. SyNPar generates synthetic null data from a model fitted to the\noriginal data and modified to reflect the null hypothesis. It then applies the\nsame estimation procedure in parallel to both the original and synthetic null\ndata to estimate coefficients that indicate feature importance. By comparing\nthe coefficients estimated from the null data with those from the original\ndata, SyNPar effectively identifies false positives, functioning as a numerical\nanalog of a likelihood ratio test. We provide theoretical guarantees for FDR\ncontrol at any desired level while ensuring that the power approaches one with\nhigh probability asymptotically. SyNPar is straightforward to implement and can\nbe applied to a wide range of statistical models, including high-dimensional\nlinear regression, generalized linear models, Cox models, and Gaussian\ngraphical models. Through extensive simulations and real data applications, we\ndemonstrate that SyNPar outperforms state-of-the-art methods, including\nknockoffs and data-splitting methods, in terms of FDR control, power, and\ncomputational efficiency.",
        "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "In scalar-tensor theories beyond Horndeski, the Vainshtein screening\nmechanism is only partially effective inside astrophysical bodies. We\ninvestigate the potential to detect this partial breaking of Vainshtein\nscreening through the tidal response of fluid bodies. Specifically, we\ncalculate the gravitomagnetic tidal Love numbers and analyze how deviations\nfrom general relativity depend on parameters governing the breaking of\nVainshtein screening in the weak-gravity regime. For fixed parameter values,\nthe relative deviations increase with higher multipoles and larger compactness.\nHowever, we demonstrate that these parameters alone are insufficient to fully\ncharacterize the tidal response of relativistic bodies in scalar-tensor\ntheories beyond Horndeski.",
        "This paper defines a new model which incorporates three key ingredients of a\nlarge class of wireless communication systems: (1) spatial interactions through\ninterference, (2) dynamics of the queueing type, with users joining and\nleaving, and (3) carrier sensing and collision avoidance as used in, e.g.,\nWiFi. In systems using (3), rather than directly accessing the shared resources\nupon arrival, a customer is considerate and waits to access them until nearby\nusers in service have left. This new model can be seen as a missing piece of a\nlarger puzzle that contains such dynamics as spatial birth-and-death processes,\nthe Poisson-Hail model, and wireless dynamics as key other pieces. It is shown\nthat, under natural assumptions, this model can be represented as a Markov\nprocess on the space of counting measures. The main results are then two-fold.\nThe first is on the shape of the stability region and, more precisely, on the\ncharacterization of the critical value of the arrival rate that separates\nstability from instability. The second is of a more qualitative or perhaps even\nethical nature. There is evidence that for natural values of the system\nparameters, the implementation of sensing and collision avoidance stabilizes a\nsystem that would be unstable if immediate access to the shared resources would\nbe granted. In other words, for these parameters, renouncing greedy access\nmakes sharing sustainable, whereas indulging in greedy access kills the system.",
        "We extend the recently introduced Clifford dressed Time-Dependent Variational\nPrinciple (TDVP) to efficiently compute many-body wavefunction amplitudes in\nthe computational basis. This advancement enhances the study of Loschmidt\nechoes, which generally require accurate calculations of the overlap between\nthe evolved state and the initial wavefunction. By incorporating Clifford\ndisentangling gates during TDVP evolution, our method effectively controls\nentanglement growth while keeping the computation of these amplitudes\naccessible. Specifically, it reduces the problem to evaluating the overlap\nbetween a Matrix Product State (MPS) and a stabilizer state, a task that\nremains computationally feasible within the proposed framework. To demonstrate\nthe effectiveness of this approach, we first benchmark it on the\none-dimensional transverse-field Ising model. We then apply it to more\nchallenging scenarios, including a non-integrable next-to-nearest-neighbor\nIsing chain and a two-dimensional Ising model. Our results highlight the\nversatility and efficiency of the Clifford-augmented MPS, showcasing its\ncapability to go beyond the evaluation of simple expectation values. This makes\nit a powerful tool for exploring various aspects of many-body quantum dynamics.",
        "The sequence of partial sums of Fibonacci numbers, beginning with $2$, $4$,\n$7$, $12$, $20$, $33,\\dots$, has several combinatorial interpretations (OEIS\nA000071). For instance, the $n$-th term in this sequence is the number of\nlength-$n$ binary words that avoid $110$. This paper proves a related but new\ninterpretation: given a length-$3$ binary word -- called the keyword -- we say\ntwo length-$n$ binary words are equivalent if one can be obtained from the\nother by some sequence of substitutions: each substitution replaces an instance\nof the keyword with its negation, or vice versa. We prove that the number of\ninduced equivalence classes is again the $n$-th term in the aforementioned\nsequence. When the keyword has length $m+1$ (instead of $3$), the same result\nholds with $m$-step Fibonacci numbers. What makes this result surprising -- and\ndistinct from the previous interpretation -- is that it does not depend on the\nkeyword, despite the fact that the sizes of the equivalence classes do. On this\nfinal point, we prove several results on the structure of equivalence classes,\nand also pose a variety of open problems.",
        "An analysis of the flavour oscillations of the charmed neutral meson is\npresented. The ratio of $D^{0} \\to K^{+} \\pi^{-}$ and $D^{0} \\to K^{-} \\pi^{+}$\ndecay rates is measured as a function of the decay time of the $D^{0}$ meson\nand compared with the charge-conjugated system to search for charge-parity\nviolation. The meson flavour at production is double-tagged by the charges of\nthe muon and pion in the preceding $\\overline{B} \\to D^{*}(2010)^{+} \\mu^{-} X$\nand ${{D^{*}(2010)^{+}} \\to D^{0}\\pi^{+}}$ decays, respectively. These decays\nare selected from proton-proton collision data collected by the LHCb experiment\nat a centre-of-mass energy of ${13\\,\\text{TeV}}$ and corresponding to an\nintegrated luminosity of ${5.4\\,\\text{fb}^{-1}}$. The flavour oscillation\nparameters, relating to the differences in mass and width of the mass\neigenstates, are found to be ${y^\\prime=(5.8\\pm1.6)\\times10^{-3}}$ and\n${(x^\\prime)^2=(0.0\\pm1.2)\\times10^{-4}}$. No evidence for charge-parity\nviolation is seen either in the flavour oscillations or in the decay, where the\ndirect charge-parity asymmetry is measured to be ${A_{D}=(2.3\\pm1.7)\\,{\\%}}$.",
        "Fractional Brownian motion is a Gaussian stochastic process with long-range\ncorrelations in time; it has been shown to be a useful model of anomalous\ndiffusion. Here, we investigate the effects of mutual interactions in an\nensemble of particles undergoing fractional Brownian motion. Specifically, we\nintroduce a mean-density interaction in which each particle in the ensemble is\ncoupled to the gradient of the total, time-integrated density produced by the\nentire ensemble. We report the results of extensive computer simulations for\nthe mean-square displacements and the probability densities of particles\nundergoing one-dimensional fractional Brownian motion with such a mean-density\ninteraction. We find two qualitatively different regimes, depending on the\nanomalous diffusion exponent $\\alpha$ characterizing the fractional Gaussian\nnoise. The motion is governed by the interactions for $\\alpha < 4\/3$ whereas it\nis dominated by the fractional Gaussian noise for $\\alpha > 4\/3$. We develop a\nscaling theory explaining our findings. We also discuss generalizations to\nhigher space dimensions and nonlinear interactions as well as applications to\nthe growth of strongly stochastic axons (e.g., serotonergic fibers) in\nvertebrate brains.",
        "In statistical inference, confidence set procedures are typically evaluated\nbased on their validity and width properties. Even when procedures achieve\nrate-optimal widths, confidence sets can still be excessively wide in practice\ndue to elusive constants, leading to extreme conservativeness, where the\nempirical coverage probability of nominal $1-\\alpha$ level confidence sets\napproaches one. This manuscript studies this gap between validity and\nconservativeness, using universal inference (Wasserman et al., 2020) with a\nregular parametric model under model misspecification as a running example. We\nidentify the source of asymptotic conservativeness and propose a general remedy\nbased on studentization and bias correction. The resulting method attains exact\nasymptotic coverage at the nominal $1-\\alpha$ level, even under model\nmisspecification, provided that the product of the estimation errors of two\nunknowns is negligible, exhibiting an intriguing resemblance to double\nrobustness in semiparametric theory.",
        "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the Euclid photometric survey will allow for precise\nstudies of galaxy clustering from a single survey, over a large range of\nredshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to\nextract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy\nmock catalogue with a tomographic approach to constrain the evolution of the\nUniverse and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}\/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference.",
        "Symmetric cumulants can improve our understanding of the joint probability\ndistribution function $ P\\left ( v_{m},v_{n},v_{k}, \\dots,\\Psi _{m},\\Psi\n_{n},\\Psi _{k},\\dots \\right )$, potentially offering new insights into the\nnature of the fluctuations of the quark-gluon plasma produced in relativistic\nheavy-ion collisions. In this work, the four-particle symmetric cumulants\n$sc_{2,3} \\left \\{ 4 \\right \\}$, six-particle symmetric cumulants $sc_{2,3,4}\n\\left \\{ 6 \\right \\}$, and the normalized cumulants $nsc_{2,3} \\left \\{ 4\n\\right \\}$ and $nsc_{2,3,4} \\left \\{ 6 \\right \\}$ originating from transverse\nmomentum conservation, collective flow, and the interplay between the two\neffects are calculated. Our results are consistent with the ATLAS data using\nthe subevent cumulant method and the simulations using the realistic Monte\nCarlo models of iEBE-VISHNU and HIJING, facilitating a more profound\nunderstanding of the origins of these symmetric cumulants in small systems.",
        "Resource allocations in an election system, often with hundreds of polling\nlocations over a territory such as a county, with the aim that voters receive\nfair and efficient services, is a challenging problem, as election resources\nare limited and the number of expected voters can be highly volatile through\nthe voting period. This paper develops two propositions to ensure efficiency,\nfairness, resilience, and security. The first is to leverage Physical Internet\n(PI) principles, notably setting up a \"secure election physical intranet\"\n(SEPI) based on open resource sharing and flow consolidation between election\nfacilities in the territory. The second is to adopt a smart dynamic resource\nallocation methodology within the SEPI based on queueing networks and\nlexicographic optimization. A queueing model is developed to provide feasible\ncombinations of resources and individual performances for each polling location\nby considering layout and utilization constraints. A two-stage lexicographic\noptimizer receives the queueing model's outputs and finds an optimal solution\nthat is less expensive, fast, and fair. A scenario-based case study validates\nthe proposed methodology based on data from the 2020 US Presidential Election\nin Fulton County, Georgia, USA.",
        "Using JWST Near Infrared Camera (NIRCam) images of the globular cluster 47\nTucanae (or NGC 104), taken at two epochs just 7 months apart, we derived\nproper-motion membership down to $m_{\\rm F322W2} \\sim 27$. We identified an\nintriguing feature at the very low-mass end of the main sequence, around $\\sim$\n0.08 solar masses, at magnitudes $m_{\\rm F322W2} \\sim 24$ and $m_{\\rm F150W2}\n\\sim 25$. This feature, dubbed \"kink\", is characterized by a prominent\ndiscontinuity in the slope of the main sequence. A similar discontinuity is\nseen in theoretical isochrones with oxygen-poor chemistries, related to the\nrapid onset of CH$_4$ absorption. We therefore hypothesize that the cluster\nhosts disproportionately more oxygen-poor stars near the bottom of the main\nsequence compared to the upper main sequence and the red giant branch. Our\nresults show no strong or conclusive evidence of a rise in the brown dwarf\nluminosity function at faint magnitudes, in contrast to previous findings\nlikely affected by faint red background galaxies. In our analysis, we accounted\nfor this contamination by using proper motion membership.",
        "Motivated by theoretically and experimentally observed structural phases with\noctagonal symmetry, we introduce a family of octagonal tilings which are\ncomposed of three prototiles. We define our tilings with respect to two\nnon-negative integers, $m$ and $n$, so that the inflation factor of a given\ntiling is $\\delta_{(m,n)}=m+n (1+\\sqrt{2})$. As such, we show that our family\nconsists of an infinite series of tilings which can be delineated into separate\n`cases' which are determined by the relationship between $m$ and $n$.\nSimilarly, we present the primitive substitution rules or decomposition of our\nprototiles, along with the statistical properties of each case, demonstrating\ntheir dependence on these integers.",
        "Non-classical mechanical states, as vital quantum resources for exploring\nmacroscopic quantum behavior, have wide applications in the study of the\nfundamental quantum mechanics and modern quantum technology. In this work, we\npropose a scheme for deterministically generating non-classical mechanical\nstates in cavity optomechanical systems. By working in the eigen-representation\nof the nonlinear optomechanical systems, we identify the carrier-wave resonance\nconditions and seek for the optimal driving pulses for state preparations.\nConcretely, we employ the reinforcement learning method to optimize the pulsed\ndriving fields, effectively suppressing the undesired transitions induced by\nboth the pulsed driving fields and dissipations. This approach enables the\nhigh-fidelity preparation of phononic Fock states and superposed Fock states in\nthe single-resonator optomechanical systems, as well as two-mode entangled\nstates in the two-resonator optomechanical systems. The statistical properties\nof the generated states are also examined. Our results open a way for quantum\nstate engineering in quantum optics and quantum information science via\nreinforcement learning."
      ]
    }
  },
  {
    "id":2412.17907,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Deep Facial Expression Recognition: A Survey",
    "start_abstract":"With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and recent success deep learning techniques in various fields, neural networks have increasingly been leveraged learn discriminative representations for automatic FER. Recent FER systems generally focus on two important issues: overfitting caused by a lack sufficient training data expression-unrelated variations, such as illumination, head pose identity bias. In this paper, we provide comprehensive survey FER, including datasets algorithms that insights into these intrinsic problems. First, describe standard pipeline system with related background knowledge suggestions applicable implementations each stage. We then introduce available are widely used literature accepted selection evaluation principles datasets. For state art review existing novel strategies designed based both static images dynamic image sequences, discuss their advantages limitations. Competitive performances benchmarks also summarized section. extend our additional issues application scenarios. Finally, remaining challenges corresponding opportunities field well future directions design robust systems.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Recognizing and reducing cognitive bias in clinical and forensic neurology"
      ],
      "abstract":[
        "In medicine, cognitive errors form the basis of bias in clinical practice. Several types are common and pervasive, may lead to inaccurate diagnosis or treatment. Forensic neurology, even when aided by current technologies, still dependent on interpretations, therefore prone bias. This article discusses 4 biases that can clinician astray. They confirmation (selective gathering neglect contradictory evidence); base rate (ignoring misusing prevailing data); hindsight (oversimplification past causation); good old days (the tendency for patients misremember exaggerate their preinjury functioning). We briefly describe strategies adopted from field psychology could minimize While debiasing is not easy, reducing such requires awareness acknowledgment our susceptibility these distortions."
      ],
      "categories":[
        "Clinical Neurology"
      ]
    },
    "list":{
      "title":[
        "Randomized measurements for multi-parameter quantum metrology",
        "Individual and cooperative superexchange enhancement in cuprates",
        "Every Call is Precious: Global Optimization of Black-Box Functions with\n  Unknown Lipschitz Constants",
        "Distinct terahertz third-harmonic generation of many-body excitonic\n  states",
        "SWIPT in Cell-Free Massive MIMO Using Stacked Intelligent Metasurfaces",
        "Some classes of permutation pentanomials",
        "Various Architectures of Colloidal Cu3(MoO4)2(OH)2 and Cu3Mo2O9; Thermal\n  Stability, Photoluminescence and Magnetic Properties of Cu3(MoO4)2(OH)2 and\n  Cu3Mo2O9 Nanosheets",
        "Heat Kernel Estimates for Schr\\\"odinger Operators in the Domain Above a\n  Bounded Lipschitz Function",
        "Mass loss along the red giant branch of the intermediate stellar\n  populations in NGC6752 and NGC2808",
        "Optical appearance of a boson star with soliton potential",
        "Discovering Polynomial and Quadratic Structure in Nonlinear Ordinary\n  Differential Equations",
        "Speeding up Lindblad dynamics via time-rescaling engineering",
        "On Oblivious Transfer Capacity of Noisy Multiple Access Channel",
        "Nonlocal Micromagnetics: Compactness Criteria, Existence of Minimizers,\n  and Brown's Fundamental Theorem",
        "Hadron Production in Open-charm Meson Pair at $e^+e^-$ Collider",
        "The large mass limit of $G_2$ and Calabi-Yau monopoles",
        "The J-PAS survey: The effect of photometric redshift errors on cosmic\n  voids",
        "Probabilistic intersection theory in Riemannian homogeneous spaces",
        "SurvHive: a package to consistently access multiple survival-analysis\n  packages",
        "Quantum Avalanches in $\\mathbb{Z}_2$-preserving Interacting Ising\n  Majorana Chain",
        "BaTiO$_3$ -- SrTiO$_3$ composites: a microscopic study on paraelectric\n  cubic inclusions",
        "Technical description and performance of the phase II version of the\n  Keck Planet Imager and Characterizer",
        "Observation of Two Cascading Screening Processes in an Iron-based\n  Superconductor",
        "Environmental Factors Can Have Opposite Biodiversity Influences on the\n  Community Temporal Stability In Aquatic Ecosystems",
        "Industrial Applications of Neutrinos",
        "Identifying compact symmetric objects with high-precision VLBI and Gaia\n  astrometry",
        "Naturalistic Computational Cognitive Science: Towards generalizable\n  models and theories that capture the full range of natural behavior",
        "Telegraph flux noise induced beating Ramsey fringe in transmon qubits",
        "On Lorentzian-Euclidean black holes and Lorentzian to Riemannian metric\n  transitions"
      ],
      "abstract":[
        "The optimal quantum measurements for estimating different unknown parameters\nin a parameterized quantum state are usually incompatible with each other.\nTraditional approaches to addressing the measurement incompatibility issue,\nsuch as the Holevo Cram\\'{e}r--Rao bound, suffer from multiple difficulties\ntowards practical applicability, as the optimal measurement strategies are\nusually state-dependent, difficult to implement and also take complex analyses\nto determine. Here we study randomized measurements as a new approach for\nmulti-parameter quantum metrology. We show quantum measurements on single\ncopies of quantum states given by 3-design perform near-optimally when\nestimating an arbitrary number of parameters in pure states and more generally,\napproximately low-rank states, whose metrological information is largely\nconcentrated in a low-dimensional subspace. The near-optimality is also shown\nin estimating the maximal number of parameters for three types of mixed states\nthat are well-conditioned on its support. Examples of fidelity estimation and\nHamiltonian estimation are explicitly provided to demonstrate the power and\nlimitation of randomized measurements in multi-parameter quantum metrology.",
        "It is now widely accepted that the antiferromagnetic coupling within high\ntemperature superconductors strongly exhibits a profound correlation with the\nupper limit of superconducting transition temperature these materials can\nreach. Thus, accurately calculating the positive and negative mechanisms that\ninfluence magnetic coupling in specific materials is crucial for the\nexploration of superconductivity at higher temperatures. Nevertheless, it is\nnotoriously difficult to establish a complete description of electron\ncorrelations employing ab initio theories because of the large number of\norbitals involved. In this study, we tackle the challenge of achieving\nhigh-level ab initio wave function theory calculations, which allow an explicit\ntreatment of electron correlations associated with a large number of\nhigh-energy orbitals. We elucidate the atomic-shell-wise contributions to the\nsuperexchange coupling in the lanthanum cuprate, including individual effects\nof high-energy orbitals (Cu 4d, 5d, 4f, 5p) and cooperative effects between the\ncore and these high-energy orbitals. Specifically, the prominent contributions\nfrom Cu 4d, 5d, 4f and 5p give rise to a rich collection of previously\nunexamined superexchange channels. We propose a p-d-f model to universally\naccount for the contributions of high-energy orbitals at copper sites. Our\ncalculations and physical rationalizations offer a more robust theoretical\nfoundation for investigating cuprate-type high-temperature superconductors.",
        "Optimizing expensive, non-convex, black-box Lipschitz continuous functions\npresents significant challenges, particularly when the Lipschitz constant of\nthe underlying function is unknown. Such problems often demand numerous\nfunction evaluations to approximate the global optimum, which can be\nprohibitive in terms of time, energy, or resources. In this work, we introduce\nEvery Call is Precious (ECP), a novel global optimization algorithm that\nminimizes unpromising evaluations by strategically focusing on potentially\noptimal regions. Unlike previous approaches, ECP eliminates the need to\nestimate the Lipschitz constant, thereby avoiding additional function\nevaluations. ECP guarantees no-regret performance for infinite evaluation\nbudgets and achieves minimax-optimal regret bounds within finite budgets.\nExtensive ablation studies validate the algorithm's robustness, while empirical\nevaluations show that ECP outperforms 10 benchmark algorithms including\nLipschitz, Bayesian, bandits, and evolutionary methods across 30\nmulti-dimensional non-convex synthetic and real-world optimization problems,\nwhich positions ECP as a competitive approach for global optimization.",
        "The dynamics of an electron-hole plasma governed by strong Coulomb\ninteraction is a challenging many-body problem.We report on experimental\nrealization of electron-hole many-body states in the picosecond time scale,\nwith tunable densities in a representative semiconductor Cu$_2$O. By using\ntime-resolved optical-pump terahertz third-harmonic-generation spectroscopy, we\nstudy the nonlinear terahertz dynamical characteristics of the many-body\nelectron-hole states. We find not only efficient and nonperturbative terahertz\nthird-harmonic yield associated with the excitonic formation, but also a\nnonmonotonic dependence of the excitonic nonlinear response on the\nelectron-hole density, reflecting the exciton dissociation at high charge\ndensity. Our results provide an efficient excitonic sensing of the\nfar-from-equilibrium electron-hole many-body states.",
        "We investigate the integration of stacked intelligent metasurfaces (SIMs)\ninto cell-free massive multiple input multiple output (CF-mMIMO) system to\nenhance the simultaneous wireless information and power transfer (SWIPT)\nperformance. Closed-form expressions for the spectral efficiency (SE) of the\ninformation-decoding receivers (IRs) and the average sum of harvested energy\n(sum-HE) at the energy-harvesting receivers (ERs) in the novel system model are\nderived to subsequently formulate a maximum total average sum-HE problem under\na minimum SE threshold per each IR. This problem jointly optimizes the SIM\nphase-shift (PS) configuration and access points' (APs) power allocation,\nrelying on long-term statistical channel state information (CSI). This\nnon-convex problem is then transformed into more tractable forms. Then,\nefficient algorithms are proposed, including a layer-by-layer heuristic method\nfor SIMs PS configuration that prioritizes sum-HE for the ERs and a successive\nconvex approximation (SCA)-based power allocation scheme to improve the\nachievable SE for the IRs. Numerical results show that our proposed algorithms\nachieve an almost 7-fold sum-HE gain as we increase the number of SIM layers,\nwhile the proposed power allocation (PPA) scheme often gains up to 40% in terms\nof the achievable minimum SE, compared to the equal power allocation.",
        "For each prime p other than 3, and each power q=p^k, we present two large\nclasses of permutation polynomials over F_{q^2} of the form X^r B(X^{q-1})\nwhich have at most five terms, where B(X) is a polynomial with coefficients in\n{1,-1}. The special case p=2 of our results comprises a vast generalization of\n76 recent results and conjectures in the literature. In case p>2, no instances\nof our permutation polynomials have appeared in the literature, and the\nconstruction of such polynomials had been posed as an open problem. Our proofs\nare short and involve no computations, in contrast to the proofs of many of the\nspecial cases of our results which were published previously.",
        "The lindgrenite compounds [Cu3(MoO4)2(OH)2] with various architectures and\nhigh crystallinity were prepared by a simple surfactant-assisted hydrothermal\nmethod. Then, the Cu3Mo2O9 samples were prepared by calcination of the\nas-synthesized Cu3(MoO4)2(OH)2. The resulting samples have high crystallinity,\ncolloidal properties, high-yield, large-scale production capability with using\nof nontoxic and inexpensive reagents and water as an environmentally solvent.\nThe scanning electron microscope studies show that the as-prepared lindgrenite\nnanostructures are well crystallized with rod, sheet and hollow sphere\nmorphologies. Meanwhile, the photoluminescence and magnetic properties of the\nnanosheet samples have been investigated that the both of Cu3(MoO4)2(OH)2 and\nCu3Mo2O9 samples have super paramagnetic behavior at room temperature and in\ncomparison with previous works, Cu3(MoO4)2(OH)2 and Cu3Mo2O9 samples\nsynthesized by the surfactant-assisted hydrothermal method in this work have a\nvery obvious red-shifted PL emission and high intensity.",
        "We give matching upper and lower bounds for the Dirichlet heat kernel of a\nSchr\\\"odinger operator $\\Delta+W$ in the domain above the graph of a bounded\nLipschitz function, in the case when $W$ decays away from the boundary faster\nthan quadratically.",
        "The morphology of the Horizontal Branch (HB) in Globular Clusters (GC) is\namong the early evidences that they contain multiple populations of stars.\nIndeed, the location of each star along the HB depends both on its initial\nhelium content (Y) and on the global average mass loss along the red giant\nbranch ($\\mu$). In most GCs, it is generally straightforward to analyse the\nfirst stellar population (standard Y), and the most extreme one (largest Y),\nwhile it is more tricky to look at the \"intermediate\" populations (mildly\nenhanced Y). In this work, we do this for the GCs NGC6752 and NGC2808; wherever\npossible the helium abundance for each stellar populations is constrained by\nusing independent measurements present in the literature. We compare population\nsynthesis models with photometric catalogues from the Hubble Space Telescope\nTreasury survey to derive the parameters of these HB stars. We find that the\nlocation of helium enriched stars on the HB is reproduced only by adopting a\nhigher value of $\\mu$ with respect to the first generation stars in all the\nanalysed stellar populations. We also find that $\\mu$ correlates with the\nhelium enhancement of the populations. This holds for both clusters. This\nfinding is naturally predicted by the model of ''pre-main sequence disc early\nloss'', previously suggested in the literature, and is consistent with the\nfindings of multiple-populations formation models that foresee the formation of\nsecond generation stars in a cooling flow.",
        "In this paper, we conduct an in-depth investigation into the optical images\nof boson stars with the solitonic potential. In the context of a celestial\nsource and a thin accretion disk, the optical characteristics of the soliton\nboson star have been derived. Considering the influence of the initial scalar\nfield $\\psi_0$ and a larger coupling parameter $\\alpha$ (the weak coupling\ncase), the optical images of boson stars primarily exhibit direct and lensed\nimages. The results demonstrate that variations in $\\psi_0$ and $\\alpha$\ninfluence the image size, whereas the observer's inclination angle $\\theta$ has\na substantial impact on the image shape. In contrast, when the coupling\nparameter $\\alpha$ is small (the strong coupling case), a sub-annular structure\nemerges within the Einstein ring for a spherically symmetric light source. In\nthe presence of a thin accretion disk, higher-order gravitational lensing\nimages emerge, indicating that photons are capable of orbiting the equatorial\nplane of the boson star multiple times. We also analyze how the effective\npotential and redshift factor depend on the parameters $\\psi_0$, $\\alpha$, and\n$\\theta$. The results indicate that at smaller values of $\\theta$,\ngravitational redshift is the dominant effect, resulting in an optical image\nfeaturing a bright ring surrounding a comparatively dim central region. At\nlarger values of $\\theta$, the Doppler effect becomes more pronounced,\nresulting in a substantial brightness disparity between the left and right\nsides of the optical image. These findings offer robust theoretical\nunderpinnings for differentiating solitonic boson stars from black holes via\nhigh-resolution astronomical observations.",
        "Dynamical systems with quadratic or polynomial drift exhibit complex\ndynamics, yet compared to nonlinear systems in general form, are often easier\nto analyze, simulate, control, and learn. Results going back over a century\nhave shown that the majority of nonpolynomial nonlinear systems can be recast\nin polynomial form, and their degree can be reduced further to quadratic. This\nprocess of polynomialization\/quadratization reveals new variables (in most\ncases, additional variables have to be added to achieve this) in which the\nsystem dynamics adhere to that specific form, which leads us to discover new\nstructures of a model. This chapter summarizes the state of the art for the\ndiscovery of polynomial and quadratic representations of finite-dimensional\ndynamical systems. We review known existence results, discuss the two prevalent\nalgorithms for automating the discovery process, and give examples in form of a\nsingle-layer neural network and a phenomenological model of cell signaling.",
        "We introduce a universal method for accelerating Lindblad dynamics that\npreserves the original trajectory. The technique provides exact fast processes\nanalytically, which are Markovian with time-independent Lindblad operators, by\ntime-rescaling a reference dynamics. In particular, the engineered control\nprotocols are based only on local interactions, and no additional control\nfields are required compared to the reference protocol. We demonstrate the\nscheme with two examples: a driven two-level system in an amplitude damping\nchannel and the dissipative transverse field Ising model. Our approach can help\nadvance techniques for quantum control and computation towards more complex\nnoisy systems.",
        "This work investigates the problem of Oblivious Transfer (OT) over a noisy\nMultiple Access Channel (MAC) involving two non-colluding senders and a single\nreceiver. The channel model is characterized by correlations among the parties,\nwith the parties assumed to be either honest-but-curious or, in the receiver's\ncase, potentially malicious. We propose a multiparty protocol for\nhonest-but-curious parties where the general MAC is reduced to a certain\ncorrelation. In scenarios where the receiver is malicious, the protocol\nachieves an achievable rate region.",
        "This paper investigates the existence and qualitative properties of\nminimizers for a class of nonlocal micromagnetic energy functionals defined on\nbounded domains. The considered energy functional consists of a symmetric\nexchange interaction, which penalizes spatial variations in magnetization, and\na magnetostatic self-energy term that accounts for long-range dipolar\ninteractions. Motivated by the extension of Brown's fundamental theorem on fine\nferromagnetic particles to nonlocal settings, we develop a rigorous variational\nframework in $L^2(\\Omega;\\mathbb{S}^2)$ under mild assumptions on the\ninteraction kernel \\( j \\), including symmetry, L\\'evy-type integrability, and\nprescribed singular behavior. For spherical domains, we generalize Browns\nfundamental results by identifying critical radii $R^*$ and $R^{**}$ that\ndelineate distinct energetic regimes: for \\( R \\leq R^* \\), the uniform\nmagnetization state is energetically preferable (\\emph{small-body regime}),\nwhereas for $R \\geq R^{**}$, non-uniform magnetization configurations become\ndominant (\\emph{large-body regime}). These transitions are analyzed through\nPoincar\\'e-type inequalities and explicit energy comparisons between uniform\nand vortex-like magnetization states.\n  Our results directly connect classical micromagnetic theory and contemporary\nnonlocal models, providing new insights into domain structure formation in\nnanoscale magnetism. Furthermore, the mathematical framework developed in this\nwork contributes to advancing theoretical foundations for applications in\nspintronics and data storage technologies.",
        "The standard model of particle physics is a well-established theoretical\nframework, yet there remain several unresolved issues that warrant further\nexperimental and theoretical exploration. In the realm of quark physics, these\ninclude understanding the nature of quark confinement and elucidating the\nmechanism linking quarks and gluons to strongly interacting particles within\nthe standard model theory, which may offer insights into underlying physics\nmechanisms. These inquiries can be addressed through the study of hadron\nproduction in open-charm meson pair final states at $e^+e^-$ annihilations\nutilizing the capabilities of BABAR, Belle, BESIII, and CLEO-c experiments,\nwhich have yielded valuable insights into non-standard hadrons over recent\ndecades. This review examines the contributions of $e^+e^-$ colliders from\nBABAR, Belle, BESIII, and CLEO-c experiments to such studies and discusses\nfuture prospects for $e^+e^-$ collider experiments.",
        "We develop a structure theory for the limit of $SU(2)$ $G_2$-monopoles (resp.\nCalabi-Yau monopoles) on a principal $SU(2)$-bundle over an asymptotically\nconical $G_2$-manifolds (resp. Calabi-Yau 3-folds) as the mass parameter tends\nto infinity, while the topologial data for the bundle stays fixed. We show how\nto extract a singular abelian $G_2$-monopole (resp. Calabi-Yau monopole) with\nDirac singularity along a calibrated cycle in the large mass limit, and we\nprove an energy identity for monopole bubbles.",
        "We investigated the impact of photometric redshift errors in the ongoing\nJavalambre Physics of the Accelerating Universe Astrophysical Survey (J-PAS) on\nvoid identification and properties using a watershed-based method, aiming to\nassess the recovery of individual voids and the overall void environment. We\ncreated galaxy mock catalogues for redshift z = 0.1 using the IllustrisTNG300-1\nsimulation, defining two datasets: an $ideal$ sample ($m_r < 21$ mag) and a\n$perturbed$ sample with the Z-coordinate errors mimicking J-PAS's line-of-sight\nerrors, derived from the precursor miniJPAS survey data. We identified voids\nusing ZOBOV, a watershed algorithm. We found 1065 voids in the $ideal$ sample\nand 2558 voids in the $perturbed$ sample. The $perturbed$ sample voids have, on\naverage, smaller sizes and denser interiors. We filtered out voids based on\ndensity and radius in order to eliminate overdense and small spurious\ninstances. The stacked density profile of filtered voids in the $perturbed$\nsample remains close to the average density even at the boundary peak,\nindicating a strong blurring of structures by the redshift errors. The number\nof $ideal$ sample voids for which at least $50\\%$ of the volume is recovered by\na void in the $perturbed$ sample is 53 (29 for the filtered sample). The volume\noccupied by these voids is less than $10\\%$ of the simulation volume. Merging\nvoids in the $perturbed$ sample marginally improves the recovery. The overall\nvolumes defined as voids in the two samples have an overlap of $80\\%$, making\nup $61\\%$ of the simulation box volume. While some statistical properties of\nvoids might be recovered sufficiently well, the watershed algorithms may not be\noptimal for recovering the large-scale structure voids if applied straight to\nphotometric redshift survey data.",
        "Let $M=G\/H$ be a Riemannian homogeneous space, where $G$ is a compact Lie\ngroup with closed subgroup $H$. Classical intersection theory states that the\nde Rham cohomology ring of $M$ describes the signed count of intersection\npoints of submanifolds $Y_1, \\ldots, Y_s$ of $M$ in general position, when the\ncodimensions add up to $\\dim M$.\n  We introduce the probabilistic intersection ring $\\mathrm{H}_{\\mathbb E}(M)$,\nwhose multiplication describes the unsigned count of intersection points, when\nthe $Y_i$ are randomly moved by independent uniformly random elements of $G$.\nThe probabilistic intersection ring $\\mathrm{H}_{\\mathbb E}(M)$ has the\nstructure of a graded commutative and associative real Banach algebra. It is\ndefined as a quotient of the ring of Grassmann zonoids of a fixed cotangent\nspace $V$ of $M$. The latter was introduced by the authors in [Adv. Math. 402,\n2022]. There is a close connection to valuations of convex bodies:\n$\\mathrm{H}_{\\mathbb E}(M)$ can be interpreted as a subspace of the space of\ntranslation invariant, even, continuous valuations on $V$, whose multiplication\ncoincides with Alesker's multiplication for smooth valuations.\n  We describe the ring structure of the probabilistic intersection ring for\nspheres, real projective space and complex projective space, relying on Fu [J.\nDiff. Geo. 72(3), 2006] for the latter case. From this, we derive an\ninteresting probabilistic intersection formula in complex projective space.\nFinally, we initiate the investigation of the probabilistic intersection ring\nfor real Grassmannians, outlining the construction of a probabilistic version\nof Schubert Calculus.",
        "Survival analysis, a foundational tool for modeling time-to-event data, has\nseen growing integration with machine learning (ML) approaches to handle the\ncomplexities of censored data and time-varying risks. Despite these advances,\nleveraging state-of-the-art survival models remains a challenge due to the\nfragmented nature of existing implementations, which lack standardized\ninterfaces and require extensive preprocessing. We introduce SurvHive, a\nPython-based framework designed to unify survival analysis methods within a\ncoherent and extensible interface modeled on scikit-learn. SurvHive integrates\nclassical statistical models with cutting-edge deep learning approaches,\nincluding transformer-based architectures and parametric survival models. Using\na consistent API, SurvHive simplifies model training, evaluation, and\noptimization, significantly reducing the barrier to entry for ML practitioners\nexploring survival analysis. The package includes enhanced support for\nhyper-parameter tuning, time-dependent risk evaluation metrics, and\ncross-validation strategies tailored to censored data. With its extensibility\nand focus on usability, SurvHive provides a bridge between survival analysis\nand the broader ML community, facilitating advancements in time-to-event\nmodeling across domains. The SurvHive code and documentation are available\nfreely at https:\/\/github.com\/compbiomed-unito\/survhive.",
        "Recent numerical works have revealed the instability of many-body localized\n(MBL) phase in disordered quantum many-body systems with finite system sizes\nand over finite timescales. This instability arises from Griffith regions that\noccur at the thermodynamic limit, which rapidly thermalize and affect the\nsurrounding typical MBL regions, introducing an avalanche mechanism into the\nsystem. Here, we consider the $\\mathbb{Z}_2$-preserving interacting Ising\nMajorana chain model, which exhibits a more complex phase diagram, where an\nergodic phase emerges between two MBL phases with different long-range order\nproperties. We calculate the dynamic characteristics of the model when coupled\nto an infinite bath under perturbation, and through scaling behavior of the\nslowest thermalization rate, we find how critical disorder strengths in\nfinite-size systems are affected by the avalanche mechanism. We also employe\nthe embedded inclusion model and use the time evolution of mutual information\nbetween each spin and the artificial Griffith region to probe the diffusion of\nthe thermal bubble. We observe that in finite-sized systems, the critical\ndisorder strength gradually drifts away from the central. Our work demonstrate\nthat both MBL paramagnetic phase and MBL spin-glass phase are unstable at\nfinite sizes.",
        "Composites of ferroelectric and paraelectric perovskites have attracted a lot\nof attention due to their application potential in energy storage as well as\nnovel computing and memory devices. So far the main focus of research has been\non superlattices and ferroelectric particles in a paraelectric matrix, while\nthe impact of paraelectric inclusions on the ferroelectric matrix is\nsurprisingly underrepresented. To close this gap in knowledge we perform\nmolecular dynamics simulations using an $ab\\ initio$ derived effective\nHamiltonian for BaTiO$_3$--SrTiO$_3$ and reveal the dependency of phase\nstability and phase transitions on the size and distances of paraelectric\ninclusions. We discuss how the combination of compressive strain and\ndepolarization fields at the SrTiO$_3$ interfaces induces large local\npolarization, complex domain structures and coexisting phases as well as\ndiffuse phase transitions and reduced coercive fields.",
        "The Keck Planet Imager and Characterizer (KPIC) is a series of upgrades for\nthe Keck II Adaptive Optics (AO) system and the NIRSPEC spectrograph to enable\ndiffraction limited, high resolution (R>30000) spectroscopy of exoplanets and\nlow mass companions in the K and L bands. Phase I consisted of single mode\nfiber injection\/extraction units (FIU\/FEU) used in conjunction with a H band\npyramid wavefront sensor. The use of single mode fibers provides a gain in\nstellar rejection, a substantial reduction in sky background, and an extremely\nstable line spread function in the spectrograph. Phase II, deployed and\ncommissioned in 2022, brought a 1000 actuator deformable mirror, beam shaping\noptics, a vortex mask, and other upgrades to the FIU\/FEU. An additional service\nmission in 2024 extended operations down to y band, delivered an atmospheric\ndispersion corrector, and provided access to two laser frequency combs. KPIC\nphase II brings higher planet throughput, lower stellar leakage and many new\nobserving modes which extend its ability to characterize exoplanets at high\nspectral resolution, building on the success of phase I. In this paper we\npresent a description of the final phase II version of KPIC, along with results\nof system level laboratory testing and characterization showing the\ninstrument's phase II throughput, stability, repeatability, and other key\nperformance metrics prior to delivery and during installation at Keck. We\noutlined the capabilities of the various observing modes enabled by the new\nmodules as well as efforts to compensate for static aberrations and non common\npath errors at Keck, which were issues that plagued phase I. Finally, we show\nresults from commissioning.",
        "Understanding how renormalized quasiparticles emerge in strongly correlated\nelectron materials provides a challenge for both experiment and theory. It has\nbeen predicted that distinctive spin and orbital screening mechanisms drive\nthis process in multiorbital materials with strong Coulomb and Hund's\ninteractions. Here, we provide the experimental evidence of both mechanisms\nfrom angle-resolved photoemission spectroscopy on RbFe$_2$As$_2$. We observe\nthat the emergence of low-energy Fe 3$d_{xy}$ quasiparticles below 90K is tied\nto spin screening. A second process changes the spectral weight at high\nenergies up to room temperature. Supported by theoretical calculations we\nattribute it to orbital screening of Fe 3d atomic excitations. These two\ncascading screening processes drive the temperature evolution from a bad metal\nto a correlated Fermi liquid.",
        "1. An understanding of how biodiversity confers ecosystem stability is\ncrucial in managing ecosystems under major environmental changes. Multiple\nbiodiversity drivers can stabilize ecosystem functions over time. However, we\nknow little about how local environmental conditions can influence these\nbiodiversity drivers, and consequently how they indirectly shape the ecological\nstability of ecosystems.\n  2. We hypothesized that environmental factors can have opposite influences\n(i.e., not necessarily either positive or negative) on the temporal stability\nof communities in different environmental ranges depending on the biodiversity\ndrivers involved. We tested this novel hypothesis by using data from a\n4-year-long field study of submerged macrophyte across a water depth gradient\nin 8 heterogeneous bays of Erhai lake (with total sample size of 30,071\nquadrats), a large lentic system in China.\n  3. Results indicate that a unimodal pattern of stability in temporal biomass\nmeasurements occurred along the water-depth gradient, and that multiple\nbiodiversity drivers (the asynchrony in species dynamics, and the stability of\ndominant species) generally increased the temporal stability of aquatic primary\nproducers. However, the effect of water depth either increased or decreased the\nstability of biomass according to the environmental conditions associated with\nsites along the water depth gradient.\n  4. Synthesis. These results reveal the influence of local environmental\nconditions on the biodiversity drivers of stability may help predict the\nfunctional consequences of biodiversity change across different scenarios of\nenvironmental change.",
        "We present a review of the current and future industrial applications of\nneutrinos. We address the industrial applications of neutrinos in geological\nand geochemical studies of the Earth's interior, in monitoring earthquakes, in\nterrestrial communications, in applications for submarines, in monitoring\nnuclear power plants and fusion reactors, in the management of fissile\nmaterials used in nuclear plants, in tracking nuclear tests, among other\napplications. We also address future possibilities for industrial applications\nof neutrinos, especially concerning communications in the solar system and\ngeotomography of solar system bodies.",
        "Compact symmetric objects (CSOs) represent a key early stage in radio galaxy\nevolution, but their reliable identification remains challenging. We develop a\nmethod to identify CSOs by combining Gaia optical astrometry with VLBI radio\nimaging. We analyze 40 CSO candidates by overlaying Gaia DR3 positions on VLBI\nmaps to locate their central engines. CSOs are confirmed when Gaia positions\nlie between symmetric radio lobes, while core-jet sources show optical\npositions coinciding with one end of the radio structure. We verify\nclassifications using spectral indices, variability, and jet kinematics from\nmulti-epoch VLBI observations. Our method identified 22 genuine CSOs and 10\ncore-jet sources, with 8 objects remaining ambiguous. Confirmed CSOs show\nkinematic ages from 20 to over 1000 years and hotspot speeds typically below\n0.5c. Five nearby CSOs show optical-radio offsets despite strong CSO\nmorphology, indicating host galaxy influence. The Gaia-VLBI method provides a\nreliable CSO identification tool. Our sample reveals diverse radio powers,\nsuggesting multiple evolutionary paths. CSO evolution appears influenced by\nboth intrinsic jet power and environmental factors, with high-power CSOs\npotentially evolving into large-scale radio galaxies while low-power CSOs often\nshow confinement by their host environments.",
        "Artificial Intelligence increasingly pursues large, complex models that\nperform many tasks within increasingly realistic domains. How, if at all,\nshould these developments in AI influence cognitive science?\n  We argue that progress in AI offers timely opportunities for cognitive\nscience to embrace experiments with increasingly naturalistic stimuli, tasks,\nand behaviors; and computational models that can accommodate these changes. We\nfirst review a growing body of research spanning neuroscience, cognitive\nscience, and AI that suggests that incorporating a broader range of\nnaturalistic experimental paradigms (and models that accommodate them) may be\nnecessary to resolve some aspects of natural intelligence and ensure that our\ntheories generalize. We then suggest that integrating recent progress in AI and\ncognitive science will enable us to engage with more naturalistic phenomena\nwithout giving up experimental control or the pursuit of theoretically grounded\nunderstanding. We offer practical guidance on how methodological practices can\ncontribute to cumulative progress in naturalistic computational cognitive\nscience, and illustrate a path towards building computational models that solve\nthe real problems of natural cognition - together with a reductive\nunderstanding of the processes and principles by which they do so.",
        "Ramsey oscillations typically exhibit an exponential decay envelope due to\nenvironmental noise. However, recent experiments have observed nonmonotonic\nRamsey fringes characterized by beating patterns, which deviate from the\nstandard behavior. These beating patterns have primarily been attributed to\ncharge-noise fluctuations. In this paper, we investigate the flux-noise origin\nof these nonmonotonic Ramsey fringes in frequency-tunable transmon qubits. We\ndevelop a random telegraph noise (RTN) model to simulate the impact of\ntelegraph-like flux-noise sources on Ramsey oscillations. Our simulations\ndemonstrate that strong flux-RTN sources can induce beating patterns in the\nRamsey fringes, showing excellent agreement with experimental observations in\ntransmon qubits influenced by electronic environment-induced flux-noise. Our\nfindings provide valuable insights into the role of flux-noise in qubit\ndecoherence and underscore the importance of considering flux-noise RTN when\nanalyzing nonmonotonic Ramsey fringes.",
        "In recent papers on spacetimes with a signature-changing metric, the concept\nof a Lorentzian-Euclidean black hole and new elements for Lorentzian-Riemannian\nsignature change have been introduced. A Lorentzian-Euclidean black hole is a\nsignature-changing modification of the Schwarzschild spacetime satisfying the\nvacuum Einstein equations in a weak sense. Here the event horizon serves as a\nboundary beyond which time becomes imaginary. We demonstrate that the proper\ntime needed to reach the horizon remains finite, consistently with the\nclassical Schwarzschild solution. About Lorentzian to Riemannian metric\ntransitions, we stress that the hypersurface where the metric signature changes\nis naturally a spacelike hypersurface which might be identified with the future\nor past causal boundary of the Lorentzian sector. Moreover, a number of\ngeometric interpretations appear, as the degeneracy of the metric corresponds\nto the collapse of the causal cones into a line, the degeneracy of the dual\nmetric corresponds to collapsing into a hyperplane, and additional geometric\nstructures on the transition hypersurface (Galilean and dual Galilean) might be\nexplored."
      ]
    }
  },
  {
    "id":2412.17907,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Recognizing and reducing cognitive bias in clinical and forensic neurology",
    "start_abstract":"In medicine, cognitive errors form the basis of bias in clinical practice. Several types are common and pervasive, may lead to inaccurate diagnosis or treatment. Forensic neurology, even when aided by current technologies, still dependent on interpretations, therefore prone bias. This article discusses 4 biases that can clinician astray. They confirmation (selective gathering neglect contradictory evidence); base rate (ignoring misusing prevailing data); hindsight (oversimplification past causation); good old days (the tendency for patients misremember exaggerate their preinjury functioning). We briefly describe strategies adopted from field psychology could minimize While debiasing is not easy, reducing such requires awareness acknowledgment our susceptibility these distortions.",
    "start_categories":[
      "Clinical Neurology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Deep Facial Expression Recognition: A Survey"
      ],
      "abstract":[
        "With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and recent success deep learning techniques in various fields, neural networks have increasingly been leveraged learn discriminative representations for automatic FER. Recent FER systems generally focus on two important issues: overfitting caused by a lack sufficient training data expression-unrelated variations, such as illumination, head pose identity bias. In this paper, we provide comprehensive survey FER, including datasets algorithms that insights into these intrinsic problems. First, describe standard pipeline system with related background knowledge suggestions applicable implementations each stage. We then introduce available are widely used literature accepted selection evaluation principles datasets. For state art review existing novel strategies designed based both static images dynamic image sequences, discuss their advantages limitations. Competitive performances benchmarks also summarized section. extend our additional issues application scenarios. Finally, remaining challenges corresponding opportunities field well future directions design robust systems."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Mitigating Ambiguities in 3D Classification with Gaussian Splatting",
        "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?",
        "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding",
        "SemiHMER: Semi-supervised Handwritten Mathematical Expression\n  Recognition using pseudo-labels",
        "A Constant Rate Quantum Computer on a Line",
        "Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal\n  Structures from Multi-view Joint Cloud",
        "Study of gravitational waves from phase transitions in three-component\n  dark matter",
        "Smart Cubing for Graph Search: A Comparative Study",
        "Seeing Delta Parameters as JPEG Images: Data-Free Delta Compression with\n  Discrete Cosine Transform",
        "Towards bandit-based prompt-tuning for in-the-wild foundation agents",
        "Integrating Causality with Neurochaos Learning: Proposed Approach and\n  Research Agenda",
        "On Gegenbauer polynomials and Wronskian determinants of trigonometric\n  functions",
        "Tackling Compressible Turbulent Multi-Component Flows with Dynamic\n  hp-Adaptation",
        "MedVAE: Efficient Automated Interpretation of Medical Images with\n  Large-Scale Generalizable Autoencoders",
        "Quantum effects in surface diffusion: application to diffusion of\n  nitrogen adatoms over GaN(0001) surface",
        "Real-Time 3D Magnetic Field Camera for a Spherical Volume",
        "A Modular Pipeline for 3D Object Tracking Using RGB Cameras",
        "Concentration around a stable equilibrium for the non-autonomous\n  $\\Phi_3^4$ model",
        "Quasi-compactness and statistical properties for discontinuous systems\n  semi-conjugated to piecewise convex maps with countable branches",
        "A central limit theorem for the giant in a stochastic block model",
        "Comprehensive Review of Neural Differential Equations for Time Series\n  Analysis",
        "Deep Ensembling with Multimodal Image Fusion for Efficient\n  Classification of Lung Cancer",
        "Unstable accretion in TW Hya: 3D simulations and comparisons with\n  observations",
        "Improving Grip Stability Using Passive Compliant Microspine Arrays for\n  Soft Robots in Unstructured Terrain",
        "Quantum critical point followed by Kondo-like behavior due to Cu\n  substitution in itinerant, antiferromagnet ${\\text{La}_{2}\\text{(Cu}_{x}\\text\n  {Ni}_{1-x})_7}$",
        "Quantum Communication Multiplexing in LP-modes Enabled by Photonic\n  Lanterns",
        "Shapiro Steps Observed in a Two-Dimensional Yukawa Solid Modulated by a\n  One-Dimensional Vibrational Periodic Substrate",
        "AdaNDV: Adaptive Number of Distinct Value Estimation via Learning to\n  Select and Fuse Estimators",
        "Fixing the Double Penalty in Data-Driven Weather Forecasting Through a\n  Modified Spherical Harmonic Loss Function"
      ],
      "abstract":[
        "3D classification with point cloud input is a fundamental problem in 3D\nvision. However, due to the discrete nature and the insufficient material\ndescription of point cloud representations, there are ambiguities in\ndistinguishing wire-like and flat surfaces, as well as transparent or\nreflective objects. To address these issues, we propose Gaussian Splatting (GS)\npoint cloud-based 3D classification. We find that the scale and rotation\ncoefficients in the GS point cloud help characterize surface types.\nSpecifically, wire-like surfaces consist of multiple slender Gaussian\nellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.\nAdditionally, the opacity in the GS point cloud represents the transparency\ncharacteristics of objects. As a result, ambiguities in point cloud-based 3D\nclassification can be mitigated utilizing GS point cloud as input. To verify\nthe effectiveness of GS point cloud input, we construct the first real-world GS\npoint cloud dataset in the community, which includes 20 categories with 200\nobjects in each category. Experiments not only validate the superiority of GS\npoint cloud input, especially in distinguishing ambiguous objects, but also\ndemonstrate the generalization ability across different classification methods.",
        "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps:\/\/github.com\/sougata-ub\/reading_between_lines",
        "In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.",
        "In this paper, we study semi-supervised Handwritten Mathematical Expression\nRecognition (HMER) via exploring both labeled data and extra unlabeled data. We\npropose a novel consistency regularization framework, termed SemiHMER, which\nintroduces dual-branch semi-supervised learning. Specifically, we enforce\nconsistency between the two networks for the same input image. The\npseudo-label, generated by one perturbed recognition network, is utilized to\nsupervise the other network using the standard cross-entropy loss. The SemiHMER\nconsistency encourages high similarity between the predictions of the two\nperturbed networks for the same input image and expands the training data by\nleveraging unlabeled data with pseudo-labels. We further introduce a\nweak-to-strong strategy by applying different levels of augmentation to each\nbranch, effectively expanding the training data and enhancing the quality of\nnetwork training. Additionally, we propose a novel module, the Global Dynamic\nCounting Module (GDCM), to enhance the performance of the HMER decoder by\nalleviating recognition inaccuracies in long-distance formula recognition and\nreducing the occurrence of repeated characters. The experimental results\ndemonstrate that our work achieves significant performance improvements, with\nan average accuracy increase of 5.47% on CROHME14, 4.87% on CROHME16, and 5.25%\non CROHME19, compared to our baselines.",
        "We prove by construction that the Bravyi-Poulin-Terhal bound on the spatial\ndensity of stabilizer codes does not generalize to stabilizer circuits. To do\nso, we construct a fault tolerant quantum computer with a coding rate above 5%\nand quasi-polylog time overhead, out of a line of qubits with nearest-neighbor\nconnectivity, and prove it has a threshold. The construction is based on\nmodifications to the tower of Hamming codes of Yamasaki and Koashi (Nature\nPhysics, 2024), with operators measured using a variant of Shor's measurement\ngadget.",
        "Multi-person motion capture over sparse angular observations is a challenging\nproblem under interference from both self- and mutual-occlusions. Existing\nworks produce accurate 2D joint detection, however, when these are triangulated\nand lifted into 3D, available solutions all struggle in selecting the most\naccurate candidates and associating them to the correct joint type and target\nidentity. As such, in order to fully utilize all accurate 2D joint location\ninformation, we propose to independently triangulate between all same-typed 2D\njoints from all camera views regardless of their target ID, forming the Joint\nCloud. Joint Cloud consist of both valid joints lifted from the same joint type\nand target ID, as well as falsely constructed ones that are from different 2D\nsources. These redundant and inaccurate candidates are processed over the\nproposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving\nthree cascaded encoders which deeply explore the trajectile, skeletal\nstructural, and view-dependent correlations among all 3D point candidates in\nthe cross-embedding space. An Optimal Token Attention Path (OTAP) module is\nproposed which subsequently selects and aggregates informative features from\nthese redundant observations for the final prediction of human motion. To\ndemonstrate the effectiveness of JCSAT, we build and publish a new multi-person\nmotion capture dataset BUMocap-X with complex interactions and severe\nocclusions. Comprehensive experiments over the newly presented as well as\nbenchmark datasets validate the effectiveness of the proposed framework, which\noutperforms all existing state-of-the-art methods, especially under challenging\nocclusion scenarios.",
        "This paper presents a dark matter model comprising three types of particles\nwith distinct spins, along with a scalar field $\\phi$ that mediates\ninteractions between Standard Model particles and dark matter. It discusses the\nelectroweak phase transition following the Big Bang, during which all particles\nare initially massless due to the inactive Higgs mechanism. As temperature\ndecreases, the effective potential reaches zero at two points, leading to two\nminima at the critical temperature ($T_c$), and eventually to a true vacuum\nstate. The formation of new vacuum bubbles, where electroweak symmetry is\nbroken and particles acquire mass, generates gravitational waves as these\nbubbles interact with the fabric of space-time. The paper derives the\ngravitational wave frequency and detection range based on the model's\nparameters, aligning with observational data from the Planck satellite and\ndetection thresholds from PandaX-4T. It concludes by comparing the predicted\nbackground gravitational wave density with the sensitivities of LISA and BBO\ndetectors.",
        "Parallel solving via cube-and-conquer is a key method for scaling SAT solvers\nto hard instances. While cube-and-conquer has proven successful for pure SAT\nproblems, notably the Pythagorean triples conjecture, its application to SAT\nsolvers extended with propagators presents unique challenges, as these\npropagators learn constraints dynamically during the search.\n  We study this problem using SAT Modulo Symmetries (SMS) as our primary test\ncase, where a symmetry-breaking propagator reduces the search space by learning\nconstraints that eliminate isomorphic graphs. Through extensive experimentation\ncomprising over 10,000 CPU hours, we systematically evaluate different\ncube-and-conquer variants on three well-studied combinatorial problems. Our\nmethodology combines prerun phases to collect learned constraints, various\ncubing strategies, and parameter tuning via algorithm configuration and\nLLM-generated design suggestions.\n  The comprehensive empirical evaluation provides new insights into effective\ncubing strategies for propagator-based SAT solving, with our best method\nachieving speedups of 2-3x from improved cubing and parameter tuning, providing\nan additional 1.5-2x improvement on harder instances.",
        "With transformer-based models and the pretrain-finetune paradigm becoming\nmainstream, the high storage and deployment costs of individual finetuned\nmodels on multiple tasks pose critical challenges. Delta compression attempts\nto lower the costs by reducing the redundancy of delta parameters (i.e., the\ndifference between the finetuned and pre-trained model weights). However,\nexisting methods usually face problems including data accessibility and\ntraining requirements. To tackle this issue, we introduce Delta-DCT, the first\ndata-free delta compression method inspired by classic JPEG image compression,\nleveraging the Discrete Cosine Transform (DCT). We first (a) group delta\nparameters within a layer into patches. Then we (b) assess the importance of\neach patch and allocate them with different quantization bit-widths.\nAfterwards, we (c) convert these patches to the DCT domain and conduct\nquantization to each patch based on the allocated bit-width. The proposed\nDelta-DCT does not require any training or data calibration, while achieving\nperformance comparable to or even surpassing original finetuned models under\n1-bit equivalent delta compression ratios on different kinds of models\nincluding: (1) recently-released LLMs of different sizes from 7B to 13B, (2)\nrelatively smaller language models including RoBERTa and T5 models, (3)\nvariants of vision transformer models, and (4) multi-modal BEiT-3 models.",
        "Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.",
        "Deep learning implemented via neural networks, has revolutionized machine\nlearning by providing methods for complex tasks such as object\ndetection\/classification and prediction. However, architectures based on deep\nneural networks have started to yield diminishing returns, primarily due to\ntheir statistical nature and inability to capture causal structure in the\ntraining data. Another issue with deep learning is its high energy consumption,\nwhich is not that desirable from a sustainability perspective.\n  Therefore, alternative approaches are being considered to address these\nissues, both of which are inspired by the functioning of the human brain. One\napproach is causal learning, which takes into account causality among the items\nin the dataset on which the neural network is trained. It is expected that this\nwill help minimize the spurious correlations that are prevalent in the learned\nrepresentations of deep neural networks. The other approach is Neurochaos\nLearning, a recent development, which draws its inspiration from the nonlinear\nchaotic firing intrinsic to neurons in biological neural networks\n(brain\/central nervous system). Both approaches have shown improved results\nover just deep learning alone.\n  To that end, in this position paper, we investigate how causal and neurochaos\nlearning approaches can be integrated together to produce better results,\nespecially in domains that contain linked data. We propose an approach for this\nintegration to enhance classification, prediction and reinforcement learning.\nWe also propose a set of research questions that need to be investigated in\norder to make this integration a reality.",
        "M. E. Larsen evaluated the Wronskian determinant of functions\n$\\{\\sin(mx)\\}_{1\\le m \\le n}$. We generalize this result and compute the\nWronskian of $\\{\\sin(mx)\\}_{1\\le m \\le n-1}\\cup \\{\\sin((k+n)x\\} $. We show that\nthis determinant can be expressed in terms of Gegenbauer orthogonal polynomials\nand we give two proofs of this result: a direct proof using recurrence\nrelations and a less direct (but, possibly, more instructive) proof based on\nDarboux-Crum transformations.",
        "In this paper, we present an hp-adaptive hybrid Discontinuous Galerkin\/Finite\nVolume method for simulating compressible, turbulent multi-component flows.\nBuilding on a previously established hp-adaptive strategy for hyperbolic gas-\nand droplet-dynamics problems, this study extends the hybrid DG\/FV approach to\nviscous flows with multiple species and incorporates non-conforming interfaces,\nenabling enhanced flexibility in grid generation. A central contribution of\nthis work lies in the computation of both convective and dissipative fluxes\nacross non-conforming element interfaces of mixed discretizations. To achieve\naccurate shock localization and scale-resolving representation of turbulent\nstructures, the operator dynamically switches between an h-refined FV sub-cell\nscheme and a p-adaptive DG method, based on an a priori modal solution\nanalysis. The method is implemented in the high-order open-source framework\nFLEXI and validated against benchmark problems, including the supersonic\nTaylor-Green vortex and a triplepoint shock interaction, demonstrating its\nrobustness and accuracy for under-resolved shock-turbulence interactions and\ncompressible multi-species scenarios. Finally, the method's capabilities are\nshowcased through an implicit large eddy simulation of an under-expanded\nhydrogen jet mixing with air, highlighting its potential for tackling\nchallenging compressible multi-species flows in engineering.",
        "Medical images are acquired at high resolutions with large fields of view in\norder to capture fine-grained features necessary for clinical decision-making.\nConsequently, training deep learning models on medical images can incur large\ncomputational costs. In this work, we address the challenge of downsizing\nmedical images in order to improve downstream computational efficiency while\npreserving clinically-relevant features. We introduce MedVAE, a family of six\nlarge-scale 2D and 3D autoencoders capable of encoding medical images as\ndownsized latent representations and decoding latent representations back to\nhigh-resolution images. We train MedVAE autoencoders using a novel two-stage\ntraining approach with 1,052,730 medical images. Across diverse tasks obtained\nfrom 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent\nrepresentations in place of high-resolution images when training downstream\nmodels can lead to efficiency benefits (up to 70x improvement in throughput)\nwhile simultaneously preserving clinically-relevant features and (2) MedVAE can\ndecode latent representations back to high-resolution images with high\nfidelity. Our work demonstrates that large-scale, generalizable autoencoders\ncan help address critical efficiency challenges in the medical domain. Our code\nis available at https:\/\/github.com\/StanfordMIMI\/MedVAE.",
        "It is shown that quantum effects play determining role in nitrogen adatom\ndiffusion due to several different factors. This could be related to the change\nof the energy of the quantum states and also due to the redistribution of\nelectrons between the quantum states, both full and resonant, via quantum\nstatistics partially governed by the Fermi energy level. These effects were\nstudied in the case of nitrogen diffusion over clean and gallium covered\nGa-terminated GaN(0001) surface. For the fractional coverage the density\nfunctional theory (DFT) calculations show that at the saddle point\nconfiguration the redistribution of electrons between different quantum states\nmay affect the surface diffusion barrier significantly. The other quantum\ninfluence occurs via the change of the minimal energy configuration. Under\nfractional Ga coverage of GaN(0001) surface the nitrogen diffusion energy\nbarrier proceeds from the resonant states governed energy minimal H3 site\nacross the saddle point in the bridge configuration. At this path the barrier\nis affected the electron redistribution between surface quantum states both in\nthe initial and the saddle point. In the case of the full GaN coverage the\ndiffusion path is from on-top N adatom configuration via H3 site that\ncorresponds to maximal energy. Therefore the diffusion barrier is Ebar= 1.18 eV\nfor clean and Ebar= 0.92 eV for (1\/6) ML to finally Ebar= 1.23 eV for full Ga\ncoverage. Thus the overall barrier is reduced to Ebar= 0.92 eV due to quantum\nstatistics effects. The identified stable N on-top configuration for the full\ncoverage is essential for atomic mechanism of GaN growth in Ga-rich regime.",
        "Accurate and efficient volumetric magnetic field measurements are essential\nfor a wide range of applications. Conventional methods are often limited in\nterms of measurement speed and applicability, or suffer from scaling problems\nat larger volumes. This work presents the development of a magnetometer array\ndesigned to measure magnetic fields within a spherical volume at a frame rate\nof 10 Hz. The array consists of 3D Hall magnetometers positioned according to a\nspherical $t$-design, allowing simultaneous magnetic field data acquisition\nfrom the surface of the sphere. The approach enables the efficient\nrepresentation of all three components of the magnetic field inside the sphere\nusing a sixth-degree polynomial, significantly reducing measurement time\ncompared to sequential methods. This work details the design, calibration, and\nmeasurement methods of the array. To evaluate its performance, we compare it to\na sequential single-sensor measurement by examining a magnetic gradient field.\nThe obtained measurement uncertainties of approx. 1% show the applicability for\na variety of applications.",
        "Object tracking is a key challenge of computer vision with various\napplications that all require different architectures. Most tracking systems\nhave limitations such as constraining all movement to a 2D plane and they often\ntrack only one object. In this paper, we present a new modular pipeline that\ncalculates 3D trajectories of multiple objects. It is adaptable to various\nsettings where multiple time-synced and stationary cameras record moving\nobjects, using off the shelf webcams. Our pipeline was tested on the Table\nSetting Dataset, where participants are recorded with various sensors as they\nset a table with tableware objects. We need to track these manipulated objects,\nusing 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699\ncamera frames, determining camera poses, discriminating between nearby and\noverlapping objects, temporary occlusions, and finally calculating a 3D\ntrajectory using the right subset of an average of 11.12.456 pixel coordinates\nper 3-minute trial. We implement a robust pipeline that results in accurate\ntrajectories with covariance of x,y,z-position as a confidence metric. It deals\ndynamically with appearing and disappearing objects, instantiating new Extended\nKalman Filters. It scales to hundreds of table-setting trials with very little\nhuman annotation input, even with the camera poses of each trial unknown. The\ncode is available at https:\/\/github.com\/LarsBredereke\/object_tracking",
        "We consider time-dependent singular stochastic partial differential equations\non the three-dimensional torus. These equations are only well-posed after one\nadds renormalization terms. In order to construct a well-defined notion of\nsolution, one should put the equation in a more general setting, like the one\nof regularity structures. In this article, we consider the alternative paradigm\nof paracontrolled distributions, and get concentration results around a stable\ndeterministic equilibrium for solutions of non-autonomous generalizations of\nthe $(\\Phi_3^4)$ model.",
        "In this paper, we establish the quasi-compactness of the transfer operator\nassociated with skew product systems that are semi-conjugate to piecewise\nconvex maps with a countably infinite number of branches. These non-invertible\nskew products admit discontinuities, with the critical set confined to a\ncountable collection of fibers. Furthermore, we demonstrate that such systems\npossess an invariant measure whose disintegration along the fibers exhibits\nbounded variation, a concept introduced and developed in this work.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Time series modeling and analysis has become critical in various domains.\nConventional methods such as RNNs and Transformers, while effective for\ndiscrete-time and regularly sampled data, face significant challenges in\ncapturing the continuous dynamics and irregular sampling patterns inherent in\nreal-world scenarios. Neural Differential Equations (NDEs) represent a paradigm\nshift by combining the flexibility of neural networks with the mathematical\nrigor of differential equations. This paper presents a comprehensive review of\nNDE-based methods for time series analysis, including neural ordinary\ndifferential equations, neural controlled differential equations, and neural\nstochastic differential equations. We provide a detailed discussion of their\nmathematical formulations, numerical methods, and applications, highlighting\ntheir ability to model continuous-time dynamics. Furthermore, we address key\nchallenges and future research directions. This survey serves as a foundation\nfor researchers and practitioners seeking to leverage NDEs for advanced time\nseries analysis.",
        "This study focuses on the classification of cancerous and healthy slices from\nmultimodal lung images. The data used in the research comprises Computed\nTomography (CT) and Positron Emission Tomography (PET) images. The proposed\nstrategy achieves the fusion of PET and CT images by utilizing Principal\nComponent Analysis (PCA) and an Autoencoder. Subsequently, a new ensemble-based\nclassifier developed, Deep Ensembled Multimodal Fusion (DEMF), employing\nmajority voting to classify the sample images under examination.\nGradient-weighted Class Activation Mapping (Grad-CAM) employed to visualize the\nclassification accuracy of cancer-affected images. Given the limited sample\nsize, a random image augmentation strategy employed during the training phase.\nThe DEMF network helps mitigate the challenges of scarce data in computer-aided\nmedical image analysis. The proposed network compared with state-of-the-art\nnetworks across three publicly available datasets. The network outperforms\nothers based on the metrics - Accuracy, F1-Score, Precision, and Recall. The\ninvestigation results highlight the effectiveness of the proposed network.",
        "We investigate the origin of photometric variability in the classical T Tauri\nstar TW Hya by comparing light curves obtained by TESS and ground-based\ntelescopes with light curves created using three-dimensional (3D)\nmagnetohydrodynamic (MHD) simulations. TW Hya is modeled as a rotating star\nwith a dipole magnetic moment, slightly tilted about the rotational axis. We\nobserved that for various model parameters, matter accretes in the unstable\nregime and produces multiple hot spots on the star's surface, which leads to\nstochastic-looking light curves similar to the observed ones. Wavelet and\nFourier spectra of observed and modeled light curves show multiple\nquasiperiodic oscillations (QPOs) with quasiperiods from less than 0.1 to 9\ndays. Models show that variation in the strength and tilt of the dipole\nmagnetosphere leads to different periodograms, where the period of the star may\ndominate or be hidden. The amplitude of QPOs associated with the stellar period\ncan be smaller than that of other QPOs if the tilt of the dipole magnetosphere\nis small and when the unstable regime is stronger. In models with small\nmagnetospheres, the short-period QPOs associated with rotation of the inner\ndisc dominate and can be mistaken for a stellar period. We show that\nlonger-period (5-9 days) QPOs can be caused by waves forming beyond the\ncorotation radius.",
        "Microspine grippers are small spines commonly found on insect legs that\nreinforce surface interaction by engaging with asperities to increase shear\nforce and traction. An array of such microspines, when integrated into the\nlimbs or undercarriage of a robot, can provide the ability to maneuver uneven\nterrains, traverse inclines, and even climb walls. Conformability and\nadaptability of soft robots makes them ideal candidates for these applications\ninvolving traversal of complex, unstructured terrains. However, there remains a\nreal-life realization gap for soft locomotors pertaining to their transition\nfrom controlled lab environment to the field by improving grip stability\nthrough effective integration of microspines. We propose a passive, compliant\nmicrospine stacked array design to enhance the locomotion capabilities of\nmobile soft robots, in our case, ones that are motor tendon actuated. We offer\na standardized microspine array integration method with effective\nsoft-compliant stiffness integration, and reduced complexity resulting from a\nsingle actuator passively controlling them. The presented design utilizes a\ntwo-row, stacked microspine array configuration that offers additional gripping\ncapabilities on extremely steep\/irregular surfaces from the top row while not\nhindering the effectiveness of the more frequently active bottom row. We\nexplore different configurations of the microspine array to account for\nchanging surface topologies and enable independent, adaptable gripping of\nasperities per microspine. Field test experiments are conducted on various\nrough surfaces including concrete, brick, compact sand, and tree roots with\nthree robots consisting of a baseline without microspines compared against two\nrobots with different combinations of microspine arrays. Tracking results\nindicate that the inclusion of microspine arrays increases planar displacement\non average by 15 and 8 times.",
        "$\\text{La}_2 \\text{Ni}_7$ is an itinerant magnet with a small saturated\nmoment of $\\sim$ 0.1 $\\mu_{B}\/\\text{Ni}$ and a series of antiferromagnetic\n(AFM) transitions at $T_1$ = 61.0 K, $T_2$ = 56.5 K and $T_3$ = 42.2 K.\nTemperature and field dependent measurements suggest a complex, anisotropic\n$H-T$ phase diagram with multiple phase lines. Here we present the growth and\ncharacterization of single crystals of the ${\\text{La}_{2}\\text{(Cu}_{x}\\text\n{Ni}_{1-x})_7}$ series for 0 $\\leq x \\leq$ 0.181. Using a suite of anisotropic\nmagnetic, transport, and thermodynamic measurements we study the evolution of\nthe three AFM transitions upon Cu substitution. For ${0 \\leq x \\leq 0.097}$,\nthe system remains magnetically ordered at base temperature with $x \\leq$\n0.012, showing signs of three primarily AFM phases. For the higher substitution\nlevels, ${0.125 \\leq x \\leq 0.181}$, there are no signatures of magnetic\nordering, but an anomalous feature in resistance and heat capacity data are\nobserved which are consistent with the Kondo effect in this system. The\nintermediate $x$ = 0.105 sample lies in between the magnetic ordered and the\nKondo regime and is in the vicinity of the AFM-quantum critical point (QCP).\nThus, ${\\text{La}_{2}\\text{(Cu}_{x}\\text {Ni}_{1-x})_7}$ is an example of a\nsmall moment system that can be tuned through a QCP. Given these data combined\nwith the fact that the $\\text{La}_2 \\text{Ni}_7$ structure has kagome-like,\nNi-sublattice running perpendicular to the crystallographic $c-$ axis, and a\n$3d$-electron flat band that contributes to the density of states near the\nFermi energy, it becomes a promising candidate to host and study exotic\nphysics.",
        "The non-cloning theorem of quantum states provides security, but also limits\nthe Secret Key Rate (SKR) for Quantum Key Distribution (QKD) implementations.\nMultiplexing is a widely used technique to enhance data rates in classical\ncommunication systems and can also increase the SKR in QKD systems. Using\nlinearly polarized (LP) modes is an attractive solution as it is compatible\nwith simple fiber designs. This work demonstrates a fiber-based QKD system\nemploying LP mode multiplexing with a photonic lantern to convert the\nfundamental mode ($LP_{01}$) in separate fibers into higher-order modes\n($LP_{11}$) in a single few-mode fiber. The performance of the system is\nsensitive to polarization dependence, mode alignment, and environmental\ncrosstalk, which requires precise polarization control to minimize Quantum Bit\nError Rate (QBER). We report a SKR of 2.34 Mbps over a 24 km 5dB loss fiber\nlink.",
        "Depinning dynamics of a two-dimensional (2D) solid dusty plasma modulated by\na one-dimensional (1D) vibrational periodic substrate are investigated using\nLangevin dynamical simulations. As the uniform driving force increases\ngradually, from the overall drift velocity varying with the driving force, four\nsignificant Shapiro steps are discovered. The data analysis indicate that, when\nthe ratio of the frequency from the drift motion over potential wells to the\nexternal frequency from the modulation substrate is close to integers, dynamic\nmode locking occurs, corresponding to the discovered Shapiro steps. Around both\ntermini of the first and fourth Shapiro steps, the transitions are found to be\nalways continuous, however, the transition between the second and third Shapiro\nsteps is discontinuous, probably due to the different arrangements of\nparticles.",
        "Estimating the Number of Distinct Values (NDV) is fundamental for numerous\ndata management tasks, especially within database applications. However, most\nexisting works primarily focus on introducing new statistical or learned\nestimators, while identifying the most suitable estimator for a given scenario\nremains largely unexplored. Therefore, we propose AdaNDV, a learned method\ndesigned to adaptively select and fuse existing estimators to address this\nissue. Specifically, (1) we propose to use learned models to distinguish\nbetween overestimated and underestimated estimators and then select appropriate\nestimators from each category. This strategy provides a complementary\nperspective by integrating overestimations and underestimations for error\ncorrection, thereby improving the accuracy of NDV estimation. (2) To further\nintegrate the estimation results, we introduce a novel fusion approach that\nemploys a learned model to predict the weights of the selected estimators and\nthen applies a weighted sum to merge them. By combining these strategies, the\nproposed AdaNDV fundamentally distinguishes itself from previous works that\ndirectly estimate NDV. Moreover, extensive experiments conducted on real-world\ndatasets, with the number of individual columns being several orders of\nmagnitude larger than in previous studies, demonstrate the superior performance\nof our method.",
        "Recent advancements in data-driven weather forecasting models have delivered\ndeterministic models that outperform the leading operational forecast systems\nbased on traditional, physics-based models. However, these data-driven models\nare typically trained with a mean squared error loss function, which causes\nsmoothing of fine scales through a \"double penalty\" effect. We develop a\nsimple, parameter-free modification to this loss function that avoids this\nproblem by separating the loss attributable to decorrelation from the loss\nattributable to spectral amplitude errors. Fine-tuning the GraphCast model with\nthis new loss function results in sharp deterministic weather forecasts, an\nincrease of the model's effective resolution from 1,250km to 160km,\nimprovements to ensemble spread, and improvements to predictions of tropical\ncyclone strength and surface wind extremes."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "start_abstract":"TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
      ],
      "abstract":[
        "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Non-uniqueness of normalized NLS ground states on bounded domains with\n  homogeneous Neumann boundary conditions",
        "Drinfeld modules with maximal Galois action",
        "Toughness of double network hydrogels: the role of reduced stress\n  propagation",
        "Traffic noise assessment in urban Bulgaria using explainable machine\n  learning",
        "Quadratic BSDEs with Singular Generators and Unbounded Terminal\n  Conditions: Theory and Applications",
        "Improved Online Confidence Bounds for Multinomial Logistic Bandits",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Polarization-controlled strong light-matter interaction with templated\n  molecular aggregates",
        "Joint Power Allocation and Phase Shift Design for Stacked Intelligent\n  Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL",
        "An exact closed walks series formula for the complexity of regular\n  graphs and some related bounds",
        "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Allostatic Control of Persistent States in Spiking Neural Networks for\n  perception and computation",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "Steady compressible Navier-Stokes-Fourier system with general\n  temperature dependent viscosities I: density estimates based on Bogovskii\n  operator",
        "The Kodaira Embedding Theorem",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Geodesic Variational Bayes for Multiway Covariances",
        "Non-commutative derived analytic moduli functors",
        "Fast, accurate, and predictive method for atom detection in\n  site-resolved images of microtrap arrays",
        "Improved bounds on collapse models from rotational noise of LISA\n  Pathfinder",
        "Degenerate parabolic equations in divergence form: fundamental solution\n  and Gaussian bounds",
        "Finiteness of non-degenerate central configurations of the planar\n  $n$-body problem with a homogeneous potential",
        "OGBoost: A Python Package for Ordinal Gradient Boosting",
        "Numerical verification of the Collatz conjecture for billion digit\n  random numbers",
        "Riemann surface foliations with non-discrete singular set",
        "Apparent nonreciprocal transport in FeSe bulk crystals",
        "An orphan flare from a plasma blob crossing the broad-line region ?",
        "Gravity-induced collisions of uncharged cloud droplets in an electric\n  field"
      ],
      "abstract":[
        "We provide a general non-uniqueness result for normalized ground states of\nnonlinear Schr\\\"odinger equations with pure power nonlinearity on bounded\ndomains with homogeneous Neumann boundary conditions, defined as global\nminimizers of the associated energy functional among functions with prescribed\nmass. Precisely, for nonlinearity powers slightly smaller than the\n$L^2$-critical exponent, we prove that there always exists at least one value\nof the mass for which normalized ground states are not unique.",
        "With a fixed prime power $q>1$, define the ring of polynomials\n$A=\\mathbb{F}_q[t]$ and its fraction field $F=\\mathbb{F}_q(t)$. For each pair\n$a=(a_1,a_2) \\in A^2$ with $a_2$ nonzero, let $\\phi(a)\\colon A\\to F\\{\\tau\\}$ be\nthe Drinfeld $A$-module of rank $2$ satisfying $t\\mapsto t+a_1\\tau+a_2\\tau^2$.\nThe Galois action on the torsion of $\\phi(a)$ gives rise to a Galois\nrepresentation $\\rho_{\\phi(a)}\\colon\n\\operatorname{Gal}(F^{\\operatorname{sep}}\/F)\\to\n\\operatorname{GL}_2(\\widehat{A})$, where $\\widehat{A}$ is the profinite\ncompletion of $A$. We show that the image of $\\rho_{\\phi(a)}$ is large for\nrandom $a$. More precisely, for all $a\\in A^2$ away from a set of density $0$,\nwe prove that the index\n$[\\operatorname{GL}_2(\\widehat{A}):\\rho_{\\phi(a)}(\\operatorname{Gal}(F^{\\operatorname{sep}}\/F))]$\ndivides $q-1$ when $q>2$ and divides $4$ when $q=2$. We also show that the\nrepresentation $\\rho_{\\phi(a)}$ is surjective for a positive density set of\n$a\\in A^2$.",
        "Double network hydrogels show remarkable mechanical performance, combining\nhigh strength and fracture toughness with sufficient stiffness to bear load,\ndespite containing only a low density of cross-linked polymer molecules in\nwater. We introduce a simple mesoscale model of a double network material,\ndetailed enough to resolve the salient microphysics of local plastic bond\nbreakage, yet simple enough to address macroscopic cracking. Load sharing\nbetween the networks results in a delocalisation of stress such that the double\nnetwork inherits both the stiffness of its stiff-and-brittle sacrificial\nnetwork and the ductility of its soft-and-ductile matrix network. The\nunderlying mechanism is a reduction in the Eshelby stress propagator between\nsacrificial bonds, inhibiting the tendency for the plastic failure of one\nsacrificial bond to propagate stress to neighbouring sacrificial bonds and\ncause a follow-on cascade of breakages. The mechanism of brittle macroscopic\ncracking is thereby suppressed, giving instead ductile deformation via\ndiffusely distributed microcracking.",
        "Fine-grained noise maps are vital for epidemiological studies on traffic\nnoise. However, detailed information on traffic noise is often limited,\nespecially in Eastern Europe. Rigid linear noise land-use regressions are\ntypically employed to estimate noise levels; however, machine learning likely\noffers more accurate noise predictions. We innovated by comparing the\npredictive accuracies of supervised machine learning models to estimate traffic\nnoise levels across the five largest Bulgarian cities. In situ A-weighted\nequivalent continuous sound levels were obtained from 232 fixed-site monitors\nacross these cities. We included transport- and land-use-related predictors\nusing 50-1,000 m buffers. Extreme gradient boosting (XGB) had the highest\nten-fold cross-validated fit (R2=0.680) and the lowest root mean square error\n(RMSE=4.739), insignificantly besting the random forest-based model (R2=0.667,\nRMSE=4.895). Support vector regression (R2=0.633, RMSE=5.358), elastic net\n(R2=0.568, RMSE=5.625), and linear regression (R2=0.548, RMSE=5.569) performed\nsignificantly worse. Shapley values for the XGB showed that the length of major\nroads within 100 m buffers, footways within 50 m buffers, residential roads\nwithin 50 m buffers, and the number of buildings within 50 m buffers were\nimportant non-linear predictors. Our spatially resolved noise maps revealed\nstriking geographic noise variations and that, on average, 96.8% of the urban\npopulation experiences harmful noise levels.",
        "We investigate a class of quadratic backward stochastic differential\nequations (BSDEs) with generators singular in $ y $. First, we establish the\nexistence of solutions and a comparison theorem, thereby extending results in\nthe literature. Additionally, we analyze the stability property and the\nFeynman-Kac formula, and prove the uniqueness of viscosity solutions for the\ncorresponding singular semilinear partial differential equations (PDEs).\nFinally, we demonstrate applications in the context of robust control linked to\nstochastic differential utility and certainty equivalent based on\n$g$-expectation. In these applications, the coefficient of the quadratic term\nin the generator captures the level of ambiguity aversion and the coefficient\nof absolute risk aversion, respectively.",
        "In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $ for sufficiently large $T$, where\n$\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the\ndimension of the contexts, and $T$ is the total number of rounds. Furthermore,\nwe introduce a Maximum Likelihood Estimation (MLE)-based algorithm,\nOFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O \\Big( d \\log\n(BT) \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "We demonstrate strong light-matter interaction for a layer of templated\nmerocyanine molecules in a planar microcavity. Using a single layer of graphene\nnanoribbons as a templating layer, we obtain an aligned layer of aggregated\nmolecules. The molecular layer displays anisotropic optical properties\nresembling those of a biaxial crystal. The anisotropic excitonic component in\nthe cavity results in strongly polarization-dependent light-matter interaction\nand in increased Rabi-energies. The increased light-matter interaction is\npossibly due to reduced molecular disorder in the templated molecular layer.\nThis conclusion is supported by an analysis based on a multi-oscillator model.\nWe further use photoluminescence microspectroscopy to demonstrate that the\nlight-matter coupling is spatially homogeneous. Our study introduces molecular\ntemplating to strong light-matter studies. The reduced disorder of the system\nas a consequence of templating is highly beneficial for engineering\nlight-matter interaction.",
        "Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer\nhigh spectral efficiency (SE) through multiple distributed access points (APs).\nHowever, the large number of antennas increases power consumption. We propose\nincorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a\ncost-effective, energy-efficient solution. This paper focuses on optimizing the\njoint power allocation of APs and the phase shift of SIMs to maximize the sum\nSE. To address this complex problem, we introduce a fully distributed\nmulti-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the\nnoisy value method with a recurrent policy in multi-agent policy optimization\n(NVR-MAPPO), enhances performance by encouraging diverse exploration under\ncentralized training and decentralized execution. Simulations demonstrate that\nNVR-MAPPO significantly improves sum SE and robustness across various\nscenarios.",
        "The complexity of a graph is the number of its labeled spanning trees. In\nthis work complexity is studied in settings that admit regular graphs. An exact\nformula is established linking complexity of the complement of a regular graph\nto numbers of closed walks in the graph by way of an infinite alternating\nseries. Some consequences of this result yield infinite classes of lower and\nupper bounds on the complexity of such graphs. Applications of these\nmathematical results to biological problems on neuronal activity are described.",
        "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "The aim of this paper is to reconsider the existence theory for steady\ncompressible Navier--Stokes--Fourier system assuming more general condition of\nthe dependence of the viscosities on the temperature in the form\n$\\mu(\\vartheta)$, $\\xi(\\vartheta) \\sim (1+\\vartheta)^\\alpha$ for $0\\leq \\alpha\n\\leq 1$. This extends the known theory for $\\alpha=1$ from and improves\nsignificantly the results for $\\alpha =0$. This paper is the first of a series\nof two papers dealing with this problem and is connected with the\nBogovskii-type estimates of the sequence of densities. This leads, among\nothers, to the limitation $\\gamma >\\frac 32$ for the pressure law\n$p(\\varrho,\\vartheta) \\sim \\varrho^\\gamma + \\varrho\\vartheta$. The paper\nconsiders both the heat-flux (Robin) and Dirichlet boundary conditions for the\ntemperature as well as both the homogeneous Dirichlet and zero inflow\/outflow\nNavier boundary conditions for the velocity. Further extension for $\\gamma >1$\nonly is based on different type of pressure estimates and will be the content\nof the subsequent paper.",
        "Chow's Theorem and GAGA are renowned results demonstrating the algebraic\nnature of projective manifolds and, more broadly, projective analytic\nvarieties. However, determining if a particular manifold is projective is not,\ngenerally, a simple task. The Kodaira Embedding Theorem provides an intrinsic\ncharacterization of projective varieties in terms of line bundles; in\nparticular, it states that a manifold is projective if and only if it admits a\npositive line bundle. We prove only the 'if' implication in this paper, giving\na sufficient condition for a manifold bundle to be embedded in projective\nspace. Along the way, we prove several other interesting results. Of particular\nnote is the Kodaira-Nakano Vanishing Theorem, a crucial tool for eliminating\nhigher cohomology of complex manifolds, as well as Lemmas 6.2 and 6.1, which\nprovide important relationships between divisors, line bundles, and blowups.\nAlthough this treatment is relatively self-contained, we omit a rigorous\ndevelopment of Hodge theory, some basic complex analysis results, and some\ntheorems regarding Cech cohomology (including Leray's Theorem).",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "This article explores the optimization of variational approximations for\nposterior covariances of Gaussian multiway arrays. To achieve this, we\nestablish a natural differential geometric optimization framework on the space\nusing the pullback of the affine-invariant metric. In the case of a truly\nseparable covariance, we demonstrate a joint approximation in the multiway\nspace outperforms a mean-field approximation in optimization efficiency and\nprovides a superior approximation to an unstructured Inverse-Wishart posterior\nunder the average Mahalanobis distance of the data while maintaining a multiway\ninterpretation. We moreover establish efficient expressions for the Euclidean\nand Riemannian gradients in both cases of the joint and mean-field\napproximation. We end with an analysis of commodity trade data.",
        "We develop a formulation for non-commutative derived analytic geometry built\nfrom differential graded (dg) algebras equipped with free entire functional\ncalculus (FEFC), relating them to simplicial FEFC algebras and to locally\nmultiplicatively convex complete topological dg algebras. The theory is\noptimally suited for accommodating analytic morphisms between functors of\nalgebraic origin, and we establish forms of Riemann-Hilbert equivalence in this\nsetting. We also investigate classes of topological dg algebras for which\nmoduli functors of analytic origin tend to behave well, and relate their\nhomotopy theory to that of FEFC algebras. Applications include the construction\nof derived non-commutative analytic moduli stacks of pro-\\'etale local systems\nand non-commutative derived twistor moduli functors, both equipped with shifted\nanalytic bisymplectic structures, and hence shifted analytic double Poisson\nstructures.",
        "We introduce a new method, rooted in estimation theory, to detect the\nindividual atoms in site-resolved images of microtrap arrays, such as optical\nlattices or optical tweezers arrays. Using simulated images, we demonstrate a\nten-fold reduction of the detection error rate compared to the popular method\nbased on Wiener deconvolution, under a wide range of experimental conditions.\nThe runtime is fully compatible with real-time applications, even for a very\nlarge arrays. Finally, we propose a rigorous definition for the signal-to-noise\nratio of an image, and show that it can be used as a predictor for the\ndetection error rate, which opens new prospect for the design of future\nexperiments.",
        "Spontaneous wavefunction collapse models offer a solution to the quantum\nmeasurement problem, by modifying the Schr\\\"odinger equation with nonlinear and\nstochastic terms. The Continuous Spontaneous Localisation (CSL) model is the\nmost studied among these models, with phenomenological parameters that are\nconstrained by experiments. Here, we exploit the recent analysis of LISA\nPathfinder's angular motion data to derive a tighter constraint than previously\nachieved with translational motion. Moreover, we identify the general\nconditions for preferring rotational measurement over translational ones for\nconstraining the CSL model.",
        "In this paper, we consider second order degenerate parabolic equations with\ncomplex, measurable, and time-dependent coefficients. The degenerate\nellipticity is dictated by a spatial $A_2$-weight. We prove that having a\ngeneralized fundamental solution with upper Gaussian bounds is equivalent to\nMoser's $L^2$-$L^\\infty$ estimates for local weak solutions. In the special\ncase of real coefficients, Moser's $L^2$-$L^\\infty$ estimates are known, which\nprovide an easier proof of Gaussian upper bounds, and a known Harnack\ninequality is then used to derive Gaussian lower bounds.",
        "We show that there exist an upper bound and a lower bound for the number of\nnon-degenerate central configurations of the n-body problem in the plane with a\nhomogeneous potential. In particular, both bounds are independent of the\nhomogeneous degree of the potential under consideration.",
        "This paper introduces OGBoost, a scikit-learn-compatible Python package for\nordinal regression using gradient boosting. Ordinal variables (e.g., rating\nscales, quality assessments) lie between nominal and continuous data,\nnecessitating specialized methods that reflect their inherent ordering. Built\non a coordinate-descent approach for optimization and the latent-variable\nframework for ordinal regression, OGBoost performs joint optimization of a\nlatent continuous regression function (functional gradient descent) and a\nthreshold vector that converts the latent continuous value into discrete class\nprobabilities (classical gradient descent). In addition to the stanadard\nmethods for scikit-learn classifiers, the GradientBoostingOrdinal class\nimplements a \"decision_function\" that returns the (scalar) value of the latent\nfunction for each observation, which can be used as a high-resolution\nalternative to class labels for comparing and ranking observations. The class\nhas the option to use cross-validation for early stopping rather than a single\nholdout validation set, a more robust approach for small and\/or imbalanced\ndatasets. Furthermore, users can select base learners with different underlying\nalgorithms and\/or hyperparameters for use throughout the boosting iterations,\nresulting in a `heterogeneous' ensemble approach that can be used as a more\nefficient alternative to hyperparameter tuning (e.g. via grid search). We\nillustrate the capabilities of OGBoost through examples, using the wine quality\ndataset from the UCI respository. The package is available on PyPI and can be\ninstalled via \"pip install ogboost\".",
        "The Collatz conjecture, also known as the 3n+1 problem, is one of the most\npopular open problems in number theory. In this note, an algorithm for the\nverification of the Collatz conjecture is presented that works on a standard PC\nfor numbers with up to ten billion decimal places.",
        "Let $\\mathcal{F}$ be a singular Riemann surface foliation on a complex\nmanifold $M$, such that the singular set $E \\subset M$ is non-discrete. We\nstudy the behavior of the foliation near the singular set $E$, particularly\nfocusing on singular points that admit invariant submanifolds (locally) passing\nthrough them. Our primary focus is on the singular points that are removable\nsingularities for some proper subfoliation. We classify singular points based\non the dimension of their invariant submanifold and, consequently, establish\nthat for hyperbolic foliations $\\mathcal{F}$, the presence of such\nsingularities ensures the continuity of the leafwise Poincar\\'{e} metric on $M\n\\setminus E$.",
        "We performed low-frequency ac first- and second-harmonic resistance\nmeasurements and dc $I-V$ measurements on bulk FeSe crystals in a temperature\nrange between 1.8 and 150 K and in magnetic field up to 14 T. We observed\nconsiderable second-harmonic resistance, indicative of nonreciprocal charge\ntransport, in some samples. By examining correlation between contact\nresistances and second-harmonic signals, we concluded that the second-harmonic\nresistance was not due to the genuine nonreciprocal transport effect but was\ncaused by joule heating at a current contact through the thermoelectric effect.\nOur conclusion is consistent with a recent preprint (Nagata \\textit{et al.},\narXiv:2409.01715), in which the authors reported a zero-field superconducting\ndiode effect in devices fabricated with FeSe flakes and attributed it to the\nthermoelectric effect.",
        "The blazar 3C 279 is well known for its prolific emission of rapid flares. A\nparticular event occurred on 12\/20\/2013, exhibiting a large flux increase with\na doubling time scale of a few hours, a very hard gamma-ray spectrum, and a\ntime-asymmetric light curve with slow decay, but no significant variations\ndetected in the optical range. We propose a novel scenario to interpret this\nflare, based on two emission zones, a stationary blob and a moving plasma blob.\nThe stationary blob, located within the BLR, accounts for the low-state\nemission. The moving blob decouples from the stationary zone, accelerates and\ncrosses the BLR. The high-energy flare is attributed to the variable external\nCompton emission as the blob moves through the BLR, while variations in the\nsynchrotron emission are negligible. Our interpretation differs from previous\ninterpretations by attributing the flare to the bulk motion and geometry of the\nexternal photon fields, without invoking varying electron injection.",
        "We investigate the collisions of uncharged, conducting droplets settling\nunder gravity in the presence of an external electric field. Previous studies\nhave derived a near-field asymptotic expression for the electric-field-induced\nattraction, suggesting that this force can overcome lubrication resistance and\ndrive surface-to-surface contact between two spherical conductors within a\nfinite time. However, for droplets moving in air, traditional lubrication\ntheory breaks down when the inter-droplet gap approaches the mean free path of\nair molecules. To account for this, we incorporate non-continuum hydrodynamic\neffects to estimate the gravity-driven collision efficiency under\nelectric-field-induced forces. This study examines how an external electric\nfield influences the trajectories of settling droplet pairs of unequal sizes.\nBy analyzing their motion, we compute collision efficiencies and explore their\ndependence on droplet size ratio, electric field strength, the angle between\nthe field and gravity, and key dimensionless parameters governing\nelectric-field-induced and van der Waals forces. Our findings reveal that\nelectric-field-induced forces significantly enhance collision efficiency,\nhighlighting their critical role in droplet coalescence dynamics."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102",
    "start_abstract":"Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
      ],
      "abstract":[
        "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "A novel Facial Recognition technique with Focusing on Masked Faces",
        "Exact Maximin Share Fairness via Adjusted Supply",
        "Network-assisted collective operations for efficient distributed quantum\n  computing",
        "A Moving Mesh Isogeometric Method Based on Harmonic Maps",
        "Quasi-two-dimensional magnetism and antiferromagnetic ground state in\n  Li$_2$FeSiO$_4$",
        "Channel deformations during elastocapillary spreading of gaseous\n  embolisms in biomimetic leaves",
        "Pretraining Generative Flow Networks with Inexpensive Rewards for\n  Molecular Graph Generation",
        "Measuring Star Formation Rates in the Milky Way from Hi-GAL 70 $\\mu$m\n  Observations",
        "Non-negative tensor factorization-based dependence map analysis for\n  local damage detection in presence of non-Gaussian noise",
        "Low-energy insulating reconstructions of Si(111)-7x7 surface with and\n  without stacking fault discovered by graph theory",
        "Cultivating Precision: Comparative Analysis of Sensor-Based Yogurt\n  Fermentation Monitoring Techniques",
        "Existence and Uniqueness of Local Solutions for a Class of Partial\n  Differential-Algebraic Equations",
        "Generalizable automated ischaemic stroke lesion segmentation with vision\n  transformers",
        "A Spatio-Temporal Dirichlet Process Mixture Model on Linear Networks for\n  Crime Data",
        "Weighted Graph Structure Learning with Attention Denoising for Node\n  Classification",
        "Recent advances about the rigorous integration of parabolic PDEs via\n  fully spectral Fourier-Chebyshev expansions",
        "On the intersection of pairs of trees",
        "Improved constraints on the Faraday rotation towards eight fast radio\n  bursts using dense grids of polarized radio galaxies",
        "\"Can you be my mum?\": Manipulating Social Robots in the Large Language\n  Models Era",
        "Transformer-based Wireless Symbol Detection Over Fading Channels",
        "Variational inference for hierarchical models with conditional scale and\n  skewness corrections",
        "What Kind of Visual Tokens Do We Need? Training-free Visual Token\n  Pruning for Multi-modal Large Language Models from the Perspective of Graph",
        "A Quantum Algorithm for the Classification of Patterns of Boolean\n  Functions",
        "Phase portraits of a family of Kolmogorov systems depending on six\n  parameters",
        "The generic Markov CoHA is not spherically generated",
        "Kac-Moody Algebras on Soft Group Manifolds",
        "Experimental evaluation of xApp Conflict Mitigation Framework in O-RAN:\n  Insights from Testbed deployment in OTIC",
        "Adiabatic transverse thermoelectric conversion enhanced by heat current\n  manipulation in artificially tilted multilayers",
        "Modelling Regional Solar Photovoltaic Capacity in Great Britain"
      ],
      "abstract":[
        "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.",
        "This work addresses fair allocation of indivisible items in settings wherein\nit is feasible to create copies of resources or dispose of tasks. We establish\nthat exact maximin share (MMS) fairness can be achieved via limited duplication\nof goods even under monotone valuations. We also show that, when allocating\nchores under monotone costs, MMS fairness is always feasible with limited\ndisposal of chores. Since monotone valuations do not admit any nontrivial\napproximation guarantees for MMS, our results highlight that such barriers can\nbe circumvented by post facto adjustments in the supply of the items.\n  We prove that, for division of $m$ goods among $n$ agents with monotone\nvaluations, there always exists an assignment of subsets of goods to the agents\nsuch that they receive at least their maximin shares and no single good is\nallocated to more than $3 \\log m$ agents. In addition, the sum of the sizes of\nthe assigned subsets does not exceed $m$. For identically ordered valuations,\nwe obtain an upper bound of $O(\\sqrt{\\log m})$ on the maximum assignment\nmultiplicity across goods and an $m + \\widetilde{O}\\left(\\frac{m}{\\sqrt{n}}\n\\right)$ bound for the total number of goods assigned. Further, for additive\nvaluations, we prove that there always exists an MMS assignment in which no\nsingle good is allocated to more than $2$ agents and the total number of goods\nassigned is at most $2m$.\n  For chores, we upper bound the number of chores that need to be discarded for\nensuring MMS fairness. We prove that, under monotone costs, there exists an MMS\nassignment in which at most $\\frac{m}{e}$ remain unassigned. For identically\nordered costs, we establish that MMS fairness can be achieved while keeping at\nmost $\\widetilde{O} \\left(\\frac{m}{n^{1\/4}} \\right)$ chores unassigned. We also\nprove that the obtained bounds for monotone valuations and monotone costs are\nessentially tight.",
        "We propose protocols for the distribution of collective quantum operations\nbetween remote quantum processing units (QPUs), a requirement for distributed\nquantum computing. Using only local operations and classical communication\n(LOCC), these protocols allow for collective multicontrolled and multitarget\ngates to be executed in network architectures similar to those used for\nhigh-performance computing. The types of gates that can be implemented\nfollowing this scheme are discussed. The Bell pair cost for a single\ndistributed multicontrolled gate is estimated, arriving to a single additional\nBell pair over the theoretically optimal calculation with pre-shared\nentanglement, demonstrating better scalability when compared to current\nproposals based on entanglement swapping through a network, and bounds are\ncalculated for general diagonal gates. A recipe is provided for the lumped\ndistribution of gates such as arbitrarily-sized Toffoli and multicontrolled Z,\nand $R_{zz}(\\theta)$ gates. Finally, we provide an exact implementation of a\ndistributed Grover's search algorithm using this protocol to partition the\ncircuit, with Bell pair cost growing linearly with the number of Grover\niterations and the number of partitions.",
        "Although the isogeometric analysis has shown its great potential in achieving\nhighly accurate numerical solutions of partial differential equations, its\nefficiency is the main factor making the method more competitive in practical\nsimulations. In this paper, an integration of isogeometric analysis and a\nmoving mesh method is proposed, providing a competitive approach to resolve the\nefficiency issue. Focusing on the Poisson equation, the implementation of the\nalgorithm and related numerical analysis are presented in detail, including the\nnumerical discretization of the governing equation utilizing isogeometric\nanalysis, and a mesh redistribution technique developed via harmonic maps. It\nis found that the isogeometric analysis brings attractive features in the\nrealization of moving mesh method, such as it provides an accurate expression\nfor moving direction of mesh nodes, and allows for more choices for\nconstructing monitor functions. Through a series of numerical experiments, the\neffectiveness of the proposed method is successfully validated and the\npotential of the method towards the practical application is also well\npresented with the simulation of a helium atom in Kohn--Sham density functional\ntheory.",
        "Our experimental (neutron diffraction, M\\\"ossbauer spectroscopy, magnetic\nsusceptibility, specific heat) and numerical studies on the evolution of short-\nand long-range magnetic order in $\\gamma_{\\rm II}$-Li\\(_2\\)FeSiO\\(_4\\) suggest\na quasi-two-dimensional (2D) nature of magnetism. The experimental data\nobtained on single crystals imply long-range antiferromagnetic order below\n$T_{\\rm N}= 17$~K. A broad maximum in magnetic susceptibility $\\chi$ at $T_{\\rm\nm}\\simeq 28$~K, observation of magnetic entropy changes up to 100~K and\nanisotropy in $\\chi$ are indicative of low-dimensional magnetism and suggest\nshort-range magnetic correlations up to 200~K. Neutron diffraction shows that\nlong-range antiferromagnetic order is characterised by the propagation vector\nk=(1\/2,0,1\/2). The ordered moment $\\mu = 2.50(2) \\mu_B$ \/Fe, at $T = 1.5$~K, is\nalong the crystallographic $a$-axis. This is consistent with the observed\nstatic hyperfine field of $B_{\\rm hyp}=14.8(3)$\\,T by M\\\"ossbauer spectroscopy\nwhich indicates significant orbital contributions. The temperature dependence\nof $B_{\\rm hyp}$ yields the critical exponent $\\beta=0.116(12)$ which is in the\nregime of the 2D Ising behaviour. LSDA+U studies exploiting the experimental\nspin structure suggest dominating magnetic exchange coupling within the\n$ac$-layers (i.e., $J_3\\simeq -6$~K and $J_6\\simeq-2$~K) while interlayer\ncoupling is much smaller and partly frustrated. This confirms the 2D nature of\nmagnetism and is in full agreement with the experimental findings.",
        "The nucleation and\/or spreading of bubbles in water under tension (due to\nwater evaporation) can be problematic for most plants along the ascending sap\nnetwork from root to leaves, named xylem. Due to global warming, trees facing\ndrought conditions are particularly threatened by the formation of such air\nembolisms, which spreads intermittently and hinder the flow of sap and could\nultimately result in their demise. PDMS-based biomimetic leaves simulating\nevapotranspiration have demonstrated that, in a linear configuration, the\nexistence of a slender constriction in the channel allows for the creation of\nintermittent embolism propagation (as an interaction between the elasticity of\nthe biomimetic leaf (mainly the deformable ceiling of the microchannels) and\nthe capillary forces at the air\/water interfaces)\n\\cite{Keiser2022}-\\cite{keiser2024}. Here we use analog PDMS-based biomimetic\nleaves in 1d and 2d. To better explore the embolism spreading mechanism, we add\nto the setup an additional technique, allowing to measure directly the\nmicrochannel's ceiling deformation versus time, which corresponds to the\npressure variations. We present here such a method that allows to have\nquantitative insights in the dynamics of embolism spreading. The coupling\nbetween channel deformations and the Laplace pressure threshold explains the\nobserved elastocapillary dynamics.",
        "Generative Flow Networks (GFlowNets) have recently emerged as a suitable\nframework for generating diverse and high-quality molecular structures by\nlearning from rewards treated as unnormalized distributions. Previous works in\nthis framework often restrict exploration by using predefined molecular\nfragments as building blocks, limiting the chemical space that can be accessed.\nIn this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative\nmodel leveraging individual atoms as building blocks to explore drug-like\nchemical space more comprehensively. We propose an unsupervised pre-training\napproach using drug-like molecule datasets, which teaches A-GFNs about\ninexpensive yet informative molecular descriptors such as drug-likeliness,\ntopological polar surface area, and synthetic accessibility scores. These\nproperties serve as proxy rewards, guiding A-GFNs towards regions of chemical\nspace that exhibit desirable pharmacological properties. We further implement a\ngoal-conditioned finetuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on a subset of ZINC\ndataset, and by employing robust evaluation metrics we show the effectiveness\nof our approach when compared to other relevant baseline methods for a wide\nrange of drug design tasks.",
        "Three methods for computing the total star formation rate of the Milky Way\nagree well with a reference value of $1.65\\pm0.19$ M$_\\odot$ yr$^{-1}$. They\nare then used to determine the radial dependence of the star formation rate and\nface-on map for the Milky Way. First, the method based on a model of star\nformation in Hi-GAL-defined dense clumps, adjusted for an increase in the\ngas-to-dust ratio with Galactocentric radius, predicts $1.65\\pm0.61$ M$_\\odot$\nyr$^{-1}$. Second, the method using the 70 $\\mu$m emission, commonly used in\nother galaxies, with a technique to assign distances to the extended emission,\npredicts $1.42^{+0.63}_{-0.44}$ M$_\\odot$ yr$^{-1}$. Finally, a method based on\ntheoretical predictions of star formation efficiency as a function of virial\nparameter, with masses corrected for metallicity dependence, applied to a\ncatalog of molecular clouds also predicts a value in agreement at $1.47$\nM$_\\odot$ yr$^{-1}$. The three methods predict the radial variation of the star\nformation rate, with remarkably good agreement from the CMZ out to about 20\nkpc. More differences were seen in face-on maps with a resolution of 0.5 kpc\nmade with the three approaches and in comparisons to the local (within 3 kpc)\nstar formation rate, indicating limitations of the methods when applied to\nsmaller scales. The 70 $\\mu$m star formation rate follows very closely the\nsurface density of molecular gas, corrected for a metallicity-dependent CO\nconversion factor. A molecular gas depletion time of 1 Gyr is consistent with\nthe data, as is a molecular Kennicutt-Schmidt relation with a power-law slope\nof $1.10 \\pm 0.06$.",
        "The time-frequency map (TFM) is frequently used in condition monitoring,\nnecessitating further processing to select an informative frequency band (IFB)\nor directly detect damage. However, selecting an IFB is challenging due to the\ncomplexity of spectral structures, non-Gaussian disturbances, and overlapping\nfault signatures in vibration signals. Additionally, dynamic operating\nconditions and low signal-to-noise ratio further complicate the identification\nof relevant features that indicate damage. To solve this problem, the present\nwork proposes a novel method for informative band selection and local damage\ndetection in rolling element bearings, utilizing non-negative tensor\nfactorization (NTF)-based dependence map analysis. The recently introduced\nconcept of the dependence map is leveraged, with a set of these maps being\nfactorized to separate informative components from non-informative ones.\nDependence maps provide valuable information on the auto-similarity of spectral\ncontent, while NTF, a powerful tool commonly used in image processing for\nfeature extraction, enhances this process. The combination of these methods\nallows for the extraction of IFBs, forming the basis for local damage\ndetection. The effectiveness of the proposed method has been validated using\nboth synthetic and real vibration signals corrupted with non-Gaussian\ndisturbances.",
        "The 7x7 reconstruction of Si(111) surface is one of the most fascinating\nconfiguration in nature, whose STM image has been well-understood by the famous\ndimer-adatom-stacking-fault model (DAS). However, the electronic property of\nthe DAS model is always confirmed to be metallic by first-principles\ncalculations, while some experiments detected insulating features. It is still\nchallenge to predict DAS-like reconstructions through traditional method to\nsolve such a puzzle. Here, we show that 7x7 reconstructions can be quickly\ndiscovered by graph theory as implemented in the graph-space based RG2 code for\ncrystal structure prediction. Two groups of reconstructions with (DAS-d8-T12,\nDAS-d8-T9H3-A, DAS-d8-T9H3-B and DAS-d8-T6H6) and without (AB-d10-T12,\nAB-d10-T9H3, AA-d10-T12 and AA-d10-T9H3) stacking-fault are discovered. They\npossess energetic stabilities comparable to the well-known DAS (DAS-d8-T12) and\nshow similar STM patterns, providing a plausible explanation for the\nexperimentally observed 7x7 reconstruction on the Si(111) surface. The\nfirst-principles calculations show that DAS-d8-T12, DAS-d8-T6H6, AB-d10-T12,\nand AA-d10-T12 are metallic, while DAS-d8-T9H3-A, DAS-d8-T9H3-B, AB-d10-T9H3\nand AA-d10-T9H3 are insulating phases with gaps of 0.043 eV, 0.182 eV, 0.043 eV\nand 0.059 eV, respectively. Our work demonstrates the predictability of the\nSi(111)-7x7 reconstruction and provides the structural candidates for\nunderstanding the experimentally observed metal-to-insulator transition.",
        "Fermented dairy products, including yogurt, are widely consumed for their\nnutritional and health benefits. While numerous methods exist to monitor and\nunderstand yogurt fermentation, the literature lacks an integrated evaluation\nof diverse sensing approaches within a single experimental framework. To\naddress this gap, this study systematically examines and compares multiple\nmeasurement techniques--electrical impedance, DC resistance, pH, optical\ntransparency, carbon dioxide concentration, ambient temperature, and relative\nhumidity--in tracking the yogurt fermentation process. By presenting a unified\nset of experimental results and assessing each method's observational\ncharacteristics, this work offers an encompassing reference point for\nresearchers seeking to understand the relative merits and limitations of\ndifferent sensing modalities. Rather than establishing definitive guidelines or\npractical recommendations, the findings provide a foundation for subsequent\ninvestigations into sensor-based fermentation monitoring, thereby contributing\nto a more comprehensive understanding of yogurt fermentation dynamics.",
        "In this work, we present a result on the local existence and uniqueness of\nsolutions to nonlinear Partial Differential-Algebraic Equations (PDAEs). By\napplying established theoretical results, we identify the conditions that\nguarantee the existence of a unique local solution. The analysis relies on\ntechniques from functional analysis, semi-group theory, and the theory of\ndifferential-algebraic systems. Additionally, we provide applications to\nillustrate the effectiveness of this result.",
        "Ischaemic stroke, a leading cause of death and disability, critically relies\non neuroimaging for characterising the anatomical pattern of injury.\nDiffusion-weighted imaging (DWI) provides the highest expressivity in ischemic\nstroke but poses substantial challenges for automated lesion segmentation:\nsusceptibility artefacts, morphological heterogeneity, age-related\ncomorbidities, time-dependent signal dynamics, instrumental variability, and\nlimited labelled data. Current U-Net-based models therefore underperform, a\nproblem accentuated by inadequate evaluation metrics that focus on mean\nperformance, neglecting anatomical, subpopulation, and acquisition-dependent\nvariability. Here, we present a high-performance DWI lesion segmentation tool\naddressing these challenges through optimized vision transformer-based\narchitectures, integration of 3563 annotated lesions from multi-site data, and\nalgorithmic enhancements, achieving state-of-the-art results. We further\npropose a novel evaluative framework assessing model fidelity, equity (across\ndemographics and lesion subtypes), anatomical precision, and robustness to\ninstrumental variability, promoting clinical and research utility. This work\nadvances stroke imaging by reconciling model expressivity with domain-specific\nchallenges and redefining performance benchmarks to prioritize equity and\ngeneralizability, critical for personalized medicine and mechanistic research.",
        "Analyzing crime events is crucial to understand crime dynamics and it is\nlargely helpful for constructing prevention policies. Point processes specified\non linear networks can provide a more accurate description of crime incidents\nby considering the geometry of the city. We propose a spatio-temporal Dirichlet\nprocess mixture model on a linear network to analyze crime events in Valencia,\nSpain. We propose a Bayesian hierarchical model with a Dirichlet process prior\nto automatically detect space-time clusters of the events and adopt a\nconvolution kernel estimator to account for the network structure in the city.\nFrom the fitted model, we provide crime hotspot visualizations that can inform\nsocial interventions to prevent crime incidents. Furthermore, we study the\nrelationships between the detected cluster centers and the city's amenities,\nwhich provides an intuitive explanation of criminal contagion.",
        "Node classification in graphs aims to predict the categories of unlabeled\nnodes by utilizing a small set of labeled nodes. However, weighted graphs often\ncontain noisy edges and anomalous edge weights, which can distort fine-grained\nrelationships between nodes and hinder accurate classification. We propose the\nEdge Weight-aware Graph Structure Learning (EWGSL) method, which combines\nweight learning and graph structure learning to address these issues. EWGSL\nimproves node classification by redefining attention coefficients in graph\nattention networks to incorporate node features and edge weights. It also\napplies graph structure learning to sparsify attention coefficients and uses a\nmodified InfoNCE loss function to enhance performance by adapting to denoised\ngraph weights. Extensive experimental results show that EWGSL has an average\nMicro-F1 improvement of 17.8% compared with the best baseline.",
        "This paper presents a novel approach to rigorously solving initial value\nproblems for semilinear parabolic partial differential equations (PDEs) using\nfully spectral Fourier-Chebyshev expansions. By reformulating the PDE as a\nsystem of nonlinear ordinary differential equations and leveraging Chebyshev\nseries in time, we reduce the problem to a zero-finding task for\nFourier-Chebyshev coefficients. A key theoretical contribution is the\nderivation of an explicit decay estimate for the inverse of the linear part of\nthe PDE, enabling larger time steps. This allows the construction of an\napproximate inverse for the Fr\\'echet derivative and the application of a\nNewton-Kantorovich theorem to establish solution existence within explicit\nerror bounds. Building on prior work, our method is extended to more complex\npartial differential equations, including the 2D Navier-Stokes equations, for\nwhich we establish global existence of the solution of the IVP for a given\nnontrivial initial condition.",
        "We consider the number of common edges in two independent spanning trees of a\ngraph $G$. For complete graphs $K_n$, we give a new proof of the fact,\noriginally obtained by Moon, that the distribution converges to a Poisson\ndistribution with expected value $2$. We also use the same method to prove an\nanalogous result for complete multipartite graphs.",
        "We present 2-4 GHz observations of polarized radio galaxies towards eight\nfast radio bursts (FRBs), producing grids of Faraday rotation measure (RM)\nsources with sky densities of 9-28 polarized sources per square degree. Using a\nBayesian interpolation framework, we constrain Galactic RM fluctuations below ~\n1 degree squared angular scales around the FRB positions. Despite the positions\nof all eight FRBs far from the Galactic plane, we constrain previously\nunresolved small-scale Galactic RM structures around six of the eight FRBs. In\ntwo of these fields, we find potential changes in the sign of the Galactic RM\nthat are not captured by previous, sparsely sampled RM grid observations. Our\nGalactic RM estimate towards the FRBs differs between a few rad m^-2 up to ~ 40\nrad m^-2 from the all-sky Galactic RM map of Hutschenreuter et al. (2022).\nExtrapolating our results to the known population of polarized FRB sources, we\nmay be incorrectly interpreting the host galaxy RM for ~ 30% of the FRB source\npopulation with current RM grid observations. Measuring small-scale Galactic RM\nvariations is crucial for identifying FRBs in low density and weakly magnetized\nenvironments, which in turn could serve as potent probes of cosmic magnetism.\nThis framework of reconstructing continuous Galactic RM structure from RM grid\nobservations can be readily applied to FRBs that fall in the sky coverage of\nupcoming large-sky radio polarization surveys of radio galaxies, such as the\nVery Large Array Sky Survey (VLASS) and the Polarization Sky Survey of the\nUniverse's Magnetism (POSSUM).",
        "Recent advancements in robots powered by large language models have enhanced\ntheir conversational abilities, enabling interactions closely resembling human\ndialogue. However, these models introduce safety and security concerns in HRI,\nas they are vulnerable to manipulation that can bypass built-in safety\nmeasures. Imagining a social robot deployed in a home, this work aims to\nunderstand how everyday users try to exploit a language model to violate\nethical principles, such as by prompting the robot to act like a life partner.\nWe conducted a pilot study involving 21 university students who interacted with\na Misty robot, attempting to circumvent its safety mechanisms across three\nscenarios based on specific HRI ethical principles: attachment, freedom, and\nempathy. Our results reveal that participants employed five techniques,\nincluding insulting and appealing to pity using emotional language. We hope\nthis work can inform future research in designing strong safeguards to ensure\nethical and secure human-robot interactions.",
        "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
        "Gaussian variational approximations are widely used for summarizing posterior\ndistributions in Bayesian models, especially in high-dimensional settings.\nHowever, a drawback of such approximations is the inability to capture skewness\nor more complex features of the posterior. Recent work suggests applying\nskewness corrections to existing Gaussian or other symmetric approximations to\naddress this limitation. We propose to incorporate the skewness correction into\nthe definition of an approximating variational family. We consider\napproximating the posterior for hierarchical models, in which there are\n``global'' and ``local'' parameters. A baseline variational approximation is\ndefined as the product of a Gaussian marginal posterior for global parameters\nand a Gaussian conditional posterior for local parameters given the global\nones. Skewness corrections are then considered. The adjustment of the\nconditional posterior term for local variables is adaptive to the global\nparameter value. Optimization of baseline variational parameters is performed\njointly with the skewness correction. Our approach allows the location, scale\nand skewness to be captured separately, without using additional parameters for\nskewness adjustments. The proposed method substantially improves accuracy for\nonly a modest increase in computational cost compared to state-of-the-art\nGaussian approximations. Good performance is demonstrated in generalized linear\nmixed models and multinomial logit discrete choice models.",
        "Recent Multimodal Large Language Models(MLLMs) often use a large number of\nvisual tokens to compensate their visual shortcoming, leading to excessive\ncomputation and obvious visual redundancy. In this paper, we investigate what\nkind of visual tokens are needed for MLLMs, and reveal that both foreground and\nbackground tokens are critical for MLLMs given the varying difficulties of\nexamples. Based on this observation, we propose a graph-based method towards\ntraining-free visual token pruning, termed G-Prune.In particular, G-Prune\nregards visual tokens as nodes, and construct their connections based on their\nsemantic similarities. Afterwards, the information flow is propagated via\nweighted links, and the most important tokens after iterations are kept for\nMLLMs, which can be front or background.To validate G-Prune, we apply it to a\nrecent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of\nbenchmarks.The experiment results show that G-Prune can greatly reduce\ncomputation overhead while retaining high performance on both coarse- and\nfine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of\nLLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops,\nrespectively.",
        "This paper introduces a novel quantum algorithm that is able to classify a\nhierarchy of classes of imbalanced Boolean functions. The fundamental\ncharacteristic of imbalanced Boolean functions is that the proportion of\nelements in their domain that take the value $0$ is not equal to the proportion\nof elements that take the value $1$. For every positive integer $n$, the\nhierarchy contains a class of Boolean functions defined based on their\nbehavioral pattern. The common trait of all the functions belonging to the same\nclass is that they possess the same imbalance ratio. Our algorithm achieves\nclassification in a straightforward manner as the final measurement reveals the\nunknown function with probability $1$. Let us also note that the proposed\nalgorithm is an optimal oracular algorithm because it can classify the\naforementioned functions with a single query to the oracle. At the same time we\nexplain in detail the methodology we followed to design this algorithm in the\nhope that it will prove general and fruitful, given that it can be easily\nmodified and extended to address other classes of imbalanced Boolean functions\nthat exhibit different behavioral patterns.",
        "Consider a general $3$-dimensional Lotka-Volterra system with a rational\nfirst integral of degree two of the form $H=x^i y^j z^k$. The restriction of\nthis Lotka-Volterra system to each surface $H(x,y,z)=h$ varying $h\\in\n\\mathbb{R}$ provide Kolmogorov systems. With the additional assumption that\nthey have a Darboux invariant of the form $x^\\ell y^m e^{st}$ they reduce to\nthe Kolmogorov systems \\begin{equation*} \\begin{split} \\dot{x}&=x \\left( a_0-\n\\mu (c_1 x + c_2 z^2 + c_3 z)\\right),\\\\ \\dot{z}&=z\\left( c_0+ c_1 x + c_2 z^2 +\nc_3 z\\right). \\end{split} \\end{equation*} In this paper we classify the phase\nportraits in the Poincar\\'e disc of all these Kolmogorov systems which depend\non six parameters.",
        "Let $Q$ be the Markov quiver, and let $W$ be an infinitely mutable potential\nfor $Q$. We calculate some low degree refined BPS invariants for the resulting\nJacobi algebra, and use them to show that the critical cohomological Hall\nalgebra $\\mathcal{H}_{Q,W}$ is not necessarily spherically generated, and is\nnot independent of the choice of infinitely mutable potential $W$. This leads\nto a counterexample to a conjecture of Gaiotto, Grygoryev and Li \\cite[\\S\n2.1]{GGL}, but also suggestions for how to modify it. In the case of generic\ncubic $W$, we discuss a way to modify the conjecture, by excluding the\nnon-spherical part via the decomposition of $\\mathcal{H}_{Q,W}$ according to\nthe characters of a discrete symmetry group.",
        "Within the so-called group geometric approach to (super)gravity and\n(super)string theories, any compact Lie group manifold $G_{c}$ can be smoothly\ndeformed into a group manifold $G_{c}^{\\mu }$ (locally diffeomorphic to $G_{c}$\nitself), which is `soft', namely, based on a non-left-invariant, intrinsic\none-form Vielbein $\\mu $, which violates the Maurer-Cartan equations and\nconsequently has a non-vanishing associated curvature two-form. Within the\nframework based on the above deformation (`softening'), we show how to\nconstruct an infinite-dimensional (infinite-rank), generalized Kac-Moody (KM)\nalgebra associated to $G_{c}^{\\mu }$, starting from the generalized KM algebras\nassociated to $G_{c}$. As an application, we consider KM algebras associated to\ndeformed manifolds such as the `soft' circle, the `soft' two-sphere and the\n`soft' three-sphere. While the generalized KM algebra associated to the\ndeformed circle is trivially isomorphic to its undeformed analogue, and hence\nnot new, the `softening' of the two- and three- sphere includes squashed\nmanifolds (and in particular, the so-called Berger three-sphere) and yields to\nnon-trivial results.",
        "Conflict Mitigation (CM) in Open Radio Access Network (O-RAN) is a topic that\nis gaining importance as commercial O-RAN deployments become more complex.\nAlthough research on CM is already covered in terms of simulated network\nscenarios, it lacks validation using real-world deployment and Over The Air\n(OTA) Radio Frequency (RF) transmission. Our objective is to conduct the first\nassessment of the Conflict Mitigation Framework (CMF) for O-RAN using a\nreal-world testbed and OTA RF transmission. This paper presents results of an\nexperiment using a dedicated testbed built in an O-RAN Open Test and\nIntegration Center (OTIC) to confirm the validity of one of the Conflict\nResolution (CR) schemes proposed by existing research. The results show that\nthe implemented conflict detection and resolution mechanisms allow a\nsignificant improvement in network operation stability by reducing the\nvariability of the measured Downlink (DL) throughput by 78%.",
        "We phenomenologically formulate and experimentally observe an adiabatic\ntransverse thermoelectric conversion enhanced by a heat current re-orientation\nin artificially tilted multilayers (ATMLs). By alternately stacking two\nmaterials with different thermal conductivities and rotating its multilayered\nstructure with respect to a longitudinal temperature gradient, off-diagonal\ncomponents in the thermal conductivity tensor are induced. This off-diagonal\nthermal conduction (ODTC) generates a finite transverse temperature gradient\nand Seebeck-effect-induced thermopower in the adiabatic condition, which is\nsuperposed on the isothermal transverse thermopower driven by the off-diagonal\nSeebeck effect (ODSE). In this study, we calculate and observe the\ntwo-dimensional temperature distribution and the resultant transverse\nthermopower in ATMLs comprising thermoelectric Co$_{2}$MnGa Heusler alloys and\nBi$_{2-a}$Sb$_{a}$Te$_{3}$ compounds. By changing the tilt angle from 0{\\deg}\nto 90{\\deg}, the transverse temperature gradient obviously appeared in the\nmiddle angles and the transverse thermopower increases up to -116.1 ${\\mu}$V\/K\nin Co$_{2}$MnGa\/Bi$_{0.2}$Sb$_{1.8}$Te$_{3}$-based ATML at the tilt angle of\n45{\\deg} whereas the isothermal contribution is estimated to be -82.6\n${\\mu}$V\/K from the analytical calculation. This hybrid action derived from\nODTC results in the significant variation of the maximum reduced efficiency for\ntransverse thermoelectric conversion from 3.1% in the isothermal limit to 8.1%\nin the adiabatic limit.",
        "Great Britain aims to meet growing electricity demand and achieve a fully\ndecarbonised grid by 2035, targeting 70 GW of solar photovoltaic (PV) capacity.\nHowever, grid constraints and connection delays hinder solar integration. To\naddress these integration challenges, various connection reform processes and\npolicies are being developed [1]. This study supports the connection reforms\nwith a model that estimates regional PV capacity at the NUTS 3 level,\nexplaining 89% of the variation in capacity, with a mean absolute error of 20\nMW and a national mean absolute percentage error of 5.4%. Artificial surfaces\nand agricultural areas are identified as key factors in deployment. The model\nhas three primary applications: disaggregating national PV capacity into\nregional capacity, benchmarking regional PV deployment between different\nregions, and forecasting future PV capacity distribution. These applications\nsupport grid operators in generation monitoring and strategic grid planning by\nidentifying regions where capacity is likely to be concentrated. This can\naddress grid connection delays, plan network expansions, and resolve land-use\nconflicts."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Llamafactory: Unified efficient fine-tuning of 100+ language models",
    "start_abstract":"Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
      ],
      "abstract":[
        "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Radiative decays of $X(3872)$ in $D{\\bar D}^*$ molecule scenario",
        "Spontaneous in-plane anomalous Hall response observed in a ferromagnetic\n  oxide",
        "Hyperbolicity and Volume of Hyperbolic Bongles",
        "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism",
        "Solar prosumage under different pricing regimes: Interactions with the\n  transmission grid",
        "Finite-temperature bubble nucleation with shifting scale hierarchies",
        "Residually finite amenable groups that are not Hilbert-Schmidt stable",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "1-shifted Lie bialgebras and their quantizations",
        "Application of the Pontryagin Maximum Principle to the robust\n  time-optimal control of two-level quantum systems",
        "Ensemble control of n-level quantum systems with a scalar control",
        "A Differential Index Measuring Rater's Capability in Educational\n  Assessment",
        "Two-dimensional higher-order Weyl semimetals",
        "Background-field method and QCD factorization",
        "Powerful rank verification for multivariate Gaussian data with any\n  covariance structure",
        "On zeros of polynomials associated with Heun class equations",
        "SDSS-IV MaStar: Quantification and Abatement of Interstellar Absorption\n  in the Largest Empirical Stellar Spectral Library",
        "The Critical Role of Dust On The [O III] Planetary Nebula Luminosity\n  Function's Bright-End Cutoff",
        "On the features of great Forbush effect during May 2024 extreme\n  geomagnetic storm",
        "The stochastic nature of migration of disc instability protoplanets in\n  three-dimensional hydrodynamical and MHD simulations of fragmenting discs",
        "Continuous functions on limits of F-decomposable systems",
        "Learning response functions of analog quantum computers: analysis of\n  neutral-atom and superconducting platforms",
        "The dynamics of small-scale magnetic fields modulated by the solar cycle",
        "Joint Communication and Sensing with Bipartite Entanglement over Bosonic\n  Channels",
        "Infectious diseases, imposing density-dependent mortality on MHC\/HLA\n  variation, can account for balancing selection and MHC\/HLA polymorphism",
        "Bipartite expansion beyond biparticity",
        "On a new robust method of inference for general time series models",
        "Modular Compilation for Quantum Chiplet Architectures",
        "A stochastic maximum principle of mean-field type with monotonicity\n  conditions"
      ],
      "abstract":[
        "We investigate the radiative decays of the $X(3872)$ to $\\gamma\nV~(V=\\rho^0,\\, \\omega)$ in the molecule scenario, where the $X(3872)$ is\nregarded as a pure hadronic molecule of the $D\\bar{D}^*+c.c$ in an $S$-wave\nwith the quantum numbers $J^{PC}=1^{++}$. The radiative processes were assumed\nto occur via the triangle hadronic loops, and the relevant calculations were\nconducted using an effective Lagrangian approach. It is found that the absolute\ndecay widths are model-dependent, but the relative width ratio is rather\nindependent of the model parameter. Moreover, the calculated results indicate\nthat the radiative decays of the $X(3872)$ are strongly influenced by the\nmolecular configuration characterized by the proportion of the charged and\nneutral constituents. We hope that the present calculations could be tested by\nthe experimental measurements.",
        "Recent observation of anomalous Hall effect (AHE) induced by magnetic field\nor spin magnetization lying in the Hall deflection plane has sparked interest\nin diverse mechanisms for inducing the Hall vector component perpendicular to\nthe applied magnetic field. Such off-diagonal coupling, which is strictly\nconstrained by symmetry of the system, provides new degrees of freedom for\nengineering Hall responses. However, spontaneous response as extensively\nstudied for out-of-plane AHE remains unexplored. Here we elucidate in-plane AHE\nin a typical ferromagnetic oxide SrRuO$_3$. The (111)-orientated ultrathin\nfilms with in-plane easy axes of spin magnetization exhibit spontaneous AHE at\nzero field, which is intrinsically coupled to the in-plane spin magnetization\nand controllable via its direction. Systematic measurements by varying\nazimuthal and polar field angles further reveal complex Hall responses shaped\nby higher-order terms allowed by trigonal distortion of the films. Our findings\nhighlight versatile and controllable in-plane Hall responses with out-of-plane\norbital ferromagnetism.",
        "We consider a simple but infinite class of staked links known as bongles. We\nprovide necessary and sufficient conditions for these bongles to be hyperbolic.\nThen, we prove that all balanced hyperbolic $n$-bongles have the same volume\nand the corresponding volume is an upper bound on the volume of any hyperbolic\n$n$-bongle for $n$ even. Moreover, all hyperbolic $n$-bongles have volume\nstrictly less than $5n(1.01494\\dots)$. We also include explicit volume\ncalculations for all hyperbolic 3-bongles through 6-bongles.",
        "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.",
        "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
        "Focusing on supercooled phase transitions in models with classical scale\nsymmetry, we formulate a state-of-the art framework for computing the\nbubble-nucleation rate, accounting for the presence of various energy scales.\nIn particular, we examine the limitations of derivative expansions in\nconstructing a thermal effective field theory for bubble nucleation. We show\nthat for gauge field fluctuations, derivative expansions diverge after the\nleading two orders due to the strong variation in gauge field masses between\nthe high- and low-temperature phases. By directly computing these contributions\nusing the fluctuation determinant, we capture these effects while also\naccounting for large explicit logarithms at two loops, utilising the exact\nrenormalisation group structure of the EFT. Finally, we demonstrate how this\napproach significantly improves nucleation rate calculations compared to\nleading-order results, providing a more robust framework for predicting\ngravitational-wave signals from supercooled phase transitions in models such as\nthe SU(2)cSM.",
        "We construct the first examples of residually finite amenable groups that are\nnot Hilbert-Schmidt (HS) stable. We construct finitely generated, class 3\nnilpotent by cyclic examples and solvable linear finitely presented examples.\nThis also provides the first examples of amenable groups that are very flexibly\nHS-stable but not flexibly HS-stable and the first examples of residually\nfinite amenable groups that are not locally HS-stable. Along the way we exhibit\n(necessarily not-finitely-generated) class 2 nilpotent groups $G = A\\rtimes \\Z$\nwith $A$ abelian such that the periodic points of the dual action are dense but\nit does not admit dense periodic measures. Finally we use the\nTikuisis-White-Winter theorem to show all of the examples are not even\noperator-HS-stable; they admit operator norm almost homomorphisms that can not\nbe HS-perturbed to true homomorphisms.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "In this paper, we define (cohomologically) 1-shifted Manin triples and\n1-shifted Lie bialgebras, and study their properties. We derive many results\nthat are parallel to those found in ordinary Lie bialgebras, including the\ndouble construction and the existence of a 1-shifted $r$-matrix satisfying the\nclassical Yang-Baxter equation.\n  Turning to quantization, we first construct a canonical quantization for each\n1-shifted metric Lie algebra $\\mathfrak{g}$, producing a deformation to the\nsymmetric monoidal category of $\\mathfrak{g}$ modules over a formal variable\n$\\hbar$. This quantization is in terms of a curved differential graded algebra.\nUnder a further technical assumption, we construct quantizations of transverse\nLagrangian subalgebras of $\\mathfrak{g}$, which is a pair of DG algebras\nconnected by Koszul duality, and give rise to monoidal module categories of the\nquantized double.\n  Finally, we apply this to Manin triples arising from Lie algebras of loop\ngroups, and construct 1-shifted meromorphic $r$-matrices. The resulting\nquantizations are the cohomologically-shifted analogue of Yangians.",
        "We study the time-optimal robust control of a two-level quantum system\nsubjected to field inhomogeneities. We apply the Pontryagin Maximum Principle\nand we introduce a reduced space onto which the optimal dynamics is projected\ndown. This reduction leads to a complete analytical derivation of the optimal\nsolution in terms of elliptic functions and elliptic integrals. Necessary\noptimality conditions are then obtained for the original system. These\nconditions are verified numerically and lead to the optimal control protocol.\nVarious examples, ranging from state-to-state transfer to the generation of a\nNot gate, illustrate this study. The connection with other geometric\noptimization approaches that have been used to solve this problem is also\ndiscussed.",
        "In this paper we discuss how a general bilinear finite-dimensional closed\nquantum system with dispersed parameters can be steered between eigenstates. We\nshow that, under suitable conditions on the separation of spectral gaps and the\nboundedness of parameter dispersion, rotating wave and adiabatic approximations\ncan be employed in cascade to achieve population inversion between arbitrary\neigenstates. We propose an explicit control law and test numerically the\nsharpness of the conditions on several examples.",
        "A rater's ability to assign accurate scores can significantly impact the\noutcomes of educational assessments. However, common indices for evaluating\nrater characteristics typically focus on either their severity or their\ndiscrimination ability (i.e., skills to differentiate between students).\nAdditionally, these indices are often developed without considering the rater's\naccuracy in scoring students at different ability levels. To address the\nlimitations, this study proposes a single-value measure to assess a rater's\ncapability of assigning accurate scores to students with varying ability\nlevels. The measure is derived from the partial derivatives of each rater's\npassing rate concerning student ability. Mathematical derivations of the index\nunder generalized multi-facet models and hierarchical rater models are\nprovided. To ease the implementation of the index, this study develops\nparameter estimation using marginal likelihood and its Laplacian approximation\nwhich allows for efficient evaluation and processing of large datasets\ninvolving numerous students and raters. Simulation studies demonstrate the\naccuracy of parameter recovery using the approximate likelihood and show how\nthe capability indices vary with different levels of rater severity. An\nempirical study further tests the practical applicability of the new measure,\nwhere raters evaluate essays on four topics: \"family,\" \"school,\" \"sport,\" and\n\"work.\" Results show that raters are most capable when rating the topic of\nfamily and least capable when rating sport, with individual raters displaying\ndifferent capabilities across the various topics.",
        "We propose a theoretical scheme to realize two-dimensional higher-order Weyl\nsemimetals using a trilayer topological insulator film coupled with a d-wave\naltermagnet. Our results show that the trilayer topological insulator exhibits\ntwo-dimensional Weyl semimetal characteristics with helical edge states.\nNotably, the Weyl points are located at four high-symmetry points in the\nBrillouin zone, and the topology of symmetric subspaces governs the formation\nof these Weyl points and edge states. Upon introducing a d-wave altermagnet\noriented along the z-direction, gaps open in the helical edge states while\npreserving two Weyl points, leading to the realization of two-dimensional\nhigher-order Weyl semimetals hosting topological corner states. The nonzero\nwinding number in the subspace along the high-symmetry line serves as a\ntopological invariant characterizing these corner states, and the other\nsubspace Hamiltonian confirms the existence of the Weyl points. Finally, a\ntopological phase diagram provides a complete topological description of the\nsystem.",
        "One method for deriving a factorization for QCD processes is to use\nsuccessive integration over fields in the functional integral. In this\napproach, we separate the fields into two categories: dynamical fields with\nmomenta above a relevant cutoff, and background fields with momenta below the\ncutoff. The dynamical fields are then integrated out in the background of the\nlow-momentum background fields. This strategy works well at tree level,\nallowing us to quickly derive QCD factorization formulas at leading order.\nHowever, to extend the approach to higher loops, it is necessary to rigorously\ndefine the functional integral over dynamical fields in an arbitrary background\nfield. This framework was carefully developed for the calculation of the\neffective action in a background field at the two-loop level in the classic\npaper by Abbott [1]. Building on this work, I specify the renormalized\nbackground-field Lagrangian and define the notion of the quantum average of an\noperator in a background field, consistent with the ``separation of scales''\nscheme mentioned earlier. As examples, I discuss the evolution of the twist-2\ngluon light-ray operator and the one-loop gluon propagator in a background\nfield near the light cone.",
        "Upon observing $n$-dimensional multivariate Gaussian data, when can we infer\nthat the largest $K$ observations came from the largest $K$ means? When $K=1$\nand the covariance is isotropic, \\cite{Gutmann} argue that this inference is\njustified when the two-sided difference-of-means test comparing the largest and\nsecond largest observation rejects. Leveraging tools from selective inference,\nwe provide a generalization of their procedure that applies for both any $K$\nand any covariance structure. We show that our procedure draws the desired\ninference whenever the two-sided difference-of-means test comparing the pair of\nobservations inside and outside the top $K$ with the smallest standardized\ndifference rejects, and sometimes even when this test fails to reject. Using\nthis insight, we argue that our procedure renders existing simultaneous\ninference approaches inadmissible when $n > 2$. When the observations are\nindependent (with possibly unequal variances) or equicorrelated, our procedure\ncorresponds exactly to running the two-sided difference-of-means test comparing\nthe pair of observations inside and outside the top $K$ with the smallest\nstandardized difference.",
        "Sch\\\"afke and Schmidt established that the asymptotics of the coefficients of\nthe local solution to some linear differential equation is related to global\nstructures of solutions. The Heun class equations have the accessory\nparameters, and we investigate the polynomials whose variable is the accessory\nparameter which appears as the coefficients of the local solution. By\ncalculating the zeros of the polynomials numerically, we obtain the data of the\nspectral related to the Heun class equations numerically.",
        "We assess the impact of CaII 3934,3969 and NaI 5891,5897 absorption arising\nin the interstellar medium (ISM) on the SDSS-IV MaNGA Stellar Library (MaStar)\nand produce corrected spectroscopy for 80% of the 24,162-star catalog. We model\nthe absorption strength of these transitions as a function of stellar distance,\nGalactic latitude, and dust reddening based upon high-spectral resolution\nstudies. With this model, we identify 6342 MaStar stars that have negligible\nISM absorption ($W^\\mathrm{ISM}$(CaII K) $<0.07$ Ang and $W^\\mathrm{ISM}$(NaI\n5891) $<0.05$ Ang). For 12,110 of the remaining stars, we replace their NaI D\nprofile (and their CaII profile for effective temperatures $T_{\\rm eff}>9000$\nK) with a coadded spectrum of low-ISM stars with similar $T_{\\rm eff}$, surface\ngravity, and metallicity. For 738 additional stars with $T_{\\rm eff}>9000$ K,\nwe replace these spectral regions with a matching ATLAS9-based BOSZ model. This\nresults in a mean reduction in $W$(CaII K) ($W$(NaI D)) of $0.4-0.7$ Ang\n($0.6-1.1$ Ang) for hot stars ($T_{\\rm eff}>7610$ K), and a mean reduction in\n$W$(NaI D) of $0.1-0.2$ Ang for cooler stars. We show that interstellar\nabsorption in simple stellar population (SSP) model spectra constructed from\nthe original library artificially enhances $W$(CaII K) by $\\gtrsim20\\%$ at\nyoung ages ($<400$ Myr); dramatically enhances the strength of stellar NaI D in\nstarbursting systems (by ${\\gtrsim}50\\%$); and enhances stellar NaI D in older\nstellar populations (${\\gtrsim}10$ Gyr) by ${\\gtrsim}10\\%$. We provide SSP\nspectra constructed from the cleaned library, and discuss the implications of\nthese effects for stellar population synthesis analyses constraining stellar\nage, [Na\/Fe] abundance, and the initial mass function.",
        "We examine the relationship between circumnebular extinction and core mass\nfor sets of [O III]-bright planetary nebulae (PNe) in the Large Magellanic\nCloud and M31. We confirm that for PNe within one magnitude of the Planetary\nNebula Luminosity Function's (PNLF's) bright-end cutoff magnitude (M*), higher\ncore-mass PNe are disproportionally affected by greater circumnebular\nextinction. We show that this result can explain why the PNLF cutoff is so\ninsensitive to population age. In younger populations, the higher-mass,\nhigher-luminosity cores experience greater circumnebular extinction from the\ndust created by their AGB progenitors compared to the lower-mass cores. We\nfurther show that when our core-mass-nebular extinction law is combined with\npost-AGB stellar evolutionary models, the result is a large range of population\nages where the brightest PNe all have nearly identical [O III] luminosities.\nFinally, we note that while there is some uncertainty about whether the oldest\nstellar populations can produce planetary nebulae as bright as M*, this issue\nis resolved if the initial-final mass relation (IFMR) for the lowest-mass stars\nresults in slightly more massive cores, as observed in some clusters.\nAlternatively, introducing a small amount of intrinsic scatter (0.022 Msun)\ninto the IFMR also addresses this uncertainty.",
        "The work investigates the features of galactic cosmic ray density and\nanisotropy behavior and their relation to solar sources, interplanetary and\ngeomagnetic disturbances from May 8 to May 13, 2024. During this time, powerful\nsolar flares and fast CMEs were recorded, leading to registration of an extreme\ngeomagnetic storm along with one of the most significant Forbush effects for\nthe entire observation period. All the calculations of cosmic ray\ncharacteristics are made using the data of global neutron monitor network and\nunique methods maintained at IZMIRAN: the Global Survey Method and the Ring of\nStations Method. It is determined that the magnitude of Forbush effect under\nstudy was 15.7% (for particles with 10 GV rigidity) and as an extreme\ngeomagnetic storm was recorded there was a significant magnetospheric effect\nobserved in the data of neutron monitors (~4%).",
        "We present a detailed analysis of the nature of migration of protoplanetary\nclumps formed via disc instability in self-consistent 3D hydrodynamical (HD)\nand magneto-hydrodynamical (MHD) simulations of self-gravitating discs.\nMotivated by the complex structure of protoplanetary clumps we do not introduce\nsink particles. We find that the orbital evolution of the clumps has a\nstochastic character but also exhibits recurrent properties over many orbits.\nClump migration is governed by two sources of gravitational torques: a torque\noriginating from a region about twice the Hill sphere around each clump's\norbit, and the torque resulting from clump-clump interactions. Compared to\nnon-magnetized companion runs, the latter are more frequent in MHD simulations,\nwhich give rise to more numerous clumps starting off at smaller masses, often\nbelow a Neptune mass. Clump-clump interactions can lead to temporary strong\naccelerations of migration in both directions, but integrated over time provide\na lesser impact than disc-driven torques. They can also lead to clump mergers\nbut do not cause ejections; a difference to previous works which adopted sink\nparticles. The local \"Hill torque\" is responsible for the fast migration,\ninward or outward. Estimating the characteristic timescales of conventional\nmigration in our regime, we find that the disc-driven migration timescales are\nin agreement with Type III migration. However, the dominant local torque is\nrapidly fluctuating, which reflects the turbulent nature of the flow. The\nresulting stochastic migration pattern is markedly different from Type III\nrunaway migration and appears to be a distinctive feature of orbital dynamics\nin a fragmenting disc.",
        "We introduce the concept of F-decomposable systems, well-ordered inverse\nsystems of Hausdorff compacta with fully closed bonding mappings. A continuous\nmapping between Hausdorff compacta is called fully closed if the intersection\nof the images of any two closed disjoint subsets is finite. We give a\ncharacterization of such systems in terms of a property of the continuous\nfunctions on their limit. When, moreover, the fibers of neighboring bonding\nmappings are metrizable, we call the limit of such a system an F_d-compact, a\nparticular case of a Fedorchuk compact. The stated property allows us to obtain\na locally uniformly rotund renorming on the space C(K), where K is an\nF_d-compact of countable spectral height.",
        "Analog quantum computation is an attractive paradigm for the simulation of\ntime-dependent quantum systems. Programmable analog quantum computers have been\nrealized in hardware using a variety of physical principles, including\nneutral-atom and superconducting technologies. The input parameters of the\nphysical Hamiltonians that are used to program the quantum simulator generally\ndiffer from the parameters that characterize the output distribution of data\nproduced under a specified quantum dynamics. The relationship between the input\nand output parameters is known as the response function of the analog device.\nHere, we introduce a streaming algorithm for learning the response function of\nanalog quantum computers from arbitrary user inputs, thus not requiring special\ncalibration runs. We use the method to learn and compare the response functions\nof several generations of analog quantum simulators based on superconducting\nand neutral-atom programmable arrays.",
        "In addition to sunspots, which represent the most easily visualized\nmanifestation of solar magnetism, cutting-edge observations of the solar\natmosphere have uncovered a plethora of magnetic flux tubes, down to the\nresolving power of modern high-resolution telescopes (a few tens of km),\nrevealing how the Sun is a fully magnetized star. These magnetic elements are\nadvected and buffeted by ambient plasma flows and turbulent convection,\nresulting in perturbations of the flux tubes that make them natural conduits\nfor channeling wave energy into the upper layers of the Sun's atmosphere and\nsignificantly contributing to the acceleration of the solar wind. Today, data\nacquired by the Helioseismic and Magnetic Imager (HMI) onboard NASA's Solar\nDynamics Observatory (SDO), have made it possible to study the dynamics of\nsmall-scale magnetic fields over long timescales. Here, for the first time, we\npresent the discovery of a modulation in the dynamical behavior of small-scale\nmagnetic concentrations in the photosphere over temporal scales consistent with\nthe solar activity cycle (i.e. 11 years), which has only been made possible by\nthe long observing lifetime of the SDO\/HMI spacecraft. Furthermore, a temporal\nvarying polarization of their perturbations is also found on similar\ntimescales. This demonstrates how the small-scale dynamics of magnetic fields\nare also affected by the global dynamo. These discoveries were realized through\nautomated tracking of magnetic fields in the solar photosphere across 11\ncontinuous years, resulting in the most extended statistical analyses of its\nkind so far, with more than 31 million magnetic concentrations examined.",
        "We consider a joint communication and sensing problem in an optical link in\nwhich a low-power transmitter attempts to communicate with a receiver while\nsimultaneously identifying the range of a defect creating a backscattered\nsignal. We model the system as a lossy thermal noise bosonic channel in which\nthe location of the target, modeled as a beamsplitter, affects the timing of\nthe backscattered signal. Motivated by the envisioned deployment of\nentanglement sharing quantum networks, we allow the transmitter to exploit\nentanglement to assist its sensing and communication. Since entanglement is\nknown to enhance sensing, as known from quantum illumination, and increase\ncommunication rates, as known from the characterization of the\nentanglement-assisted capacity, the transmitter is faced with a trade-off and\nmust judiciously allocate its entanglement resources. Our main result is a\ncharacterization of the trade-offs incurred in the form of an achievable\nrate\/error-exponent region which can beat time-sharing in certain cases. The\nproof of our result relies on technical results of independent interests, by\nwhich we carefully show how to extend the known asymptotic characterization of\nmulti-hypothesis testing Chernoff exponent in finite-dimensional spaces to\ninfinite-dimensional spaces and provide a characterization of phase shift\nkeying modulated displaced thermal states in Fock basis.",
        "The human MHC transplantation loci (HLA-A, -B, -C, -DPB1, -DQB1, -DRB1) are\nthe most polymorphic in the human genome. It is generally accepted this\npolymorphism reflects a role in presenting pathogen-derived peptide to the\nadaptive immune system. Proposed mechanisms for the polymorphism such as\nnegative frequency-dependent selection (NFDS) and heterozygote advantage (HA)\nfocus on HLA alleles, not haplotypes. Here, we propose a model for the\npolymorphism in which infectious diseases impose independent density-dependent\nregulation on HLA haplotypes. More specifically, a complex pathogen environment\ndrives extensive host polymorphism through a guild of HLA haplotypes that are\nspecialised and show incomplete peptide recognition. Separation of haplotype\nguilds is maintained by limiting similarity. The outcome is a wide and stable\nrange of haplotype densities at steady-state in which effective Fisher\nfitnesses are zero. Densities, and therefore frequencies, emerge theoretically\nas alternative measures of fitness. A catalogue of ranked frequencies is\ntherefore one of ranked fitnesses. The model is supported by data from a range\nof sources including a Caucasian HLA dataset compiled by the US National Marrow\nDonor Program (NMDP). These provide evidence of positive selection on the top\n350-2000 5-locus HLA haplotypes taken from an overall NMDP sample set of 10E5.\nHigh-fitness haplotypes drive the selection of 137 high-frequency alleles\nspread across the 5 HLA loci under consideration. These alleles demonstrate\npositive epistasis and pleiotropy in the formation of haplotypes. Allelic\npleiotropy creates a network of highly inter-related HLA haplotypes that\naccount for 97% of the census sample. We suggest this network has properties of\na quasi-species and is itself under selection. We also suggest this is the\norigin of balancing selection in the HLA system.",
        "The recently suggested bipartite analysis extends the Kauffman planar\ndecomposition to arbitrary $N$, i.e. extends it from the Jones polynomial to\nthe HOMFLY polynomial. This provides a generic and straightforward\nnon-perturbative calculus in an arbitrary Chern--Simons theory. Technically,\nthis approach is restricted to knots and links which possess bipartite\nrealizations, i.e. can be entirely glued from antiparallel lock (two-vertex)\ntangles rather than single-vertex $R$-matrices. However, we demonstrate that\nthe resulting positive decomposition (PD), i.e. the representation of the\nfundamental HOMFLY polynomials as positive integer polynomials of the three\nparameters $\\phi$, $\\bar\\phi$ and $D$, exists for arbitrary knots, not only\nbipartite ones. This poses new questions about the true significance of\nbipartite expansion, which appears to make sense far beyond its original scope,\nand its generalizations to higher representations. We have provided two\nexplanations for the existence of the PD for non-bipartite knots. An\ninteresting option is to resolve a particular bipartite vertex in a\nnot-fully-bipartite diagram and reduce the HOMFLY polynomial to a linear\ncombination of those for smaller diagrams. If the resulting diagrams correspond\nto bipartite links, this option provides a PD even to an initially\nnon-bipartite knot. Another possibility for a non-bipartite knot is to have a\nbipartite clone with the same HOMFLY polynomial providing this PD. We also\nsuggest a promising criterium for the existence of a bipartite realization\nbehind a given PD, which is based on the study of the precursor Jones\npolynomials.",
        "In this article, we propose a novel logistic quasi-maximum likelihood\nestimation (LQMLE) for general parametric time series models. Compared to the\nclassical Gaussian QMLE and existing robust estimations, it enjoys many\ndistinctive advantages, such as robustness in respect of distributional\nmisspecification and heavy-tailedness of the innovation, more resiliency to\noutliers, smoothness and strict concavity of the log logistic quasi-likelihood\nfunction, and boundedness of the influence function among others. Under some\nmild conditions, we establish the strong consistency and asymptotic normality\nof the LQMLE. Moreover, we propose a new and vital parameter identifiability\ncondition to ensure desirable asymptotics of the LQMLE. Further, based on the\nLQMLE, we consider the Wald test and the Lagrange multiplier test for the\nunknown parameters, and derive the limiting distributions of the corresponding\ntest statistics. The applicability of our methodology is demonstrated by\nseveral time series models, including DAR, GARCH, ARMA-GARCH, DTARMACH, and\nEXPAR. Numerical simulation studies are carried out to assess the finite-sample\nperformance of our methodology, and an empirical example is analyzed to\nillustrate its usefulness.",
        "As quantum computing technology continues to mature, industry is adopting\nmodular quantum architectures to keep quantum scaling on the projected path and\nmeet performance targets. However, the complexity of chiplet-based quantum\ndevices, coupled with their growing size, presents an imminent scalability\nchallenge for quantum compilation. Contemporary compilation methods are not\nwell-suited to chiplet architectures. In particular, existing qubit allocation\nmethods are often unable to contend with inter-chiplet links, which don't\nnecessary support a universal basis gate set. Furthermore, existing methods of\nlogical-to-physical qubit placement, swap insertion (routing), unitary\nsynthesis, and\/or optimization are typically not designed for qubit links of\nsignificantly varying levels of duration or fidelity. In this work, we propose\nSEQC, a complete and parallelized compilation pipeline optimized for\nchiplet-based quantum computers, including several novel methods for qubit\nplacement, qubit routing, and circuit optimization. SEQC attains up to a 36%\nincrease in circuit fidelity, accompanied by execution time improvements of up\nto 1.92x. Additionally, owing to its ability to parallelize compilation, SEQC\nachieves consistent solve time improvements of 2-4x over a chiplet-aware Qiskit\nbaseline.",
        "The objective of this paper is to weaken the Lipschitz condition to a\nmonotonicity condition and to study the corresponding Pontryagin stochastic\nmaximum principle (SMP) for a mean-field optimal control problem under\nmonotonicity conditions.The dynamics of the controlled state process is\ngoverned by a mean-field stochastic differential equation (SDE) whose\ncoefficients depend not only on the control, the controlled state process\nitself but also on its law, and in particular, these coefficients satisfy the\nmonotonicity condition with respect to both the controlled state process and\nits distribution. The associated cost functional is also of mean-field type.\nUnder the assumption of a convex control domain we derive the SMP, which\nprovides a necessary optimality condition for control processes. Under\nadditional convexity assumptions on the Hamiltonian, we further prove that this\nnecessary condition is also a sufficient one. To achieve this, we first address\nthe challenges related to the existence and the uniqueness of solutions for\nmean-field backward stochastic differential equations and mean-field SDEs whose\ncoefficients satisfy monotonicity conditions with respect to both the solution\nas well as its distribution. On the other hand we also construct several\nillustrative examples demonstrating the generality of our results compared to\nexisting literature."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model",
    "start_abstract":"Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Llamafactory: Unified efficient fine-tuning of 100+ language models"
      ],
      "abstract":[
        "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Computation of the Hilbert Series for the Support-Minors Modeling of the\n  MinRank Problem",
        "A glimpse into an effective world",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "Giant Kohn anomaly and chiral phonons in the charge density wave phase\n  of 1H-NbSe$_2$",
        "A study on $T$-equivalent graphs",
        "Criteria for unbiased estimation: applications to noise-agnostic sensing\n  and learnability of quantum channel",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Applied Machine Learning Methods with Long-Short Term Memory Based\n  Recurrent Neural Networks for Multivariate Temperature Prediction",
        "GenMetaLoc: Learning to Learn Environment-Aware Fingerprint Generation\n  for Sample Efficient Wireless Localization",
        "On the structure of some one-generator nilpotent braces",
        "Multi-Instance Partial-Label Learning with Margin Adjustment",
        "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility",
        "Evaluation of Large Language Models via Coupled Token Generation",
        "Hand-Object Contact Detection using Grasp Quality Metrics",
        "Processes on Wasserstein spaces and energy-minimizing particle\n  representations in fractional Sobolev spaces",
        "Phase-matching of high harmonic generation in twisted solids",
        "Bridging conformal field theory and parton approaches to SU(n)_k chiral\n  spin liquids",
        "Research on a Driver's Perceived Risk Prediction Model Considering\n  Traffic Scene Interaction",
        "Latents of latents to delineate pixels: hybrid Matryoshka\n  autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor\n  and low-data regimes",
        "Trend-encoded Probabilistic Multi-order Model: A Non-Machine Learning\n  Approach for Enhanced Stock Market Forecasts",
        "Analyzing Swimming Performance Using Drone Captured Aerial Videos",
        "Spontaneous Magnon Decays from Nonrelativistic Time-Reversal Symmetry\n  Breaking in Altermagnets",
        "On The Origin of Cultural Biases in Language Models: From Pre-training\n  Data to Linguistic Phenomena",
        "Guidelines for Applying RL and MARL in Cybersecurity Applications",
        "Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic\n  Optimization",
        "Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving\n  Test",
        "ZK Secret Santa",
        "Velocity Map Imaging Spectrometer Optimized for Reduction of Background\n  from Scattered UV Light",
        "Prediction-Assisted Online Distributed Deep Learning Workload Scheduling\n  in GPU Clusters"
      ],
      "abstract":[
        "The MinRank problem is a simple linear algebra problem: given matrices with\ncoefficients in a field, find a non trivial linear combination of the matrices\nthat has a small rank. There are several algebraic modeling of the problem. The\nmain ones are: the Kipnis-Shamir modeling, the Minors modeling and the\nSupport-Minors modeling. The Minors modeling has been studied by Faug\\`ere et\nal. in 2010, where the authors provide an analysis of the complexity of\ncomputing a Gr\\\"obner basis of the modeling, through the computation of the\nexact Hilbert Series for a generic instance. For the Support-Minors modeling,\nthe first terms of the Hilbert Series are given by Bardet et al. in 2020 based\non an heuristic and experimental work. In this work, we provide a formula and a\nproof for the complete Hilbert Series of the Support Minors modeling for\ngeneric instances. This is done by adapting well known results on determinantal\nideals to an ideal generated by a particular subset of the set of all minors of\na matrix of variables. We then show that this ideal is generated by a\nparticular subset of the set of all minors of a matrix of variables. We then\nshow that this ideal is generated by standard monomials having a particular\nshape, and derive the Hilbert Series by counting the number of such standard\nmonomials. Following the work done for the Minors Modeling, we then transfer\nthe properties of this particular determinantal ideal to ideals generated by\nthe Support Minors system, by adding generic forms. This work allows to make a\nprecise comparison between the Minors and Support Minors modeling, and a\nprecise estimate of the complexity of solving MinRank instances for the\nparameters of the Mirath signature scheme that is currently at the second round\nof the NIST standardization process for Additional Digital Signature Schemes.",
        "Our contribution aims to celebrate the immeasurable contribution that Tom Kuo\nhas provided to the understanding of the structure of atomic nuclei, and also\nof the infinite nuclear matter, in terms of the fundamental principles\ngoverning the realistic nuclear potential. The authors want to testify Tom\nKuo's heritage and impact on their approach to the study of nuclear systems by\nreviewing some recent findings on the role of the two-body component of\nshell-model effective $\\beta$-decay operators. The focus is spotted on the\nso-called Pauli-blocking effect, that plays a non-negligible role in nuclei\ncharacterized by a large number of valence nucleons.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "Despite extensive investigations, many aspects of charge density waves (CDWs)\nremain elusive, especially the relative roles of electron-phonon coupling and\nFermi surface nesting as the underlying driving mechanisms responsible for the\nemergence of the CDW vector $\\bl Q_{CDW}$. It is puzzling that even though\nelectrons interact strongly with optical phonons in many correlated systems,\nthe actual mode softening is of an acoustic mode. Here we consider monolayer\n1H-NbSe$_2$ as an exemplar system, and through an accurate computation of the\nphonon self-energy, including its off-diagonal components. We provide\ncompelling evidence that the relevant mode is a longitudinal optical phonon\nthat softens by anti-crossing several intervening phonon bands. We also show\nthat $\\bl Q_{CDW}$ is fixed by the convolution of the susceptibility and\nelectron-phonon coupling, and that the softened phonons are circularly\npolarized.",
        "In his article [J. Comb. Theory Ser. B 16 (1974), 168-174], Tutte called two\ngraphs $T$-equivalent (i.e., codichromatic) if they have the same Tutte\npolynomial and showed that graphs $G$ and $G'$ are $T$-equivalent if $G'$ is\nobtained from $G$ by flipping a rotor (i.e., replacing it by its mirror) of\norder at most $5$, where a rotor of order $k$ in $G$ is an induced subgraph $R$\nhaving an automorphism $\\psi$ with a vertex orbit $\\{\\psi^i(u): i\\ge 0\\}$ of\nsize $k$ such that every vertex of $R$ is only adjacent to vertices in $R$\nunless it is in this vertex orbit. In this article, we first show the above\nresult due to Tutte can be extended to a rotor $R$ of order $k\\ge 6$ if the\nsubgraph of $G$ induced by all those edges of $G$ which are not in $R$\nsatisfies certain conditions. Also, we provide a new method for generating\ninfinitely many non-isomorphic $T$-equivalent pairs of graphs.",
        "We establish the necessary and sufficient conditions for unbiased estimation\nin multi-parameter estimation tasks. More specifically, we first consider\nquantum state estimation, where multiple parameters are encoded in a quantum\nstate, and derive two equivalent necessary and sufficient conditions for an\nunbiased estimation: one formulated in terms of the quantum Fisher information\nmatrix (QFIM) and the other based on the derivatives of the encoded state.\nFurthermore, we introduce a generalized quantum Cram\\'er-Rao bound, which\nprovides a fundamental achievable lower bound on the estimation error even when\nthe QFIM is non-invertible. To demonstrate the utility of our framework, we\nconsider phase estimation under unknown Pauli noise. We show that while\nunbiased phase estimation is infeasible with a naive scheme, employing an\nentangled probe with a noiseless ancilla enables unbiased estimation. Next, we\nextend our analysis to quantum channel estimation (equivalently, quantum\nchannel learning), where the goal is to estimate parameters characterizing an\nunknown quantum channel. We establish the necessary and sufficient condition\nfor unbiased estimation of these parameters. Notably, by interpreting unbiased\nestimation as learnability, our result applies to the fundamental learnability\nof parameters in general quantum channels. As a concrete application, we\ninvestigate the learnability of noise affecting non-Clifford gates via cycle\nbenchmarking.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
        "Existing fingerprinting-based localization methods often require extensive\ndata collection and struggle to generalize to new environments. In contrast to\nprevious environment-unknown MetaLoc, we propose GenMetaLoc in this paper,\nwhich first introduces meta-learning to enable the generation of dense\nfingerprint databases from an environment-aware perspective. In the model\naspect, the learning-to-learn mechanism accelerates the fingerprint generation\nprocess by facilitating rapid adaptation to new environments with minimal data.\nAdditionally, we incorporate 3D point cloud data from the first Fresnel zone\nbetween the transmitter and receiver, which describes the obstacles\ndistribution in the environment and serves as a condition to guide the\ndiffusion model in generating more accurate fingerprints. In the data\nprocessing aspect, unlike most studies that focus solely on channel state\ninformation (CSI) amplitude or phase, we present a comprehensive processing\nthat addresses both, correcting errors from WiFi hardware limitations such as\namplitude discrepancies and frequency offsets. For the data collection\nplatform, we develop an uplink wireless localization system that leverages the\nsensing capabilities of existing commercial WiFi devices and mobile phones,\nthus reducing the need for additional deployment costs. Experimental results on\nreal datasets show that our framework outperforms baseline methods.",
        "This article provides a detailed description of some nilpotent left braces\ngenerated by one element.",
        "Multi-instance partial-label learning (MIPL) is an emerging learning\nframework where each training sample is represented as a multi-instance bag\nassociated with a candidate label set. Existing MIPL algorithms often overlook\nthe margins for attention scores and predicted probabilities, leading to\nsuboptimal generalization performance. A critical issue with these algorithms\nis that the highest prediction probability of the classifier may appear on a\nnon-candidate label. In this paper, we propose an algorithm named MIPLMA, i.e.,\nMulti-Instance Partial-Label learning with Margin Adjustment, which adjusts the\nmargins for attention scores and predicted probabilities. We introduce a\nmargin-aware attention mechanism to dynamically adjust the margins for\nattention scores and propose a margin distribution loss to constrain the\nmargins between the predicted probabilities on candidate and non-candidate\nlabel sets. Experimental results demonstrate the superior performance of MIPLMA\nover existing MIPL algorithms, as well as other well-established multi-instance\nlearning algorithms and partial-label learning algorithms.",
        "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems.",
        "State of the art large language models rely on randomization to respond to a\nprompt. As an immediate consequence, a model may respond differently to the\nsame prompt if asked multiple times. In this work, we argue that the evaluation\nand ranking of large language models should control for the randomization\nunderpinning their functioning. Our starting point is the development of a\ncausal model for coupled autoregressive generation, which allows different\nlarge language models to sample responses with the same source of randomness.\nBuilding upon our causal model, we first show that, on evaluations based on\nbenchmark datasets, coupled autoregressive generation leads to the same\nconclusions as vanilla autoregressive generation but using provably fewer\nsamples. However, we further show that, on evaluations based on (human)\npairwise comparisons, coupled and vanilla autoregressive generation can\nsurprisingly lead to different rankings when comparing more than two models,\neven with an infinite amount of samples. This suggests that the apparent\nadvantage of a model over others in existing evaluation protocols may not be\ngenuine but rather confounded by the randomness inherent to the generation\nprocess. To illustrate and complement our theoretical results, we conduct\nexperiments with several large language models from the Llama family. We find\nthat, across multiple knowledge areas from the popular MMLU benchmark dataset,\ncoupled autoregressive generation requires up to 40% fewer samples to reach the\nsame conclusions as vanilla autoregressive generation. Further, using data from\nthe LMSYS Chatbot Arena platform, we find that the win-rates derived from\npairwise comparisons by a strong large language model to prompts differ under\ncoupled and vanilla autoregressive generation.",
        "We propose a novel hand-object contact detection system based on grasp\nquality metrics extracted from object and hand poses, and evaluated its\nperformance using the DexYCB dataset. Our evaluation demonstrated the system's\nhigh accuracy (approaching 90%). Future work will focus on a real-time\nimplementation using vision-based estimation, and integrating it to a\nrobot-to-human handover system.",
        "Given a probability-measure-valued process $(\\mu_t)$, we aim to find, among\nall path-continuous stochastic processes whose one-dimensional time marginals\ncoincide almost surely with $(\\mu_t)$ (if there is any), a process that\nminimizes a given energy in expectation. Building on our recent study\n(arXiv:2502.12068), where the minimization of fractional Sobolev energy was\ninvestigated for deterministic paths on Wasserstein spaces, we now extend the\nresults to the stochastic setting to address some applications that originally\nmotivated our study. Two applications are given. We construct minimizing\nparticle representations for processes on Wasserstein spaces on $\\mathbb{R}$\nwith H\\\"{o}lder regularity, using optimal transportation. We prove the\nexistence of minimizing particle representations for solutions to stochastic\nFokker--Planck--Kolmogorov equations on $\\mathbb{R}^\\mathrm{d}$ satisfying an\nintegrability condition, using the stochastic superposition principle of\nLacker--Shkolnikov--Zhang (J. Eur. Math. Soc. 25, 3229--3288 (2023)).",
        "High harmonic generation (HHG) in solids could enable attosecond and\nultraviolet light sources with high compactness, great controllability and rich\nfunctions. However, the HHG process is accompanied by a quite large wavevector\nmismatch that is uncompensated by any traditional phase-matching method,\ndirectly limiting its energy conversion efficiency. Here, we propose an\neffective strategy for phase-matching of HHG with arbitrary harmonic orders in\nsolids. Two flakes of solids with an interlayer twist induce a nonlinear\noptical phase that depends on the crystal symmetry, twist angle and harmonic\norder, which can be accurately designed to compensate for the phase mismatch in\nHHG. Guided by the twist-phase-matching theory, we achieved a record-high\nconversion efficiency of $~1.5\\times10^{-5}$ for the fifth HHG in twisted\nhexagonal boron nitride crystals with a total thickness of only 1 ${\\mu}m$. Our\nwork establishes a foundation for developing ultrashort-wavelength and\nultrafast-pulse laser sources in compact solid-state tabletop systems for\nfundamental and applied sciences.",
        "We employ the SU(n)_k Wess-Zumino-Witten (WZW) model in conformal field\ntheory to construct lattice wave functions in both one and two dimensions. It\nis unveiled that these wave functions can be reinterpreted as parton states,\nwhich enables efficient conversion to matrix product states such that many\nphysical properties can be evaluated directly. In one dimension, these wave\nfunctions describe critical spin chains whose universality classes are in\none-to-one correspondence with the WZW models used in the construction. In two\ndimensions, our constructions yield model wave functions for chiral spin\nliquids, and we show how to find all topological sectors of them in a\nsystematic way. Using the null vectors of Kac-Moody algebras, parent\nHamiltonians of the SU(3)_k series are derived. The SU(3)_k chiral spin liquids\nare lattice analogs of non-Abelian spin-singlet fractional quantum Hall states,\nand the k = 2 member hosts Fibonacci anyons.",
        "In the field of conditional autonomous driving technology, driver perceived\nrisk prediction plays a crucial role in reducing traffic risks and ensuring\npassenger safety. This study introduces an innovative perceived risk prediction\nmodel for human-machine interaction in intelligent driving systems. The model\naims to enhance prediction accuracy and, thereby, ensure passenger safety.\nThrough a comprehensive analysis of risk impact mechanisms, we identify three\nkey categories of factors, both subjective and objective, influencing perceived\nrisk: driver's personal characteristics, ego-vehicle motion, and surrounding\nenvironment characteristics. We then propose a deep-learning-based risk\nprediction network that uses the first two categories of factors as inputs. The\nnetwork captures the interactive relationships among traffic participants in\ndynamic driving scenarios. Additionally, we design a personalized modeling\nstrategy that incorporates driver-specific traits to improve prediction\naccuracy. To ensure high-quality training data, we conducted a rigorous video\nrating experiment. Experimental results show that the proposed network achieves\na 10.0% performance improvement over state-of-the-art methods. These findings\nsuggest that the proposed network has significant potential to enhance the\nsafety of conditional autonomous driving systems.",
        "Medical images are often high-resolution and lose important detail if\ndownsampled, making pixel-level methods such as semantic segmentation much less\nefficient if performed on a low-dimensional image. We propose a low-rank\nMatryoshka projection and a hybrid segmenting architecture that preserves\nimportant information while retaining sufficient pixel geometry for pixel-level\ntasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the\nhierarchical encoding of the Matryoshka Autoencoder with the spatial\nreconstruction capabilities of a U-Net decoder, leveraging multi-scale feature\nextraction and skip connections to enhance accuracy and generalisation. We\napply it to the problem of segmenting the left ventricle (LV) in\nechocardiographic images using the Stanford EchoNet-D dataset, including 1,000\nstandardised video-mask pairs of cardiac ultrasound videos resized to 112x112\npixels. The MatAE-UNet model achieves a Mean IoU of 77.68\\%, Mean Pixel\nAccuracy of 97.46\\%, and Dice Coefficient of 86.91\\%, outperforming the\nbaseline U-Net, which attains a Mean IoU of 74.70\\%, Mean Pixel Accuracy of\n97.31\\%, and Dice Coefficient of 85.20\\%. The results highlight the potential\nof using the U-Net in the recursive Matroshka latent space for imaging problems\nwith low-contrast such as echocardiographic analysis.",
        "In recent years, the dominance of machine learning in stock market\nforecasting has been evident. While these models have shown decreasing\nprediction errors, their robustness across different datasets has been a\nconcern. A successful stock market prediction model minimizes prediction errors\nand showcases robustness across various data sets, indicating superior\nforecasting performance. This study introduces a novel multiple lag order\nprobabilistic model based on trend encoding (TeMoP) that enhances stock market\npredictions through a probabilistic approach. Results across different stock\nindexes from nine countries demonstrate that the TeMoP outperforms the\nstate-of-the-art machine learning models in predicting accuracy and\nstabilization.",
        "Monitoring swimmer performance is crucial for improving training and\nenhancing athletic techniques. Traditional methods for tracking swimmers, such\nas above-water and underwater cameras, face limitations due to the need for\nmultiple cameras and obstructions from water splashes. This paper presents a\nnovel approach for tracking swimmers using a moving UAV. The proposed system\nemploys a UAV equipped with a high-resolution camera to capture aerial footage\nof the swimmers. The footage is then processed using computer vision algorithms\nto extract the swimmers' positions and movements. This approach offers several\nadvantages, including single camera use and comprehensive coverage. The\nsystem's accuracy is evaluated with both training and in competition videos.\nThe results demonstrate the system's ability to accurately track swimmers'\nmovements, limb angles, stroke duration and velocity with the maximum error of\n0.3 seconds and 0.35~m\/s for stroke duration and velocity, respectively.",
        "Quasiparticles are central to condensed matter physics, but their stability\ncan be undermined by quantum many-body interactions. Magnons, quasiparticles in\nquantum magnets, are particularly intriguing because their properties are\ngoverned by both real and spin space. While crystal symmetries may be low, spin\ninteractions often remain approximately isotropic, limiting spontaneous magnon\ndecay. Textbook wisdom holds that collinear Heisenberg magnets follow a\ndichotomy: ferromagnets host stable magnons, while antiferromagnetic magnons\nmay decay depending on dispersion curvature. Up to now, relativistic spin-orbit\ncoupling and noncollinear order that connect spin space to real space, were\nshown to introduce more complex magnon instability mechanisms. Here, we show\nthat even in nonrelativistic isotropic collinear systems, this conventional\ndichotomy is disrupted in altermagnets. Altermagnets, a newly identified class\nof collinear magnets, exhibit compensated spin order with nonrelativistic\ntime-reversal symmetry breaking and even-parity band splitting. Using kinematic\nanalysis, nonlinear spin-wave theory, and quantum simulations, we reveal that\neven weak band splitting opens a decay phase space, driving quasiparticle\nbreakdown. Additionally, d-wave altermagnets form a rare ``island of\nstability'' at the Brillouin zone center. Our findings establish a\nquasiparticle stability trichotomy in collinear Heisenberg magnets and position\naltermagnets as a promising platform for unconventional spin dynamics.",
        "Language Models (LMs) have been shown to exhibit a strong preference towards\nentities associated with Western culture when operating in non-Western\nlanguages. In this paper, we aim to uncover the origins of entity-related\ncultural biases in LMs by analyzing several contributing factors, including the\nrepresentation of entities in pre-training data and the impact of variations in\nlinguistic phenomena across languages. We introduce CAMeL-2, a parallel\nArabic-English benchmark of 58,086 entities associated with Arab and Western\ncultures and 367 masked natural contexts for entities. Our evaluations using\nCAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in\nEnglish compared to Arabic. We find that LMs struggle in Arabic with entities\nthat appear at high frequencies in pre-training, where entities can hold\nmultiple word senses. This also extends to entities that exhibit high lexical\noverlap with languages that are not Arabic but use the Arabic script. Further,\nwe show how frequency-based tokenization leads to this issue in LMs, which gets\nworse with larger Arabic vocabularies. We will make CAMeL-2 available at:\nhttps:\/\/github.com\/tareknaous\/camel2",
        "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
        "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1\/(2p+1)}\/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho\/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios.",
        "Autonomous driving (AD) testing constitutes a critical methodology for\nassessing performance benchmarks prior to product deployment. The creation of\nsegmented scenarios within a simulated environment is acknowledged as a robust\nand effective strategy; however, the process of tailoring these scenarios often\nnecessitates laborious and time-consuming manual efforts, thereby hindering the\ndevelopment and implementation of AD technologies. In response to this\nchallenge, we introduce Text2Scenario, a framework that leverages a Large\nLanguage Model (LLM) to autonomously generate simulation test scenarios that\nclosely align with user specifications, derived from their natural language\ninputs. Specifically, an LLM, equipped with a meticulously engineered input\nprompt scheme functions as a text parser for test scenario descriptions,\nextracting from a hierarchically organized scenario repository the components\nthat most accurately reflect the user's preferences. Subsequently, by\nexploiting the precedence of scenario components, the process involves\nsequentially matching and linking scenario representations within a Domain\nSpecific Language corpus, ultimately fabricating executable test scenarios. The\nexperimental results demonstrate that such prompt engineering can meticulously\nextract the nuanced details of scenario elements embedded within various\ndescriptive formats, with the majority of generated scenarios aligning closely\nwith the user's initial expectations, allowing for the efficient and precise\nevaluation of diverse AD stacks void of the labor-intensive need for manual\nscenario configuration. Project page:\nhttps:\/\/caixxuan.github.io\/Text2Scenario.GitHub.io.",
        "This paper proposes a three-step Secret Santa algorithm with setup that\nleverages Zero Knowledge Proofs (ZKP) to set up gift sender\/receiver relations\nwhile maintaining the sender's confidentiality. The algorithm maintains a\npermutational derangement and does not require a central authority to perform\nsuccessfully. The described approach can be implemented in Solidity provided\nthe integration with a transaction relayer.",
        "Velocity map imaging spectroscopy is a powerful technique for detecting the\nmomentum distribution of photoelectrons resulting from an ionization experiment\non atoms or molecules. However, when used with ultraviolet light sources,\nscattered photons can lead to the emission of photoelectrons from the\nspectrometer's electrodes, giving rise to severe noise disturbing the desired\nsignal. We present a velocity map imaging spectrometer optimized to reduce\nunwanted background signals. The primary modifications to the conventional\ndesign include spectrometer electrode geometries with small cross section\nexposed to the scattered photons, with blocked pathways for photoelectrons from\nthe electrodes to the detector, as well as the incorporation of optical\nbaffles. Compared to a conventional design optimized solely on the\nspectrometer's photoelectron momentum resolution, we have achieved the\nelimination of 99.9 \\% of the background noise without substantial compromise\nto the resolution. Note that most of the improvements were achieved without the\nnecessity of high-grade windows, reducing the sensitivity to window degradation\nby UV light. We give general guidelines on efficiently coping with the\nlong-standing experimental problem of electron background originating from\nscattered light by considering it already in the design stage of a new\nspectrometer.",
        "The recent explosive growth of deep learning (DL) models has necessitated a\ncompelling need for efficient job scheduling for distributed deep learning\ntraining with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes\nan adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling\nalgorithm, a novel prediction-assisted online scheduling approach designed to\nmitigate the challenges associated with DL cluster scheduling. By modeling each\njob as a graph corresponding to heterogeneous Deep Neural Network (DNN) models\nand their associated distributed training configurations, A-SRPT strategically\nassigns jobs to the available GPUs, thereby minimizing inter-server\ncommunication overhead. Observing that most DDLwMP jobs recur, A-SRPT\nincorporates a random forest regression model to predict training iterations.\nCrucially, A-SRPT maps the complex scheduling problem into a single-machine\ninstance, which is addressed optimally by a preemptive\n\"shortest-remaining-processing-time-first\" strategy. This optimized solution\nserves as a guide for actual job scheduling within the GPU clusters, leading to\na theoretically provable competitive scheduling efficiency. We conduct\nextensive real-world testbed and simulation experiments to verify our proposed\nalgorithms."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"2016 Alzheimer's disease facts and figures",
    "start_abstract":"This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
      ],
      "abstract":[
        "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Dirichlet's Lemma in Number Fields",
        "Forecasting Monthly Residential Natural Gas Demand Using\n  Just-In-Time-Learning Modeling",
        "Non-Hermitian Aharonov-Bohm Cage in Bosonic Bogoliubov-de Gennes Systems",
        "Einstein multiply warped products and generalized Kasner manifolds with\n  multidimensional base",
        "Probing Topological Anderson Transition in Quasiperiodic Photonic\n  Lattices via Chiral Displacement and Wavelength Tuning",
        "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "Subwavelength plasmonic antennas based on asymmetric\n  split-ring-resonators for high near-field enhancements",
        "Poisoning Bayesian Inference via Data Deletion and Replication",
        "Parking Space Detection in the City of Granada",
        "COFO: COdeFOrces dataset for Program Classification, Recognition and\n  Tagging",
        "Estimating treatment effects with competing intercurrent events in\n  randomized controlled trials",
        "$\\eta$, $\\eta^\\prime$ mesons from lattice QCD in fully physical\n  conditions",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "An empirical formulation of accelerated molecular dynamics for\n  simulating and predicting microstructure evolution in materials",
        "Dense $2$-connected planar graphs and the planar Tur\\'{a}n number of\n  $2C_k$",
        "Reducing T Gates with Unitary Synthesis",
        "Discrete Lyapunov functional for cyclic systems of differential\n  equations with time-variable or state-dependent delay",
        "Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset",
        "A Design of Denser-Graph-Frequency Graph Fourier Frames for Graph Signal\n  Analysis",
        "Practical Introduction to FEM with GMSH: A MATLAB\/Octave Perspective",
        "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions",
        "Compactness of commutators of rough singular integrals",
        "Visual WetlandBirds Dataset: Bird Species Identification and Behavior\n  Recognition in Videos",
        "Acoustic waves interacting with non--locally reacting surfaces in a\n  Lagrangian framework",
        "Overview of the Amphion Toolkit (v0.2)",
        "Flora: Efficient Cloud Resource Selection for Big Data Processing via\n  Job Classification",
        "High-intensity wave vortices around subwavelength holes: from ocean\n  tides to nanooptics",
        "Impact of phonon lifetimes on the single-photon indistinguishability in\n  quantum emitters based on 2D materials"
      ],
      "abstract":[
        "Dirichlet's Lemma states that every primitive quadratic Dirichlet character\n$\\chi$ can be written in the form $\\chi(n) = (\\frac{\\Delta}n)$ for a suitable\nquadratic discriminant $\\Delta$. In this article we define a group, the\nseparant class group, that measures the extent to which Dirichlet's Lemma fails\nin general number fields $F$. As an application we will show that over fields\nwith trivial separant class groups, genus theory of quadratic extensions can be\nmade as explicit as over the rationals.",
        "Natural gas (NG) is relatively a clean source of energy, particularly\ncompared to fossil fuels, and worldwide consumption of NG has been increasing\nalmost linearly in the last two decades. A similar trend can also be seen in\nTurkey, while another similarity is the high dependence on imports for the\ncontinuous NG supply. It is crucial to accurately forecast future NG demand\n(NGD) in Turkey, especially, for import contracts; in this respect, forecasts\nof monthly NGD for the following year are of utmost importance. In the current\nstudy, the historical monthly NG consumption data between 2014 and 2024\nprovided by SOCAR, the local residential NG distribution company for two cities\nin Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD\nforecasts for a period of one year and nine months using various time series\nmodels, including SARIMA and ETS models, and a novel proposed machine learning\nmethod. The proposed method, named Just-in-Time-Learning-Gaussian Process\nRegression (JITL-GPR), uses a novel feature representation for the past NG\ndemand values; instead of using past demand values as column-wise separate\nfeatures, they are placed on a two-dimensional (2-D) grid of year-month values.\nFor each test point, a kernel function, tailored for the NGD predictions, is\nused in GPR to predict the query point. Since a model is constructed separately\nfor each test point, the proposed method is, indeed, an example of JITL. The\nJITL-GPR method is easy to use and optimize, and offers a reduction in forecast\nerrors compared to traditional time series methods and a state-of-the-art\ncombination model; therefore, it is a promising tool for NGD forecasting in\nsimilar settings.",
        "The non-Hermitian Aharonov-Bohm (AB) cage is a unique localization phenomenon\nthat confines all possible excitations. This confinement leads to fully flat\nspectra in momentum space, which are typically accompanied with the degeneracy\nwith various types. Classifying the degeneracy type is crucial for studying the\ndynamical properties of the non-Hermitian AB cage, but the methods for such\nclassification and their physical connections remain not very clear. Here, we\nconstruct a non-Hermitian AB cage in a bosonic Bogoliubov-de Gennes (BdG)\nsystem with various types of degenerate flat bands (DFBs). Using the transfer\nmatrix, we demonstrate the localization mechanism for the formation of AB cage\nand derive the minimal polynomial in mathematics for classifying the degeneracy\ntypes of DFBs, thus providing comprehensive understanding of the correspondence\namong the degeneracy type of DFBs, the minimal polynomial, and the transfer\nmatrix. With such correspondence, we propose a scheme to realize highly\ndegenerate flat bands.",
        "The purpose of this paper is to provide conditions for the existence or non\nexistence of non trivial Einstein multiply warped products, specially of\ngeneralised Kasner type; as well as to show estimates of the Einstein parameter\nthat condition the existence of such metrics.",
        "The interplay of topology and disorder in quantum dynamics has recently\nattracted significant attention across diverse platforms, including solid-state\ndevices, ultracold atoms, and photonic systems. Here, we report on a\ntopological Anderson transition caused by quasiperiodic intra-cell coupling\ndisorder in photonic Su-Schrieffer-Heeger lattices. As the quasiperiodic\nstrength is varied, the system exhibits a reentrant transition from a trivial\nphase to a topological phase and back to a trivial phase, accompanied by the\nclosing and reopening of the band gap around zero energy. Unlike the\ntraditional detection of photonic topological edge modes, we measure the mean\nchiral displacement from the transport of light in the bulk of the lattices. In\nour photonic lattices with a fixed length, the propagation dynamics is\nretrieved by varying the wavelength of light, which tunes the inter-waveguide\ncouplings.",
        "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.",
        "As for plasmonic antenna structures that generate localized near-field\nenhancement, the most effective current implementations are based on electric\ndipole resonance modes, but this approach also imposes limitations on their\nfurther optimization. Here we introduce an ASRR structure whose ASR mode\nenables differential charge distribution across both sides of the split.\nThrough asymmetric regulation, charges at one end can become highly localized,\nthereby achieving efficient near-field enhancement. The formation of this\nstructure was initially driven by a hybrid computational framework integrating\nevolutionary optimization with residual neural networks, and subsequently\nsimplified into an ASRR prototype using the Occam's Razor principle. The ASRR\ndimer structure can achieve an electric field intensity enhancement over 6.5\ntimes larger than a traditional nanorod dimer, while maintaining a compact size\n(<1\/3 the working wavelength). The ASRR configuration also demonstrates\nsuperior Purcell factor and fluorescence enhancement. These results can find\napplications in surface-enhanced spectroscopy, nonlinear optics, and quantum\nlight-matter interactions.",
        "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
        "This paper addresses the challenge of parking space detection in urban areas,\nfocusing on the city of Granada. Utilizing aerial imagery, we develop and apply\nsemantic segmentation techniques to accurately identify parked cars, moving\ncars and roads. A significant aspect of our research is the creation of a\nproprietary dataset specific to Granada, which is instrumental in training our\nneural network model. We employ Fully Convolutional Networks, Pyramid Networks\nand Dilated Convolutions, demonstrating their effectiveness in urban semantic\nsegmentation. Our approach involves comparative analysis and optimization of\nvarious models, including Dynamic U-Net, PSPNet and DeepLabV3+, tailored for\nthe segmentation of aerial images. The study includes a thorough\nexperimentation phase, using datasets such as UDD5 and UAVid, alongside our\ncustom Granada dataset. We evaluate our models using metrics like Foreground\nAccuracy, Dice Coefficient and Jaccard Index. Our results indicate that\nDeepLabV3+ offers the most promising performance. We conclude with future\ndirections, emphasizing the need for a dedicated neural network for parked car\ndetection and the potential for application in other urban environments. This\nwork contributes to the fields of urban planning and traffic management,\nproviding insights into efficient utilization of parking spaces through\nadvanced image processing techniques.",
        "In recent years, a lot of technological advances in computer science have\naided software programmers to create innovative and real-time user-friendly\nsoftware. With the creation of the software and the urging interest of people\nto learn to write software, there is a large collection of source codes that\ncan be found on the web, also known as Big Code, which can be used as a source\nof data for driving the machine learning applications tending to solve certain\nsoftware engineering problems. In this paper, we present COFO, a dataset\nconsisting of 809 classes\/problems with a total of 369K source codes written in\nC, C++, Java, and Python programming languages, along with other metadata such\nas code tags, problem specification, and input-output specifications. COFO has\nbeen scraped from the openly available Codeforces website using a\nselenium-beautifulsoup-python based scraper. We envision that this dataset can\nbe useful for solving machine learning-based problems like program\nclassification\/recognition, tagging, predicting program properties, and code\ncomprehension.",
        "The analysis of randomized controlled trials is often complicated by\nintercurrent events--events that occur after treatment initiation and may\nimpact outcome assessment. These events may lead to patients discontinuing\ntheir assigned treatment or dropping out of the trial entirely. In an analysis\nof data from two recent immunology trials, we categorize intercurrent events\ninto two broad types: those unrelated to treatment (e.g., withdrawal from the\nstudy due to external factors like pandemics or relocation) and those related\nto treatment (e.g., adverse events or lack of efficacy). We adopt distinct\nstrategies to handle each type, aiming to target a clinically more relevant\nestimand. For treatment-related intercurrent events, they often meaningfully\ndescribe the patient's outcome, we employ a composite variable strategy, where\nwe attribute an outcome value that reflects the lack of treatment success. For\ntreatment-unrelated intercurrent events, we adopt a hypothetical strategy that\nassumes these event times are conditionally independent of the outcome, given\ntreatment and covariates, and envisions a scenario in which the intercurrent\nevents do not occur. We establish the nonparametric identification and\nsemiparametric estimation theory for the causal estimand and introduce doubly\nrobust estimators. We illustrate our methods through the re-analysis of two\nrandomized trials on baricitinib for Systemic Lupus Erythematosus. We classify\nintercurrent events, apply four estimators, and compare our approach with\ncommon ad-hoc methods, highlighting the robustness and practical implications\nof our framework.",
        "We determine masses and mixing parameters of the $\\eta$ and $M_{\\eta^\\prime}$\nmeson in lattice QCD. The calculations are carried out on a set of 13 ETMC\ngauge ensembles with $N_f=2+1+1$ (maximally) twisted-mass Clover-improved\nquarks. These ensemble cover four values of the lattice spacing\n$a=0.057\\mathrm{fm},...,0.092\\mathrm{fm}$ and pion masses from\n$140\\mathrm{MeV}$ to $360\\mathrm{MeV}$, including three ensembles at physical\nquark masses and six ensembles with $M_\\pi<200\\mathrm{MeV}$. The strange-quark\ncontribution is treated in a mixed-action approach using Osterwalder-Seiler\nfermions to avoid complications due to flavor mixing in the heavy quark sector\nand to enable the use of the one-end trick in the computation of strange\nquark-disconnected diagrams. With the strange-quark mass tuned to its physical\nvalue and several ensembles having close-to-physical light-quark mass,\nuncertainties related to the chiral extrapolations are reduced significantly\ncompared to earlier studies. Physical results are computed with fully\ncontrolled systematics from a combined chiral, continuum and infinite-volume\nextrapolation, and a full error budget is obtained from model averages over of\nvarious fit ans\\\"atze and data cuts. Our results for the masses are given by\n$M_\\eta=551(16)\\mathrm{MeV}$ and $M_{\\eta^\\prime}=972(20)\\mathrm{MeV}$,\nrespectively, where statistical and systematic errors have been added in\nquadrature. For the mixing angle and decay-constant parameters the\nFeldmann-Kroll-Stech scheme is employed to compute them from pseudoscalar\nmatrix elements in the quark-flavor basis. For the mixing angle we obtain\n$\\phi^\\mathrm{phys}=39.3(2.0)^\\circ$ and our results for the decay-constant\nparameters are given by $f_l^\\mathrm{phys}=138.6(4.4)\\mathrm{MeV}$ and\n$f_s^\\mathrm{phys}=170.7(3.3)\\mathrm{MeV}$.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "Despite its widespread use in materials science, conventional molecular\ndynamics simulations are severely constrained by timescale limitations. To\naddress this shortcoming, we propose an empirical formulation of accelerated\nmolecular dynamics method, adapted from a collective-variable-based extended\nsystem dynamics framework. While this framework is originally developed for\nefficient free energy sampling and reaction pathway determination of specific\nrare events in condensed matter, we have modified it to enable accelerated\nmolecular dynamics simulation and prediction of microstructure evolution of\nmaterials across a broad range of scenarios. In essence, the nearest neighbor\noff-centering absolute displacement (NNOAD), which quantifies the deviation of\nan atom from the geometric center of its nearest neighbors in materials, is\nintroduced. We propose that the collection of NNOADs of all atoms can serve as\na generalized reaction coordinate for various structural transitions in\nmaterials. The NNOAD of each atom, represented by its three components, is\ncoupled with three additional dynamic variables assigned to the atom. Time\nevolution of the additional dynamic variables follows Langevin equation, while\nNos\\'e-Hoover dynamics is employed to thermostat the system. Through careful\nanalysis and benchmark simulations, we established appropriate parameter ranges\nfor the equations in our method. Application of this method to several test\ncases demonstrates its effectiveness and consistency in accelerating molecular\ndynamics simulations and predicting various microstructure evolutions of\nmaterials over much longer timescale. We also provide a preliminary theoretical\nanalysis and qualitative justification of the method, offering insights into\nits underlying principles.",
        "Shi, Walsh and Yu demonstrated that any dense circuit graph contains a large\nnear-triangulation. We extend the result to $2$-connected plane graphs, thereby\naddressing a question posed by them. Using the result, we prove that the planar\nTu\\'{a}n number of $2C_k$ is $\\left[3-\\Theta(k^{\\log_23})^{-1}\\right]n$ when\n$k\\geq 5$.",
        "Quantum error correction is essential for achieving practical quantum\ncomputing but has a significant computational overhead. Among fault-tolerant\n(FT) gate operations, non-Clifford gates, such as $T$, are particularly\nexpensive due to their reliance on magic state distillation. These costly $T$\ngates appear frequently in FT circuits as many quantum algorithms require\narbitrary single-qubit rotations, such as $R_x$ and $R_z$ gates, which must be\ndecomposed into a sequence of $T$ and Clifford gates. In many quantum circuits,\n$R_x$ and $R_z$ gates can be fused to form a single $U3$ unitary. However,\nexisting synthesis methods, such as gridsynth, rely on indirect decompositions,\nrequiring separate $R_z$ decompositions that result in a threefold increase in\n$T$ count.\n  This work presents a novel FT synthesis algorithm that directly synthesizes\narbitrary single-qubit unitaries, avoiding the overhead of separate $R_z$\ndecompositions. By leveraging tensor network-based search, our approach enables\nnative $U3$ synthesis, reducing the $T$ count, Clifford gate count, and\napproximation error. Compared to gridsynth-based circuit synthesis, for 187\nrepresentative benchmarks, our design reduces the $T$ count by up to\n$3.5\\times$, and Clifford gates by $7\\times$, resulting in up to $4\\times$\nimprovement in overall circuit infidelity.",
        "We consider nonautonomous cyclic systems of delay differential equations with\nvariable delay. Under suitable feedback assumptions, we define an (integer\nvalued) Lyapunov functional related to the number of sign changes of the\ncoordinate functions of solutions. We prove that this functional possesses\nproperties analogous to those established by Mallet-Paret and Sell for the\nconstant delay case and by Krisztin and Arino for the scalar case. We also\napply the results to equations with state-dependent delays.",
        "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization.",
        "This paper introduces a design method for densergraph-frequency graph Fourier\nframes (DGFFs) to enhance graph signal processing and analysis. The graph\nFourier transform (GFT) enables us to analyze graph signals in the graph\nspectral domain and facilitates various graph signal processing tasks, such as\nfiltering, sampling and reconstruction, denoising, and so on. However, the\nconventional GFT faces two significant limitations. First, unlike the discrete\nFourier transform and its variants (such as discrete cosine transforms), the\ngraph frequencies of the derived graph Fourier basis (GFB) from a given graph\ntend to be unevenly distributed or localized, which leads to biased spectral\nanalysis. Second, the GFB used in GFT does not provide an efficient sparse\nrepresentation of graph signals compared to overcomplete systems like frames.\nTo overcome these challenges, we propose adding oscillating vectors with\nintermediate graph frequencies between the original vectors in the GFB for both\nundirected and directed graphs, constructing GFFs with densergraph frequencies.\nThe resulting DGFFs are expected to enable more accurate graph signal analysis.\nFurthermore, we propose a graph filtering method based on the DGFFs. In\nexperiments, we apply the DGFFs to practical applications such as graph signal\nrecovery, demonstrating superior performance compared to existing GFBs.",
        "The Finite Element Method (FEM) is a powerful computational tool for solving\npartial differential equations (PDEs). Although commercial and open-source FEM\nsoftware packages are widely available, an independent implementation of FEM\nprovides significant educational value, provides a deeper understanding of the\nmethod, and enables the development of custom solutions tailored to specialized\napplications or integration with other solvers. This work introduces a 3D\n$\\mathbb{P}_m$-element FEM implementation in MATLAB\/Octave that is designed to\nbalance educational clarity with computational efficiency. A key feature is its\nintegration with GMSH, an open-source 3D mesh generator with CAD capabilities\nthat streamlines mesh generation for complex geometries. By leveraging GMSH\ndata structures, we provide a seamless connection between geometric modeling\nand numerical simulation. The implementation focuses on solving the general\nconvection-diffusion-advection equation and serves as a flexible foundation for\naddressing advanced problems, including elasticity, mixed formulations, and\nintegration with other numerical methods.",
        "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions.",
        "We study the two-weighted off-diagonal compactness of commutators of rough\nsingular integral operators $T_\\Omega$ that are associated with a kernel\n$\\Omega\\in L^q(\\mathbb{S}^{d-1})$. We establish a characterisation of\ncompactness of the commutator $[b,T_\\Omega]$ in terms of the function $b$\nbelonging to a suitable space of functions with vanishing mean oscillation. Our\nresults expand upon the previous compactness characterisations for\nCalder\\'on-Zygmund operators. Additionally, we prove a matrix-weighted\ncompactness result for $[b,T_\\Omega]$ by applying the so-called matrix-weighted\nKolmogorov-Riesz theorem.",
        "The current biodiversity loss crisis makes animal monitoring a relevant field\nof study. In light of this, data collected through monitoring can provide\nessential insights, and information for decision-making aimed at preserving\nglobal biodiversity. Despite the importance of such data, there is a notable\nscarcity of datasets featuring videos of birds, and none of the existing\ndatasets offer detailed annotations of bird behaviors in video format. In\nresponse to this gap, our study introduces the first fine-grained video dataset\nspecifically designed for bird behavior detection and species classification.\nThis dataset addresses the need for comprehensive bird video datasets and\nprovides detailed data on bird actions, facilitating the development of deep\nlearning models to recognize these, similar to the advancements made in human\naction recognition. The proposed dataset comprises 178 videos recorded in\nSpanish wetlands, capturing 13 different bird species performing 7 distinct\nbehavior classes. In addition, we also present baseline results using state of\nthe art models on two tasks: bird behavior recognition and species\nclassification.",
        "The paper deals with a family of evolution problems arising in the physical\nmodeling of small amplitude acoustic phenomena occurring in a fluid, bounded by\na surface of extended reaction. They are all derived in a Lagrangian framework.\n  We study well-posedness of these problems, their mutual relations, and their\nrelations with other evolution problems modeling the same physical phenomena.\nThey are those introduced in an Eulerian framework and those which deal with\nthe (standard in Theoretical Acoustics) velocity potential. The latter reduce\nto the well--known wave equation with acoustic boundary conditions.\n  Finally, we prove that all problems are asymptotically stable provided the\nsystem is linearly damped.",
        "Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ndesigned to lower the entry barrier for junior researchers and engineers in\nthese fields. It provides a versatile framework that supports a variety of\ngeneration tasks and models. In this report, we introduce Amphion v0.2, the\nsecond major release developed in 2024. This release features a 100K-hour\nopen-source multilingual dataset, a robust data preparation pipeline, and novel\nmodels for tasks such as text-to-speech, audio coding, and voice conversion.\nFurthermore, the report includes multiple tutorials that guide users through\nthe functionalities and usage of the newly released models.",
        "Distributed dataflow systems like Spark and Flink enable data-parallel\nprocessing of large datasets on clusters of cloud resources. Yet, selecting\nappropriate computational resources for dataflow jobs is often challenging. For\nefficient execution, individual resource allocations, such as memory and CPU\ncores, must meet the specific resource demands of the job. Meanwhile, the\nchoices of cloud configurations are often plentiful, especially in public\nclouds, and the current cost of the available resource options can fluctuate.\n  Addressing this challenge, we present Flora, a low-overhead approach to\ncost-optimizing cloud cluster configurations for big data processing. Flora\nlets users categorize jobs according to their data access patterns and derives\nsuitable cluster resource configurations from executions of test jobs of the\nsame category, considering current resource costs. In our evaluation on a new\ndataset comprising 180 Spark job executions on Google Cloud, Flora's cluster\nresource selections exhibit an average deviation below 6% from the most\ncost-optimal solution, with a maximum deviation below 24%.",
        "Vortices are ubiquitous in nature; they appear in a variety of phenomena\nranging from galaxy formation in astrophysics to topological defects in quantum\nfluids. In particular, wave vortices have attracted enormous attention and\nfound applications in optics, acoustics, electron microscopy, etc. Such\nvortices carry quantized phase singularities accompanied by zero intensity in\nthe center, and quantum-like orbital angular momentum, with the minimum\nlocalization scale of the wavelength. Here we describe a conceptually novel\ntype of wave vortices, which can appear around arbitrarily small `holes' (i.e.,\nexcluded areas or defects) in a homogeneous 2D plane. Such vortices are\ncharacterized by high intensity and confinement at the edges of the hole and\nhence subwavelength localization of the angular momentum. We demonstrate the\nappearance of such vortices in: (i) optical near fields around metallic\nnanodiscs on a dielectric substrate, (ii) phonon-polariton fields around\nnanoholes in a polaritonic slab, and (iii) ocean tidal waves around islands of\nNew Zealand and Madagascar. We also propose a simple toy model of the\ngeneration of such subwavelength vortices via the interference of a\npoint-dipole source and a plane wave, where the vortex sign is controlled by\nthe mutual phase between these waves. Our findings open avenues for\nsubwavelength vortex\/angular-momentum-based applications in various wave\nfields.",
        "Localized excitons in two-dimensional (2D) materials are considered as\npromising sources of single photons on demand. The photon indistinguishability\nas key figure of merit for quantum information processing is strongly\ninfluenced by the coupling of charge excitations to lattice vibrations of the\nsurrounding semiconductor material. Here, we quantify the impact of\nexciton-acoustic-phonon-interaction and cavity QED effects on photon\nindistinguishability in a Hong-Ou-Mandel setup by solving fully quantum\nmechanical equations for the coupled QD-cavity-phonon system including\nnon-Markovian effects. We find a strong reduction of indistinguishability\ncompared to 3D systems due to increased exciton-phonon coupling efficiency.\nMoreover, we show that the coherence properties of photons are significantly\ninfluenced by the finite phonon lifetime in the surrounding material giving\nrise to pure dephasing. Only if these limitations are overcome, localized\nexcitons in 2D semiconductors can become a new avenue for quantum light\nsources."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)",
    "start_abstract":"<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "2016 Alzheimer's disease facts and figures"
      ],
      "abstract":[
        "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Axion Stabilization in Modular Cosmology",
        "Reconstruction of space-dependence and nonlinearity of a reaction term\n  in a subdiffusion equation",
        "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas",
        "On the spectral gap of negatively curved covers",
        "Fluid Reconfigurable Intelligent Surfaces: Joint On-Off Selection and\n  Beamforming with Discrete Phase Shifts",
        "Forecasting Local Ionospheric Parameters Using Transformers",
        "Fourier dimension of the graph of fractional Brownian motion with $H \\ge\n  1\/2$",
        "A generalization of Zwegers' multivariable $\\mu$-function",
        "Enumeration of lattices of nullity $k$ and containing $r$ comparable\n  reducible elements",
        "Coherent manifolds",
        "Study of event and particle selection effects on elliptic flow\n  background at the isobar experiments based on AMPT model",
        "Progress of the TianQin project",
        "Dimension-free Score Matching and Time Bootstrapping for Diffusion\n  Models",
        "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence",
        "Modulating Optical Properties through Cation Substitution:\n  Composition-Property Relationships in $M^I_3$$M^{III}$P$_3$O$_9$N:Eu$^{2+}$\n  ($M^I$=Na, K; $M^{III}$=Al, Ga, In)",
        "No Galaxy-Scale [CII] Fast Outflow in the z=6.72 Red Quasar HSC\n  J1205$-$0000",
        "A fully conservative discrete velocity Boltzmann solver with parallel\n  adaptive mesh refinement for compressible flows",
        "Twist-enabled Transmissive Metasurface with Co-polarized Geometric Phase",
        "A Klain-Schneider Theorem for Vector-Valued Valuations on Convex\n  Functions",
        "Real-time simulation of jet energy loss and entropy production in\n  high-energy scattering with matter",
        "Optimizing High-Dimensional Oblique Splits",
        "A modified dynamic diffusion finite element method with optimal\n  convergence rate for convection-diffusion-reaction equations",
        "An Unexplained Origin for the Unusual Globular Cluster System in the\n  Ultra-diffuse Galaxy FCC 224",
        "Distributed Observer for Descriptor Linear System: The Luenberger\n  Observer Method",
        "Sharing quantum nonlocality and teleportation over long distance using\n  optical hybrid states",
        "Stabilization of magnetic bubbles in [Ni\/Co]$_{n}$ multilayers on an\n  oxygen-reconstructed Nb(110) surface via an ultra-thin Cu interlayer",
        "Energetics and dynamics of membrane necks in particle wrapping",
        "Neutral but Impactful: Gallium Cluster-Induced Nanopores from\n  Beam-Blanked Gallium Ion Sources",
        "Investigation of Polymer Association Behaviors in Solvents Using a\n  Coarse-Grained Model"
      ],
      "abstract":[
        "The $SL(2,\\mathbb{Z})$ invariant $\\alpha$-attractor models have plateau\npotentials with respect to the inflaton and axion fields. The potential in the\naxion direction is almost exactly flat during inflation, hence, the axion field\nremains nearly massless. In this paper, we develop a generalized class of such\nmodels, where the $SL(2,\\mathbb{Z})$ symmetry is preserved, but the axion\nacquires a large mass and becomes strongly stabilized during inflation, which\neliminates isocurvature perturbations in this scenario. Inflation in such\ntwo-field models occurs as in the single-field $\\alpha$-attractors and leads to\nthe same cosmological predictions.",
        "In this paper we study the simultaneous reconstruction of two coefficients in\na reaction-subdiffusion equation, namely a nonlinearity and a space dependent\nfactor. The fact that these are coupled in a multiplicative matter makes the\nreconstruction particularly challenging. Several situations of overposed data\nare considered: boundary observations over a time interval, interior\nobservations at final time, as well as a combination thereof. We devise fixed\npoint schemes and also describe application of a frozen Newton method. In the\nfinal time data case we prove convergence of the fixed point scheme as well as\nuniqueness of both coefficients. Numerical experiments illustrate performance\nof the reconstruction methods, in particular dependence on the differentiation\norder in the subdiffusion equation.",
        "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees.",
        "Given a negatively curved compact Riemannian surface $X$, we give an explicit\nestimate, valid with high probability as the degree goes to infinity, of the\nfirst non-trivial eigenvalue of the Laplacian on random Riemannian covers of\n$X$. The explicit gap is given in terms of the bottom of the spectrum of the\nuniversal cover of $X$ and the topological entropy of the geodesic flow on X.\nThis result generalizes in variable curvature a result of Magee-Naud-Puder for\nhyperbolic surfaces. We then formulate a conjecture on the optimal spectral gap\nand show that there exists covers with near optimal spectral gaps using a\nresult of Louder-Magee and techniques of strong convergence from random matrix\ntheory.",
        "This letter proposes a fluid reconfigurable intelligent surface (FRIS)\nparadigm, extending the conventional reconfigurable intelligent surface (RIS)\ntechnology to incorporate position reconfigurability of the elements. In our\nmodel, a `fluid' element is realized by a dense matrix of subelements over a\ngiven space and dynamically selecting specific elements for signal modulation\nbased on channel conditions. Specifically, we consider a FRIS-assisted\nsingle-user single-input single-output (SU-SISO) system and formulate an\noptimization problem that can jointly optimize element selection and their\ndiscrete phase shifts to maximize the achievable rate. To address this problem\nefficiently, we propose an iterative algorithm based on the cross-entropy\noptimization (CEO) framework. Simulation results reveal that FRIS achieves\nsignificant performance gains over traditional RIS.",
        "We present a novel method for forecasting key ionospheric parameters using\ntransformer-based neural networks. The model provides accurate forecasts and\nuncertainty quantification of the F2-layer peak plasma frequency (foF2), the\nF2-layer peak density height (hmF2), and total electron content (TEC) for a\ngiven geographic location. It supports a number of exogenous variables,\nincluding F10.7cm solar flux and disturbance storm time (Dst). We demonstrate\nhow transformers can be trained in a data assimilation-like fashion that use\nthese exogenous variables along with na\\\"ive predictions from climatology to\ngenerate 24-hour forecasts with non-parametric uncertainty bounds. We call this\nmethod the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that\nthe trained model can generalize to new geographic locations and time periods\nnot seen during training, and we compare its performance to that of the\nInternational Reference Ionosphere (IRI).",
        "We prove that the Fourier dimension of the graph of fractional Brownian\nmotion with Hurst index greater than $1\/2$ is almost surely 1. This extends the\nresult of Fraser and Sahlsten (2018) for the Brownian motion and verifies\npartly the conjecture of Fraser, Orponen and Sahlsten (2014). We introduce a\ncombinatorial integration by parts formula to compute the moments of the\nFourier transform of the graph measure. The proof of our main result is based\non this integration by parts formula together with Fa\\`a di Bruno's formula and\nstrong local nondeterminism of fractional Brownian motion. We also show that\nthe Fourier dimension of the graph of a symmetric $\\alpha$-stable process with\n$\\alpha\\in[1,2]$ is almost surely 1.",
        "We introduce a one parameter deformation of Zwegers' multivariable\n$\\mu$-function by applying iterations of the $q$-Borel summation method, which\nis also a multivariate analogue of the generalized $\\mu$-function introduced by\nthe authors. For this deformed multivariable $\\mu$-function, we give some\nformulas, for example, forward shift formula, translation and\n$\\mathfrak{S}_{N+1}$-symmetry. Further we mention modular formulas for the\nZwegers' original multivariable $\\mu$-function.",
        "In 2002 Thakare et al.\\ counted non-isomorphic lattices on $n$ elements,\nhaving nullity up to two. In 2020 Bhavale and Waphare introduced the concept of\nRC-lattices as the class of all lattices in which all the reducible elements\nare comparable. In this paper, we enumerate all non-isomorphic RC-lattices on\n$n$ elements. For this purpose, firstly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$, and containing\n$2 \\leq r \\leq 2k$ reducible elements. Secondly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$. This work is in\nrespect of Birkhoff's open problem of enumerating all finite lattices on $n$\nelements.",
        "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
        "Measurement of the Chiral Magnetic Effect (CME) has been a popular topic of\nhigh-energy nuclear physics in the last decade. The flow correlation $\\gamma$\nbetween charged hadron pairs of the same and opposite charges and their\ndifference $\\Delta \\gamma$ were measured to separate the CME-driven signal from\nthe collective flow background especially second-order elliptic $v_{2}$. The\nSTAR experiment have stepped further to the isobar experiment to compare\n$\\gamma$ and $\\Delta \\gamma$ between Ru+Ru and Zr+Zr\n~\\cite{PhysRevC.105.014901}, which were theoretically expected to produce the\nsame elliptic flow background but different CME signals. However, the measured\nflow backgrounds also differ between Ru+Ru and Zr+Zr, indicating more\nfine-tuning of RP and centrality definition necessary.\n  This analysis applied the AMPT model~\\cite{PhysRevC.72.064901} to simulate\nthe same collision system and energy as the STAR isobar experiment. Since the\nAMPT model does not include magnetic field effects, we expect comparing its\noutput between Ru+Ru and Zr+Zr collision systems can provide an insight of the\npossible bias of flow background definition, and help improve the measurement\nof CME signal in real experiments. Multiple combinations of centrality and flow\ndefinition were chosen to study how the $v_2$ and their difference would be\naffected, especially by varying the particles selection of charge versus\nneutral properties and broadening (pseudo-)rapidity regions, while STAR CME\nwork relied on charged-only particles at central rapidity.",
        "TianQin is a future space-based gravitational wave observatory targeting the\nfrequency window of $10^{-4}$ Hz $\\sim 1$ Hz. A large variety of gravitational\nwave sources are expected in this frequency band, including the merger of\nmassive black hole binaries, the inspiral of extreme\/intermediate mass ratio\nsystems, stellar-mass black hole binaries, Galactic compact binaries, and so\non. TianQin will consist of three Earth orbiting satellites on nearly identical\norbits with orbital radii of about $10^5$ km. The satellites will form a normal\ntriangle constellation whose plane is nearly perpendicular to the ecliptic\nplane. The TianQin project has been progressing smoothly following the ``0123\"\ntechnology roadmap. In step ``0\", the TianQin laser ranging station has been\nconstructed and it has successfully ranged to all the five retro-reflectors on\nthe Moon. In step ``1\", the drag-free control technology has been tested and\ndemonstrated using the TianQin-1 satellite. In step ``2\", the inter-satellite\nlaser interferometry technology will be tested using the pair of TianQin-2\nsatellites. The TianQin-2 mission has been officially approved and the\nsatellites will be launched around 2026. In step ``3\", i.e., the TianQin-3\nmission, three identical satellites will be launched around 2035 to form the\nspace-based gravitational wave detector, TianQin, and to start gravitational\nwave detection in space.",
        "Diffusion models generate samples by estimating the score function of the\ntarget distribution at various noise levels. The model is trained using samples\ndrawn from the target distribution, progressively adding noise. In this work,\nwe establish the first (nearly) dimension-free sample complexity bounds for\nlearning these score functions, achieving a double exponential improvement in\ndimension over prior results. A key aspect of our analysis is the use of a\nsingle function approximator to jointly estimate scores across noise levels, a\ncritical feature of diffusion models in practice which enables generalization\nacross timesteps. Our analysis introduces a novel martingale-based error\ndecomposition and sharp variance bounds, enabling efficient learning from\ndependent data generated by Markov processes, which may be of independent\ninterest. Building on these insights, we propose Bootstrapped Score Matching\n(BSM), a variance reduction technique that utilizes previously learned scores\nto improve accuracy at higher noise levels. These results provide crucial\ninsights into the efficiency and effectiveness of diffusion models for\ngenerative modeling.",
        "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}\/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https:\/\/github.com\/fshp971\/adv-icl.",
        "Developing phosphors with narrow photoluminescence emission peaks and high\nchromatic stability holds significant importance in light-emitting diode (LED)\ndisplay technologies, where a wide color gamut is essential to achieve the Rec.\n2020 specifications. This research focuses on the optical properties of a solid\nsolution: $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N [$M^I$=Na, K;\n$M^{III}$=Al, (Al$_{0.75}$Ga$_{0.25}$), (Al$_{0.5}$Ga$_{0.5}$),\n(Al$_{0.25}$Ga$_{0.75}$), Ga, (Ga$_{0.75}$In$_{0.25}$), (Ga$_{0.5}$In$_{0.5}$)]\nto understand how the narrow-emitting photoluminescence in\nK$_3$AlP$_3$O$_9$N:Eu$^{2+}$ can evolve during host structure cation\nsubstitution. Photoluminescence measurements at low temperature (15 K) support\nthat Eu$^{2+}$ replaces three crystallographically independent Na$^+$ sites in\nNa$_{2.97}$Eu$_{0.015}$AlP$_3$O$_9$N, similar to the parent K$^+$ phosphor, but\nsubstituting Ga$^{3+}$ and In$^{3+}$ for Al$^{3+}$ leads to a change in\nEu$^{2+}$ site preference, narrowing the full-width-at-half-maximum (fwhm) of\nthe emission peak. The chromatic stability and photoluminescence quantum yield\nare also enhanced with higher Ga$^{3+}$ content in the host but not with\nIn$^{3+}$. Thermoluminescence analysis indicates the relationship between trap\nstates and the enhanced quantum yield with Ga$^{3+}$ leads to the series' best\nperformance. The analysis of the $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N\nseries offers insight into the potential method for modulating optical\nproperties with cation substitution in the host structure.",
        "HSC 120505.09-000027.9 (J1205$-$0000) is one of the highest redshift\n($z=6.72$) dust-reddened quasars (red quasars) known to date. We present an\nimproved analysis of Atacama Large Millimeter\/submillimeter Array data of the\n[CII] $158\\ \\rm{\\mu m}$ line and the underlying rest-frame far-infrared (FIR)\ncontinuum emission, previously reported in Izumi et al. (2021a), toward\nJ1205$-$0000. Red quasars are thought to be a transitional phase from an\nobscured starburst to a luminous blue quasar, in some cases associated with\nmassive outflows driven by the active galactic nucleus (AGN). J1205$-$0000 has\na high FIR luminosity, $L_{\\mathrm{FIR}}=2.5\\times 10^{12}\\ L_{\\odot}$ and a\ntotal IR luminosity of $L_{\\mathrm{TIR}}=3.5\\times 10^{12}\\ L_{\\odot}$,\ncorresponding to a star formation rate (SFR) of $\\sim 528\\ M_{\\odot}\\\n\\mathrm{yr}^{-1}$. With the [CII]-based dynamical mass of $\\sim 1 \\times\n10^{11}~M_\\odot$, we conclude that J1205$-$0000 is hosted by a starburst\ngalaxy. In contradiction to Izumi et al. (2021a), our improved analysis shows\nno hint of a broad component in the [CII] line spectrum. Thus there is no\nevidence for a host galaxy-scale fast [CII] outflow, despite the fact that\nJ1205$-$0000 has fast nuclear ionized outflows seen in the rest-frame UV. We\nexplore several scenarios for this discrepancy (e.g., early phase of AGN\nfeedback, reliability of the [CII] line as a tracer of outflows), and we claim\nthat it is still too early to conclude that there is no significant negative\nAGN feedback on star formation in this red quasar.",
        "This paper presents a parallel and fully conservative adaptive mesh\nrefinement (AMR) implementation of a finite-volume-based kinetic solver for\ncompressible flows. Time-dependent H-type refinement is combined with a\ntwo-population quasi-equilibrium Bhatnagar-Gross-Krook discrete velocity\nBoltzmann model. A validation has shown that conservation laws are strictly\npreserved through the application of refluxing operations at coarse-fine\ninterfaces. Moreover, the targeted macroscopic moments of Euler and\nNavier-Stokes-Fourier level flows were accurately recovered with correct and\nGalilean invariant dispersion rates for a temperature range over three orders\nof magnitude and dissipation rates of all eigen-modes up to Mach of order 1.8.\nResults for one- and two-dimensional benchmarks up to Mach numbers of 3.2 and\ntemperature ratios of 7, such as the Sod and Lax shock tubes, the Shu-Osher and\nseveral Riemann problems, as well as viscous shock-vortex interactions, have\ndemonstrated that the solver precisely captures reference solutions. Excellent\nperformance in obtaining sensitive quantities was proven, for example in the\ntest case involving nonlinear acoustics, whilst, for the same accuracy and\nfidelity of the solution, the AMR methodology significantly reduced\ncomputational cost and memory footprints. Over all demonstrated two-dimensional\nproblems, up to a 4- to 9-fold reduction was achieved and an upper limit of the\nAMR overhead of 30% was found in a case with very cost-intensive parameter\nchoice. The proposed solver marks an accurate, efficient and scalable framework\nfor kinetic simulations of compressible flows with moderate supersonic speeds\nand discontinuities, offering a valuable tool for studying complex problems in\nfluid dynamics.",
        "Metasurfaces have offered unprecedented control over electromagnetic (EM)\nwaves across a wide range of frequency spectrum by manipulating its phase,\namplitude, and polarization at subwavelength scales. Full wavefront control\nusing metasurfaces requires 2{\\pi} phase modulation, which is essential for\nadvanced optical and photonic engineering. Common approaches, such as the\nPancharatnam-Berry (PB) phases and resonant phases, face stringent limitations:\nPB phases essentially depend on circular polarization conversion, while\nresonant phases are inherently narrowband and require a complex design process.\nTo overcome these challenges, we propose a broadband metasurface with a\nco-polarized transmissive geometric phase that achieves 2{\\pi} phase coverage\nwhile conserving the circular polarization of incident EM waves. This\nco-polarized phase is enabled by a local twist angle between the upper and\nlower metallic patterns, forming a branch cut in the parameter space determined\nby the twist angle and frequency. The branch cut connects phase singularities\nof opposite chirality, ensuring broadband 2{\\pi} phase coverage. We\nexperimentally validate the presence of the branch cut and demonstrate\nbroadband generation of arbitrary orbital angular momentum (OAM) for\nco-polarized output. Our approach provides a versatile method for designing\nbroadband metasurfaces without altering circular polarizations, paving the way\nfor development of compact optical and photonic devices.",
        "A functional analog of the Klain-Schneider theorem for vector-valued\nvaluations on convex functions is established, providing a classification of\ncontinuous, translation covariant, simple valuations. Under additional rotation\nequivariance assumptions, an analytic counterpart of the moment vector is\ncharacterized alongside a new epi-translation invariant valuation. The former\narises as the top-degree operator in a family of functional intrinsic moments,\nwhich are linked to functional intrinsic volumes through translations. The\nlatter represents the top-degree operator in a class of Minkowski vectors,\nwhich are introduced in this article and which lack classical counterparts on\nconvex bodies, as they vanish due to the Minkowski relations. Additional\nclassification results are obtained for homogeneous valuations of extremal\ndegrees.",
        "In analogy to high-energy nuclear scattering experiments, we study a\nreal-time scattering process between a propagating state and a dense target in\n$1+1$-d massive QED. In our setup, we identify three distinct regimes that\nqualitatively characterize the evolution: for a dilute medium, the incoming\nprobe state evolves nearly ballistically; in an intermediate setting, it\ntraverses the matter, locally exciting it; and for dense targets, one\napproaches a black-disk limit, where the matter acts as a strong wall\npotential. We find evidence that the probe's energy loss rate scales linearly\nwith the path length in the medium, and we study how the entanglement entropy\nreveals the mixing between the probe and medium states. With the goal of one\nday replicating high-energy nuclear experiments in quantum devices, we briefly\ndiscuss how the current tensor network-based simulations can be translated to a\nquantum simulator.",
        "Orthogonal-split trees perform well, but evidence suggests oblique splits can\nenhance their performance. This paper explores optimizing high-dimensional\n$s$-sparse oblique splits from $\\{(\\vec{w}, \\vec{w}^{\\top}\\boldsymbol{X}_{i}) :\ni\\in \\{1,\\dots, n\\}, \\vec{w} \\in \\mathbb{R}^p, \\| \\vec{w} \\|_{2} = 1, \\|\n\\vec{w} \\|_{0} \\leq s \\}$ for growing oblique trees, where $ s $ is a\nuser-defined sparsity parameter. We establish a connection between SID\nconvergence and $s_0$-sparse oblique splits with $s_0\\ge 1$, showing that the\nSID function class expands as $s_0$ increases, enabling the capture of more\ncomplex data-generating functions such as the $s_0$-dimensional XOR function.\nThus, $s_0$ represents the unknown potential complexity of the underlying\ndata-generating function. Learning these complex functions requires an\n$s$-sparse oblique tree with $s \\geq s_0$ and greater computational resources.\nThis highlights a trade-off between statistical accuracy, governed by the SID\nfunction class size depending on $s_0$, and computational cost. In contrast,\nprevious studies have explored the problem of SID convergence using orthogonal\nsplits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we\nintroduce a practical framework for oblique trees that integrates optimized\noblique splits alongside orthogonal splits into random forests. The proposed\napproach is assessed through simulations and real-data experiments, comparing\nits performance against various oblique tree models.",
        "In this paper, we develop a modified nonlinear dynamic diffusion (DD) finite\nelement method for convection-diffusion-reaction equations. This method is free\nof stabilization parameters and is capable of precluding spurious oscillations.\nWe prove existence and, under an assumption of small mesh size, uniqueness of\nthe discrete solution, and derive the optimal first order convergence rate of\nthe approximation error in the energy norm plus a dissipation term. Numerical\nexamples are provided to verify the theoretical analysis.",
        "We study the quiescent ultra-diffuse galaxy FCC 224 in the Fornax cluster\nusing Hubble Space Telescope (HST) imaging, motivated by peculiar properties of\nits globular cluster (GC) system revealed in shallower imaging. The surface\nbrightness fluctuation distance of FCC 224 measured from HST is $18.6 \\pm 2.7$\nMpc, consistent with the Fornax Cluster distance. We use Prospector to infer\nthe stellar population from a combination of multi-wavelength photometry (HST,\nground-based, WISE) and Keck Cosmic Web Imager spectroscopy. The galaxy has a\nmass-weighted age of $\\sim$ 10 Gyr, metallicity [M\/H] of $\\sim -1.25$ dex, and\na very short formation $e$-folding time of $\\tau \\sim 0.3$ Gyr. Its 12\ncandidate GCs exhibit highly homogeneous $g_{\\rm 475}-I_{\\rm 814}$ colors,\nmerely 0.04 mag bluer than the diffuse starlight, which supports a single burst\nformation scenario for this galaxy. We confirm a top-heavy GC luminosity\nfunction, similar to the two dark matter deficient galaxies NGC 1052-DF2 and\nDF4. However, FCC 224 differs from those galaxies with relatively small GC\nsizes of $\\sim$ 3 pc ($\\sim 35\\%$ smaller than typical for other dwarfs), and\nwith radial mass segregation in its GC system. We are not yet able to identify\na formation scenario to explain all of the GC properties in FCC 224. Follow-up\nmeasurements of the dark matter content in FCC 224 will be crucial because of\nthe mix of similarities and differences among FCC 224, DF2, and DF4.",
        "This paper concerns the distributed observer for the descriptor linear\nsystem. Unlike centralized descriptor system observers, in the case of\ndistributed observers, each agent either finds it difficult to independently\neliminate impulses, or the observer dynamics after eliminating pulses cannot be\nimplemented. To overcome this issue, this paper develops the structure of the\ndistributed observer in two different scenarios, and the observer parameters\nare presented through a novel design. Moreover, we provide two implementation\nmethods for distributed observer in different scenarios. As a result, each\nlocal observer has the ability to reconstruct the states of the underlying\nsystem, including its impulse phenomenon. Finally, simulation results verify\nthe validity of our results.",
        "We analyze sharing Bell-type nonlocal correlation between two distant parties\nwith optical hybrid states comprising a single photon polarization state and a\nmultiphoton coherent state. By deploying entanglement swapping over the\ncoherent state parts at the middle station, we show that the optical hybrid\nstates can efficiently generate a polarization-entangled state that violates\nClauser-Horne-Shimony-Holt (CHSH) Bell-inequality well over a metropolitan\ndistance. We further assess the quality of the shared entangled state in the\ninformation processing task of quantum teleportation of an unknown polarization\nqubit. Our results with realistic devices, embedding detection inefficiency and\ntransmission losses, indicate the viability of faithful quantum teleportation\nover large distances, consistent with the quality of the shared correlation.",
        "Magnetic thin films hosting topological spin textures, such as magnetic\nskyrmions, hold high potential for breakthroughs in the field of spintronics,\ndue to good scalability and energy efficiency. Novel computational\narchitectures such as memory-in-logic devices rely on material platforms able\nto host those topological spin textures. Furthermore, recently proposed designs\nof novel quantum information technologies are based on heterostructures where\ntopological spin textures are in direct proximity to a superconducting layer.\nHere, we demonstrate the stabilization of out-of-plane magnetic bubbles in\nhighly ordered [Ni\/Co]$_{n}$ multilayers on a Nb(110) single crystal. This is\nachieved without the need for removal of the well-known Nb(110)-oxide surface\nreconstruction, due to the introduction of a one-atom-thick Cu interlayer in\nbetween the Nb substrate and the magnetic multilayer. The Cu interlayer\ngenerates a well-ordered hexagonal surface, which is key for the epitaxial\ngrowth of the [Ni\/Co]$_{n}$ multilayers hosting the desired out-of-plane\nanisotropy. The magnetic ground state of the prepared material stacks is\ndirectly imaged via spin-polarized low energy electron microscopy (SPLEEM),\nrevealing the presence of magnetic bubble domains with lateral sizes as small\nas 450 nm.",
        "Transport of microscopic objects across biological membranes usually involves\nmembrane deformation to enclose the object followed by detachment of the\nengulfed particle. However, in artificial membranes, this last topological\nremodelling step is in many cases not spontaneous due to the elastic stability\nof the neck structure formed upon complete particle wrapping. In this work, we\nuse optical trapping to induce the wrapping of a non-adhesive microsphere by\nthe membrane of a giant lipid vesicle and investigate the energetics and\ndynamics of the resulting neck structure. We find that neck formation occurs as\na result of membrane shape energy minimization under the application of\nexternal force. Remarkably, increasing membrane tension could reopen the neck\nand reverse the wrapping process. This process shows a clear hysteresis and a\ndegree of reversibility. Neck cleavage and particle detachment into the\nvesicle's interior could not be triggered in the range of our optical forces.\nSystematic studies on the thermal dynamics of wrapped particles allowed to\nestablish that diffusion properties of the system are in agreement with a\ncoupling of the particle motion with the neck structure, modeled as a solid\ninclusion within the membrane. Interestingly, the wrapped particle dynamics\nexhibited a tension dependency, which can be described as the sum of several\ndrag contributions.",
        "Neutral atoms emitted from liquid metal ion sources are an often-overlooked\nsource of contamination and damage in focused ion beam microscopy. Beyond ions\nand single atoms, these sources also emit atom clusters. While most studies\nhave investigated charged clusters, here we demonstrate that neutral clusters\nare also emitted. These neutral clusters bypass the electrostatic beam blanking\nsystem, allowing them to impinge on samples even when the ion beam is blanked.\nWe investigate this phenomenon using thin (<20 nm) freestanding membranes of\nhexagonal boron nitride, silicon, and silicon nitride as targets. Randomly\ndispersed nanopores that form upon neutral cluster exposure are revealed. The\naverage nanopore diameter is ~2 nm with a narrow size distribution, suggesting\nthat the atom clusters emitted from the source have a preferred size. Various\nelectron microscopy techniques are used to characterize the nanopores,\nincluding high-resolution transmission electron microscopy, multislice\nptychography, and electron energy-loss spectroscopy. Finally, we show how\nelectron irradiation in the transmission electron microscope can be used to\nboth remove any amorphous material that may clog the pores and to controllably\ngrow the pores to specific sizes. Tunable nanopores such as these are\ninteresting for nanofluidic applications involving size-selective membranes.",
        "The associative interaction, such as hydrogen bonding, can bring about\nversatile functionalities to polymer systems, which has been investigated by\ntremendous researches, but the fundamental understanding on association process\nis still lacking. In this study, a reaction-controlled association model is\nproposed to delve into the polymer association activities in solvents, which is\nproved to obey the principle of thermodynamics. Additionally, associative\npolymer chain configurational bias method is developed to improve sampling\nefficiency, demonstrating a significantly faster relaxation process. First, we\nset non-bonded interactions to be zero, and only keep the chain connectivity\nand association. It is found that the association process intrinsically follows\nBernoulli process by comparing the simulation results and analytic results.\nNext, we include non-bonded interactions into the simulation to examine its\neffects. It emerged that the excluded volume effect and solvents immiscibility\neffects can result in inhomogeneous associating probability distribution along\nthe chain contour, in contrast to the homogeneity observed in ideal systems,\nthereby shifting from the binomial distribution to Poisson binomial\ndistribution. At last, the study is extended to cooperative association\nsystems. The incorporation of cooperative association can lead to the\ncoexistence of coil and globule state at the transition point, verified by the\npotential of mean force calculation. Finally, a mathematical model is proposed,\nillustrating the changes in statistical weight induced by sequence enthalpy\nbias, which is the consequence of cooperative behaviors."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
    "start_abstract":"Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "A primer on deep learning in genomics"
      ],
      "abstract":[
        "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Spatial locking of chimera states to frequency heterogeneity in\n  nonlocally coupled oscillators",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Wafer-scale waveguide sidewall roughness scattering loss\n  characterization by image processing",
        "The Aesthetic Imperative of Lev Landau's Geometric Reductionism in\n  Theoretical Physics",
        "BrainOOD: Out-of-distribution Generalizable Brain Network Analysis",
        "Fenchel's conjecture on NEC groups",
        "Using the STIX background detector as a proxy for GOES",
        "Exercises in Iterational Asymptotics II",
        "Tools for Unbinned Unfolding",
        "On stabilization at a soliton for generalized Korteweg--De Vries pure\n  power equation for any power $p\\in (1,5)$",
        "An improved evaluation of the electroweak contribution to $(g-2)_\\mu$",
        "The puzzle of isolated and quenched dwarf galaxies in cosmic voids",
        "Site-engineered ferromagnetism in Ca and Cr co-substituted Bismuth\n  Ferrite Nanoparticles",
        "A modified two-stage search framework for constrained multi-gradient\n  descent",
        "Ultrafast Charge Separation on the Nanoscale Induced by a Uniform Field",
        "Digital Twin Aided Channel Estimation: Zone-Specific Subspace Prediction\n  and Calibration",
        "End-to-End Detector Optimization with Diffusion models: A Case Study in\n  Sampling Calorimeters",
        "Sound insulation performance of multi-layer membrane-type acoustic\n  metamaterials based on orthogonal experiments",
        "Direct Sampling of Confined Polygons in Linear Time",
        "Towards a Manifestly Causal Approach to Particle Scattering",
        "Cycle Patterns and Mean Payoff Games",
        "Hypercubic structures behind $\\hat{Z}$-invariants",
        "Spherically symmetric horizonless solutions and their frozen states in\n  Bardeen spacetime with Proca field",
        "Explainable Brain Age Gap Prediction in Neurodegenerative Conditions\n  using coVariance Neural Networks",
        "Geometrical constructions of purity testing protocols and their\n  applications to quantum communication",
        "Langevin model for soliton molecules in ultrafast fiber ring laser\n  cavity: investigating experimentally the interplay between noise and inertia",
        "A hybrid-dimensional Stokes--Brinkman--Darcy model for arbitrary flows\n  to the fluid--porous interface",
        "Relighting the fire in Hickson Compact Group (HCG) 15: magnetised fossil\n  plasma revealed by the SKA Pathfinders & Precursors",
        "Richardson-Gaudin states of non-zero seniority I: matrix elements"
      ],
      "abstract":[
        "Chimera states in systems of nonlocally coupled oscillators, i.e.,\nself-organized coexistence of coherent and incoherent oscillator populations,\nhave attracted much attention. In this study, we consider the effect of\nfrequency heterogeneities on the chimera state and reveal that it induces\nspatial locking of the chimera state, i.e., the coherent and incoherent domains\nalign with lower and higher frequency regions, respectively, in a self-adaptive\nmanner. Using an extended self-consistency approach, we show that such\nspatially locked chimera states can be reproduced as steady solutions of the\nsystem in the continuum limit. Furthermore, we develop a variational argument\nto explain the mechanism leading to spatial locking. Our analysis reveals how\nheterogeneity can affect the collective dynamics of the chimera states and\noffers insights into their control and applications.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "Photonic integrated circuits (PICs) are vital for developing affordable,\nhigh-performance optoelectronic devices that can be manufactured at an\nindustrial scale, driving innovation and efficiency in various applications.\nOptical loss of modes in thin film waveguides and devices is a critical measure\nof their performance. Thin films growth, lithography, masking, and etching\nprocesses are imperfect processes that introduce significant sidewall and\ntop-surface roughness and cause dominating optical losses in waveguides and\nphotonic structures. These roughness as perturbations couple light from guided\nto far-field radiation modes, leading to scattering losses that can be\nestimated from theoretical models. Typically, with UV-based lithography\nsidewall roughness is found to be significantly larger than wafer-top surface\nroughness. Atomic force microscopy (AFM) imaging measurement gives 3D and\nhigh-resolution roughness profile but the measurement is inconvenient, costly,\nand unscalable for large-scale PICs and at wafer-scale. Here, we evaluate the\nsidewall roughness profile based on 2D high-resolution scanning electron\nmicroscope imaging. We characterized the loss on two homemade nitride and oxide\nfilms on 3-inch silicon wafers with 12 waveguide devices on each and co-related\nthe scattering loss estimated from a 2D image-based sidewall profile and\ntheoretical Payne model. The lowest loss of guided fundamental transverse\nelectric (TE$_{0}$) is found at 0.075 dB\/cm at 633 nm across 24 devices, which\nis a record at visible wavelength. Our work shows a 100% success in edge\ndetection in image processing to estimate autocorrelation function and optical\nmode loss. These demonstrations offer valuable insights into waveguide sidewall\nroughness and comparison of experimental and 2D SEM image-processing-based loss\nestimations.",
        "This paper explores the ontological and epistemological foundations of Lev\nLandau's theoretical physics through the lens of his unpublished philosophical\nnotes and scientific practice. We identify a unique form of geometric\nreductionism where physical laws emerge as inevitable consequences of symmetry\nbreaking in progressively constrained phase spaces. Landau's dismissal of\nquantum interpretation debates and his famous \"axiomatic minimalism\" in the\nCourse of Theoretical Physics are shown to stem from a deep epistemological\ncommitment to dimensional aesthetics - the belief that fundamental truths must\nmanifest through dimensional economy in mathematical representations.",
        "In neuroscience, identifying distinct patterns linked to neurological\ndisorders, such as Alzheimer's and Autism, is critical for early diagnosis and\neffective intervention. Graph Neural Networks (GNNs) have shown promising in\nanalyzing brain networks, but there are two major challenges in using GNNs: (1)\ndistribution shifts in multi-site brain network data, leading to poor\nOut-of-Distribution (OOD) generalization, and (2) limited interpretability in\nidentifying key brain regions critical to neurological disorders. Existing\ngraph OOD methods, while effective in other domains, struggle with the unique\ncharacteristics of brain networks. To bridge these gaps, we introduce BrainOOD,\na novel framework tailored for brain networks that enhances GNNs' OOD\ngeneralization and interpretability. BrainOOD framework consists of a feature\nselector and a structure extractor, which incorporates various auxiliary losses\nincluding an improved Graph Information Bottleneck (GIB) objective to recover\ncausal subgraphs. By aligning structure selection across brain networks and\nfiltering noisy features, BrainOOD offers reliable interpretations of critical\nbrain regions. Our approach outperforms 16 existing methods and improves\ngeneralization to OOD subjects by up to 8.5%. Case studies highlight the\nscientific validity of the patterns extracted, which aligns with the findings\nin known neuroscience literature. We also propose the first OOD brain network\nbenchmark, which provides a foundation for future research in this field. Our\ncode is available at https:\/\/github.com\/AngusMonroe\/BrainOOD.",
        "A classical discovery known as Fenchel's conjecture and proved in the 1950s,\nshows that every co-compact Fuchsian group $F$ has a normal subgroup of finite\nindex isomorphic to the fundamental group of a compact unbordered orientable\nsurface, or in algebraic terms, that $F$ has a normal subgroup of finite index\nthat contains no element of finite order other than the identity. In this paper\nwe initiate and make progress on an extension of Fenchel's conjecture by\nconsidering the following question: Does every planar non-Euclidean\ncrystallographic group $\\Gamma$ containing transformations that reverse\norientation have a normal subgroup of finite index isomorphic to the\nfundamental group of a compact unbordered non-orientable surface? We answer\nthis question in the affirmative in the case where the orbit space of $\\Gamma$\nis a nonorientable surface, and also in the case where this orbit space is a\nbordered orientable surface of positive genus. In the case where the genus of\nthe quotient is $0$, we have an affirmative answer in many subcases, but the\nquestion is still open for others.",
        "Context. The Spectrometer\/Telescope for Imaging X-Rays (STIX) onboard Solar\nOrbiter was designed to observe solar flares in the X-ray range of 4-150 keV,\nproviding spectral, temporal and spatial information. Besides 30 imaging\ndetectors, STIX has two additional detectors, the coarse flare locator (CFL)\nand the background (BKG) detector. Flares observed from Earth are classified\nusing their peak X-ray flux observed by the GOES satellites. Roughly half of\nall flares observed by STIX are located on the backside of the Sun. These\nflares lack a GOES-class classification.\n  Aims. In this paper, we describe the calibration of the BKG detector aperture\nsizes. Using the calibrated measurements of the BKG detector, we explore the\nrelationship between the peak flux for flares jointly observed by STIX and\nGOES. This allows us to estimate the GOES flare classes of backside flares\nusing STIX measurements.\n  Methods. We looked at the 500 largest flares observed by both STIX and GOES\nin the time range Feb. 21 to Apr. 23. Aperture size calibration is done by\ncomparing 4-10 keV counts of the BKG detector with the CFL measurements. In a\nsecond step, we correlate the calibrated STIX BKG peak flux with the GOES peak\nflux for individual flares.\n  Results. We calibrated the BKG detector aperture sizes of STIX. Further, we\nshowed that for the larger flares a close power law fit exists between the STIX\nBKG and GOES peak flux with a Pearson correlation coefficient of 0.97. This\ncorrelation provides a GOES proxy with a one sigma uncertainty of 11%. We were\nable to show that the BKG detector can reliably measure a broad range of GOES\nflare classes from roughly B5 up to at least X85 (assuming a radial distance of\n1AU), making it an interesting detector-concept for future space weather\nmissions. The largest flare observed by STIX to date is an estimated X16.5\n$\\pm$ 1.8 backside flare on the 20 Mai 2024.",
        "The nonlinear recurrences we consider here include the functions $3x(1-x)$\nand $\\cos(x)$, which possess attractive fixed points $2\/3$ and $0.739...$\n(Dottie's number). Detailed asymptotics for oscillatory convergence are found,\nstarting with a 1960 paper by Wolfgang Thron. Another function,\n$x\/(1+x\\ln(1+x))$, gives rise to a sequence with monotonic convergence to $0$\nbut requires substantial work to calculate its associated constant $C$.",
        "Machine learning has enabled differential cross section measurements that are\nnot discretized. Going beyond the traditional histogram-based paradigm, these\nunbinned unfolding methods are rapidly being integrated into experimental\nworkflows. In order to enable widespread adaptation and standardization, we\ndevelop methods, benchmarks, and software for unbinned unfolding. For\nmethodology, we demonstrate the utility of boosted decision trees for unfolding\nwith a relatively small number of high-level features. This complements\nstate-of-the-art deep learning models capable of unfolding the full phase\nspace. To benchmark unbinned unfolding methods, we develop an extension of\nexisting dataset to include acceptance effects, a necessary challenge for real\nmeasurements. Additionally, we directly compare binned and unbinned methods\nusing discretized inputs for the latter in order to control for the binning\nitself. Lastly, we have assembled two software packages for the OmniFold\nunbinned unfolding method that should serve as the starting point for any\nfuture analyses using this technique. One package is based on the widely-used\nRooUnfold framework and the other is a standalone package available through the\nPython Package Index (PyPI).",
        "We apply our idea, which previously we used in the analysis of the pure power\nNLS, consisting in spitting the virial inequality method into a large energy\ninequality combined with Kato smoothing, to the case of generalized\nKorteweg--De Vries pure power equations. We assume that a solution remains for\nall positive times very close to a soliton and then we prove an asymptotic\nstability result for $t\\to +\\infty$.",
        "A precise evaluation of the electroweak contribution to the anomalous\nmagnetic moment of the muon requires control over all aspects of the Standard\nModel, ranging from Higgs physics, over multi-loop computations for bosonic and\n(heavy-)fermion diagrams, to non-perturbative effects in the presence of light\nquarks. Currently, the dominant uncertainties arise from such hadronic effects\nin the vector-vector-axial-vector three-point function, an improved\nunderstanding of which has recently emerged in the context of hadronic\nlight-by-light scattering. Profiting from these developments as well as new\nperturbative and non-perturbative input for the charm contribution, we obtain\n$a_\\mu^\\text{EW}=154.4(4)\\times 10^{-11}$.",
        "We report, for the first time, the detection of a sample of quenched and\nisolated dwarf galaxies (with 8.9 $<$ log(M$_{\\rm \\star}$\/M$_{\\rm \\odot}$) $<$\n9.5) in the least dense regions of the cosmic web, including voids, filaments,\nand walls. These dwarfs have no neighbouring galaxy within 1.0~Mpc in projected\ndistance. Based on the full spectral fitting of their central spectra using\nSloan Digital Sky Survey data, these galaxies are gas-deprived, exhibit stellar\nmass assembly very similar to dwarfs in the central regions of galaxy clusters,\nand have experienced no significant star formation in the past 2 Gyr.\nAdditionally, analysis of r-band images from the Dark Energy Camera Legacy\nSurvey showed that these dwarf galaxies host a central Nuclear Star Cluster\n(NSC). Detecting quenched, isolated dwarf galaxies in cosmic voids indicates\nthat environmental factors are not the sole drivers of their quenching.\nInternal mechanisms, such as feedback from in-situ star formation, also\ncontributing to the NSC formation, black holes, or variations in conditions\nduring their formation, offer potential explanations for star formation\nsuppression in these galaxies. These findings highlight the need for a\nsignificant revision in our understanding of baryonic physics, particularly\nconcerning the formation and evolution of low-mass galaxies.",
        "Multiferroic perovskites that exhibit room temperature magnetization and\npolarization have immense potential in the next generation of magneto-electric\nand spintronic memory devices. In this work, the magnetic and ferroelectric\nproperties of Bismuth Ferrite, BiFeO3 (BFO) nanoparticles (NPs) were enhanced\nthrough simultaneous A and B site Ca and Cr co-substitution. Novel compositions\nof Bi0.97Ca0.03CrxFe1-xO3 (x=0, 0.01, 0.03, 0.05) were synthesized using the\nsol-gel route and annealed at 550 degrees Celcius. Rietveld Refinement of XRD\npatterns confirmed high phase purity, while SEM analysis revealed a decreasing\ntrend in average particle size with increasing dopant concentration. Hysteresis\nloops showed enhanced magnetic properties as particle size approached the spin\ncycloid wavelength (around 62 nm), disrupting the intrinsic antiferromagnetic\nordering of BFO. Moreover, the presence of exchange bias in the NPs was linked\nto the formation of core-shell structure. Temperature dependent magnetization\nstudies showed an increase in N\\'eel temperature upon Ca substitution. XPS\nanalysis confirmed that Bi0.97Ca0.03FeO3 samples exhibited the highest oxygen\nvacancy concentration, while Fe3+ remained the dominant oxidation state across\nall compositions. Ferroelectric polarization loop measurements showed enhanced\nremanent polarization in doped samples, with leakage linked to oxygen vacancies\nand extrinsic microstructural effects.",
        "\\cite{desideri2012multiple} proposed a multi-gradient descent algorithm\n(MGDA) that can improve all objectives based on seeking the minim norm point in\nthe convex hull consisting of objectives function gradients as the common\ndescent direction, which has become the cornerstone of the multi-musk learning\nand multi-objective optimization. However, the logic to seek a common descent\ndirection through finding the minim-norm point in the gradient convex hull may\nfail under constraints, no matter whether taking the constraints into\nconsideration directly or projecting it into the feasible region after finding\nthe common descent direction with no constraints. Therefore, we proposed a\ntwo-stage search framework. In the first stage, a min-max search algorithm is\nimplemented to minimize the upper bound of the directional derivatives under\nconstraints, and the weak Pareto stationary can be theoretically reached in the\nfirst stage. In the second stage, the Pareto stationary can be theoretically\nobtained through the minimization of the lower bound of the directional\nderivatives under constraints. In numerical studies, we show the effectiveness\nof our proposed framework from the calibration of the multi-regime fundamental\ndiagram model and large-scale multi-objective portfolio problem.",
        "When illuminated by white light, atoms, molecules, and materials absorb only\ncertain characteristic energy contributions based on their absorption\nproperties. Here, we show that this effect can be translated from energy to\nspace: a spatially uniform laser pulse can create strongly localized carrier\nexcitations, including excitons, and spatial charge separation on the\nsub-nanometer scale within a few femtoseconds, opening new avenues for\nnanoelectronics and bringing petahertz switching within reach. Using\nnonequilibrium Green functions simulations we demonstrate this effect by\nexciting targeted areas of small graphene nanoribbon heterostructures by\ncareful choice of the laser energy and polarization.",
        "Effective channel estimation in sparse and high-dimensional environments is\nessential for next-generation wireless systems, particularly in large-scale\nMIMO deployments. This paper introduces a novel framework that leverages\ndigital twins (DTs) as priors to enable efficient zone-specific subspace-based\nchannel estimation (CE). Subspace-based CE significantly reduces feedback\noverhead by focusing on the dominant channel components, exploiting sparsity in\nthe angular domain while preserving estimation accuracy. While DT channels may\nexhibit inaccuracies, their coarse-grained subspaces provide a powerful\nstarting point, reducing the search space and accelerating convergence. The\nframework employs a two-step clustering process on the Grassmann manifold,\ncombined with reinforcement learning (RL), to iteratively calibrate subspaces\nand align them with real-world counterparts. Simulations show that digital\ntwins not only enable near-optimal performance but also enhance the accuracy of\nsubspace calibration through RL, highlighting their potential as a step towards\nlearnable digital twins.",
        "Recent advances in machine learning have opened new avenues for optimizing\ndetector designs in high-energy physics, where the complex interplay of\ngeometry, materials, and physics processes has traditionally posed a\nsignificant challenge. In this work, we introduce the $\\textit{end-to-end}$ AI\nDetector Optimization framework (AIDO) that leverages a diffusion model as a\nsurrogate for the full simulation and reconstruction chain, enabling\ngradient-based design exploration in both continuous and discrete parameter\nspaces. Although this framework is applicable to a broad range of detectors, we\nillustrate its power using the specific example of a sampling calorimeter,\nfocusing on charged pions and photons as representative incident particles. Our\nresults demonstrate that the diffusion model effectively captures critical\nperformance metrics for calorimeter design, guiding the automatic search for\nlayer arrangement and material composition that aligns with known calorimeter\nprinciples. The success of this proof-of-concept study provides a foundation\nfor future applications of end-to-end optimization to more complex detector\nsystems, offering a promising path toward systematically exploring the vast\ndesign space in next-generation experiments.",
        "The challenge of achieving effective sound insulation using metamaterials\npersists in the field. In this research endeavor, a novel three-layer\nmembrane-type acoustic metamaterial is introduced as a potential solution.\nThrough the application of orthogonal experiments, remarkable sound insulation\ncapabilities are demonstrated within the frequency spectrum of 100-1200 Hz. The\nsound insulation principle of membrane-type acoustic metamaterial is obtained\nthrough the analysis of eigenmodes at the peak and trough points, combined with\nsound transmission loss. In addition, an orthogonal experiment is utilized to\npinpoint the critical factors that impact sound insulation performance. By\nusing relative bandwidth as the classification criterion, the optimal\ncombination of influencing factors is determined, thereby improving the sound\ntransmission loss of the multi-layer membrane-type acoustic metamaterial\nstructure and broadening the sound insulation bandwidth. This study not only\ncontributes a fresh and practical approach to insulation material design but\nalso offers valuable insights into advancing sound insulation technology.",
        "We present an algorithm for sampling tightly confined random equilateral\nclosed polygons in three-space which has runtime linear in the number of edges.\nUsing symplectic geometry, sampling such polygons reduces to sampling a moment\npolytope, and in our confinement model this polytope turns out to be very\nnatural from a combinatorial point of view. This connection to combinatorics\nyields both our fast sampling algorithm and explicit formulas for the expected\ndistances of vertices to the origin. We use our algorithm to investigate the\nexpected total curvature of confined polygons, leading to a very precise\nconjecture for the asymptotics of total curvature.",
        "We introduce a new, probability-level approach to calculations in scalar\nfield particle scattering. The approach involves the implicit summation over\nfinal states, which makes causality manifest since retarded propagators emerge\nnaturally. Novel diagrams represent algebraic terms at the probability level,\nakin to Feynman diagrams at the amplitude level. We present a list of rules\nwhich generate all probability-level diagrams for particle scattering processes\nin which one is fully inclusive over final states that contain no initial-state\nparticles. The inclusivity and causal structure of this formalism may offer\ninsights into the cancellation of infrared divergences if applied to gauge\ntheory calculations.",
        "We introduce the concept of a \\emph{cycle pattern} for directed graphs as\nfunctions from the set of cycles to the set $\\{-,0,+\\}$. The key example for\nsuch a pattern is derived from a weight function, giving rise to the sign of\nthe total weight of the edges for each cycle. Hence, cycle patterns describe a\nfundamental structure of a weighted digraph, and they arise naturally in games\non graphs, in particular parity games, mean payoff games, and energy games.\n  Our contribution is threefold: we analyze the structure and derive hardness\nresults for the realization of cycle patterns by weight functions. Then we use\nthem to show hardness of solving games given the limited information of a cycle\npattern. Finally, we identify a novel geometric hardness measure for solving\nmean payoff games (MPG) using the framework of linear decision trees, and use\ncycle patterns to derive lower bounds with respect to this measure, for large\nclasses of algorithms for MPGs.",
        "We propose an abelian categorification of $\\hat{Z}$-invariants for Seifert\n$3$-manifolds. First, we give a recursive combinatorial derivation of these\n$\\hat{Z}$-invariants using graphs with certain hypercubic structures. Next, we\nconsider such graphs as annotated Loewy diagrams in an abelian category,\nallowing non-split extensions by the ambiguity of embedding of subobjects. If\nsuch an extension has good algebraic group actions, then the above derivation\nof $\\hat{Z}$-invariants in the Grothendieck group of the abelian category can\nbe understood in terms of the theory of shift systems, i.e., Weyl-type\ncharacter formula of the nested Feigin-Tipunin constructions. For the project\nof developing the dictionary between logarithmic CFTs and 3-manifolds, these\ndiscussions give a glimpse of a hypothetical and prototypical, but unified\nconstruction\/research method for the former from the new perspective,\nreductions of representation theories by recursive structures.",
        "In this paper, we construct a static spherical symmetric Bardeen-Proca star\n(BPS) model, which consists of the electromagnetic field and Proca field\nminimally coupled with gravity. The introduction of the Proca field disrupts\nthe formation of event horizons, ensuring that these solutions are globally\nregular throughout the spacetime. We obtain families of BPS solutions under\nseveral magnetic charge conditions. Based on these results, we further\ninvestigate the ADM mass, Noether charge, and energy density distribution of\nthem. We find that when the magnetic charge is sufficiently large, solutions\nwith a critical horizon $r_{cH}$ emerge as $\\omega \\rightarrow 0$, and the time\ncomponent of the metric approaches zero inside $r_{cH}$. To an observer at\ninfinity, the collapse process of the matter near the critical horizon appears\nfrozen. Consequently, we refer to the solution with $r_{cH}$ as the frozen\nBardeen-Proca star (FBPS). Additionally, we also investigate the circular\ngeodesic orbits of BPS. For the light ring, we find that the light rings always\nappear in pairs, located on both sides of the critical horizon and moving\nfurther apart as the frequency $\\omega$ decreases. For timelike circular\norbits, we investigate their distribution in the spacetime of BPSs and\nhighlight four representative families of BPS solutions.",
        "Brain age is the estimate of biological age derived from neuroimaging\ndatasets using machine learning algorithms. Increasing \\textit{brain age gap}\ncharacterized by an elevated brain age relative to the chronological age can\nreflect increased vulnerability to neurodegeneration and cognitive decline.\nHence, brain age gap is a promising biomarker for monitoring brain health.\nHowever, black-box machine learning approaches to brain age gap prediction have\nlimited practical utility. Recent studies on coVariance neural networks (VNN)\nhave proposed a relatively transparent deep learning pipeline for neuroimaging\ndata analyses, which possesses two key features: (i) inherent\n\\textit{anatomically interpretablity} of derived biomarkers; and (ii) a\nmethodologically interpretable perspective based on \\textit{linkage with\neigenvectors of anatomic covariance matrix}. In this paper, we apply the\nVNN-based approach to study brain age gap using cortical thickness features for\nvarious prevalent neurodegenerative conditions. Our results reveal distinct\nanatomic patterns for brain age gap in Alzheimer's disease, frontotemporal\ndementia, and atypical Parkinsonian disorders. Furthermore, we demonstrate that\nthe distinct anatomic patterns of brain age gap are linked with the differences\nin how VNN leverages the eigenspectrum of the anatomic covariance matrix, thus\nlending explainability to the reported results.",
        "Purity testing protocols (PTPs), i.e., protocols that decide with high\nprobability whether or not a distributed bipartite quantum state is maximally\nentangled, have been proven to be a useful tool in many quantum communication\napplications. In this paper, we provide geometrical constructions for such\nprotocols that originate directly from classical linear error correcting codes\n(LECCs), in a way that the properties of the resulting PTPs are completely\ndetermined from those of the LECCs used in the construction. We investigate the\nimplications of our results in various tasks, including error detection,\nentanglement purification for general quantum error models and quantum message\nauthentication.",
        "The dynamics of soliton molecules in ultrafast fiber ring laser cavity is\nstrongly influenced by noise. We show how a parsimonious Langevin model can be\nconstructed from experimental data, resulting in a mathematical description\nthat encompasses both the deterministic and stochastic properties of the\nevolution of the soliton molecules. In particular, we were able to probe the\nresponse dynamics of the soliton molecule to an external kick in a sub-critical\napproach, namely without the need to actually disturb the systems under\ninvestigation. Moreover, the noise experienced by the dissipative solitonic\nsystem, including its distribution and correlation, can now be also analyzed in\ndetails. Our strategy can be applied to any systems where the individual motion\nof its constitutive particles can be traced; the case of optical\nsolitonic-system laser presented here serving as a proof-of-principle\ndemonstration.",
        "Mathematical modelling of coupled flow systems containing a free-flow region\nin contact with a porous medium is challenging, especially for arbitrary flow\ndirections to the fluid--porous interface. Transport processes in the free flow\nand porous medium are typically described by distinct equations: the Stokes\nequations and Darcy's law, respectively, with an appropriate set of coupling\nconditions at the common interface. Classical interface conditions based on the\nBeavers--Joseph condition are not accurate for general flows. Several\ngeneralisations are recently developed for arbitrary flows at the interface,\nsome of them are however only theoretically formulated and still need to be\nvalidated.\n  In this manuscript, we propose an alternative to couple free flow and\nporous-medium flow, namely, the hybrid-dimensional Stokes--Brinkman--Darcy\nmodel. Such formulation incorporates the averaged Brinkman equations within a\ncomplex interface between the free-flow and porous-medium regions. The complex\ninterface acts as a buffer zone facilitating storage and transport of mass and\nmomentum and the model is applicable for arbitrary flow directions. We validate\nthe proposed hybrid-dimensional model against the pore-scale resolved model in\nmultiple examples and compare numerical simulation results also with the\nclassical and generalised coupling conditions from the literature. The proposed\nhybrid-dimensional model demonstrates its applicability to describe arbitrary\ncoupled flows and shows its advantages in comparison to other generalised\ncoupling conditions.",
        "In the context of the life cycle and evolution of active galactic nuclei\n(AGN), the environment plays an important role. In particular, the over-dense\nenvironments of galaxy groups, where dynamical interactions and bulk motions\nhave significant impact, offer an excellent but under-explored window into the\nlife cycles of AGN and the processes that shape the evolution of relativistic\nplasma. Pilot Survey observations with the Australian Square Kilometre Array\nPathfinder (ASKAP) Evolutionary Map of the Universe (EMU) survey recovered\ndiffuse emission associated with the nearby (z = 0.0228) galaxy group HCG15,\nwhich was revealed to be strongly linearly polarised. We study the properties\nof this emission in unprecedented detail to settle open questions about its\nnature and its relation to the group-member galaxies. We perform a\nmulti-frequency spectropolarimetric study of HCG15 incorporating our ASKAP EMU\nobservations as well as new data from MeerKAT, LOFAR, the GMRT, and the Karl G.\nJansky Very Large Array (VLA), plus X-ray data from XMM-Newton and optical\nspectra from the Himalayan Chandra Telescope (HCT). Our study confirms that the\ndiffuse structure represents remnant emission from historic AGN activity,\nlikely associated with HCG15-D, some 80-86 Myr ago (based on ageing analysis).\nWe detect significant highly linearly-polarised emission from a diffuse\n'ridge'-like structure with a highly ordered magnetic field. Our analysis\nsuggests that this emission is generated by draping of magnetic field lines in\nthe intra-group medium (IGrM), although further exploration with simulations\nwould aid our understanding. We confirm that HCG15-C is a group-member galaxy.\nFinally, we report the detection of thermal emission associated with a\nbackground cluster at redshift z ~ 0.87 projected onto the IGrM of HCG15, which\nmatches the position and redshift of the recent SZ detection of ACT-CL\nJ0207.8+0209.",
        "Seniority-zero wavefunctions describe bond-breaking processes qualitatively.\nAs eigenvectors of a model Hamiltonian, Richardson-Gaudin states provide a\nclear physical picture and allow for systematic improvement via standard single\nreference approaches. Until now, this treatment has been done in the\nseniority-zero channel. In this manuscript, the corresponding states with\nhigher seniorities are identified, and their couplings through the Coulomb\nHamiltonian are computed. In every case, the couplings between the states are\ncomputed from the cofactors of their effective overlap matrix. Proof of\nprinciple calculations demonstrate that a single reference configuration\ninteraction is comparable with seniority-based configuration interaction\ncomputations at a substantially reduced cost. The next manuscript in this\nseries will identify the corresponding Slater-Condon rules and make the\ncomputations feasible."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"A primer on deep learning in genomics",
    "start_abstract":"Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
      ],
      "abstract":[
        "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Nonexistence of traveling wave solutions in the fractional Rosenau-Hyman\n  equation via homotopy perturbation method",
        "FORTE: An Open-Source System for Cost-Effective and Scalable\n  Environmental Monitoring",
        "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "Combinatorial construction of symplectic 6-manifolds via bifibration\n  structures",
        "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
        "Make Optimization Once and for All with Fine-grained Guidance",
        "Circular-polarization-selective perfect reflection from chiral\n  superconductors",
        "Forecasting Frontier Language Model Agent Capabilities",
        "Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation\n  Research",
        "Experimental demonstration of entanglement pumping with bosonic logical\n  qubits",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit\n  Copyright-Infringing Content from Large Language Models",
        "Primordial Black Hole Formation via Inverted Bubble Collapse",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "Reinforcement Learning for Quantum Control under Physical Constraints",
        "Melting of rods on a sphere via an intermediate hexatic phase",
        "Symmetric quasi-coherent sheaves",
        "Federated Dynamic Modeling and Learning for Spatiotemporal Data\n  Forecasting",
        "Strong Field QED, Astrophysics, and Laboratory Astrophysics",
        "Stellar occultation observations of (38628) Huya and its satellite: a\n  detailed look into the system",
        "Fluholoscopy. Compact and Simple Platform Combining Fluorescence and\n  Holographic Microscopy",
        "DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps",
        "Sequential HW-Aware Precoding: Over-the-air cancellation of HWI in\n  Downlink Cell-Free Massive MIMO with Serial Fronthaul",
        "Origin of time and probability in quantum cosmology",
        "The geometric impact of the quantum Hall interface on a cone",
        "Effective theories for nuclei at high energies",
        "Survey on Single-Image Reflection Removal using Deep Learning Techniques",
        "Artificially Generated Visual Scanpath Improves Multi-label Thoracic\n  Disease Classification in Chest X-Ray Images",
        "The (Exact) Price of Cardinality for Indivisible Goods: A Parametric\n  Perspective"
      ],
      "abstract":[
        "We apply the homotopy perturbation method to construct series solutions for\nthe fractional Rosenau-Hyman (fRH) equation and study their dynamics. Unlike\nthe classical RH equation where compactons arise from truncated periodic\nsolutions, we show that spatial nonlocality prevents the existence of\ncompactons, and therefore periodic traveling waves are considered. By\nasymptotic analyses involving the Mittag-Leffler function, it is shown that the\nquadratic fRH equation exhibits bifurcation with respect to the order of the\ntemporal fractional derivative, leading to the eventual pinning of wave\npropagation. Additionally, numerical results suggest potential finite time\nblow-up in the cubic fRH. While HPM proves effective in constructing analytic\nsolutions, we identify cases of divergence, underscoring the need for further\nresearch into its convergence properties and broader applicability.",
        "Forests are an essential part of our biosphere, regulating climate, acting as\na sink for greenhouse gases, and providing numerous other ecosystem services.\nHowever, they are negatively impacted by climatic stressors such as drought or\nheat waves. In this paper, we introduce FORTE, an open-source system for\nenvironmental monitoring with the aim of understanding how forests react to\nsuch stressors. It consists of two key components: (1) a wireless sensor\nnetwork (WSN) deployed in the forest for data collection, and (2) a Data\nInfrastructure for data processing, storage, and visualization. The WSN\ncontains a Central Unit capable of transmitting data to the Data Infrastructure\nvia LTE-M and several spatially independent Satellites that collect data over\nlarge areas and transmit them wirelessly to the Central Unit. Our prototype\ndeployments show that our solution is cost-effective compared to commercial\nsolutions, energy-efficient with sensor nodes lasting for several months on a\nsingle charge, and reliable in terms of data quality. FORTE's flexible\narchitecture makes it suitable for a wide range of environmental monitoring\napplications beyond forest monitoring. The contributions of this paper are\nthree-fold. First, we describe the high-level requirements necessary for\ndeveloping an environmental monitoring system. Second, we present an\narchitecture and prototype implementation of the requirements by introducing\nour FORTE platform and demonstrating its effectiveness through multiple field\ntests. Lastly, we provide source code, documentation, and hardware design\nartifacts as part of our open-source repository.",
        "Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR \/ +18.4% BEM over RAG for commenting in CoR.",
        "A bifibration structure on a $6$-manifold is a map to either the complex\nprojective plane $\\mathbb{P}^2$ or a $\\mathbb{P}^1$-bundle over $\\mathbb{P}^1$,\nsuch that its composition with the projection to $\\mathbb{P}^1$ is a\n($6$-dimensional) Lefschetz fibration\/pencil, and its restriction to the\npreimage of a generic $\\mathbb{P}^1$-fiber is also a ($4$-dimensional)\nLefschetz fibration\/pencil. This object has been studied by Auroux, Katzarkov,\nSeidel, among others. From a pair consisting of a monodromy representation of a\nLefschetz fibration\/pencil on a $4$-manifold and a relation in a braid group,\nwhich are mutually compatible in an appropriate sense, we construct a\nbifibration structure on a closed symplectic $6$-manifold, producing the given\ncompatible pair as its monodromies. We further establish methods for computing\ntopological invariants of symplectic $6$-manifolds, including Chern numbers,\nfrom compatible pairs. Additionally, we provide an explicit example of a\ncompatible pair, conjectured to correspond to a bifibration structure derived\nfrom the degree-$2$ Veronese embedding of the $3$-dimensional complex\nprojective space. This example can be viewed as a higher-dimensional analogue\nof the lantern relation in the mapping class group of the four-punctured\nsphere. Our results not only extend the applicability of combinatorial\ntechniques to higher-dimensional symplectic geometry but also offer a unified\nframework for systematically exploring symplectic $6$-manifolds.",
        "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https:\/\/github.com\/Eureka-Maggie\/MIGE.",
        "Learning to Optimize (L2O) enhances optimization efficiency with integrated\nneural networks. L2O paradigms achieve great outcomes, e.g., refitting\noptimizer, generating unseen solutions iteratively or directly. However,\nconventional L2O methods require intricate design and rely on specific\noptimization processes, limiting scalability and generalization. Our analyses\nexplore general framework for learning optimization, called Diff-L2O, focusing\non augmenting sampled solutions from a wider view rather than local updates in\nreal optimization process only. Meanwhile, we give the related generalization\nbound, showing that the sample diversity of Diff-L2O brings better performance.\nThis bound can be simply applied to other fields, discussing diversity,\nmean-variance, and different tasks. Diff-L2O's strong compatibility is\nempirically verified with only minute-level training, comparing with other\nhour-levels.",
        "Integrating mirrors with magnetic components is crucial for constructing\nchiral optical cavities, which provide tunable platforms for\ntime-reversal-asymmetric light-matter interactions. Here, we introduce\nsingle-crystal circular-polarization-selective mirrors based on chiral\nsuperconductors, which break time-reversal symmetry by themselves eliminating\nthe need for additional components. We show that a\ncircular-polarization-selective perfect reflection (CSPR) occurs for\nstrong-coupling superconductors in the BCS-BEC crossover regime or beyond if\nthe optical Hall conductivity is significant in the unit of conductivity\nquantum per unit layer, $e^2\/ha_z$, where $a_z$ is the lattice constant along\nthe surface normal. While the optical Hall conductivity in chiral\nsuperconductors is typically tiny, we classify three routes to obtain a large\nvalue. We demonstrate the significant optical Hall conductivity and the\nresulting CSPR with two examples: (1) superconductivity in doped quantum Hall\ninsulators and (2) chiral pairing that preserves the Bogoliubov Fermi surfaces\nin the weak-pairing limit. We also discuss the application of our theory to the\nrecently discovered chiral superconducting phase in rhombohedral graphene. Our\ntheory reveals the potential of these classes of chiral superconductors as\npromising elements for building high-quality-factor terahertz chiral cavities.",
        "As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.",
        "Effective strategies for sensor data management are essential for advancing\ntransportation research, especially in the current data-driven era, due to the\nadvent of novel applications in artificial intelligence. This paper presents\ncomprehensive guidelines for managing transportation sensor data, encompassing\nboth archived static data and real-time data streams. The real-time system\narchitecture integrates various applications with data acquisition systems\n(DAQ). By deploying the in-house designed, open-source Avena software platform\nalongside the NATS messaging system as a secure communication broker, reliable\ndata exchange is ensured. While robust databases like TimescaleDB facilitate\norganized storage, visualization platforms like Grafana provide real-time\nmonitoring capabilities.\n  In contrast, static data standards address the challenges in handling\nunstructured, voluminous datasets. The standards advocate for a combination of\ncost-effective bulk cloud storage for unprocessed sensor data and relational\ndatabases for recording summarized analyses. They highlight the role of cloud\ndata transfer tools like FME for efficient migration of sensor data from local\nstorages onto the cloud. Further, integration of robust visualization tools\ninto the framework helps in deriving patterns and trends from these complex\ndatasets.\n  The proposals were applied to INDOT's real-world case studies involving the\nI-65 and I-69 Greenfield districts. For real-time data collection, Campbell\nScientific DAQ systems were used, enabling continuous generation and monitoring\nof sensor metrics. In the case of the archived I-69 database, summary data was\ncompiled in Oracle, while the unprocessed data was stored in SharePoint. The\nresults underline the effectiveness of the proposed guidelines and motivate\ntheir adoption in research projects.",
        "Entanglement is crucial for quantum networks and computation, yet maintaining\nhigh-fidelity entangled quantum states is hindered by decoherence and\nresource-intensive purification methods. Here, we experimentally demonstrate\nentanglement pumping, utilizing bosonic quantum error correction (QEC) codes as\nlong-coherence-time storage qubits. By repetitively generating entanglement\nwith short-coherence-time qubits and injecting it into QEC-protected logical\nqubits, our approach effectively preserves entanglement. Through error\ndetection to discard error states and entanglement pumping to mitigate errors\nwithin the code space, we extend the lifespan of entangled logical qubits by\nnearly 50% compared to the case without entanglement pumping. This work\nhighlights the potential of bosonic logical qubits for scalable quantum\nnetworks and introduces a novel paradigm for efficient entanglement management.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "As the capabilities of large language models (LLMs) continue to expand, their\nusage has become increasingly prevalent. However, as reflected in numerous\nongoing lawsuits regarding LLM-generated content, addressing copyright\ninfringement remains a significant challenge. In this paper, we introduce\nPoisonedParrot: the first stealthy data poisoning attack that induces an LLM to\ngenerate copyrighted content even when the model has not been directly trained\non the specific copyrighted material. PoisonedParrot integrates small fragments\nof copyrighted text into the poison samples using an off-the-shelf LLM. Despite\nits simplicity, evaluated in a wide range of experiments, PoisonedParrot is\nsurprisingly effective at priming the model to generate copyrighted content\nwith no discernible side effects. Moreover, we discover that existing defenses\nare largely ineffective against our attack. Finally, we make the first attempt\nat mitigating copyright-infringement poisoning attacks by proposing a defense:\nParrotTrap. We encourage the community to explore this emerging threat model\nfurther.",
        "We propose a novel mechanism of primordial black hole (PBH) formation through\ninverted bubble collapse. In this scenario, bubbles nucleate sparsely in an\nincomplete first-order phase transition, followed by a bulk phase transition in\nthe rest of the universe that inverts these pre-existing bubbles into false\nvacuum regions. These spherically symmetric false-vacuum bubbles subsequently\ncollapse to form PBHs. Unlike conventional PBH formation mechanisms associated\nwith domain wall collapse or bubble coalescence, our inverted bubble collapse\nmechanism naturally ensures spherical collapse. We demonstrate that, when\napplied to the singlet extension of the Standard Model, this mechanism can\nproduce highly monochromatic PBHs with masses up to ${\\cal\nO}(10^{-7}\\,\\text{-}\\,10^{-5}) M_\\odot$, which potentially explain the\nmicrolensing events observed in the OGLE and Subaru HSC data.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "Quantum optimal control is concerned with the realisation of desired dynamics\nin quantum systems, serving as a linchpin for advancing quantum technologies\nand fundamental research. Analytic approaches and standard optimisation\nalgorithms do not yield satisfactory solutions for large quantum systems, and\nespecially not for real world quantum systems which are open and noisy. We\ndevise a physics-informed Reinforcement Learning (RL) algorithm that restricts\nthe space of possible solutions. We incorporate priors about the desired time\nscales of the quantum state dynamics - as well as realistic control signal\nlimitations - as constraints to the RL algorithm. These physics-informed\nconstraints additionally improve computational scalability by facilitating\nparallel optimisation. We evaluate our method on three broadly relevant quantum\nsystems (multi-level $\\Lambda$ system, Rydberg atom and superconducting\ntransmon) and incorporate real-world complications, arising from dissipation\nand control signal perturbations. We achieve both higher fidelities - which\nexceed 0.999 across all systems - and better robustness to time-dependent\nperturbations and experimental imperfections than previous methods. Lastly, we\ndemonstrate that incorporating multi-step feedback can yield solutions robust\neven to strong perturbations.",
        "We have studied, using molecular dynamics simulations, the pressure-induced\nmelting in a monolayer of soft repulsive spherocylinders whose centers of mass\nare constrained to move on the surface of a sphere. We show that the\norientational degrees of freedom of the spherocylinders exhibit nematic order,\nwhereas the positions of their centers of mass exhibit melting transitions that\ndepend on the radius of the confining spherical surface. Our system presents a\nunique scenario where the decoupling of the orientational degrees of freedom\nfrom the positional degrees of freedom leads to an effectively two-dimensional\n(2D) crystal-to-liquid transition on a spherical surface. Further study of the\nnature of this 2D melting on a sphere shows that the transition is a two-step\nprocess, and there exists a very small window of an intermediate hexatic phase\nbetween crystal and liquid phases. Similar results are found for flat\nmonolayers (with the radius of the sphere $ R \\rightarrow \\infty $). We show\nthat, interestingly, the structure of the defects, originating from the\ncurvature of the substrate, also changes during melting.",
        "Using methods of stable homotopy theory, the category of symmetric\nquasi-coherent sheaves associated with non-commutative graded algebras with\nextra symmetries is introduced and studied in this paper. It is shown to be a\nclosed symmetric monoidal Grothendieck category with invertible generators. It\nis proven that the category of quasi-coherent sheaves on a projective scheme is\nrecovered out of symmetric quasi-coherent sheaves.",
        "This paper presents an advanced Federated Learning (FL) framework for\nforecasting complex spatiotemporal data, improving upon recent state-of-the-art\nmodels. In the proposed approach, the original Gated Recurrent Unit (GRU)\nmodule within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent\nNetwork (DSTGCRN) modeling is first replaced with a Long Short-Term Memory\n(LSTM) network, enabling the resulting model to more effectively capture\nlong-term dependencies inherent to time series data. The resulting architecture\nsignificantly improves the model's capacity to handle complex temporal patterns\nin diverse forecasting applications. Furthermore, the proposed FL framework\nintegrates a novel Client-Side Validation (CSV) mechanism, introducing a\ncritical validation step at the client level before incorporating aggregated\nparameters from the central server into local models. This ensures that only\nthe most effective updates are adopted, improving both the robustness and\naccuracy of the forecasting model across clients. The efficiency of our\napproach is demonstrated through extensive experiments on real-world\napplications, including public datasets for multimodal transport demand\nforecasting and private datasets for Origin-Destination (OD) matrix forecasting\nin urban areas. The results demonstrate substantial improvements over\nconventional methods, highlighting the framework's ability to capture complex\nspatiotemporal dependencies while preserving data privacy. This work not only\nprovides a scalable and privacy-preserving solution for real-time,\nregion-specific forecasting and management but also underscores the potential\nof leveraging distributed data sources in a FL context. We provide our\nalgorithms as open-source on GitHub.",
        "Astrophysical compact objects, such as magnetars, neutron star mergers, etc,\nhave strong electromagnetic fields beyond the Schwinger field ($B_c = 4.4\n\\times 10^{13}\\, {\\rm G}$). In strong electric fields, electron-positron pairs\nare produced from the vacuum, gamma rays create electron-positron pairs in\nstrong magnetic fields, and propagating photons experience vacuum refringence,\netc. Astrophysical compact objects with strong electromagnetic fields open a\nwindow for probing fundamental physics beyond weak field QED. Ultra-intense\nlasers and high-energy charged particles may simulate extreme astrophysical\nphenomena.",
        "The physical and orbital parameters of Trans-Neptunian Objects (TNOs) provide\nvaluable information about the Solar System's formation and evolution. In\nparticular, the characterization of binaries provides insights into the\nformation mechanisms that may be playing a role at such large distances from\nthe Sun. Studies show two distinct populations, and (38628) Huya occupies an\nintermediate position between the unequal-size binaries and those with\ncomponents of roughly equal sizes. In this work, we predicted and observed\nthree stellar occultation events by Huya. Huya and its satellite - S\/2012\n(38628) 1 - were detected during occultations in March 2021 and again in June\n2023. Additionally, an attempt to detect Huya in February 2023 resulted in an\nadditional single-chord detection of the secondary. A spherical body with a\nminimum diameter of D = 165 km can explain the three single-chord observations\nand provide a lower limit for the satellite size. The astrometry of Huya's\nsystem, as derived from the occultations and supplemented by observations from\nthe Hubble Space Telescope and Keck Observatory, provided constraints on the\nsatellite orbit and the mass of the system. Therefore, assuming the secondary\nis in an equatorial orbit around the primary, the limb fitting was constrained\nby the satellite orbit position angle. The system density, calculated by\nsumming the most precise measurement of Huya's volume to the spherical\nsatellite average volume, is $\\rho_{1}$ = 1073 $\\pm$ 66 kg m$^{-3}$. The\ndensity that the object would have assuming a Maclaurin equilibrium shape with\na rotational period of 6.725 $\\pm$ 0.01 hours is $\\rho_{2}$ = 768 $\\pm$ 42 kg\nm$^{-3}$. This difference rules out the Maclaurin equilibrium assumption for\nthe main body shape.",
        "The combination of different imaging modalities into single imaging platforms\nhas a strong potential in biomedical sciences since it permits the analysis of\ncomplementary properties of the target sample. Here, we report on an extremely\nsimple, cost-effective, and compact microscope platform for achieving\nsimultaneous fluorescence and quantitative phase imaging modes with the\ncapability of working in a single snapshot. It is based on the use of a single\nillumination wavelength to both excite the sample fluorescence and provide\ncoherent illumination for phase imaging. After passing the microscope layout,\nthe two imaging paths are separated by using a bandpass filter and the two\nimaging modes are simultaneously obtained by using two digital cameras. We\nfirst present calibration and analysis of both fluorescence and phase imaging\nmodalities working independently and, later on, experimental validation for the\nproposed common-path dual-mode imaging platform considering static (resolution\ntest targets, fluorescent micro-beads and water-suspended lab-made cultures) as\nwell as dynamic (flowing fluorescent beads, human sperm cells and live\nspecimens from lab-made cultures) samples.",
        "The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https:\/\/github.com\/Noodulz\/dejAIvu.",
        "This paper addresses the critical challenge of mitigating hardware\nimpairments (HWIs) in downlink cell-free massive MIMO (CF-mMIMO) networks while\nensuring computational scalability. We propose a novel sequential\nhardware-aware (HW-aware) precoding technique that leverages the serial\nfronthaul topology to perform over-the-air HWI cancellation. This approach\ninvolves sequentially exchanging approximated user-perceived distortion\ninformation among successive access points (APs) for over-the-air HWI\nmitigation. Each AP independently computes its spatial multiplexing weights and\ntransmits signals that counteract the distortions introduced by the preceding\nAP. We develop a problem formulation and present a closed-form solution for\nthis method. For performance evaluation, we study two reference methods taken\neither from centralized massive MIMO literature (Tone Reservation [TR]) or\ntailored for CF-mMIMO networks (PAPR-aware precoding), both focusing on\nreducing the PAPR of OFDM signals in the downlink. Results indicate that the\nsequential HW-aware approach achieves a substantial increase in spectral\nefficiency (SE) in high-distortion scenarios, with an average SE increase\nfactor of 1.8 under severe distortions. Additionally, the proposed method,\nwhich is executed locally, demonstrates better scalability, achieving a\nreduction of up to 40\\% and 72\\% in the total number of complex multiplications\ncompared to the PAPR-aware and TR approaches, respectively. Finally, the\nsequential HW-aware precoder offers high performance even when applied to\ncost-effective APs with few antennas, presenting a promising and practical\nsolution for HWI compensation in CF-mMIMO systems with serial fronthaul.",
        "We discuss how the classical notions of time and causal structure may emerge\ntogether with quantum-mechanical probabilities from a universal quantum state.\nFor this, the process of decoherence between semiclassical branches is\nimportant. Our discussion is based on quantum geometrodynamics, a canonical\napproach to quantum gravity. In this framework, a particular boundary condition\nmay illuminate the issue of the arrow of (classical) time in connection to the\ngrowth of entanglement entropy.",
        "Recently, quantum Hall interface has become a popular subject of research;\ndistinct from that of the quantum Hall edge, which is constrained by external\nbackground confinement, the interface has the freedom to move, likely towards a\nstring-like state. In disk geometry, it was known that the interface energy has\nan extra correction due to its curvature which depends on the size of the disk.\nIn this work, we analytically calculate the energy of the integer quantum Hall\ninterface on a cone surface which has the advantage that its curvature is more\neasily adjustable. By tuning the length and curvature of the interface by the\ncone angle parameter $\\beta$, we analyze the dependence of the quantum Hall\ninterface energy on the curvature and verify this geometric correction.\nMoreover, we find that the tip of the cone geometry has an extra contribution\nto the energy that reflects on the $u_2,u_4$ term.",
        "We discuss the application of the Color Glass Condensate (CGC), an effective\nfield theory of Quantum Chromodynamics (QCD), to describe high-energy nuclear\ninteractions. We first provide an introduction to the methods and language of\nthe CGC, its role in understanding gluon saturation in heavy-ion collisions at\nthe LHC and RHIC, and its relevance in various scattering processes such as\nDeep Inelastic Scattering (DIS). The application of the CGC effective field\ntheory to describe hadron-hadron collisions is discussed in the scope of\nasymmetric \\textit{dilute-dense} collisions, and Heavy-Ion Collisions in the\n\\textit{dense-dense} limit. The review covers theoretical foundations, recent\nadvancements, and phenomenological applications, focusing on using the CGC to\ndetermine the initial conditions of heavy-ion collisions.",
        "The phenomenon of reflection is quite common in digital images, posing\nsignificant challenges for various applications such as computer vision,\nphotography, and image processing. Traditional methods for reflection removal\noften struggle to achieve clean results while maintaining high fidelity and\nrobustness, particularly in real-world scenarios. Over the past few decades,\nnumerous deep learning-based approaches for reflection removal have emerged,\nyielding impressive results. In this survey, we conduct a comprehensive review\nof the current literature by focusing on key venues such as ICCV, ECCV, CVPR,\nNeurIPS, etc., as these conferences and journals have been central to advances\nin the field. Our review follows a structured paper selection process, and we\ncritically assess both single-stage and two-stage deep learning methods for\nreflection removal. The contribution of this survey is three-fold: first, we\nprovide a comprehensive summary of the most recent work on single-image\nreflection removal; second, we outline task hypotheses, current deep learning\ntechniques, publicly available datasets, and relevant evaluation metrics; and\nthird, we identify key challenges and opportunities in deep learning-based\nreflection removal, highlighting the potential of this rapidly evolving\nresearch area.",
        "Expert radiologists visually scan Chest X-Ray (CXR) images, sequentially\nfixating on anatomical structures to perform disease diagnosis. An automatic\nmulti-label classifier of diseases in CXR images can benefit by incorporating\naspects of the radiologists' approach. Recorded visual scanpaths of\nradiologists on CXR images can be used for the said purpose. But, such\nscanpaths are not available for most CXR images, which creates a gap even for\nmodern deep learning based classifiers. This paper proposes to mitigate this\ngap by generating effective artificial visual scanpaths using a visual scanpath\nprediction model for CXR images. Further, a multi-class multi-label classifier\nframework is proposed that uses a generated scanpath and visual image features\nto classify diseases in CXR images. While the scanpath predictor is based on a\nrecurrent neural network, the multi-label classifier involves a novel iterative\nsequential model with an attention module. We show that our scanpath predictor\ngenerates human-like visual scanpaths. We also demonstrate that the use of\nartificial visual scanpaths improves multi-class multi-label disease\nclassification results on CXR images. The above observations are made from\nexperiments involving around 0.2 million CXR images from 2 widely-used datasets\nconsidering the multi-label classification of 14 pathological findings. Code\nlink: https:\/\/github.com\/ashishverma03\/SDC",
        "We adopt a parametric approach to analyze the worst-case degradation in\nsocial welfare when the allocation of indivisible goods is constrained to be\nfair. Specifically, we are concerned with cardinality-constrained allocations,\nwhich require that each agent has at most $k$ items in their allocated bundle.\nWe propose the notion of the price of cardinality, which captures the\nworst-case multiplicative loss of utilitarian or egalitarian social welfare\nresulting from imposing the cardinality constraint. We then characterize tight\nor almost-tight bounds on the price of cardinality as exact functions of the\ninstance parameters, demonstrating how the social welfare improves as $k$ is\nincreased. In particular, one of our main results refines and generalizes the\nexisting asymptotic bound on the price of balancedness, as studied by Bei et\nal. [BLMS21]. We also further extend our analysis to the problem where the\nitems are partitioned into disjoint categories, and each category has its own\ncardinality constraint. Through a parametric study of the price of cardinality,\nwe provide a framework which aids decision makers in choosing an ideal level of\ncardinality-based fairness, using their knowledge of the potential loss of\nutilitarian and egalitarian social welfare."
      ]
    }
  },
  {
    "id":2411.08664,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Comprehensive Inorganic Chemistry III (Third Edition)",
    "start_abstract":"Comprehensive Inorganic Chemistry III, a ten-volume reference work, is intended to cover fundamental principles, recent discoveries, and significant applications of elements and their compounds. Authored by renowned experts in the field and edited by a world-class editorial board, each chapter provides a thorough and in-depth overview of the topic covered, featuring resources which will be useful to students, researchers, faculty as well as those in the industry. Comprehensive Inorganic Chemistry III focuses on main group chemistry, biological inorganic chemistry, solid state and materials chemistry, catalysis, and new developments in electrochemistry and photochemistry, as well as NMR and diffraction methods for studying inorganic compounds. The work expands on our 2013 work Comprehensive Inorganic Chemistry II while also adding new volumes on cutting-edge research areas and techniques for studying inorganic compounds. Researchers seeking background information on a specific problem involving the synthesis of inorganic compounds, as well as applications for numerous elements from the periodic table, and their compounds, will be able to rely on and refer to this authoritative scientific resource time and again.",
    "start_categories":[
      "Material"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Multimodal Foundation Models for Material Property Prediction and Discovery"
      ],
      "abstract":[
        "Artificial intelligence is transforming computational materials science, improving the prediction of material properties, and accelerating the discovery of novel materials. Recently, publicly available material data repositories have grown rapidly. This growth encompasses not only more materials but also a greater variety and quantity of their associated properties. Existing machine learning efforts in materials science focus primarily on single-modality tasks, i.e. relationships between materials and a single physical property, thus not taking advantage of the rich and multimodal set of material properties. Here, we introduce Multimodal Learning for Materials (MultiMat), which enables self-supervised multi-modality training of foundation models for materials. We demonstrate our framework's potential using data from the Materials Project database on multiple axes: (i) MultiMat achieves state-of-the-art performance for challenging material property prediction tasks; (ii) MultiMat enables novel and accurate material discovery via latent space similarity, enabling screening for stable materials with desired properties; and (iii) MultiMat encodes interpretable emergent features that may provide novel scientific insights."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Group Shapley with Robust Significance Testing and Its Application to\n  Bond Recovery Rate Prediction",
        "Streaming Self-Corrected Dual-Comb Spectrometer",
        "A Scenario Analysis of Ethical Issues in Dark Patterns and Their\n  Research",
        "On the lattice formulation of the union-closed sets conjecture",
        "Generative AI for Vision: A Comprehensive Study of Frameworks and\n  Applications",
        "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
        "Dynamical Landauer principle: Thermodynamic criteria of transmitting\n  classical information",
        "Mitigation of birefringence in cavity-based quantum networks using\n  frequency-encoded photons",
        "Efficient Finetuning for Dimensional Speech Emotion Recognition in the\n  Age of Transformers",
        "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs",
        "Nanosatellite Constellation and Ground Station Co-design for Low-Latency\n  Critical Event Detection",
        "Robust Body Composition Analysis by Generating 3D CT Volumes from\n  Limited 2D Slices",
        "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed\n  Knowledge Distillation",
        "Asymmetrical Latent Representation for Individual Treatment Effect\n  Modeling",
        "Probing negative differential resistance in silicon with a P-I-N\n  diode-integrated T center ensemble",
        "Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN-LSTM\n  With WOA-GWO Parameter Optimization: Global COVID-19 Case Study",
        "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model\n  Merging",
        "Center-guided Classifier for Semantic Segmentation of Remote Sensing\n  Images",
        "A dynamic copula model for probabilistic forecasting of non-Gaussian\n  multivariate time series",
        "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
        "Black holes in thermal bath live shorter: implications for primordial\n  black holes",
        "Employing deep-learning techniques for the conservative-to-primitive\n  recovery in binary neutron star simulations",
        "Pure momentum-shift bulk photovoltaic effect in ferroelectric flat-band\n  Mott insulators",
        "Leveraging Large Language Models for Building Interpretable Rule-Based\n  Data-to-Text Systems",
        "Exoplanet Occurrence Rate with Age for FGK Stars in Kepler",
        "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
        "Model-based Elaboration of a Requirements and Design Pattern Catalogue\n  for Sustainable Systems",
        "MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric\n  Grids based Implicit Neural Representation",
        "Hitting probabilities, thermal capacity, and Hausdorff dimension results\n  for the Brownian sheet"
      ],
      "abstract":[
        "We propose Group Shapley, a metric that extends the classical\nindividual-level Shapley value framework to evaluate the importance of feature\ngroups, addressing the structured nature of predictors commonly found in\nbusiness and economic data. More importantly, we develop a significance testing\nprocedure based on a three-cumulant chi-square approximation and establish the\nasymptotic properties of the test statistics for Group Shapley values. Our\napproach can effectively handle challenging scenarios, including sparse or\nskewed distributions and small sample sizes, outperforming alternative tests\nsuch as the Wald test. Simulations confirm that the proposed test maintains\nrobust empirical size and demonstrates enhanced power under diverse conditions.\nTo illustrate the method's practical relevance in advancing Explainable AI, we\napply our framework to bond recovery rate predictions using a global dataset\n(1996-2023) comprising 2,094 observations and 98 features, grouped into 16\nsubgroups and five broader categories: bond characteristics, firm fundamentals,\nindustry-specific factors, market-related variables, and macroeconomic\nindicators. Our results identify the market-related variables group as the most\ninfluential. Furthermore, Lorenz curves and Gini indices reveal that Group\nShapley assigns feature importance more equitably compared to individual\nShapley values.",
        "Here, we radically simplify coherently averaged dual-comb spectroscopy by\nintroducing a real-time self-correction system: a radio frequency\nsystem-on-chip computes each incoming dual-comb interferogram's phase,\nfrequency, and arrival time, calculates changes in the combs' carrier-envelope\noffset frequencies and repetition rates, and immediately phase-corrects the\nincoming interferogram data stream. The algorithm supports up to 0.3 GHz\ninterferogram frequency bandwidth and thus combines fast measurement times\n(corresponding to high detunings) with broadband optical detection. Using the\nsystem, we achieve comb-resolved spectroscopy with Fourier-limited linewidth,\ncoherent averaging over arbitrarily long durations, and a signal-to-noise ratio\nof up to 2100. Iodine and acetylene spectroscopy yields excellent agreement\nwith literature over an optical bandwidth of more than 10 THz in the visible\nand near-infrared. Our approach only requires three optical and three\nelectronic components and makes instantaneous dual-comb spectroscopy available\nto everyday applications.",
        "Context: Dark patterns are user interface or other software designs that\ndeceive or manipulate users to do things they would not otherwise do. Even\nthough dark patterns have been under active research for a long time, including\nparticularly in computer science but recently also in other fields such as law,\nsystematic applied ethical assessments have generally received only a little\nattention. Objective: The present work evaluates ethical concerns in dark\npatterns and their research in software engineering and closely associated\ndisciplines. The evaluation is extended to cover not only dark patterns\nthemselves but also the research ethics and applied ethics involved in\nstudying, developing, and deploying them. Method: A scenario analysis is used\nto evaluate six theoretical dark pattern scenarios. The ethical evaluation is\ncarried out by focusing on the three main branches of normative ethics;\nutilitarianism, deontology, and virtue ethics. In terms of deontology, the\nevaluation is framed and restricted to the laws enacted in the European Union.\nResults: The evaluation results indicate that dark patterns are not universally\nmorally bad. That said, numerous ethical issues with practical relevance are\ndemonstrated and elaborated. Some of these may have societal consequences.\nConclusion: Dark patterns are ethically problematic but not always. Therefore,\nethical assessments are necessary. The two main theoretical concepts behind\ndark patterns, deception and manipulation, lead to various issues also in\nresearch ethics. It can be recommended that dark patterns should be evaluated\non case-by-case basis, considering all of the three main branches of normative\nethics in an evaluation. Analogous points apply to legal evaluations,\nespecially when considering that the real or perceived harms caused by dark\npatterns cover both material and non-material harms to natural persons.",
        "The union-closed sets conjecture, also known as Frankl's conjecture, is a\nwell-studied problem with various formulations. In terms of lattices, the\nconjecture states that every finite lattice $L$ with more than one element\ncontains a join-irreducible element that is less than or equal to at most half\nof the elements in $L$. In this work, we obtain several necessary conditions\nfor any counterexample $\\tilde{L}$ of minimum size.",
        "Generative AI is transforming image synthesis, enabling the creation of\nhigh-quality, diverse, and photorealistic visuals across industries like\ndesign, media, healthcare, and autonomous systems. Advances in techniques such\nas image-to-image translation, text-to-image generation, domain transfer, and\nmultimodal alignment have broadened the scope of automated visual content\ncreation, supporting a wide spectrum of applications. These advancements are\ndriven by models like Generative Adversarial Networks (GANs), conditional\nframeworks, and diffusion-based approaches such as Stable Diffusion. This work\npresents a structured classification of image generation techniques based on\nthe nature of the input, organizing methods by input modalities like noisy\nvectors, latent representations, and conditional inputs. We explore the\nprinciples behind these models, highlight key frameworks including DALL-E,\nControlNet, and DeepSeek Janus-Pro, and address challenges such as\ncomputational costs, data biases, and output alignment with user intent. By\noffering this input-centric perspective, this study bridges technical depth\nwith practical insights, providing researchers and practitioners with a\ncomprehensive resource to harness generative AI for real-world applications.",
        "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
        "Transmitting energy and information are two essential aspects of nature.\nRecent findings suggest they are closely related, while a quantitative\nequivalence between them is still unknown. This thus motivates us to ask: Can\ninformation transmission tasks equal certain energy transmission tasks? We\nanswer this question positively by bounding various one-shot classical\ncapacities via different energy transmission tasks. Such bounds provide the\nphysical implication that, in the one-shot regime, transmitting $n$ bits of\nclassical information is equivalent to $n\\times k_BT\\ln2$ transmitted energy.\nUnexpectedly, these bounds further uncover a dynamical version of Landauer's\nprinciple, showing the strong link between \"transmitting\" (rather than\n\"erasing\") information and energy. Finally, in the asymptotic regime, our\nfindings further provide thermodynamic meanings for\nHolevo-Schumacher-Westmoreland Theorem and a series of strong converse\nproperties as well as no-go theorems.",
        "Atom-cavity systems offer unique advantages for building large-scale\ndistributed quantum computers by providing strong atom-photon coupling while\nallowing for high-fidelity local operations of atomic qubits. However, in\nprevalent schemes where the photonic state is encoded in polarization, cavity\nbirefringence introduces an energy splitting of the cavity eigenmodes and\nalters the polarization states, thus limiting the fidelity of remote\nentanglement generation. To address this challenge, we propose a scheme that\nencodes the photonic qubit in the frequency degree-of-freedom. The scheme\nrelies on resonant coupling of multiple transverse cavity modes to different\natomic transitions that are well-separated in frequency. We numerically\ninvestigate the temporal properties of the photonic wavepacket, two-photon\ninterference visibility, and atom-atom entanglement fidelity under various\ncavity polarization-mode splittings and find that our scheme is less affected\nby cavity birefringence. Finally, we propose practical implementations in two\ntrapped ion systems, using the fine structure splitting in the metastable D\nstate of $\\mathrm{^{40}Ca^{+}}$, and the hyperfine splitting in the ground\nstate of $\\mathrm{^{225}Ra^{+}}$. Our study presents an alternative approach\nfor cavity-based quantum networks that is less sensitive to birefringent\neffects, and is applicable to a variety of atomic and solid-state\nemitter-cavity interfaces.",
        "Accurate speech emotion recognition is essential for developing human-facing\nsystems. Recent advancements have included finetuning large, pretrained\ntransformer models like Wav2Vec 2.0. However, the finetuning process requires\nsubstantial computational resources, including high-memory GPUs and significant\nprocessing time. As the demand for accurate emotion recognition continues to\ngrow, efficient finetuning approaches are needed to reduce the computational\nburden. Our study focuses on dimensional emotion recognition, predicting\nattributes such as activation (calm to excited) and valence (negative to\npositive). We present various finetuning techniques, including full finetuning,\npartial finetuning of transformer layers, finetuning with mixed precision,\npartial finetuning with caching, and low-rank adaptation (LoRA) on the Wav2Vec\n2.0 base model. We find that partial finetuning with mixed precision achieves\nperformance comparable to full finetuning while increasing training speed by\n67%. Caching intermediate representations further boosts efficiency, yielding\nan 88% speedup and a 71% reduction in learnable parameters. We recommend\nfinetuning the final three transformer layers in mixed precision to balance\nperformance and training efficiency, and adding intermediate representation\ncaching for optimal speed with minimal performance trade-offs. These findings\nlower the barriers to finetuning speech emotion recognition systems, making\naccurate emotion recognition more accessible to a broader range of researchers\nand practitioners.",
        "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured\/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.",
        "Advancements in nanosatellite technology lead to more Earth-observation\nsatellites in low-Earth orbit. We explore using nanosatellite constellations to\nachieve low-latency detection for time-critical events, such as forest fires,\noil spills, and floods. The detection latency comprises three parts: capture,\ncompute and transmission. Previous solutions reduce transmission latency, but\nwe find that the bottleneck is capture latency, accounting for more than 90% of\nend-to-end latency. We present a measurement study on how various satellite and\nground station design factors affect latency. We offer design guidance to\noperators on how to choose satellite orbital configurations and design an\nalgorithm to choose ground station locations. For six use cases, our design\nguidance reduces end-to-end latency by 5.6 to 8.2 times compared to the\nexisting system.",
        "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%.",
        "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect or ungrounded content, which limits their reliability in\nhigh-stakes applications. A key factor contributing to hallucination is the use\nof hard labels during training, which enforce deterministic supervision,\nencourage overconfidence, and disregard the uncertainty inherent in natural\nlanguage. To address this, we propose mitigating hallucination through\nknowledge distillation (KD), where a teacher model provides smoothed soft\nlabels to a student model, reducing overconfidence and improving factual\ngrounding. We apply KD during supervised finetuning on instructional data,\nevaluating its effectiveness across LLMs from different families. Experimental\nresults on summarization benchmarks demonstrate that KD reduces hallucination\ncompared to standard finetuning while preserving performance on general NLP\ntasks. These findings highlight KD as a promising approach for mitigating\nhallucination in LLMs and improving model reliability.",
        "Conditional Average Treatment Effect (CATE) estimation, at the heart of\ncounterfactual reasoning, is a crucial challenge for causal modeling both\ntheoretically and applicatively, in domains such as healthcare, sociology, or\nadvertising. Borrowing domain adaptation principles, a popular design maps the\nsample representation to a latent space that balances control and treated\npopulations while enabling the prediction of the potential outcomes. This paper\npresents a new CATE estimation approach based on the asymmetrical search for\ntwo latent spaces called Asymmetrical Latent Representation for Individual\nTreatment Effect (ALRITE), where the two latent spaces are respectively\nintended to optimize the counterfactual prediction accuracy on the control and\nthe treated samples. Under moderate assumptions, ALRITE admits an upper bound\non the precision of the estimation of heterogeneous effects (PEHE), and the\napproach is empirically successfully validated compared to the state-of-the-art",
        "The T center in silicon has recently emerged as a promising candidate for\nscalable quantum technologies, due to its telecommunications band optical\ntransition and microwave addressable ground state spin. The immense promise of\nthe T center is driven by its silicon host material; silicon is by far the most\nmature, manufacturable semiconductor material for integrated photonic and\nelectronic devices. Here, we present the first study of T-centers in an\nelectrical device. We study an ensemble of T centers coupled to a buried\nlateral P-I-N diode in silicon, observing the T-center's optical response to\nstatic and dynamic electric fields. We utilize the defect's optical response as\na probe of device nonlinearity, observing a phase transition of the carrier\ndensity into a stable oscillatory regime characteristic of negative\ndifferential resistance. These findings provide fundamental insight into the\nphysics of the T-center for improved quantum device performance and open a\npromising new direction for defect-based local quantum sensing in semiconductor\ndevices.",
        "Effective epidemic modeling is essential for managing public health crises,\nrequiring robust methods to predict disease spread and optimize resource\nallocation. This study introduces a novel deep learning framework that advances\ntime series forecasting for infectious diseases, with its application to COVID\n19 data as a critical case study. Our hybrid approach integrates Convolutional\nNeural Networks (CNNs) and Long Short Term Memory (LSTM) models to capture\nspatial and temporal dynamics of disease transmission across diverse regions.\nThe CNN extracts spatial features from raw epidemiological data, while the LSTM\nmodels temporal patterns, yielding precise and adaptable predictions. To\nmaximize performance, we employ a hybrid optimization strategy combining the\nWhale Optimization Algorithm (WOA) and Gray Wolf Optimization (GWO) to fine\ntune hyperparameters, such as learning rates, batch sizes, and training epochs\nenhancing model efficiency and accuracy. Applied to COVID 19 case data from 24\ncountries across six continents, our method outperforms established benchmarks,\nincluding ARIMA and standalone LSTM models, with statistically significant\ngains in predictive accuracy (e.g., reduced RMSE). This framework demonstrates\nits potential as a versatile method for forecasting epidemic trends, offering\ninsights for resource planning and decision making in both historical contexts,\nlike the COVID 19 pandemic, and future outbreaks.",
        "While most current approaches rely on further training techniques, such as\nfine-tuning or reinforcement learning, to enhance model capacities, model\nmerging stands out for its ability of improving models without requiring any\nadditional training. In this paper, we propose a unified framework for model\nmerging based on low-rank estimation of task vectors without the need for\naccess to the base model, named \\textsc{LoRE-Merging}. Our approach is\nmotivated by the observation that task vectors from fine-tuned models\nfrequently exhibit a limited number of dominant singular values, making\nlow-rank estimations less prone to interference. We implement the method by\nformulating the merging problem as an optimization problem. Extensive empirical\nexperiments demonstrate the effectiveness of our framework in mitigating\ninterference and preserving task-specific information, thereby advancing the\nstate-of-the-art performance in model merging techniques.",
        "Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps:\/\/github.com\/xwmaxwma\/rssegmentation.",
        "Multivariate time series (MTS) data often include a heterogeneous mix of\nnon-Gaussian distributional features (asymmetry, multimodality, heavy tails)\nand data types (continuous and discrete variables). Traditional MTS methods\nbased on convenient parametric distributions are typically ill-equipped to\nmodel this heterogeneity. Copula models provide an appealing alternative, but\npresent significant obstacles for fully Bayesian inference and probabilistic\nforecasting. To overcome these challenges, we propose a novel and general\nstrategy for posterior approximation in MTS copula models and apply it to a\nGaussian copula built from a dynamic factor model. This framework provides\nscalable, fully Bayesian inference for cross-sectional and serial dependencies\nand nonparametrically learns heterogeneous marginal distributions. We validate\nthis approach by establishing posterior consistency and confirm excellent\nfinite-sample performance even under model misspecification using simulated\ndata. We apply our method to crime count and macroeconomic MTS data and find\nsuperior probabilistic forecasting performance compared to popular MTS models.\nThese results demonstrate that the proposed method is a versatile,\ngeneral-purpose utility for probabilistic forecasting of MTS that works well\nacross of range of applications with minimal user input.",
        "As LLM-as-a-Judge emerges as a new paradigm for assessing large language\nmodels (LLMs), concerns have been raised regarding the alignment, bias, and\nstability of LLM evaluators. While substantial work has focused on alignment\nand bias, little research has concentrated on the stability of LLM evaluators.\nIn this paper, we conduct extensive experiments involving 9 widely used LLM\nevaluators across 2 different evaluation settings to investigate the\nuncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators\nexhibit varying uncertainty based on model families and sizes. With careful\ncomparative analyses, we find that employing special prompting strategies,\nwhether during inference or post-training, can alleviate evaluation uncertainty\nto some extent. By utilizing uncertainty to enhance LLM's reliability and\ndetection capability in Out-Of-Distribution (OOD) data, we further fine-tune an\nuncertainty-aware LLM evaluator named ConfiLM using a human-annotated\nfine-tuning set and assess ConfiLM's OOD evaluation ability on a manually\ndesigned test set sourced from the 2024 Olympics. Experimental results\ndemonstrate that incorporating uncertainty as additional information during the\nfine-tuning phase can largely improve the model's evaluation performance in OOD\nscenarios. The code and data are released at:\nhttps:\/\/github.com\/hasakiXie123\/LLM-Evaluator-Uncertainty.",
        "Hawking radiation from a non-extremal black hole is known to be approximately\nPlanckian. The thermal spectrum receives multiple corrections including\ngreybody factors and due to kinematical restrictions on the infrared and\nultraviolet frequencies. We show that another significant correction to the\nspectrum arises if the black hole is assumed to live in a thermal bath and the\nemitted radiation gets thermalised at the bath temperature. This modification\nreshapes the thermal spectrum, and leads to appreciable deviation from standard\nresults including modification in the decay rate of black holes. We argue that\nthis altered decay rate has significance for cosmology and, in a realistic\nsetting, show that it alters the life time of primordial black holes (PBHs) in\nthe early universe. In particular, the very light PBHs formed right after the\nend of inflation decay faster which may have interesting phenomenological\nimplications.",
        "The detection of GW170817, together with its electromagnetic counterparts,\nhas proven that binary neutron star mergers are of central importance to the\nfield of nuclear astrophysics, e.g., through a better understanding of the\nformation of elements and novel constraints on the supranuclear dense equation\nof state governing the matter inside neutron stars. Essential for understanding\nthe binary coalescence are numerical-relativity simulations, which typically\ncome with high computational costs requiring high-performance computing\nfacilities. In this work, we build on recent studies to investigate whether\nnovel techniques, such as neural networks, can be employed in the conversion of\nconservative variables to primitive hydrodynamical variables, such as pressure\nand density. In this regard, we perform -- to the best of our knowledge -- the\nfirst binary neutron star merger simulations in which such methods are\nemployed. We show that this method results in stable simulations, reaching\naccuracies similar to traditional methods with an overall comparable\ncomputational cost. These simulations serve as a proof of principle that, in\nthe future, deep learning techniques could be used within numerical-relativity\nsimulations. However, further improvements are necessary to offer a\ncomputational advantage compared to traditional methods.",
        "The shift current photovoltaic effect is conventionally understood as the\nreal-space displacement of a wave packet induced by photoexcitation. However,\nthis interpretation becomes insufficient in flat-band systems, where\nquasiparticles are too massive to accelerate in real space under the optical\nelectric field. Here, we developed a physically consistent method to decompose\nthe shift current into real-space and momentum-space components. A surprising\npure momentum-space shift current is found theoretically in flat-band Mott\ninsulator Nb$_3$X$_8$ (X = Cl, Br, I) monolayers. This work underscores that\nsignificant shift current responses can emerge even in systems with minimal\ninterband polarization differences, highlighting the potential for exploring\nnovel bulk photovoltaic effects in flat-band Mott insulators.",
        "We introduce a simple approach that uses a large language model (LLM) to\nautomatically implement a fully interpretable rule-based data-to-text system in\npure Python. Experimental evaluation on the WebNLG dataset showed that such a\nconstructed system produces text of better quality (according to the BLEU and\nBLEURT metrics) than the same LLM prompted to directly produce outputs, and\nproduces fewer hallucinations than a BART language model fine-tuned on the same\ndata. Furthermore, at runtime, the approach generates text in a fraction of the\nprocessing time required by neural approaches, using only a single CPU",
        "We measure exoplanet occurrence rate as a function of isochrone and\ngyrochronology ages using confirmed and candidate planets identified in Q1-17\nDR25 Kepler data. We employ Kepler's pipeline detection efficiency to correct\nfor the expected number of planets in each age bin. We examine the occurrence\nrates for planets with radii $0.2 \\leq Rp \\leq 20$ R$_\\oplus$ and orbital\nperiods $0.2 \\leq P \\leq 100$ days for FGK stars with ages between $1.5-8$ Gyr\nusing the inverse detection efficiency method. We find no significant trend\nbetween occurrence rate and stellar ages; a slight, decreasing trend (within\n$1.5-2.5$ $\\sigma$) only emerges for low-mass and metal-rich stars that\ndominate our sample. We isolate the effects of mass and metallicity on the\noccurrence rate trend with age, but find the results to be inconclusive due to\nweak trends and small sample size. Our results hint that the exoplanet\noccurrence rate may decrease over time due to dynamical instability from\nplanet-planet scattering or planet ejection, but accurate ages and larger\nsample sizes are needed to resolve a clear relation between occurrence rate and\nage.",
        "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\n$\\textbf{MagicID}$, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
        "Designing sustainable systems involves complex interactions between\nenvironmental resources, social impact\/adoption, and financial costs\/benefits.\nIn a constrained world, achieving a balanced design across those dimensions has\nbecome challenging. However a number of strategies have emerged to tackle\nspecific aspects such as preserving resources, improving the circularity in\nproduct lifecycles and ensuring global fairness. This paper explores how to\ncapture constitutive elements of those strategies using a modelling approach\nbased on a reference sustainability meta-model and pattern template. After\nproposing an extension to the meta-modelling to enable the structuring of a\npattern catalogue, we highlight how it can be populated on two case studies\nrespectively covering fairness and circularity.",
        "This paper presents MetricGrids, a novel grid-based neural representation\nthat combines elementary metric grids in various metric spaces to approximate\ncomplex nonlinear signals. While grid-based representations are widely adopted\nfor their efficiency and scalability, the existing feature grids with linear\nindexing for continuous-space points can only provide degenerate linear latent\nspace representations, and such representations cannot be adequately\ncompensated to represent complex nonlinear signals by the following compact\ndecoder. To address this problem while keeping the simplicity of a regular grid\nstructure, our approach builds upon the standard grid-based paradigm by\nconstructing multiple elementary metric grids as high-order terms to\napproximate complex nonlinearities, following the Taylor expansion principle.\nFurthermore, we enhance model compactness with hash encoding based on different\nsparsities of the grids to prevent detrimental hash collisions, and a\nhigh-order extrapolation decoder to reduce explicit grid storage requirements.\nexperimental results on both 2D and 3D reconstructions demonstrate the superior\nfitting and rendering accuracy of the proposed method across diverse signal\ntypes, validating its robustness and generalizability. Code is available at\nhttps:\/\/github.com\/wangshu31\/MetricGrids}{https:\/\/github.com\/wangshu31\/MetricGrids.",
        "Let $W= \\{W(t): t \\in \\mathbb{R}_+^N \\}$ be an $(N, d)$-Brownian sheet and\nlet $E \\subset (0, \\infty)^N$ and $F \\subset \\mathbb{R}^d$ be compact sets. We\nprove a necessary and sufficient condition for $W(E)$ to intersect $F$ with\npositive probability and determine the essential supremum of the Hausdorff\ndimension of the intersection set $W(E)\\cap F$ in terms of the thermal capacity\nof $E \\times F$. This extends the previous results of Khoshnevisan and Xiao\n(2015) for the Brownian motion and Khoshnevisan and Shi (1999) for the Brownian\nsheet in the special case when $E \\subset (0, \\infty)^N$ is an interval."
      ]
    }
  },
  {
    "id":2411.08664,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Multimodal Foundation Models for Material Property Prediction and Discovery",
    "start_abstract":"Artificial intelligence is transforming computational materials science, improving the prediction of material properties, and accelerating the discovery of novel materials. Recently, publicly available material data repositories have grown rapidly. This growth encompasses not only more materials but also a greater variety and quantity of their associated properties. Existing machine learning efforts in materials science focus primarily on single-modality tasks, i.e. relationships between materials and a single physical property, thus not taking advantage of the rich and multimodal set of material properties. Here, we introduce Multimodal Learning for Materials (MultiMat), which enables self-supervised multi-modality training of foundation models for materials. We demonstrate our framework's potential using data from the Materials Project database on multiple axes: (i) MultiMat achieves state-of-the-art performance for challenging material property prediction tasks; (ii) MultiMat enables novel and accurate material discovery via latent space similarity, enabling screening for stable materials with desired properties; and (iii) MultiMat encodes interpretable emergent features that may provide novel scientific insights.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Comprehensive Inorganic Chemistry III (Third Edition)"
      ],
      "abstract":[
        "Comprehensive Inorganic Chemistry III, a ten-volume reference work, is intended to cover fundamental principles, recent discoveries, and significant applications of elements and their compounds. Authored by renowned experts in the field and edited by a world-class editorial board, each chapter provides a thorough and in-depth overview of the topic covered, featuring resources which will be useful to students, researchers, faculty as well as those in the industry. Comprehensive Inorganic Chemistry III focuses on main group chemistry, biological inorganic chemistry, solid state and materials chemistry, catalysis, and new developments in electrochemistry and photochemistry, as well as NMR and diffraction methods for studying inorganic compounds. The work expands on our 2013 work Comprehensive Inorganic Chemistry II while also adding new volumes on cutting-edge research areas and techniques for studying inorganic compounds. Researchers seeking background information on a specific problem involving the synthesis of inorganic compounds, as well as applications for numerous elements from the periodic table, and their compounds, will be able to rely on and refer to this authoritative scientific resource time and again."
      ],
      "categories":[
        "Material"
      ]
    },
    "list":{
      "title":[
        "Hyperuniformity and hyperfluctuations of random measures in commutative\n  spaces",
        "Direction-Dependent Conduction Polarity in Altermagnetic CrSb",
        "Quasi-reversible parametric instability in presence of noise",
        "Chemical abundance inventory in phosphorus-rich stars",
        "Local well-posedness for a system of modified KdV equations in\n  modulation spaces",
        "Rational Group Algebras of Camina $p$-groups",
        "Zero loss guarantees and explicit minimizers for generic\n  overparametrized Deep Learning networks",
        "Convergence guarantees for response prediction for latent structure\n  network time series",
        "Bounds on the number of squares in recurrence sequences: $y_{0}=b^{2}$\n  (I)",
        "A uniform action of the dihedral group $ Z_2\\times D_3$ on\n  Littlewood--Richardson coefficients",
        "Topological Holography for 2+1-D Gapped and Gapless Phases with\n  Generalized Symmetries",
        "An Accurate Efficient Analytic Model of Fidelity under Depolarizing\n  Noise oriented to Large Scale Quantum System Design",
        "Transfer Learning Analysis of Variational Quantum Circuits",
        "Mean Field Game of Controls with State Reflections: Existence and Limit\n  Theory",
        "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
        "Field-induced phase transitions in the Kitaev-Heisenberg model: A\n  sign-problem-free quantum Monte Carlo study and possible application to\n  $\\alpha$-RuCl3",
        "Robust triple-q magnetic order with trainable spin vorticity in\n  Na$_2$Co$_2$TeO$_6$",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Spatial Data Science Languages: commonalities and needs",
        "A Large-dimensional Analysis of ESPRIT DoA Estimation: Inconsistency and\n  a Correction via RMT",
        "Exact matching as an alternative to propensity score matching",
        "Constructing QCQP Instances Equivalent to Their SDP Relaxations",
        "Concentration inequalities and large deviations for continuous greedy\n  animals and paths",
        "ECHO21: Exploring the Cosmos with Hydrogen Observation",
        "Uniform large-scale $\\varepsilon$-regularity for entropic optimal\n  transport",
        "Quantum Characterization, Verification, and Validation",
        "System-environmental entanglement in critical spin systems under\n  $ZZ$-decoherence and its relation to strong and weak symmetries",
        "Growing black-hole hair in nonminimally coupled biscalar gravity",
        "Remarks on the Higgs Branch of 5d Conformal Matter"
      ],
      "abstract":[
        "We introduce the notion of Bartlett spectral measure for isometrically\ninvariant random measures on proper metric commutative spaces. When the\nunderlying Gelfand pair corresponds to a higher-rank, connected, simple matrix\nLie group with finite center and a maximal compact subgroup, we prove that the\nnumber variance is asymptotically bounded above, uniformly across all random\nmeasures, by the volume raised to a power strictly less than 2. On Euclidean\nand real hyperbolic spaces we define a notion of heat kernel hyperuniformity\nfor random distributions that generalizes hyperuniformity of random measures,\nand we prove that every sufficiently well behaved spectral measure can be\nrealized as the Bartlett spectral measure of an invariant Gaussian random\ndistribution. We also compute Bartlett spectral measures for invariant\ndeterminantal point processes in commutative spaces, providing a spectral proof\nof hyperuniformity for infinite polyanalytic ensembles in the complex plane, as\nwell as heat kernel non-hyperuniformity of the zero set of the standard\nhyperbolic Gaussian analytic function on the Poincar\\'e disk.",
        "CrSb has recently gained immense attention as an altermagnetic candidate.\nThis work reports on the experimental observation of direction-dependent\nconduction polarity (DDCP) in altermagnetic CrSb through Hall and Seebeck\nthermopower measurements. Conduction is dominated by holes along the c-axis and\nby electrons in the ab-plane of the hexagonal crystal of CrSb. Density\nfunctional theory (DFT) calculations indicate that DDCP in CrSb arises from a\nmulticarrier mechanism, where electrons and holes living in distinct bands\ndominate conduction along different crystallographic directions. Furthermore,\nDFT predicts that DDCP exists within a narrow energy window near the Fermi\nlevel and is sensitive to small doping levels. This prediction is\nexperimentally validated by the loss of DDCP in hole-doped\nCr$_{0.98}$V$_{0.02}$Sb. These findings highlight the potential for tunable\nelectronic behavior in CrSb, offering promising avenues for applications in\ndevices that require both p-type and n-type functionalities within a single\nmaterial.",
        "We present an experimental and theoretical study of the effect of\nspatio-temporal fluctuations in quasi-reversible systems displaying a spatial\nquintic supercritical bifurcation. The saturation mechanism is drastically\nchanged by the inclusion of fluctuations. Experimentally, we observe the\nmodification of the bifurcation diagram of parametrically amplified surface\nwaves as spatiotemporal fluctuations stemming from an underlying vortex flow\nare included. Theoretically, we characterize the noise-dependent effective\ndynamics in a model system, the parametrically driven nonlinear Schr\\\"odinger\nequation, subjected to noise which allows us to rationalize the effect of the\nunderlying vortex flow on the surface waves",
        "We provide an overview of the latest advances in the study of phosphorus-rich\nstars, covering their detailed chemical abundance analyses and innovative\nmining approaches. Following the discovery of 16 low-mass and low-metallicity\nstars rich in P, we expanded this sample by demonstrating that a recently\nidentified group of Si-rich giants is also P-rich. A detailed abundance\nanalysis was conducted on the nearinfrared spectra from APOGEE-2 DR17,\nencompassing 13 elements. Subsequently, a similar analysis was performed on the\noptical UVES spectra of four P-rich stars, resulting in the abundance\ndetermination of 48 light and heavy elements. This comprehensive analysis\nfurther refined the chemical fingerprint of these peculiar stars, which was\nemployed to evaluate the plausibility of various nucleosynthetic formation\nscenarios. In order to obtain a statistically more reliable chemical\nfingerprint in the future, we explored the use of unsupervised machine learning\nalgorithms to identify additional P-rich stars in extensive spectroscopic\nsurveys, such as APOGEE-2. The primary objective of this research is to\nidentify the progenitor of these stars and determine whether current\nnucleosynthetic models require revision or if a completely new source of P in\nthe Galaxy is responsible for the existence of the P-rich stars.",
        "In this work, we consider the initial value problem (IVP) for a system of\nmodified Korteweg-de Vries (mKdV) equations\n  \\begin{equation*}\n  \\begin{cases} \\partial_t v + \\partial_x^3 v+ \\partial_x (v w^2) = 0,\n\\hspace{0.98 cm} v(x,0)=\\psi(x),\\\\ \\partial_t w + \\alpha \\partial_x^3\nw+\\partial_x (v^2 w) = 0,\\hspace{0.5 cm} w(x,0)=\\phi(x). \\end{cases}\n\\end{equation*} The main interest is in addressing the well-posedness issues of\nthe IVP when the initial data are considered in the modulation space\n$M_s^{2,p}(\\mathbb{R})$, $p\\geq 2$. In the case when $0<\\alpha\\ne 1$, we derive\nnew trilinear estimates in these spaces and prove that the IVP is locally\nwell-posed for data in $M_s^{2,p}(\\mathbb{R})$ whenever $s>\n\\frac14-\\frac{1}{p}$ and $p\\geq 2$.",
        "In this article, we present a combinatorial formula for the Wedderburn\ndecomposition of rational group algebras of Camina $p$-groups, where $p$ is a\nprime. We also provide a complete set of primitive central idempotents of\nrational group algebras of these groups.",
        "We determine sufficient conditions for overparametrized deep learning (DL)\nnetworks to guarantee the attainability of zero loss in the context of\nsupervised learning, for the $\\mathcal{L}^2$ cost and {\\em generic} training\ndata. We present an explicit construction of the zero loss minimizers without\ninvoking gradient descent. On the other hand, we point out that increase of\ndepth can deteriorate the efficiency of cost minimization using a gradient\ndescent algorithm by analyzing the conditions for rank loss of the training\nJacobian. Our results clarify key aspects on the dichotomy between zero loss\nreachability in underparametrized versus overparametrized DL.",
        "In this article, we propose a technique to predict the response associated\nwith an unlabeled time series of networks in a semisupervised setting. Our\nmodel involves a collection of time series of random networks of growing size,\nwhere some of the time series are associated with responses. Assuming that the\ncollection of time series admits an unknown lower dimensional structure, our\nmethod exploits the underlying structure to consistently predict responses at\nthe unlabeled time series of networks. Each time series represents a multilayer\nnetwork on a common set of nodes, and raw stress embedding, a popular\ndimensionality reduction tool, is used for capturing the unknown latent low\ndimensional structure. Apart from establishing theoretical convergence\nguarantees and supporting them with numerical results, we demonstrate the use\nof our method in the analysis of real-world biological learning circuits of\nlarval Drosophila.",
        "We continue and generalise our earlier investigations of the number of\nsquares in binary recurrence sequences. Here we consider sequences, $\\left\\{\ny_{k} \\right\\}$, arising from the solutions of generalised negative Pell\nequations, $X^{2}-dY^{2}=c$, where $-c$ and $y_{0}$ are any positive squares.\nWe show that there are at most $5$ distinct squares in such sequences when\n$y_{0}=1,4,\\ldots,11^{2}$, or once $d$ exceeds an explicit lower bound.",
        "We show that the dihedral group $ Z_2\\times D_3$ of order twelve acts\nfaithfully on the set LR, either consisting of Littlewood-Richardson tableaux,\nor their companion tableaux, or Knutson-Tao hives or Knutson-Tao-Woodward\npuzzles,via involutions which simultaneously conjugate or shuffle a\nLittlewood-Richardson triple of partitions. The action of $ Z_2\\times D_3$\ncarries a linear time index two subgroup $H\\simeq D_3$ action, where an\ninvolution which goes from $H$ into the other coset of H is difficult in the\nsense that it is not manifest neither exhibited by simple means. Pak and\nVallejo have earlier made this observation with respect to the subgroup of\nindex two in the symmetric group $ S_3$ consisting of cyclic permutations which\nH extends. The other half LR symmetries, not in the range of the H-action, are\nhidden and consist of commutativity and conjugation symmetries. Their\nexhibition is reduced to the action of a remaining generator of $ Z_2\\times\nD_3$, which belongs to the other coset of H, and enables to reduce in linear\ntime all known LR commuters and transposers to each other, and to the Luzstig-\nSch\\\"utzenberger involution. A hive is specified by superimposing the companion\ntableau pair of an LR tableau, and its $Z_2\\times D_3$-symmetries are exhibited\nvia the corresponding LR companion tableau pair. The action of $ Z_2\\times D_3$\non puzzles, naturally in bijection with Purbhoo mosaics, is consistent with the\nmigration map on mosaics which translates to jeu de taquin slides or\ntableau-switching on LR tableaux. Their H-symmetries are reduced to simple\nprocedures on a puzzle via label swapping together with simple reflections of\nan equilateral triangle, that is, puzzle dualities, and rotations on an\nequilateral triangle.",
        "We study topological holography for 2+1-D gapped and gapless phases with\ngeneralized symmetries using tools from higher linear algebra and higher\ncondensation theory. We focus on bosonic fusion 2-category symmetries, where\nthe Symmetry Topological Field Theory (SymTFT) are 3+1D Dijkgraaf-Witten\ntheories.\n  (1). Gapped phases are obtained from the sandwich construction with gapped\nsymmetry and physical boundaries. A gapped boundary of the 3+1D SymTFT is\ncalled minimal if it has no intrinsic 2+1-D topological order. We derive the\ngeneral structure of a sandwich construction with minimal gapped symmetry and\nphysical boundaries, including the underlying topological order and the\nsymmetry action. We also study some concrete examples with 2-group or\nnon-invertible symmetries.\n  (2). For gapless phases, we show that the SymTFT provides a complete\ndescription of the \\textit{topological skeleton} of a gapless phase. The\ntopological skeleton of a gapless phase is the higher categorical structure of\nits topological defects. We rigorously establish this relation for 2+1-D\ngapless phases with finite group symmetries. For a gapless phase with a finite\ngroup symmetry, its topological skeleton(also known as gapless SPT(gSPT)) can\nbe characterized by the decorated domain wall construction. We give a precise\nformulation of this using spectral sequence. We show that certain class of\ncondensable algebras in the SymTFT $\\mathcal{Z}_1[2\\mathbf{Vec}_G]$, which we\ncall minimal condensable algebras, has exactly the same structure. We further\ngive a cohomological classification of minimal condensable algebras, which\nenables us to compute the classification of 2+1-D $G$-gSPTs via ordinary group\ncohomology. Finally we use SymTFT to construct 2+1-D gSPT with generalized\nsymmetries, including an intrinsically gSPT(igSPT) with exact non-invertible\nfusion 2-category symmetry and anomalous 2-group IR symmetry.",
        "Fidelity is one of the most valuable and commonly used metrics for assessing\nthe performance of quantum circuits on error-prone quantum processors. Several\napproaches have been proposed to estimate circuit fidelity without the need of\nexecuting it on quantum hardware, but they often face limitations in\nscalability or accuracy. In this work, we present a comprehensive theoretical\nframework to predict the fidelity of quantum circuits under depolarizing noise.\nBuilding on theoretical results, we propose an efficient fidelity estimation\nalgorithm based on device calibration data. The method is thoroughly validated\nthrough simulation and execution on real hardware, demonstrating improved\naccuracy compared to state-of-the-art alternatives. The proposed approach\nprovides a scalable and practical tool for benchmarking quantum hardware,\ncomparing quantum software techniques such as compilation methods, obtaining\ncomputation bounds for quantum systems, and guiding hardware design decisions,\nmaking it a critical resource for the development and evaluation of quantum\ncomputing technologies.",
        "This work analyzes transfer learning of the Variational Quantum Circuit\n(VQC). Our framework begins with a pretrained VQC configured in one domain and\ncalculates the transition of 1-parameter unitary subgroups required for a new\ndomain. A formalism is established to investigate the adaptability and\ncapability of a VQC under the analysis of loss bounds. Our theory observes\nknowledge transfer in VQCs and provides a heuristic interpretation for the\nmechanism. An analytical fine-tuning method is derived to attain the optimal\ntransition for adaptations of similar domains.",
        "This paper studies mean field game (MFG) of controls by featuring the joint\ndistribution of the state and the control with the reflected state process\nalong an exogenous stochastic reflection boundary. We contribute to the\nliterature with a customized relaxed formulation and some new compactification\narguments to establish the existence of a Markovian mean field equilibrium\n(MFE) in the weak sense. We consider an enlarged canonical space, utilizing the\ndynamic Skorokhod mapping, to accommodate the stochastic reflection boundary\nprocess. A fixed-point argument on the extended space using an extension\ntransformation technique is developed to tackle challenges from the joint\nmeasure flow of the state and the relaxed control that may not be continuous.\nFurthermore, the bidirectional connections between the MFG and the $N$-player\ngame are also established in the context of joint law dependence and state\nreflections. We first show that any weak limit of empirical measures induced by\n$\\boldsymbol{\\epsilon}$-Nash equilibria in $N$-player games must be supported\nexclusively on the set of relaxed mean field equilibria, analogous to the\npropagation of chaos in mean field control problems. We then prove the\nconvergence result that a Markovian MFE in the weak sense can be approximated\nby a sequence of constructed $\\boldsymbol{\\epsilon}$-Nash equilibria in the\nweak sense in $N$-player games when $N$ tends to infinity.",
        "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
        "The frustrated magnet $\\alpha$-RuCl3 is one of the prime candidates for\nrealizing a Kitaev quantum spin liquid (QSL). However, the existence of a\nfield-induced intermediate QSL phase in this material remains under debate.\nHere, we employ sign-free numerically exact quantum Monte Carlo simulations to\ninvestigate the Kitaev-Heisenberg (KH) model on the honeycomb lattice with\n$K=-2J$ under an applied magnetic field along the z-direction. Our findings\nreveal that the system undergoes a direct quantum phase transition from a\nzigzag magnetically ordered phase to a spin-polarized phase at zero\ntemperature, which belongs to the 3D XY universality class. At finite\ntemperatures, a Berezinskii-Kosterlitz-Thouless transition line separates the\nspin-polarized phase from a quasi-long-range ordered state, eventually\nterminating at the quantum critical point. Our results convincingly show that\nthere is no intermediate QSL phase in the KH model with a z-direction magnetic\nfield, which we believe will shed important light on understanding experimental\nobservations in $\\alpha$-RuCl3.",
        "Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$\npossesses novel triple-$\\mathbf{q}$ magnetic order instead of conventional\nsingle-$\\mathbf{q}$ zigzag order. Here we present dedicated experiments in\nsearch for distinct properties expected of the triple-$\\mathbf{q}$ order,\nnamely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking\nfields and fictitious magnetic fields generated by the spin vorticity. In\nstructurally pristine single crystals, we show that $C_3$ symmetry-breaking\nin-plane uniaxial strains do not affect the order's magnetic neutron\ndiffraction signals. We further show that $\\mathbf{c}$-axis propagating light\nexhibits large Faraday rotations in the ordered state due to the spin\nvorticity, the sign of which can be trained via the system's ferrimagnetic\nmoment. These results are in favor of the triple-$\\mathbf{q}$ order in\nNa$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Recent workshops brought together several developers, educators and users of\nsoftware packages extending popular languages for spatial data handling, with a\nprimary focus on R, Python and Julia. Common challenges discussed included\nhandling of spatial or spatio-temporal support, geodetic coordinates, in-memory\nvector data formats, data cubes, inter-package dependencies, packaging upstream\nlibraries, differences in habits or conventions between the GIS and physical\nmodelling communities, and statistical models. The following set of insights\nhave been formulated: (i) considering software problems across data science\nlanguage silos helps to understand and standardise analysis approaches, also\noutside the domain of formal standardisation bodies; (ii) whether attribute\nvariables have block or point support, and whether they are spatially intensive\nor extensive has consequences for permitted operations, and hence for software\nimplementing those; (iii) handling geometries on the sphere rather than on the\nflat plane requires modifications to the logic of {\\em simple features}, (iv)\nmanaging communities and fostering diversity is a necessary, on-going effort,\nand (v) tools for cross-language development need more attention and support.",
        "In this paper, we perform asymptotic analyses of the widely used ESPRIT\ndirection-of-arrival (DoA) estimator for large arrays, where the array size $N$\nand the number of snapshots $T$ grow to infinity at the same pace. In this\nlarge-dimensional regime, the sample covariance matrix (SCM) is known to be a\npoor eigenspectral estimator of the population covariance. We show that the\nclassical ESPRIT algorithm, that relies on the SCM, and as a consequence of the\nlarge-dimensional inconsistency of the SCM, produces inconsistent DoA estimates\nas $N,T \\to \\infty$ with $N\/T \\to c \\in (0,\\infty)$, for both widely- and\nclosely-spaced DoAs. Leveraging tools from random matrix theory (RMT), we\npropose an improved G-ESPRIT method and prove its consistency in the same\nlarge-dimensional setting. From a technical perspective, we derive a novel\nbound on the eigenvalue differences between two potentially non-Hermitian\nrandom matrices, which may be of independent interest. Numerical simulations\nare provided to corroborate our theoretical findings.",
        "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
        "General quadratically constrained quadratic programs (QCQPs) are challenging\nto solve as they are known to be NP-hard. A popular approach to approximating\nQCQP solutions is to use semidefinite programming (SDP) relaxations. It is\nwell-known that the optimal value $\\eta$ of the SDP relaxation problem bounds\nthe optimal value $\\zeta$ of the QCQP from below, i.e., $\\eta \\leq \\zeta$. The\ntwo problems are considered equivalent if $\\eta = \\zeta$. In the recent paper\nby Arima, Kim and Kojima [arXiv:2409.07213], a class of QCQPs that are\nequivalent to their SDP relaxations are proposed with no condition imposed on\nthe quadratic objective function, which can be chosen arbitrarily. In this\nwork, we explore the construction of various QCQP instances within this class\nto complement the results in [arXiv:2409.07213]. Specifically, we first\nconstruct QCQP instances with two variables and then extend them to higher\ndimensions. We also discuss how to compute an optimal QCQP solution from the\nSDP relaxation.",
        "Consider the continuous greedy paths model: given a $d$-dimensional Poisson\npoint process with positive marks interpreted as masses, let $\\mathrm P(\\ell)$\ndenote the maximum mass gathered by a path of length $\\ell$ starting from the\norigin. It is known that $\\mathrm P(\\ell)\/\\ell converges a.s.\\ to a\ndeterministic constant $\\mathrm P$. We show that the lower-tail deviation\nprobability for $\\mathrm P(\\ell) has order $\\mathrm{exp}(-\\ell^2)$ and, under\nexponential moment assumption on the mass distribution, that the upper-tail\ndeviation probability has order $\\mathrm{exp}(-\\ell)$. In the latter regime, we\nprove the existence and some properties -notably, convexity -of the\ncorresponding rate function. An immediate corollary is the large deviation\nprinciple at speed $\\ell$ for $\\mathrm P(\\ell)$. Along the proof we show an\nupper-tail concentration inequality in the case where marks are bounded. All of\nthe above also holds for greedy animals and have versions where the paths or\nanimals involved have two anchors instead of one.",
        "We introduce a python package called ECHO21 for generating global 21-cm\nsignal from the dark ages through cosmic dawn to the end of reionization.\nBecause of its analytical-prescription-based foundation, ECHO21 generates a\nsingle model in $\\mathcal{O}(1)\\,$s. The code can generate a large set of\nsignals, ideal for building emulators and performing astrophysical or\ncosmological inference from a given 21-cm dataset. The code is MPI parallel\nwith reasonable scalability and thus, can be run on high-performance computers.\nWe offer six astrophysical parameters that control the Lyman-$\\alpha$\nemissivity, X-ray emissivity, emissivity of ionizing photons, and star\nformation rate. A critical component of 21-cm modelling, but ignored by\nmajority of public codes, that we include is the Ly$\\alpha$ heating. For a\ncertain range of astrophysical parameters, the Ly$\\alpha$ heating could even\ndominate the X-ray heating. In addition to astrophysical parameters, in ECHO21\nit is just as easy to vary the standard cosmological parameters which makes it\npossible to combine constraints from 21-cm observations and other cosmological\nprobes. Further, we offer two models of star formation rate; a\nphysically-motivated and an empirically-motivated. Since the latter is directly\ninferred by HST\/JWST observations, it makes ECHO21 an appropriate tool to build\nsynergies between 21-cm observations and galaxy surveys. With a number of 21-cm\nexperiments soon to provide cosmic dawn 21-cm data, ECHO21 is a handy package\nfor making quick but sufficiently realistic astrophysical inferences.",
        "We study the regularity properties of the minimisers of entropic optimal\ntransport providing a natural analogue of the $\\varepsilon$-regularity theory\nof quadratic optimal transport in the entropic setting. More precisely, we show\nthat if the minimiser of the entropic problem satisfies a gradient BMO-type\nestimate at some scale, the same estimate holds all the way down to the natural\nlength-scale associated to the entropic regularisation.\n  Our result follows from a more general $\\varepsilon$-regularity theory for\noptimal transport costs which can be viewed as perturbations of quadratic\noptimal transport. We consider such a perturbed cost and require that, under a\ncertain class of admissible affine rescalings, the minimiser remains a local\nquasi-minimiser of the quadratic problem (in an appropriate sense) and that the\ncost of \"long trajectories\" of minimisers (and their rescalings) is small.\nUnder these assumptions, we show that the minimiser satisfies an appropriate\n$C^{2,\\alpha}$ Morrey$\\unicode{x2013}$Campanato-type estimate which is valid up\nto the scale of quasi-minimality.",
        "Quantum characterization, verification, and validation (QCVV) is a set of\ntechniques to probe, describe, and assess the behavior of quantum bits\n(qubits), quantum information-processing registers, and quantum computers. QCVV\nprotocols probe and describe the effects of unwanted decoherence so that it can\nbe eliminated or mitigated. They can be usefully divided into characterization\ntechniques that estimate predictive models for a device's behavior from data,\nand benchmarking techniques that assess overall performance of a device. In\nthis introductory article, we briefly summarize the history of QCVV, introduce\nthe mathematical models and metrics upon which it relies, and then summarize\nthe foundational fields of tomography, randomized benchmarking, and holistic\nbenchmarks. We conclude with brief descriptions of (and references to) advanced\ntopics including gate set tomography, phase estimation, Pauli noise learning,\ncharacterization of mid-circuit measurements and non-Markovianity, classical\nshadows, verification and certification, and logical qubit assessment.",
        "Open quantum many-body system exhibits nontrivial behavior under decoherence.\nIn particular, system-environmental entanglement is one of quantities to\ncharacterize mixed state properties under decoherence. In this study, we\ninvestigate the behavior of the system-environmental entanglement for critical\nspin chains under nearest-neighbor $ZZ$ -decoherence. We numerically find that\nthe system-environmental entanglement exhibits a specific scaling law including\na system-independent universal term (\"$g$-function\"). For the critical XXZ\nmodel, transition to strong-to-weak spontaneously symmetry breaking mixed state\ntakes place. In that case, the $g$-function changes its value at decoherent\ntransition point and gets double the value of system under single-site\n$Z$-decoherence, which was recently studied by conformal field theory. By\nstudying Shannon entropy, we clarify origin of this $g$-function behavior.",
        "Black holes offer a unique laboratory for fundamental physics and are crucial\nfor probing theories beyond Einstein's theory of General Relativity. In this\npaper, we consider 4D effective field theories with scalar fields. We focus on\naxi-dilaton gravity, a quadratic gravity theory with two kinetically coupled\nscalar fields, an axion and a dilaton. To evolve these fields around black\nholes, we introduce Canuda-AxiDil, the first open-source, parameterized\nnumerical relativity code for quadratic and bi-scalar gravity. Using this code,\nwe perform single black hole simulations to show the dynamical formation of\naxion and dilaton hairs. Through these simulations, we measure the impact of\nblack-hole spin and curvature coupling strength on the axion and dilaton, and\nshow that a kinetic coupling between the fields increases the observed\ndeviations from General Relativity. Furthermore, we simulate the axion and\ndilaton fields around a binary black hole coalescence demonstrating the growth\nof axion hair during the inspiral and the production of radiative modes for\nboth fields.",
        "Among the elementary building blocks in the atomic classification of 5d SCFTs\nthere are 5d bifundamental conformal matter theories of various kinds. In this\nwork we study the Higgs branch of these models and of the corresponding\nmolecules arising from their fusion. To this aim we use two complementary\nindependent strategies. On the one hand for the type $A$ and $D$ conformal\nmatter, we identify dual $(p,q)$ brane webs in IIB and exploit them to read off\nthe corresponding magnetic quivers. On the other hand, we exploit circle\nreductions and study the resulting 4d $\\mathcal N=2$ SCFTs, giving an\nalternative derivation of their Higgs branches which extend also to the $E$\ntypes."
      ]
    }
  },
  {
    "id":2411.0908,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Music Transformer: Generating Music with Long-Term Structure",
    "start_abstract":"Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "On the use of AI for Generation of Functional Music to Improve Mental Health"
      ],
      "abstract":[
        "Increasingly music has been shown to have both physical and mental health benefits including improvements in cardiovascular health, a link reduction of cases dementia elderly populations, markers general well-being such as stress reduction. Here, we describe short case studies addressing (anxiety, stress-reduction) through AI-driven generation. Engaging active listening music-making activities (especially for at risk age groups) can be particularly beneficial, the practice therapy helpful range use across wide range. However, access prohibitive terms expertize, materials, cost. Furthermore existing functional outcomes (such targeted improvement suggested above) hindered by issues repetition subsequent over-familiarity with material. In this paper, machine learning approaches which create informed biophysiological measurement two studies, target emotional states opposing ends Cartesian affective space (a dimensional emotion points ranging from descriptors relaxation, fear). Galvanic skin response is used marker psychological arousal an estimate state control signal training algorithm. This algorithm creates non-linear time series musical features sound synthesis \u201con-the-fly\u201d, using perceptually feature similarity model. We find interaction between familiarity perceived response. also report on psychometric evaluation generated material, consider how these - similar techniques might useful generation tasks, example, nonlinear sound-tracking that found interactive media or video games."
      ],
      "categories":[
        "Mental Health"
      ]
    },
    "list":{
      "title":[
        "Reversible Switching of the Environment-Protected Quantum Spin Hall\n  Insulator Bismuthene at the Graphene\/SiC Interface",
        "Optical control of the crystal structure in the bilayer nickelate\n  superconductor La$_3$Ni$_2$O$_7$ via nonlinear phononics",
        "The gyrokinetic field invariant and electromagnetic temperature-gradient\n  instabilities in `good-curvature' plasmas",
        "On the Jump Conditions for Shock Waves in Condensed Materials",
        "Hardware-Efficient Entanglement Distillation Using Bosonic Systems",
        "Decentralized Projection-free Online Upper-Linearizable Optimization\n  with Applications to DR-Submodular Optimization",
        "Refined enumeration of two-rowed set-valued standard tableaux via\n  two-coloured Motzkin paths",
        "Identification and characterisation of the gamma-ray counterpart of the\n  transitional pulsar candidate CXOU J110926.4-650224",
        "From surface Fermi arcs to Fermi loops in the Dirac semimetal Cd3As2",
        "Hybridization between surface flat bands and bulk bands in the\n  topological nodal-line semimetal Sn$_{0.15}$NbSe$_{1.75}$ probed via\n  soft-point-contact spectroscopy",
        "ALICE Fast Interaction Trigger Upgrade",
        "Single-shot and two-shot decoding with generalized bicycle codes",
        "Josephson vortices and persistent current in a double-ring supersolid\n  system",
        "TDCOSMO: XX. WFI2033--4723, the First Quadruply-Imaged Quasar Modeled\n  with JWST Imaging",
        "The Q-Spellbook: Crafting Surface Code Layouts and Magic State Protocols\n  for Large-Scale Quantum Computing",
        "Seismic wavefield solutions via physics-guided generative neural\n  operator",
        "The uniqueness of Lyapunov rank among symmetric cones",
        "Atom-Chip Compatible Optical Lattice",
        "The Distributionally Robust Optimization Model of Sparse Principal\n  Component Analysis",
        "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models",
        "Group Sparsity Methods for Compressive Space-Frequency Channel\n  Estimation and Spatial Equalization in Fluid Antenna System",
        "A degenerate Takens--Bogdanov bifurcation in a normal form of Lorenz's\n  equations",
        "Dynamical Shortcomings in the Generalized SU(2) Proca Theory: Challenges\n  for Cosmic Acceleration",
        "Leap into the future: shortcut to dynamics for quantum mixtures",
        "Spectral analysis of the X-ray flares in the 2023 outburst of the new\n  black binary transient Swift J1727.8--1613 observed with Insight-HXMT",
        "Masses of blocks of the $\\Lambda$-coalescent with dust via stochastic\n  flows",
        "Gaia 19cwm -- an eclipsing dwarf nova of WZ Sge type with a magnetic\n  white dwarf",
        "Enhanced quantum radiation with flying-focus laser pulses",
        "Forecasted Detection Limits on the (Dark) Matter Density in Supermassive\n  Black Hole Binaries for LISA"
      ],
      "abstract":[
        "Quantum Spin Hall Insulators (QSHI) have been extensively studied both\ntheoretically and experimentally because they exhibit robust helical edge\nstates driven by spin-orbit coupling and offer the potential for applications\nin spintronics through dissipationless spin transport. However, to realize\ndevices, it is indispensable to gain control over the interaction of the active\nlayer with the substrate, and to protect it from environmental influences. Here\nwe show that a single layer of elemental Bi, formed by intercalation of an\nepitaxial graphene buffer layer on SiC(0001), is a promising candidate for a\nQSHI. This layer can be reversibly switched between an electronically inactive\nprecursor state and a ``bismuthene state'', the latter exhibiting the predicted\nband structure of a true two-dimensional bismuthene layer. Switching is\naccomplished by hydrogenation (dehydrogenation) of the sample, i.e., a partial\npassivation (activation) of dangling bonds of the SiC substrate, causing a\nlateral shift of Bi atoms involving a change of the adsorption site. In the\nbismuthene state, the Bi honeycomb layer is a prospective QSHI, inherently\nprotected by the graphene sheet above and the H-passivated substrate below.\nThus, our results represent an important step towards protected QSHI systems\nbeyond graphene.",
        "Superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ occurs when the\ninterlayer Ni-O-Ni bond angle becomes straight under pressure, suggesting a\nstrong relationship between the crystal structure and the emergence of\nsuperconductivity. In this study, we theoretically propose a way to control the\ncrystal structure of La$_3$Ni$_2$O$_7$ toward the tetragonal symmetry via light\nirradiation instead of pressure using the idea of nonlinear phononics. Here,\nresonant optical excitation of an infrared-active (IR) lattice vibration\ninduces a nonlinear Raman-mode displacement through the anharmonic\nphonon-phonon coupling. We calculate the light-induced phonon dynamics on the\nanharmonic lattice potential determined by first-principles calculation. We\nfind that the interlayer Ni-O-Ni bond angle gets slightly closer to straight\nwhen an appropriate IR mode is selectively excited. Our study suggests that\nlight irradiation can be a promising way for structural control of\nLa$_3$Ni$_2$O$_7$.",
        "Curvature-driven instabilities are ubiquitous in magnetised fusion plasmas.\nBy analysing the conservation laws of the gyrokinetic system of equations, we\ndemonstrate that the well-known spatial localisation of these instabilities to\nregions of `bad magnetic curvature' can be explained using the conservation law\nfor a sign-indefinite quadratic quantity that we call the `gyrokinetic field\ninvariant'. Its evolution equation allows us to define the local effective\nmagnetic curvature whose sign demarcates the regions of `good' and `bad'\ncurvature, which, under some additional simplifying assumptions, can be shown\nto correspond to the inboard (high-field) and outboard (low-field) sides of a\ntokamak plasma, respectively. We find that, given some reasonable assumptions,\nelectrostatic curvature-driven modes are always localised to the regions of bad\nmagnetic curvature, regardless of the specific character of the instability.\nMore importantly, we also deduce that any mode that is unstable in the region\nof good magnetic curvature must be electromagnetic in nature. As a concrete\nexample, we present the magnetic-drift mode, a novel good-curvature\nelectromagnetic instability, and compare its properties with the well-known\nelectron-temperature-gradient instability. Finally, we discuss the relevance of\nthe magnetic-drift mode for high-$\\beta$ fusion plasmas, and in particular its\nrelationship with microtearing modes.",
        "In this article, we have proposed Rankine-Hugoniot (RH) boundary conditions\nat the normal shock front, which is passing through the condensed material.\nThese RH conditions are quite general, and their convenient forms for the\nparticle velocity, mass density, pressure, and temperature have been presented\nin terms of the upstream Mach number and the material parameters for the weak\nand the strong shocks, respectively. Finally, the effects on the mechanical\nquantities of the shock-compressed materials, e.g., titanium Ti6Al4V, stainless\nsteel 304, aluminum 6061-T6, etc., have been discussed.",
        "High-fidelity entanglement shared between distant quantum systems is an\nessential resource for quantum communication and computation. Entanglement\ndistillation addresses this need by converting multiple noisy Bell pairs into\nfewer higher-fidelity pairs, using only local quantum operations and classical\ncommunication. However, this approach typically requires a substantial overhead\nin the number of qubits. To bypass this hurdle, we propose to leverage the\nhigh-dimensional Hilbert space of a single pair of bosonic systems to store a\nlarge amount of entanglement, replacing the need for multi-qubit systems. To\ndistill entanglement in such a setup, we devise a new entanglement distillation\nprotocol, tailored for bosonic systems. The protocol converts a\nhighly-entangled noisy state between two bosonic systems into a\nlower-dimensional but high-fidelity entangled state, using only local bosonic\noperations. We show that our protocol significantly enhances the fidelity of\nthe entangled state in the presence of naturally occurring loss and dephasing\nerrors. Compared to methods relying on multiple Bell pairs, our scheme offers a\nmore hardware-efficient strategy, providing a practical route toward the\nrealization of entanglement distillation.",
        "We introduce a novel framework for decentralized projection-free\noptimization, extending projection-free methods to a broader class of\nupper-linearizable functions. Our approach leverages decentralized optimization\ntechniques with the flexibility of upper-linearizable function frameworks,\neffectively generalizing traditional DR-submodular function optimization. We\nobtain the regret of $O(T^{1-\\theta\/2})$ with communication complexity of\n$O(T^{\\theta})$ and number of linear optimization oracle calls of\n$O(T^{2\\theta})$ for decentralized upper-linearizable function optimization,\nfor any $0\\le \\theta \\le 1$. This approach allows for the first results for\nmonotone up-concave optimization with general convex constraints and\nnon-monotone up-concave optimization with general convex constraints. Further,\nthe above results for first order feedback are extended to zeroth order,\nsemi-bandit, and bandit feedback.",
        "We derive formulae for the number of set-valued standard tableaux of\ntwo-rowed shapes, keeping track of the total number of entries, the number of\nentries in the first row, and the number of entries in the second row. Key in\nthe proofs is a bijection with two-coloured Motzkin paths followed by\ngenerating function computations and coefficient extraction helped by the\nLagrange inversion formula.",
        "Transitional millisecond pulsars (tMSPs) represent a crucial link between the\nrotation-powered and accretion-powered states of binary pulsars. During their\nactive X-ray state, tMSPs are the only low-mass X-ray binary systems detected\nup to GeV energies by the Fermi Large Area Telescope (LAT). CXOU\nJ110926.4-650224 is a newly discovered tMSP candidate in an active X-ray state,\npotentially spatially compatible with a faint gamma-ray source listed in the\nlatest Fermi-LAT point-source catalogue as 4FGL J1110.3-6501. Confirming the\nassociation between CXOU J110926.4-650224 and the Fermi source is a key step\ntoward validating its classification as a tMSP. In this study, we analyse\nFermi-LAT data collected from August 2008 to June 2023 to achieve a more\naccurate localisation of the gamma-ray source, characterise its spectral\nproperties, and investigate potential time variability. By thoroughly\nreconstructing the gamma-ray background around the source using a weighted\nlikelihood model, we obtain a new localisation that aligns with the position of\nthe X-ray source at the 95% confidence level, with a Test Statistic value of\n$\\sim 42$. This establishes a spatial association between the gamma-ray source\nand CXOU J110926.4-650224. The gamma-ray emission is adequately described by a\npower-law model with a photon index of $\\Gamma = 2.5 \\pm 0.1$ and a\ncorresponding flux of $(3.7\\pm0.9) \\times 10^{-12}$ erg cm$^{-2}$ s$^{-1}$ in\nthe 0.1-300 GeV range.",
        "Arc-like topological surface states, i.e., surface Fermi arcs, have long been\nrecognized as the hallmark of Dirac semimetals. However, recent theories\nsuggest that the surface Fermi arcs could evolve into closed Fermi loops, akin\nto surface states in topological insulators, while preserving the bulk Dirac\nsemimetal phase. Here we experimentally reveal the evolution of Fermi arcs to\nFermi loops in the surface-modified Dirac semimetal Cd3As2 nanoplate through\ngate voltage-dependent spin transport and quantum oscillation measurements.\nSurface modification, achieved by heavy metal atom deposition and water\nmolecule adsorption, leads to an increase in the current-induced spin\npolarization at higher gate voltages, contrasting with the decrease observed in\nthe pristine nanoplate. We also observe surface Shubnikov-de Haas oscillations\nwith frequencies that scale linearly with gate voltage, aligning with a Fermi\nloop scenario. These findings indicate a transition from Fermi arcs to a closed\nFermi loop in the surface-modified Cd3As2 nanoplate, consistent with the\ntheoretically predicted fragile topological nature of Cd3As2. Our research\noffers profound insights into the transitions among these subtle topological\nstates in Dirac semimetals, paving the way for manipulating topological surface\nstates for high-performance spintronic devices.",
        "We report a detailed study of soft-point-contact spectroscopy of the\nsuperconducting topological nodal-line semimetal Sn$_{0.15}$NbSe$_{1.75}$ with\nthe superconducting transition temperature $T_{c}=9.5$ K. In the normal state,\nwe observe prominent asymmetric double peaks in the differential conductance\n$dI\/dV$. The asymmetric $dI\/dV$ curves are attributed to Fano resonance,\nquantum interference between two distinct tunneling paths of transmitting\nelectrons into flat energy bands and dispersive bands. A phenomenological\ndouble Fano resonance model reveals the hybridization between these bands below\nthe hybridization temperature $T_{\\mathrm{hyb}}=23$ K. This hybridization\ndrives an opening of a pseudogap below a characteristic temperature\n$T_{\\mathrm{PG}}=6.8$ K. In the superconducting state, we observe an unusual\nupper critical field that increases linearly with decreasing temperatures from\n$0.4T_{c}$ to $0.01T_{c}$, suggestive of a possible exotic superconducting\nstate. Our results suggest the presence of surface flat energy bands that stem\nfrom nontrivial topological nature of nodal lines in the bulk band structure\nand the hybridization between the surface flat bands and bulk bands in\nSn$_{0.15}$NbSe$_{1.75}$.",
        "This proceeding provides an expanded overview of the Fast Interaction Trigger\n(FIT) system performance, focusing on new developments such as the prospective\nintegration of the ALICE Low-Level Front-End Device (ALFRED) into the Detector\nControl System (DCS) and an upgraded Front-End Electronics (FEE) approach to\nenhance dynamic range and operational reliability. The first upgrade is\ndedicated to integrating FIT with ALICE central systems, while the second aims\nto improve signal processing from the scintillation arrays (FV0 and FDD).\nAdditionally, we propose forward-detector applications in future ALICE upgrades\n(Run 5 and beyond).\n  We also present the latest performance results, illustrated with relevant\nplots, including collision-time measurements for pp and Pb--Pb collision\nsystems, collision centrality determination based on the amplitude signals from\nthe FT0 detector, trigger performance metrics, and the improved DCS\narchitecture.",
        "Generalized-bicycle (GB) quantum error-correcting codes have naturally\nredundant minimum-weight stabilizer generators. To use this redundancy, we\nconstructed several short GB codes with relatively large dimensions, distances,\nand syndrome distances, also admitting fault-tolerant near-time-optimal\nsyndrome measurement schedules. We simulated their performance both under\nphenomenological noise and standard circuit noise, using sliding window\nsequential decoding protocol covering $T\\ge 1$ measurement rounds at a time,\nbased on an in-house binary BP+OSD decoder. While true single-shot decoding\n($T=1$) may suffer from a significant loss of accuracy, already two-shot\n($T=2$) decoding gives nearly the same logical error rates as multi-shot with\nmuch larger $T$. Comparison with the same codes but redundant stabilizer\ngenerators dropped show significantly improved decoding accuracy for all\n$T\\ge1$.",
        "We theoretically investigate the properties of ultra-cold dipolar atoms in\nradially coupled, concentric annular traps created by a potential barrier. The\nnon-rotating ground-state phases are investigated across the\nsuperfluid-supersolid phase transition, revealing a particle imbalance between\nthe two rings and a preferential density modulation in the outer ring. Near the\nphase transition on the superfluid side, applying rotation can induce density\nmodulations in either ring, depending on the angular momentum and barrier\nstrength. For low angular momentum, such rotation-induced density modulation\nforms in the outer ring, while for high angular momentum and weak barriers, it\nemerges in the inner ring. Rotation can lead to persistent currents and the\nnucleation of a vortex residing either at the center (central vortex) or at the\nring junction (Josephson vortex). Josephson vortices can also form at the\njunctions of the localized density sites induced by rotation in the inner ring,\na behavior that is unique to our system. By switching off the trap and allowing\nthe system to expand, distinct interference patterns emerge, which can be\nanalyzed to identify and distinguish between various vortex configurations, and\nthus can be observed in current state-of-the-art experiments.",
        "Gravitational time delays offer unique, independent measurements of the\nHubble constant, $H_0$. Precise measurements of $H_0$ stand as one of the most\npressing challenges in modern cosmology, and to do so with time delays requires\nprecise lens models. While much work has focused on streamlining the modeling\nprocess to keep pace with the erumpent discovery of strongly-lensed systems, a\ncritical step toward reducing uncertainty in $H_0$ comes from increasing the\nprecision of individual lens models themselves. In this work, we demonstrate\nthat the unprecedented imaging capabilities of JWST make this goal attainable.\nWe present the first lens model for time-delay cosmography derived from JWST\ndata, applied to the quadruply-imaged quasar WFI2033--4723. While the primary\nsource of systematic uncertainty in time-delay cosmography is currently the\nmass-sheet degeneracy (MSD), the sensitivity of models to this MSD varies on\nhow the point spread function (PSF) errors are mitigated. As the PSF is also\nthe primary source of uncertainty in lens modeling, we focus on a comparison of\ndifferent PSF modeling methods. Within the context of power-law models, we\nrecover results in agreement with previous Hubble Space Telescope (HST)-based\nmodels, but with better precision of key lensing parameters through\nimplementation of new PSF modeling techniques. Despite the record-holding\nprecision of this system's HST modeling, we achieve an additional 22% increase\nin precision of the Fermat potential difference, directly reducing\nuncertainties of cosmological inference. These results would produce a 3%\n($1\\sigma$ of the modeling error) shift of $H_0$ towards a higher value for\nthis lens, keeping all else constant. This work substantiates the\ngroundbreaking potential of JWST for time-delay cosmography and lays the\ngroundwork for modeling systems previously too faint to provide meaningful\nconstraints on $H_0$.",
        "Quantum error correction is a cornerstone of reliable quantum computing, with\nsurface codes emerging as a prominent method for protecting quantum\ninformation. Surface codes are efficient for Clifford gates but require magic\nstate distillation protocols to process non-Clifford gates, such as T gates,\nessential for universal quantum computation. In large-scale quantum\narchitectures capable of correcting arbitrary circuits, specialized surface\ncodes for data qubits and distinct codes for magic state distillation are\nneeded. These architectures can be organized into data blocks and distillation\nblocks. The system works by having distillation blocks produce magic states and\ndata blocks consume them, causing stalls due to either a shortage or excess of\nmagic states. This bottleneck presents an opportunity to optimize quantum space\nby balancing data and distillation blocks. While prior research offers insights\ninto selecting distillation protocols and estimating qubit requirements, it\nlacks a tailored optimization approach. We present a framework for optimizing\nlarge-scale quantum architectures, focusing on data block layouts and magic\nstate distillation protocols. We evaluate three data block layouts and four\ndistillation protocols under three optimization strategies: minimizing tiles,\nminimizing steps, and achieving a balanced trade-off. Through a comparative\nanalysis of brute force, dynamic programming, greedy, and random algorithms, we\nfind that brute force delivers optimal results, while greedy deviates by 7% for\nminimizing steps and dynamic programming matches brute force in tile\nminimization. We observe that total steps increase with columns, while total\ntiles scale with qubits. Finally, we propose a heuristic to help users select\nalgorithms suited to their objectives, enabling scalable and efficient quantum\narchitectures.",
        "Current neural operators often struggle to generalize to complex,\nout-of-distribution conditions, limiting their ability in seismic wavefield\nrepresentation. To address this, we propose a generative neural operator (GNO)\nthat leverages generative diffusion models (GDMs) to learn the underlying\nstatistical distribution of scattered wavefields while incorporating a\nphysics-guided sampling process at each inference step. This physics guidance\nenforces wave equation-based constraints corresponding to specific velocity\nmodels, driving the iteratively generated wavefields toward physically\nconsistent solutions. By training the diffusion model on wavefields\ncorresponding to a diverse dataset of velocity models, frequencies, and source\npositions, our GNO enables to rapidly synthesize high-fidelity wavefields at\ninference time. Numerical experiments demonstrate that our GNO not only\nproduces accurate wavefields matching numerical reference solutions, but also\ngeneralizes effectively to previously unseen velocity models and frequencies.",
        "The Lyapunov rank of a cone is the dimension of the Lie algebra of its\nautomorphism group. It is invariant under linear isomorphism and in general not\nunique - two or more non-isomorphic cones can share the same Lyapunov rank. It\nis therefore not possible in general to identify cones using Lyapunov rank. But\nsuppose we look only among symmetric cones. Are there any that can be uniquely\nidentified (up to isomorphism) by their Lyapunov ranks? We provide a complete\nanswer for irreducible cones and make some progress in the general case.",
        "A lattice beam configuration which results in an isotropic 3D trap near the\nsurface of an atom chip is described. The lattice is formed near the surface of\na reflectively coated atom chip, where three incident beams and three reflected\nbeams intersect. The coherent interference of these six beams form a\nphase-stable optical lattice which extends to the surface of the atom chip. The\nlattice is experimentally realized and the trap frequency is measured.\nDegenerate Raman sideband cooling is performed in the optical lattice, cooling\n80 million atoms to 1.1 $\\mu$K.",
        "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA.",
        "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.",
        "Fluid Antenna System (FAS) unlocks unprecedented flexibility in wireless\nchannel optimization through spatial reconfigurability. However, its practical\ndeployment is hindered by the coupled challenges posed by high-dimensional\nchannel estimation and real-time position optimization. This paper bridges\nwireless propagation physics with compressed sensing theory to address these\nchallenges through three aspects. First, we establish a group-sparse recovery\nframework for space-frequency characteristics (SFC) in FAS, formally\ncharacterizing leakage-induced sparsity degradation from limited aperture and\nbandwidth as a structured group-sparsity problem. By deriving\ndictionary-adapted group restricted isometry property (D-GRIP), we prove tight\nrecovery bounds for a convex $\\ell_1\/\\ell_2$-mixed norm optimization\nformulation that preserves leakage-aware sparsity patterns. Second, we develop\na Descending Correlation Group Orthogonal Matching Pursuit (DC-GOMP) algorithm\nthat systematically relaxes leakage constraints to reduce subcoherence. This\napproach enables robust FSC recovery with accelerated convergence and superior\nperformance compared to conventional compressive sensing methods like OMP or\nGOMP. Third, we formulate spatial equalization (SE) as a mixed-integer linear\nprogramming (MILP) problem, ensuring optimality through the branch-and-bound\nmethod. To achieve real-time implementability while maintaining near-optimal\nperformance, we complement this with a greedy algorithm.\n  Simulation results demonstrate the proposed channel estimation algorithm\neffectively resolves energy misallocation and enables recovery of weak details,\nachieving superior recovery accuracy and convergence rate. The SE framework\nsuppresses deep fading phenomena and reduces hardware deployment overhead while\nmaintaining equivalent link reliability.",
        "In this work we consider an unfolding of a normal form of the Lorenz system\nnear a triple-zero singularity. We are interested in the analysis of\ndouble-zero bifurcations emerging from that singularity. Their local study\nprovide partial results that are extended by means of numerical continuation\nmethods. Specifically, a curve of heteroclinic connections is detected. It has\na degenerate point from which infinitely many homoclinic connections emerge. In\nthis way, we can partially understand the dynamics near the triple-zero\nsingularity.",
        "The Generalized SU(2) Proca (GSU2P) theory has recently garnered attention\nfor its potential to describe key phases of cosmic evolution, including\nprimordial inflation and late-time accelerated expansion. However, its full\ncosmological implications remain unexplored. In this work, we perform a\ncomprehensive analysis of the dynamical properties of the GSU2P theory in a\nflat, homogeneous, and isotropic spacetime, through a dynamical-system\napproach. Our analysis reveals the presence of three pairs of fixed points, one\nof them corresponding to de-Sitter expansion which may represent either a\nstable or unstable phase in the evolution of the universe. These points,\nnonetheless, give rise to an indeterminate or infinite Hubble parameter, which\nrenders them cosmologically unviable. Additionally, we find two key\npseudostationary states: the ``attractor lines'', along which the system\nexhibits constant-roll dynamics, and the ``central zone'', characterized by\noscillatory radiation-like behaviour of the field. The dynamics within the\ncentral zone could represent a graceful exit from the primordial inflationary\nphase to a radiation dominated phase, or a state of the dark energy component\nprior to the late-time cosmic acceleration. However, within the central zone,\nthe dynamics of the vector field leads to recurrent instances of a nonphysical\nexpansion rate. The absence of a limit cycle in the central zone further\nexacerbates the issue, as the system may follow unbounded phase-space\ntrajectories, and the expansion rate becomes complex once it escapes the\nregion. Collectively, these challenges undermine the viability of the GSU2P\ntheory as a cosmological model for cosmic acceleration.",
        "The study of the long-time dynamics of quantum systems can be a real\nchallenge, especially in systems like ultracold gases, where the required\ntimescales may be longer than the lifetime of the system\n  itself. In this work, we show that it is possible to access the\n  long-time dynamics of a strongly repulsive atomic gas mixture in\n  shorter times. The shortcut-to-dynamics protocol that we propose\n  does not modify the fate of the observables, but effectively jumps\n  ahead in time without changing the system's inherent evolution. Just\n  like the next-chapter button in a movie player that allows to quickly reach\nthe part of the movie one wants to watch, it is a leap into the future.",
        "The new black hole transient Swift J1727.8--1613 exhibited a series of X-ray\nflares during its 2023 outburst extensively observed with Insight-HXMT. We\nanalyze the spectra of the flaring period using a series of models consisting\nof a multi-color disk and several different non-thermal components, and several\nconsistent conclusions are obtained among these models. First, Swift\nJ1727.8--1613 was in the transition process from the hard intermediate state\n(HIMS) to the very high state (VHS) during the first flaring period (MJD\n60197--60204), and afterwards it exhibited typical VHS parameter\ncharacteristics, such as high temperature of the disk inner radius and a steep\npower-law spectrum with a photon index of 2.6. Second, the flares in the VHS\nare characterized by a rapid increase in the flux of accretion disk,\naccompanied by a simultaneous rapid expansion of the inner radius, which could\nbe apparent if the accretion disk hardening factor varies significantly. The\nstrong power-law component during the VHS is likely produced by synchrotron\nself-Compton process in the relativistic jets, in agreement with the observed\nweak reflection component and lack of correlation with the disk component.",
        "We study the masses of blocks of the $\\Lambda$-coalescent with dust and some\naspects of their large and small time behaviors. To do so, we start by\nassociating the $\\Lambda$-coalescent to a nested interval-partition constructed\nfrom the flow of inverses, introduced by Bertoin and Le Gall in [Ann. inst.\nHenri Poincare (B) Probab. Stat. 41(3), 307-333 (2003)], of the\n$\\Lambda$-Fleming-Viot flow, and prove Poisson representations for the masses\nof blocks in terms of the flow of inverses. The representations enable us to\nuse the power of stochastic calculus to study the masses of blocks. We apply\nthis method to study the long and small time behaviors. In particular, for all\n$k>1$, we determine the decay rate of the expectation of the $k$-th largest\nblock as time goes to infinity and find that a cut-off phenomenon, related to\nthe presence of dust, occurs: the decay rate is increasing for small indices\n$k$ but remains constant after a fixed index depending on the measure\n$\\Lambda$.",
        "The spectral and photometric studies of the cataclysmic variable Gaia 19cwm\n(or ZTF19aamkwxk) have been performed. Based on the analysis of long-term\nvariability, it is concluded that the object belongs to WZ Sge type stars. The\nlight curves show eclipses recurring with an orbital period of $86.32048 \\pm\n0.00005$ min, as well as an out-of-eclipse variability with a period of\n$\\approx 6.45$ min. The latter period is stable for $\\sim 4$ years and appears\nto correspond to the rotation of a magnetic white dwarf, i.e., Gaia 19cwm is an\nintermediate polar. The Gaia 19cwm spectra show photospheric lines of the white\ndwarf, and Doppler tomograms demonstrate the presence of an accretion disk and\na hot spot. Analysis of the eclipse light curve gives an estimates of the white\ndwarf mass $M_1 = 0.66\\pm0.06$ M$_{\\odot}$, the donor mass $M_2 = 0.073 \\pm\n0.015$ M$_{\\odot}$, and the orbital inclination $i=83.8 \\pm 1.1^{\\circ}$.\nModeling of the spectral energy distribution gives the white dwarf temperature\nof $T_{eff}\\approx 13000 $ K. The X-ray luminosity $L_X = (1.6 \\pm 0.3) \\times\n10^{31}$ erg\/s allows to assign Gaia 19cwm to a small group of low-luminosity\nintermediate polars.",
        "The emission of a photon by an electron in an intense laser field is one of\nthe most fundamental processes in electrodynamics and underlies the many\napplications that utilize high-energy photon beams. This process is typically\nstudied for electrons colliding head-on with a stationary-focus laser pulse.\nHere, we show that the energy lost by electrons and the yield of emitted\nphotons can be substantially increased by replacing a stationary-focus pulse\nwith an equal-energy flying-focus pulse whose focus co-propagates with the\nelectrons. These advantages of the flying focus are a result of operating in\nthe quantum regime of the interaction, where the energy loss and photon yield\nscale more favorably with the interaction time than the laser intensity.\nSimulations of 10 GeV electrons colliding with 10 J pulses demonstrate these\nadvantages and predict a $5\\times$ increase in the yield of 1-20 MeV photons\nwith a flying focus pulse, which would impact applications in medicine,\nmaterial science, and nuclear physics.",
        "Supermassive black hole binaries (SMBHBs) are among the most powerful known\nsources of gravitational waves (GWs). Accordingly, these systems could dominate\nGW emission in the micro- and millihertz frequency range. Within this domain,\nSMBHs evolve rapidly and merge with each other. Dynamical friction from stars\nand gas at the centers of galaxies typically helps to bring together two SMBHs\nwhen they are at relatively far separations ($\\approx$ kpc $-$ 100 pc), but\nbecomes less efficient at smaller separations. However, dark matter (DM) spikes\naround SMBHs could enhance dynamical friction at close separations and, thus,\nshorten the evolution times. In this paper, we simulate the effects of DM\nspikes on GW signals in the micro- to millihertz frequency range and confirm\nthat the GW signals from SMBHBs with DM spikes can be clearly distinguished\nfrom those without any additional matter. Making use of the projected\nsensitivity curve of the Laser Interferometer Space Antenna (LISA), we forecast\nupper limits for the (dark) matter density for given future SMBHB observations.\nWe then compare these thresholds with the theoretical density profiles expected\nfor self-interacting dark matter (SIDM) spikes."
      ]
    }
  },
  {
    "id":2411.0908,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"On the use of AI for Generation of Functional Music to Improve Mental Health",
    "start_abstract":"Increasingly music has been shown to have both physical and mental health benefits including improvements in cardiovascular health, a link reduction of cases dementia elderly populations, markers general well-being such as stress reduction. Here, we describe short case studies addressing (anxiety, stress-reduction) through AI-driven generation. Engaging active listening music-making activities (especially for at risk age groups) can be particularly beneficial, the practice therapy helpful range use across wide range. However, access prohibitive terms expertize, materials, cost. Furthermore existing functional outcomes (such targeted improvement suggested above) hindered by issues repetition subsequent over-familiarity with material. In this paper, machine learning approaches which create informed biophysiological measurement two studies, target emotional states opposing ends Cartesian affective space (a dimensional emotion points ranging from descriptors relaxation, fear). Galvanic skin response is used marker psychological arousal an estimate state control signal training algorithm. This algorithm creates non-linear time series musical features sound synthesis \u201con-the-fly\u201d, using perceptually feature similarity model. We find interaction between familiarity perceived response. also report on psychometric evaluation generated material, consider how these - similar techniques might useful generation tasks, example, nonlinear sound-tracking that found interactive media or video games.",
    "start_categories":[
      "Mental Health"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Music Transformer: Generating Music with Long-Term Structure"
      ],
      "abstract":[
        "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Efficient Distributed Optimization under Heavy-Tailed Noise",
        "Screening and localization in the nonlinear Anderson problem",
        "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
        "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS\n  Demosaicing",
        "The slicing conjecture via small ball estimates",
        "On the Proportional Principal Stratum Hazards Model",
        "A Reference Architecture for Autonomous Networks: An Agent-Based\n  Approach",
        "The Large Hadron electron Collider as a bridge project for CERN",
        "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
        "On spectral scaling laws for averaged turbulence on the sphere",
        "Flaring Activities of Fast Rotating Stars have Solar-like Latitudinal\n  Distribution",
        "Underlying Semantic Diffusion for Effective and Efficient In-Context\n  Learning",
        "Path-Adaptive Matting for Efficient Inference Under Various\n  Computational Cost Constraints",
        "Automated Detection and Analysis of Minor Deformations in Flat Walls Due\n  to Railway Vibrations Using LiDAR and Machine Learning",
        "LiPS: Large-Scale Humanoid Robot Reinforcement Learning with\n  Parallel-Series Structures",
        "Dissecting the Impact of Model Misspecification in Data-driven\n  Optimization",
        "REINFORCE-ING Chemical Language Models in Drug Design",
        "First loosely coherent search for continuous gravitational wave sources\n  with substellar companions in the Orion spur",
        "Simpliciality of vector-valued function spaces",
        "An enhanced term in the Szeg\\H{o}-type asymptotics for the free massless\n  Dirac operator",
        "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End\n  Full-Length Song Generation with Latent Diffusion",
        "GO-VMP: Global Optimization for View Motion Planning in Fruit Mapping",
        "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python\n  using LLMs",
        "Symmetry of information for space-bounded online Kolmogorov complexity",
        "Toward Automated Potential Primary Asset Identification in Verilog\n  Designs",
        "Exploring Bayesian olfactory search in realistic turbulent flows",
        "Spherical Dense Text-to-Image Synthesis",
        "Fixed-Budget Change Point Identification in Piecewise Constant Bandits",
        "Hard Lefschetz Condition on symplectic non-K\\\"ahler solvmanifolds"
      ],
      "abstract":[
        "Distributed optimization has become the default training paradigm in modern\nmachine learning due to the growing scale of models and datasets. To mitigate\ncommunication overhead, local updates are often applied before global\naggregation, resulting in a nested optimization approach with inner and outer\nsteps. However, heavy-tailed stochastic gradient noise remains a significant\nchallenge, particularly in attention-based models, hindering effective\ntraining. In this work, we propose TailOPT, an efficient framework designed to\naddress heavy-tailed noise by leveraging adaptive optimization or clipping\ntechniques. We establish convergence guarantees for the TailOPT framework under\nheavy-tailed noise with potentially unbounded gradient variance and local\nupdates. Among its variants, we highlight a memory and communication efficient\ninstantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping\nat both the inner and outer optimizers, achieving adaptive-like performance\n(e.g., Adam) without the cost of maintaining or transmitting additional\ngradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates\nsuperior performance on several language tasks and models, outperforming\nstate-of-the-art methods.",
        "We resolve an existing question concerning the localization of a wave packet\nby random potential in the presence of weak nonlinearity. The problem has\ngained considerable interest in the literature, and it continues to attract\nattention due to its connection with the general properties of behavior of\nsystems with competition between nonlinearity, nonlocality and randomness. We\nfind that the nonlinearly localized state occurs through a finite polarization\nresponse from the lattice well beyond the assumptions of a perturbation-theory\napproach. For the vanishing polarization response the nonlinear localization\nlength diverges permitting unlimited spreading of the nonlinear field.",
        "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures.",
        "Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps:\/\/github.com\/Clausy9\/BMTNet.",
        "Bourgain's slicing conjecture was recently resolved by Joseph Lehec and Bo'az\nKlartag. We present an alternative proof by establishing small ball probability\nestimates for isotropic log-concave measures. Our approach relies on the\nstochastic localization process and Guan's bound, techniques also used by\nKlartag and Lehec. The link between small ball probabilities and the slicing\nconjecture was first observed by Dafnis and Paouris and is established through\nMilman's theory of M-ellipsoids.",
        "In clinical trials involving both mortality and morbidity, an active\ntreatment can influence the observed risk of the first non-fatal event either\ndirectly, through its effect on the non-fatal event process, or indirectly,\nthrough its effect on the death process, or both. Discerning the direct effect\nof treatment on the first non-fatal event holds clinical interest. However,\nwith the competing risk of death, the Cox proportional hazards model that\ntreats death as non-informative censoring and evaluates treatment effects on\ntime to the first non-fatal event provides an estimate of the cause-specific\nhazard ratio, which may not correspond to the direct effect. To obtain the\ndirect effect on the first non-fatal event, within the principal stratification\nframework, we define the principal stratum hazard and introduce the\nProportional Principal Stratum Hazards model. This model estimates the\nprincipal stratum hazard ratio, which reflects the direct effect on the first\nnon-fatal event in the presence of death and simplifies to the hazard ratio in\nthe absence of death. The principal stratum membership is identified using the\nshared frailty model, which assumes independence between the first non-fatal\nevent process and the potential death process from the counterfactual arm,\nconditional on per-subject random frailty. Simulation studies are conducted to\nverify the reliability of our estimators. We illustrate the method using the\nCarvedilol Prospective Randomized Cumulative Survival trial which involves\nheart-failure events.",
        "The vision of autonomous systems is becoming increasingly important in many\napplication areas, where the aim is to replace humans with agents. These\ninclude autonomous vehicles and other agents' applications in business\nprocesses and problem-solving. For networks, the increasing scale and operation\nand management (O&M) complexity drive the need for autonomous networks (AN).\nThe technical objective of AN is to ensure trustworthy O&M without human\nintervention for higher efficiency and lower operating costs. However,\nrealizing AN seems more difficult than autonomous vehicles. It encounters\nchallenges of networks' structural and functional complexity, which operate as\ndistributed dynamic systems governed by various technical and economic\nconstraints. A key problem lies in formulating a rigorous development\nmethodology that facilitates a seamless transition from traditional networks to\nAN. Central to this methodology is the definition of a reference architecture\nfor network agents, which specifies the required functionalities for their\nrealization, regardless of implementation choices. This article proposes a\nreference architecture characterizing main functional features, illustrating\nits application with network use cases. It shows how artificial intelligence\ncomponents can be used to implement the required functionality and its\ncoordination. The latter is achieved through the management and generation of\nshared domain-specific knowledge stored in long-term memory, ensuring the\noverall consistency of decisions and their execution. The article concludes\nwith a discussion of architecture specialization for building network layer\nagents. It also identifies the main technical challenges ahead, such as\nsatisfying essential requirements at development or runtime, as well as the\nissue of coordinating agents to achieve collective intelligence in meeting\noverall network goals.",
        "The LHeC is the project for delivering electron-nucleon collisions at CERN\nusing the HL-LHC beams. An Energy Recovery Linac in racetrack configuration\nwill provide 50 GeV electrons to achieve centre-of-mass energies around 1\nTeV\/nucleon and instantaneous luminosities around $10^{34}$ cm$^{-2}$s$^{-1}$.\nThe LHeC program elaborated in the CDR of 2021 included a phase with concurrent\noperation of electron-hadron and hadron-hadron collisions, followed by a\nstandalone phase of electron-hadron collisions only. In view of the current\nHL-LHC schedule, in this paper we have examined the possibilities of a program\nafter the regular HL-LHC program with only electron-proton operation. In this\noperation mode, the LHeC would serve as an impactful bridge project between\nmajor colliders at CERN. The standalone physics program comprises electroweak,\nHiggs, top-quark, BSM and strong-interaction physics. In addition, it empowers\nthe physics analyses at the HL-LHC by retrofitting measurements and searches\nwith significantly more precise knowledge of the proton structure and\n$\\alpha_s$. The accelerator technology deployed in the Energy Recovery Linac\nfor the LHeC is a major stepping-stone for the performance, cost reduction and\ntraining for future colliders. The capital investments in the LHeC electron\naccelerator can be reused in a cost-efficient way as the injector for the\nFCC-ee. Finally, data from the LHeC are essential to enable the physics\npotential of any new high-energy hadron collider. The operational plan of 6\nyears easily fits in the period between two major colliders at CERN. Similar to\nthe LHeC empowering the HL-LHC physics program, the FCC-eh would be an\nimpactful addition to the FCC physics program.",
        "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https:\/\/github.com\/Eureka-Maggie\/MIGE.",
        "Spectral analysis for a class of Lagrangian-averaged Navier--Stokes (LANS)\nequations on the sphere is carried out. The equations arise from the\nNavier--Stokes equations by applying a Helmholtz filter of width $\\alpha$ to\nthe advecting velocity $\\beta$ times. Power laws for the energy spectrum are\nderived and indicate a $\\beta$-dependent scaling at wave numbers $l$ with\n$\\alpha l\\gg 1$. The energy and enstrophy transfer rates distinctly depend on\nthe averaging, allowing control over the energy flux and the enstrophy flux\nseparately through the choice of averaging operator. A necessary condition on\nthe averaging operator is derived for the existence of the inverse cascade in\ntwo-dimensional turbulence. Numerical experiments with a structure-preserving\nintegrator confirm the expected energy spectrum scalings and the robustness of\nthe double cascade under choices of the averaging operator.",
        "The dynamo theory has always been one of the biggest mysteries in stellar\nphysics. One key reason for its uncertainty is poor knowledge of the dynamo\nprocess on stars except the Sun. The most important observation feature of\nsolar dynamo is that active regions only appear at low latitudes, which\nprovides a crucial constraint to the dynamo theory, while Doppler imaging, the\ncurrent technique to spatially resolve stellar hemisphere, is difficult to\ndistinguish the equatorial region . Hence, the latitudinal distribution of\nactive regions (LDAR) of stars is ambiguous and controversial, mainly due to\nthe limit of the current technique for spatially resolving the stellar surface.\nFast rotating stars, which are young and active, are thought to operate with a\ndifferent dynamo process than the Sun. We study their LDAR and compare them\nwith the Sun to reveal the underlying dynamo process. Flares are drastic and\nobservational activity events, which occur in active regions. Here, we propose\na new method to study how the apparent flaring activity varies with respect to\nthe inclination to determine the LDAR of fast rotating stars.We find that the\nLDAR of fast rotating stars is consistent with that of the Sun, contrary to\nexpectations. Our results provide a crucial constraint to stellar dynamo,\nindicating that the solar-like dynamo also applies to fast rotating stars, even\nspanning different stages of their evolution.",
        "Diffusion models has emerged as a powerful framework for tasks like image\ncontrollable generation and dense prediction. However, existing models often\nstruggle to capture underlying semantics (e.g., edges, textures, shapes) and\neffectively utilize in-context learning, limiting their contextual\nunderstanding and image generation quality. Additionally, high computational\ncosts and slow inference speeds hinder their real-time applicability. To\naddress these challenges, we propose Underlying Semantic Diffusion\n(US-Diffusion), an enhanced diffusion model that boosts underlying semantics\nlearning, computational efficiency, and in-context learning capabilities on\nmulti-task scenarios. We introduce Separate & Gather Adapter (SGA), which\ndecouples input conditions for different tasks while sharing the architecture,\nenabling better in-context learning and generalization across diverse visual\ndomains. We also present a Feedback-Aided Learning (FAL) framework, which\nleverages feedback signals to guide the model in capturing semantic details and\ndynamically adapting to task-specific contextual cues. Furthermore, we propose\na plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time\nsteps with high-noise levels, which aims at optimizing training and inference\nefficiency while maintaining strong in-context learning performance.\nExperimental results demonstrate that US-Diffusion outperforms the\nstate-of-the-art method, achieving an average reduction of 7.47 in FID on\nMap2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks,\nwhile achieving approximately 9.45 times faster inference speed. Our method\nalso demonstrates superior training efficiency and in-context learning\ncapabilities, excelling in new datasets and tasks, highlighting its robustness\nand adaptability across diverse visual domains.",
        "In this paper, we explore a novel image matting task aimed at achieving\nefficient inference under various computational cost constraints, specifically\nFLOP limitations, using a single matting network. Existing matting methods\nwhich have not explored scalable architectures or path-learning strategies,\nfail to tackle this challenge. To overcome these limitations, we introduce\nPath-Adaptive Matting (PAM), a framework that dynamically adjusts network paths\nbased on image contexts and computational cost constraints. We formulate the\ntraining of the computational cost-constrained matting network as a bilevel\noptimization problem, jointly optimizing the matting network and the path\nestimator. Building on this formalization, we design a path-adaptive matting\narchitecture by incorporating path selection layers and learnable connect\nlayers to estimate optimal paths and perform efficient inference within a\nunified network. Furthermore, we propose a performance-aware path-learning\nstrategy to generate path labels online by evaluating a few paths sampled from\nthe prior distribution of optimal paths and network estimations, enabling\nrobust and efficient online path learning. Experiments on five image matting\ndatasets demonstrate that the proposed PAM framework achieves competitive\nperformance across a range of computational cost constraints.",
        "This study introduces an advanced methodology for automatically identifying\nminor deformations in flat walls caused by vibrations from nearby railway\ntracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys\nand AI\/ML techniques to collect and analyze data. The scan data is processed\ninto a detailed point cloud, which is segmented to distinguish ground points,\ntrees, buildings, and other objects. The analysis focuses on identifying\nsections along flat walls and estimating their deformations relative to the\nground orientation.\n  Findings from the study, conducted at the RGIPT campus, reveal significant\ndeformations in walls close to the railway corridor, with the highest\ndeformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,\nwalls further from the corridor show negligible deformations. The developed\nautomated process for feature extraction and deformation monitoring\ndemonstrates potential for structural health monitoring. By integrating LiDAR\ndata with machine learning, the methodology provides an efficient system for\nidentifying and analyzing structural deformations, highlighting the importance\nof continuous monitoring for ensuring structural integrity and public safety in\nurban infrastructure. This approach represents a substantial advancement in\nautomated feature extraction and deformation analysis, contributing to more\neffective management of urban infrastructure.",
        "In recent years, research on humanoid robots has garnered significant\nattention, particularly in reinforcement learning based control algorithms,\nwhich have achieved major breakthroughs. Compared to traditional model-based\ncontrol algorithms, reinforcement learning based algorithms demonstrate\nsubstantial advantages in handling complex tasks. Leveraging the large-scale\nparallel computing capabilities of GPUs, contemporary humanoid robots can\nundergo extensive parallel training in simulated environments. A physical\nsimulation platform capable of large-scale parallel training is crucial for the\ndevelopment of humanoid robots. As one of the most complex robot forms,\nhumanoid robots typically possess intricate mechanical structures, encompassing\nnumerous series and parallel mechanisms. However, many reinforcement learning\nbased humanoid robot control algorithms currently employ open-loop topologies\nduring training, deferring the conversion to series-parallel structures until\nthe sim2real phase. This approach is primarily due to the limitations of\nphysics engines, as current GPU-based physics engines often only support\nopen-loop topologies or have limited capabilities in simulating\nmulti-rigid-body closed-loop topologies. For enabling reinforcement\nlearning-based humanoid robot control algorithms to train in large-scale\nparallel environments, we propose a novel training method LiPS. By\nincorporating multi-rigid-body dynamics modeling in the simulation environment,\nwe significantly reduce the sim2real gap and the difficulty of converting to\nparallel structures during model deployment, thereby robustly supporting\nlarge-scale reinforcement learning for humanoid robots.",
        "Data-driven optimization aims to translate a machine learning model into\ndecision-making by optimizing decisions on estimated costs. Such a pipeline can\nbe conducted by fitting a distributional model which is then plugged into the\ntarget optimization problem. While this fitting can utilize traditional methods\nsuch as maximum likelihood, a more recent approach uses estimation-optimization\nintegration that minimizes decision error instead of estimation error. Although\nintuitive, the statistical benefit of the latter approach is not well\nunderstood yet is important to guide the prescriptive usage of machine\nlearning. In this paper, we dissect the performance comparisons between these\napproaches in terms of the amount of model misspecification. In particular, we\nshow how the integrated approach offers a ``universal double benefit'' on the\ntop two dominating terms of regret when the underlying model is misspecified,\nwhile the traditional approach can be advantageous when the model is nearly\nwell-specified. Our comparison is powered by finite-sample tail regret bounds\nthat are derived via new higher-order expansions of regrets and the leveraging\nof a recent Berry-Esseen theorem.",
        "Chemical language models, combined with reinforcement learning, have shown\nsignificant promise to efficiently traverse large chemical spaces in drug\ndesign. However, the performance of various RL algorithms and their best\npractices for practical drug design are still unclear. Here, starting from the\nprinciples of the REINFORCE algorithm, we investigate the effect of different\ncomponents from RL theory including experience replay, hill-climbing, baselines\nto reduce variance, and alternative reward shaping. Additionally we demonstrate\nhow RL hyperparameters can be fine-tuned for effectiveness, efficiency, or\nchemical regularization as demonstrated using the MolOpt benchmark.",
        "We report on the first loosely coherent search for binary systems. We\nsearched 0.06 rad disk in the Orion spur, covering gravitational wave\nfrequencies from 100 to 700 Hz and frequency derivatives between -1e-11 to\n1e-11 Hz\/s. A follow-up was performed, which found no outliers. An atlas of\nresults from the first stage of the search is made publicly available.",
        "We investigate integral representation of vector-valued function spaces,\ni.e., of subspaces $H\\subset C(K,E)$, where $K$ is a compact space and $E$ is a\n(real or complex) Banach space. We point out that there are two possible ways\nof generalizing representation theorems known from the scalar case -- either\none may represent (all) functionals from $H^*$ using $E^*$-valued vector\nmeasures on $K$ (as it is done in the literature) or one may represent (some)\noperators from $L(H,E)$ by scalar measures on $K$ using the Bochner integral.\nThese two ways lead to two different notions of simpliciality which we call\n`vector simpliciality' and `weak simpliciality'. It turns out that these two\nnotions are in general incomparable. Moreover, the weak simpliciality is not\naffected by renorming the target space $E$, while vector simpliciality may be\naffected. Further, if $H$ contains constants, vector simpliciality is strictly\nstronger and admits several characterizations (partially analogous to the\ncharacterizations known in the scalar case). We also study orderings of\nmeasures inspired by C.J.K.~Batty which may be (in special cases) used to\ncharacterize $H$-boundary measures. Finally, we give a finer version of\nrepresentation theorem using positive measures on $K\\times B_{E^*}$ and\ncharacterize uniqueness in this case.",
        "We consider a regularised Fermi projection of the Hamiltonian of the massless\nDirac equation at Fermi energy zero. The matrix-valued symbol of the resulting\noperator is discontinuous in the origin. For this operator, we prove\nSzeg\\H{o}-type asymptotics with the spatial cut-off domains given by\n$d$-dimensional cubes. For analytic test functions, we obtain a $d$-term\nasymptotic expansion and provide an upper bound of logarithmic order for the\nremaining terms. This bound does not depend on the regularisation. In the\nspecial case that the test function is given by a polynomial of degree less or\nequal than three, we prove a $(d+1)$-term asymptotic expansion with an error\nterm of constant order. The additional term is of logarithmic order and its\ncoefficient is independent of the regularisation.",
        "Recent advancements in music generation have garnered significant attention,\nyet existing approaches face critical limitations. Some current generative\nmodels can only synthesize either the vocal track or the accompaniment track.\nWhile some models can generate combined vocal and accompaniment, they typically\nrely on meticulously designed multi-stage cascading architectures and intricate\ndata pipelines, hindering scalability. Additionally, most systems are\nrestricted to generating short musical segments rather than full-length songs.\nFurthermore, widely used language model-based methods suffer from slow\ninference speeds. To address these challenges, we propose DiffRhythm, the first\nlatent diffusion-based song generation model capable of synthesizing complete\nsongs with both vocal and accompaniment for durations of up to 4m45s in only\nten seconds, maintaining high musicality and intelligibility. Despite its\nremarkable capabilities, DiffRhythm is designed to be simple and elegant: it\neliminates the need for complex data preparation, employs a straightforward\nmodel structure, and requires only lyrics and a style prompt during inference.\nAdditionally, its non-autoregressive structure ensures fast inference speeds.\nThis simplicity guarantees the scalability of DiffRhythm. Moreover, we release\nthe complete training code along with the pre-trained model on large-scale data\nto promote reproducibility and further research.",
        "Automating labor-intensive tasks such as crop monitoring with robots is\nessential for enhancing production and conserving resources. However,\nautonomously monitoring horticulture crops remains challenging due to their\ncomplex structures, which often result in fruit occlusions. Existing view\nplanning methods attempt to reduce occlusions but either struggle to achieve\nadequate coverage or incur high robot motion costs. We introduce a global\noptimization approach for view motion planning that aims to minimize robot\nmotion costs while maximizing fruit coverage. To this end, we leverage coverage\nconstraints derived from the set covering problem (SCP) within a shortest\nHamiltonian path problem (SHPP) formulation. While both SCP and SHPP are\nwell-established, their tailored integration enables a unified framework that\ncomputes a global view path with minimized motion while ensuring full coverage\nof selected targets. Given the NP-hard nature of the problem, we employ a\nregion-prior-based selection of coverage targets and a sparse graph structure\nto achieve effective optimization outcomes within a limited time. Experiments\nin simulation demonstrate that our method detects more fruits, enhances surface\ncoverage, and achieves higher volume accuracy than the motion-efficient\nbaseline with a moderate increase in motion cost, while significantly reducing\nmotion costs compared to the coverage-focused baseline. Real-world experiments\nfurther confirm the practical applicability of our approach.",
        "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues.",
        "The even online Kolmogorov complexity of a string $x = x_1 x_2 \\cdots x_{n}$\nis the minimal length of a program that for all $i\\le n\/2$, on input $x_1x_3\n\\cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The\nsum of the odd and even complexities is called the dialogue complexity.\n  In [Bauwens, 2014] it is proven that for all $n$, there exist $n$-bit $x$ for\nwhich the dialogue complexity exceeds the Kolmogorov complexity by $n\\log \\frac\n4 3 + O(\\log n)$. Let $\\mathrm C^s(x)$ denote the Kolmogorov complexity with\nspace bound~$s$. Here, we prove that the space-bounded dialogue complexity with\nbound $s + 6n + O(1)$ is at most $\\mathrm C^{s}(x) + O(\\log (sn))$, where\n$n=|x|$.",
        "With greater design complexity, the challenge to anticipate and mitigate\nsecurity issues provides more responsibility for the designer. As hardware\nprovides the foundation of a secure system, we need tools and techniques that\nsupport engineers to improve trust and help them address security concerns.\nKnowing the security assets in a design is fundamental to downstream security\nanalyses, such as threat modeling, weakness identification, and verification.\nThis paper proposes an automated approach for the initial identification of\npotential security assets in a Verilog design. Taking inspiration from manual\nasset identification methodologies, we analyze open-source hardware designs in\nthree IP families and identify patterns and commonalities likely to indicate\nstructural assets. Through iterative refinement, we provide a potential set of\nprimary security assets and thus help to reduce the manual search space.",
        "The problem of tracking the source of a passive scalar in a turbulent flow is\nrelevant to flying insect behavior and several other applications. Extensive\nprevious work has shown that certain Bayesian strategies, such as \"infotaxis,\"\ncan be very effective for this difficult \"olfactory search\" problem. More\nrecently, a quasi-optimal Bayesian strategy was computed under the assumption\nthat encounters with the scalar are independent. However, the Bayesian approach\nhas not been adequately studied in realistic flows which exhibit spatiotemporal\ncorrelations. In this work, we perform direct numerical simulations (DNS) of an\nincompressible flow at $\\mathrm{Re}_\\lambda\\simeq150,$ while tracking\nLagrangian particles which are emitted by a point source and imposing a uniform\nmean flow with several magnitudes (including zero). We extract the\nspatially-dependent statistics of encounters with the particles, which we use\nto build Bayesian policies, including generalized (\"space-aware\") infotactic\nheuristics and quasi-optimal policies. We then assess the relative performance\nof these policies when they are used to search using scalar cue data from the\nDNS, and in particular study how this performance depends on correlations\nbetween encounters. Among other results, we find that quasi-optimal strategies\ncontinue to outperform heuristics in the presence of strong mean flow but fail\nto do so in the absence of a mean flow. We also explore how to choose optimal\nsearch parameters, including the frequency and threshold concentration of\nobservation.",
        "Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image",
        "We study the piecewise constant bandit problem where the expected reward is a\npiecewise constant function with one change point (discontinuity) across the\naction space $[0,1]$ and the learner's aim is to locate the change point. Under\nthe assumption of a fixed exploration budget, we provide the first\nnon-asymptotic analysis of policies designed to locate abrupt changes in the\nmean reward function under bandit feedback. We study the problem under a large\nand small budget regime, and for both settings establish lower bounds on the\nerror probability and provide algorithms with near matching upper bounds.\nInterestingly, our results show a separation in the complexity of the two\nregimes. We then propose a regime adaptive algorithm which is near optimal for\nboth small and large budgets simultaneously. We complement our theoretical\nanalysis with experimental results in simulated environments to support our\nfindings.",
        "We provide new families of compact complex manifolds with no K\\\"ahler\nstructure carrying symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. These examples are obtained as compact quotients of the solvable\nLie group $\\mathbb{C}^{2n} \\ltimes_{\\rho} \\mathbb{C}^{2m}$, for which we\nconstruct explicit lattices. By cohomological computations we prove that such\nmanifolds carry symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. Furthermore, we compute the Kodaira dimension of an almost-K\\\"ahler\nstructure and generators for the de Rham and Dolbeault cohomologies."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Solar Cells for Indoor Applications: Progress and Development",
    "start_abstract":"The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications.",
    "start_categories":[
      "astro-ph.SR"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      ],
      "abstract":[
        "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Limited Effectiveness of LLM-based Data Augmentation for COVID-19\n  Misinformation Stance Detection",
        "The normal growth of linear groups over formal power serieses",
        "The multi-level friendship paradox for sparse random graphs",
        "The duality resolution at $n=p=2$",
        "Optimal Control of the Navier-Stokes equations via Pressure Boundary\n  Conditions",
        "Clinically Ready Magnetic Microrobots for Targeted Therapies",
        "How can representation dimension dominate structurally pruned LLMs?",
        "Deterministic Global Optimization over trained Kolmogorov Arnold\n  Networks",
        "EOG Communication Interface for Quadriplegics: Prototype & Signal\n  Processing",
        "The Popularity Hypothesis in Software Security: A Large-Scale\n  Replication with PHP Packages",
        "Transformer Based Time-Series Forecasting for Stock",
        "Defending Against Gradient Inversion Attacks for Biomedical Images via\n  Learnable Data Perturbation",
        "Cognitive AI framework: advances in the simulation of human thought",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Negativity in Self-Admitted Technical Debt: How Sentiment Influences\n  Prioritization",
        "LGL-BCI: A Motor-Imagery-Based Brain-Computer Interface with Geometric\n  Learning",
        "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
        "InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models",
        "Translate Smart, not Hard: Cascaded Translation Systems with\n  Quality-Aware Deferral",
        "Two characterizations of Sheffer-Dunkl sequences",
        "A random polymer approach to the weak disorder phase of the vertex\n  reinforced jump process",
        "A priori estimates of Mizohata-Takeuchi type for the Navier-Lam\\'e\n  operator",
        "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits",
        "Prediction-Assisted Online Distributed Deep Learning Workload Scheduling\n  in GPU Clusters",
        "ComplexBeat: Breathing Rate Estimation from Complex CSI",
        "A Basis Theorem for Rings with Commuting Operators in Characteristic\n  Zero",
        "Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs",
        "$q$-Numerical radius of sectorial matrices and $2 \\times 2$ operator\n  matrices",
        "Application of Artificial Intelligence (AI) in Civil Engineering"
      ],
      "abstract":[
        "Misinformation surrounding emerging outbreaks poses a serious societal\nthreat, making robust countermeasures essential. One promising approach is\nstance detection (SD), which identifies whether social media posts support or\noppose misleading claims. In this work, we finetune classifiers on COVID-19\nmisinformation SD datasets consisting of claims and corresponding tweets.\nSpecifically, we test controllable misinformation generation (CMG) using large\nlanguage models (LLMs) as a method for data augmentation. While CMG\ndemonstrates the potential for expanding training datasets, our experiments\nreveal that performance gains over traditional augmentation methods are often\nminimal and inconsistent, primarily due to built-in safeguards within LLMs. We\nrelease our code and datasets to facilitate further research on misinformation\ndetection and generation.",
        "Put $R=\\F[[t_1, \\ldots, t_d]])$. We estimate the number of normal subgroups\nof $\\mathrm{SL}_2^1(\\F[[t_1, \\ldots, t_d]])$ for $p>2$, the number of ideals in\nthe Lie algebra $\\Lie(R)$, and the number of ideals in the associative algebra\n$R$.",
        "In Hazra, den Hollander and Parvaneh (2025) we analysed the friendship\nparadox for sparse random graphs. For four classes of random graphs we\ncharacterised the empirical distribution of the friendship biases between\nvertices and their neighbours at distance $1$, proving convergence as\n$n\\to\\infty$ to a limiting distribution, with $n$ the number of vertices, and\nidentifying moments and tail exponents of the limiting distribution. In the\npresent paper we look at the multi-level friendship bias between vertices and\ntheir neighbours at distance $k \\in \\mathbb{N}$ obtained via a $k$-step\nexploration according to a backtracking or a non-backtracking random walk. We\nidentify the limit of empirical distribution of the multi-level friendship\nbiases as $n\\to\\infty$ and\/or $k\\to\\infty$. We show that for non-backtracking\nexploration the two limits commute for a large class of sparse random graphs,\nincluding those that locally converge to a rooted Galton-Watson tree. In\nparticular, we show that the same limit arises when $k$ depends on $n$, i.e.,\n$k=k_n$, provided $\\lim_{n\\to\\infty} k_n = \\infty$ under some mild conditions.\nWe exhibit cases where the two limits do not commute and show the relevance of\nthe mixing time of the exploration.",
        "Working at the prime $2$ and chromatic height $2$, we construct a finite\nresolution of the homotopy fixed points of Morava $E$-theory with respect to\nthe subgroup $\\mathbb{G}_2^1$ of the Morava stabilizer group. This is an\nupgrade of the finite resolution of the homotopy fixed points of $E$-theory\nwith respect to the subgroup $\\mathbb{S}_2^1$ constructed in work of\nGoerss-Henn-Mahowald-Rezk, Beaudry and Bobkova-Goerss.",
        "In this work we study an optimal control problem subject to the instationary\nNavier-Stokes equations, where the control enters via an inhomogeneous\nNeumann\/Do-Nothing boundary condition. Despite the Navier-Stokes equations with\nthese boundary conditions not being well-posed for large times and\/or data, we\nobtain wellposedness of the optimal control problem by choosing a proper\ntracking type term. In order to discuss the regularity of the optimal control,\nstate and adjoint state, we present new results on $L^2(I;H^2(\\Omega))$\nregularity of solutions to a Stokes problem with mixed inhomogeneous boundary\nconditions.",
        "Systemic drug administration often causes off-target effects limiting the\nefficacy of advanced therapies. Targeted drug delivery approaches increase\nlocal drug concentrations at the diseased site while minimizing systemic drug\nexposure. We present a magnetically guided microrobotic drug delivery system\ncapable of precise navigation under physiological conditions. This platform\nintegrates a clinical electromagnetic navigation system, a custom-designed\nrelease catheter, and a dissolvable capsule for accurate therapeutic delivery.\nIn vitro tests showed precise navigation in human vasculature models, and in\nvivo experiments confirmed tracking under fluoroscopy and successful navigation\nin large animal models. The microrobot balances magnetic material\nconcentration, contrast agent loading, and therapeutic drug capacity, enabling\neffective hosting of therapeutics despite the integration complexity of its\ncomponents, offering a promising solution for precise targeted drug delivery.",
        "Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct.",
        "To address the challenge of tractability for optimizing mathematical models\nin science and engineering, surrogate models are often employed. Recently, a\nnew class of machine learning models named Kolmogorov Arnold Networks (KANs)\nhave been proposed. It was reported that KANs can approximate a given\ninput\/output relationship with a high level of accuracy, requiring\nsignificantly fewer parameters than multilayer perceptrons. Hence, we aim to\nassess the suitability of deterministic global optimization of trained KANs by\nproposing their Mixed-Integer Nonlinear Programming (MINLP) formulation. We\nconduct extensive computational experiments for different KAN architectures.\nAdditionally, we propose alternative convex hull reformulation, local support\nand redundant constraints for the formulation aimed at improving the\neffectiveness of the MINLP formulation of the KAN. KANs demonstrate high\naccuracy while requiring relatively modest computational effort to optimize\nthem, particularly for cases with less than five inputs or outputs. For cases\nwith higher inputs or outputs, carefully considering the KAN architecture\nduring training may improve its effectiveness while optimizing over a trained\nKAN. Overall, we observe that KANs offer a promising alternative as surrogate\nmodels for deterministic global optimization.",
        "Electrooculography (EOG) is an electrophysiological signal that determines\nthe human eye orientation and is therefore widely used in Human Tracking\nInterfaces (HCI). The purpose of this project is to develop a communication\nmethod for quadriplegic patients using EOG signals aimed at text and voice\ngeneration. The system consists of 3D eye movement tracking embedded using a\ncustom-built prototype to measure the eyeball's left-right and up-down\nmovements. The ESP32 board, which has a set of parameters to convert the data\ninto content displayed on LCDs and MP3 players, is used to capture and process\nthe signal. helps people by facilitating more natural and efficient symptom\nexpression. The blink system will be able to incorporate face masks and more\neye tests as it continues to develop. Even if it might work, more research and\nclinical trials are needed to evaluate the system's usefulness and ensure that\nit performs as planned in real-world scenarios. With this project, assistive\ntechnology will make significant progress and improve the lives of many who\nsuffer from severe motor impairments.",
        "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security.",
        "To the naked eye, stock prices are considered chaotic, dynamic, and\nunpredictable. Indeed, it is one of the most difficult forecasting tasks that\nhundreds of millions of retail traders and professional traders around the\nworld try to do every second even before the market opens. With recent advances\nin the development of machine learning and the amount of data the market\ngenerated over years, applying machine learning techniques such as deep\nlearning neural networks is unavoidable. In this work, we modeled the task as a\nmultivariate forecasting problem, instead of a naive autoregression problem.\nThe multivariate analysis is done using the attention mechanism via applying a\nmutated version of the Transformer, \"Stockformer\", which we created.",
        "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data.",
        "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "Self-Admitted Technical Debt, or SATD, is a self-admission of technical debt\npresent in a software system. To effectively manage SATD, developers need to\nestimate its priority and assess the effort required to fix the described\ntechnical debt. About a quarter of descriptions of SATD in software systems\nexpress some form of negativity or negative emotions when describing technical\ndebt. In this paper, we report on an experiment conducted with 59 respondents\nto study whether negativity expressed in the description of SATD\n\\textbf{actually} affects the prioritization of SATD. The respondents are a mix\nof professional developers and students, and in the experiment, we asked\nparticipants to prioritize four vignettes: two expressing negativity and two\nexpressing neutral sentiment. To ensure realism, vignettes were based on\nexisting SATD. We find that negativity causes between one-third and half of\ndevelopers to prioritize SATD, in which negativity is expressed as having more\npriority. Developers affected by negativity when prioritizing SATD are twice as\nlikely to increase their estimation of urgency and 1.5 times as likely to\nincrease their estimation of importance and effort for SATD compared to the\nlikelihood of decreasing these prioritization scores. Our findings show how\ndevelopers actively use negativity in SATD to determine how urgently a\nparticular instance of TD should be addressed. However, our study also\ndescribes a gap in the actions and belief of developers. Even if 33% to 50% use\nnegativity to prioritize SATD, 67% of developers believe that using negativity\nas a proxy for priority is unacceptable. Therefore, we would not recommend\nusing negativity as a proxy for priority. However, we also recognize that\ndevelopers might unavoidably express negativity when describing technical debt.",
        "Brain--computer interfaces are groundbreaking technology whereby brain\nsignals are used to control external devices. Despite some advances in recent\nyears, electroencephalogram (EEG)-based motor-imagery tasks face challenges,\nsuch as amplitude and phase variability and complex spatial correlations, with\na need for smaller models and faster inference. In this study, we develop a\nprototype, called the Lightweight Geometric Learning Brain--Computer Interface\n(LGL-BCI), which uses our customized geometric deep learning architecture for\nswift model inference without sacrificing accuracy. LGL-BCI contains an EEG\nchannel selection module via a feature decomposition algorithm to reduce the\ndimensionality of a symmetric positive definite matrix, providing adaptiveness\namong the continuously changing EEG signal. Meanwhile, a built-in lossless\ntransformation helps boost the inference speed. The performance of our solution\nwas evaluated using two real-world EEG devices and two public EEG datasets.\nLGL-BCI demonstrated significant improvements, achieving an accuracy of 82.54%\ncompared to 62.22% for the state-of-the-art approach. Furthermore, LGL-BCI uses\nfewer parameters (64.9K vs. 183.7K), highlighting its computational efficiency.\nThese findings underscore both the superior accuracy and computational\nefficiency of LGL-BCI, demonstrating the feasibility and robustness of\ngeometric deep learning in motor-imagery brain--computer interface\napplications.",
        "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
        "Prompt tuning has become a popular strategy for adapting Vision-Language\nModels (VLMs) to zero\/few-shot visual recognition tasks. Some prompting\ntechniques introduce prior knowledge due to its richness, but when learnable\ntokens are randomly initialized and disconnected from prior knowledge, they\ntend to overfit on seen classes and struggle with domain shifts for unseen\nones. To address this issue, we propose the InPK model, which infuses\nclass-specific prior knowledge into the learnable tokens during initialization,\nthus enabling the model to explicitly focus on class-relevant information.\nFurthermore, to mitigate the weakening of class information by multi-layer\nencoders, we continuously reinforce the interaction between learnable tokens\nand prior knowledge across multiple feature levels. This progressive\ninteraction allows the learnable tokens to better capture the fine-grained\ndifferences and universal visual concepts within prior knowledge, enabling the\nmodel to extract more discriminative and generalized text features. Even for\nunseen classes, the learned interaction allows the model to capture their\ncommon representations and infer their appropriate positions within the\nexisting semantic structure. Moreover, we introduce a learnable text-to-vision\nprojection layer to accommodate the text adjustments, ensuring better alignment\nof visual-text semantics. Extensive experiments on 11 recognition datasets show\nthat InPK significantly outperforms state-of-the-art methods in multiple\nzero\/few-shot image classification tasks.",
        "Larger models often outperform smaller ones but come with high computational\ncosts. Cascading offers a potential solution. By default, it uses smaller\nmodels and defers only some instances to larger, more powerful models. However,\ndesigning effective deferral rules remains a challenge. In this paper, we\npropose a simple yet effective approach for machine translation, using existing\nquality estimation (QE) metrics as deferral rules. We show that QE-based\ndeferral allows a cascaded system to match the performance of a larger model\nwhile invoking it for a small fraction (30% to 50%) of the examples,\nsignificantly reducing computational costs. We validate this approach through\nboth automatic and human evaluation.",
        "Sheffer polynomials can be characterized using different Stieltjes integrals.\nThese families of polynomials have been recently extended to the Dunkl context.\nIn this way some classical operators as the derivative operator or the\ndifference operator are replaced as analogous operators in the Dunkl universe.\nIn this paper we establish two Stieltjes integrals that help us to characterize\nthe Sheffer-Dunkl polynomials.",
        "In this paper, we study the transient phase of the Vertex Reinforced Jump\nProcess (VRJP) in dimension $d\\geq 3$. In Sabot, Zeng (2019), the authors\nintroduce a positive martingale and show that the VRJP is recurrent if and only\nif that martingale converges to $0$. On $\\mathbb{Z}^d$, $d\\ge 3$, with constant\nconductances $W$, it can be shown that there is a critical value\n$0<W_c(\\mathbb{Z}^d)<\\infty$, such that the martingale converges to $0$ if\n$W<W_c(\\mathbb{Z}^d)$ or to a positive limit if $W>W_c(\\mathbb{Z}^d)$. On the\nother hand, the VRJP martingale can be interpreted as the partition function of\na non-directed polymer with a very specific $1$-dependent random potential. In\nthis paper, we focus on the question of the $L^p$ integrability of the VRJP\nmartingale, which is related to the (diffusive) behavior of the VRJP. First,\ntaking inspiration from the work of Junk (2022) for directed polymers in\n$\\mathbb{Z}^{1+d}$, we prove that on the half-space $\\mathbb{H}_d$ of\n$\\mathbb{Z}^d$, for all $W>W_c(\\mathbb{H}_d)$ there is some $\\delta>0$ such\nthat the VRJP martingale is in $L^{1+\\delta}$. Second, we prove that, in\ndimension $d\\geq 4$, the VRJP martingale is in $L^{p}$ for all $p>1$ above the\n``slab critical point'' $W_c^{\\mathrm{slab}} (\\mathbb{Z}^d) = \\lim_{m\\to\\infty}\nW_c(\\mathbb{Z}^{d-1} \\times \\{-m,\\ldots,m\\})$. We also propose some related\nconjectures.",
        "The Mizohata-Takeuchi conjecture for the resolvent of the Navier-Lam\\'e\nequation is a weighted estimate with weights in the so-called Mizohata-Takeuchi\nclass for this operator when one approaches the spectrum (Limiting Absorption\nPrinciples). We prove this conjecture in dimensions 2 and 3 for weights with a\nradial majorant in the Mizohata-Takeuchi class. This result can be seen as an\nextension of the analogue for the Laplacian given in [8]. We also prove that\nradial weights in this class are not invariant for the Hardy-Littlewood maximal\nfunction, hence the methods in [6] used to extend estimates for the Laplacian\nto the Navier-Lam\\'e case, do not work.",
        "We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.",
        "The recent explosive growth of deep learning (DL) models has necessitated a\ncompelling need for efficient job scheduling for distributed deep learning\ntraining with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes\nan adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling\nalgorithm, a novel prediction-assisted online scheduling approach designed to\nmitigate the challenges associated with DL cluster scheduling. By modeling each\njob as a graph corresponding to heterogeneous Deep Neural Network (DNN) models\nand their associated distributed training configurations, A-SRPT strategically\nassigns jobs to the available GPUs, thereby minimizing inter-server\ncommunication overhead. Observing that most DDLwMP jobs recur, A-SRPT\nincorporates a random forest regression model to predict training iterations.\nCrucially, A-SRPT maps the complex scheduling problem into a single-machine\ninstance, which is addressed optimally by a preemptive\n\"shortest-remaining-processing-time-first\" strategy. This optimized solution\nserves as a guide for actual job scheduling within the GPU clusters, leading to\na theoretically provable competitive scheduling efficiency. We conduct\nextensive real-world testbed and simulation experiments to verify our proposed\nalgorithms.",
        "In this paper, we explore the use of channel state information (CSI) from a\nWiFi system to estimate the breathing rate of a person in a room. In order to\nextract WiFi CSI components that are sensitive to breathing, we propose to\nconsider the delay domain channel impulse response (CIR), while most\nstate-of-the-art methods consider its frequency domain representation. One\nobstacle while processing the CSI data is that its amplitude and phase are\nhighly distorted by measurement uncertainties. We thus also propose an\namplitude calibration method and a phase offset calibration method for CSI\nmeasured in orthogonal frequency-division multiplexing (OFDM) multiple-input\nmultiple-output (MIMO) systems. Finally, we implement a complete breathing rate\nestimation system in order to showcase the effectiveness of our proposed\ncalibration and CSI extraction methods.",
        "Motivated by the differential basis theorem of Kolchin and the\ndifference-differential basis theorem of Cohn, in this paper we present a basis\ntheorem for polynomial rings equipped with commuting generalised Hasse-Schmidt\noperators (in the sense of Moosa and Scanlon). We recover Kolchin and Cohn's\nresults as special cases of our main theorem.",
        "Linguistic evaluations of how well LMs generalize to produce or understand\nnovel text often implicitly take for granted that natural languages are\ngenerated by symbolic rules. Grammaticality is thought to be determined by\nwhether or not sentences obey such rules. Interpretation is believed to be\ncompositionally generated by syntactic rules operating on meaningful words.\nSemantic parsing is intended to map sentences into formal logic. Failures of\nLMs to obey strict rules have been taken to reveal that LMs do not produce or\nunderstand language like humans. Here we suggest that LMs' failures to obey\nsymbolic rules may be a feature rather than a bug, because natural languages\nare not based on rules. New utterances are produced and understood by a\ncombination of flexible interrelated and context-dependent schemata or\nconstructions. We encourage researchers to reimagine appropriate benchmarks and\nanalyses that acknowledge the rich flexible generalizations that comprise\nnatural languages.",
        "This article focuses on several significant bounds of $q$-numerical radius\n$w_q(A)$ for sectorial matrix $A$ which refine and generalize previously\nestablished bounds. One of the significant bounds we have derived is as\nfollows:\n  \\[\\frac{|q|^2\\cos^2\\alpha}{2} \\|A^*A+AA^*\\| \\le w_q^2(A)\\le\n\\frac{\\left(\\sqrt{(1-|q|^2)\\left(1+2sin^2(\\alpha)\\right)}+ |q|\\right)^2}{2}\n\\|A^*A+AA^*\\|,\\]\n  where $ A $ is a sectorial matrix. Also, upper bounds for commutator and\nanti-commutator matrices and relations between $w_q(A^t)$ and $w_q^t(A)$ for\nnon-integral power $t\\in [0,1]$ are also obtained. Moreover, a few significant\nestimations of $q$-numerical radius of off-diagonal $2\\times2$ operator\nmatrices are developed.",
        "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "start_abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement).",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Solar Cells for Indoor Applications: Progress and Development"
      ],
      "abstract":[
        "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
      ],
      "categories":[
        "astro-ph.SR"
      ]
    },
    "list":{
      "title":[
        "The MAGPI Survey: the subtle role of environment and not-so-subtle\n  impact of generations of stars on galaxy dynamics",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Noncommutative Phantom BTZ Black Hole",
        "Spectral synthesis in multidimensional Fourier algebras",
        "Chiral Vibrational Modes in Small Molecules",
        "Stability of oscillations in the spatially extended May-Leonard model",
        "Zero modes and Dirac-(logarithmic) Sobolev-type inequalities",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Phonon anomalies within the polar charge density wave phase of\n  superconductor Mo$_3$Al$_2$C with structural chirality",
        "Tuning Quantum States at Chirality-Reversed Planar Interface in Weyl\n  Semimetals using an Interstitial Layer",
        "Using Matrix-Free Tensor-Network Optimizations to Construct a\n  Reduced-Scaling and Robust Second-Order M{\\o}ller-Plesset Theory",
        "A filtered two-step variational integrator for charged-particle dynamics\n  in a normal or strong magnetic field",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Prediction for close approaches with terrestrial planets of asteroids\n  from the main belt",
        "Infrared Metaplasmonics",
        "Atmospheric Circulation of Close-In Extrasolar Giant Planets: The\n  Diabatic Equivalent-Barotropic Model",
        "Morphogenesis of cheese flowers through scraping",
        "Quantum correlations and spatial localization in trapped one-dimensional\n  ultra-cold Bose-Bose-Bose mixtures",
        "Finitely additive measures on Boolean algebras",
        "Effective theory of light Dirac neutrino portal dark matter with\n  observable ${\\Delta N_{\\rm eff}}$",
        "Hardware Acceleration for HPS Algorithms in Two and Three Dimensions",
        "Generalized Uncertainty Relation Between an Observable and Its\n  Derivative",
        "The tardigrade as an emerging model organism for systems neuroscience",
        "Euclid Quick Data Release (Q1). A probabilistic classification of\n  quenched galaxies",
        "Interactive visualization of large molecular systems with VTX: example\n  with a minimal whole-cell model",
        "Phase space geometry of collective spin systems: Scaling and Fractality",
        "The uncommon intracluster medium features of the first massive clusters\n  selected independently of their baryon content",
        "Improved Decoding of Tanner Codes",
        "A diagrammatic formulation of local realism"
      ],
      "abstract":[
        "The stellar age and mass of galaxies have been suggested as the primary\ndeterminants for the dynamical state of galaxies, with environment seemingly\nplaying no or only a very minor role. We use a sample of 77 galaxies at\nintermediate redshift (z~0.3) in the Middle-Ages Galaxies Properties with\nIntegral field spectroscopy (MAGPI) Survey to study the subtle impact of\nenvironment on galaxy dynamics. We use a combination of statistical techniques\n(simple and partial correlations and principal component analysis) to isolate\nthe contribution of environment on galaxy dynamics, while explicitly accounting\nfor known factors such as stellar age, star formation histories and stellar\nmasses. We consider these dynamical parameters: high-order kinematics of the\nline-of-sight velocity distribution (parametrised by the Gauss-Hermite\ncoefficients $h_3$ and $h_4$), kinematic asymmetries $V_{\\rm asym}$ derived\nusing kinemetry and the observational spin parameter proxy $\\lambda_{R_e}$. Of\nthese, the mean $h_4$ is the only parameter found to have a significant\ncorrelation with environment as parametrised by group dynamical mass. This\ncorrelation exists even after accounting for age and stellar mass trends.\nFinally, we confirm that variations in the spin parameter $\\lambda_{R_e}$ are\nmost strongly (anti-)correlated with age as seen in local studies, and show\nthat this dependence is well-established by z~0.3.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "This work explores the thermodynamic and geometric properties of phantom BTZ\nblack holes within the framework of noncommutative spacetime, where\nnoncommutative effects are incorporated via Lorentzian distributions for mass\nand charge. The resulting modifications in spacetime geometry introduce\nsignificant alterations to horizon structures and curvature singularities. A\ncomprehensive and comparative thermodynamic analysis is conducted, examining\nthe differences between phantom and ordinary matter cases. This includes an\ninvestigation of Hawking temperature, entropy, heat capacity, and stability\ncriteria. Additionally, the black hole is analyzed as a thermodynamic heat\nengine, with its efficiency evaluated as a function of noncommutative\nparameters. Our findings highlight the profound impact of noncommutativity on\nthe thermodynamic behavior and efficiency of phantom BTZ black holes, revealing\nnew insights into the interplay between quantum spacetime effects and exotic\nfield dynamics. The results indicate that noncommutative corrections not only\nmodify the stability conditions of these black holes but also play a crucial\nrole in governing phase transitions. Furthermore, we demonstrate that\nnoncommutativity influences energy extraction processes, refining our\nunderstanding of black hole thermodynamics in lower-dimensional spacetimes and\ndistinguishing the behavior of phantom and ordinary matter cases.",
        "Let $G$ be a locally compact group and let $A^n(G)$ denote the\n$n$-dimensional Fourier algebra, introduced by Todorov and Turowska. We\ninvestigate spectral synthesis properties of the multidimensional Fourier\nalgebra $A^n(G).$ In particular, we prove versions of the subgroup lemma,\ninjection, and inverse projection theorems for both spectral sets and Ditkin\nsets. Additionally, we provide a result on the parallel synthesis between\n$A^n(G)$ and $A^{n+1}(G)$ and finally prove Malliavin's theorem.",
        "The development of quantitative methods for characterizing molecular\nchirality can provide an important tool for studying chirality induced\nphenomena in molecular systems. Significant progress has been made in recent\nyears toward understanding the chirality of molecular normal vibrational modes,\nmostly focusing on vibrations of helical molecular structures. In the present\nstudy, we examine the applicability two methodologies previously used for\nhelical structures for the quantification of the chirality of molecular normal\nmodes across a range of small, not necessarily helical, molecules. The first\napproach involves the application of the Continuous Chirality Measure (CCM) to\neach normal mode by associating the mode with a structure formed by imposing\nthe corresponding motion about a common origin. The second approach assigns to\neach normal mode a pseudoscalar defined as the product of atomic linear and\nangular momentum summed over all atoms. In particular, using the CCM also as a\nmeasure of the chirality of the underlying molecular structure, we establish\nthe existence of correlation between the chirality of molecular normal modes\nand that of the underlying molecular structure. Furthermore, we find that\nnormal modes associated with different frequency ranges of the molecular\nvibrational spectrum exhibit distinct handedness behavior.",
        "The May-Leonard model for three competing species, symmetric with respect to\n  cyclic permutation of the variables and extended by diffusive terms, is\nconsidered.\n  Exact time-periodic solutions of the system have been found, and their\nstability\n  with respect to spatially periodic disturbances is studied. The stability of\nsolu tions with respect to longwave spatial modulations is revealed. A period\ndoubling\n  instability breaking the spatial uniformity is found.",
        "We study the decay rate of the zero modes of the Dirac operator with a\nmatrix-valued potential that is considered here without any regularity\nassumptions, compared to the existing literature. For the Dirac operator and\nfor Clifford-valued functions we prove the $L^p$-$L^2$ Dirac Sobolev inequality\nwith explicit constant, as well as the $L^p$-$L^q$ Dirac-Sobolev inequalities.\nWe prove its logarithmic counterpart for $q=2$, extending it to its Gaussian\nversion of Gross, as well as show Nash and Poincar\\'e inequalities in this\nsetting, with explicit values for constants.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "We employ polarization-resolved Raman spectroscopy to study the lattice\ndynamics of the polar charge density wave phase of the superconductor\nMo$_3$Al$_2$C with structural chirality. We show the phononic signatures of the\ncharge density wave transition at $T^*$=155K in Mo$_3$Al$_2$C. The detailed\ntemperature dependence of these phonon modes' frequency,\nhalf-width-at-half-maximum, and the integrated area below $T^*$ reveal\nanomalies at an intermediate temperature $T'\\sim$100K, especially for the\nlow-energy modes at 130cm$^{-1}$ and 180cm$^{-1}$. Since these low-energy modes\nare dominated by Mo-related lattice vibration, we propose that lattice\nanomalies at $T'$ within the charge density wave phase are related to a\nmodification of the Mo displacements while preserving the crystal symmetry.",
        "The electronic band structure of Weyl semimetals possesses pairs of linear\nband crossings, called Weyl nodes, characterized by opposite chirality charges\nassociated with each node. The momentum space position of the nodes can reverse\nacross a planar interface and these host Fermi-arc-like bound states, in\naddition to scattering states. We show that a magnetic interstitial layer can\ntune these states in three distinct ways. The electrostatic potential and one\nof the in-plane magnetic potential components control the shape of the bound\nstate Fermi-arcs. For moderate values of the same in-plane magnetic potential\nelectrons are spin-filtered across the interface, while both the in-plane\nmagnetic components and the electrostatic potential control the transmission of\nelectrons. The ratio of in-plane to out-of-plane magnetic components can be\nused to turn on or turn off the magnetic potential effects, since the latter\ndoes not affect the interface states. The tunability arises from spin-momentum\nlocking and chirality reversal at the interface. Thus, the effects can mix or\ninterchange depending on the specific material but the states will remain\ntunable.",
        "We investigate the application of the canonical polyadic decomposition (CPD)\nto the tensor hypercontraction (THC) and Laplace transform (LT) approximated\nsecond-order M{\\o}ller-Plesset (MP2) method. By introducing these\ndecompositions we formally reduce the scaling of the canonical MP2 method from\n$\\mathcal{O}(N^5)$ to $\\mathcal{O}(N^3)$ and the storage complexity from\n$\\mathcal{O}(N^4)$ to $\\mathcal{O}(N^2)$. We are able to construct the THC\nrepresentation in $\\mathcal{O}(N^3)$ time by employing the interpolative\nseparable density fitting decomposition strategy. Furthermore, we introduce a\nCPD optimization strategy that takes advantage of the THC representation to\ndecompose the order-four two-electron integral tensor with a computational\nscaling of $\\mathcal{O}(N^3)$. Finally, we show that the rank of the CPD in the\napproximation of MP2 scales linearly with system size and that this CPD-ISDF-LT\nMP2 strategy realizes a performance advantage over canonical LT MP2 in both\ncomputational wall-times and memory resource requirements.",
        "This article is concerned with a new filtered two-step variational integrator\nfor solving the charged-particle dynamics in a mildly non-homogeneous normal or\nstrong magnetic field with a dimensionless parameter $\\epsilon$ inversely\nproportional to the strength of the magnetic field. In the case of a normal\nmagnetic field ($\\epsilon \\approx 1$), second-order error bounds and long time\nenergy and momentum conservations are obtained. Moreover, the proof of the\nlong-term analysis is accomplished by the backward error analysis. For the\nstrong magnetic field ($0<\\epsilon \\ll1$), this paper clarifies the behaviour\nof the filtered variational integrator for both a large stepsize $h^2 \\geq\n\\epsilon$ and a smaller stepsize $ h \\sim \\epsilon$. The approach to analysing\nthe error bounds for these two stepsizes is based on comparing the modulated\nFourier expansions of the exact and the numerical solutions. It is shown that\nthe proposed integrator achieves a second-order accuracy $\\mathcal{O}(h^2)$ in\nthe position and in the parallel velocity for a large step size and an\n$\\mathcal{O}(\\epsilon)$ accuracy for a smaller stepsize. This paper also yields\nthe long time energy and magnetic moment conservations for the strong magnetic\nfield by developing the modulated Fourier expansion of the proposed scheme. All\nthe theoretical results of the error behaviour and long-term conservations are\nnumerically demonstrated by two numerical experiments.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Potentially Hazardous Asteroids (PHAs), a special subset of Near-Earth\nObjects, are both dangerous and scientifically valuable. PHAs that truly\nundergo close approaches with the Earth (dubbed CAPHAs) are of particular\ninterest and extensively studied. The concept and study of CAPHA can be\nextended to other Solar system planets, which have significant implications for\nfuture planet-based observations and explorations. In this work, we conduct\nnumerical simulations that incorporate the Yarkovsky effect to study the\ntransformation of main belt asteroids into CAPHAs of terrestrial planets, using\nprecise nominal timesteps, especially to ensure the reliability of the results\nfor Mercury and Venus. Our simulations predict a total of 1893 Mercury-CAPHAs,\n3014 Venus-CAPHAs, 3791 Earth-CAPHAs and 18066 Mars-CAPHAs, with an occurrence\nfrequency of about 1, 9, 15 and 66 per year, respectively. The values for\nMars-CAPHAs are consistent with our previous work, which were based on\nsimulations with a larger nominal timestep. The predicted occurrence frequency\nand velocity distribution of Earth-CAPHAs are in reasonable agreement with the\nobserved population of Earth-CAPHAs. We also find that certain asteroids can be\ncaught in close approach with different planets at different times, raising an\ninteresting possibility of using them as transportation between terrestrial\nplanets in the future.",
        "Plasmonic response in metals, defined as the ability to support subwavelength\nconfinement of surface plasmon modes, is typically limited to a narrow\nfrequency range below the metals' plasma frequency. This places severe\nlimitations on the operational wavelengths of plasmonic materials and devices.\nHowever, when the volume of a metal film is massively decreased, highly\nconfined quasi-two-dimensional surface plasmon modes can be supported out to\nwavelengths well beyond the plasma wavelength. While this has, thus far, been\nachieved using ultra-thin (nm-scale) metals, such films are quite difficult to\nrealize, and suffer from even higher losses than bulk plasmonic films. To\nextend the plasmonic response to the infrared, here we introduce the concept of\nmetaplasmonics, representing a novel plasmonic modality with a host of\nappealing properties. By fabricating and characterizing a series of\nmetaplasmonic nanoribbons, we demonstrate large confinement, high quality\nfactors, and large near-field enhancements across a broad wavelength range,\nextending well beyond the limited bandwidth of traditional plasmonic materials.\nWe demonstrate $35\\times$ plasmon wavelength reduction, and our numerical\nsimulations suggest that further wavelength reduction, up to a factor of 150,\nis achievable using our approach. The demonstration of the metaplasmonics\nparadigm offers a promising path to fill the near- and mid-infrared\ntechnological gap for high quality plasmonic materials, and provides a new\nmaterial system to study the effects of extreme plasmonic confinement for\napplications in nonlinear and quantum plasmonics.",
        "We extend the description of equivalent-barotropic equations for exoplanets\nto the diabatic case -- that is, with explicit heating and\/or cooling\nrepresentation, rather than with a stationary deflection of the bottom bounding\nsurface. In the diabatic case, the equation for potential temperature (or\nentropy) is directly forced and cannot be decoupled from the equations for\nmomentum and nonlinear pressure, the mass-like variable; and, the isentropic\nsurfaces do not remain coincident with material surfaces. Here the formalism is\npresented for an atmosphere with the Lamb vertical structure, as the formalism\nis substantially simplified under the structure. The equations presented set\nthe stage for accurate global simulations which permit small-scale vortices,\ngravity waves, and fronts observed in current three-dimensional global\nsimulations to be studied in detail.",
        "The \"Tete de moine\" Swiss cheese is generally served by scraping the surface\nof a cylindrical loaf with a sharp tool. This produces thin sheets of cheese\nthat are strongly wrinkled at the edge, resembling frilly flowers and enhancing\nthe tasting experience. In this work we unveil the physical mechanisms at play\nin this scraping-induced morphogenesis. We measure the deformation of the\ncheese during scraping and show that plastic deformation occurs everywhere, but\nfind a larger plastic contraction in the inner part of the flower, causing its\nbuckling into shape. We show that it surprisingly derives from the lower\nfriction coefficient evidenced on the cheese close to its crust. Our analysis\nprovides the tools for a better control of chip morphogenesis through\nplasticity in the shaping of other delicacies, but also in metal cutting.",
        "We systematically investigate and illustrate the complete ground-state phase\ndiagram for a one-dimensional, three-species mixture of a few repulsively\ninteracting bosons trapped harmonically. To numerically obtain the solutions to\nthe many-body Schr\\\"{o}dinger equation, we employ the improved Exact\nDiagonalization method [T. D. Anh-Tai {\\it et al.}, SciPost Physics 15, 048\n(2023)], which is capable of treating strongly-correlated few-body systems from\nfirst principles in an efficiently truncated Hilbert space. We present our\ncomprehensive results for all possible combinations of intra- and interspecies\ninteractions in the extreme limits that are either the ideal limit ($g=0$) or\nclose to the hard-core limit ($g\\to\\infty$). These results show the emergence\nof unique ground-state properties related to correlations, coherence and\nspatial localization stemming from strongly repulsive interactions.",
        "In this article, we conduct a detailed study of \\emph{finitely additive\nmeasures} (fams) in the context of Boolean algebras, focusing on three specific\ntopics: freeness and approximation, existence and extension criteria, and\nintegration theory. In the first topic, we present a classification of\n\\emph{free} finitely additive measures, that is, those for which the measure of\nfinite sets is zero, in terms of approximation to uniform probability measures.\nThis inspires a weaker version of this notion, which we call the \\emph{uniform\napproximation property}, characterized in terms of freeness and another\nwell-determined type of fams we call \\emph{uniformly supported}. In the second\ntopic, we study criteria for existence and extension of finitely additive\nmeasures for Boolean algebras, offering a relatively short proof of the\n\\emph{compatibility theorem} for fams.\n  Finally, we study a Riemann-type integration theory on fields of sets with\nrespect to finitely additive measures, allowing us to extend and generalize\nsome classical concepts and results from real analysis, such as Riemann\nintegration over rectangles in $\\mathbb{R}^{n}$ and the Jordan measure. We also\ngeneralize the extension criteria for fams allowing desired values of integrals\nof a given set of functions. At the end, we explore the connection between\nintegration in fields of sets and the Lebesgue integration in the Stone space\nof the corresponding field, where we establish a characterization of\nintegrability in the sense of the Lebesgue-Vitali theorem, which follows as a\nconsequence of our results.",
        "We study the possibility of light Dirac neutrino portal dark matter (DM) in\nan effective field theory (EFT) setup. Dirac nature of light neutrino\nautomatically includes its right chiral part $\\nu_R$ which, in our setup, also\nacts like a portal between DM and the standard model (SM) particles.\nConsidering a Dirac fermion singlet DM stabilised by an unbroken $Z_2$\nsymmetry, we write down all possible dimension-6 effective operators involving\nDM-$\\nu_R$ as well as $\\nu_R$-SM which conserve $Z_2$, global lepton number and\nSM gauge symmetries. DM thermalisation also ensures the thermalisation of\n$\\nu_R$, leading to enhanced effective relativistic degrees of freedom $N_{\\rm\neff}$, within reach of future cosmic microwave background (CMB) experiments. We\nstudy the complementarity among DM and CMB related observations for different\nLorentz structures of effective operators. We also propose two UV completions\nbased on the popularly studied gauges $\\rm B-L$ and left-right symmetric model\nframeworks.",
        "We provide a flexible, open-source framework for hardware acceleration,\nnamely massively-parallel execution on general-purpose graphics processing\nunits (GPUs), applied to the hierarchical Poincar\\'e--Steklov (HPS) family of\nalgorithms for building fast direct solvers for linear elliptic partial\ndifferential equations. To take full advantage of the power of hardware\nacceleration, we propose two variants of HPS algorithms to improve performance\non two- and three-dimensional problems. In the two-dimensional setting, we\nintroduce a novel recomputation strategy that minimizes costly data transfers\nto and from the GPU; in three dimensions, we modify and extend the adaptive\ndiscretization technique of Geldermans and Gillman [2019] to greatly reduce\npeak memory usage. We provide an open-source implementation of these methods\nwritten in JAX, a high-level accelerated linear algebra package, which allows\nfor the first integration of a high-order fast direct solver with automatic\ndifferentiation tools. We conclude with extensive numerical examples showing\nour methods are fast and accurate on two- and three-dimensional problems.",
        "The generalized uncertainty connection between the fluctuations of a quantum\nobservable and its temporal derivative is derived in this study, we demonstrate\nthat the product of an observable's uncertainties and its time derivative is\nbounded by half the modulus of the expectation value of the commutator between\nthe observable and its derivative, using the Cauchy Schwarz inequality and the\nstandard definitions of operator variances. In order to connect the dynamical\nevolution of observables to their inherent uncertainties, we reformulate the\nbound in terms of a double commutator by expressing the derivative in terms of\nthe Hamiltonian via the Heisenberg equation of motion. Next, we apply this\ngeneralized relation to a spin particle to demonstrate its usefulness in a\nmagnetic field that changes over time, and expand the study to include\nobservables that have a clear temporal dependence. Our findings provide greater\nunderstanding of quantum dynamics and the influence of time-dependent\ninteractions on measurement precision in addition to recovering the traditional\nuncertainty relations for static systems.",
        "We present the case for developing the tardigrade (Hypsibius exemplaris) into\na model organism for systems neuroscience. These microscopic, transparent\nanimals (~300-500 microns) are among the smallest known to possess both limbs\n(eight) and eyes (two), with a nervous system of only a few hundred neurons\norganized into a multi-lobed brain, ventral nerve cord, and a series of ganglia\nalong the body. Despite their neuroanatomical simplicity, tardigrades exhibit\ncomplex behaviors, including multi-limbed walking gaits, individual limb\ngrasping, phototaxis, and transitions between active and dormant states. These\nbehaviors position tardigrades as a uniquely powerful system for addressing\ncertain fundamental questions in systems neuroscience, such as: How do nervous\nsystems coordinate multi-limbed behaviors? How are top-down and bottom-up motor\ncontrol systems integrated? How is stereovision-guided navigation implemented?\nWhat mechanisms underlie neural resilience and recovery during environmental\nstress? We review current knowledge of tardigrade neuroanatomy, behavior, and\ngenomics, and we identify opportunities and challenges for leveraging their\nunique biology. We propose developing essential neuroscientific tools for\ntardigrades, including genetic engineering and live neuroimaging, alongside\nbehavioral assays linking neural activity to outputs. Leveraging their\nevolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can\nadapt existing toolkits to accelerate tardigrade research - providing a bridge\nbetween simpler invertebrate systems and more complex neural architectures.",
        "Investigating the drivers of the quenching of star formation in galaxies is\nkey to understanding their evolution. The Euclid mission will provide rich\nspatial and spectral data from optical to infrared wavelengths for millions of\ngalaxies, enabling precise measurements of their star formation histories.\nUsing the first Euclid Quick Data Release (Q1), we developed a probabilistic\nclassification framework, that combines the average specific star-formation\nrate ($\\rm sSFR_\\tau$) inferred over two timescales ($\\tau={10^8,10^9}$ yr), to\ncategorize galaxies as `Ageing' (secularly evolving), `Quenched' (recently\nhalted star formation), or `Retired' (dominated by old stars). We validated\nthis methodology using synthetic observations from the IllustrisTNG simulation.\nTwo classification methods were employed: a probabilistic approach, integrating\nposterior distributions, and a model-driven method optimizing sample purity and\ncompleteness using IllustrisTNG. At $z<0.1$ and $M_\\ast \\gtrsim 3\\times10^{8}\\,\nM_\\odot$, we obtain Euclid class fractions of 68-72%, 8-17%, and 14-19% for\nAgeing, Quenched, and Retired populations, respectively, consistent with\nprevious studies. The evolution with redshift shows increasing\/decreasing\nfraction of Ageing\/Retired galaxies. The fraction of quenched systems shows a\nweaker dependence on stellar mass and redshift, varying between 5% and 15%. We\nanalysed the mass-size-metallicity relation for each population. Ageing\ngalaxies generally exhibit disc morphologies and low metallicities. Retired\ngalaxies show compact structures and enhanced chemical enrichment, while\nQuenched galaxies form an intermediate population, more compact and chemically\nevolved than Ageing systems. This work demonstrates Euclid's great potential\nfor elucidating the physical nature of the quenching mechanisms that govern\ngalaxy evolution.",
        "VTX is an open-source molecular visualization software designed to overcome\nthe scaling limitations of existing real-time molecular visualization software\nwhen handling massive molecular datasets. VTX employs a meshless molecular\ngraphics engine utilizing impostor-based techniques and adaptive\nlevel-of-detail (LOD) rendering. This approach significantly reduces memory\nusage and enables real-time visualization and manipulation of large molecular\nsystems. Performance benchmarks against VMD, PyMOL, and ChimeraX using a\n114-million-bead Martini minimal whole-cell model demonstrate VTX's efficiency,\nmaintaining consistent frame rates even under interactive manipulation on\nstandard computer hardware. VTX incorporates features such as screen-space\nambient occlusion (SSAO) for enhanced depth perception and free-fly navigation\nfor intuitive exploration of large molecular systems. VTX is open-source and\nfree for non commercial use. Binaries for Windows and Ubuntu Linux are\navailable at \\href{http:\/\/vtx.drugdesign.fr}{http:\/\/vtx.drugdesign.fr}. VTX\nsource code is available at\n\\href{https:\/\/github.com\/VTX-Molecular-Visualization}{https:\/\/github.com\/VTX-Molecular-Visualization}.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework.",
        "Our current knowledge of the thermodynamic properties of galaxy clusters\ncomes primarily from detailed studies of clusters selected by their minority\ncomponents: hot baryons. Most of these studies select the clusters using the\ncomponent that is being investigated, the intracluster medium (ICM), making the\nsample choice prone to selection effects. Weak-gravitational lensing allows us\nto select clusters by the total mass component and, being independent of the\ntype of matter, makes the sample choice unbiased with respect to the baryon\ncontent. In this paper, we study four galaxy clusters at intermediate redshift\n($0.25<z<0.61$), selected from the weak-lensing survey of Miyazaki et al.\n(2018). We derive core-excised X-ray luminosities, richness-based masses,\nCompton parameters, and profiles of mass, pressure and electron densities.\nThese quantities are derived from shear data, Compton maps, and our own X-ray\nand SZ follow-up. When compared to ICM-selected clusters of the same mass, in\nthe range $2$ to $5 \\ 10^{14}$ M$_\\odot$, our small sample of four clusters is\nexpected to have on average 0.2 rare ($>2\\sigma$) features, while we observed\non average two rare features in each one of the seven explored properties:\nrichness, core-excised luminosity, Compton parameter, pressure and electron\npressure profiles, and central values of them. The abundance of rare and unique\nfeatures in such a small sample indicates a fundamental bias in our knowledge\nof the thermodynamic properties of clusters when derived from ICM-selected\nsamples.",
        "In this paper, we present improved decoding algorithms for expander-based\nTanner codes.\n  We begin by developing a randomized linear-time decoding algorithm that,\nunder the condition that $ \\delta d_0 > 2 $, corrects up to $ \\alpha n $ errors\nfor a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \\alpha, \\delta)\n$-bipartite expander with $n$ left vertices, and $ C_0 \\subseteq \\mathbb{F}_2^d\n$ is a linear inner code with minimum distance $ d_0 $. This result improves\nupon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024),\nwhich required $ \\delta d_0 > 3 $.\n  We further derandomize the algorithm to obtain a deterministic linear-time\ndecoding algorithm with the same decoding radius. Our algorithm improves upon\nthe previous deterministic algorithm of Cheng et al.\\ by achieving a decoding\nradius of $ \\alpha n $, compared with the previous radius of $\n\\frac{2\\alpha}{d_0(1 + 0.5c\\delta) }n$.\n  Additionally, we investigate the size-expansion trade-off introduced by the\nrecent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to\nprovide new bounds on the minimum distance of Tanner codes. Specifically, we\nprove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately\n$f_\\delta^{-1} \\left( \\frac{1}{d_0} \\right) \\alpha n $, where $ f_\\delta(\\cdot)\n$ is the Size-Expansion Function. As another application, we improve the\ndecoding radius of our decoding algorithms from $\\alpha n$ to approximately\n$f_\\delta^{-1}(\\frac{2}{d_0})\\alpha n$.",
        "Given two parties performing experiments in separate laboratories, we provide\na diagrammatic formulation of what it means for the joint statistics of their\nexperiments to satisfy local realism. In particular, we show that the\nprinciples of locality and realism are both captured by a single commutative\ndiagram in the category of probability-preserving maps between finite\nprobability spaces, and we also show that an assumption of such a diagrammatic\nformulation of local realism implies the standard CHSH inequality associated\nwith dichotomic random variables. As quantum theory is known not to satisfy\nlocal realism, our formulation of local realism in terms of commutative\ndiagrams provides yet another way in which the notion of non-commutativity\nplays a fundamental role in quantum theory. We note that we do not assume any\nprior knowledge of category theory or quantum theory, as this work is intended\nfor philosophers, mathematicians and physicists alike."
      ]
    }
  }
]