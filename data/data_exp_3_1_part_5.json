[
  {
    "id":2411.04775,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"A Data\u2013Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition",
    "start_abstract":"The Koopman operator is a linear but infinite dimensional operator that\ngoverns the evolution of scalar observables defined on the state space of an\nautonomous dynamical system, and is a powerful tool for the analysis and\ndecomposition of nonlinear dynamical systems. In this manuscript, we present a\ndata driven method for approximating the leading eigenvalues, eigenfunctions,\nand modes of the Koopman operator. The method requires a data set of snapshot\npairs and a dictionary of scalar observables, but does not require explicit\ngoverning equations or interaction with a \"black box\" integrator. We will show\nthat this approach is, in effect, an extension of Dynamic Mode Decomposition\n(DMD), which has been used to approximate the Koopman eigenvalues and modes.\nFurthermore, if the data provided to the method are generated by a Markov\nprocess instead of a deterministic dynamical system, the algorithm approximates\nthe eigenfunctions of the Kolmogorov backward equation, which could be\nconsidered as the \"stochastic Koopman operator\" [1]. Finally, four illustrative\nexamples are presented: two that highlight the quantitative performance of the\nmethod when presented with either deterministic or stochastic data, and two\nthat show potential applications of the Koopman eigenfunctions.",
    "start_categories":[
      "Mathematical Analysis"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b21",
        "b14"
      ],
      "title":[
        "Towards Scalable Koopman Operator Learning: Convergence Rates and A Distributed Learning Algorithm",
        "VAMPnets for deep learning of molecular kinetics"
      ],
      "abstract":[
        "We propose an alternating optimization algorithm to the nonconvex Koopman operator learning problem for nonlinear dynamic systems. show that proposed will converge a critical point with rate O(1\/T) and $O\\left( {\\frac{1}{{\\log T}}} \\right)$ constant diminishing rates, respectively, under some mild conditions. To cope high dimensional dynamical systems, we present first-ever distributed algorithm. has same convergence properties as centralized learning, in absence of optimal tracker, so long basis functions satisfy set state-based decomposition Numerical experiments are provided complement our theoretical results.",
        "Abstract There is an increasing demand for computing the relevant structures, equilibria, and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation simulated coordinates into structural features, dimension reduction, clustering dimension-reduced data, estimation a Markov state model or related interconversion rates between structures. This handcrafted approach demands substantial amount modeling expertise, poor decisions at any step will lead to large errors. Here we variational processes (VAMP) develop deep learning framework using neural networks, dubbed VAMPnets. A VAMPnet encodes entire mapping states, thus combining whole data processing pipeline in single end-to-end framework. Our method performs equally better than state-of-the-art provides easily interpretable few-state kinetic models."
      ],
      "categories":[
        "eess.SP",
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "The complexities of falling freely",
        "On Construction, Properties and Simulation of Haar-Based Multifractional\n  Processes",
        "Characterization of Residual Charge Images in LSST Camera e2v CCDs",
        "A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration",
        "CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial\n  Forecasting by Integrating Time Series Patterns and Salient Macroeconomic\n  Announcements",
        "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders\n  Decomposition",
        "Recent progress in high-temperature superconducting undulators",
        "Steady State Classification of Allee Effect System",
        "Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial\n  CDW Ground State",
        "ARFlow: Autogressive Flow with Hybrid Linear Attention",
        "Neighborhoods, connectivity, and diameter of the nilpotent graph of a\n  finite group",
        "The day-ahead scenario generation method for new energy based on an\n  improved conditional generative diffusion model",
        "A Jacobian-free Newton-Krylov method for cell-centred finite volume\n  solid mechanics",
        "The defocusing Calogero--Moser derivative nonlinear Schr{\\\"o}dinger\n  equation with a nonvanishing condition at infinity",
        "MemorySAM: Memorize Modalities and Semantics with Segment Anything Model\n  2 for Multi-modal Semantic Segmentation",
        "Lifetime measurement of the 5s5p 1P1 state in strontium",
        "Frequency-resolved time lags due to X-ray disk reprocessing in AGN",
        "Regularity of edge ideals of powers of graphs",
        "Symmetries of the q-deformed real projective line",
        "Functoriality of Coulomb branches",
        "Exploring Large Language Models for Translating Romanian Computational\n  Problems into English",
        "On the Conditional Phase Distribution of the TWDP Multipath Fading\n  Process",
        "Free products and rescalings involving non-separable abelian von Neumann\n  algebras",
        "The effect of a band gap gradient on the radiative losses in the open\n  circuit voltage of solar cells",
        "Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy",
        "READ: Reinforcement-based Adversarial Learning for Text Classification\n  with Limited Labeled Data",
        "Heating of a semi-infinite Hooke chain",
        "Pullback measure attractors for non-autonomous stochastic\n  FitzHugh-Nagumo system with distribution dependence on unbounded domains"
      ],
      "abstract":[
        "Suppose you drop a coin from 10 feet above the ground. How long does it take\nto reach the ground? This routine exercise is well-known to every AP physics\nand calculus student: the answer is given by a formula that assumes constant\nacceleration due to gravity. But what if you ask the same question in the more\nrealistic scenario of non-constant acceleration following an inverse square\nlaw? In this article, we explain the analysis of this realistic scenario using\nfreshman-level calculus and examine some implications. As a bonus, we also\nanswer the following intriguing question: Suppose the Earth were to\ninstantaneously collapse to a mathematical point at its center. How long would\nit take for us surface dwellers to fall to the center?",
        "Multifractional processes extend the concept of fractional Brownian motion by\nreplacing the constant Hurst parameter with a time-varying Hurst function. This\nextension allows for modulation of the roughness of sample paths over time. The\npaper introduces a new class of multifractional processes, the Gaussian\nHaar-based multifractional processes (GHBMP), which is based on the Haar\nwavelet series representations. The resulting processes cover a significantly\nbroader set of Hurst functions compared to the existing literature, enhancing\ntheir suitability for both practical applications and theoretical studies. The\ntheoretical properties of these processes are investigated. Simulation studies\nconducted for various Hurst functions validate the proposed model and\ndemonstrate its applicability, even for Hurst functions exhibiting\ndiscontinuous behaviour.",
        "LSST Camera CCDs produced by the manufacturer e2v exhibit strong and novel\nresidual charge images when exposed to bright sources. These manifest in images\nfollowing bright exposures both in the same pixel areas as the bright source,\nand in the pixels trailing between the source and the serial register. Both of\nthese pose systematic challenges to the Rubin Observatory Legacy Survey of\nSpace and Time instrument signature removal. The latter trail region is\nespecially impactful as it affects a much larger pixel area in a less well\ndefined position. In our study of this effect at UC Davis, we imaged bright\nspots to characterize these residual charge effects. We find a strong\ndependence of the residual charge on the parallel clocking scheme, including\nthe relative levels of the clocking voltages, and the timing of gate phase\ntransition during the parallel transfer. Our study points to independent causes\nof residual charge in the bright spot region and trail region. We propose\npotential causes in both regions and suggest methodologies for minimizing\nresidual charge. We consider the trade-offs to these methods including\ndecreasing the camera's full well and dynamic range at the high end. Some of\nthese results and suggestions have been reviewed by the camera commissioning\nteam and may result in changes made to the clocking voltage scheme on the LSST\nCamera.",
        "Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.",
        "Accurately forecasting the impact of macroeconomic events is critical for\ninvestors and policymakers. Salient events like monetary policy decisions and\nemployment reports often trigger market movements by shaping expectations of\neconomic growth and risk, thereby establishing causal relationships between\nevents and market behavior. Existing forecasting methods typically focus either\non textual analysis or time-series modeling, but fail to capture the\nmulti-modal nature of financial markets and the causal relationship between\nevents and price movements. To address these gaps, we propose CAMEF\n(Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a\nmulti-modality framework that effectively integrates textual and time-series\ndata with a causal learning mechanism and an LLM-based counterfactual event\naugmentation technique for causal-enhanced financial forecasting. Our\ncontributions include: (1) a multi-modal framework that captures causal\nrelationships between policy texts and historical price data; (2) a new\nfinancial dataset with six types of macroeconomic releases from 2008 to April\n2024, and high-frequency real trading data for five key U.S. financial assets;\nand (3) an LLM-based counterfactual event augmentation strategy. We compare\nCAMEF to state-of-the-art transformer-based time-series and multi-modal\nbaselines, and perform ablation studies to validate the effectiveness of the\ncausal learning mechanism and event types.",
        "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available.",
        "Considerable effort has been devoted to the development of superconducting\nundulators (SCUs) intended for particle accelerator-based light sources,\nincluding synchrotrons and free electron laser (FEL) facilities. Recently, a\nhigh-temperature superconducting (HTS) undulator prototype, consisting of\nstaggered-array Re-Ba-Cu-O bulks, achieved an on-axis sinusoidal magnetic field\nprofile with a peak amplitude B$_0$ of 2.1 T and a period length of 10 mm,\nresulting in a deflection parameter K = 1.96. Such a short period HTS undulator\nnot only enables the generation of higher-energy photons, but also supports the\nconstruction of economically feasible and compact FELs with shorter linear\naccelerators (LINACs). This article provides a comprehensive review of recent\nadvances in the staggered-array bulk HTS undulator as well as other types of\nHTS undulators. Furthermore, it offers insights into the development of\nengineering HTS undulator prototypes designed for deployment in synchrotron and\nfree electron laser (FEL) facilities. We conclude by discussing opportunities\nfor and the challenges facing the use of HTS undulators in practical\napplications.",
        "In this paper, we consider the steady state classification problem of the\nAllee effect system for multiple tribes. First, we reduce the high-dimensional\nmodel into several two-dimensional and three-dimensional algebraic systems such\nthat we can prove a comprehensive formula of the border polynomial for\narbitrary dimension. Then, we propose an efficient algorithm for classifying\nthe generic parameters according to the number of steady states, and we\nsuccessfully complete the computation for up to the seven-dimensional Allee\neffect system.",
        "We evaluate the Hall coefficient characterising magnetotransport in an\nintercalated graphite CaC$_6$ with the Fermi surface reconstructed by an\nuniaxial charge density wave from closed pockets to open sheets. As the typical\norder parameter, corresponding to the pseudo-gap in electronic spectrum and\nconsequently to spacing between electron trajectories in reciprocal space, is\nof the order of $10^2$K, magnetic breakdown in strong experimentally achievable\nfields of the order of 10T is inevitable. The classical expressions for the\ncomponents of the magnetoconductivity tensor are strongly modified by magnetic\nfield-assisted over-gap tunneling causing quantum interference. Due to magnetic\nbreakdown, all magnetoconductivity components undergo strong quantum\noscillations reflected in the Hall coefficient. In their nature, these are\ndifferent than standard Shubnikov de Haas oscillations which would not appear\nin a system with an open Fermi surface.",
        "Flow models are effective at progressively generating realistic images, but\nthey generally struggle to capture long-range dependencies during the\ngeneration process as they compress all the information from previous time\nsteps into a single corrupted image. To address this limitation, we propose\nintegrating autoregressive modeling -- known for its excellence in modeling\ncomplex, high-dimensional joint probability distributions -- into flow models.\nDuring training, at each step, we construct causally-ordered sequences by\nsampling multiple images from the same semantic category and applying different\nlevels of noise, where images with higher noise levels serve as causal\npredecessors to those with lower noise levels. This design enables the model to\nlearn broader category-level variations while maintaining proper causal\nrelationships in the flow process. During generation, the model\nautoregressively conditions the previously generated images from earlier\ndenoising steps, forming a contextual and coherent generation trajectory.\nAdditionally, we design a customized hybrid linear attention mechanism tailored\nto our modeling approach to enhance computational efficiency. Our approach,\ntermed ARFlow, under 400k training steps, achieves 14.08 FID scores on ImageNet\nat 128 * 128 without classifier-free guidance, reaching 4.34 FID with\nclassifier-free guidance 1.5, significantly outperforming the previous\nflow-based model SiT's 9.17 FID. Extensive ablation studies demonstrate the\neffectiveness of our modeling strategy and chunk-wise attention design.",
        "The nilpotent graph of a group $G$ is the simple and undirected graph whose\nvertices are the elements of $G$ and two distinct vertices are adjacent if they\ngenerate a nilpotent subgroup of $G$. Here we discuss some topological\nproperties of the nilpotent graph of a finite group $G$. Indeed, we\ncharacterize finite solvable groups whose closed neighborhoods are nilpotent\nsubgroups. Moreover, we study the connectivity of the graph $\\Gamma(G)$\nobtained removing all universal vertices from the nilpotent graph of $G$. Some\nupper bounds to the diameter of $\\Gamma(G)$ are provided when $G$ belongs to\nsome classes of groups.",
        "In the context of the rising share of new energy generation, accurately\ngenerating new energy output scenarios is crucial for day-ahead power system\nscheduling. Deep learning-based scenario generation methods can address this\nneed, but their black-box nature raises concerns about interpretability. To\ntackle this issue, this paper introduces a method for day-ahead new energy\nscenario generation based on an improved conditional generative diffusion\nmodel. This method is built on the theoretical framework of Markov chains and\nvariational inference. It first transforms historical data into pure noise\nthrough a diffusion process, then uses conditional information to guide the\ndenoising process, ultimately generating scenarios that satisfy the conditional\ndistribution. Additionally, the noise table is improved to a cosine form,\nenhancing the quality of the generated scenarios. When applied to actual wind\nand solar output data, the results demonstrate that this method effectively\ngenerates new energy output scenarios with good adaptability.",
        "This study investigates the efficacy of Jacobian-free Newton-Krylov methods\nin finite-volume solid mechanics. Traditional Newton-based approaches require\nexplicit Jacobian matrix formation and storage, which can be computationally\nexpensive and memory-intensive. In contrast, Jacobian-free Newton-Krylov\nmethods approximate the Jacobian's action using finite differences, combined\nwith Krylov subspace solvers such as the generalised minimal residual method\n(GMRES), enabling seamless integration into existing segregated finite-volume\nframeworks without major code refactoring. This work proposes and benchmarks\nthe performance of a compact-stencil Jacobian-free Newton-Krylov method against\na conventional segregated approach on a suite of test cases, encompassing\nvarying geometric dimensions, nonlinearities, dynamic responses, and material\nbehaviours. Key metrics, including computational cost, memory efficiency, and\nrobustness, are evaluated, along with the influence of preconditioning\nstrategies and stabilisation scaling. Results show that the proposed\nJacobian-free Newton-Krylov method outperforms the segregated approach in all\nlinear and nonlinear elastic cases, achieving order-of-magnitude speedups in\nmany instances; however, divergence is observed in elastoplastic cases,\nhighlighting areas for further development. It is found that preconditioning\nchoice impacts performance: a LU direct solver is fastest in small to\nmoderately-sized cases, while a multigrid method is more effective for larger\nproblems. The findings demonstrate that Jacobian-free Newton-Krylov methods are\npromising for advancing finite-volume solid mechanics simulations, particularly\nfor existing segregated frameworks where minimal modifications enable their\nadoption. The described implementations are available in the solids4foam\ntoolbox for OpenFOAM, inviting the community to explore, extend, and compare\nthese procedures.",
        "We consider the defocusing Calogero--Moser derivative nonlinear\nSchr{\\\"o}dinger equation\\begin{align*}i \\partial_{t} u+\\partial_{x}^2 u-2\\Pi\nD\\left(|u|^{2}\\right)u=0, \\quad (t,x ) \\in \\mathbb{R} \\times\n\\mathbb{R}\\end{align*}posed on $E := \\left\\{u \\in L^{\\infty}(\\mathbb{R}): u'\n\\in L^{2}(\\mathbb{R}), u'' \\in L^{2}(\\mathbb{R}), |u|^{2}-1 \\in\nL^{2}(\\mathbb{R})\\right\\}$. We prove the global well-posedness of this equation\nin $E$. Moreover, we give an explicit formula for the chiral solution to this\nequation.",
        "Research has focused on Multi-Modal Semantic Segmentation (MMSS), where\npixel-wise predictions are derived from multiple visual modalities captured by\ndiverse sensors. Recently, the large vision model, Segment Anything Model 2\n(SAM2), has shown strong zero-shot segmentation performance on both images and\nvideos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be\nadapted to multi-modal data? 2. How can SAM2 better understand semantics?\nInspired by cross-frame correlation in videos, we propose to treat multi-modal\ndata as a sequence of frames representing the same scene. Our key idea is to\n''memorize'' the modality-agnostic information and 'memorize' the semantics\nrelated to the targeted scene. To achieve this, we apply SAM2's memory\nmechanisms across multi-modal data to capture modality-agnostic features.\nMeanwhile, to memorize the semantic knowledge, we propose a training-only\nSemantic Prototype Memory Module (SPMM) to store category-level prototypes\nacross training for facilitating SAM2's transition from instance to semantic\nsegmentation. A prototypical adaptation loss is imposed between global and\nlocal prototypes iteratively to align and refine SAM2's semantic understanding.\nExtensive experimental results demonstrate that our proposed MemorySAM\noutperforms SoTA methods by large margins on both synthetic and real-world\nbenchmarks (65.38% on DELIVER, 52.88% on MCubeS). Source code will be made\npublicly available.",
        "We present a direct lifetime measurement of the $5s5p~^1P_1$ state of\nstrontium using time-correlated single-photon counting of laser induced\nfluorescence in a hot atomic beam. To achieve fast switch-off times and a high\nsignal-to-noise ratio, we excite the strontium atoms with a femtosecond pulsed\nlaser at $\\approx$461 nm and collect the fluorescence onto a hybrid\nsingle-photon detector. Analysis of the measured exponential decay gives a\nlifetime of the $^1P_1$ state of $\\tau = (5.216 \\pm 0.006_{stat} \\pm\n0.013_{sys})$ ns, where all the systematic effects have been thoroughly\nconsidered.",
        "Over the last years, a number of broadband reverberation mapping campaigns\nhave been conducted to explore the short-term UV and optical variability of\nnearby AGN. Despite the extensive data collected, the origin of the observed\nvariability is still debated in the literature. Frequency-resolved time lags\noffer a promising approach to distinguish between different scenarios, as they\nprobe variability on different time scales. In this study, we present the\nexpected frequency-resolved lags resulting from X-ray reprocessing in the\naccretion disk. The predicted lags are found to feature a general shape that\nresembles that of observational measurements, while exhibiting strong\ndependence on various physical parameters. Additionally, we compare our model\npredictions to observational data for the case of NGC 5548, concluding that the\nX-ray illumination of the disk can effectively account for the observed\nfrequency-resolved lags and power spectra in a self-consistent way. To date,\nX-ray disk reprocessing is the only physical model that has successfully\nreproduced the observed multi-wavelength variability, in both amplitude and\ntime delays, across a range of temporal frequencies.",
        "We prove that the regularity of edge ideals of powers of forests is weakly\ndecreasing. We then compute the regularity of edge ideals of powers of cycles.",
        "We generalize in two steps the quantized action of the modular group on\n$q$-deformed real numbers introduced by Morier-Genoud and Ovsienko. First, we\nlet the projective general linear group $PGL_2(\\mathbb{Z})$ act on $q$-real\nnumbers via a $q$-deformed action. The quantized matrices we get have\ncombinatorial interpretations. Then we consider an extension of the group\n$PGL_2(\\mathbb{Z})$ by the $2$-elements cyclic group, and define a quantized\naction of this extension on $q$-real numbers. We deduce from these actions some\nunderlying relations between $q$-real numbers, and between left and right\nversions of $q$-deformed rational numbers. In particular we investigate the\ncase of some algebraic numbers of degree $4$ and $6$. We also prove that the\nway of quantizing real numbers defined by Morier-Genoud and Ovsienko is an\ninjective process.",
        "We prove that the affine closure of the cotangent bundle of the parabolic\nbase affine space for GL_n or SL_n is a Coulomb branch, which confirms a\nconjecture of Bourget-Dancer-Grimminger-Hanany-Zhong. In particular, we show\nthat the ring of functions on the cotangent bundle of the parabolic base affine\nspace of GL_n or SL_n is finitely generated.\n  We prove this by showing that, if we are given a map $H \\to G$ of complex\nreductive groups and a representation of $G$ satisfying an assumption we call\ngluable, then the Coulomb branch for the induced representation of $H$ is\nobtained from the corresponding Coulomb branch for $G$ by a certain Hamiltonian\nreduction procedure. In particular, we show that the Coulomb branch associated\nto any quiver with no loops can be obtained from Coulomb branches associated to\nquivers with exactly two vertices using this procedure.",
        "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios.",
        "In this paper, the conditional phase distribution of the two-wave with\ndiffuse power (TWDP) process is derived as a closed-form and as an\ninfinite-series expression. For the obtained infinite series expression, a\ntruncation analysis is performed and the truncated expression is used to\nexamine the influence of different channel conditions on the behavior of the\nTWDP phase. All the results are verified through Monte Carlo simulations.",
        "For a self-symmetric tracial von Neumann algebra $A$, we study rescalings of\n$A^{*n} * L\\mathbb{F}_r$ for $n \\in \\mathbb{N}$ and $r \\in (1, \\infty]$ and use\nthem to obtain an interpolation $\\mathcal{F}_{s,r}(A)$ for all real numbers\n$s>0$ and $1-s < r \\leq \\infty$. We get formulas for their free products, and\nfree products with finite-dimensional or hyperfinite von Neumann algebras. In\nparticular, for any such $A$, we can compute compressions $(A^{*n})^t$ for\n$0<t<1$, and the Murray-von Neumann fundamental group of $A^{*\\infty}$. When\n$A$ is also non-separable and abelian, this answers two questions in Section\n4.3 of recent work of Boutonnet-Drimbe-Ioana-Popa.",
        "The radiative open circuit voltage loss in a solar cell occurs because the\nabsorptance spectrum near the band gap shows gradual increase rather than sharp\nstep function like transition. This broadening effect has been attributed to\nband gap fluctuations and or to Urbach tails. In this report, we use modelling\nbased on Planck s generalized law to distinguish between these two effects. Our\nresults demonstrate that Urbach tails have only a minimal effect on the\nabsorptance edge broadening and clarify that even an ideal direct semiconductor\nwith no band gap fluctuations shows broadening at the absorptance onset.\nFurthermore, state of the art inorganic thin film solar cells often incorporate\na band gap gradient across their thickness, which can further contribute to\nabsorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,\nwe perform a comprehensive analysis of voltage losses through absolute\nphotoluminescence and electroluminescence spectroscopy, combined with\nphotospectrometry and high-spatial-resolution cathodoluminescence measurements.\nWe find that the loss analysis based on the combination of radiative,\ngeneration and non-radiative losses is complete. Samples with a graded band gap\nprofile show more pronounced broadening of the absorptance onset and up to 16\nmV higher radiative losses compared to the samples with uniform band gap. There\nis indication, that band gap-graded samples also have larger lateral band gap\ninhomogeneity.",
        "We present the discovery of Andromeda XXXV, the faintest Andromeda satellite\ngalaxy discovered to date, identified as an overdensity of stars in the\nPan-Andromeda Archaeological Survey and confirmed via Hubble Space Telescope\nimaging. Located at a heliocentric distance of $927^{+76}_{-63}$ kpc and\n$158^{+57}_{-45}$ kpc from Andromeda, Andromeda XXXV is an extended ($r_h =\n53\\,^{+13}_{-11}$ pc), elliptical ($\\epsilon = 0.4\\, \\pm 0.2$), metal-poor\n($[\\text{Fe}\/\\text{H}]\\sim-1.9$) system, and the least luminous ($M_V=-5.2 \\pm\n0.3$) of Andromeda's dwarf satellites discovered so far. Andromeda XXXV's\nproperties are consistent with the known population of dwarf galaxies around\nthe Local Group, bearing close structural resemblance to the Canes Venatici II\nand Hydra II Milky Way (MW) dwarf satellite galaxies. Its stellar population,\ncharacterized by a red horizontal branch or a red clump feature, mirrors that\nof other Andromeda satellite galaxies in showing evidence for a spread in age\nand metallicity, with no signs of younger stellar generations. This\nage-metallicity spread is not observed in MW satellites of comparable stellar\nmass, and highlights the persistent differences between the satellite systems\nof Andromeda and the MW, extending even into the ultrafaint regime.",
        "Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets.",
        "We consider unsteady ballistic heat transport in a semi-infinite Hooke chain\nwith a free end and an arbitrary heat source. An analytical description of the\nevolution of the kinetic temperature is proposed in both discrete (exact) and\ncontinuum (approximate) formulations. The continualization of the discrete\nsolution for kinetic temperature is performed through a large-time asymptotic\nestimate of the fundamental solution of the dynamical problem for the instantly\nperturbed conservative semi-infinite chain at the fronts of the incident and\nreflected thermal waves. By analyzing the continuum solution, we observe that\nany instantaneous heat supply (i.e., a heat pulse) results in the\nanti-localization of the reflected thermal wave. We demonstrate that sudden\npoint heat supply leads to a transition to a non-equilibrium steady state,\nwhich, unexpectedly, may exist even in the non-dissipative case. The results of\nthis paper are expected to provide insight into the continuum description of\nnanoscale heat transport.",
        "This paper is primarily focused on the asymptotic dynamics of a\nnon-autonomous stochastic FitzHugh-Nagumo system with distribution dependence,\nspecifically on unbounded domains $\\mathbb{R}^{n}$. Initially, we establish the\nwell-posedness of solutions for the FitzHugh-Nagumo system with distribution\ndependence by utilizing the Banach fixed-point theorem. Subsequently, we\ndemonstrate the existence and uniqueness of pullback measure attractors for\nthis system through the application of splitting techniques, tail-end estimates\nand Vitali's theorem."
      ]
    }
  },
  {
    "id":2411.04775,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Towards Scalable Koopman Operator Learning: Convergence Rates and A Distributed Learning Algorithm",
    "start_abstract":"We propose an alternating optimization algorithm to the nonconvex Koopman operator learning problem for nonlinear dynamic systems. show that proposed will converge a critical point with rate O(1\/T) and $O\\left( {\\frac{1}{{\\log T}}} \\right)$ constant diminishing rates, respectively, under some mild conditions. To cope high dimensional dynamical systems, we present first-ever distributed algorithm. has same convergence properties as centralized learning, in absence of optimal tracker, so long basis functions satisfy set state-based decomposition Numerical experiments are provided complement our theoretical results.",
    "start_categories":[
      "eess.SP",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "A Data\u2013Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition"
      ],
      "abstract":[
        "The Koopman operator is a linear but infinite dimensional operator that\ngoverns the evolution of scalar observables defined on the state space of an\nautonomous dynamical system, and is a powerful tool for the analysis and\ndecomposition of nonlinear dynamical systems. In this manuscript, we present a\ndata driven method for approximating the leading eigenvalues, eigenfunctions,\nand modes of the Koopman operator. The method requires a data set of snapshot\npairs and a dictionary of scalar observables, but does not require explicit\ngoverning equations or interaction with a \"black box\" integrator. We will show\nthat this approach is, in effect, an extension of Dynamic Mode Decomposition\n(DMD), which has been used to approximate the Koopman eigenvalues and modes.\nFurthermore, if the data provided to the method are generated by a Markov\nprocess instead of a deterministic dynamical system, the algorithm approximates\nthe eigenfunctions of the Kolmogorov backward equation, which could be\nconsidered as the \"stochastic Koopman operator\" [1]. Finally, four illustrative\nexamples are presented: two that highlight the quantitative performance of the\nmethod when presented with either deterministic or stochastic data, and two\nthat show potential applications of the Koopman eigenfunctions."
      ],
      "categories":[
        "Mathematical Analysis"
      ]
    },
    "list":{
      "title":[
        "Decay of resolvent kernels and Schr\\\"odinger eigenstates for L\\'evy\n  operators",
        "Maximizing nanoparticle light absorption: size, geometry, and a prospect\n  for metal alloys",
        "On reflected isotropic stable processes",
        "Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations",
        "Geometrically simple counterexamples to a local-global principle for\n  quadratic twists",
        "Physics-Informed Super-Resolution Diffusion for 6D Phase Space\n  Diagnostics",
        "A Catalog of Stellar and Dust Properties for 500,000 Stars in the\n  Southwest Bar of the Small Magellanic Cloud",
        "Invariance of three-dimensional Bessel bridges in terms of time reversal",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Modeling Cost-Associated Cooperation: A Dilemma of Species Interaction\n  Unveiling New Aspects of Fear Effect",
        "Journey from the Wilson exact RG towards the Wegner-Morris Fokker-Planck\n  RG and the Carosso field-coarsening via Langevin stochastic processes",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Continuous Selection of Unitaries in II$_1$ Factors",
        "Electric-field sensing with driven-dissipative time crystals in\n  room-temperature Rydberg vapor",
        "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem",
        "Classification of simple quandles of small order",
        "Calibration and Option Pricing with Stochastic Volatility and Double\n  Exponential Jumps",
        "Three-Dimensional to Layered Halide Perovskites: A Parameter-Free Hybrid\n  Functional Method for Predicting Electronic Band Gaps",
        "Twisted torus knots with Horadam parameters",
        "Baseline filtering and peak reconstruction for haloscope-like axion\n  searches",
        "Chiral Symmetry in Dense Matter with Meson Condensation",
        "Higgs Thermal Nonequilibrium in Primordial QGP",
        "A Study of the Provincial Road Networks of Canada as Complex Networks",
        "Uncertainty quantification for stationary and time-dependent PDEs\n  subject to Gevrey regular random domain deformations",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Global Well-Posedness of a Nonlinear Fokker-Planck Type Model of Grain\n  Growth",
        "Structure formation in the local Universe and the cosmological constant",
        "PAC Learnability of Scenario Decision-Making Algorithms: Necessary and\n  Sufficient Conditions",
        "Super-Eddington Accretion in Quasars"
      ],
      "abstract":[
        "We study the spatial decay behaviour of resolvent kernels for a large class\nof non-local L\\'evy operators and bound states of the corresponding\nSchr\\\"odinger operators. Our findings naturally lead us to proving results for\nL\\'evy measures, which have subexponential or exponential decay, respectively.\nThis leads to sharp transitions in the the decay rates of the resolvent\nkernels. We obtain estimates that allow us to describe and understand the\nintricate decay behaviour of the resolvent kernels and the bound states in\neither regime, extending findings by Carmona, Masters and Simon for fractional\nLaplacians (the subexponential regime) and classical relativistic operators\n(the exponential regime). Our proofs are mainly based on methods from the\ntheory of operator semigroups.",
        "In this work we show how to maximize absorption of plasmonic nanoparticles in\nterms of size, geometry and material. For that reason the interaction of\nnanoparticles with light was decomposed into different effects. We determined\nthat the main effect dictating the optimal amount of optical losses is\nradiation damping, and how it depends on nanoparticle size and geometry. Based\non this, we find that for many combinations of sizes and geometries losses in\npure metals are far from optimal. To overcome the aforementioned issue,\nalloying is presented as straightforward and flexible way of modulating the\noptical losses. Furthermore, strategies for tuning the optical losses to values\nabove, between, and even below those in pure plasmonic metals are developed in\nterms of selecting the right alloy composition. In some cases, alloys showed a\nmultifold increase in absorption when compared to pure plasmonic metals. The\nphysical reasons governing such changes are elucidated based on the electronic\nstructure changes during alloying of different metals, which enables\ngeneralization of the results to other systems. Besides increasing absorption,\nelectronic structure changes can also be utilized for channeling the absorbed\nenergy to suit different purposes, such as hot carrier generation for\nphotocatalysis or solar energy harvesting. Overall, these results establish\nalloying as a powerful tool for designing nanostructures for applications that\nutilize light absorption.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "We show that contact-rich motion planning is also sparsity-rich when viewed\nas polynomial optimization (POP). We can exploit not only the correlative and\nterm sparsity patterns that are general to all POPs, but also specialized\nsparsity patterns from the robot kinematic structure and the separability of\ncontact modes. Such sparsity enables the design of high-order but sparse\nsemidefinite programming (SDPs) relaxations--building upon Lasserre's moment\nand sums of squares hierarchy--that (i) can be solved in seconds by\noff-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to\nthe nonconvex contact-rich planning problems with small certified\nsuboptimality. Through extensive experiments both in simulation (Push Bot, Push\nBox, Push Box with Obstacles, and Planar Hand) and real world (Push T), we\ndemonstrate the power of using convex SDP relaxations to generate global\ncontact-rich motion plans. As a contribution of independent interest, we\nrelease the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++\nwith interfaces to both Python and Matlab--that automates sparsity exploitation\nfor robotics and beyond.",
        "Two abelian varieties $A$ and $B$ over a number field $K$ are said to be\nstrongly locally quadratic twists if they are quadratic twists at every\ncompletion of $K$. While it was known that this does not imply that $A$ and $B$\nare quadratic twists over $K$, the only known counterexamples (necessarily of\ndimension $\\geq 4$) are not geometrically simple. We show that, for every prime\n$p\\equiv 13 \\pmod{24}$, there exists a pair of geometrically simple abelian\nvarieties of dimension $p-1$ over $\\mathbb{Q}$ that are strongly locally\nquadratic twists but not quadratic twists. The proof is based on Galois\ncohomology computations and class field theory.",
        "Adaptive physics-informed super-resolution diffusion is developed for\nnon-invasive virtual diagnostics of the 6D phase space density of charged\nparticle beams. An adaptive variational autoencoder (VAE) embeds initial beam\ncondition images and scalar measurements to a low-dimensional latent space from\nwhich a 326 pixel 6D tensor representation of the beam's 6D phase space density\nis generated. Projecting from a 6D tensor generates physically consistent 2D\nprojections. Physics-guided super-resolution diffusion transforms\nlow-resolution images of the 6D density to high resolution 256x256 pixel\nimages. Un-supervised adaptive latent space tuning enables tracking of\ntime-varying beams without knowledge of time-varying initial conditions. The\nmethod is demonstrated with experimental data and multi-particle simulations at\nthe HiRES UED. The general approach is applicable to a wide range of complex\ndynamic systems evolving in high-dimensional phase space. The method is shown\nto be robust to distribution shift without re-training.",
        "We present a catalog of individual stellar and dust extinction properties\nalong close to 500,000 sight lines in the southwest bar of the Small Magellanic\nCloud (SMC). The catalog is based on multiband Hubble Space Telescope\nphotometric data spanning near-ultraviolet to near-infrared wavelengths from\nthe Small Magellanic Cloud Investigation of Dust and Gas Evolution survey\n(SMIDGE) covering a 100 x 200 pc area. We use the probabilistic technique of\nthe Bayesian Extinction And Stellar Tool (BEAST) to model the spectral energy\ndistributions of individual stars in SMIDGE and include the effects of\nobservational uncertainties in the data. We compare BEAST-derived dust\nextinction properties with tracers of the interstellar medium, such as the\nemission from the 12CO (2-1) transition (I(CO)), the dust mass surface density\n({\\Sigma}dust) from far-IR emission, the H I column density (N(HI)) from the\n21cm transition, and the mass fraction of polycyclic aromatic hydrocarbons\n(PAHs; qPAH, derived from IR emission). We find that the dust extinction (A(V\n)) in the SMIDGE field is strongly correlated with {\\Sigma}dust and I(CO), and\nless so with N(HI) and qPAH, and suggest potential explanations. Our extinction\nmeasurements are also sensitive to the presence of the 2175 {\\AA} bump in the\nextinction curve toward UV bright stars. While most do not show evidence for\nthe bump, we identify ~200 lines of sight that are 2175 {\\AA} bump candidates.\nFurthermore, we find distinct structures in the dust extinction-distance\ndistributions that provide insights into the 3D geometry of the SMC.",
        "Given $a,b\\ge 0$ and $t>0$, let $\\rho =\\{ \\rho _{s}\\} _{0\\le s\\le t}$ be a\nthree-dimensional Bessel bridge from $a$ to $b$ over $[0,t]$. In this paper,\nbased on a conditional identity in law between Brownian bridges stemming from\nPitman's theorem, we show in particular that the process given by\n\\begin{align*}\n  \\rho _{s}+\\Bigl| b-a+\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr|\n  -\\Bigl|\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr| ,\\quad 0\\le s\\le t, \\end{align*} has the same law as the time reversal\n$\\{ \\rho _{t-s}\\} _{0\\le s\\le t}$ of $\\rho $. As an immediate application,\nletting $R=\\{ R_{s}\\} _{s\\ge 0}$ be a three-dimensional Bessel process starting\nfrom $a$, we obtain the following time-reversal and time-inversion results on\n$R$: $\\{ R_{t-s}\\} _{0\\le s\\le t}$ is identical in law with the process given\nby \\begin{align*}\n  R_{s}+R_{t}-2\\min _{s\\le u\\le t}R_{u},\\quad 0\\le s\\le t, \\end{align*} when\n$a=0$, and $\\{ sR_{1\/s}\\} _{s>0}$ is identical in law with the process given by\n\\begin{align*}\n  R_{s}-2(1+s)\\min _{0\\le u\\le s}\\frac{R_{u}}{1+u}+a(1+s),\\quad s>0,\n\\end{align*} for every $a\\ge 0$.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
        "Within the Wilson RG of 'incomplete integration' as a function of the RG-time\n$t$, the non-linear differential RG flow for the energy $E_t[\\phi(.)]$\ntranslates for the probability distribution $P_t[\\phi(.)] \\sim e^{-\nE_t[\\phi(.)]} $ into the linear Fokker-Planck RG flow associated to independent\nnon-identical Ornstein-Uhlenbeck processes for the Fourier modes. The\ncorresponding Langevin stochastic differential equation for the real-space\nfield $\\phi_t(\\vec x)$ can be then interpreted within the Carosso perspective\nas genuine infinitesimal coarsening-transformations that are the analog of\nspin-blocking, and whose irreversible character is essential to overcome the\nparadox of the naive description of the Wegner-Morris RG flow as a mere\ninfinitesimal change of variables in the partition function integral. This\ninterpretation suggests to consider new RG-schemes, in particular the Carosso\nRG where the Langevin SDE corresponds to the well known stochastic heat\nequation or the Edwards-Wilkinson dynamics. We stress the advantages of this\nstochastic formulation of exact RG flows. While statistical field theory is\nusually written in infinite space, we focus here on the formulation on a large\nvolume $L^d$ with periodic boundary conditions, in order to distinguish between\nextensive and intensives observables while keeping the translation-invariance.\nSince the empirical magnetization $m_e \\equiv \\frac{1}{L^d} \\int_{L^d} d^d \\vec\nx \\ \\phi(\\vec x) $ is an intensive variable corresponding to the zero-momentum\nFourier coefficient of the field, its probability distribution $p_L(m_e)$ can\nbe obtained from the gradual integration over all the other Fourier\ncoefficients associated to non-vanishing-momenta via exact differential RG, in\norder to obtain the large deviation properties with respect to the volume\n$L^d$.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We prove continuous-valued analogues of the basic fact that Murray-von\nNeumann subequivalence of projections in II$_1$ factors is completely\ndetermined by tracial evaluations. We moreover use this result to solve the\nso-called trace problem in the case of factorial trivial $W^\\ast$-bundles whose\nbase space has covering dimension at most 1. Our arguments are based on\napplications of a continuous selection theorem due to Michael to von Neumann\nalgebras.",
        "Mode competition in nonequilibrium Rydberg gases enables the exploration of\nemergent many-body phases. This work leverages this emergent phase for electric\nfield detection at room temperature. Sensitive frequency-resolved electric\nfield measurements at very low-frequencies (VLF) are of central importance in a\nwide range of applications where deep-penetration is required in\ncommunications, navigation and imaging or surveying. The long wavelengths on\norder of 10-100 km (3-30 kHz) limit the efficiency, sensitivity, and bandwidth\nof compact classical detectors that are constrained by the Chu limit.\nRydberg-atom electrometers are an attractive approach for microwave\nelectric-field sensors but have reduced sensitivity at lower-frequencies. Very\nrecent efforts to advance the standard Rydberg-atoms approach is based on DC\nelectric-field (E-field) Stark shifting and have resulted in sensitivities\nbetween 67.9-2.2 uVcm-1Hz-1\/2 (0.1-10 kHz) by fine optimization of the DC\nE-field. A major challenge in these approaches is the need for embedded\nelectrodes or plates due to DC E-field Stark screening effect, which can\nperturb coupling of VLF signals when injected from external sources. In this\narticle, it is demonstrated that state-of-art sensitivity (~1.6-2.3\nuVcm-1Hz-1\/2) can instead be achieved using limit-cycle oscillations in\ndriven-dissipative Rydberg atoms by using a magnetic field (B-field) to develop\nmode-competition between nearby Rydberg states. The mode-competition between\nnearby Rydberg-states develop an effective transition centered at the\noscillation frequency capable of supporting external VLF E-field coupling in\nthe ~10-15kHz regime without the requirement for fine optimization of the\nB-field magnitude.",
        "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches.",
        "In this article, we define quasiprimitive quandles and describe them with the\nhelp of quasiprimitive permutation groups. As a consequence, we enumerate\nfinite non-affine simple quandles up to order $4096$.",
        "This work examines a stochastic volatility model with double-exponential\njumps in the context of option pricing. The model has been considered in\nprevious research articles, but no thorough analysis has been conducted to\nstudy its quality of calibration and pricing capabilities thus far. We provide\nevidence that this model outperforms challenger models possessing similar\nfeatures (stochastic volatility and jumps), especially in the fit of the short\nterm implied volatility smile, and that it is particularly tractable for the\npricing of exotic options from different generations. The article utilizes\nFourier pricing techniques (the PROJ method and its refinements) for different\ntypes of claims and several generations of exotics (Asian options, cliquets,\nbarrier options, and options on realized variance), and all source codes are\nmade publicly available to facilitate adoption and future research. The results\nindicate that this model is highly promising, thanks to the asymmetry of the\njumps distribution allowing it to capture richer dynamics than a normal jump\nsize distribution. The parameters all have meaningful econometrics\ninterpretations that are important for adoption by risk-managers.",
        "Accurately predicting electronic band gaps in halide perovskites using ab\ninitio density functional theory (DFT) is essential for their application in\noptoelectronic devices. Standard hybrid functionals such as HSE and PBE0 can\novercome the limitations of DFT with reasonable computational cost but are\nknown to underestimate the measured band gaps for layered halide perovskites.\nIn this study, we assess the performance of the doubly screened\ndielectric-dependent hybrid (DSH) functional for predicting band gaps in\nthree-dimensional (3D) and layered hybrid perovskites. We show that the DSH\nfunctional, which employs material-dependent mixing parameters derived from\nmacroscopic dielectric constants, provides accurate band gap predictions for 3D\nhalide perovskites when structural local disorder is considered. For layered\nhybrid perovskites, DSH functional based on average dielectric constants\noverestimates the band gaps. To improve the predictions and stay in a\nparameter-free ab initio workflow, we propose to use the calculated dielectric\nconstant of the respective 3D perovskites. We find that the DSH functionals\nusing the dielectric constants of the 3D perovskite accurately predict\nexperimental gaps, with the lowest mean absolute errors compared to HSE and\nPBE0 for layered perovskites with various organic spacers, as well as for\nmultilayered $BA_2MA_{n-1}Pb_{n}I_{3n-1}$ with n = 2, 3. Notably, the HSE\nfunctional systematically underestimates the band gaps in layered perovskites.\nWe attribute the root of this failure to the absence of non-local long-range\ndielectric screening, a critical factor for halide perovskites. The\ncomputational framework introduced here provides an efficient parameter-free ab\ninitio methodology for predicting the electronic properties of 3D and layered\nhalide perovskites and their heterostructures, aiding in developing advanced\noptoelectronic devices.",
        "Sangyop Lee has done much work to determine the knot types of twisted torus\nknots, including classifying the twisted torus knots which are the unknot.\nAmong the unknotted twisted torus knots are those of the form $(F_{n+2}, F_n,\nF_{n+1}, -1)$, where $F_i$ is the $i$th Fibonacci number. Here, we consider\ntwisted torus knots with parameters that are defined recursively, similarly to\nthe Fibonacci sequence. We call these \\textit{Horadam parameters}, after the\ngeneralization of the Fibonacci sequence introduced by A.F. Horadam. Here, we\nprovide families of twisted torus knots that generalize Lee's work with Horadam\nparameters. Additionally, we provide lists of primitive\/primitive and\nprimitive\/Seifert twisted torus knots and connect these lists to the Horadam\ntwisted torus knots.",
        "Axions are well-motivated dark matter particles. Many experiments are looking\nfor their experimental evidence. For haloscopes, the problem reduces to the\nidentification of a peak above a noisy baseline. Its modeling, however, may\nproblematic. State-of-the-art analysis rely on the Savitzky-Golay (SG)\nfiltering, which is intrinsically affected by any possible over fluctuation,\nleading to biased results. In this paper we study the efficiency that different\nextensions of SG can provide in the peak reconstruction in a standard\nhaloscope-like experiment. We show that, once the correlations among bins are\ntaken into account, there is no appreciable difference. The standard SG remains\nthe advisable choice because of its numerical efficiency.",
        "Kaon condensation in hyperon-mixed matter [($Y$+$K$) phase], which may be\nrealized in neutron stars, is discussed on the basis of chiral symmetry. With\nthe use of the effective chiral Lagrangian for kaon--baryon and kaon--kaon\ninteractions; coupled with the relativistic mean field theory and universal\nthree-baryon repulsive interaction, we clarify the effects of the $s$-wave\nkaon--baryon scalar interaction simulated by the kaon--baryon sigma terms and\nvector interaction (Tomozawa--Weinberg term) on kaon properties in\nhyperon-mixed matter, the onset density of kaon condensation, and the equation\nof state with the ($Y$+$K$) phase. In particular, the quark condensates in the\n($Y$+$K$) phase are obtained, and their relevance to chiral symmetry\nrestoration is discussed.",
        "In this work we investigate the chemical and kinetic nonequilibrium dynamics\nof the Higgs boson during the primordial Universe QGP (quark-gluon plasma)\nepoch $130\\mathrm{\\,GeV}>T>10\\mathrm{\\,GeV}$. We show that the Higgs bosons is\nalways out of chemical abundance equilibrium with a fugacity $\\Upsilon_h =\n0.69$ due to virtual decay channels. Additionally, Higgs momentum distribution\nis found to be ``cold'' for $T<40$\\,GeV, since the scattering rate drops below\nthe production rate.",
        "In this paper, we report on a study of the road networks of the provinces of\nCanada as complex networks. A number of statistical features have been analyzed\nand compared with two random models. In addition, we have also studied the\nresilience of these road networks under random failures and targeted attacks.",
        "We study uncertainty quantification for partial differential equations\nsubject to domain uncertainty. We parameterize the random domain using the\nmodel recently considered by Chernov and Le (2024) as well as Harbrecht,\nSchmidlin, and Schwab (2024) in which the input random field is assumed to\nbelong to a Gevrey smoothness class. This approach has the advantage of being\nsubstantially more general than models which assume a particular parametric\nrepresentation of the input random field such as a Karhunen-Loeve series\nexpansion. We consider both the Poisson equation as well as the heat equation\nand design randomly shifted lattice quasi-Monte Carlo (QMC) cubature rules for\nthe computation of the expected solution under domain uncertainty. We show that\nthese QMC rules exhibit dimension-independent, essentially linear cubature\nconvergence rates in this framework. In addition, we complete the error\nanalysis by taking into account the approximation errors incurred by dimension\ntruncation of the random input field and finite element discretization.\nNumerical experiments are presented to confirm the theoretical rates.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "Most technologically useful materials spanning multiple length scales are\npolycrystalline. Polycrystalline microstructures are composed of a myriad of\nsmall crystals or grains with different lattice orientations which are\nseparated by interfaces or grain boundaries. The changes in the grain and grain\nboundary structure of polycrystals highly influence the materials properties\nincluding, but not limited to, electrical, mechanical, and thermal. Thus, an\nunderstanding of how microstructures evolve is essential for the engineering of\nnew materials. In this paper, we consider a recently introduced nonlinear\nFokker-Planck-type system and establish a global well-posedness result for it.\nSuch systems under specific energy laws emerge in the modeling of the grain\nboundary dynamics in polycrystals.",
        "The structure formation in the local Universe is considered within the\nweak-field modification of General Relativity involving the cosmological\nconstant. This approach enables to describe the dynamics of groups and clusters\nof galaxies, to explain the discrepancy in the observational properties of the\nlocal (late) and the global (early) Universe, i.e. the Hubble tension as a\nresult of two flows, local and global ones, with non-equal Hubble parameters.\nThe kinetic analysis with the modified gravitational potential involving the\ncosmological constant is shown to predict semi-periodical structure of\nfilaments in the local universe. In the local scale this complements the\nZeldovich pancake theory of evolution of the primordial density perturbations\nand of structure formation in the cosmological scale. The role of the\ncosmological constant is outlined in rescaling of the physical constants from\none aeon to another within the Conformal Cyclic Cosmology.",
        "We study the PAC property of scenario decision-making algorithms, that is,\nthe ability to make a decision that has an arbitrarily low risk of violating an\nunknown safety constraint, provided sufficiently many realizations (called\nscenarios) of the safety constraint are sampled. Sufficient conditions for\nscenario decision-making algorithms to be PAC are available in the literature,\nsuch as finiteness of the VC dimension of its associated classifier and\nexistence of a compression scheme. We study the question of whether these\nsufficient conditions are also necessary. We show with counterexamples that\nthis is not the case in general. This contrasts with binary classification\nlearning, for which the analogous conditions are sufficient and necessary.\nPopular scenario decision-making algorithms, such as scenario optimization,\nenjoy additional properties, such as stability and consistency. We show that\neven under these additional assumptions the above conclusions hold. Finally, we\nderive a necessary condition for scenario decision-making algorithms to be PAC,\ninspired by the VC dimension and the so-called no-free-lunch theorem.",
        "This review provides an observational perspective on the fundamental\nproperties of super-Eddington accretion onto supermassive black holes in\nquasars. It begins by outlining the selection criteria, particularly focusing\non optical and UV broad-line intensity ratios, used to identify a population of\nunobscured super-Eddington candidates. Several defining features place these\ncandidates at the extreme end of the Population A in main sequence of quasars:\namong them are the highest observed singly-ionized iron emission, extreme\noutflow velocities in UV resonance lines, and unusually high metal abundances.\nThese key properties reflect the coexistence of a virialized sub-system within\nthe broad-line region alongside powerful outflows, with the observed gas\nenrichment likely driven by nuclear or circumnuclear star formation. The most\ncompelling evidence for the occurrence of super-Eddington accretion onto\nsupermassive black holes comes from recent observations of massive black holes\nat early cosmic epochs. These black holes require rapid growth rates that are\nonly achievable through radiatively inefficient super-Eddington accretion.\nFurthermore, extreme Eddington ratios, close to or slightly exceeding unity,\nare consistent with the saturation of radiative output per unit mass predicted\nby accretion disk theory for super-Eddington accretion rates. The extreme\nproperties of super-Eddington candidates suggest that these quasars could make\nthem stable and well-defined cosmological distance indicators, leveraging the\ncorrelation between broad-line width and luminosity expected in virialized\nsystems. Finally, several analogies with accretion processes around\nstellar-mass black holes, particularly in the high\/soft state, are explored to\nprovide additional insight into the mechanisms driving super-Eddington\naccretion."
      ]
    }
  },
  {
    "id":2411.04775,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"VAMPnets for deep learning of molecular kinetics",
    "start_abstract":"Abstract There is an increasing demand for computing the relevant structures, equilibria, and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation simulated coordinates into structural features, dimension reduction, clustering dimension-reduced data, estimation a Markov state model or related interconversion rates between structures. This handcrafted approach demands substantial amount modeling expertise, poor decisions at any step will lead to large errors. Here we variational processes (VAMP) develop deep learning framework using neural networks, dubbed VAMPnets. A VAMPnet encodes entire mapping states, thus combining whole data processing pipeline in single end-to-end framework. Our method performs equally better than state-of-the-art provides easily interpretable few-state kinetic models.",
    "start_categories":[
      "eess.SP",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "A Data\u2013Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition"
      ],
      "abstract":[
        "The Koopman operator is a linear but infinite dimensional operator that\ngoverns the evolution of scalar observables defined on the state space of an\nautonomous dynamical system, and is a powerful tool for the analysis and\ndecomposition of nonlinear dynamical systems. In this manuscript, we present a\ndata driven method for approximating the leading eigenvalues, eigenfunctions,\nand modes of the Koopman operator. The method requires a data set of snapshot\npairs and a dictionary of scalar observables, but does not require explicit\ngoverning equations or interaction with a \"black box\" integrator. We will show\nthat this approach is, in effect, an extension of Dynamic Mode Decomposition\n(DMD), which has been used to approximate the Koopman eigenvalues and modes.\nFurthermore, if the data provided to the method are generated by a Markov\nprocess instead of a deterministic dynamical system, the algorithm approximates\nthe eigenfunctions of the Kolmogorov backward equation, which could be\nconsidered as the \"stochastic Koopman operator\" [1]. Finally, four illustrative\nexamples are presented: two that highlight the quantitative performance of the\nmethod when presented with either deterministic or stochastic data, and two\nthat show potential applications of the Koopman eigenfunctions."
      ],
      "categories":[
        "Mathematical Analysis"
      ]
    },
    "list":{
      "title":[
        "Decay of resolvent kernels and Schr\\\"odinger eigenstates for L\\'evy\n  operators",
        "Maximizing nanoparticle light absorption: size, geometry, and a prospect\n  for metal alloys",
        "On reflected isotropic stable processes",
        "Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations",
        "Geometrically simple counterexamples to a local-global principle for\n  quadratic twists",
        "Physics-Informed Super-Resolution Diffusion for 6D Phase Space\n  Diagnostics",
        "A Catalog of Stellar and Dust Properties for 500,000 Stars in the\n  Southwest Bar of the Small Magellanic Cloud",
        "Invariance of three-dimensional Bessel bridges in terms of time reversal",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Modeling Cost-Associated Cooperation: A Dilemma of Species Interaction\n  Unveiling New Aspects of Fear Effect",
        "Journey from the Wilson exact RG towards the Wegner-Morris Fokker-Planck\n  RG and the Carosso field-coarsening via Langevin stochastic processes",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Continuous Selection of Unitaries in II$_1$ Factors",
        "Electric-field sensing with driven-dissipative time crystals in\n  room-temperature Rydberg vapor",
        "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem",
        "Classification of simple quandles of small order",
        "Calibration and Option Pricing with Stochastic Volatility and Double\n  Exponential Jumps",
        "Three-Dimensional to Layered Halide Perovskites: A Parameter-Free Hybrid\n  Functional Method for Predicting Electronic Band Gaps",
        "Twisted torus knots with Horadam parameters",
        "Baseline filtering and peak reconstruction for haloscope-like axion\n  searches",
        "Chiral Symmetry in Dense Matter with Meson Condensation",
        "Higgs Thermal Nonequilibrium in Primordial QGP",
        "A Study of the Provincial Road Networks of Canada as Complex Networks",
        "Uncertainty quantification for stationary and time-dependent PDEs\n  subject to Gevrey regular random domain deformations",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Global Well-Posedness of a Nonlinear Fokker-Planck Type Model of Grain\n  Growth",
        "Structure formation in the local Universe and the cosmological constant",
        "PAC Learnability of Scenario Decision-Making Algorithms: Necessary and\n  Sufficient Conditions",
        "Super-Eddington Accretion in Quasars"
      ],
      "abstract":[
        "We study the spatial decay behaviour of resolvent kernels for a large class\nof non-local L\\'evy operators and bound states of the corresponding\nSchr\\\"odinger operators. Our findings naturally lead us to proving results for\nL\\'evy measures, which have subexponential or exponential decay, respectively.\nThis leads to sharp transitions in the the decay rates of the resolvent\nkernels. We obtain estimates that allow us to describe and understand the\nintricate decay behaviour of the resolvent kernels and the bound states in\neither regime, extending findings by Carmona, Masters and Simon for fractional\nLaplacians (the subexponential regime) and classical relativistic operators\n(the exponential regime). Our proofs are mainly based on methods from the\ntheory of operator semigroups.",
        "In this work we show how to maximize absorption of plasmonic nanoparticles in\nterms of size, geometry and material. For that reason the interaction of\nnanoparticles with light was decomposed into different effects. We determined\nthat the main effect dictating the optimal amount of optical losses is\nradiation damping, and how it depends on nanoparticle size and geometry. Based\non this, we find that for many combinations of sizes and geometries losses in\npure metals are far from optimal. To overcome the aforementioned issue,\nalloying is presented as straightforward and flexible way of modulating the\noptical losses. Furthermore, strategies for tuning the optical losses to values\nabove, between, and even below those in pure plasmonic metals are developed in\nterms of selecting the right alloy composition. In some cases, alloys showed a\nmultifold increase in absorption when compared to pure plasmonic metals. The\nphysical reasons governing such changes are elucidated based on the electronic\nstructure changes during alloying of different metals, which enables\ngeneralization of the results to other systems. Besides increasing absorption,\nelectronic structure changes can also be utilized for channeling the absorbed\nenergy to suit different purposes, such as hot carrier generation for\nphotocatalysis or solar energy harvesting. Overall, these results establish\nalloying as a powerful tool for designing nanostructures for applications that\nutilize light absorption.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "We show that contact-rich motion planning is also sparsity-rich when viewed\nas polynomial optimization (POP). We can exploit not only the correlative and\nterm sparsity patterns that are general to all POPs, but also specialized\nsparsity patterns from the robot kinematic structure and the separability of\ncontact modes. Such sparsity enables the design of high-order but sparse\nsemidefinite programming (SDPs) relaxations--building upon Lasserre's moment\nand sums of squares hierarchy--that (i) can be solved in seconds by\noff-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to\nthe nonconvex contact-rich planning problems with small certified\nsuboptimality. Through extensive experiments both in simulation (Push Bot, Push\nBox, Push Box with Obstacles, and Planar Hand) and real world (Push T), we\ndemonstrate the power of using convex SDP relaxations to generate global\ncontact-rich motion plans. As a contribution of independent interest, we\nrelease the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++\nwith interfaces to both Python and Matlab--that automates sparsity exploitation\nfor robotics and beyond.",
        "Two abelian varieties $A$ and $B$ over a number field $K$ are said to be\nstrongly locally quadratic twists if they are quadratic twists at every\ncompletion of $K$. While it was known that this does not imply that $A$ and $B$\nare quadratic twists over $K$, the only known counterexamples (necessarily of\ndimension $\\geq 4$) are not geometrically simple. We show that, for every prime\n$p\\equiv 13 \\pmod{24}$, there exists a pair of geometrically simple abelian\nvarieties of dimension $p-1$ over $\\mathbb{Q}$ that are strongly locally\nquadratic twists but not quadratic twists. The proof is based on Galois\ncohomology computations and class field theory.",
        "Adaptive physics-informed super-resolution diffusion is developed for\nnon-invasive virtual diagnostics of the 6D phase space density of charged\nparticle beams. An adaptive variational autoencoder (VAE) embeds initial beam\ncondition images and scalar measurements to a low-dimensional latent space from\nwhich a 326 pixel 6D tensor representation of the beam's 6D phase space density\nis generated. Projecting from a 6D tensor generates physically consistent 2D\nprojections. Physics-guided super-resolution diffusion transforms\nlow-resolution images of the 6D density to high resolution 256x256 pixel\nimages. Un-supervised adaptive latent space tuning enables tracking of\ntime-varying beams without knowledge of time-varying initial conditions. The\nmethod is demonstrated with experimental data and multi-particle simulations at\nthe HiRES UED. The general approach is applicable to a wide range of complex\ndynamic systems evolving in high-dimensional phase space. The method is shown\nto be robust to distribution shift without re-training.",
        "We present a catalog of individual stellar and dust extinction properties\nalong close to 500,000 sight lines in the southwest bar of the Small Magellanic\nCloud (SMC). The catalog is based on multiband Hubble Space Telescope\nphotometric data spanning near-ultraviolet to near-infrared wavelengths from\nthe Small Magellanic Cloud Investigation of Dust and Gas Evolution survey\n(SMIDGE) covering a 100 x 200 pc area. We use the probabilistic technique of\nthe Bayesian Extinction And Stellar Tool (BEAST) to model the spectral energy\ndistributions of individual stars in SMIDGE and include the effects of\nobservational uncertainties in the data. We compare BEAST-derived dust\nextinction properties with tracers of the interstellar medium, such as the\nemission from the 12CO (2-1) transition (I(CO)), the dust mass surface density\n({\\Sigma}dust) from far-IR emission, the H I column density (N(HI)) from the\n21cm transition, and the mass fraction of polycyclic aromatic hydrocarbons\n(PAHs; qPAH, derived from IR emission). We find that the dust extinction (A(V\n)) in the SMIDGE field is strongly correlated with {\\Sigma}dust and I(CO), and\nless so with N(HI) and qPAH, and suggest potential explanations. Our extinction\nmeasurements are also sensitive to the presence of the 2175 {\\AA} bump in the\nextinction curve toward UV bright stars. While most do not show evidence for\nthe bump, we identify ~200 lines of sight that are 2175 {\\AA} bump candidates.\nFurthermore, we find distinct structures in the dust extinction-distance\ndistributions that provide insights into the 3D geometry of the SMC.",
        "Given $a,b\\ge 0$ and $t>0$, let $\\rho =\\{ \\rho _{s}\\} _{0\\le s\\le t}$ be a\nthree-dimensional Bessel bridge from $a$ to $b$ over $[0,t]$. In this paper,\nbased on a conditional identity in law between Brownian bridges stemming from\nPitman's theorem, we show in particular that the process given by\n\\begin{align*}\n  \\rho _{s}+\\Bigl| b-a+\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr|\n  -\\Bigl|\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr| ,\\quad 0\\le s\\le t, \\end{align*} has the same law as the time reversal\n$\\{ \\rho _{t-s}\\} _{0\\le s\\le t}$ of $\\rho $. As an immediate application,\nletting $R=\\{ R_{s}\\} _{s\\ge 0}$ be a three-dimensional Bessel process starting\nfrom $a$, we obtain the following time-reversal and time-inversion results on\n$R$: $\\{ R_{t-s}\\} _{0\\le s\\le t}$ is identical in law with the process given\nby \\begin{align*}\n  R_{s}+R_{t}-2\\min _{s\\le u\\le t}R_{u},\\quad 0\\le s\\le t, \\end{align*} when\n$a=0$, and $\\{ sR_{1\/s}\\} _{s>0}$ is identical in law with the process given by\n\\begin{align*}\n  R_{s}-2(1+s)\\min _{0\\le u\\le s}\\frac{R_{u}}{1+u}+a(1+s),\\quad s>0,\n\\end{align*} for every $a\\ge 0$.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
        "Within the Wilson RG of 'incomplete integration' as a function of the RG-time\n$t$, the non-linear differential RG flow for the energy $E_t[\\phi(.)]$\ntranslates for the probability distribution $P_t[\\phi(.)] \\sim e^{-\nE_t[\\phi(.)]} $ into the linear Fokker-Planck RG flow associated to independent\nnon-identical Ornstein-Uhlenbeck processes for the Fourier modes. The\ncorresponding Langevin stochastic differential equation for the real-space\nfield $\\phi_t(\\vec x)$ can be then interpreted within the Carosso perspective\nas genuine infinitesimal coarsening-transformations that are the analog of\nspin-blocking, and whose irreversible character is essential to overcome the\nparadox of the naive description of the Wegner-Morris RG flow as a mere\ninfinitesimal change of variables in the partition function integral. This\ninterpretation suggests to consider new RG-schemes, in particular the Carosso\nRG where the Langevin SDE corresponds to the well known stochastic heat\nequation or the Edwards-Wilkinson dynamics. We stress the advantages of this\nstochastic formulation of exact RG flows. While statistical field theory is\nusually written in infinite space, we focus here on the formulation on a large\nvolume $L^d$ with periodic boundary conditions, in order to distinguish between\nextensive and intensives observables while keeping the translation-invariance.\nSince the empirical magnetization $m_e \\equiv \\frac{1}{L^d} \\int_{L^d} d^d \\vec\nx \\ \\phi(\\vec x) $ is an intensive variable corresponding to the zero-momentum\nFourier coefficient of the field, its probability distribution $p_L(m_e)$ can\nbe obtained from the gradual integration over all the other Fourier\ncoefficients associated to non-vanishing-momenta via exact differential RG, in\norder to obtain the large deviation properties with respect to the volume\n$L^d$.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We prove continuous-valued analogues of the basic fact that Murray-von\nNeumann subequivalence of projections in II$_1$ factors is completely\ndetermined by tracial evaluations. We moreover use this result to solve the\nso-called trace problem in the case of factorial trivial $W^\\ast$-bundles whose\nbase space has covering dimension at most 1. Our arguments are based on\napplications of a continuous selection theorem due to Michael to von Neumann\nalgebras.",
        "Mode competition in nonequilibrium Rydberg gases enables the exploration of\nemergent many-body phases. This work leverages this emergent phase for electric\nfield detection at room temperature. Sensitive frequency-resolved electric\nfield measurements at very low-frequencies (VLF) are of central importance in a\nwide range of applications where deep-penetration is required in\ncommunications, navigation and imaging or surveying. The long wavelengths on\norder of 10-100 km (3-30 kHz) limit the efficiency, sensitivity, and bandwidth\nof compact classical detectors that are constrained by the Chu limit.\nRydberg-atom electrometers are an attractive approach for microwave\nelectric-field sensors but have reduced sensitivity at lower-frequencies. Very\nrecent efforts to advance the standard Rydberg-atoms approach is based on DC\nelectric-field (E-field) Stark shifting and have resulted in sensitivities\nbetween 67.9-2.2 uVcm-1Hz-1\/2 (0.1-10 kHz) by fine optimization of the DC\nE-field. A major challenge in these approaches is the need for embedded\nelectrodes or plates due to DC E-field Stark screening effect, which can\nperturb coupling of VLF signals when injected from external sources. In this\narticle, it is demonstrated that state-of-art sensitivity (~1.6-2.3\nuVcm-1Hz-1\/2) can instead be achieved using limit-cycle oscillations in\ndriven-dissipative Rydberg atoms by using a magnetic field (B-field) to develop\nmode-competition between nearby Rydberg states. The mode-competition between\nnearby Rydberg-states develop an effective transition centered at the\noscillation frequency capable of supporting external VLF E-field coupling in\nthe ~10-15kHz regime without the requirement for fine optimization of the\nB-field magnitude.",
        "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches.",
        "In this article, we define quasiprimitive quandles and describe them with the\nhelp of quasiprimitive permutation groups. As a consequence, we enumerate\nfinite non-affine simple quandles up to order $4096$.",
        "This work examines a stochastic volatility model with double-exponential\njumps in the context of option pricing. The model has been considered in\nprevious research articles, but no thorough analysis has been conducted to\nstudy its quality of calibration and pricing capabilities thus far. We provide\nevidence that this model outperforms challenger models possessing similar\nfeatures (stochastic volatility and jumps), especially in the fit of the short\nterm implied volatility smile, and that it is particularly tractable for the\npricing of exotic options from different generations. The article utilizes\nFourier pricing techniques (the PROJ method and its refinements) for different\ntypes of claims and several generations of exotics (Asian options, cliquets,\nbarrier options, and options on realized variance), and all source codes are\nmade publicly available to facilitate adoption and future research. The results\nindicate that this model is highly promising, thanks to the asymmetry of the\njumps distribution allowing it to capture richer dynamics than a normal jump\nsize distribution. The parameters all have meaningful econometrics\ninterpretations that are important for adoption by risk-managers.",
        "Accurately predicting electronic band gaps in halide perovskites using ab\ninitio density functional theory (DFT) is essential for their application in\noptoelectronic devices. Standard hybrid functionals such as HSE and PBE0 can\novercome the limitations of DFT with reasonable computational cost but are\nknown to underestimate the measured band gaps for layered halide perovskites.\nIn this study, we assess the performance of the doubly screened\ndielectric-dependent hybrid (DSH) functional for predicting band gaps in\nthree-dimensional (3D) and layered hybrid perovskites. We show that the DSH\nfunctional, which employs material-dependent mixing parameters derived from\nmacroscopic dielectric constants, provides accurate band gap predictions for 3D\nhalide perovskites when structural local disorder is considered. For layered\nhybrid perovskites, DSH functional based on average dielectric constants\noverestimates the band gaps. To improve the predictions and stay in a\nparameter-free ab initio workflow, we propose to use the calculated dielectric\nconstant of the respective 3D perovskites. We find that the DSH functionals\nusing the dielectric constants of the 3D perovskite accurately predict\nexperimental gaps, with the lowest mean absolute errors compared to HSE and\nPBE0 for layered perovskites with various organic spacers, as well as for\nmultilayered $BA_2MA_{n-1}Pb_{n}I_{3n-1}$ with n = 2, 3. Notably, the HSE\nfunctional systematically underestimates the band gaps in layered perovskites.\nWe attribute the root of this failure to the absence of non-local long-range\ndielectric screening, a critical factor for halide perovskites. The\ncomputational framework introduced here provides an efficient parameter-free ab\ninitio methodology for predicting the electronic properties of 3D and layered\nhalide perovskites and their heterostructures, aiding in developing advanced\noptoelectronic devices.",
        "Sangyop Lee has done much work to determine the knot types of twisted torus\nknots, including classifying the twisted torus knots which are the unknot.\nAmong the unknotted twisted torus knots are those of the form $(F_{n+2}, F_n,\nF_{n+1}, -1)$, where $F_i$ is the $i$th Fibonacci number. Here, we consider\ntwisted torus knots with parameters that are defined recursively, similarly to\nthe Fibonacci sequence. We call these \\textit{Horadam parameters}, after the\ngeneralization of the Fibonacci sequence introduced by A.F. Horadam. Here, we\nprovide families of twisted torus knots that generalize Lee's work with Horadam\nparameters. Additionally, we provide lists of primitive\/primitive and\nprimitive\/Seifert twisted torus knots and connect these lists to the Horadam\ntwisted torus knots.",
        "Axions are well-motivated dark matter particles. Many experiments are looking\nfor their experimental evidence. For haloscopes, the problem reduces to the\nidentification of a peak above a noisy baseline. Its modeling, however, may\nproblematic. State-of-the-art analysis rely on the Savitzky-Golay (SG)\nfiltering, which is intrinsically affected by any possible over fluctuation,\nleading to biased results. In this paper we study the efficiency that different\nextensions of SG can provide in the peak reconstruction in a standard\nhaloscope-like experiment. We show that, once the correlations among bins are\ntaken into account, there is no appreciable difference. The standard SG remains\nthe advisable choice because of its numerical efficiency.",
        "Kaon condensation in hyperon-mixed matter [($Y$+$K$) phase], which may be\nrealized in neutron stars, is discussed on the basis of chiral symmetry. With\nthe use of the effective chiral Lagrangian for kaon--baryon and kaon--kaon\ninteractions; coupled with the relativistic mean field theory and universal\nthree-baryon repulsive interaction, we clarify the effects of the $s$-wave\nkaon--baryon scalar interaction simulated by the kaon--baryon sigma terms and\nvector interaction (Tomozawa--Weinberg term) on kaon properties in\nhyperon-mixed matter, the onset density of kaon condensation, and the equation\nof state with the ($Y$+$K$) phase. In particular, the quark condensates in the\n($Y$+$K$) phase are obtained, and their relevance to chiral symmetry\nrestoration is discussed.",
        "In this work we investigate the chemical and kinetic nonequilibrium dynamics\nof the Higgs boson during the primordial Universe QGP (quark-gluon plasma)\nepoch $130\\mathrm{\\,GeV}>T>10\\mathrm{\\,GeV}$. We show that the Higgs bosons is\nalways out of chemical abundance equilibrium with a fugacity $\\Upsilon_h =\n0.69$ due to virtual decay channels. Additionally, Higgs momentum distribution\nis found to be ``cold'' for $T<40$\\,GeV, since the scattering rate drops below\nthe production rate.",
        "In this paper, we report on a study of the road networks of the provinces of\nCanada as complex networks. A number of statistical features have been analyzed\nand compared with two random models. In addition, we have also studied the\nresilience of these road networks under random failures and targeted attacks.",
        "We study uncertainty quantification for partial differential equations\nsubject to domain uncertainty. We parameterize the random domain using the\nmodel recently considered by Chernov and Le (2024) as well as Harbrecht,\nSchmidlin, and Schwab (2024) in which the input random field is assumed to\nbelong to a Gevrey smoothness class. This approach has the advantage of being\nsubstantially more general than models which assume a particular parametric\nrepresentation of the input random field such as a Karhunen-Loeve series\nexpansion. We consider both the Poisson equation as well as the heat equation\nand design randomly shifted lattice quasi-Monte Carlo (QMC) cubature rules for\nthe computation of the expected solution under domain uncertainty. We show that\nthese QMC rules exhibit dimension-independent, essentially linear cubature\nconvergence rates in this framework. In addition, we complete the error\nanalysis by taking into account the approximation errors incurred by dimension\ntruncation of the random input field and finite element discretization.\nNumerical experiments are presented to confirm the theoretical rates.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "Most technologically useful materials spanning multiple length scales are\npolycrystalline. Polycrystalline microstructures are composed of a myriad of\nsmall crystals or grains with different lattice orientations which are\nseparated by interfaces or grain boundaries. The changes in the grain and grain\nboundary structure of polycrystals highly influence the materials properties\nincluding, but not limited to, electrical, mechanical, and thermal. Thus, an\nunderstanding of how microstructures evolve is essential for the engineering of\nnew materials. In this paper, we consider a recently introduced nonlinear\nFokker-Planck-type system and establish a global well-posedness result for it.\nSuch systems under specific energy laws emerge in the modeling of the grain\nboundary dynamics in polycrystals.",
        "The structure formation in the local Universe is considered within the\nweak-field modification of General Relativity involving the cosmological\nconstant. This approach enables to describe the dynamics of groups and clusters\nof galaxies, to explain the discrepancy in the observational properties of the\nlocal (late) and the global (early) Universe, i.e. the Hubble tension as a\nresult of two flows, local and global ones, with non-equal Hubble parameters.\nThe kinetic analysis with the modified gravitational potential involving the\ncosmological constant is shown to predict semi-periodical structure of\nfilaments in the local universe. In the local scale this complements the\nZeldovich pancake theory of evolution of the primordial density perturbations\nand of structure formation in the cosmological scale. The role of the\ncosmological constant is outlined in rescaling of the physical constants from\none aeon to another within the Conformal Cyclic Cosmology.",
        "We study the PAC property of scenario decision-making algorithms, that is,\nthe ability to make a decision that has an arbitrarily low risk of violating an\nunknown safety constraint, provided sufficiently many realizations (called\nscenarios) of the safety constraint are sampled. Sufficient conditions for\nscenario decision-making algorithms to be PAC are available in the literature,\nsuch as finiteness of the VC dimension of its associated classifier and\nexistence of a compression scheme. We study the question of whether these\nsufficient conditions are also necessary. We show with counterexamples that\nthis is not the case in general. This contrasts with binary classification\nlearning, for which the analogous conditions are sufficient and necessary.\nPopular scenario decision-making algorithms, such as scenario optimization,\nenjoy additional properties, such as stability and consistency. We show that\neven under these additional assumptions the above conclusions hold. Finally, we\nderive a necessary condition for scenario decision-making algorithms to be PAC,\ninspired by the VC dimension and the so-called no-free-lunch theorem.",
        "This review provides an observational perspective on the fundamental\nproperties of super-Eddington accretion onto supermassive black holes in\nquasars. It begins by outlining the selection criteria, particularly focusing\non optical and UV broad-line intensity ratios, used to identify a population of\nunobscured super-Eddington candidates. Several defining features place these\ncandidates at the extreme end of the Population A in main sequence of quasars:\namong them are the highest observed singly-ionized iron emission, extreme\noutflow velocities in UV resonance lines, and unusually high metal abundances.\nThese key properties reflect the coexistence of a virialized sub-system within\nthe broad-line region alongside powerful outflows, with the observed gas\nenrichment likely driven by nuclear or circumnuclear star formation. The most\ncompelling evidence for the occurrence of super-Eddington accretion onto\nsupermassive black holes comes from recent observations of massive black holes\nat early cosmic epochs. These black holes require rapid growth rates that are\nonly achievable through radiatively inefficient super-Eddington accretion.\nFurthermore, extreme Eddington ratios, close to or slightly exceeding unity,\nare consistent with the saturation of radiative output per unit mass predicted\nby accretion disk theory for super-Eddington accretion rates. The extreme\nproperties of super-Eddington candidates suggest that these quasars could make\nthem stable and well-defined cosmological distance indicators, leveraging the\ncorrelation between broad-line width and luminosity expected in virialized\nsystems. Finally, several analogies with accretion processes around\nstellar-mass black holes, particularly in the high\/soft state, are explored to\nprovide additional insight into the mechanisms driving super-Eddington\naccretion."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Multiscale modeling and simulation of brain blood flow",
    "start_abstract":"The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks"
      ],
      "abstract":[
        "Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Behaviour of Newton Polygon over polynomial composition",
        "Paying Attention to Facts: Quantifying the Knowledge Capacity of\n  Attention Layers",
        "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine\n  Learning Techniques",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "Avoiding spurious sharpness minimization broadens applicability of SAM",
        "Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on\n  African American English",
        "MuJoCo Playground",
        "Adaptive Mesh Refinement for Variational Inequalities",
        "Efficient and Universal Neural-Network Decoder for Stabilizer-Based\n  Quantum Error Correction",
        "A View of the Certainty-Equivalence Method for PAC RL as an Application\n  of the Trajectory Tree Method",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Agentic AI Systems Applied to tasks in Financial Services: Modeling and\n  model risk management crews",
        "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT",
        "Balanced Rate-Distortion Optimization in Learned Image Compression",
        "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation",
        "Vectorial Kato inequality for $p$-harmonic maps with optimal constant",
        "All-order solution of ladders and rainbows in Minimal Subtraction",
        "Sequential One-Sided Hypothesis Testing of Markov Chains",
        "Advancing C-C Coupling of Electrocatalytic CO2 Reduction Reaction for\n  C2+ Products",
        "Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models",
        "Formation Control of Multi-agent System with Local Interaction and\n  Artificial Potential Field",
        "Dynamic Bragg microcavities in collisions of unipolar light pulses of\n  unusual shape in two- and three-level medium",
        "Ordinal Exponentiation in Homotopy Type Theory",
        "Hamiltonian Heat Baths, Coarse-Graining and Irreversibility: A\n  Microscopic Dynamical Entropy from Classical Mechanics",
        "RemiHaven: Integrating \"In-Town\" and \"Out-of-Town\" Peers to Provide\n  Personalized Reminiscence Support for Older Drifters",
        "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots",
        "Derived deformation functors, Koszul duality, and Maurer-Cartan spaces",
        "Leveraging Registers in Vision Transformers for Robust Adaptation",
        "TherAIssist: Assisting Art Therapy Homework and Client-Practitioner\n  Collaboration through Human-AI Interaction"
      ],
      "abstract":[
        "In this paper, we study the structure of Newton polygons for compositions of\npolynomials over the rationals. We establish sufficient conditions under which\nthe successive vertices of the Newton polygon of the composition $ g(f^n(x)) $\nwith respect to a prime $ p $ can be explicitly described in terms of the\nNewton polygon of the polynomial $ g(x) $. Our results provide deeper insights\ninto how the Newton polygon of a polynomial evolves under iteration and\ncomposition, with applications to the study of dynamical irreducibility,\neventual stability, non-monogenity of tower of number fields, etc.",
        "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.",
        "Fire safety practices are important to reduce the extent of destruction\ncaused by fire. While smoke alarms help save lives, firefighters struggle with\nthe increasing number of false alarms. This paper presents a precise and\nefficient Weighted ensemble model for decreasing false alarms. It estimates the\ndensity, computes weights according to the high and low-density regions,\nforwards the high region weights to KNN and low region weights to XGBoost and\ncombines the predictions. The proposed model is effective at reducing response\ntime, increasing fire safety, and minimizing the damage that fires cause. A\nspecifically designed dataset for smoke detection is utilized to test the\nproposed model. In addition, a variety of ML models, such as Logistic\nRegression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB),\nK-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient\nBoosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To\nmaximize the use of the smoke detection dataset, all the algorithms utilize the\nSMOTE re-sampling technique. After evaluating the assessment criteria, this\npaper presents a concise summary of the comprehensive findings obtained by\ncomparing the outcomes of all models.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nreasoning tasks, leading to their widespread deployment. However, recent\nstudies have highlighted concerning biases in these models, particularly in\ntheir handling of dialectal variations like African American English (AAE). In\nthis work, we systematically investigate dialectal disparities in LLM reasoning\ntasks. We develop an experimental framework comparing LLM performance given\nStandard American English (SAE) and AAE prompts, combining LLM-based dialect\nconversion with established linguistic analyses. We find that LLMs consistently\nproduce less accurate responses and simpler reasoning chains and explanations\nfor AAE inputs compared to equivalent SAE questions, with disparities most\npronounced in social science and humanities domains. These findings highlight\nsystematic differences in how LLMs process and reason about different language\nvarieties, raising important questions about the development and deployment of\nthese systems in our multilingual and multidialectal world. Our code repository\nis publicly available at https:\/\/github.com\/Runtaozhou\/dialect_bias_eval.",
        "We introduce MuJoCo Playground, a fully open-source framework for robot\nlearning built with MJX, with the express goal of streamlining simulation,\ntraining, and sim-to-real transfer onto robots. With a simple \"pip install\nplayground\", researchers can train policies in minutes on a single GPU.\nPlayground supports diverse robotic platforms, including quadrupeds, humanoids,\ndexterous hands, and robotic arms, enabling zero-shot sim-to-real transfer from\nboth state and pixel inputs. This is achieved through an integrated stack\ncomprising a physics engine, batch renderer, and training environments. Along\nwith video results, the entire framework is freely available at\nplayground.mujoco.org",
        "Variational inequalities play a pivotal role in a wide array of scientific\nand engineering applications. This project presents two techniques for adaptive\nmesh refinement (AMR) in the context of variational inequalities, with a\nspecific focus on the classical obstacle problem.\n  We propose two distinct AMR strategies: Variable Coefficient Elliptic\nSmoothing (VCES) and Unstructured Dilation Operator (UDO). VCES uses a nodal\nactive set indicator function as the initial iterate to a time-dependent heat\nequation problem. Solving a single step of this problem has the effect of\nsmoothing the indicator about the free boundary. We threshold this smoothed\nindicator function to identify elements near the free boundary. Key parameters\nsuch as timestep and threshold values significantly influence the efficacy of\nthis method.\n  The second strategy, UDO, focuses on the discrete identification of elements\nadjacent to the free boundary, employing a graph-based approach to mark\nneighboring elements for refinement. This technique resembles the dilation\nmorphological operation in image processing, but tailored for unstructured\nmeshes.\n  We also examine the theory of variational inequalities, the convergence\nbehavior of finite element solutions, and implementation in the Firedrake\nfinite element library. Convergence analysis reveals that accurate free\nboundary estimation is pivotal for solver performance. Numerical experiments\ndemonstrate the effectiveness of the proposed methods in dynamically enhancing\nmesh resolution around free boundaries, thereby improving the convergence rates\nand computational efficiency of variational inequality solvers. Our approach\nintegrates seamlessly with existing Firedrake numerical solvers, and it is\npromising for solving more complex free boundary problems.",
        "Quantum error correction is crucial for large-scale quantum computing, but\nthe absence of efficient decoders for new codes like quantum low-density\nparity-check (QLDPC) codes has hindered progress. Here we introduce a universal\ndecoder based on linear attention sequence modeling and graph neural network\nthat operates directly on any stabilizer code's graph structure. Our numerical\nexperiments demonstrate that this decoder outperforms specialized algorithms in\nboth accuracy and speed across diverse stabilizer codes, including surface\ncodes, color codes, and QLDPC codes. The decoder maintains linear time scaling\nwith syndrome measurements and requires no structural modifications between\ndifferent codes. For the Bivariate Bicycle code with distance 12, our approach\nachieves a 39.4% lower logical error rate than previous best decoders while\nrequiring only ~1% of the decoding time. These results provide a practical,\nuniversal solution for quantum error correction, eliminating the need for\ncode-specific decoders.",
        "Reinforcement learning (RL) enables an agent interacting with an unknown MDP\n$M$ to optimise its behaviour by observing transitions sampled from $M$. A\nnatural entity that emerges in the agent's reasoning is $\\widehat{M}$, the\nmaximum likelihood estimate of $M$ based on the observed transitions. The\nwell-known \\textit{certainty-equivalence} method (CEM) dictates that the agent\nupdate its behaviour to $\\widehat{\\pi}$, which is an optimal policy for\n$\\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy\nminimax-optimal sample complexity in some regions of the parameter space for\nPAC RL with a generative model~\\citep{Agarwal2020GenModel}.\n  A seemingly unrelated algorithm is the ``trajectory tree method''\n(TTM)~\\citep{Kearns+MN:1999}, originally developed for efficient decision-time\nplanning in large POMDPs. This paper presents a theoretical investigation that\nstems from the surprising finding that CEM may indeed be viewed as an\napplication of TTM. The qualitative benefits of this view are (1) new and\nsimple proofs of sample complexity upper bounds for CEM, in fact under a (2)\nweaker assumption on the rewards than is prevalent in the current literature.\nOur analysis applies to both non-stationary and stationary MDPs.\nQuantitatively, we obtain (3) improvements in the sample-complexity upper\nbounds for CEM both for non-stationary and stationary MDPs, in the regime that\nthe ``mistake probability'' $\\delta$ is small. Additionally, we show (4) a\nlower bound on the sample complexity for finite-horizon MDPs, which establishes\nthe minimax-optimality of our upper bound for non-stationary MDPs in the\nsmall-$\\delta$ regime.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews that can effectively collaborate to perform complex\nmodeling and model risk management (MRM) tasks. The modeling crew consists of a\nmanager and multiple agents who perform specific tasks such as exploratory data\nanalysis, feature engineering, model selection, hyperparameter tuning, model\ntraining, model evaluation, and writing documentation. The MRM crew consists of\na manager along with specialized agents who perform tasks such as checking\ncompliance of modeling documentation, model replication, conceptual soundness,\nanalysis of outcomes, and writing documentation. We demonstrate the\neffectiveness and robustness of modeling and MRM crews by presenting a series\nof numerical examples applied to credit card fraud detection, credit card\napproval, and portfolio credit risk modeling datasets.",
        "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
        "Learned image compression (LIC) using deep learning architectures has seen\nsignificant advancements, yet standard rate-distortion (R-D) optimization often\nencounters imbalanced updates due to diverse gradients of the rate and\ndistortion objectives. This imbalance can lead to suboptimal optimization,\nwhere one objective dominates, thereby reducing overall compression efficiency.\nTo address this challenge, we reformulate R-D optimization as a multi-objective\noptimization (MOO) problem and introduce two balanced R-D optimization\nstrategies that adaptively adjust gradient updates to achieve more equitable\nimprovements in both rate and distortion. The first proposed strategy utilizes\na coarse-to-fine gradient descent approach along standard R-D optimization\ntrajectories, making it particularly suitable for training LIC models from\nscratch. The second proposed strategy analytically addresses the reformulated\noptimization as a quadratic programming problem with an equality constraint,\nwhich is ideal for fine-tuning existing models. Experimental results\ndemonstrate that both proposed methods enhance the R-D performance of LIC\nmodels, achieving around a 2\\% BD-Rate reduction with acceptable additional\ntraining cost, leading to a more balanced and efficient optimization process.\nCode will be available at https:\/\/gitlab.com\/viper-purdue\/Balanced-RD.",
        "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https:\/\/github.com\/Zhenxuan-Zhang\/GEMA_score.",
        "We derive the sharp vectorial Kato inequality for $p$-harmonic mappings.\nSurprisingly, the optimal constant differs from the one obtained for scalar\nvalued $p$-harmonic functions by Chang, Chen, and Wei. As an application we\ndemonstrate how this inequality can be used in the study of regularity of\n$p$-harmonic maps. Furthermore, in the case of $p$-harmonic maps from $B^3$ to\n$\\mathbb{S}^3$, we enhance the known range of $p$ values for which regularity\nis achieved. Specifically, we establish that for $p \\in [2, 2.642]$, minimizing\n$p$-harmonic maps must be regular.",
        "In dimensional regularization with $D=D_0-2\\epsilon$, the minimal subtraction\n(MS) scheme is characterized by counterterms that only consist of singular\nterms in $\\epsilon$. We develop a general method to compute the infinite sums\nof massless ladder or rainbow Feynman integrals in MS at $D_0$. Our method is\nbased on relating the MS-solution to a kinematic solution at a\ncoupling-dependent renormalization point. If the $\\epsilon$-dependent Mellin\ntransform of the kernel diagram of the insertions can be computed in closed\nform, we typically obtain a closed expression for the all-order solution in MS.\nAs examples, we consider Yukawa theory and $\\phi^4$ theory in $D_0=4$, and\n$\\phi^3$ theory in $D_0=6$.",
        "We study the problem of sequentially testing whether a given stochastic\nprocess is generated by a known Markov chain. Formally, given access to a\nstream of random variables, we want to quickly determine whether this sequence\nis a trajectory of a Markov chain with a known transition matrix $P$ (null\nhypothesis) or not (composite alternative hypothesis). This problem naturally\narises in many engineering problems.\n  The main technical challenge is to develop a sequential testing scheme that\nadapts its sample size to the unknown alternative. Indeed, if we knew the\nalternative distribution (that is, the transition matrix) $Q$, a natural\napproach would be to use a generalization of Wald's sequential probability\nratio test (SPRT). Building on this intuition, we propose and analyze a family\nof one-sided SPRT-type tests for our problem that use a data-driven estimator\n$\\hat{Q}$. In particular, we show that if the deployed estimator admits a\nworst-case regret guarantee scaling as $\\mathcal{O}\\left( \\log{t} \\right)$,\nthen the performance of our test asymptotically matches that of SPRT in the\nsimple hypothesis testing case. In other words, our test automatically adapts\nto the unknown hardness of the problem, without any prior information. We end\nwith a discussion of known Markov chain estimators with $\\mathcal{O}\\left(\n\\log{t} \\right)$ regret.",
        "The production of multicarbon (C2+) products through electrocatalytic CO2\nreduction reaction (CO2RR) is crucial to addressing global environmental\nchallenges and advancing sustainable energy solutions. However, efficiently\nproducing these high-value chemicals via C-C coupling reactions is a\nsignificant challenge. This requires catalysts with optimized surface\nconfigurations and electronic properties capable of breaking the scaling\nrelations among various intermediates. In this report, we introduce the\nfundamentals of electrocatalytic CO2RR and the mechanism of C-C coupling. We\nexamine the effects of catalytic surface interactions with key intermediates\nand reaction pathways, and discuss emerging strategies for enhancing C-C\ncoupling reactions toward C2+ products. Despite varieties of these strategies,\nwe summarize direct clues for the proper design of the catalyst for the\nelectrocatalytic CO2RR towards C2+ products, aiming to provide valuable\ninsights to broad readers in the field.",
        "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https:\/\/github.com\/horizonsinzqs\/QueryAttack.",
        "A novel local interaction control method (LICM) is proposed in this paper to\nrealize the formation control of multi-agent system (MAS). A local interaction\nleader follower (LILF) structure is provided by coupling the advantages of\ninformation consensus and leader follower frame, the agents can obtain the\nstate information of the leader by interacting with their neighbours, which\nwill reduce the communication overhead of the system and the dependence on a\nsingle node of the topology. In addition, the artificial potential field (APF)\nmethod is introduced to achieve obstacle avoidance and collision avoidance\nbetween agents. Inspired by the stress response of animals, a stress response\nmechanism-artificial potential field (SRM-APF) is proposed, which will be\ntriggered when the local minimum problem of APF occurs. Ultimately, the\nsimulation experiments of three formation shapes, including triangular\nformation, square formation and hexagonal formation, validate the effectiveness\nof the proposed method.",
        "Unipolar light pulses with a non-zero electric area due to the unidirectional\naction on charged particles can be used for the ultrafast control of the\nproperties of quantum systems. To control atomic properties in an efficient\nway, it is necessary to vary the temporal shape of the pulses used. This has\nled to the problem of obtaining pulses of an unusual shape, such as a\nrectangular one. A number of new phenomena, not possible with conventional\nmulti-cycle pulses, were discovered by analyzing the interaction of such\nunipolar pulses with matter. These include the formation of dynamic\nmicrocavities at each resonant transition of a multilevel medium when such\npulses collide with the medium. In this work, we compare the behavior of\ndynamic microcavities in a two-level and a three-level medium when unipolar\npulses of unusual shape (rectangular) are collided with the medium. We do this\non the basis of the numerical solution of the system for the density matrix of\nthe medium and the wave equation for the electric field. Medium parameters\ncorrespond to atomic hydrogen. It is shown that for rectangular pulses in a\nthree-level medium, the dynamics of the cavities can be very different from the\ntwo-level model, as opposed to pulses of other shapes (e.g. Gaussian shape).\nWhen the third level of the medium is taken into account, the self-induced\ntransparency-like regime disappears. Differences in the dynamics of resonators\nin a three-level medium are revealed when the pulses behave like 2{\\pi} pulses\nof self-induced transparency.",
        "While ordinals have traditionally been studied mostly in classical\nframeworks, constructive ordinal theory has seen significant progress in recent\nyears. However, a general constructive treatment of ordinal exponentiation has\nthus far been missing. We present two seemingly different definitions of\nconstructive ordinal exponentiation in the setting of homotopy type theory. The\nfirst is abstract, uses suprema of ordinals, and is solely motivated by the\nexpected equations. The second is more concrete, based on decreasing lists, and\ncan be seen as a constructive version of a classical construction by\nSierpi\\'{n}ski based on functions with finite support. We show that our two\napproaches are equivalent (whenever it makes sense to ask the question), and\nuse this equivalence to prove algebraic laws and decidability properties of the\nexponential. All our results are formalized in the proof assistant Agda.",
        "The Hamiltonian evolution of an isolated classical system is reversible, yet\nthe second law of thermodynamics states that its entropy can only increase.\nThis has confounded attempts to identify a `Microscopic Dynamical Entropy'\n(MDE), by which we mean an entropy computable from the system's evolving\nphase-space density $\\rho(t)$, that equates {\\em quantitatively} to its\nthermodynamic entropy $S^{\\rm th}(t)$, both within and beyond equilibrium.\nSpecifically, under Hamiltonian dynamics the Gibbs entropy of $\\rho$ is\nconserved in time; those of coarse-grained approximants to $\\rho$ show a second\nlaw but remain quantitatively unrelated to heat flow. Moreover coarse-graining\ngenerally destroys the Hamiltonian evolution, giving paradoxical predictions\nwhen $\\rho(t)$ exactly rewinds, as it does after velocity-reversal. Here we\nderive the MDE for an isolated system XY in which subsystem Y acts as a heat\nbath for subsystem X. We allow $\\rho_{XY}(t)$ to evolve without\ncoarse-graining, but compute its entropy by disregarding the detailed structure\nof $\\rho_{Y|X}$. The Gibbs entropy of the resulting phase-space density\n$\\tilde\\rho_{XY}(t)$ comprises the MDE for the purposes of both classical and\nstochastic thermodynamics. The MDE obeys the second law whenever $\\rho_X$\nevolves independently of the details of Y, yet correctly rewinds after\nvelocity-reversal of the full XY system.",
        "With increasing social mobility and an aging society, more older adults in\nChina are migrating to new cities, known as \"older drifters.\" Due to fewer\nsocial connections and cultural adaptation challenges, they face negative\nemotions such as loneliness and depression. While reminiscence-based\ninterventions have been used to improve older adults' psychological well-being,\nchallenges such as the lack of tangible materials and limited social resources\nconstrain the feasibility of traditional reminiscence approaches for older\ndrifters. To address this challenge, we designed RemiHaven, a personalized\nreminiscence support tool based on a two-phase formative study. It integrates\n\"In-Town\" and \"Out-of-Town\" peer agents to enhance personalization, engagement,\nand emotional resonance in the reminiscence process, powered by Multimodal\nLarge Language Models (MLLMs). Our evaluations show RemiHaven's strengths in\nsupporting reminiscence while identifying potential challenges. We conclude by\noffering insights for the future design of reminiscence support tools for older\nmigrants.",
        "Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps:\/\/github.com\/sahars93\/RL-Navigation.",
        "We summarise the chain of comparisons showing Hinich's derived Maurer-Cartan\nfunctor gives an equivalence between differential graded Lie algebras and\nderived Schlessinger functors on Artinian differential graded-commutative\nalgebras. We include some motivating deformation problems and analogues for\nmore general Koszul dual pairs of operads.",
        "Vision Transformers (ViTs) have shown success across a variety of tasks due\nto their ability to capture global image representations. Recent studies have\nidentified the existence of high-norm tokens in ViTs, which can interfere with\nunsupervised object discovery. To address this, the use of \"registers\" which\nare additional tokens that isolate high norm patch tokens while capturing\nglobal image-level information has been proposed. While registers have been\nstudied extensively for object discovery, their generalization properties\nparticularly in out-of-distribution (OOD) scenarios, remains underexplored. In\nthis paper, we examine the utility of register token embeddings in providing\nadditional features for improving generalization and anomaly rejection. To that\nend, we propose a simple method that combines the special CLS token embedding\ncommonly employed in ViTs with the average-pooled register embeddings to create\nfeature representations which are subsequently used for training a downstream\nclassifier. We find that this enhances OOD generalization and anomaly\nrejection, while maintaining in-distribution (ID) performance. Extensive\nexperiments across multiple ViT backbones trained with and without registers\nreveal consistent improvements of 2-4\\% in top-1 OOD accuracy and a 2-3\\%\nreduction in false positive rates for anomaly detection. Importantly, these\ngains are achieved without additional computational overhead.",
        "Art therapy homework is essential for fostering clients' reflection on daily\nexperiences between sessions. However, current practices present challenges:\nclients often lack guidance for completing tasks that combine art-making and\nverbal expression, while therapists find it difficult to track and tailor\nhomework. How HCI systems might support art therapy homework remains\nunderexplored. To address this, we present TherAIssist, comprising a\nclient-facing application leveraging human-AI co-creative art-making and\nconversational agents to facilitate homework, and a therapist-facing\napplication enabling customization of homework agents and AI-compiled homework\nhistory. A 30-day field study with 24 clients and 5 therapists showed how\nTherAIssist supported clients' homework and reflection in their everyday\nsettings. Results also revealed how therapists infused their practice\nprinciples and personal touch into the agents to offer tailored homework, and\nhow AI-compiled homework history became a meaningful resource for in-session\ninteractions. Implications for designing human-AI systems to facilitate\nasynchronous client-practitioner collaboration are discussed."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks",
    "start_abstract":"Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Multiscale modeling and simulation of brain blood flow"
      ],
      "abstract":[
        "The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Fundamental Trade-off Between Computation and Communication in Private\n  Coded Distributed Computing",
        "Reinforcement-Learning Portfolio Allocation with Dynamic Embedding of\n  Market Information",
        "Adaptive Moment Estimation Optimization Algorithm Using Projection\n  Gradient for Deep Learning",
        "Completely Integrable Foliations: Singular Locus, Invariant Curves and\n  Topological Counterparts",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Efficient Framework for Solving Plasma Waves with Arbitrary\n  Distributions",
        "Scalable skewed Bayesian inference for latent Gaussian models",
        "Signal-to-noise ratio aware minimax analysis of sparse linear regression",
        "Unveiling the Dynamics and Genesis of Small-scale Fine Structure Loops\n  in the Lower Solar Atmosphere",
        "Data mining the functional architecture of the brain's circuitry",
        "LuxNAS: A Coherent Photonic Neural Network Powered by Neural\n  Architecture Search",
        "Single spin asymmetry in forward $pA$ collisions from Pomeron-Odderon\n  interference",
        "Keldysh field theory approach to direct electric and thermoelectric\n  currents in quantum dots coupled to superconducting leads",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Alpha-element abundance patterns in star-forming regions of the local\n  Universe",
        "Edge spectrum for truncated $\\mathbb{Z}_2$-insulators",
        "Sub-kHz single-frequency pulsed semiconductor laser based on NPRO\n  injection locking",
        "Enhancing Olfactory Perception Through Large Language Models:\n  Integrating Sensory Data for Advanced Odor Recognition",
        "On Lorentzian-Euclidean black holes and Lorentzian to Riemannian metric\n  transitions",
        "Network topology of the Euro Area interbank market",
        "High-Dimensional Bayesian Optimization Using Both Random and Supervised\n  Embeddings",
        "Observe Gamma-Rays and Neutrinos Associated with Ultra-High Energy\n  Cosmic Rays",
        "Role extraction by matrix equations and generalized random walks",
        "Inheritance of shadowing for dynamical semigroups",
        "Image Reconstruction from an Elastically Distorted Scan",
        "Investigating Solar Wind Outflows from Open-Closed Magnetic Field\n  Structures Using Coordinated Solar Orbiter and Hinode Observations",
        "Reed-Muller Codes on CQ Channels via a New Correlation Bound for Quantum\n  Observables",
        "Transitions to Intermittent Chaos in Quorum Sensing Dynamics",
        "SIAC Accuracy Enhancement of Stochastic Galerkin Solutions for Wave\n  Equations with Uncertain Coefficients"
      ],
      "abstract":[
        "Distributed computing enables scalable machine learning by distributing tasks\nacross multiple nodes, but ensuring privacy in such systems remains a\nchallenge. This paper introduces a private coded distributed computing model\nthat integrates privacy constraints to keep task assignments hidden. By\nleveraging placement delivery arrays (PDAs), we design an extended PDA\nframework to characterize achievable computation and communication loads under\nprivacy constraints. By constructing two classes of extended PDAs, we explore\nthe trade-offs between computation and communication, showing that although\nprivacy increases communication overhead, it can be significantly alleviated\nthrough optimized PDA-based coded strategies.",
        "We develop a portfolio allocation framework that leverages deep learning\ntechniques to address challenges arising from high-dimensional, non-stationary,\nand low-signal-to-noise market information. Our approach includes a dynamic\nembedding method that reduces the non-stationary, high-dimensional state space\ninto a lower-dimensional representation. We design a reinforcement learning\n(RL) framework that integrates generative autoencoders and online meta-learning\nto dynamically embed market information, enabling the RL agent to focus on the\nmost impactful parts of the state space for portfolio allocation decisions.\nEmpirical analysis based on the top 500 U.S. stocks demonstrates that our\nframework outperforms common portfolio benchmarks and the predict-then-optimize\n(PTO) approach using machine learning, particularly during periods of market\nstress. Traditional factor models do not fully explain this superior\nperformance. The framework's ability to time volatility reduces its market\nexposure during turbulent times. Ablation studies confirm the robustness of\nthis performance across various reinforcement learning algorithms.\nAdditionally, the embedding and meta-learning techniques effectively manage the\ncomplexities of high-dimensional, noisy, and non-stationary financial data,\nenhancing both portfolio performance and risk management.",
        "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10\/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
        "We study codimension $q \\geq 2$ holomorphic foliations defined in a\nneighborhood of a point $P$ of a complex manifold that are completely\nintegrable, i.e. with $q$ independent meromorphic first integrals. We show that\neither $P$ is a regular point, a non-isolated singularity or there are\ninfinitely many invariant analytic varieties through $P$ of the same dimension\nas the foliation, the so called separatrices. Moreover, we see that this\nphenomenon is of topological nature.\n  Indeed, we introduce topological counterparts of completely integrable local\nholomorphic foliations and tools, specially the concept of total holonomy\ngroup, to build holomorphic first integrals if they have isolated separatrices.\nAs a result, we provide a topological characterization of completely integrable\nnon-degenerated elementary isolated singularities of vector fields with an\nisolated separatrix.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "Plasma, which constitutes 99\\% of the visible matter in the universe, is\ncharacterized by a wide range of waves and instabilities that play a pivotal\nrole in space physics, astrophysics, laser-plasma interactions, fusion\nresearch, and laboratory experiments. The linear physics of these phenomena is\ndescribed by kinetic dispersion relations (KDR). However, solving KDRs for\narbitrary velocity distributions remains a significant challenge, particularly\nfor non-Maxwellian distributions frequently observed in various plasma\nenvironments. This work introduces a novel, efficient, and unified numerical\nframework to address this challenge. The proposed method rapidly and accurately\nyields all significant solutions of KDRs for nearly arbitrary velocity\ndistributions, supporting both unstable and damped modes across all frequencies\nand wavevectors. The approach expands plasma species' velocity distribution\nfunctions using a series of carefully chosen orthogonal basis functions and\nemploys a highly accurate rational approximation to transform the problem into\nan equivalent matrix eigenvalue problem, eliminating the need for initial\nguesses. The efficiency and versatility of this framework are demonstrated,\nenabling simplified studies of plasma waves with arbitrary distributions. This\nadvancement paves the way for uncovering new physics in natural plasma\nenvironments, such as spacecraft observations in space plasmas, and\napplications like wave heating in fusion research.",
        "Approximate Bayesian inference for the class of latent Gaussian models can be\nachieved efficiently with integrated nested Laplace approximations (INLA).\nBased on recent reformulations in the INLA methodology, we propose a further\nextension that is necessary in some cases like heavy-tailed likelihoods or\nbinary regression with imbalanced data. This extension formulates a skewed\nversion of the Laplace method such that some marginals are skewed and some are\nkept Gaussian while the dependence is maintained with the Gaussian copula from\nthe Laplace method. Our approach is formulated to be scalable in model and data\nsize, using a variational inferential framework enveloped in INLA. We\nillustrate the necessity and performance using simulated cases, as well as a\ncase study of a rare disease where class imbalance is naturally present.",
        "We consider parameter estimation under sparse linear regression -- an\nextensively studied problem in high-dimensional statistics and compressed\nsensing. While the minimax framework has been one of the most fundamental\napproaches for studying statistical optimality in this problem, we identify two\nimportant issues that the existing minimax analyses face: (i) The\nsignal-to-noise ratio appears to have no effect on the minimax optimality,\nwhile it shows a major impact in numerical simulations. (ii) Estimators such as\nbest subset selection and Lasso are shown to be minimax optimal, yet they\nexhibit significantly different performances in simulations. In this paper, we\ntackle the two issues by employing a minimax framework that accounts for\nvariations in the signal-to-noise ratio (SNR), termed the SNR-aware minimax\nframework. We adopt a delicate higher-order asymptotic analysis technique to\nobtain the SNR-aware minimax risk. Our theoretical findings determine three\ndistinct SNR regimes: low-SNR, medium-SNR, and high-SNR, wherein minimax\noptimal estimators exhibit markedly different behaviors. The new theory not\nonly offers much better elaborations for empirical results, but also brings new\ninsights to the estimation of sparse signals in noisy data.",
        "Recent high-resolution solar observations have unveiled the presence of\nsmall-scale loop-like structures in the lower solar atmosphere, often referred\nto as unresolved fine structures, low-lying loops, and miniature hot loops.\nThese structures undergo rapid changes within minutes, and their formation\nmechanism has remained elusive. In this study, we conducted a comprehensive\nanalysis of two small loops utilizing data from the Interface Region Imaging\nSpectrograph (IRIS), the Goode Solar Telescope (GST) at Big Bear Solar\nObservatory, and the Atmospheric Imaging Assembly (AIA) and the Helioseismic\nMagnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), aiming to\nelucidate the underlying process behind their formation. The GST observations\nrevealed that these loops, with lengths of $\\sim$3.5 Mm and heights of $\\sim$1\nMm, manifest as bright emission structures in H$\\alpha$ wing images,\nparticularly prominent in the red wing. IRIS observations showcased these loops\nin 1330 angstrom slit-jaw images, with TR and chromospheric line spectra\nexhibiting significant enhancement and broadening above the loops, indicative\nof plasmoid-mediated reconnection during their formation. Additionally, we\nobserved upward-erupting jets above these loops across various passbands.\nFurthermore, differential emission measurement analysis reveals an enhanced\nemission measure at the location of these loops, suggesting the presence of\nplasma exceeding 1 MK. Based on our observations, we propose that these loops\nand associated jets align with the minifilament eruption model. Our findings\nsuggest a unified mechanism governing the formation of small-scale loops and\njets akin to larger-scale X-ray jets.",
        "The brain is a highly complex organ consisting of a myriad of subsystems that\nflexibly interact and adapt over time and context to enable perception,\ncognition, and behavior. Understanding the multi-scale nature of the brain,\ni.e., how circuit- and moleclular-level interactions build up the fundamental\ncomponents of brain function, holds incredible potential for developing\ninterventions for neurodegenerative and psychiatric diseases, as well as open\nnew understanding into our very nature. Historically technological limitations\nhave forced systems neuroscience to be local in anatomy (localized, small\nneural populations in single brain areas), in behavior (studying single tasks),\nin time (focusing on specific stages of learning or development), and in\nmodality (focusing on imaging single biological quantities). New developments\nin neural recording technology and behavioral monitoring now provide the data\nneeded to break free of local neuroscience to global neuroscience: i.e.,\nunderstanding how the brain's many subsystem interact, adapt, and change across\nthe multitude of behaviors animals and humans must perform to thrive.\nSpecifically, while we have much knowledge of the anatomical architecture of\nthe brain (i.e., the hardware), we finally are approaching the data needed to\nfind the functional architecture and discover the fundamental properties of the\nsoftware that runs on the hardware. We must take this opportunity to bridge\nbetween the vast amounts of data to discover this functional architecture which\nwill face numerous challenges from low-level data alignment up to high level\nquestions of interpretable mathematical models of behavior that can synthesize\nthe myriad of datasets together.",
        "We demonstrate a novel coherent photonic neural network using tunable\nphase-change-material-based couplers and neural architecture search. Compared\nto the MZI-based Clements network, our results indicate 85% reduction in the\nnetwork footprint while maintaining the accuracy.",
        "Working in the hybrid framework of the high energy $pA$ collisions we\nidentify a new contribution to transverse single spin asymmetry (SSA). The\nphase necessary for the SSA is provided by the Pomeron-Odderon interference in\nthe dense nuclear target. The complete formula for the $pA \\to h X$ polarized\ncross section also contains the transversity distribution for the polarized\nprojectile as well as the real part of the twist-3 fragmentation function. We\nnumerically estimate the asymmetry $A_N$ and its nuclear dependence. Based on a\nmodel computation we find that $A_N$ can be a percent level in the forward and\nlow-$P_{h\\perp}$ region. For large nuclei we find significant suppression, with\n$A_N \\propto A^{-7\/6}$ parametrically. As a notable feature we find a node of\n$A_N$ as a function of the $P_{h\\perp}$ around the values of the initial\nsaturation scale that could be used to test this mechanism experimentally.",
        "We study the transport properties of a quantum dot contacted to two\nsuperconducting reservoirs by means of the Keldysh field theory approach. We\ndetermine the direct current occurring at equilibrium and the electric and\nthermoelectric currents triggered when the system is driven out of equilibrium\nby a voltage or a temperature bias, also for a normal-quantum\ndot-superconductor junction. In particular, we derive and present for the first\ntime the explicit expression of the thermoelectric current in a\nsuperconductor-quantum dot-superconductor junction for any values of the\ntemperature difference between the superconducting leads. We show that in the\nlinear response regime, in addition to the Josephson current, a weakly\nphase-dependent thermoelectric contribution occurs, providing that\nelectron-hole symmetry is broken. Far from linearity, instead, other\ncontributions arise which lead to thermoelectric effects, dominant at weak\ncoupling, also in the presence of particle-hole symmetry.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "(Abridged) We reassess the alpha-element abundance ratios (Ne\/O, S\/O, Ar\/O)\nwith respect to metallicity in ~1000 spectra of Galactic and extragalactic HII\nregions and star-forming galaxies (SFGs) of the local Universe. Using the DEep\nSpectra of Ionised REgions Database (DESIRED) Extended project (DESIRED-E),\nwhich includes spectra with direct electron temperature determinations, we\nhomogeneously derive physical conditions and chemical abundances for all\nobjects. Various ionisation correction factor (ICF) schemes are analyzed for\nNe, S, and Ar to identify the most reliable abundance estimates. Our findings\nindicate that the ICF scheme by Izotov et al. (2006) better reproduces the\nNe\/O, S\/O, and Ar\/O trends. Ne\/O ratios in HII regions display large dispersion\nand no clear dependence on O\/H, suggesting that current ICF(Ne) schemes fail\nfor these objects. However, SFGs show consistent linear relations with slightly\npositive slopes for log(Ne\/O) vs. 12+log(O\/H) or 12+log(Ne\/H), likely\ninfluenced by metallicity-dependent O dust depletion and ICF effects. The\nlog(S\/O) vs. 12+log(O\/H) distribution is largely constant, especially for HII\nregions or combined samples (SFGs + HII regions). Conversely, log(S\/O) vs.\n12+log(S\/H) shows a tight linear fit with a positive slope, flattening at\n12+log(S\/H) < 6.0, suggesting S contributions from SNe Ia. For log(Ar\/O) vs.\n12+log(O\/H), similar trends emerge for HII regions and SFGs, independent of\nionisation degree or ICF(Ar). A slight log(Ar\/O) decrease with increasing\n12+log(O\/H) contrasts with log(Ar\/O) vs. 12+log(Ar\/H), which shows a small\npositive slope, indicating a possible minor Ar contribution from SNe Ia.",
        "Fermionic time-reversal-invariant insulators in two dimensions -- class AII\nin the Kitaev table -- come in two different topological phases. These are\ncharacterized by a $\\mathbb{Z}_2$-index: the Fu-Kane-Mele index. We prove that\nif two such insulators with different indices occupy regions containing\narbitrarily large balls, then the spectrum of the resulting operator fills the\nbulk spectral gap. Our argument follows a proof by contradiction developed in\nan earlier work by two of the authors for quantum Hall systems. It boils down\nto showing that the $\\mathbb{Z}_2$-index can be computed only from bulk\ninformation in sufficiently large balls. This is achieved via a result of\nindependent interest: a local trace formula for the $\\mathbb{Z}_2$-index.",
        "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
        "The integration of biological principles into artificial olfactory systems\nhas led to significant advancements in odor detection and classification.\nInspired by the intricate mechanisms of natural olfaction, researchers are\ndeveloping sophisticated systems that mimic the functionality of biological\nolfactory pathways. These systems utilize high-density chemoresistive sensor\narrays (HCSA) combined with advanced computational techniques, such as\nFPGA-accelerated glomerular convergence circuits (FGCC) and hierarchical graph\nneural networks (HGNN). This bioinspired approach enables real-time adaptive\nresponses to volatile organic compounds (VOCs), enhancing the accuracy and\nefficiency of odor identification. At the core of these innovations is the\nmultiparametric sigmoidal sensor activation (MPSA), which quantifies VOCs by\nleveraging the diverse responses of sensor arrays. The implementation of\nlateral inhibition via programmable synaptic crossbars (LIPSC) further refines\nodor processing by mimicking neural interactions found in biological systems.\nAdditionally, temporal self-organizing maps (TSOM) facilitate dynamic\nclustering of odor patterns, allowing for a nuanced understanding of complex\nodor environments. A novel aspect of this research lies in the Grassmannian\nmanifold embedding (GME) of odor profiles, which provides a mathematical\nframework for representing and analyzing the multidimensional nature of odors.\nCoupled with Hamiltonian Monte Carlo-optimized feedback (HMC-FB), this system\neffectively compensates for drift in sensor readings, ensuring consistent\nperformance over time. By bridging the gap between biological inspiration and\ntechnological innovation, these artificial olfactory systems are poised to\nrevolutionize applications ranging from environmental monitoring to food safety\nand healthcare diagnostics.",
        "In recent papers on spacetimes with a signature-changing metric, the concept\nof a Lorentzian-Euclidean black hole and new elements for Lorentzian-Riemannian\nsignature change have been introduced. A Lorentzian-Euclidean black hole is a\nsignature-changing modification of the Schwarzschild spacetime satisfying the\nvacuum Einstein equations in a weak sense. Here the event horizon serves as a\nboundary beyond which time becomes imaginary. We demonstrate that the proper\ntime needed to reach the horizon remains finite, consistently with the\nclassical Schwarzschild solution. About Lorentzian to Riemannian metric\ntransitions, we stress that the hypersurface where the metric signature changes\nis naturally a spacelike hypersurface which might be identified with the future\nor past causal boundary of the Lorentzian sector. Moreover, a number of\ngeometric interpretations appear, as the degeneracy of the metric corresponds\nto the collapse of the causal cones into a line, the degeneracy of the dual\nmetric corresponds to collapsing into a hyperplane, and additional geometric\nstructures on the transition hypersurface (Galilean and dual Galilean) might be\nexplored.",
        "The rapidly increasing availability of large amounts of granular financial\ndata, paired with the advances of big data related technologies induces the\nneed of suitable analytics that can represent and extract meaningful\ninformation from such data. In this paper we propose a multi-layer network\napproach to distill the Euro Area (EA) banking system in different distinct\nlayers. Each layer of the network represents a specific type of financial\nrelationship between banks, based on various sources of EA granular data\ncollections. The resulting multi-layer network allows one to describe, analyze\nand compare the topology and structure of EA banks from different perspectives,\neventually yielding a more complete picture of the financial market. This\ngranular information representation has the potential to enable researchers and\npractitioners to better apprehend financial system dynamics as well as to\nsupport financial policies to manage and monitor financial risk from a more\nholistic point of view.",
        "Bayesian optimization (BO) is one of the most powerful strategies to solve\ncomputationally expensive-to-evaluate blackbox optimization problems. However,\nBO methods are conventionally used for optimization problems of small dimension\nbecause of the curse of dimensionality. In this paper, a high-dimensionnal\noptimization method incorporating linear embedding subspaces of small dimension\nis proposed to efficiently perform the optimization. An adaptive learning\nstrategy for these linear embeddings is carried out in conjunction with the\noptimization. The resulting BO method, named efficient global optimization\ncoupled with random and supervised embedding (EGORSE), combines in an adaptive\nway both random and supervised linear embeddings. EGORSE has been compared to\nstate-of-the-art algorithms and tested on academic examples with a number of\ndesign variables ranging from 10 to 600. The obtained results show the high\npotential of EGORSE to solve high-dimensional blackbox optimization problems,\nin terms of both CPU time and the limited number of calls to the expensive\nblackbox simulation.",
        "IceCube measures a diffuse neutrino flux comparable to the Waxman-Bahcall\nbound, which suggests the possibility that the ultra-high energy cosmic rays\n(UHECRs) have a common origin with diffuse high energy neutrinos. We propose\nhigh energy gamma-ray and\/or neutrino observations toward the arrival\ndirections of UHECRs to search for the sources and test this possibility. We\ncalculate the detection probability of gamma-ray\/neutrino sources, and find\nthat the average probability per UHECR of >10 EeV is $\\sim$10% if the\nsensitivity of the gamma-ray or neutrino telescope is $\\sim$10$^{-12}$ erg\ncm$^{-2}$s$^{-1}$ and the source number density is $\\sim$10$^{-5}$ Mpc$^{-3}$.\nFuture gamma-ray and neutrino observations toward UHECRs, e.g., by LHAASO-WCDA,\nCTA, IceCube\/Gen2, are encouraged to constrain the density of UHECR sources or\neven identify the sources of UHECRs.",
        "The nodes in a network can be grouped into 'roles' based on similar\nconnection patterns. This is usually achieved by defining a pairwise node\nsimilarity matrix and then clustering rows and columns of this matrix. This\npaper presents a new similarity matrix for solving role extraction problems in\ndirected networks, which is defined as the solution of a matrix equation and\ncomputes node similarities based on random walks that can proceed along the\nlink direction and in the opposite direction. The resulting node similarity\nmeasure performs remarkably in role extraction tasks on directed networks with\nheterogeneous node degree distributions.",
        "We extend the single-perturbation approach (developed in our earlier\npublications for the case of a single map) to the analysis of the shadowing\nproperty for semigroups of endomorphisms. Our approach allows to give a\nconstructive representation for a true trajectory which shadows a given\npseudo-trajectory. One of the main motivations is the question of inheritance:\ndoes the presence of shadowing for all generators of a semigroup imply\nshadowing for the semigroup and vice versa. Somewhat surprisingly, the answer\nto these questions is generally negative. Moreover, the situation with\nshadowing turns out to be quite different in a semigroup and in a\nnon-autonomous system, despite the fact that the latter can be represented as a\nsingle branch of the former.",
        "We consider the problem of inverting the artifacts associated with scanning a\npage from an open book, i.e. \"xeroxing.\" The process typically leads to a\nnon-uniform combination of distortion, blurring and darkening owing to the fact\nthat the page is bound to a stiff spine that causes the sheet of paper to be\nbent inhomogeneously. Complementing purely data-driven approaches, we use\nknowledge about the geometry and elasticity of the curved sheet to pose and\nsolve a minimal physically consistent inverse problem to reconstruct the image.\nOur results rely on 3 dimensionless parameters, all of which can be measured\nfor a scanner, and show that we can improve on the data-driven approaches. More\nbroadly, our results might serve as a \"textbook\" example and a tutorial of how\nknowledge of generative mechanisms can speed up the solution of inverse\nproblems.",
        "ESA\/NASA's Solar Orbiter (SO) allows us to study the solar corona at closer\ndistances and from different perspectives, which helps us to gain significant\ninsights into the origins of the solar wind. In this work, we present the\nanalysis of solar wind outflows from two locations: a narrow open-field\ncorridor and a small, mid-latitude coronal hole. These outflows were observed\noff-limb by the Metis coronagraph onboard SO and on-disk by the Extreme\nUltraviolet Imaging Spectrometer (EIS) onboard Hinode. Magnetic field\nextrapolations suggest that the upflow regions seen in EIS were the sources of\nthe outflowing solar wind observed with Metis. We find that the plasma\nassociated with the narrow open-field corridor has higher electron densities\nand lower outflow velocities compared to the coronal hole plasma in the middle\ncorona, even though the plasma properties of the two source regions in the low\ncorona are found to be relatively similar. The speed of solar wind from the\nopen-field corridor also shows no correlation with the magnetic field expansion\nfactor, unlike the coronal hole. These pronounced differences at higher\naltitudes may arise from the dynamic nature of the low-middle corona, in which\nreconnection can readily occur and may play an important role in driving solar\nwind variability.",
        "The question of whether Reed-Muller (RM) codes achieve capacity on binary\nmemoryless symmetric (BMS) channels has drawn attention since it was resolved\npositively for the binary erasure channel by Kudekar et al. in 2016. In 2021,\nReeves and Pfister extended this to prove the bit-error probability vanishes on\nBMS channels when the code rate is less than capacity. In 2023, Abbe and Sandon\nimproved this to show the block-error probability also goes to zero. These\nresults analyze decoding functions using symmetry and the nested structure of\nRM codes. In this work, we focus on binary-input symmetric classical-quantum\n(BSCQ) channels and the Holevo capacity. For a BSCQ, we consider observables\nthat estimate the channel input in the sense of minimizing the mean-squared\nerror (MSE). Using the orthogonal decomposition of these observables under a\nweighted inner product, we establish a recursive relation for the minimum MSE\nestimate of a single bit in the RM code. Our results show that any set of\n$2^{o(\\sqrt{\\log N})}$ bits can be decoded with a high probability when the\ncode rate is less than the Holevo capacity.",
        "This study analyses the dynamical consequences of heterogeneous temporal\ndelays within a quorum sensing-inspired (QS-inspired) system, specifically\naddressing the differential response kinetics of two subpopulations to\nsignalling molecules. A nonlinear delay differential equation (DDE) model,\npredicated upon an activator-inhibitor framework, is formulated to represent\nthe interspecies interactions. Key analytical techniques, including the\nderivation of the pseudo-characteristic polynomial and the determination of\nHopf bifurcation criteria, are employed to investigate the stability properties\nof steady-state solutions. The analysis reveals the critical role of multiple,\ndissimilar delays in modulating system dynamics and inducing bifurcations.\nNumerical simulations, conducted in conjunction with analytical results, reveal\nthe emergence of periodic self-sustained oscillations and intermittent chaotic\nbehaviour. These observations emphasise the intricate relationship between\ntemporal heterogeneity and the stability landscape of systems exhibiting\nQS-inspired dynamics. This interplay highlights the capacity for temporal\nvariations to induce complex dynamical transitions within such systems. These\nfindings assist to the comprehension of temporal dynamics within these and\nrelated systems, and may contribute to the development of strategies aimed at\nmodulating intercellular communication and engineering synthetic biological\nsystems with temporal control.",
        "This article establishes the usefulness of the Smoothness-Increasing\nAccuracy-Increasing (SIAC) filter for reducing the errors in the mean and\nvariance for a wave equation with uncertain coefficients solved via generalized\npolynomial chaos (gPC) whose coefficients are approximated using discontinuous\nGalerkin (DG-gPC). Theoretical error estimates that utilize information in the\nnegative-order norm are established. While the gPC approximation leads to order\nof accuracy of $m-1\/2$ for a sufficiently smooth solution (smoothness of $m$ in\nrandom space), the approximated coefficients solved via DG improves from order\n$k+1$ to $2k+1$ for a solution of smoothness $2k+2$ in physical space. Our\nnumerical examples verify the performance of the filter for improving the\nquality of the approximation and reducing the numerical error and significantly\neliminating the noise from the spatial approximation of the mean and variance.\nFurther, we illustrate how the errors are effected by both the choice of\nsmoothness of the kernel and number of function translates in the kernel.\nHence, this article opens the applicability of SIAC filters to other hyperbolic\nproblems with uncertainty, and other stochastic equations."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Quantum machine learning in chemistry and materials",
    "start_abstract":"Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models.",
    "start_categories":[
      "cs.ET"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b26"
      ],
      "title":[
        "Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble"
      ],
      "abstract":[
        "Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Universality for catalytic equations and fully parked trees",
        "A Production Routing Problem with Mobile Inventories",
        "Wikipedia Contributions in the Wake of ChatGPT",
        "Gaussian credible intervals in Bayesian nonparametric estimation of the\n  unseen",
        "Identifying rich clubs in spatiotemporal interaction networks",
        "First-principle based Floquet engineering of solids in the velocity\n  gauge",
        "A Peanut-hull-PLA based 3D printing filament with antimicrobial effect",
        "Non-reciprocity and multibody interactions in acoustically levitated\n  particle systems: A three body problem",
        "On a reaction-diffusion virus model with general boundary conditions in\n  heterogeneous environments",
        "Schmid-Higgs Mode in the Presence of Pair-Breaking Interactions",
        "Thom polynomials for singularities of maps",
        "$\\texttt{PrecisionLauricella}$: package for numerical computation of\n  Lauricella functions depending on a parameter",
        "Unidentified Aerial Phenomena. Characterization of Dark UAPs",
        "Accumulation of Charge on an Extremal Black Hole's Event Horizon",
        "Expression of special stretched $9j$ coefficients in terms of $_5F_4$\n  hypergeometric series",
        "Generalization Bounds for Equivariant Networks on Markov Data",
        "Meaningful, Useful and Legitimate Information in the Use of Index\n  Numbers for Decision Making",
        "Thermal Conduction and Thermal-Driven Winds in Magnetized Viscous\n  Accretion Disk Dynamics",
        "Optimizing Bidding Curves for Renewable Energy in Two-Settlement\n  Electricity Markets",
        "Three-loop chiral effective potential in the Wess-Zumino model",
        "A splitting theorem for manifolds with spectral nonnegative Ricci\n  curvature and mean-convex boundary",
        "Scalable solution chemical synthesis and comprehensive analysis of\n  Bi2Te3 and Sb2Te3",
        "Observation of the dimer-singlet phase in the one-dimensional S = 1\/2\n  Heisenberg antiferromagnet Cu(Ampy)ClBr (Ampy= C6H8N2 =\n  2-(Aminomethyl)pyridine)",
        "ZnO@C\/PVDF Electrospun Membrane as Piezoelectric Nanogenerator for\n  Wearable Applications",
        "Restoring thermalization in long-range quantum magnets with staggered\n  magnetic fields",
        "Functional equation arising in behavioral sciences: solvability and\n  collocation scheme in H\\\"older spaces",
        "LZMidi: Compression-Based Symbolic Music Generation",
        "Detection and control of electronic orbital magnetism by spin waves in\n  honeycomb ferromagnets",
        "Oscillation-eliminating central DG schemes for hyperbolic conservation\n  laws"
      ],
      "abstract":[
        "We show that critical parking trees conditioned to be fully parked converge\nin the scaling limits towards the Brownian growth-fragmentation tree, a\nself-similar Markov tree different from Aldous' Brownian tree recently\nintroduced and studied by Bertoin, Curien and Riera. As a by-product of our\nstudy, we prove that positive non-linear polynomial equations involving a\ncatalytic variable display a universal polynomial exponent $5\/2$ at their\nsingularity, confirming a conjecture by Chapuy, Schaeffer and Drmota & Hainzl.\nCompared to previous analytical works on the subject, our approach is\nprobabilistic and exploits an underlying random walk hidden in the random tree\nmodel.",
        "Hydrogen is an energy vector, and one possible way to reduce CO 2 emissions.\nThis paper focuses on a hydrogen transport problem where mobile storage units\nare moved by trucks between sources to be refilled and destinations to meet\ndemands, involving swap operations upon arrival. This contrasts with existing\nliterature where inventories remain stationary. The objective is to optimize\ndaily routing and refilling schedules of the mobile storages. We model the\nproblem as a flow problem on a time-expanded graph, where each node of the\ngraph is indexed by a time-interval and a location and then, we give an\nequivalent Mixed Integer Linear Programming (MILP) formulation of the problem.\nFor small to medium-sized instances, this formulation can be efficiently solved\nusing standard MILP solvers. However, for larger instances, the computational\ncomplexity increases significantly due to the highly combinatorial nature of\nthe refilling process at the sources. To address this challenge, we propose a\ntwo-step heuristic that enhances.",
        "How has Wikipedia activity changed for articles with content similar to\nChatGPT following its introduction? We estimate the impact using\ndifferences-in-differences models, with dissimilar Wikipedia articles as a\nbaseline for comparison, to examine how changes in voluntary knowledge\ncontributions and information-seeking behavior differ by article content. Our\nanalysis reveals that newly created, popular articles whose content overlaps\nwith ChatGPT 3.5 saw a greater decline in editing and viewership after the\nNovember 2022 launch of ChatGPT than dissimilar articles did. These findings\nindicate heterogeneous substitution effects, where users selectively engage\nless with existing platforms when AI provides comparable content. This points\nto potential uneven impacts on the future of human-driven online knowledge\ncontributions.",
        "The unseen-species problem assumes $n\\geq1$ samples from a population of\nindividuals belonging to different species, possibly infinite, and calls for\nestimating the number $K_{n,m}$ of hitherto unseen species that would be\nobserved if $m\\geq1$ new samples were collected from the same population. This\nis a long-standing problem in statistics, which has gained renewed relevance in\nbiological and physical sciences, particularly in settings with large values of\n$n$ and $m$. In this paper, we adopt a Bayesian nonparametric approach to the\nunseen-species problem under the Pitman-Yor prior, and propose a novel\nmethodology to derive large $m$ asymptotic credible intervals for $K_{n,m}$,\nfor any $n\\geq1$. By leveraging a Gaussian central limit theorem for the\nposterior distribution of $K_{n,m}$, our method improves upon competitors in\ntwo key aspects: firstly, it enables the full parameterization of the\nPitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need\nof Monte Carlo sampling, enhancing computational efficiency. We validate the\nproposed method on synthetic and real data, demonstrating that it improves the\nempirical performance of competitors by significantly narrowing the gap between\nasymptotic and exact credible intervals for any $m\\geq1$.",
        "Spatial networks are widely used in various fields to represent and analyze\ninteractions or relationships between locations or spatially distributed\nentities.There is a network science concept known as the 'rich club'\nphenomenon, which describes the tendency of 'rich' nodes to form densely\ninterconnected sub-networks. Although there are established methods to quantify\ntopological, weighted, and temporal rich clubs individually, there is limited\nresearch on measuring the rich club effect in spatially-weighted temporal\nnetworks, which could be particularly useful for studying dynamic spatial\ninteraction networks. To address this gap, we introduce the spatially-weighted\ntemporal rich club (WTRC), a metric that quantifies the strength and\nconsistency of connections between rich nodes in a spatiotemporal network.\nAdditionally, we present a unified rich club framework that distinguishes the\nWTRC effect from other rich club effects, providing a way to measure\ntopological, weighted, and temporal rich club effects together. Through two\ncase studies of human mobility networks at different spatial scales, we\ndemonstrate how the WTRC is able to identify significant weighted temporal rich\nclub effects, whereas the unweighted equivalent in the same network either\nfails to detect a rich club effect or inaccurately estimates its significance.\nIn each case study, we explore the spatial layout and temporal variations\nrevealed by the WTRC analysis, showcasing its particular value in studying\nspatiotemporal interaction networks. This research offers new insights into the\nstudy of spatiotemporal networks, with critical implications for applications\nsuch as transportation, redistricting, and epidemiology.",
        "We introduce a practical and accurate strategy to capture light-matter\ninteractions using the Floquet formalism in the velocity gauge in combination\nwith realistic first-principle models of solids. The velocity gauge, defined by\nthe linear coupling to the vector potential, is a standard method to capture\nthe light-matter interaction in solids. However, its use with first-principle\nmodels has been limited by the challenging fact that it requires a large number\nof bands for convergence and its incompatibility with non-local pseudopotential\nplane wave methods. To improve its convergence properties, we explicitly take\ninto account the truncation of Hilbert space in the construction of the Floquet\nHamiltonian in the velocity gauge. To avoid the incompatibility with the\npseudopotentials, we base our computations on generalized tight-binding\nHamiltonians derived from first-principles through maximally-localized Wannier\nfunctions. We exemplify the approach by computing the optical absorption\nspectra of laser-dressed trans-polyacetylene chain using realistic electronic\nstructure. We show that, by proceeding in this way, Floquet consideration\ninvolving the truncated Hilbert spaces reproduces the full basis calculations\nwith only a few bands and with significantly reduced computation time. The\nstrategy has been implemented in FloqticS, a general code for the Floquet\nengineering of the optical properties of materials. Overall, this work\nintroduces a useful theoretical tool to realize Floquet engineering of\nrealistic solids in the velocity gauge.",
        "Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an\nabundant biomass source with a long shelf life. In this study, we incorporate\npeanut hull powder into PLA polymer, imparting recyclability, biodegradability,\nand biocompatibility, along with the antimicrobial properties of AHL particles.\nIn particular, we treat AHL particles as a reinforcement for PLA polymer to\nproduce 3D printing filament compatible with the fused filament fabrication\n(FFF) 3D printing method. We provide a step-by-step method for preparing AHL\nparticles, incorporating them into PLA, and ultimately forming high-quality\nfilaments. We assess the quality of the filaments in terms of extruded\ndimensions, mechanical strength, and elastic modulus, along with physical\nproperties such as porosity and melt flow index. We evaluate the printability\nand wettability of the filaments as well. Notably, and unlike other\nbiomass-based reinforcements in PLA, AHL preserves the filament's strength and\nenhances its elastic modulus. 3D-printed components fabricated using our\nPLA-AHL filaments successfully retain their antimicrobial properties and\nexhibit increased overall hardness. However, this comes at the expense of\nforming more microvoids and a rougher surface, making the material more prone\nto fracture and leading to a slight reduction in fracture toughness with\nincreasing AHL mass fraction.",
        "In active fluids and active solids the constituents individually generate\nmovement by each extracting energy from their environment or from their own\nsource. Non-reciprocal interactions among these active constituents then enable\nnovel collective behavior that often can be strikingly counterintuitive.\nHowever, non-reciprocity in these cases typically requires that the interacting\nbodies have different physical properties or it needs to be programmed\nexplicitly into all pairwise interactions. Here we show that collective\nactivity in a driven system can emerge spontaneously through multibody\nnonreciprocal forces, even if all bodies are individually non-active and have\nidentical properties. We demonstrate this with as few as three identical\nspheres, acoustically levitated in air, which exhibit collective activity as\nthey interact through non-pairwise forces: similar to the classic gravitational\nthree-body problem, the interaction between two spheres depends sensitively on\nthe relative position of the third sphere. Non-reciprocity arises naturally\nfrom both near-field sound scattering and microstreaming forces among the\nspheres. The underdamped dynamics in air furthermore make it possible to go\nbeyond collective center-of-mass propulsion or rotation and observe internal,\nengine-like reconfigurations that follow limit cycles. These findings open up\nnew possibilities for self-assembly, where now multibody interactions not only\ndetermine the resulting structure but also drive the spontaneously emerging\ndynamics.",
        "To describe the propagation of West Nile virus and\/or Zika virus, in this\npaper, we propose and study a time-periodic reaction-diffusion model with\ngeneral boundary conditions in heterogeneous environments and with four\nunknowns: susceptible host, infectious host, susceptible vector and infectious\nvector. We can prove that such problem has a positive time periodic solution if\nand only if host and vector persist and the basic reproduction ratio is greater\nthan one, and moreover the positive time periodic solution is unique and\nglobally asymptotically stable when it exists.",
        "Collective modes in superconductors provided the first realization of the\nHiggs mechanism. The transverse Goldstone mode acquires a gap (i.e. a mass)\nwhen it hybridizes with the electromagnetic gauge field. The longitudinal\nSchmid-Higgs mode, on the other hand, is always massive. In conventional BCS\ntheory, its gap is exactly $2\\Delta$, coinciding with the excitation threshold\nfor quasiparticles. Being situated right at the edge of the continuum spectrum\nit gives rise to peculiar dynamics for the Schmid-Higgs mode. For instance,\nwhen suddenly excited at $t=0$, it exhibits algebraically decaying oscillations\nof the form $\\sim \\sin(2\\Delta t)\/{t}^{1\/2}$. In this study, we explore the\nbehavior of Schmid-Higgs oscillations in the presence of pair-breaking\nmechanisms, such as magnetic impurities or in-plane magnetic fields. These\nprocesses suppress the quasiparticle excitation threshold down to\n$2\\varepsilon_g < 2\\Delta$, potentially placing the longitudinal mode within\nthe continuum spectrum. Despite this, we show that the algebraically decaying\noscillations persist, taking the form $\\sim \\sin(2\\varepsilon_g t)\/t^2$. The\nSchmid-Higgs mode becomes truly overdamped and exponentially decaying only in\nthe gapless superconductors with $\\varepsilon_g=0$.",
        "This is a gentle introduction to a general theory of universal polynomials\nassociated to classification of map-germs, called Thom polynomials. The theory\nwas originated by Ren\\'e Thom in the 1950s and has since been evolved in\nvarious aspects by many authors. In a nutshell, this is about intersection\ntheory on certain moduli spaces, say `classifying spaces of\nmono\/multi-singularities of maps', which provides consistent and deep insights\ninto both classical and modern enumerative geometry with many potential\napplications.",
        "We introduce the $\\texttt{PrecisionLauricella}$ package, a computational tool\ndeveloped in Wolfram Mathematica for high-precision numerical evaluations of\nLauricella functions with indices linearly dependent on a parameter,\n$\\varepsilon$. The package leverages a method based on analytical continuation\nvia Frobenius generalized power series, providing an efficient and accurate\nalternative to conventional approaches relying on multi-dimensional series\nexpansions or Mellin--Barnes representations. This one-dimensional approach is\nparticularly advantageous for high-precision calculations and facilitates\nfurther optimization through $\\varepsilon$-dependent reconstruction from\nevaluations at specific numerical values, enabling efficient parallelization.\nThe underlying mathematical framework for this method has been detailed in our\nprevious work, while the current paper focuses on the design, implementation,\nand practical applications of the $\\texttt{PrecisionLauricella}$ package.",
        "We use high-tech observations of Unidentified Aerial Phenomena (UAP) class\nobjects to evaluate their characteristics. We present data in three cases. (1)\nMulti-side daytime observations of UAPs over Kiev. (2) Night observations of a\ngroup of objects in the vicinity of the Moon. (3) UAP observations in the\ncombat zone in Ukraine. Dark UAPs in the visible wavelength range are observed\nonly during the day. At night they can only be seen in the infrared wavelength\nrange. We note large sizes of UAPs, from three to six kilometers.They exhibit\nlarge velocities, from 2.5 Mach and much larger. They have low albedo, from\nthree percent and below, that is, they actually exhibit features of a\ncompletely black body.",
        "We numerically analyze the behavior of a charged scalar field on a fixed\nextremal Reissner-Nordstr\\\"om background. We find an extension of the Aretakis\ninstability characterized by an accumulation of charge on the extremal event\nhorizon. In particular, when the charge coupling to the scalar field is\nsufficiently large, the charge density on the horizon asymptotes to a nonzero\nconstant at late times. By constructing monochromatic initial data at the onset\nof charged superradiance, we give evidence supporting the claim that this\ninstability is connected to the presence of a nearly zero-damped mode.\nThroughout this work, we employ a numerical integration scheme in compactified\ndouble-null coordinates, which allows us to capture the asymptotic behavior of\nthe matter at the boundaries of the spacetime.",
        "The Clebsch-Gordan coefficients or Wigner $3j$ symbols are known to be\nproportional to a $_3F_2(1)$ hypergeometric series, and Racah $6j$ coefficients\nto a $_4F_3(1)$. In general, however, non-trivial $9j$ symbols can not be\nexpressed as a $_5F_4$. In this letter, we show, using the Dougall-Ramanujan\nidentity, that special stretched $9j$ symbols can be reformulated as $_5F_4(1)$\nhypergeometric series.",
        "Equivariant neural networks play a pivotal role in analyzing datasets with\nsymmetry properties, particularly in complex data structures. However,\nintegrating equivariance with Markov properties presents notable challenges due\nto the inherent dependencies within such data. Previous research has primarily\nconcentrated on establishing generalization bounds under the assumption of\nindependently and identically distributed data, frequently neglecting the\ninfluence of Markov dependencies. In this study, we investigate the impact of\nMarkov properties on generalization performance alongside the role of\nequivariance within this context. We begin by applying a new McDiarmid's\ninequality to derive a generalization bound for neural networks trained on\nMarkov datasets, using Rademacher complexity as a central measure of model\ncapacity. Subsequently, we utilize group theory to compute the covering number\nunder equivariant constraints, enabling us to obtain an upper bound on the\nRademacher complexity based on this covering number. This bound provides\npractical insights into selecting low-dimensional irreducible representations,\nenhancing generalization performance for fixed-width equivariant neural\nnetworks.",
        "Often information relevant to a decision is summarized in an index number.\nThis paper explores conditions under which conclusions using index numbers are\nrelevant to the decision that needs to be made. Specifically it explores the\nidea that a statement using scales of measurement is meaningful in the sense\nthat its truth or falsity does not depend on an arbitrary choice of parameters;\nthe concept that a conclusion using index numbers is useful for the specific\ndecision that needs to be made; and the notion that such a conclusion is\nlegitimate in the sense that it is collected and used in a way that satisfies\ncultural, historical, organizational and legal constraints. While\nmeaningfulness is a precisely defined concept, usefulness and legitimacy are\nnot, and the paper explores properties of these concepts that lay the\ngroundwork for making them more precise. Many examples involving two well-known\nand widely-used index numbers, body mass indices and air pollution indices, are\nused to explore the properties of and interrelationships among meaningfulness,\nusefulness, and legitimacy.",
        "This paper investigates the effects of saturated thermal conduction (TC) and\nthermal-driven winds (TDWs) on magnetized advection-dominated accretion onto a\nrotating black hole (BH). We incorporate dissipative processes in the\nmagnetized accretion flow and expect the accretion disk to be threaded by\npredominantly toroidal and turbulent magnetic fields. We solve the\nmagnetohydrodynamics equations and construct a self-consistent steady model of\nthe magnetized accretion flow surrounding a rotating BH, which includes TC and\nTDWs. We seek global accretion solutions spanning from the BH horizon to a\nlarge distance and analyze the solution's characteristics as a function of\ndissipation parameters. Accretion solutions with multiple critical points may\nexhibit shock waves if they meet the standing shock criteria. We found steady,\nglobal transonic, and shocked accretion solutions around the rotating BH. In\nparticular, the wind parameter ($m$) and the saturated conduction parameter\n($\\Phi_{\\rm s}$) significantly influence the dynamical behavior of shocks. The\nshock location moves away from the BH horizon as $\\Phi_{\\rm s}$ and $m$\nincrease, assuming fixed conditions at the disk's outer edge. Our formalism\nexplains the declining phase of BH outbursts, characterized by a monotonic\ndecrease in QPO frequency as the burst decays. Based on our findings, we\nconclude that the combined effect of $\\Phi_{\\rm s}$ and $m$ parameters\nsubstantially alters the steady shock specific energy vs angular momentum\nparameter space and also modifies the corresponding post-shock luminosity vs\nQPO frequency parameter space. We propose, based on our theoretical model, that\nthe $\\Phi_{\\rm s}$ and $m$ parameters may significantly influence the evolution\nof the BH outbursts.",
        "Coordination of day-ahead and real-time electricity markets is imperative for\ncost-effective electricity supply and also to provide efficient incentives for\nthe energy transition. Although stochastic market designs feature the\nleast-cost coordination, they are incompatible with current deterministic\nmarkets. This paper proposes a new approach for compatible coordination in\ntwo-settlement markets based on benchmark bidding curves for variable renewable\nenergy. These curves are optimized based on a bilevel optimization problem,\nanticipating per-scenario responses of deterministic market-clearing problems\nand ultimately minimizing the expected cost across day-ahead and real-time\nmarkets. Although the general bilevel model is challenging to solve, we\ntheoretically prove that a single-segment bidding curve with a zero bidding\nprice is sufficient to achieve system optimality if the marginal cost of\nvariable renewable energy is zero, thus addressing the computational challenge.\nIn practice, variable renewable energy producers can be allowed to bid\nmulti-segment curves with non-zero prices. We test the bilevel framework for\nboth single- and multiple-segment bidding curves under the assumption of fixed\nbidding prices. We leverage duality theory and McCormick envelopes to derive\nthe linear programming approximation of the bilevel problem, which scales to\npractical systems such as a 1576-bus NYISO system. We benchmark the proposed\ncoordination and find absolute dominance over the baseline solution, which\nassumes that renewables agnostically bid their expected forecasts. We also\ndemonstrate that our proposed scheme provides a good approximation of the\nleast-cost, yet unattainable in practice, stochastic market outcome.",
        "We calculate the three-loop contribution to the chiral effective potential in\nthe massless Wess-Zumino model. It is shown that while the non-renormalisation\ntheorem forbids divergent contributions to the chiral potential, in the\nmassless case the finite corrections survive. There are only three three-loop\nsupergraphs that give rise to a superfield effective action in the pure chiral\nsector. Two of them are UV finite while the third requires one-loop counterterm\ncorresponding to the chiral field renormalisation.",
        "We prove a splitting theorem for a smooth noncompact manifold with (possibly\nnoncompact) boundary. We show that if a noncompact manifold of dimension $n\\geq\n2$ has $\\lambda_1(-\\alpha\\Delta+\\operatorname{Ric})\\geq 0$ for some\n$\\alpha<\\frac{4}{n-1}$ and mean-convex boundary, then it is either isometric to\n$\\Sigma\\times \\mathbb{R}_{\\geq 0}$ for a closed manifold $\\Sigma$ with\nnonnegative Ricci curvature or it has no interior ends.",
        "Thermoelectric (TE) materials can directly convert heat into electrical\nenergy. However, they sustain costly production procedures and batch-to-batch\nperformance variations. Therefore, developing scalable synthetic techniques for\nlarge-scale and reproducible quality TE materials is critical for advancing TE\ntechnology. This study developed a facile, high throughput, solution-chemical\nsynthetic technique. Microwave-assisted thermolysis process, providing\nenergy-efficient volumetric heating, was used for the synthesis of bismuth and\nantimony telluride (Bi2Te3, Sb2Te3). As-made materials were characterized using\nvarious techniques, including XRPD, SEM, TEM, XAS, and XPS. Detailed\ninvestigation of the local atomic structure of the synthesized Bi2Te3 and\nSb2Te3 powder samples was conducted through synchrotron radiation XAS\nexperiments. The sintered TE materials exhibited low thermal conductivity,\nachieving the highest TE figure-of-merit values of 0.7 (573 K) and 0.9 (523 K)\nfor n-type Bi2Te3 and p-type Sb2Te3, respectively, shifted significantly to the\nhigh-temperature region when compared to earlier reports, highlighting their\npotential for power generation applications. The scalable, energyand\ntime-efficient synthetic method developed, along with the demonstration of its\npotential for TE materials, opens the door for a wider application of these\nmaterials with minimal environmental impact.",
        "Spin-1\/2 Heisenberg antiferromagnetic frustrated spin chain systems display\nexotic ground states with unconventional excitations and distinct quantum phase\ntransitions as the ratio of next-nearest-neighbor to nearest-neighbor coupling\nis tuned. We present a comprehensive investigation of the structural, magnetic,\nand thermodynamics properties of the spin-1\/2 compound, Cu(Ampy)ClBr (Ampy=\nC6H8N2 = 2-(Aminomethyl)pyridine) via x-ray diffraction, magnetization,\nspecific heat, 1H nuclear magnetic resonance (NMR), electron spin resonance\n(ESR), and muon spin relaxation (muSR) techniques. The crystal structure\nfeatures an anisotropic triangular chain lattice of magnetic Cu2+ ions. Our\nbulk and local probe experiments detect neither long-range magnetic ordering\nnor spin freezing down to 0.06 K despite the presence of moderate\nantiferromagnetic interaction between Cu2+ spins as reflected by a Curie-Weiss\ntemperature of about -9 K from the bulk susceptibility data. A broad maximum is\nobserved at about 9 K in magnetic susceptibility and specific heat data,\nindicating the onset of short-range spin correlations. At low temperatures, the\nzero-field magnetic specific heat and the 1H NMR spin-lattice relaxation rate\nfollow an exponential temperature dependence, indicating the presence of gapped\nmagnetic excitations. Furthermore, persistent spin dynamics down to 0.088 K\nobserved by zero-field muSR evidences lack of any static magnetism. We\nattribute these experimental results to the stabilization of a dimer-singlet\nphase in the presence of a next-near neighbor interaction and of a randomness\nin the exchange coupling driven by Cl\/Br mixing.",
        "The rapid growth of wearable technology demands sustainable, flexible, and\nlightweight energy sources for various applications ranging from health\nmonitoring to electronic textiles. Although wearable devices based on the\npiezoelectric effect are widespread, achieving simultaneous breathability,\nwaterproof, and enhanced piezoelectric performance remains challenging. Herein,\nthis study aims to develop a piezoelectric nanogenerator (PENG) using ZnO\nnanofillers in two morphologies (nanoparticles and nanorods), with a carbon\ncoating (ZnO@C) core-cell structure to enhance piezoelectric performance.\nElectrospinning technique was employed to fabricate a lightweight, breathable,\nand water-resistant ZnO@C\/PVDF membrane, enabling in situ electrical poling and\nmechanical stretching to enhance electroactive \\b{eta}-phase formation and thus\nimprove piezoelectric performance. A maximum power density of 384.83 {\\mu}W\/cm3\nwas obtained at RL = 104 k{\\Omega}, with a maximum Vout = 19.9 V for ZnO@C\nnanorod-incorporated PVDF samples. The results demonstrate that ZnO@C nanorods\nexhibit superior voltage output due to their larger surface-to-volume ratio,\nleading to enhanced interaction with PVDF chains compared to nanoparticles. The\nfabricated membrane showed promising results with a water vapor transmission\nrate (WVTR) of ~0.5 kg\/m2\/day, indicating excellent breathability, and a water\ncontact angle of ~116{\\deg}, demonstrating significant waterproofness. These\nfindings highlight the potential of the ZnO@C\/PVDF electrospun membrane as an\neffective piezoelectric nanogenerator and energy harvester for wearable\napplications.",
        "Quantum systems with strong long-range interactions are thought to resist\nthermalization because of their discrete energy spectra. We show that applying\na staggered magnetic field to a strong long-range Heisenberg antiferromagnet\nrestores thermalization for a large class of initial states by breaking\npermutational symmetry. Using self-consistent mean-field theory and exact\ndiagonalization, we reveal that the energy spectrum, while composed of discrete\nsubspaces, collectively forms a dense spectrum. The equilibration time is\nindependent of system size and depends only on the fluctuations in the initial\nstate. For initial states at low to intermediate energies, the long-time\naverage aligns with the microcanonical ensemble. However, for states in the\nmiddle of the spectrum the long-time average depends on the initial state due\nto quantum scar-like eigenstates localized at unstable points in classical\nphase space. Our results can be readily tested on a range of experimental\nplatforms, including Rydberg atoms or optical cavities.",
        "We consider a generalization of a functional equation that models the\nlearning process in various animal species. The equation can be considered\nnonlocal, as it is built with a convex combination of the unknown function\nevaluated at mixed arguments. This makes the equation contain two terms with\nvanishing delays. We prove the existence and uniqueness of the solution in the\nH\\\"older space which is a natural function space to consider. In the second\npart of the paper, we devise an efficient numerical collocation method used to\nfind an approximation to the main problem. We prove the convergence of the\nscheme and, in passing, several properties of the linear interpolation operator\nacting on the H\\\"older space. Numerical simulations verify that the order of\nconvergence of the method (measured in the supremum norm) is equal to the order\nof H\\\"older continuity.",
        "Recent advances in symbolic music generation primarily rely on deep learning\nmodels such as Transformers, GANs, and diffusion models. While these approaches\nachieve high-quality results, they require substantial computational resources,\nlimiting their scalability. We introduce LZMidi, a lightweight symbolic music\ngeneration framework based on a Lempel-Ziv (LZ78)-induced sequential\nprobability assignment (SPA). By leveraging the discrete and sequential\nstructure of MIDI data, our approach enables efficient music generation on\nstandard CPUs with minimal training and inference costs. Theoretically, we\nestablish universal convergence guarantees for our approach, underscoring its\nreliability and robustness. Compared to state-of-the-art diffusion models,\nLZMidi achieves competitive Frechet Audio Distance (FAD), Wasserstein Distance\n(WD), and Kullback-Leibler (KL) scores, while significantly reducing\ncomputational overhead - up to 30x faster training and 300x faster generation.\nOur results position LZMidi as a significant advancement in compression-based\nlearning, highlighting how universal compression techniques can efficiently\nmodel and generate structured sequential data, such as symbolic music, with\npractical scalability and theoretical rigor.",
        "Exploring and manipulating the orbital degrees of freedom in solids has\nbecome a fascinating research topic in modern magnetism. Here, we demonstrate\nthat spin waves can provide a way to control electronic orbital magnetism by\nthe mechanism of scalar spin chirality, allowing for experimental detection\nusing techniques such as the magneto-optical Kerr effect and scanning\ntransmission electron microscopy. By applying linear spin wave theory, we\nuncover that electronic magnon-driven orbital magnetization is extremely\nsensitive to the character of the magnonic excitations. Furthermore, we show\nthat both the induced electronic orbital magnetism and the Nernst transport\nproperties of the orbital angular momentum can be regulated by the strength of\nthe Dzyaloshinskii-Moriya interaction, Kitaev interaction, as well as the\ndirection and magnitude of the external magnetic field. We argue that\nmagnon-mediated electronic orbital magnetism presents an emergent variable\nwhich has to be taken into account when considering the physics of coupling\nmagnonic excitiations to phonons and light.",
        "This paper proposes and analyzes a class of essentially non-oscillatory\ncentral discontinuous Galerkin (CDG) methods for general hyperbolic\nconservation laws. First, we introduce a novel compact, non-oscillatory\nstabilization mechanism that effectively suppresses spurious oscillations while\npreserving the high-order accuracy of CDG methods. Unlike existing\nlimiter-based approaches that rely on large stencils or problem-specific\nparameters for oscillation control, our dual damping mechanism is inspired by\nCDG-based numerical dissipation and leverages overlapping solutions within the\nCDG framework, significantly enhancing stability while maintaining compactness.\nOur approach is free of problem-dependent parameters and complex characteristic\ndecomposition, making it efficient and robust. Second, we provide a rigorous\nstability and optimal error analysis for fully discrete Runge-Kutta (RK) CDG\nschemes, addressing a gap in the theoretical understanding of these methods.\nSpecifically, we establish the approximate skew-symmetry and weak boundedness\nof the CDG discretization. These results enable us to rigorously analyze the\nfully discrete error estimates for our oscillation-eliminating CDG (OECDG)\nmethod, a challenging task due to its nonlinear nature, even for linear\nadvection equations. Building on this framework, we reformulate nonlinear\noscillation-eliminating CDG schemes as linear RK CDG schemes with a nonlinear\nsource term, extending error estimates beyond the linear case to schemes with\nnonlinear oscillation control. While existing error analyses for DG or CDG\nschemes have largely been restricted to linear cases without nonlinear\noscillation-control techniques, our analysis represents an important\ntheoretical advancement. Experiments validate the theoretical findings and\ndemonstrate the effectiveness of the OECDG method."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b26",
    "start_title":"Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble",
    "start_abstract":"Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Quantum machine learning in chemistry and materials"
      ],
      "abstract":[
        "Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models."
      ],
      "categories":[
        "cs.ET"
      ]
    },
    "list":{
      "title":[
        "6KSFx Synth Dataset",
        "Optimized detection of cyber-attacks on IoT networks via hybrid deep\n  learning models",
        "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects",
        "Superficial Self-Improved Reasoners Benefit from Model Merging",
        "SpeHeatal: A Cluster-Enhanced Segmentation Method for Sperm Morphology\n  Analysis",
        "Global well-posedness of the defocusing nonlinear wave equation outside\n  of a ball with radial data for $3<p<5$",
        "Improving Discriminator Guidance in Diffusion Models",
        "From Occasional to Steady: Habit Formation Insights From a Comprehensive\n  Fitness Study",
        "HEATS: A Hierarchical Framework for Efficient Autonomous Target Search\n  with Mobile Manipulators",
        "Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement\n  Learning: Design and Experiment",
        "Learning to reset in target search problems",
        "Analysis and Optimization of Robustness in Multiplex Flow Networks\n  Against Cascading Failures",
        "Adapting Beyond the Depth Limit: Counter Strategies in Large Imperfect\n  Information Games",
        "New Dataset and Methods for Fine-Grained Compositional Referring\n  Expression Comprehension via Specialist-MLLM Collaboration",
        "FUNU: Boosting Machine Unlearning Efficiency by Filtering Unnecessary\n  Unlearning",
        "Finite Sample Identification of Partially Observed Bilinear Dynamical\n  Systems",
        "Data collaboration for causal inference from limited medical testing and\n  medication data",
        "Knudsen boundary layer equations with incoming boundary condition: full\n  range of cutoff collision kernels and Mach numbers of the far field",
        "Fast computation of the TGOSPA metric for multiple target tracking via\n  unbalanced optimal transport",
        "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in\n  Speech Dialogue Systems",
        "FeNeC: Enhancing Continual Learning via Feature Clustering with\n  Neighbor- or Logit-Based Classification",
        "Training Medical Large Vision-Language Models with Abnormal-Aware\n  Feedback",
        "The Pitfalls of Imitation Learning when Actions are Continuous",
        "A four-term exact sequence of fundamental groups of orbit configuration\n  spaces",
        "Exact Schwinger functions for a class of bounded interactions in $d\\geq\n  2$",
        "A threshold for Poisson behavior of non-stationary product measures",
        "ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local\n  Planner for Unstructured and Dynamic Environments",
        "On Two Parameter Time-Changed Poisson Random Fields with Drifts",
        "Efficient Knowledge Feeding to Language Models: A Novel Integrated\n  Encoder-Decoder Architecture"
      ],
      "abstract":[
        "Procedural audio, often referred to as \"digital Foley\", generates sound from\nscratch using computational processes. It represents an innovative approach to\nsound-effects creation. However, the development and adoption of procedural\naudio has been constrained by a lack of publicly available datasets and models,\nwhich hinders evaluation and optimization. To address this important gap, this\npaper presents a dataset of 6000 synthetic audio samples specifically designed\nto advance research and development in sound synthesis within 30 sound\ncategories. By offering a description of the diverse synthesis methods used in\neach sound category and supporting the creation of robust evaluation\nframeworks, this dataset not only highlights the potential of procedural audio,\nbut also provides a resource for researchers, audio developers, and sound\ndesigners. This contribution can accelerate the progress of procedural audio,\nopening up new possibilities in digital sound design.",
        "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies.",
        "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules.",
        "As scaled language models (LMs) approach human-level reasoning capabilities,\nself-improvement emerges as a solution to synthesizing high-quality data\ncorpus. While previous research has identified model collapse as a risk in\nself-improvement, where model outputs become increasingly deterministic, we\ndiscover a more fundamental challenge: the superficial self-improved reasoners\nphenomenon. In particular, our analysis reveals that even when LMs show\nimproved in-domain (ID) reasoning accuracy, they actually compromise their\ngeneralized reasoning capabilities on out-of-domain (OOD) tasks due to\nmemorization rather than genuine. Through a systematic investigation of LM\narchitecture, we discover that during self-improvement, LM weight updates are\nconcentrated in less reasoning-critical layers, leading to superficial\nlearning. To address this, we propose Iterative Model Merging (IMM), a method\nthat strategically combines weights from original and self-improved models to\npreserve generalization while incorporating genuine reasoning improvements. Our\napproach effectively mitigates both LM collapse and superficial learning,\nmoving towards more stable self-improving systems.",
        "The accurate assessment of sperm morphology is crucial in andrological\ndiagnostics, where the segmentation of sperm images presents significant\nchallenges. Existing approaches frequently rely on large annotated datasets and\noften struggle with the segmentation of overlapping sperm and the presence of\ndye impurities. To address these challenges, this paper first analyzes the\nissue of overlapping sperm tails from a geometric perspective and introduces a\nnovel clustering algorithm, Con2Dis, which effectively segments overlapping\ntails by considering three essential factors: CONnectivity, CONformity, and\nDIStance. Building on this foundation, we propose an unsupervised method,\nSpeHeatal, designed for the comprehensive segmentation of the SPErm HEAd and\nTAiL. SpeHeatal employs the Segment Anything Model(SAM) to generate masks for\nsperm heads while filtering out dye impurities, utilizes Con2Dis to segment\ntails, and then applies a tailored mask splicing technique to produce complete\nsperm masks. Experimental results underscore the superior performance of\nSpeHeatal, particularly in handling images with overlapping sperm.",
        "We continue the study of the Dirichlet boundary value problem of nonlinear\nwave equation with radial data in the exterior $\\Omega = \\mathbb{R}^3\\backslash\n\\bar{B}(0,1)$. We combine the distorted Fourier truncation method in\n\\cite{Bourgain98:FTM}, the global-in-time (endpoint) Strichartz estimates in\n\\cite{XuYang:NLW} with the energy method in \\cite{GallPlan03:NLW} to prove the\nglobal well-posedness of the radial solution to the defocusing,\nenergy-subcriticial nonlinear wave equation outside of a ball in $\\left(\\dot\nH^{s}_{D}(\\Omega) \\cap L^{p+1}(\\Omega) \\right)\\times \\dot H^{s-1}_{D}(\\Omega)$\nwith $1-\\frac{(p+3)(1-s_c)}{4(2p-3)}<s<1$, $s_c=\\frac{3}{2}-\\frac{2}{p-1} $,\nwhich extends the result for the cubic nonlinearity in \\cite{XuYang:NLW} to the\ncase $3<p<5$. Except from the argument in \\cite{XuYang:NLW}, another new\ningredient is that we need make use of the radial Sobolev inequality to deal\nwith the super-conformal nonlinearity in addition to the Sobolev inequality.",
        "Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.",
        "Exercising regularly is widely recognized as a cornerstone of health, yet the\nchallenge of sustaining consistent exercise habits persists. Understanding the\nfactors that influence the formation of these habits is crucial for developing\neffective interventions. This study utilizes data from Mars Athletic Club,\nT\\\"urkiye's largest sports chain, to investigate the dynamics of gym attendance\nand habit formation. The general problem addressed by this study is identifying\nthe critical periods and factors that contribute to the successful\nestablishment of consistent exercise routines among gym-goers. Here we show\nthat there are specific periods during which gym attendance is most crucial for\nhabit formation. By developing a survival metric based on gym attendance\npatterns, we pinpoint these critical periods and segment members into distinct\nclusters based on their visit patterns. Our analysis reveals significant\ndifferences in how various subgroups respond to interventions, such as group\nclasses, personal trainer sessions, and visiting different clubs. Using causal\ninference analysis, we demonstrate that personalized guidance and social\ndynamics are key drivers of sustained long-term engagement. By systematically\nexamining these variables and considering the specific characteristics of\ndifferent clusters, our research demonstrates the importance of a tailored,\nmulti-dimensional approach to promoting exercise habits, which integrates\nsocial dynamics, personalized guidance, and strategic interventions to sustain\nlong-term engagement.",
        "Utilizing robots for autonomous target search in complex and unknown\nenvironments can greatly improve the efficiency of search and rescue missions.\nHowever, existing methods have shown inadequate performance due to hardware\nplatform limitations, inefficient viewpoint selection strategies, and\nconservative motion planning. In this work, we propose HEATS, which enhances\nthe search capability of mobile manipulators in complex and unknown\nenvironments. We design a target viewpoint planner tailored to the strengths of\nmobile manipulators, ensuring efficient and comprehensive viewpoint planning.\nSupported by this, a whole-body motion planner integrates global path search\nwith local IPC optimization, enabling the mobile manipulator to safely and\nagilely visit target viewpoints, significantly improving search performance. We\npresent extensive simulated and real-world tests, in which our method\ndemonstrates reduced search time, higher target search completeness, and lower\nmovement cost compared to classic and state-of-the-art approaches. Our method\nwill be open-sourced for community benefit.",
        "This paper addresses the multi-robot pursuit problem for an unknown target,\nencompassing both target state estimation and pursuit control. First, in state\nestimation, we focus on using only bearing information, as it is readily\navailable from vision sensors and effective for small, distant targets.\nChallenges such as instability due to the nonlinearity of bearing measurements\nand singularities in the two-angle representation are addressed through a\nproposed uniform bearing-only information filter. This filter integrates\nmultiple 3D bearing measurements, provides a concise formulation, and enhances\nstability and resilience to target loss caused by limited field of view (FoV).\nSecond, in target pursuit control within complex environments, where challenges\nsuch as heterogeneity and limited FoV arise, conventional methods like\ndifferential games or Voronoi partitioning often prove inadequate. To address\nthese limitations, we propose a novel multiagent reinforcement learning (MARL)\nframework, enabling multiple heterogeneous vehicles to search, localize, and\nfollow a target while effectively handling those challenges. Third, to bridge\nthe sim-to-real gap, we propose two key techniques: incorporating adjustable\nlow-level control gains in training to replicate the dynamics of real-world\nautonomous ground vehicles (AGVs), and proposing spectral-normalized RL\nalgorithms to enhance policy smoothness and robustness. Finally, we demonstrate\nthe successful zero-shot transfer of the MARL controllers to AGVs, validating\nthe effectiveness and practical feasibility of our approach. The accompanying\nvideo is available at https:\/\/youtu.be\/HO7FJyZiJ3E.",
        "Target search problems are central to a wide range of fields, from biological\nforaging to the optimization algorithms. Recently, the ability to reset the\nsearch has been shown to significantly improve the searcher's efficiency.\nHowever, the optimal resetting strategy depends on the specific properties of\nthe search problem and can often be challenging to determine. In this work, we\npropose a reinforcement learning (RL)-based framework to train agents capable\nof optimizing their search efficiency in environments by learning how to reset.\nFirst, we validate the approach in a well-established benchmark: the Brownian\nsearch with resetting. There, RL agents consistently recover strategies closely\nresembling the sharp resetting distribution, known to be optimal in this\nscenario. We then extend the framework by allowing agents to control not only\nwhen to reset, but also their spatial dynamics through turning actions. In this\nmore complex setting, the agents discover strategies that adapt both resetting\nand turning to the properties of the environment, outperforming the proposed\nbenchmarks. These results demonstrate how reinforcement learning can serve both\nas an optimization tool and a mechanism for uncovering new, interpretable\nstrategies in stochastic search processes with resetting.",
        "Networked systems are susceptible to cascading failures, where the failure of\nan initial set of nodes propagates through the network, often leading to\nsystem-wide failures. In this work, we propose a multiplex flow network model\nto study robustness against cascading failures triggered by random failures.\nThe model is inspired by systems where nodes carry or support multiple types of\nflows, and failures result in the redistribution of flows within the same layer\nrather than between layers. To represent different types of interdependencies\nbetween the layers of the multiplex network, we define two cases of failure\nconditions: layer-independent overload and layer-influenced overload. We\nprovide recursive equations and their solutions to calculate the steady-state\nfraction of surviving nodes, validate them through a set of simulation\nexperiments, and discuss optimal load-capacity allocation strategies. Our\nresults demonstrate that allocating the total excess capacity to each layer\nproportional to the mean effective load in the layer and distributing that\nexcess capacity equally among the nodes within the layer ensures maximum\nrobustness. The proposed framework for different failure conditions allows us\nto analyze the two overload conditions presented and can be extended to explore\nmore complex interdependent relationships.",
        "We study the problem of adapting to a known sub-rational opponent during\nonline play while remaining robust to rational opponents. We focus on large\nimperfect-information (zero-sum) games, which makes it impossible to inspect\nthe whole game tree at once and necessitates the use of depth-limited search.\nHowever, all existing methods assume rational play beyond the depth-limit,\nwhich only allows them to adapt a very limited portion of the opponent's\nbehaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses\na strategy-portfolio approach - which we refer to as matrix-valued states - for\ndepth-limited search. This allows the algorithm to fully utilise all\ninformation about the opponent model, making it the first robust-adaptation\nmethod to be able to do so in large imperfect-information games. As an\nadditional benefit, the use of matrix-valued states makes the algorithm simpler\nthan traditional methods based on optimal value functions. Our experimental\nresults in poker and battleship show that ABD yields more than a twofold\nincrease in utility when facing opponents who make mistakes beyond the depth\nlimit and also delivers significant improvements in utility and safety against\nrandomly generated opponents.",
        "Referring Expression Comprehension (REC) is a foundational cross-modal task\nthat evaluates the interplay of language understanding, image comprehension,\nand language-to-image grounding. To advance this field, we introduce a new REC\ndataset with two key features. First, it is designed with controllable\ndifficulty levels, requiring fine-grained reasoning across object categories,\nattributes, and relationships. Second, it incorporates negative text and images\ngenerated through fine-grained editing, explicitly testing a model's ability to\nreject non-existent targets, an often-overlooked yet critical challenge in\nexisting datasets. To address fine-grained compositional REC, we propose novel\nmethods based on a Specialist-MLLM collaboration framework, leveraging the\ncomplementary strengths of them: Specialist Models handle simpler tasks\nefficiently, while MLLMs are better suited for complex reasoning. Based on this\nsynergy, we introduce two collaborative strategies. The first, Slow-Fast\nAdaptation (SFA), employs a routing mechanism to adaptively delegate simple\ntasks to Specialist Models and complex tasks to MLLMs. Additionally, common\nerror patterns in both models are mitigated through a target-refocus strategy.\nThe second, Candidate Region Selection (CRS), generates multiple bounding box\ncandidates based on Specialist Model and uses the advanced reasoning\ncapabilities of MLLMs to identify the correct target. Extensive experiments on\nour dataset and other challenging compositional benchmarks validate the\neffectiveness of our approaches. The SFA strategy achieves a trade-off between\nlocalization accuracy and efficiency, and the CRS strategy greatly boosts the\nperformance of both Specialist Models and MLLMs. We aim for this work to offer\nvaluable insights into solving complex real-world tasks by strategically\ncombining existing tools for maximum effectiveness, rather than reinventing\nthem.",
        "Machine unlearning is an emerging field that selectively removes specific\ndata samples from a trained model. This capability is crucial for addressing\nprivacy concerns, complying with data protection regulations, and correcting\nerrors or biases introduced by certain data. Unlike traditional machine\nlearning, where models are typically static once trained, machine unlearning\nfacilitates dynamic updates that enable the model to ``forget'' information\nwithout requiring complete retraining from scratch. There are various machine\nunlearning methods, some of which are more time-efficient when data removal\nrequests are fewer.\n  To decrease the execution time of such machine unlearning methods, we aim to\nreduce the size of data removal requests based on the fundamental assumption\nthat the removal of certain data would not result in a distinguishable\nretrained model. We first propose the concept of unnecessary unlearning, which\nindicates that the model would not alter noticeably after removing some data\npoints. Subsequently, we review existing solutions that can be used to solve\nour problem. We highlight their limitations in adaptability to different\nunlearning scenarios and their reliance on manually selected parameters. We\nconsequently put forward FUNU, a method to identify data points that lead to\nunnecessary unlearning. FUNU circumvents the limitations of existing solutions.\nThe idea is to discover data points within the removal requests that have\nsimilar neighbors in the remaining dataset. We utilize a reference model to set\nparameters for finding neighbors, inspired from the area of model memorization.\nWe provide a theoretical analysis of the privacy guarantee offered by FUNU and\nconduct extensive experiments to validate its efficacy.",
        "We consider the problem of learning a realization of a partially observed\nbilinear dynamical system (BLDS) from noisy input-output data. Given a single\ntrajectory of input-output samples, we provide a finite time analysis for\nlearning the system's Markov-like parameters, from which a balanced realization\nof the bilinear system can be obtained. Our bilinear system identification\nalgorithm learns the system's Markov-like parameters by regressing the outputs\nto highly correlated, nonlinear, and heavy-tailed covariates. Moreover, the\nstability of BLDS depends on the sequence of inputs used to excite the system.\nThese properties, unique to partially observed bilinear dynamical systems, pose\nsignificant challenges to the analysis of our algorithm for learning the\nunknown dynamics. We address these challenges and provide high probability\nerror bounds on our identification algorithm under a uniform stability\nassumption. Our analysis provides insights into system theoretic quantities\nthat affect learning accuracy and sample complexity. Lastly, we perform\nnumerical experiments with synthetic data to reinforce these insights.",
        "Observational studies enable causal inferences when randomized controlled\ntrials (RCTs) are not feasible. However, integrating sensitive medical data\nacross multiple institutions introduces significant privacy challenges. The\ndata collaboration quasi-experiment (DC-QE) framework addresses these concerns\nby sharing \"intermediate representations\" -- dimensionality-reduced data\nderived from raw data -- instead of the raw data. While the DC-QE can estimate\ntreatment effects, its application to medical data remains unexplored. This\nstudy applied the DC-QE framework to medical data from a single institution to\nsimulate distributed data environments under independent and identically\ndistributed (IID) and non-IID conditions. We propose a novel method for\ngenerating intermediate representations within the DC-QE framework.\nExperimental results demonstrated that DC-QE consistently outperformed\nindividual analyses across various accuracy metrics, closely approximating the\nperformance of centralized analysis. The proposed method further improved\nperformance, particularly under non-IID conditions. These outcomes highlight\nthe potential of the DC-QE framework as a robust approach for\nprivacy-preserving causal inferences in healthcare. Broader adoption of this\nframework and increased use of intermediate representations could grant\nresearchers access to larger, more diverse datasets while safeguarding patient\nconfidentiality. This approach may ultimately aid in identifying previously\nunrecognized causal relationships, support drug repurposing efforts, and\nenhance therapeutic interventions for rare diseases.",
        "This paper establishes tahe existence and uniqueness of the nonlinear Knudsen\nlayer equation with incoming boundary conditions. It is well-known that the\nsolvability conditions of the problem vary with the Mach number of the far\nMaxwellian $\\mathcal{M}^\\infty$. We consider full ranges of cutoff collision\nkernels (i.e., $- 3 < \\gamma \\leq 1$) and all the Mach numbers of the far field\nin the $L^\\infty_{x,v}$ framework. Additionally, the solution exhibits\nexponential decay $\\exp \\{- c x^\\frac{2}{3 - \\gamma} - c |v|^2 \\}$ for some $c\n> 0$. To address the general angular cutoff collision kernel, we introduce a\n$(x,v)$-mixed weight $\\sigma$. The proof is essentially bsed on adding an\nartificial damping term.",
        "In multiple target tracking, it is important to be able to evaluate the\nperformance of different tracking algorithms. The trajectory generalized\noptimal sub-pattern assignment metric (TGOSPA) is a recently proposed metric\nfor such evaluations. The TGOSPA metric is computed as the solution to an\noptimization problem, but for large tracking scenarios, solving this problem\nbecomes computationally demanding. In this paper, we present an approximation\nalgorithm for evaluating the TGOSPA metric, based on casting the TGOSPA problem\nas an unbalanced multimarginal optimal transport problem. Following recent\nadvances in computational optimal transport, we introduce an entropy\nregularization and derive an iterative scheme for solving the Lagrangian dual\nof the regularized problem. Numerical results suggest that our proposed\nalgorithm is more computationally efficient than the alternative of computing\nthe exact metric using a linear programming solver, while still providing an\nadequate approximation of the metric.",
        "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly\nenhanced the naturalness of human-machine interaction by enabling real-time\nbidirectional communication. However, existing approaches face challenges such\nas difficulties in independent module optimization and contextual noise\ninterference due to highly coupled architectural designs and oversimplified\nbinary state modeling. This paper proposes FlexDuo, a flexible full-duplex\ncontrol module that decouples duplex control from spoken dialogue systems\nthrough a plug-and-play architectural design. Furthermore, inspired by human\ninformation-filtering mechanisms in conversations, we introduce an explicit\nIdle state. On one hand, the Idle state filters redundant noise and irrelevant\naudio to enhance dialogue quality. On the other hand, it establishes a semantic\nintegrity-based buffering mechanism, reducing the risk of mutual interruptions\nwhile ensuring accurate response transitions. Experimental results on the\nFisher corpus demonstrate that FlexDuo reduces the false interruption rate by\n24.9% and improves response accuracy by 7.6% compared to integrated full-duplex\ndialogue system baselines. It also outperforms voice activity detection (VAD)\ncontrolled baseline systems in both Chinese and English dialogue quality. The\nproposed modular architecture and state-based dialogue model provide a novel\ntechnical pathway for building flexible and efficient duplex dialogue systems.",
        "The ability of deep learning models to learn continuously is essential for\nadapting to new data categories and evolving data distributions. In recent\nyears, approaches leveraging frozen feature extractors after an initial\nlearning phase have been extensively studied. Many of these methods estimate\nper-class covariance matrices and prototypes based on backbone-derived feature\nrepresentations. Within this paradigm, we introduce FeNeC (Feature Neighborhood\nClassifier) and FeNeC-Log, its variant based on the log-likelihood function.\nOur approach generalizes the existing concept by incorporating data clustering\nto capture greater intra-class variability. Utilizing the Mahalanobis distance,\nour models classify samples either through a nearest neighbor approach or\ntrainable logit values assigned to consecutive classes. Our proposition may be\nreduced to the existing approaches in a special case while extending them with\nthe ability of more flexible adaptation to data. We demonstrate that two FeNeC\nvariants achieve competitive performance in scenarios where task identities are\nunknown and establish state-of-the-art results on several benchmarks.",
        "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images and responding to human queries based on these\nimages. However, there remain challenges in visual localization in medical\nimages, which is crucial for abnormality detection and interpretation. To\naddress these issues, we propose a novel UMed-LVLM designed with Unveiling\nMedical abnormalities. Specifically, we collect a Medical Abnormalities\nUnveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM\ntraining. To collect MAU dataset, we propose a prompt method utilizing the\nGPT-4V to generate diagnoses based on identified abnormal areas in medical\nimages. Moreover, the two-stage training method includes Abnormal-Aware\nInstruction Tuning and Abnormal-Aware Rewarding, comprising Abnormal\nLocalization Rewarding and Vision Relevance Rewarding. Experimental results\ndemonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying and\nunderstanding medical abnormality. In addition, this work shows that enhancing\nthe abnormality detection capabilities of Med-LVLMs significantly improves\ntheir understanding of medical images and generalization capability.",
        "We study the problem of imitating an expert demonstrator in a discrete-time,\ncontinuous state-and-action control system. We show that, even if the dynamics\nare stable (i.e. contracting exponentially quickly), and the expert is smooth\nand deterministic, any smooth, deterministic imitator policy necessarily\nsuffers error on execution that is exponentially larger, as a function of\nproblem horizon, than the error under the distribution of expert training data.\nOur negative result applies to both behavior cloning and offline-RL algorithms,\nunless they produce highly \"improper\" imitator policies--those which are\nnon-smooth, non-Markovian, or which exhibit highly state-dependent\nstochasticity--or unless the expert trajectory distribution is sufficiently\n\"spread.\" We provide experimental evidence of the benefits of these more\ncomplex policy parameterizations, explicating the benefits of today's popular\npolicy parameterizations in robot learning (e.g. action-chunking and Diffusion\nPolicies). We also establish a host of complementary negative and positive\nresults for imitation in control systems.",
        "We deduce that the fundamental groups of the orbit configuration spaces of an\neffective and properly discontinuous action of a discrete group on a connected\naspherical 2-manifold, with isolated fixed points, fit into a four-term exact\nsequence. This comes as a consequence of the four-term exact sequence of\norbifold braid groups ([16], [11] and [17]). The proof relates these two exact\nsequences and also draws a new consequence (Corollary 2.3) on the later one.",
        "We consider a scalar Euclidean QFT with interaction given by a bounded,\nmeasurable function $V$ such that $V^{\\pm}:=\\lim_{w\\to \\pm\\infty}V(w)$ exist.\nWe find a field renormalization such that all the $n$-point connected Schwinger\nfunctions for $n\\neq 2$ exist non-perturbatively in the UV limit. They coincide\nwith the tree-level one-particle irreducible Schwinger functions of the\n$\\mathrm{erf}(\\phi\/\\sqrt{2})$ interaction with a coupling constant $\\frac{1}{2}\n(V^+ - V^-)$. By a slight modification of our construction we can change this\ncoupling constant to $\\frac{1}{2} (V_+ - V_-)$, where $V_{\\pm}:= \\lim_{w\\to\n0^{\\pm}} V(w)$. Thereby non-Gaussianity of these latter theories is governed by\na discontinuity of $V$ at zero. The open problem of controlling also the\ntwo-point function of these QFTs is discussed.",
        "Let $\\gamma_{n}= O (\\log^{-c}n)$ and let $\\nu$ be the infinite product\nmeasure whose $n$-th marginal is Bernoulli$(1\/2+\\gamma_{n})$. We show that\n$c=1\/2$ is the threshold, above which $\\nu$-almost every point is simply\nPoisson generic in the sense of Peres-Weiss, and below which this can fail.\nThis provides a range in which $\\nu$ is singular with respect to the uniform\nproduct measure, but $\\nu$-almost every point is simply Poisson generic.",
        "Deep Reinforcement Learning (DRL) has demonstrated potential in addressing\nrobotic local planning problems, yet its efficacy remains constrained in highly\nunstructured and dynamic environments. To address these challenges, this study\nproposes the ColorDynamic framework. First, an end-to-end DRL formulation is\nestablished, which maps raw sensor data directly to control commands, thereby\nensuring compatibility with unstructured environments. Under this formulation,\na novel network, Transqer, is introduced. The Transqer enables online DRL\nlearning from temporal transitions, substantially enhancing decision-making in\ndynamic scenarios. To facilitate scalable training of Transqer with diverse\ndata, an efficient simulation platform E-Sparrow, along with a data\naugmentation technique leveraging symmetric invariance, are developed.\nComparative evaluations against state-of-the-art methods, alongside assessments\nof generalizability, scalability, and real-time performance, were conducted to\nvalidate the effectiveness of ColorDynamic. Results indicate that our approach\nachieves a success rate exceeding 90% while exhibiting real-time capacity\n(1.2-1.3 ms per planning). Additionally, ablation studies were performed to\ncorroborate the contributions of individual components. Building on this, the\nOkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and\nreal-world experiments demonstrating its superiority and applicability in\ncomplex scenarios. The codebase and experimental demonstrations have been\nopen-sourced on our website to facilitate reproducibility and further research.",
        "We study the composition of bivariate L\\'evy process with bivariate inverse\nsubordinator. The explicit expressions for its dispersion and auto correlation\nmatrices are obtained. Also, the time-changed two parameter L\\'evy processes\nwith rectangular increments are studied. We introduce some time-changed\nvariants of the Poisson random field in plane with and without drift, and\nderive the associated fractional differential equations for their\ndistributions. Later, we consider some time-changed L\\'evy processes where the\ntime-changing components are two parameter Poisson random fields with drifts.\nMoreover, two parameter coordinatewise semigroup operators associated with some\nof the introduced processes are discussed.",
        "This paper introduces a novel approach to efficiently feeding knowledge to\nlanguage models (LLMs) during prediction by integrating retrieval and\ngeneration processes within a unified framework. While the Retrieval-Augmented\nGeneration (RAG) model addresses gaps in LLMs' training data and knowledge\nlimits, it is hindered by token limit restrictions and dependency on the\nretrieval system's accuracy. Our proposed architecture incorporates in-context\nvectors (ICV) to overcome these challenges. ICV recasts in-context learning by\nusing latent embeddings of LLMs to create a vector that captures essential task\ninformation. This vector is then used to shift the latent states of the LLM,\nenhancing the generation process without adding demonstration examples to the\nprompt. ICV directly integrates information into the model, enabling it to\nprocess this information more effectively. Our extensive experimental\nevaluation demonstrates that ICV outperforms standard in-context learning and\nfine-tuning across question-answering, information retrieval, and other tasks.\nThis approach mitigates the limitations of current RAG models and offers a more\nrobust solution for handling extensive and diverse datasets. Despite leveraging\na fraction of the parameters, our ICV-enhanced model achieves competitive\nperformance against models like LLaMA-3, Gemma, and Phi-3, significantly\nreducing computational costs and memory requirements. ICV reduces prompt\nlength, is easy to control, surpasses token limitations, and is computationally\nefficient compared to fine-tuning."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Cerebral aneurysms. New Engl. J. Medicine",
    "start_abstract":"Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Data-driven science and engineering: machine learning, dynamical systems, and control"
      ],
      "abstract":[
        "\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Effects of oxidation and impurities in lithium surfaces on the emitting\n  wall plasma sheath",
        "Conservative, pressure-equilibrium-preserving discontinuous Galerkin\n  method for compressible, multicomponent flows",
        "MAISTEP -- a new grid-based machine learning tool for inferring stellar\n  parameters I. Ages of giant-planet host stars",
        "Moment-based Characterization of Spatially Distributed Sources in SAR\n  Tomography",
        "Infinite-temperature thermostats by energy localization in a\n  nonequilibrium setup",
        "A Systematic Evaluation of Generative Models on Tabular Transportation\n  Data",
        "Grey system model on time scales",
        "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing\n  Attacks",
        "A Resilient and Energy-Efficient Smart Metering Infrastructure Utilizing\n  a Self-Organizing UAV Swarm",
        "Single-molecule phosphorescence and intersystem crossing in a coupled\n  exciton-plasmon system",
        "SEAlign: Alignment Training for Software Engineering Agent",
        "Discrete-time weak approximation of a Black-Scholes model with drift and\n  volatility Markov switching",
        "Long-term evolution of Sco X-1: implications for the current spin\n  frequency and ellipticity of the neutron star",
        "Modular and Integrated AI Control Framework across Fiber and Wireless\n  Networks for 6G",
        "Antiferromagnetic and bond-order-wave phases in the half-filled\n  two-dimensional optical Su-Schrieffer-Heeger-Hubbard model",
        "Polarization-controlled strong light-matter interaction with templated\n  molecular aggregates",
        "Performant LLM Agentic Framework for Conversational AI",
        "Efficient Langevin sampling with position-dependent diffusion",
        "Counting Black Hole Microstates at Future Null Infinity",
        "Issues with Neural Tangent Kernel Approach to Neural Networks",
        "Charged Black Holes in the Kalb-Ramond Background with Lorentz\n  Violation: Null Geodesics and Optical Appearance of a Thin Accretion Disk",
        "Electronic band structures of topological kagome materials",
        "Layered Topological Antiferromagnetic Metal at Room Temperature --\n  YbMn$_2$Ge$_2$",
        "Rigid and flexible Wasserstein spaces",
        "Cascading Bandits Robust to Adversarial Corruptions",
        "Uniqueness of Transonic Shock Solutions for Two-Dimensional Steady\n  Compressible Euler Flows in an Expanding Nozzle",
        "Kinematically induced dipole anisotropy in line-emitting galaxy number\n  counts and line intensity maps",
        "Digging into the Interior of Hot Cores with ALMA (DIHCA). V. Deuterium\n  Fractionation of Methanol",
        "Leptogenesis in five-dimensional asymptotic Grand Unification models"
      ],
      "abstract":[
        "Use of lithium as a surface coating in fusion devices improves plasma\nperformance, but the change in wall properties affects the secondary electron\nemission properties of the material. Lithium oxidizes easily, which drives the\nemission yield well above unity. We present here simulations demonstrating the\nchange in sheath structure from monotonic to the nonmonotonic space-charge\nlimited sheath using an energy-dependent data-driven emission model which\nself-consistently captures both secondary emission and backscattering\npopulations. Increased secondary electron emission from the material has\nramifications for the degradation and erosion of the wall. Results shows that\nthe oxidation leads to an increased electron flux into the wall, and a reduced\nion flux. The net transfer of energy to the surface is significantly greater\nfor the oxidized case than for the pure lithium case. High reflection rates of\nlow-energy backscattered particles leads to a high re-emission rate at the\nwall.",
        "This paper concerns preservation of velocity and pressure equilibria in\nsmooth, compressible, multicomponent flows in the inviscid limit. First, we\nderive the velocity-equilibrium and pressure-equilibrium conditions of a\nstandard discontinuous Galerkin method that discretizes the conservative form\nof the compressible, multicomponent Euler equations. We show that under certain\nconstraints on the numerical flux, the scheme is\nvelocity-equilibrium-preserving. However, standard discontinuous Galerkin\nschemes are not pressure-equilibrium-preserving. Therefore, we introduce a\ndiscontinuous Galerkin method that discretizes the pressure-evolution equation\nin place of the total-energy conservation equation. Semidiscrete conservation\nof total energy, which would otherwise be lost, is restored via the correction\nterms of [Abgrall, J. Comput. Phys., 372, 2018, pp. 640-666] and [Abgrall et\nal., J. Comput. Phys., 453, 2022, 110955]. Since the addition of the correction\nterms prevents exact preservation of pressure and velocity equilibria, we\npropose modifications that then lead to a velocity-equilibrium-preserving,\npressure-equilibrium-preserving, and (semidiscretely) energy-conservative\ndiscontinuous Galerkin scheme, although there are certain tradeoffs. Additional\nextensions are also introduced. We apply the developed scheme to smooth,\ninterfacial flows involving mixtures of thermally perfect gases initially in\npressure and velocity equilibria to demonstrate its performance in one, two,\nand three spatial dimensions.",
        "Our understanding of exoplanet demographics partly depends on their\ncorresponding host star parameters. With the majority of exoplanet-host stars\nhaving only atmospheric constraints available, robust inference of their\nparameters is susceptible to the approach used. The goal of this work is to\ndevelop a grid-based machine learning tool capable of determining the stellar\nradius, mass, and age using only atmospheric constraints and to analyse the age\ndistribution of stars hosting giant planets. Our machine learning approach\ninvolves combining four tree-based machine learning algorithms (Random Forest,\nExtra Trees, Extreme Gradient Boosting, and CatBoost) trained on a grid of\nstellar models to infer stellar radius, mass, and age using Teff, [Fe\/H], and\nluminosities. We perform a detailed statistical analysis to compare the\ninferences of our tool with those based on seismic data from the APOKASC and\nLEGACY samples. Finally, we apply our tool to determine the ages of stars\nhosting giant planets. Comparing the stellar parameter inferences from our\nmachine learning tool with those from the APOKASC and LEGACY, we find a bias\n(and a scatter) of -0.5\\% (5\\%) and -0.2\\% (2\\%) in radius, 6\\% (5\\%) per cent\nand -2\\% (3\\%) in mass, and -9\\% (16\\%) and 7\\% (23\\%) in age, respectively.\nTherefore, our machine learning predictions are commensurate with seismic\ninferences. When applying our model to a sample of stars hosting Jupiter-mass\nplanets, we find the average age estimates for the hosts of Hot Jupiters, Warm\nJupiters, and Cold Jupiters to be 1.98, 2.98, and 3.51 Gyr, respectively. These\nstatistical ages of the host stars confirm previous predictions - based on\nstellar model ages for a relatively small number of hosts, as well as on the\naverage age-velocity dispersion relation - that stars hosting Hot Jupiters are\nstatistically younger than those hosting Warm and Cold Jupiters.",
        "This paper presents a non-parametric method for 3-D imaging of natural\nvolumes using Synthetic Aperture Radar tomography. This array processing-based\ntechnique aims at characterizing a spatially distributed density of incoherent\nsources, whose shape is imprecisely known. The proposed technique estimates the\nmoments of the reflectivity density using a low-complexity covariance matching\napproach, and retrieves the mean location, dispersion, and power of the\ndistributed source. Numerical simulations of realistic tomographic scenarios\nshow that the proposed model-free scheme achieves better accuracy than slightly\nmisspecified maximum likelihood estimators, derived from approximately known\ndistribution shapes.",
        "Some lattice models having two conservation laws may display an equilibrium\nphase transition from a homogeneous (positive temperature - PT) to a condensed\n(negative temperature) phase, where a finite fraction of the energy is\nlocalized in a few sites. We study one such stochastic model in an\nout-of-equilibrium setup, where the ends of the lattice chain are attached to\ntwo PT baths. We show that localized peaks may spontaneously emerge, acting as\ninfinite-temperature heat baths. The number $N_b$ of peaks is expected to grow\nin time $t$ as $N_b \\sim \\sqrt{\\ln t}$, as a consequence of an effective\nfreezing of the dynamics. Asymptotically, the chain spontaneously subdivides\ninto three intervals: the two external ones lying inside the PT region; the\nmiddle one characterized by peaks superposed to a background lying along the\ninfinite-temperature line. In the thermodynamic limit, the Onsager formalism\nallows determining the shape of the whole profile.",
        "The sharing of large-scale transportation data is beneficial for\ntransportation planning and policymaking. However, it also raises significant\nsecurity and privacy concerns, as the data may include identifiable personal\ninformation, such as individuals' home locations. To address these concerns,\nsynthetic data generation based on real transportation data offers a promising\nsolution that allows privacy protection while potentially preserving data\nutility. Although there are various synthetic data generation techniques, they\nare often not tailored to the unique characteristics of transportation data,\nsuch as the inherent structure of transportation networks formed by all trips\nin the datasets. In this paper, we use New York City taxi data as a case study\nto conduct a systematic evaluation of the performance of widely used tabular\ndata generative models. In addition to traditional metrics such as distribution\nsimilarity, coverage, and privacy preservation, we propose a novel graph-based\nmetric tailored specifically for transportation data. This metric evaluates the\nsimilarity between real and synthetic transportation networks, providing\npotentially deeper insights into their structural and functional alignment. We\nalso introduced an improved privacy metric to address the limitations of the\ncommonly-used one. Our experimental results reveal that existing tabular data\ngenerative models often fail to perform as consistently as claimed in the\nliterature, particularly when applied to transportation data use cases.\nFurthermore, our novel graph metric reveals a significant gap between synthetic\nand real data. This work underscores the potential need to develop generative\nmodels specifically tailored to take advantage of the unique characteristics of\nemerging domains, such as transportation.",
        "The Grey System Theory (GST) is a powerful mathematical framework employed\nfor modeling systems with uncertain or incomplete information. This paper\nproposes an integration of the GST with time scales, a generalized approach\nthat encompasses both discrete and continuous time models. The proposed model,\ncalled the Grey System Model on Time Scales (GST-T), offers a robust solution\nfor analyzing hybrid systems where events occur on varying time domains.",
        "The rapid advancements in generative AI models, such as ChatGPT, have\nintroduced both significant benefits and new risks within the cybersecurity\nlandscape. This paper investigates the potential misuse of the latest AI model,\nChatGPT-4o Mini, in facilitating social engineering attacks, with a particular\nfocus on phishing, one of the most pressing cybersecurity threats today. While\nexisting literature primarily addresses the technical aspects, such as\njailbreaking techniques, none have fully explored the free and straightforward\nexecution of a comprehensive phishing campaign by novice users using ChatGPT-4o\nMini. In this study, we examine the vulnerabilities of AI-driven chatbot\nservices in 2025, specifically how methods like jailbreaking and reverse\npsychology can bypass ethical safeguards, allowing ChatGPT to generate phishing\ncontent, suggest hacking tools, and assist in carrying out phishing attacks.\nOur findings underscore the alarming ease with which even inexperienced users\ncan execute sophisticated phishing campaigns, emphasizing the urgent need for\nstronger cybersecurity measures and heightened user awareness in the age of AI.",
        "The smart metering infrastructure may become one of the key elements in\nefficiently managing energy in smart cities. At the same time, traditional\nmeasurement record collection is performed by manual methods, which raises\ncost, safety, and accuracy issues. This paper proposes an innovative SMI\narchitecture based on an unmanned aerial vehicle swarm organizing itself for\nthe autonomous data collection in smart metering infrastructure with\nscalability and cost-effectiveness while minimizing risks. We design an\narchitecture-based comprehensive system with various phases of operation,\ncommunication protocols, and robust failure-handling mechanisms to ensure\nreliable operations. We further perform extensive simulations in maintenance of\nprecise formations during flight, efficient data collection from smart meters,\nand adaptation to various failure scenarios. Importantly, we analyze the energy\nconsumption of the proposed system in both drone flight operations and network\ncommunication. We now propose a battery sizing strategy and provide an estimate\nof the operational lifetime of the swarm, underlining the feasibility and\npracticality of our approach. Our results show that UAV swarms have great\npotential to revolutionize smart metering and to bring a further brick to\ngreener and more resilient smart cities.",
        "Scanning the sharp metal tip of a scanning tunneling microscope (STM) over a\nmolecule allows tuning the coupling between the tip plasmon and a molecular\nfluorescence emitter. This allows access to local variations of fluorescence\nfield enhancement and wavelength shifts, which are central parameters for\ncharacterizing the plasmon-exciton coupling. Performing the same for\nphosphorescence with molecular scale resolution remains a significant\nchallenge. In this study, we present the first investigation of phosphorescence\nfrom isolated Pt-Phthalocyanine molecules by analyzing tip-enhanced emission\nspectra in both current-induced and laser-induced phosphorescence. The latter\ndirectly monitors singlet-to-triplet state intersystem crossing of a molecule\nbelow the tip. The study paves the way to a detailed understanding of triplet\nexcitation pathways and their potential control at sub-molecular length scales.\nAdditionally, the coupling of organic phosphors to plasmonic structures is a\npromising route for the improving light-emitting diodes.",
        "Recent advances in code generation models have demonstrated impressive\ncapabilities in automating software development tasks, yet these models still\nstruggle in real-world software engineering scenarios. Although current\ntraining methods, particularly post-training, excel at solving competitive\nprogramming problems, they fail to adequately prepare models for the\ncomplexities of practical software development. This misalignment raises the\ncritical question: Are existing alignment training methods well suited for\nreal-world software engineering tasks? In this study, we identify this issue\nand propose SEAlign, a novel alignment framework designed to bridge the gap\nbetween code generation models and real-world software development tasks.\nSEAlign leverages the unique characteristics of software engineering processes,\nincluding high-quality workflow steps, to enhance model capabilities. Our\nframework further employs Monte Carlo Tree Search for fine-grained alignment in\nmulti-step decision processes, followed by preference optimization on critical\nactions to ensure models meet real-world requirements. We evaluate SEAlign on\nthree standard agentic benchmarks for real-world software engineering,\nincluding HumanEvalFix, SWE-Bench-Lite, and SWE-Bench-Verified. Experimental\nresults demonstrate state-of-the-art performance with minimal training\noverhead. In addition, we develop an agent-based software development platform\nusing SEAlign, which successfully automates the creation of several small\napplications. Human evaluations of these applications highlight significant\nimprovements in both task performance and user experience. Our findings\nunderscore the potential of SEAlign to accelerate the adoption of large code\nmodels in real-world software development. We believe that this research makes\na meaningful step towards fully automated software engineering.",
        "We consider a continuous-time financial market with an asset whose price is\nmodeled by a linear stochastic differential equation with drift and volatility\nswitching driven by a uniformly ergodic jump Markov process with a countable\nstate space (in fact, this is a Black-Scholes model with Markov switching). We\nconstruct a multiplicative scheme of series of discrete-time markets with\ndiscrete-time Markov switching. First, we establish that the discrete-time\nswitching Markov chains weakly converge to the limit continuous-time Markov\nprocess. Second, having this in hand, we apply conditioning on Markov chains\nand prove that the discrete-time market models themselves weakly converge to\nthe Black-Scholes model with Markov switching. The convergence is proved under\nvery general assumptions both on the discrete-time net profits and on a\ngenerator of a continuous-time Markov switching process.",
        "Sco X-1 is the brightest observed extra-solar X-ray source, which is a\nneutron star (NS) low-mass X-ray binary (LMXB), and is thought to have a strong\npotential for continuous gravitational waves (CW) detection due to its high\naccretion rate and relative proximity. Here, we compute the long-term evolution\nof its parameters, particularly the NS spin frequency ($\\nu$) and the surface\nmagnetic field ($B$), to probe its nature and its potential for CW detection.\nWe find that Sco X-1 is an unusually young ($\\sim7\\times10^6$ yr) LMXB and\nconstrain the current NS mass to $\\sim 1.4-1.6~{\\rm M}_\\odot$. Our computations\nreveal a rapid $B$ decay, with the maximum current value of $\\sim\n1.8\\times10^8$ G, which can be useful to constrain the decay models. Note that\nthe maximum current $\\nu$ value is $\\sim 550$ Hz, implying that, unlike what is\ngenerally believed, a CW emission is not required to explain the current source\nproperties. However, $\\nu$ will exceed an observed cut-off frequency of $\\sim\n730$ Hz, and perhaps even the NS break-up frequency, in the future, without a\nCW emission. The minimum NS mass quadrupole moment ($Q$) to avoid this is $\\sim\n(2-3)\\times10^{37}$ g cm$^2$, corresponding to a CW strain of $\\sim 10^{-26}$.\nOur estimation of current $\\nu$ values can improve the CW search sensitivity.",
        "The rapid evolution of communication networks towards 6G increasingly\nincorporates advanced AI-driven controls across various network segments to\nachieve intelligent, zero-touch operation. This paper proposes a comprehensive\nand modular framework for AI controllers, designed to be highly flexible and\nadaptable for use across both fiber optical and radio networks. Building on the\nprinciples established by the O-RAN Alliance for near-Real-Time RAN Intelligent\nControllers (near-RT RICs), our framework extends this AI-driven control into\nthe optical domain. Our approach addresses the critical need for a unified AI\ncontrol framework across diverse network transport technologies and domains,\nenabling the development of intelligent, automated, and scalable 6G networks.",
        "Electron-phonon ($e$-ph) interactions arise in many strongly correlated\nquantum materials from the modulation of the nearest-neighbor hopping\nintegrals, as in the celebrated Su-Schrieffer-Heeger (SSH) model. Nevertheless,\nrelatively few non-perturbative studies of correlated SSH models have been\nconducted in dimensions greater than one, and those that have been done have\nprimarily focused on bond models, where generalized displacements independently\nmodulate each hopping integral. We conducted a sign-problem free determinant\nquantum Monte Carlo study of the optical SSH-Hubbard model on a two-dimensional\nsquare lattice, where site-centered phonon modes simultaneously modulate pairs\nof nearest-neighbor hopping integrals. We report the model's low-temperature\nphase diagram in the challenging adiabatic regime ($\\Omega\/E_\\mathrm{F} \\sim\n1\/8$). It exhibits insulating antiferromagnetic Mott and bond-order-wave (BOW)\nphases with a narrow region of coexistence between them. We also find that a\ncritical $e$-ph coupling is required to stabilize the BOW phase in the small\n$U$ limit. Lastly, in stark contrast to recent findings for the model's bond\nvariant, we find no evidence for a long-range antiferromagnetism in the pure\n$(U\/t=0)$ optical SSH model.",
        "We demonstrate strong light-matter interaction for a layer of templated\nmerocyanine molecules in a planar microcavity. Using a single layer of graphene\nnanoribbons as a templating layer, we obtain an aligned layer of aggregated\nmolecules. The molecular layer displays anisotropic optical properties\nresembling those of a biaxial crystal. The anisotropic excitonic component in\nthe cavity results in strongly polarization-dependent light-matter interaction\nand in increased Rabi-energies. The increased light-matter interaction is\npossibly due to reduced molecular disorder in the templated molecular layer.\nThis conclusion is supported by an analysis based on a multi-oscillator model.\nWe further use photoluminescence microspectroscopy to demonstrate that the\nlight-matter coupling is spatially homogeneous. Our study introduces molecular\ntemplating to strong light-matter studies. The reduced disorder of the system\nas a consequence of templating is highly beneficial for engineering\nlight-matter interaction.",
        "The rise of Agentic applications and automation in the Voice AI industry has\nled to an increased reliance on Large Language Models (LLMs) to navigate\ngraph-based logic workflows composed of nodes and edges. However, existing\nmethods face challenges such as alignment errors in complex workflows and\nhallucinations caused by excessive context size. To address these limitations,\nwe introduce the Performant Agentic Framework (PAF), a novel system that\nassists LLMs in selecting appropriate nodes and executing actions in order when\ntraversing complex graphs. PAF combines LLM-based reasoning with a\nmathematically grounded vector scoring mechanism, achieving both higher\naccuracy and reduced latency. Our approach dynamically balances strict\nadherence to predefined paths with flexible node jumps to handle various user\ninputs efficiently. Experiments demonstrate that PAF significantly outperforms\nbaseline methods, paving the way for scalable, real-time Conversational AI\nsystems in complex business environments.",
        "We introduce a numerical method for Brownian dynamics with position dependent\ndiffusion tensor which is second order accurate for sampling the invariant\nmeasure while requiring only one force evaluation per timestep. Analysis of the\nsampling bias is performed using the algebraic framework of exotic aromatic\nButcher-series. Numerical experiments confirm the theoretical order of\nconvergence and illustrate the efficiency of the new method.",
        "We propose a black hole microstate counting method based on the canonical\nquantization of the asymptotic symmetries of a two-dimensional dilaton-gravity\nsystem at future null infinity. This dilaton-gravity is obtained from the\ns-wave reduction of an $N$-dimensional Einstein gravity over a generic\nasymptotically flat black hole solution. We show that a Cardy-type formula\nleads to the Bekenstein-Hawking entropy. In this scenario, the quantized bulk\ndegrees of freedom act as a source for the dual field theory operators living\nat future null infinity that are soft gravitons of the higher dimensional\nsystem, up to some gauge fixing.",
        "Neural tangent kernels (NTKs) have been proposed to study the behavior of\ntrained neural networks from the perspective of Gaussian processes. An\nimportant result in this body of work is the theorem of equivalence between a\ntrained neural network and kernel regression with the corresponding NTK. This\ntheorem allows for an interpretation of neural networks as special cases of\nkernel regression. However, does this theorem of equivalence hold in practice?\n  In this paper, we revisit the derivation of the NTK rigorously and conduct\nnumerical experiments to evaluate this equivalence theorem. We observe that\nadding a layer to a neural network and the corresponding updated NTK do not\nyield matching changes in the predictor error. Furthermore, we observe that\nkernel regression with a Gaussian process kernel in the literature that does\nnot account for neural network training produces prediction errors very close\nto that of kernel regression with NTKs. These observations suggest the\nequivalence theorem does not hold well in practice and puts into question\nwhether neural tangent kernels adequately address the training process of\nneural networks.",
        "In this paper, we investigate the optical appearance of a charged black hole\nin the Kalb-Ramond background, incorporating a Lorentz-violating parameter\n$l=0.01$. By analyzing the null geodesics, we derive the photon sphere, event\nhorizon, effective potential, and critical impact parameters. We then employ a\nray-tracing technique to study the trajectories of photons surrounding a thin\naccretion disk. Three different emission models are considered to explore the\nobserved intensity profiles of direct rings, lensing rings, and photon sphere.\nBy comparing these results with those of the standard Reissner-Nordstr\\\"om\nblack hole ($l=0$) and the Kalb-Ramond black hole with different values of\nLorentz-violating parameter (specifically, $l=0.05$ and $l=0.1$ respectively),\nwe find that the Lorentz symmetry breaking will lead to a decrease in the radii\nof the photon sphere, the event horizon, and the innermost stable circular\norbit. Consequently, this makes the detection of these black holes more\nchallenging.",
        "The kagome lattice has garnered significant attention due to its ability to\nhost quantum spin Fermi liquid states. Recently, the combination of unique\nlattice geometry, electron-electron correlations, and adjustable magnetism in\nsolid kagome materials has led to the discovery of numerous fascinating quantum\nproperties. These include unconventional superconductivity, charge and spin\ndensity waves (CDW\/SDW), pair density waves (PDW), and Chern insulator phases.\nThese emergent states are closely associated with the distinctive\ncharacteristics of the kagome lattice's electronic structure, such as van Hove\nsingularities, Dirac fermions, and flat bands, which can exhibit exotic\nquasi-particle excitations under different symmetries and magnetic conditions.\nRecently, various quantum kagome materials have been developed, typically\nconsisting of kagome layers stacked along the $z$-axis with atoms either\nfilling the geometric centers of the kagome lattice or embedded between the\nlayers. In this topical review, we begin by introducing the fundamental\nproperties of several kagome materials. To gain an in-depth understanding of\nthe relationship between topology and correlation, we then discuss the complex\nphenomena observed in these systems. These include the simplest kagome metal\n$T_3X$, kagome intercalation metal $TX$, and the ternary compounds $AT_6X_6$\nand $RT_3X_5$ ($A$ = Li, Mg, Ca, or rare earth; $T$ = V, Cr, Mn, Fe, Co, Ni;\n$X$ = Sn, Ge; $R$ = K, Rb, Cs). Finally, we provide a perspective on future\nexperimental work in this field.",
        "Metallic antiferromagnets are essential for efficient spintronic applications\ndue to their fast switching and high mobility, yet room-temperature metallic\nantiferromagnets are rare. Here, we investigate YbMn$_2$Ge$_2$, a room\ntemperature antiferromagnet, and establish it as an exfoliable layered metal\nwith altermagnetic surface states. Using multi-orbital Hubbard model\ncalculations, we reveal that its robust metallic AFM ordering is stabilized by\nelectronic correlations and a partially nested Fermi surface. Furthermore, we\nshow that YbMn$_2$Ge$_2$ hosts symmetry-protected topological Dirac crossings,\nconnecting unique even-order spin-polarized surface states with parabolic and\ninverted Mexican-hat-like dispersion. Our findings position YbMn$_2$Ge$_2$ as a\npromising platform for exploring the interplay of correlation, topology, and\nsurface altermagnetism of layered antiferromagnets.",
        "In this paper, we study isometries of $p$-Wasserstein spaces. In our first\nresult, for every complete and separable metric space $X$ and for every\n$p\\geq1$, we construct a metric space $Y$ such that $X$ embeds isometrically\ninto $Y$, and the $p$-Wasserstein space over $Y$ admits mass-splitting\nisometries. Our second result is about embeddings into rigid constructions. We\nshow that any complete and separable metric space $X$ can be embedded\nisometrically into a metric space $Y$ such that the $1$-Wasserstein space is\nisometrically rigid.",
        "Online learning to rank sequentially recommends a small list of items to\nusers from a large candidate set and receives the users' click feedback. In\nmany real-world scenarios, users browse the recommended list in order and click\nthe first attractive item without checking the rest. Such behaviors are usually\nformulated as the cascade model. Many recent works study algorithms for\ncascading bandits, an online learning to rank framework in the cascade model.\nHowever, the performance of existing methods may drop significantly if part of\nthe user feedback is adversarially corrupted (e.g., click fraud). In this work,\nwe study how to resist adversarial corruptions in cascading bandits. We first\nformulate the ``\\textit{Cascading Bandits with Adversarial Corruptions}\" (CBAC)\nproblem, which assumes that there is an adaptive adversary that may manipulate\nthe user feedback. Then we propose two robust algorithms for this problem,\nwhich assume the corruption level is known and agnostic, respectively. We show\nthat both algorithms can achieve logarithmic regret when the algorithm is not\nunder attack, and the regret increases linearly with the corruption level. The\nexperimental results also verify the robustness of our methods.",
        "In this paper, we are trying to show the uniqueness of transonic shock\nsolutions in an expanding nozzle under certain conditions and assumptions on\nthe boundary data and the shock solution. The idea is to compare two transonic\nshock solutions and show that they should coincide if the perturbation of the\nnozzle is sufficiently small. To this end, a condition on the pressure of the\nflow across the shock front is proposed, such that a priori estimates for the\nsubsonic flow behind the shock front could be established without the\nassumption that it is a small perturbation of the unperturbed uniform subsonic\nstate. With the help of these estimates, the uniqueness of the position of the\nintersection point between the shock front and the nozzle boundary could be\nfurther established by demonstrating the monotonicity of the solvability\ncondition for the elliptic sub-problem of the subsonic flow behind the shock\nfront. Then, via contraction arguments, two transonic shock solutions could be\nverified to coincide as the perturbation is small, which leads to the\nuniqueness of the transonic shock solution.",
        "The motion of the solar system against an isotropic radiation background,\nsuch as the cosmic microwave background, induces a dipole anisotropy in the\nbackground due to the Doppler effect. Flux-limited observation of the continuum\nradiation from galaxies also has been studied extensively to show a dipole\nanisotropy due to the Doppler effect and the aberration effect. We show that a\nsimilar dipole anisotropy exists in spectral-line intensity maps, represented\nas either galaxy number counts or the diffuse intensity maps. The amplitude of\nthese dipole anisotropies is determined by not only the solar velocity against\nthe large-scale structures but also the temporal evolution of the monopole\n(sky-average) component. Measuring the dipole at multiple frequencies, which\nhave mutually independent origins due to their occurrence from multiple\nredshifts, can provide a very accurate measure of the solar velocity thanks to\nthe redundant information. We find that such a measurement can even constrain\nastrophysical parameters in the nearby universe. We explore the potential for\ndipole measurement of existing and upcoming surveys, and conclude that the\nspectral number count of galaxies through SPHEREx will be optimal for the first\nmeasurement of the dipole anisotropy in the spectral-line galaxy distribution.\nLIM surveys with reasonable accuracy are also found to be promising. We also\ndiscuss whether these experiments might reveal a peculiar nature of our local\nuniverse, that seems to call for a non-standard cosmology other than the simple\nLambdaCDM model as suggested by recent measures of the baryon acoustic\noscillation signatures and the Alcock-Paczynski tests.",
        "We have observed the $^{13}$CH$_3$OH $5_1-4_1$ A$^+$, $^{13}$CH$_3$OH\n$14_1-13_2$ A$^-$, and CH$_2$DOH $8_{2,6}-8_{1,7}$ $e_0$ lines toward 24\nhigh-mass star-forming regions by using Atacama Large Millimeter\/submillimeter\nArray (ALMA) with an angular resolution of about 0$^{\\prime\\prime}$.3. This\nresolution corresponds to a linear scale of 400-1600 au, allowing us to resolve\nindividual cores properly. We detected the $^{13}$CH$_3$OH and CH$_2$DOH\nemission near the continuum peaks in many of these regions. From the two\n$^{13}$CH$_3$OH lines, we calculated the temperature toward the $^{13}$CH$_3$OH\npeaks, and confirm that the emission traces hot ($>$100 K) regions. The\n$N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in the observed high-mass\nstar-forming regions is found to be lower than that in low-mass star-forming\nregions. We have found no correlation between the\n$N$(CH$_2$DOH)\/$N$($^{13}$CH$_3$OH) or $N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH)\nratios and either temperatures or distance to the sources, and have also found\na source-to-source variation in these ratios. Our model calculations predict\nthat the $N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in hot cores depends on the\nduration of the cold phase; the shorter the cold phase, the lower the deuterium\nfractionation in the hot cores. We have suggested that the lower\n$N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in high-mass star-forming regions\ncompared to that in low-mass star-forming regions is due to the shorter\nduration of the cold phase and that the diversity in the\n$N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in high-mass star-forming regions is\ndue to the diversity in the length of the cold prestellar phase, and not the\ntime that the objects have been in the hot core phase.",
        "Asymptotic grand unification models can be constructed in five dimensions\ncompactified on an orbifold. We demonstrate that the parameter space of such\nmodels admit solutions that naturally achieve the baryon asymmetry via\nleptogenesis, these solutions indicating that the scale of the extra dimensions\nis much higher than the TeV scale."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Data-driven science and engineering: machine learning, dynamical systems, and control",
    "start_abstract":"\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Cerebral aneurysms. New Engl. J. Medicine"
      ],
      "abstract":[
        "Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "On the hierarchy of plate models for a singularly perturbed multi-well\n  nonlinear elastic energy",
        "Some topological genera and Jacobi forms",
        "A Fresh Perspective on Water Dynamics in Aqueous Salt Solutions",
        "Probing topological matter and fermion dynamics on a neutral-atom\n  quantum computer",
        "Irregularities of distribution and Fourier transforms of\n  multi-dimensional convex bodies",
        "Photonic heat amplifiers based on Anderson insulators",
        "Frequency-noise-insensitive universal control of Kerr-cat qubits",
        "Bilateral Bailey pairs and Rogers-Ramanujan type identities",
        "Entropy Inequalities Constrain Holographic Erasure Correction",
        "From Data to Combinatorial Multivector field Through an\n  Optimization-Based Framework",
        "Model Predictive and Reinforcement Learning Methods for Active Flow\n  Control of an Airfoil with Dual-point Excitation of Plasma Actuators",
        "Bursty acceleration and 3D trajectories of electrons in a solar flare",
        "Test fields and naked singularities: is the second law the cosmic\n  censor?",
        "Lonely passenger problem: the more buses there are, the more lonely\n  passengers there will be",
        "Universal self-gravitating skyrmions",
        "The Deligne Complex for the $B_3$ Artin Group",
        "Odd and even derivations, transposed Poisson superalgebra and 3-Lie\n  superalgebra",
        "A Linear Quantum Coupler for Clean Bosonic Control",
        "Arbitrary control of the flow of light using pseudomagnetic fields in\n  photonic crystals at telecommunication wavelengths",
        "On the decay instability of electron acoustic waves",
        "Prescribed Chern scalar curvature flow on compact Hermitian manifolds\n  with negative Gauduchon degree",
        "The fundamental representation of pricing adjustments",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Damping Tuning Considering Random Disturbances Adopting Distributionally\n  Robust Optimization",
        "Category-Selective Neurons in Deep Networks: Comparing Purely Visual and\n  Visual-Language Models",
        "Exact Parent Hamiltonians for All Landau Level States in a Half-flux\n  Lattice",
        "Evolution of the near-core rotation frequency of 2,497 intermediate-mass\n  stars from their dominant gravito-inertial mode",
        "A note on uniform continuity of monotone functions",
        "Two in context learning tasks with complex functions"
      ],
      "abstract":[
        "In the celebrated work of Friesecke, James and M\\\"uller '06 the authors\nderive a hierarchy of models for plates by carefully analyzing the\n$\\Gamma$-convergence of the rescaled nonlinear elastic energy. The key\ningredient of their proofs is the rigidity estimate proved in an earlier work\nof theirs. Here we consider the case in which the elastic energy has a\nmulti-well structure: this type of functional arises, for example, in the study\nof solid-solid phase transitions. Since the rigidity estimate fails in the case\nof compatible wells, we follow Alicandro, Dal Maso, Lazzaroni and Palombaro '18\nand add a regularization term to the energy that penalizes jumps from one well\nto another, leading to good compactness properties. In this setting we recover\nthe full hierarchy of plate models with an explicit dependence on the wells.\nFinally, we study the convergence of energy minimizers with suitable external\nforces and full Neumann boundary conditions. To do so, we adapt the definition\nof optimal rotations introduced by Maor, Mora '21.",
        "We revisit and elucidate the $\\widehat{A}$-genus, Hirzebruch's $L$-genus and\nWitten's $W$-genus, cobordism invariants of special classes of manifolds. After\nslight modification, involving Hecke's trick, we find that the\n$\\widehat{A}$-genus and $L$-genus arise directly from Jacobi's theta function.\nIn this way, for every $k\\geq 0,$ we obtain exact formulas for the quasimodular\nexpressions of $\\widehat{A}_k$ and $L_k$ as ``traces'' of partition Eisenstein\nseries \\[\\widehat{\\mathcal{A}}_k(\\tau)=\n\\operatorname{Tr}_k(\\phi_{\\widehat{A}};\\tau)\\ \\ \\ \\ \\ \\ {\\text {\\rm and}}\\ \\ \\\n\\ \\ \\ \\mathcal{L}_k(\\tau)= \\operatorname{Tr}_k(\\phi_L;\\tau). \\] Surprisingly,\nRamanujan defined twists of the $\\widehat{\\mathcal{A}}_k(\\tau)$ in his ``lost\nnotebook'' in his study of derivatives of theta functions, decades before Borel\nand Hirzebruch rediscovered them in the context of spin manifolds. In addition,\nwe show that the nonholomorphic $G_2^{\\star}$-completion of the characteristic\nseries of the Witten genus is the Jacobi theta function avatar of the\n$\\widehat{A}$-genus.",
        "Molecular dynamics in pure water and aqueous salt solutions remain\nincompletely understood, partly due to the apparent contradictions between\nresults from different spectroscopic techniques. In this work, we demonstrate,\nby detailed comparison of light scattering and dielectric spectroscopy data for\npure water and aqueous lithium chloride solutions, that these apparent\ncontradictions can be resolved by accounting for orientational\ncross-correlations of neighboring molecules. Remarkably, a single structural\nrelaxation mode with largely temperature- and concentration-independent shape\ncan be identified in all spectra, from room temperature down to the deeply\nsupercooled regime. These results provide a new perspective for the study of\nmolecular dynamics in aqueous salt solutions.",
        "Quantum simulations of many-body systems are among the most promising\napplications of quantum computers. In particular, models based on\nstrongly-correlated fermions are central to our understanding of quantum\nchemistry and materials problems, and can lead to exotic, topological phases of\nmatter. However, due to the non-local nature of fermions, such models are\nchallenging to simulate with qubit devices. Here we realize a digital quantum\nsimulation architecture for two-dimensional fermionic systems based on\nreconfigurable atom arrays. We utilize a fermion-to-qubit mapping based on\nKitaev's model on a honeycomb lattice, in which fermionic statistics are\nencoded using long-range entangled states. We prepare these states efficiently\nusing measurement and feedforward, realize subsequent fermionic evolution\nthrough Floquet engineering with tunable entangling gates interspersed with\natom rearrangement, and improve results with built-in error detection.\nLeveraging this fermion description of the Kitaev spin model, we efficiently\nprepare topological states across its complex phase diagram and verify the\nnon-Abelian spin liquid phase by evaluating an odd Chern number. We further\nexplore this two-dimensional fermion system by realizing tunable dynamics and\ndirectly probing fermion exchange statistics. Finally, we simulate strong\ninteractions and study dynamics of the Fermi-Hubbard model on a square lattice.\nThese results pave the way for digital quantum simulations of complex fermionic\nsystems for materials science, chemistry, and high-energy physics.",
        "W. Schmidt, H. Montgomery, and J. Beck proved a result on irregularities of\ndistribution with respect to $d$-dimensional balls. In this paper, we extend\ntheir result to any $d$-dimensional convex body with a smooth boundary and\nfinite order of contact. As an intermediate step, we prove a geometric\ninequality for the Fourier transform of the characteristic function of a convex\nbody.",
        "A photonic heat amplifier (PHA) designed for cryogenic operations is\nintroduced and analyzed. This device comprises two Anderson insulator\nreservoirs connected by lossless lines, allowing them to exchange heat through\nphotonic modes. This configuration enables negative differential thermal\nconductance (NDTC), which can be harnessed to amplify thermal signals. To\nachieve this, we maintain one reservoir at a high temperature, serving as the\nsource terminal of a thermal transistor. Concurrently, in the other one, we\nestablish tunnel contacts to metallic reservoirs, which function as the gate\nand drain terminals. With this arrangement, it is possible to control the heat\nflux exchange between the source and drain by adjusting the gate temperature.\nWe present two distinct parameter choices that yield different performances:\nthe first emphasizes modulating the source-drain heat current, while the second\nfocuses on the temperature modulation of the colder Anderson insulator. Lastly,\nwe present a potential design variation in which all electronic reservoirs are\nthermally connected through only photonic modes, allowing interactions between\ndistant elements. The proposal of the PHA addresses the lack of thermal\ntransistors and amplifiers in the mK range while being compatible with the rich\ntoolbox of circuit quantum electrodynamics. It can be adapted to various\napplications, including sensing and developing thermal circuits and control\ndevices at sub-Kelvin temperatures, which are relevant to quantum technologies.",
        "We theoretically study the influence of frequency uncertainties on the\noperation of a Kerr-cat qubit. As the mean photon number increases, Kerr-cat\nqubits provide an increasing level of protection against phase errors induced\nby unknown frequency shifts during idling and X rotations. However, realizing\nrotations about the other principal axes (e.g., Y and Z axes) while preserving\nrobustness is nontrivial. To address this challenge, we propose a universal set\nof gate schemes which circumvents the tradeoff between protection and\ncontrollability in Kerr-cat qubits and retains robustness to unknown frequency\nshifts to at least first order. Assuming an effective Kerr oscillator model, we\ntheoretically and numerically analyze the robustness of elementary gates on\nKerr-cat qubits, with special focus on gates along nontrivial rotation axes. An\nappealing application of this qubit design would include tunable\nsuperconducting platforms, where the induced protection against frequency noise\nwould allow for a more flexible choice of operating point and thus the\npotential mitigation of the impact of spurious two-level systems.",
        "Rogers-Ramanujan type identities occur in various branches of mathematics and\nphysics. As a classic and powerful tool to deal with Rogers-Ramanujan type\nidentities, the theory of Bailey's lemma has been extensively studied and\ngeneralized. In this paper, we found a bilateral Bailey pair that naturally\narises from the q-binomial theorem. By applying the bilateral versions of\nBailey lemmas, Bailey chains and Bailey lattices, we derive a number of\nRogers-Ramanujan type identities, which unify many known identities as special\ncases. Further combined with the bilateral Bailey chains due to Berkovich,\nMcCoy and Schilling and the bilateral Bailey lattices due to Jouhet et al., we\nalso obtain identities on Appell-Lerch series and identities of Andrews-Gordon\ntype. Moreover, by applying Andrews and Warnaar's bilateral Bailey lemmas, we\nderive identities on Hecke-type series.",
        "We interpret holographic entropy inequalities in terms of erasure correction.\nThe non-saturation of an inequality is a necessary condition for certain\nschemes of holographic erasure correction, manifested in the bulk as non-empty\noverlaps of corresponding entanglement wedges.",
        "This paper extends and generalizes previous works on constructing\ncombinatorial multivector fields from continuous systems (see [10]) and the\nconstruction of combinatorial vector fields from data (see [2]) by introducing\nan optimization based framework for the construction of combinatorial\nmultivector fields from finite vector field data. We address key challenges in\nconvexity, computational complexity and resolution, providing theoretical\nguarantees and practical methodologies for generating combinatorial\nrepresentation of the dynamics of our data.",
        "This paper presents an analysis of Model Predictive Control (MPC) and\nReinforcement Learning (RL) approaches for active flow control over a NACA\n(National Advisory Committee for Aeronautics) 4412 airfoil around the static\nstall condition at a Reynolds number of 4*10^5. The Reynolds Averaged\nNavier-Stokes (RANS) equations with the Scale-Adaptive Simulation (SAS)\nturbulence model are utilized for the numerical simulations. The dielectric\nbarrier discharge (DBD) plasma actuators were employed in dual excitation mode\nfor flow separation control. The study systematically evaluates adaptive MPC,\ntemporal difference reinforcement learning (TDRL), and deep Q-learning (DQL)\nbased on optimizing the excitation frequency and expediting the time to\nidentify stable conditions. Moreover, an integrated approach that combines\nsignal processing with DQL is examined. The results demonstrate that while MPC\nand RL significantly improve flow control, RL approaches offer superior\nadaptability and performance. In optimal conditions, a lift coefficient of\naround 1.619 was achieved within less than 2.5 seconds with an excitation\nfrequency of 100 or 200 Hz. This research highlights that RL-based approaches\ncould perform better in flow control applications than MPC.",
        "During a solar flare, electrons are accelerated to non-thermal energies as a\nresult of magnetic reconnection. These electrons then propagate upwards and\ndownwards from the energy release site along magnetic field lines and produce\nradio and X-ray emission. On 11 November 2022, an M5.1 solar flare was observed\nby the Spectrometer\/Telescope for Imaging X-rays (STIX) on board Solar Orbiter\ntogether with various ground- and space-based radio instruments. The flare was\nassociated with several fine hard X-ray (HXR) structures and a complex set of\nmetric radio bursts (type III, J, and narrowband). By studying the evolution of\nX-ray, extreme ultraviolet, and radio sources, we aim to study the trajectories\nof the flare-accelerated electrons in the lower solar atmosphere and low\ncorona. We used observations from the STIX on board Solar Orbiter to study the\nevolution of X-ray sources. Using radio imaging from the Nan\\c{c}ay Radio\nheliograph (NRH) and the Newkirk density model, we constructed 3D trajectories\nof 14 radio bursts. Imaging of the HXR fine structures shows several sources at\ndifferent times. The STIX and NRH imaging shows correlated changes in the\nlocation of the HXR and radio source at the highest frequency during the most\nintense impulsive period. Imaging and 3D trajectories of all the bursts show\nthat electrons are getting accelerated at different locations and along several\ndistinct field lines. The longitude and latitude extent of the trajectories are\n~30 arcsec and ~ 152 arcsec. We find that the electrons producing HXR and radio\nemission have similar acceleration origins. Importantly, our study supports the\nscenario that the flare acceleration process is temporally and spatially\nfragmentary, and during each of these small-scale processes, the electron beams\nare injected into a very fibrous environment and produce complex HXR and radio\nemission.",
        "It has been claimed that a Kerr-Newman black hole can generically be overspun\nby neutral test fields, and it has been argued that even when backreactions are\ntaken into account, the black hole can still be destroyed. In this paper, we\nrevisit the weak cosmic censorship conjecture for a Kerr-Newman black hole with\na test scalar field and a neutrino field, and point out that the assumption in\nprevious work regarding the energy and angular momentum of the test fields\nabsorbed by the black hole violates the second law of black hole\nthermodynamics. By solving the test scalar field and neutrino field near the\nevent horizon explicitly, we demonstrate that an extremal Kerr-Newman black\nhole cannot be overspun by a test scalar field but can be destroyed by a\nneutrino field. Our results indicate that the condition required to overspin an\nextremal Kerr-Newman black hole coincides with the condition needed to violate\nthe second law of black hole thermodynamics. Furthermore, we observe that such\na violation of the second law might inevitably result in a breakdown of the\nweak cosmic censorship conjecture.",
        "Empty buses are standing at a bus station. $n$ passengers arrive, and they\neach board a bus completely at random (meaning that they choose uniformly and\nindependently). Then all buses depart. We show that the more buses there are,\nthe more likely it is that someone (i.e. at least one passenger) travels alone\n(while $n$ is fixed). More generally, we show that the number of lonely\npassengers increases with the number of buses, in the sense of stochastic\ndominance. This problem turned out to be surprisingly difficult, with no short\nsolution known to the author so far, despite the efforts of many experts. Some\nof the results can also be formulated as properties of Stirling numbers of the\nsecond kind.",
        "The self-gravitating skyrmion is an exact solution of the Einstein\n$SU(2)$-Skyrme model describing a topological soliton with baryon number $B=1$,\nliving in a $4$-dimensional space-time in the presence of a cosmological\nconstant. Here we show that, using the maximal embedding Ansatz of $SU(2)$ into\n$SU(N)$ in the Euler angles parametrization, this solution can be generalized\nto include arbitrary values of the flavor number and, consequently, allowing\nhigher values of the topological charge. Also, we show that higher-order\ncorrections in the 't Hooft expansion can be considered while still preserving\nthe analytical nature of the solutions. Finally we will show that from the\ngravitational solutions it is possible to construct skyrmions in flat\nspace-time at a finite volume.",
        "We show that the piecewise Euclidean Moussong metric on the Deligne complex\nof the Artin group of type $B_3$ is $\\mathrm{CAT}(0)$. We do this by\nestablishing a criteria for a complex made of $B_3$ simplices to be\n$\\mathrm{CAT}(1)$ in terms of embedded edge paths, which in particular applies\nto the spherical Deligne complex of type $B_3$. This provides one more step to\nshowing that the Moussong metric is $\\mathrm{CAT}(0)$ for any 3-dimensional\nArtin group.",
        "One important example of a transposed Poisson algebra can be constructed by\nmeans of a commutative algebra and its derivation. This approach can be\nextended to superalgebras, that is, one can construct a transposed Poisson\nsuperalgebra given a commutative superalgebra and its even derivation. In this\npaper we show that including odd derivations in the framework of this approach\nrequires introducing a new notion. It is a super vector space with two\noperations that satisfy the compatibility condition of transposed Poisson\nsuperalgebra. The first operation is determined by a left supermodule over\ncommutative superalgebra and the second is a Jordan bracket. Then it is proved\nthat the super vector space generated by an odd derivation of a commutative\nsuperalgebra satisfies all the requirements of introduced notion. We also show\nhow to construct a 3-Lie superalgebra if we are given a transposed Poisson\nsuperalgebra and its even derivation.",
        "Quantum computing with superconducting circuits relies on high-fidelity\ndriven nonlinear processes. An ideal quantum nonlinearity would selectively\nactivate desired coherent processes at high strength, without activating\nparasitic mixing products or introducing additional decoherence. The wide\nbandwidth of the Josephson nonlinearity makes this difficult, with undesired\ndrive-induced transitions and decoherence limiting qubit readout, gates,\ncouplers and amplifiers. Significant strides have been recently made into\nbuilding better `quantum mixers', with promise being shown by Kerr-free\nthree-wave mixers that suppress driven frequency shifts, and balanced quantum\nmixers that explicitly forbid a significant fraction of parasitic processes. We\npropose a novel mixer that combines both these strengths, with engineered\nselection rules that make it essentially linear (not just Kerr-free) when idle,\nand activate clean parametric processes even when driven at high strength.\nFurther, its ideal Hamiltonian is simple to analyze analytically, and we show\nthat this ideal behavior is first-order insensitive to dominant experimental\nimperfections. We expect this mixer to allow significant advances in high-Q\ncontrol, readout, and amplification.",
        "In photonics, the idea of controlling light in a similar way that magnetic\nfields control electrons has always been attractive. It can be realized by\nsynthesizing pseudomagnetic fields (PMFs) in photonic crystals (PhCs). Previous\nworks mainly focus on the Landau levels and the robust transport of the chiral\nstates. More versatile control over light using complex nonuniform PMFs such as\nthe flexible splitting and routing of light has been elusive, which hinders\ntheir application in practical photonic integrated circuits. Here we propose an\nuniversal and systematic methodology to design nonuniform PMFs and arbitrarily\ncontrol the flow of light in silicon PhCs at telecommunication wavelengths. As\nproofs of concept, a low-loss S-bend and a highly efficient 50:50 power\nsplitter based on PMFs are experimentally demonstrated. A high-speed data\ntransmission experiment is performed on these devices to prove their\napplicability in real communication systems. The proposed method offers a new\nparadigm for the exploration of fundamental physics and the development of\nnovel nanophotonic devices.",
        "Electron acoustic waves (EAWs) are nonlinear plasma modes characterized by\nelectron trapping, which suppresses the usual Landau damping. Despite being\npredicted in the 1990s, their excitation and decay mechanisms remain a subject\nof active research. This study investigates the nonlinear dynamics of EAWs,\nfocusing on their excitation, decay instability, and the role of vortex merging\nin phase space. Using Eulerian Vlasov-Poisson simulations, we reproduce the\nexcitation of stable EAWs via an external resonant driving force and explore\ntheir decay under the effect of a low-amplitude perturbation. The study\nidentifies a distinct 2 to 1 decay process, where an EAW with a shorter\nwavelength merges into a longer-wavelength mode, driven by vortex dynamics in\nphase space. We find that the instability is triggered by a small fraction of\nparticles capable of transitioning between potential wells, facilitating energy\nexchange between two adjacent phase space holes and vortex merging. Our\nsimulations highlight the chaotic nature of particle trajectories in the\nvicinity of the separatrix between trapped and free phase space regions, which\nsignificantly contributes to the instability growth. Additionally, we analyze\nthe influence of the perturbation amplitude on the growth rate of the\ninstability, shedding light on the critical role of phase-space dynamics in the\ndecay process. These findings offer a deeper understanding of the nonlinear\nbehavior of plasma waves and suggest future directions for studying plasma wave\nstability in more complex systems, as the decay mechanism discussed here is\nlikely to be universal in plasmas with closed separatrices in phase space,\nunderscoring its significance in nonlinear plasma dynamics.",
        "In this paper, we present a unified flow approach to prescribed Chern scalar\ncurvature problem on compact Hermitian manifolds with negative Gauduchon\ndegree. When the conformal class of its Hermitian metric contains a balanced\nmetric, we give some sufficient conditions on the candidate curvature function\n$f$ which guaranties the convergence of the flow to a conformal Hermitian\nmetric whose Chern scalar curvature is $f$.",
        "This article consolidates and extends past work on derivative pricing\nadjustments, including XVA, by providing an encapsulating representation of the\nadjustment between any two derivative pricing functions, within an Ito\nSDE\/parabolic PDE framework. We give examples of this representation\nencapsulating others from the past 20 years, ranging from a well known option\npricing adjustment introduced by Gatheral, to the collection of\nsemi-replication XVA originating from Burgard & Kjaer. To highlight extensions,\nwe discuss certain meta-adjustments beyond XVA, designed to help signal and\nmitigate XVA model risk.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "In scenarios where high penetration of renewable energy sources (RES) is\nconnected to the grid over long distances, the output of RES exhibits\nsignificant fluctuations, making it difficult to accurately characterize. The\nintermittency and uncertainty of these fluctuations pose challenges to the\nstability of the power system. This paper proposes a distributionally robust\ndamping optimization control framework (DRDOC) to address the uncertainty in\nthe true distribution of random disturbances caused by RES. First, the\ninstallation location of damping controllers and key control parameters are\ndetermined through Sobol sensitivity indices and participation factors. Next, a\nnonlinear relationship between damping and random disturbances is established\nwith Polynomial Chaos Expansion (PCE). The uncertainty in the distribution of\ndisturbances is captured by ambiguity sets. The DRDOC is formulated as a convex\noptimization problem, which is further simplified for efficient computation.\nFinally, the optimal control parameters are derived through convex optimization\ntechniques. Simulation results demonstrate the effectiveness and distribution\nrobustness of the proposed DRDOC.",
        "Category-selective regions in the human brain, such as the fusiform face area\n(FFA), extrastriate body area (EBA), parahippocampal place area (PPA), and\nvisual word form area (VWFA), play a crucial role in high-level visual\nprocessing. Here, we investigate whether artificial neural networks (ANNs)\nexhibit similar category-selective neurons and how these neurons vary across\nmodel layers and between purely visual and vision-language models. Inspired by\nfMRI functional localizer experiments, we presented images from different\ncategories (faces, bodies, scenes, words, scrambled scenes, and scrambled\nwords) to deep networks and identified category-selective neurons using\nstatistical criteria. Comparing ResNet and the structurally controlled\nResNet-based CLIP model, we found that both models contain category-selective\nneurons, with their proportion increasing across layers, mirroring category\nselectivity in higher-level visual brain regions. However, CLIP exhibited a\nhigher proportion but lower specificity of category-selective neurons compared\nto ResNet. Additionally, CLIP's category-selective neurons were more evenly\ndistributed across feature maps and demonstrated greater representational\nconsistency across layers. These findings suggest that language learning\nincreases the number of category-selective neurons while reducing their\nselectivity strength, reshaping visual representations in deep networks. Our\nstudy provides insights into how ANNs mirror biological vision and how\nmultimodal learning influences category-selective representations.",
        "Realizing topological flat bands with tailored single-particle Hilbert spaces\nis a critical step toward exploring many-body phases, such as those featuring\nanyonic excitations. One prominent example is the Kapit-Mueller model, a\nvariant of the Harper-Hofstadter model that stabilizes lattice analogs of the\nlowest Landau level states. The Kapit-Mueller model is constructed based on the\nPoisson summation rule, an exact lattice sum rule for coherent states. In this\nwork, we consider higher Landau-level generalizations of the Poisson summation\nrule, from which we derive families of parent Hamiltonians on a half-flux\nlattice which have exact flat bands whose flatband wavefunctions are lattice\nversion of higher Landau level states. Focusing on generic Bravais lattices\nwith only translation and inversion symmetries, we discuss how these symmetries\nenforced gaplessness and singular points for odd Landau level series, and how\nto achieve fully gapped parent Hamiltonians by mixing even and odd series. Our\nmodel points to a large class of tight-binding models with suitable energetic\nand quantum geometries that are potentially useful for realizing non-Abelian\nfractionalized states when interactions are included. The model exhibits fast\ndecay hopping amplitudes, making it potentially realizable with neutral atoms\nin optical lattices.",
        "We combined Gaia DR3 and TESS photometric light curves to estimate the\ninternal physical properties of 2,497 gravity-mode pulsators. We relied on\nasteroseismic properties of Kepler $\\gamma\\,$Dor and SPB stars to derive the\nnear-core rotation frequency, $f_{\\rm rot}$, of the Gaia-discovered pulsators\nfrom their dominant prograde dipole gravito-inertial pulsation mode. We offer a\nrecipe based on linear regression to deduce $f_{\\rm rot}$ from the dominant\ngravito-inertial mode frequency. It is applicable to prograde dipole modes with\nan amplitude above 4mmag and occurring in the sub-inertial regime. By applying\nit to the 2,497 pulsators, we have increased the sample of intermediate-mass\ndwarfs with such an asteroseismic observable by a factor of 4. We used the\nestimate of $f_{\\rm rot}$ to deduce spin parameters between 2 and 6, while the\nsample's near-core rotation rates range from 0.7% to 25% of the critical\nKeplerian rate. We used $f_{\\rm rot}$, along with the Gaia effective\ntemperature and luminosity to deduce the (convective core) mass, radius, and\nevolutionary stage from grid modelling based on rotating stellar models. We\nderived a decline of $f_{\\rm rot}$ with a factor of 2 during the main-sequence\nevolution for this population of field stars, which covers a mass range from\n1.3M$_\\odot$ to 7M$_\\odot$. We found observational evidence for an increase in\nthe radial order of excited gravity modes as the stars evolve. For 969\npulsators, we derived an upper limit of the radial differential rotation\nbetween the convective core boundary and the surface from Gaia's vbroad\nmeasurement and found values up to 5.4. Our recipe for the near-core rotation\nfrequency from the dominant gravito-inertial mode detected in the independent\nGaia and TESS light curves is easy to use, facilitates applications to large\nsamples, and allows to map their angular momentum and evolutionary stage in the\nMilky Way.",
        "We prove that it is consistent with ZFC that for every non-decreasing\nfunction $f:[0,1]\\to [0,1]$, each subset of $[0,1]$ of cardinality $\\mathfrak\nc$ contains a set of cardinality $\\mathfrak c$ on which $f$ is uniformly\ncontinuous. We show that this statement follows from the assumptions that\n$\\mathfrak d^* < \\mathfrak c$ and $\\mathfrak c$ is regular, where $\\mathfrak\nd^*\\leq \\mathfrak d$ is the smallest cardinality $\\kappa$ such that any two\ndisjoint countable dense sets in the Cantor set can be separated by sets each\nof which is an intersection of at most $\\kappa$-many open sets in the Cantor\nset. We establish also that $\\mathfrak d^*=\\min\\{\\mathfrak u, \\mathfrak\nd\\}=\\min\\{\\mathfrak r, \\mathfrak d\\}$, thus giving an alternative proof of the\nlatter equality established by J. Aubrey in 2004.",
        "We examine two in context learning (ICL) tasks with mathematical functions in\nseveral train and test settings for transformer models. Our study generalizes\nwork on linear functions by showing that small transformers, even models with\nattention layers only, can approximate arbitrary polynomial functions and hence\ncontinuous functions under certain conditions. Our models also can approximate\npreviously unseen classes of polynomial functions, as well as the zeros of\ncomplex functions. Our models perform far better on this task than LLMs like\nGPT4 and involve complex reasoning when provided with suitable training data\nand methods. Our models also have important limitations; they fail to\ngeneralize outside of training distributions and so don't learn class forms of\nfunctions. We explain why this is so."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Addressing disparities in the global epidemiology of stroke",
    "start_abstract":"Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Artificial intelligence applications in stroke"
      ],
      "abstract":[
        "Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Complex Brillouin Zone for Localised Modes in Hermitian and\n  Non-Hermitian Problems",
        "Explicit adaptive time stepping for the Cahn-Hilliard equation by\n  exponential Krylov subspace and Chebyshev polynomial methods",
        "On reflected isotropic stable processes",
        "Quantum Quandaries: Unraveling Encoding Vulnerabilities in Quantum\n  Neural Networks",
        "Evaluating Compression and Nanoindentation in FCC Nickel: A Methodology\n  for Interatomic Potential Selection",
        "Integrated Sensing and Communication System Based on Radio Frequency\n  Resonance Beam",
        "Activity of low mass stars in the light of spot signature in the Fourier\n  domain",
        "Impact of structural distortions on the correlated electronic structure\n  of orbital-selective Mott insulating Na$_3$Co$_2$SbO$_6$ under strains",
        "Influence of nanoparticulates and microgrooves on the secondary electron\n  yield and electrical resistance of laser-treated copper surfaces",
        "Accelerated DC loadflow solver for topology optimization",
        "Predicting Spin-Dependent Coulomb Interaction Based on the Yang-Mills\n  Equations",
        "Experimental realization of a quantum heat engine based on\n  dissipation-engineered superconducting circuits",
        "Range-Only Dynamic Output Feedback Controller for Safe and Secure Target\n  Circumnavigation",
        "Quantum Diffie-Hellman key exchange",
        "Criteria for ion acceleration in laboratory magnetized\n  quasi-perpendicular collisionless shocks: when are 2D simulations enough?",
        "Repulsive interatomic potentials calculated at three levels of theory",
        "Language Model Re-rankers are Steered by Lexical Similarities",
        "Parametric instability of ultracold Bose gases with long-range\n  interaction and quantum fluctuations trapped in optical lattices",
        "A heuristic for the deployment of collecting routes for urban recycle\n  stations (eco-points)",
        "AF-KAN: Activation Function-Based Kolmogorov-Arnold Networks for\n  Efficient Representation Learning",
        "On sparsity of integral points in orbits and correspondences with big\n  pullbacks under iterates",
        "Adaptive Score Alignment Learning for Continual Perceptual Quality\n  Assessment of 360-Degree Videos in Virtual Reality",
        "Towards Multi-Stakeholder Evaluation of ML Models: A Crowdsourcing Study\n  on Metric Preferences in Job-matching System",
        "Primordial Black Hole Formation via Inverted Bubble Collapse",
        "Integrating Language Models for Enhanced Network State Monitoring in\n  DRL-Based SFC Provisioning",
        "On the examples of Egyptian fractions in Liber Abaci",
        "New Fashion Products Performance Forecasting: A Survey on Evolutions,\n  Models and Emerging Trends",
        "A Taxonomy for Evaluating Generalist Robot Policies",
        "Skill and spatial mismatches for sustainable development in Brazil"
      ],
      "abstract":[
        "We develop a mathematical and numerical framework for studying evanescent\nwaves in subwavelength band gap materials. By establishing a link between the\ncomplex Brillouin zone and various Hermitian and non-Hermitian phenomena,\nincluding defect localisation in band gap materials and the non-Hermitian skin\neffect, we provide a unified perspective on these systems. In two-dimensional\nstructures, we develop analytical techniques and numerical methods to study\nsingularities of the complex band structure. This way, we demonstrate that gap\nfunctions effectively predict the decay rates of defect states. Furthermore, we\npresent an analysis of the Floquet transform with respect to complex\nquasimomenta. Based on this, we show that evanescent waves may undergo a phase\ntransition, where local oscillations drastically depend on the location of\ncorresponding frequency inside the band gap.",
        "The Cahn-Hilliard equation has been widely employed within various\nmathematical models in physics, chemistry and engineering. Explicit stabilized\ntime stepping methods can be attractive for time integration of the\nCahn-Hilliard equation, especially on parallel and hybrid supercomputers. In\nthis paper, we propose an exponential time integration method for the\nCahn-Hilliard equation and describe its efficient Krylov subspace based\nimplementation. We compare the method to a Chebyshev polynomial local iteration\nmodified (LIM) time stepping scheme. Both methods are explicit (i.e., do not\ninvolve linear system solution) and tested with both constant and adaptively\nchosen time steps.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "Quantum computing (QC) has the potential to revolutionize fields like machine\nlearning, security, and healthcare. Quantum machine learning (QML) has emerged\nas a promising area, enhancing learning algorithms using quantum computers.\nHowever, QML models are lucrative targets due to their high training costs and\nextensive training times. The scarcity of quantum resources and long wait times\nfurther exacerbate the challenge. Additionally, QML providers may rely on third\nparty quantum clouds for hosting models, exposing them and their training data\nto potential threats. As QML as a Service (QMLaaS) becomes more prevalent,\nreliance on third party quantum clouds poses a significant security risk. This\nwork demonstrates that adversaries in quantum cloud environments can exploit\nwhite box access to QML models to infer the users encoding scheme by analyzing\ncircuit transpilation artifacts. The extracted data can be reused for training\nclone models or sold for profit. We validate the proposed attack through\nsimulations, achieving high accuracy in distinguishing between encoding\nschemes. We report that 95% of the time, the encoding can be predicted\ncorrectly. To mitigate this threat, we propose a transient obfuscation layer\nthat masks encoding fingerprints using randomized rotations and entanglement,\nreducing adversarial detection to near random chance 42% , with a depth\noverhead of 8.5% for a 5 layer QNN design.",
        "We performed molecular dynamics simulations to investigate the mechanical\nresponse of face-centered cubic (FCC) nickel under uniaxial compression and\nnanoindentation using traditional interatomic potentials, including the\nEmbedded Atom Method (EAM) and Modified Embedded Atom Method (MEAM). By\ncalculating the generalized stacking fault energy (GSFE), we analyzed the\ndissociated slip paths responsible for stacking fault formation and partial\nShockley dislocations during mechanical loading. Our findings highlight the\ncritical importance of selecting appropriate interatomic potentials to model\ncompression and nanoindentation tests accurately, aligning simulations with\nexperimental observations. We propose a practical methodology for identifying\nempirical interatomic potentials suitable for mechanical testing of\nsingle-element materials. This approach establishes a benchmark for FCC nickel\nsimulations and provides a basis for extending these methods to more complex\nNi-based alloys, facilitating comparisons with experimental results such as\nthose from electron microscopy.",
        "To address the challenge of complex beam control in traditional\nmultiple-input multiple-output (MIMO) systems, research has proposed\nestablishing adaptive beam alignment by utilizing retro-directive antenna (RDA)\narrays to create echo resonance between the base station (BS) and user\nequipment (UE), thereby reducing system computational load. However, in\nconventional resonant beam systems (RBS), the use of the same frequency for\nuplink and downlink inevitably leads to echo interference issues. Therefore,\nthis paper proposes an innovative design for an resonance beam-based integrated\nsensing and communication (RB-ISAC) system to achieve efficient passive sensing\nand bidirectional communication. In this system, the UE does not actively\ntransmit signals but instead relies on a passive phase conjugation and\nfrequency conversion structure to separate the uplink and downlink carrier\nfrequencies. Additionally, through effective compensation for signal\npropagation loss, resonance is achieved after multiple iterations, at which\npoint the beam's field distribution becomes a low-diffraction-loss,\nhigh-focusing pattern, automatically aligning the transmitter with the\nreceiver. This enables high-precision passive positioning while facilitating\nuplink and downlink communication. Simulation results demonstrate that the\nproposed system can achieve resonance after multiple iterations, and can\nsupport uplink and downlink communication within a range of 5 meters while\nachieving passive direction of arrival (DOA) estimation with an error of less\nthan 2 degrees.",
        "Context. Magnetic fields exhibit a wide variety of behaviours in low mass\nstars and further characterization is required to understand these\nobservations. Stellar photometry from space missions such as MOST, CoRoT,\nKepler, and, in future PLATO, provide thousands of highly precise light curves\n(LC) that can shed new light upon stellar activity, in particular through the\nsignature of transiting spots. Aims. We study the impact of star spots on light\ncurves in the Fourier domain, reducing the degeneracies encountered by direct\nspot modelling in the temporal domain, and use this new formulation to explore\nthe spot properties from the available data. Methods. We propose a model of LC\npower spectra at low frequency based on a description of spot transits that\nallows us to retrieve information about the amplitude of their photometric\nimpact $\\mathcal{H}$, and about the spot mean lifetime over the observation\n$\\tau_{\\rm life}$ when the power spectrum exibits rotation peaks. We first\nvalidate this method with simulated LCs and then apply it to the Kepler data to\nextract global trends over a set of more than 37 755 stars. Results. This\nanalysis leads to a classification of the sample into \"peakless\" or \"with\npeaks\" spectra, and enables the identification of different activity regimes\nbased on $\\mathcal{H}$ and $\\tau_{\\rm life}$ for different ranges of Rossby\nnumber. More specifically, we observe an intense regime of activity between Ro\n= 0.7 and Ro = 1, for stars with masses under 1$M_\\odot$. Conclusions. This new\nsystematic method can be used to provide new observational constraints on\nstellar activity (and possibly a link with stellar magnetism) when applied to\nlarge photometric datasets, such as those from the future PLATO mission.",
        "Na$_{3}$Co$_{2}$SbO$_6$ is a promising candidate to realize the Kitaev spin\nliquid phase since the large Kitaev spin exchange interaction is tunable via\nthe change in electronic structure, such as the trigonal crystal field\nsplitting ($\\Delta_{TCF}$). Here, we show that the uncorrelated electronic\nstructure of Na$_{3}$Co$_{2}$SbO$_6$ is rather insensitive to the strain effect\ndue to the low crystal symmetry accompanied by oxygen displacements and the\npresence of Sb $s$ orbitals. This suggests that the Kitaev spin-exchange\ninteraction obtained from perturbation theory also does not depend much on the\nstrain effect. Using density functional theory plus dynamical mean field\ntheory, we find that the correlated electronic structure of\nNa$_{3}$Co$_{2}$SbO$_6$ is an orbital selective Mott insulating state where the\ntrigonal $a_{1g}$ orbital is insulating due to correlation-assisted\nhybridization, while other $d$ orbitals behave as typical Mott insulators,\nresulting in tunability of $\\Delta_{TCF}$ under the strain effect effectively.\nOur results show that the local Co-site symmetry and dynamical correlation\neffects will play an important role in engineering the novel magnetic phase in\nthis and related materials.",
        "Laser surface structuring has proven to be an effective technique for\nachieving a copper surface with secondary electron yield (SEY) values close to\nor below unity. However, the attributes that minimize SEY, such as moderately\ndeep grooves and redeposited nanoparticles, may lead to undesirable\nconsequences, including increased radio frequency surface resistance. This\ninvestigation systematically examined data about different cleaning procedures\ndesigned to eliminate redeposited adsorbed particulates. Various analysis\ntechniques were used iteratively after each consecutive cleaning step,\nproviding insights into the evolving surface characteristics. The collected\nexperimental results identified distinct impacts of microgrooves, groove\norientation, and associated particulates on secondary electron yield and\nsurface resistance. Exposing the crests while retaining high particulate\ncoverage in the grooves leads to reduced SEY values and surface resistance,\nsuggesting that the tips of the grooves exert a more significant influence on\nsurface current density than the groove depth. At the same time, nanoparticles\nin the grooves have a more significant impact on SEY values than the exposed\ntips at the surface.",
        "We present a massively parallel solver that accelerates DC loadflow\ncomputations for power grid topology optimization tasks. Our approach leverages\nlow-rank updates of the Power Transfer Distribution Factors (PTDFs) to\nrepresent substation splits, line outages, and reconfigurations without ever\nrefactorizing the system. Furthermore, we implement the core routines on\nGraphics Processing Units (GPUs), thereby exploiting their high-throughput\narchitecture for linear algebra. A two-level decomposition separates changes in\nbranch topology from changes in nodal injections, enabling additional speed-ups\nby an in-the-loop brute force search over injection variations at minimal\nadditional cost. We demonstrate billion-loadflow-per-second performance on\npower grids of varying sizes in workload settings which are typical for\ngradient-free topology optimization such as Reinforcement Learning or Quality\nDiversity methods. While adopting the DC approximation sacrifices some accuracy\nand prohibits the computation of voltage magnitudes, we show that this\nsacrifice unlocks new scales of computational feasibility, offering a powerful\ntool for large-scale grid planning and operational topology optimization.",
        "The standard Coulomb interaction is one of four fundamental interactions in\nNature. It is interesting to know how will the standard Coulomb interaction be\nmodified when it meets spin. Since the standard Coulomb potential is a simple\nbut fundamental solution of Maxwell's equations, hence Maxwell's equations can\npredict the existence of the standard Coulomb potential. The Yang-Mills\nequations are the natural generalizations of Maxwell's equations from the\nAbelian potentials to the non-Abelian ones, thus based on the Yang-Mills\nequations, one can predict the reasonable form of spin-dependent Coulomb\npotential, which naturally reduces the standard Coulomb potential if without\nconsidering the spin. Our work sheds a new light to how to couple spin with\nfundamental interactions.",
        "Quantum heat engines (QHEs) have attracted long-standing scientific interest,\nespecially inspired by considerations of the interplay between heat and work\nwith the quantization of energy levels, quantum superposition, and\nentanglement. Operating QHEs calls for effective control of the thermal\nreservoirs and the eigenenergies of the quantum working medium of the engine.\nAlthough superconducting circuits enable accurate engineering of controlled\nquantum systems, beneficial in quantum computing, this framework has not yet\nbeen employed to experimentally realize a cyclic QHE. Here, we experimentally\ndemonstrate a quantum heat engine based on superconducting circuits, using a\nsingle-junction quantum-circuit refrigerator (QCR) as a two-way tunable heat\nreservoir coupled to a flux-tunable transmon qubit acting as the working medium\nof the engine. We implement a quantum Otto cycle by a tailored drive on the QCR\nto sequentially induce cooling and heating, interleaved with flux ramps that\ncontrol the qubit frequency. Utilizing single-shot qubit readout, we monitor\nthe evolution of the qubit state during several cycles of the heat engine and\nmeasure positive output powers and efficiencies that agree with our simulations\nof the quantum evolution. Our results verify theoretical models on the\nthermodynamics of quantum heat engines and advance the control of\ndissipation-engineered thermal environments. These proof-of-concept results\npave the way for explorations on possible advantages of QHEs with respect to\nclassical heat engines.",
        "The safety and security of robotic systems are paramount when navigating\naround a hostile target. This paper addresses the problem of circumnavigating\nan unknown target by a unicycle robot while ensuring it maintains a desired\nsafe distance and remains within the sensing region around the target\nthroughout its motion. The proposed control design methodology is based on the\nconstruction of a joint Lyapunov function that incorporates: (i) a quadratic\npotential function characterizing the desired target-circumnavigation\nobjective, and (ii) a barrier Lyapunov function-based potential term to enforce\nsafety and sensing constraints on the robot's motion. A notable feature of the\nproposed control design is its reliance exclusively on local range measurements\nbetween the robot and the target, realized using a dynamic output feedback\ncontroller that treats the range as the only observable output for feedback.\nUsing the Lyapunov stability theory, we show that the desired equilibrium of\nthe closed-loop system is asymptotically stable, and the prescribed safety and\nsecurity constraints are met under the proposed controllers. We also obtain\nrestrictive bounds on the post-design signals and provide both simulation and\nexperimental results to validate the theoretical contributions.",
        "The Diffie-Hellman key exchange plays a crucial role in conventional\ncryptography, as it allows two legitimate users to establish a common, usually\nephemeral, secret key. Its security relies on the discrete-logarithm problem,\nwhich is considered to be a mathematical one-way function, while the final key\nis formed by random independent actions of the two users. In the present work\nwe investigate the extension of Diffie-Hellman key exchange to the quantum\nsetting, where the two legitimate users exchange independent random quantum\nstates. The proposed protocol relies on the bijective mapping of integers onto\na set of symmetric coherent states, and we investigate the regime of parameters\nfor which the map behaves as a quantum one-way function. Its security is\nanalyzed in the framework of minimum-error-discrimination and\nphoton-number-splitting attacks, while its performance and the challenges in a\npossible realization are also discussed.",
        "The study of collisionless shocks and their role in cosmic ray acceleration\nhas gained importance through observations and simulations, driving interest in\nreproducing these conditions in laboratory experiments using high-power lasers.\nIn this work, we examine the role of three-dimensional (3D) effects in ion\nacceleration in quasi-perpendicular shocks under laboratory-relevant\nconditions. Using hybrid particle-in-cell simulations (kinetic ions and fluid\nelectrons), we explore how the Alfv\\'enic and sonic Mach numbers, along with\nplasma beta, influence ion energization, unlocked only in 3D, and establish\nscaling criteria for when conducting 3D simulations is necessary. Our results\nshow that efficient ion acceleration requires Alfv\\'enic Mach numbers $\\geq 25$\nand sonic Mach numbers $\\geq 13$, with plasma-$\\beta \\leq 5$. We theoretically\nfound that, while 2D simulations suffice for current laboratory-accessible\nshock conditions, 3D effects become crucial for shock velocities exceeding 1000\nkm\/s and experiments sustaining the shock for at least 10 ns. We surveyed\nprevious laboratory experiments on collisionless shocks and found that 3D\neffects are unimportant under those conditions, implying that 1D and 2D\nsimulations should be enough to model the accelerated ion spectra. However, we\ndo find that the same experiments are realistically close to accessing the\nregime relevant to 3D effects, an exciting prospect for future laboratory\nefforts. We propose modifications to past experimental configurations to\noptimize and control 3D effects on ion acceleration. These proposed experiments\ncould be used to benchmark plasma astrophysics kinetic codes and\/or employed as\ncontrollable sources of energetic particles.",
        "The high-energy repulsive interaction between nuclei at distances much\nsmaller than the equilibrium bond length is the key quantity determining the\nnuclear stopping power and atom scattering in keV and MeV radiation events.\nThis interaction is traditionally modeled within orbital-free density\nfunctional theory with frozen atomic electron densities, following the\nZiegler-Biersack-Littmark (ZBL) model. In this work, we calculate atom pair\nspecific repulsive interatomic potentials with the ZBL model, and compare them\nto two kinds of quantum chemical calculations - second-order M{\\o}ller-Plesset\nperturbation theory in flexible Gaussian basis sets as well as density\nfunctional theory with numerical atomic orbital basis sets - which go well\nbeyond the limitations in the ZBL model, allowing the density to relax in the\ncalculations. We show that the repulsive interatomic potentials predicted by\nthe two quantum chemical models agree within $\\sim$ 1% for potential energies\nabove 30 eV, while the ZBL pair-specific potentials and universal ZBL\npotentials differ much more from either of these calculations. We provide new\npair-specific fits of the screening functions in terms of 3 exponentials to the\ncalculations for all pairs $Z_1$-$Z_2$ for $1 \\leq Z_i \\leq 92$, and show that\nthey agree within $\\sim 2$% with the raw data. We use the new potentials to\nsimulate ion implantation depth profiles in single crystalline Si and show very\ngood agreement with experiment. However, we also show that under channeling\nconditions, the attractive part of the potential can affect the depth profiles.\nThe full data sets of all the calculated interatomic potentials as well as\nanalytic fits to the data are shared as open access.",
        "Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information.\nTo understand whether LM re-rankers always live up to this assumption, we\nevaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our\nresults show that LM re-rankers struggle to outperform a simple BM25 re-ranker\non DRUID. Leveraging a novel separation metric based on BM25 scores, we explain\nand identify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation.",
        "We investigate the dynamical instabilities of an ultracold Bose-Bose mixture\nwith long-range dipole-dipole interactions, trapped in deep optical lattices\nand subject to periodically varying contact interaction. The effect of\nbeyond-mean-field corrections due to quantum fluctuations is considered. In the\ntight-binding regime, we employ Wannier functions to derive a discrete\nnonlinear Schr\\\"odinger equation that captures the dynamics of the system.\nUsing linear stability analysis coupled to multiple scale expansion, we\nsystematically study the modulational and parametric instabilities of the\nsystem, identifying the conditions under which these instabilities emerge. The\ncorresponding instability domains are found and examined. The roles of\nlong-range interactions and quantum fluctuations are highlighted, demonstrating\ntheir significant impact on the stability and dynamics of the lattice system.\nOur analytical predictions are validated through direct numerical simulations,\nwhich confirm the instability diagrams through the effective onset of\ninstabilities and reveal the intricate interplay between interaction strength,\nquantum fluctuation, and the long-range nature of the dipole-dipole forces.\nThis work provides insights into the control and manipulation of ultracold\nquantum gases in optical lattices, which have potential applications in quantum\nsimulation and condensed matter physics.",
        "The rapid and constant increase in urban population has led to a drastic rise\nin urban solid waste production with worrying consequences for the environment\nand society. In many cities, an efficient waste management combined with a\nsuitable design of vehicle routes (VR) can lead to benefits in the\nenvironmental, economic, and social impacts. The general population is becoming\nincreasingly aware of the need for the separation of the various categories of\nmunicipal solid waste. The numerous materials collected include glass, PET or\nbatteries, and electric components, which are sorted at the eco-points. The\nmanagement of eco-points gives rise to several problems that can be formulated\nanalytically. The location and number of eco-point containers, the\ndetermination of the fleet size for picking up the collected waste, and the\ndesign of itineraries are all intertwined, and present computationally\ndifficult problems, and therefore must be solved in a sequential way. In this\npaper, a mathematical model has been formulated, based on the Bin Packing (BP)\nand VR schemes, for the deployment of routes of mobile containers in the\nselective collection of urban solid waste. A heuristic algorithm has also been\ndeveloped, which considers two different configurations of the containers to\nsolve the proposed mathematical programming model. The results obtained from\nthe numerical simulations show the validation of the proposed methodology\ncarried out for the benchmark of the Sioux Falls network and the specific real\ncase study.",
        "Kolmogorov-Arnold Networks (KANs) have inspired numerous works exploring\ntheir applications across a wide range of scientific problems, with the\npotential to replace Multilayer Perceptrons (MLPs). While many KANs are\ndesigned using basis and polynomial functions, such as B-splines, ReLU-KAN\nutilizes a combination of ReLU functions to mimic the structure of B-splines\nand take advantage of ReLU's speed. However, ReLU-KAN is not built for multiple\ninputs, and its limitations stem from ReLU's handling of negative values, which\ncan restrict feature extraction. To address these issues, we introduce\nActivation Function-Based Kolmogorov-Arnold Networks (AF-KAN), expanding\nReLU-KAN with various activations and their function combinations. This novel\nKAN also incorporates parameter reduction methods, primarily attention\nmechanisms and data normalization, to enhance performance on image\nclassification datasets. We explore different activation functions, function\ncombinations, grid sizes, and spline orders to validate the effectiveness of\nAF-KAN and determine its optimal configuration. In the experiments, AF-KAN\nsignificantly outperforms MLP, ReLU-KAN, and other KANs with the same parameter\ncount. It also remains competitive even when using fewer than 6 to 10 times the\nparameters while maintaining the same network structure. However, AF-KAN\nrequires a longer training time and consumes more FLOPs. The repository for\nthis work is available at https:\/\/github.com\/hoangthangta\/All-KAN.",
        "We prove new unconditional results of sparsity of integral points on orbits\nunder many maps and correspondences in arbitrary dimensions, generalizing\ntheorems of Yasufuku(2015) and others. The main ingredients are new diophantine\napproximation tools and recent constructions for correspondences due to Ingram\n(2011).",
        "Virtual Reality Video Quality Assessment (VR-VQA) aims to evaluate the\nperceptual quality of 360-degree videos, which is crucial for ensuring a\ndistortion-free user experience. Traditional VR-VQA methods trained on static\ndatasets with limited distortion diversity struggle to balance correlation and\nprecision. This becomes particularly critical when generalizing to diverse VR\ncontent and continually adapting to dynamic and evolving video distribution\nvariations. To address these challenges, we propose a novel approach for\nassessing the perceptual quality of VR videos, Adaptive Score Alignment\nLearning (ASAL). ASAL integrates correlation loss with error loss to enhance\nalignment with human subjective ratings and precision in predicting perceptual\nquality. In particular, ASAL can naturally adapt to continually changing\ndistributions through a feature space smoothing process that enhances\ngeneralization to unseen content. To further improve continual adaptation to\ndynamic VR environments, we extend ASAL with adaptive memory replay as a novel\nContinul Learning (CL) framework. Unlike traditional CL models, ASAL utilizes\nkey frame extraction and feature adaptation to address the unique challenges of\nnon-stationary variations with both the computation and storage restrictions of\nVR devices. We establish a comprehensive benchmark for VR-VQA and its CL\ncounterpart, introducing new data splits and evaluation metrics. Our\nexperiments demonstrate that ASAL outperforms recent strong baseline models,\nachieving overall correlation gains of up to 4.78\\% in the static joint\ntraining setting and 12.19\\% in the dynamic CL setting on various datasets.\nThis validates the effectiveness of ASAL in addressing the inherent challenges\nof VR-VQA.Our code is available at https:\/\/github.com\/ZhouKanglei\/ASAL_CVQA.",
        "While machine learning (ML) technology affects diverse stakeholders, there is\nno one-size-fits-all metric to evaluate the quality of outputs, including\nperformance and fairness. Using predetermined metrics without soliciting\nstakeholder opinions is problematic because it leads to an unfair disregard for\nstakeholders in the ML pipeline. In this study, to establish practical ways to\nincorporate diverse stakeholder opinions into the selection of metrics for ML,\nwe investigate participants' preferences for different metrics by using\ncrowdsourcing. We ask 837 participants to choose a better model from two\nhypothetical ML models in a hypothetical job-matching system twenty times and\ncalculate their utility values for seven metrics. To examine the participants'\nfeedback in detail, we divide them into five clusters based on their utility\nvalues and analyze the tendencies of each cluster, including their preferences\nfor metrics and common attributes. Based on the results, we discuss the points\nthat should be considered when selecting appropriate metrics and evaluating ML\nmodels with multiple stakeholders.",
        "We propose a novel mechanism of primordial black hole (PBH) formation through\ninverted bubble collapse. In this scenario, bubbles nucleate sparsely in an\nincomplete first-order phase transition, followed by a bulk phase transition in\nthe rest of the universe that inverts these pre-existing bubbles into false\nvacuum regions. These spherically symmetric false-vacuum bubbles subsequently\ncollapse to form PBHs. Unlike conventional PBH formation mechanisms associated\nwith domain wall collapse or bubble coalescence, our inverted bubble collapse\nmechanism naturally ensures spherical collapse. We demonstrate that, when\napplied to the singlet extension of the Standard Model, this mechanism can\nproduce highly monochromatic PBHs with masses up to ${\\cal\nO}(10^{-7}\\,\\text{-}\\,10^{-5}) M_\\odot$, which potentially explain the\nmicrolensing events observed in the OGLE and Subaru HSC data.",
        "Efficient Service Function Chain (SFC) provisioning and Virtual Network\nFunction (VNF) placement are critical for enhancing network performance in\nmodern architectures such as Software-Defined Networking (SDN) and Network\nFunction Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids\ndecision-making in dynamic network environments, its reliance on structured\ninputs and predefined rules limits adaptability in unforeseen scenarios.\nAdditionally, incorrect actions by a DRL agent may require numerous training\niterations to correct, potentially reinforcing suboptimal policies and\ndegrading performance. This paper integrates DRL with Language Models (LMs),\nspecifically Bidirectional Encoder Representations from Transformers (BERT) and\nDistilBERT, to enhance network management. By feeding final VNF allocations\nfrom DRL into the LM, the system can process and respond to queries related to\nSFCs, DCs, and VNFs, enabling real-time insights into resource utilization,\nbottleneck detection, and future demand planning. The LMs are fine-tuned to our\ndomain-specific dataset using Low-Rank Adaptation (LoRA). Results show that\nBERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and\nhigher confidence (0.83 compared to 0.74), though BERT requires approximately\n46% more processing time.",
        "The focus of this note is to formulate the algorithms and give the examples\nused by Fibonacci in Liber Abaci to expand any fraction into a sum of unit\nfractions. The description in Liber Abaci is all verbal and the examples are\nnumbers which may lead to different algorithmic descriptions with the same\ninput and results. An additional complication is that the manuscripts that\nexist are copies of an older manuscript and in the process new errors are\nintroduced. Additional errors may also be introduced in the transcript and the\ntranslation. Fibonacci introduces seven categories each with several numerical\nexamples. We give a precise description of the computational procedure using\nstandard mathematical notation.",
        "The fast fashion industry's insatiable demand for new styles and rapid\nproduction cycles has led to a significant environmental burden.\nOverproduction, excessive waste, and harmful chemicals have contributed to the\nnegative environmental impact of the industry. To mitigate these issues, a\nparadigm shift that prioritizes sustainability and efficiency is urgently\nneeded. Integrating learning-based predictive analytics into the fashion\nindustry represents a significant opportunity to address environmental\nchallenges and drive sustainable practices. By forecasting fashion trends and\noptimizing production, brands can reduce their ecological footprint while\nremaining competitive in a rapidly changing market. However, one of the key\nchallenges in forecasting fashion sales is the dynamic nature of consumer\npreferences. Fashion is acyclical, with trends constantly evolving and\nresurfacing. In addition, cultural changes and unexpected events can disrupt\nestablished patterns. This problem is also known as New Fashion Products\nPerformance Forecasting (NFPPF), and it has recently gained more and more\ninterest in the global research landscape. Given its multidisciplinary nature,\nthe field of NFPPF has been approached from many different angles. This\ncomprehensive survey wishes to provide an up-to-date overview that focuses on\nlearning-based NFPPF strategies. The survey is based on the Preferred Reporting\nItems for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,\nallowing for a systematic and complete literature review. In particular, we\npropose the first taxonomy that covers the learning panorama for NFPPF,\nexamining in detail the different methodologies used to increase the amount of\nmultimodal information, as well as the state-of-the-art available datasets.\nFinally, we discuss the challenges and future directions.",
        "Machine learning for robotics promises to unlock generalization to novel\ntasks and environments. Guided by this promise, many recent works have focused\non scaling up robot data collection and developing larger, more expressive\npolicies to achieve this. But how do we measure progress towards this goal of\npolicy generalization in practice? Evaluating and quantifying generalization is\nthe Wild West of modern robotics, with each work proposing and measuring\ndifferent types of generalization in their own, often difficult to reproduce,\nsettings. In this work, our goal is (1) to outline the forms of generalization\nwe believe are important in robot manipulation in a comprehensive and\nfine-grained manner, and (2) to provide reproducible guidelines for measuring\nthese notions of generalization. We first propose STAR-Gen, a taxonomy of\ngeneralization for robot manipulation structured around visual, semantic, and\nbehavioral generalization. We discuss how our taxonomy encompasses most prior\nnotions of generalization in robotics. Next, we instantiate STAR-Gen with a\nconcrete real-world benchmark based on the widely-used Bridge V2 dataset. We\nevaluate a variety of state-of-the-art models on this benchmark to demonstrate\nthe utility of our taxonomy in practice. Our taxonomy of generalization can\nyield many interesting insights into existing models: for example, we observe\nthat current vision-language-action models struggle with various types of\nsemantic generalization, despite the promise of pre-training on internet-scale\nlanguage datasets. We believe STAR-Gen and our guidelines can improve the\ndissemination and evaluation of progress towards generalization in robotics,\nwhich we hope will guide model design and future data collection efforts. We\nprovide videos and demos at our website stargen-taxonomy.github.io.",
        "Structural change is necessary for all countries transitioning to a more\nenvironmentally sustainable economy, but what are the likely impacts on\nworkers? Studies often find that green transition scenarios result in net\npositive job creation numbers overall but rarely provide insights into the more\ngranular dynamics of the labour market. This paper combines a dynamic labour\nmarket simulation model with development scenarios focused on agriculture and\ngreen manufacturing. We study how, within the context of a green transition,\nproductivity shifts in different sectors and regions, with differing\nenvironmental impacts, may affect and be constrained by the labour market in\nBrazil. By accounting for labour market frictions associated with skill and\nspatial mismatches, we find that productivity shocks, if not well managed, can\nexacerbate inequality. Agricultural workers tend to be the most negatively\naffected as they are less occupationally and geographically mobile. Our results\nhighlight the importance of well-targeted labour market policies to ensure the\ngreen transition is just and equitable."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Artificial intelligence applications in stroke",
    "start_abstract":"Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Addressing disparities in the global epidemiology of stroke"
      ],
      "abstract":[
        "Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Multidimensional moment problem and Stieltjes transform",
        "Algebraic solution of the Jacobi inverse problem and explicit addition\n  laws",
        "Yang-Mills-Utiyama Theory and Graviweak Correspondence",
        "Measuring Top Yukawa Coupling through $2\\rightarrow 3$ VBS at Muon\n  Collider",
        "Phonons in Electron Crystals with Berry Curvature",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing\n  Recovery Rate Predictions",
        "A Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE)\n  XVII. Statistical properties of individual HII regions in unperturbed systems",
        "Birth of magnetized low-mass protostars and circumstellar disks",
        "The soccer model, stochastic ordering and martingale transport",
        "MNE: overparametrized neural evolution with applications to diffusion\n  processes and sampling",
        "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading\n  Strategies with Temporal Kolmogorov-Arnold Networks",
        "On generalized Tur{\\'a}n problems with bounded matching number and\n  circumference",
        "The sliding tile puzzle, roots to polynomials, and $\\textbf{P}$ vs.\n  $\\textbf{NP}$ complexity",
        "Multi-constraint Graph Partitioning Problems Via Recursive Bipartition\n  Algorithm Based on Subspace Minimization Conjugate Gradient Method",
        "Single-qubit probes for temperature estimation in the presence of\n  collective baths",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Symplectic-Amoeba formulation of the non-Bloch band theory for\n  one-dimensional two-band systems",
        "$K_2$-regularity and normality",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Structure of operator algebras for matrix orthogonal polynomials",
        "Algorithm to generate hierarchical structure of desiccation crack\n  patterns",
        "Gravitomagnetic tidal response of relativistic stars in partially\n  screened scalar-tensor theories",
        "Aspherical 4-manifolds with elementary amenable fundamental group",
        "Higher order div-curl type estimates for elliptic linear differential\n  operators on localizable Hardy spaces",
        "Quantifying the imaginarity via different distance measures",
        "Ultracategories as colax algebras for a pseudo-monad on CAT",
        "Shoot-through layers in upright proton arcs unlock advantages in plan\n  quality and range verification",
        "Consensus About Classical Reality in a Quantum Universe"
      ],
      "abstract":[
        "The truncated multidimensional moment problem is studied in terms of the\nStieltjes transform as the interpolation problem. A step-by-step algorithm is\nconstructed for the multidimensional moment problem and the set of solutions is\nfound in terms of continued fractions.",
        "We formulate a solution to the Algebraic version of the Inverse Jacobi\nproblem. Using this solution we produce explicit addition laws on any algebraic\ncurve generalizing the law suggested by Leykin [2] in the case of (n, s)\ncurves. This gives a positive answer to a question asked by T. Shaska whether\naddition laws appearing in [2] can be produced in a coordinate free manner.",
        "This report provides a geometrical Yang-Mills theory, including gravity. The\ntheory treats the space-time symmetry of the local Lorentz group in the same\nmanner as the internal gauge symmetry. We extend this general relativistic\nYang-Mills theory in the Minkowski space-time to a broader space, including\nEuclidean and Minkowskian spaces. In the extended space, we can transport\ntopological achievements from the Euclidean Yang-Mills theory into the\nLorentzian Yang-Mills theory. This extension also provides a relation between\nspace-time and internal symmetry. The perspective provided by the extended\ntheory suggests the novel relation between gravity and the weak force, leading\nus to the graviweak correspondence.",
        "We study the measurement of top Yukawa coupling through $2\\rightarrow 3$ VBS\nat future muon colliders, focusing on the lepton and semi-lepton channels of\n$\\nu\\nu tth\/z$. First, analyzing the partonic amplitudes of $W_LW_L\\rightarrow\nt\\bar t h\/Z_L$ and simulating the full processes of $\\nu\\nu tth\/z$ without\ndecaying, we find they are highly sensitive to the anomalous top Yukawa $\\delta\ny_t$. This sensitivity is enhanced by selecting helicities of the final $t\\bar\nt$ and $Z$ to be $t_L\\bar t_L+t_R\\bar t_R$ and $Z_L$, which serves as the\ndefault and core setting of our analysis. We then obtain the limits on $\\delta\ny_t$ with this setting, giving $[-1.0\\%, 1.1\\%]$ for $\\nu\\nu tth$ only and\n$[-0.36\\%, 0.92\\%]$ for $\\nu\\nu tth$ and $\\nu\\nu ttz$ combined at $30$ TeV and\n$1\\sigma$. Second, we proceed to analyze the processes after decaying and with\nbackground processes. To enhance the sensitivity to $\\delta y_t$, our settings\ninclude selecting the helicities of the final particles, as well as applying\nsuitable cuts. However, we don't do bin-by-bin analysis. We obtain the limits\non $\\delta y_t$ for those channels at $10\/30$ TeV and $1\\sigma\/2 \\sigma$. The\nbest limit is from the semi-lepton channel of $\\nu\\nu tth$. With spin tagging\nefficiency at $\\epsilon_s=0.9$, it gives $[-1.6\\% , 1.8\\%]$ at $1\\sigma$ and $\n[-2.4\\%, 2.7\\% ]$ at $2\\sigma$ at $30$ TeV; $[-7.0\\%, 6.7\\%]$ at $1\\sigma$ and\n$[-9.8\\%, 9.8\\%]$ at $2\\sigma$ at $10$ TeV.",
        "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
        "The Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE) is a\nblind narrow-band Halpha+[NII] imaging survey of the Virgo cluster carried out\nwith MegaCam at the CFHT telescope. The survey provides deep narrow-band images\nfor 385 galaxies hosting star forming HII regions. We identify individual HII\nregions and measure their main physical properties such as Halpha luminosity,\nequivalent diameter, and electron density with the purpose of deriving standard\nrelations as reference for future local and high-z studies of HII regions in\nstar forming systems in different environments. For this purpose we use a\ncomplete sample of ~ 13.000 HII regions of luminosity L(Halpha)>= 10^37 erg\ns^-1 to derive the main statistical properties of HII regions in unperturbed\nsystems, identified as those galaxies with a normal HI gas content (64\nobjects). These are the composite Halpha luminosity function, equivalent\ndiameter and electron density distribution, and luminosity-size relation. We\nalso derive the main scaling relations between several parameters\nrepresentative of the HII regions properties (total number, luminosity of the\nfirst ranked regions, fraction of the diffuse component, best fit parameters of\nthe Schechter luminosity function measured for individual galaxies) and those\ncharacterising the properties of the host galaxies (stellar mass, star\nformation rate and specific star formation rate, stellar mass and star\nformation rate surface density, metallicity, molecular-to-atomic gas ratio,\ntotal gas-to-dust mass ratio). We briefly discuss the results of this analysis\nand their implications in the study of the star formation process in galaxy\ndiscs.",
        "Although protostars and disks are often studied separately owing to numerical\nand observational challenges, breakthroughs in recent years have highlighted\nthe need to study both objects in concert. The role of magnetic fields in this\nregard must be investigated. We aim to describe the birth of the protostar and\nthat of its disk, as well as their early joint evolution following the second\ncollapse. We wish to study the structure of the nascent star-disk system, while\nfocusing on the innermost sub-AU region. We carry out high resolution 3D RMHD\nsimulations, describing the collapse of dense cloud cores to stellar densities.\nThe calculations reach $\\approx 2.3$ yr after protostellar birth. Our\nsimulations are also compared to their hydro counterpart to better isolate the\nrole of magnetic fields. When accounting for ambipolar diffusion, the\nefficiency of magnetic braking is drastically reduced and the nascent protostar\nreaches breakup velocity, thus forming a rotationally supported disk. The\ndiffusion of the magnetic field also allows for the implantation of a $\\sim\n\\mathrm{kG}$ field in the protostar, which is thereafter maintained. The\nmagnetic field is mainly toroidal in the star-disk system, although a notable\nvertical component threads it. We also show that the nascent disk is prone to\nthe MRI, although our resolution is inadequate to capture the mechanism. We\nnote a sensitivity of the disk's properties with regards to the angular\nmomentum inherited prior to the second collapse, as well as the magnetic field\nstrength. These calculations carry multiple implications on several issues in\nstellar formation theory, and offer perspectives for future modeling of the\nsystem. Should the fossil field hypothesis to explain the origins of magnetic\nfields in young stellar objects hold, we show that a $\\sim \\mathrm{kG}$ field\nstrength may be implanted and maintained in the protostar at birth.",
        "Tournaments are competitions between a number of teams, the outcome of which\ndetermines the relative strength or rank of each team. In many cases, the\nstrength of a team in the tournament is given by a score. Perhaps, the most\nstriking mathematical result on the tournament is Moon's theorem, which\nprovides a necessary and sufficient condition for a feasible score sequence via\nmajorization. To give a probabilistic interpretation of Moon's result, Aldous\nand Kolesnik introduced the soccer model,the existence of which gives a short\nproof of Moon's theorem. However, the existence proof of Aldous and Kolesnik is\nnonconstructive, leading to the question of a ``canonical'' construction of the\nsoccer model. The purpose of this paper is to provide explicit constructions of\nthe soccer model with an additional stochastic ordering constraint, which can\nbe formulated by martingale transport. Two solutions are given: one is by\nsolving an entropy optimization problem via Sinkhorn's algorithm, and the other\nrelies on the idea of shadow couplings. It turns out that both constructions\nyield the property of strong stochastic transitivity. The nontransitive\nsituations of the soccer model are also considered.",
        "We propose a framework for solving evolution equations within parametric\nfunction classes, especially ones that are specified by neural networks. We\ncall this framework the minimal neural evolution (MNE) because it is motivated\nby the goal of seeking the smallest instantaneous change in the neural network\nparameters that is compatible with exact solution of the evolution equation at\na set of evolving collocation points. Formally, the MNE is quite similar to the\nrecently introduced Neural Galerkin framework, but a difference in perspective\nmotivates an alternative sketching procedure that effectively reduces the\nlinear systems solved within the integrator to a size that is interpretable as\nan effective rank of the evolving neural tangent kernel, while maintaining a\nsmooth evolution equation for the neural network parameters. We focus\nspecifically on the application of this framework to diffusion processes, where\nthe score function allows us to define intuitive dynamics for the collocation\npoints. These can in turn be propagated jointly with the neural network\nparameters using a high-order adaptive integrator. In particular, we\ndemonstrate how the Ornstein-Uhlenbeck diffusion process can be used for the\ntask of sampling from a probability distribution given a formula for the\ndensity but no training data. This framework extends naturally to allow for\nconditional sampling and marginalization, and we show how to systematically\nremove the sampling bias due to parametric approximation error. We validate the\nefficiency, systematic improvability, and scalability of our approach on\nillustrative examples in low and high spatial dimensions.",
        "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment.",
        "Let \\( \\mathcal{F} \\) be a family of graphs. The generalized Tur\\'an number\n\\( \\operatorname{ex}(n, K_r, \\mathcal{F}) \\) is the maximum number of $K_r$ in\nan \\( n \\)-vertex graph that does not contain any member of \\( \\mathcal{F} \\)\nas a subgraph. Recently, Alon and Frankl initiated the study of Tur\\'an\nproblems with bounded matching number. In this paper, we determine the\ngeneralized Tur\\'an number of \\( C_{\\geq k} \\) with bounded matching number.",
        "This work explores the relationship between solution space and time\ncomplexity in the context of the $\\textbf{P}$ vs. $\\textbf{NP}$ problem,\nparticularly through the lens of the sliding tile puzzle and root finding\nalgorithms. We focus on the trade-off between finding a solution and verifying\nit, highlighting how understanding the structure of the solution space can\ninform the complexity of these problems. By examining the relationship between\nthe number of possible configurations and the time complexity required to\ntraverse this space we demonstrate that the minimal time to verify a solution\nis often smaller than the time required to discover it. Our results suggest\nthat the efficiency of solving $\\textbf{NP}$-complete problems is not only\ndetermined by the ability to find solutions but also by how effectively we can\nnavigate and characterize the solution space. This study contributes to the\nongoing discourse on computational complexity, particularly in understanding\nthe interplay between solution space size, algorithm design, and the inherent\nchallenges of finding versus verifying solutions.",
        "The graph partitioning problem is a well-known NP-hard problem. In this\npaper, we formulate a 0-1 quadratic integer programming model for the graph\npartitioning problem with vertex weight constraints and fixed vertex\nconstraints, and propose a recursive bipartition algorithm based on the\nsubspace minimization conjugate gradient method. To alleviate the difficulty of\nsolving the model, the constrained problem is transformed into an unconstrained\noptimization problem using equilibrium terms, elimination methods, and\ntrigonometric properties, and solved via an accelerated subspace minimization\nconjugate gradient algorithm. Initial feasible partitions are generated using a\nhyperplane rounding algorithm, followed by heuristic refinement strategies,\nincluding one-neighborhood and two-interchange adjustments, to iteratively\nimprove the results. Numerical experiments on knapsack-constrained graph\npartitioning and industrial examples demonstrate the effectiveness and\nfeasibility of the proposed algorithm.",
        "We study the performance of single-qubit probes for temperature estimation in\nthe presence of collective baths. We consider a system of two qubits, each\nlocally dissipating into its own bath while being coupled to a common bath. In\nthis setup, we investigate different scenarios for temperature estimation of\nboth the common and local baths. First, we explore how the precision of a\nsingle-qubit probe for the temperature of the common bath may be enhanced by\nthe collective effects generated by the bath itself, if the second qubit is in\nresonance with the probe. We also analyze how the presence of additional local\nbaths on each qubit may jeopardize, or improve, this result. Next, we show that\none qubit may serve as a probe to measure the temperature of the local bath,\naffecting the other qubit by exploiting their interaction mediated by the\ncommon bath. This approach enables remote temperature sensing without directly\ncoupling the probe to the target qubit or its local environment, thereby\nminimizing potential disturbances and practical challenges. However, in the\nabsence of a direct qubit-qubit coupling, this protocol works only for very\nhigh temperatures of the local bath whose temperature we aim at estimating.\nThis being said, remote temperature sensing works for broader temperature\nregimes in the presence of a direct coupling between the qubits. Furthermore,\nwe also investigate the impact of dephasing and the dynamics of quantum\ncorrelations in the model for remote temperature sensing.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "The non-Hermitian skin effect is a topological phenomenon, resulting in the\ncondensation of bulk modes near the boundaries. Due to the localization of bulk\nmodes at the edges, boundary effects remain significant even in the\nthermodynamic limit. This makes conventional Bloch band theory inapplicable and\nhinders the accurate computation of the spectrum. The Amoeba formulation\naddresses this problem by determining the potential from which the spectrum can\nbe derived using the generalized Szeg\\\"o's limit theorem, reducing the problem\nto an optimization of the Ronkin function. While this theory provides novel\ninsights into non-Hermitian physics, challenges arise from the multiband nature\nand symmetry-protected degeneracies, even in one-dimensional cases. In this\nwork, we investigate one-dimensional two-band class AII$^\\dagger$ systems,\nwhere Kramers pairs invalidate the conventional Amoeba formalism. We find that\nthese challenges can be overcome by optimizing the band-resolved Ronkin\nfunctions, which is achieved by extrapolating the total Ronkin function.\nFinally, we propose a generalized Szeg\\\"o's limit theorem for class\nAII$^\\dagger$ and numerically demonstrate that our approach correctly computes\nthe potential and localization length.",
        "We take a fresh look at the relationship between $K$-regularity and\nregularity of schemes, proving two results in this direction. First, we show\nthat $K_2$-regular affine algebras over fields of characteristic zero are\nnormal. Second, we improve on Vorst's $K$-regularity bound in the case of local\ncomplete intersections; this is related to recent work on higher du Bois\nsingularities.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "In this paper, we study the structure of the differential operator algebra \\(\n\\mathcal{D}(W) \\) and its associated eigenvalue algebra \\( \\Lambda(W) \\) for\nmatrix-valued orthogonal polynomials. While \\( \\Lambda(W) \\) is isomorphic to\n\\( \\mathcal{D}(W) \\), its simpler framework allows us to efficiently derive\nstrong results about \\( \\mathcal{D}(W) \\) and its center \\( \\mathcal{Z}(W) \\).\nWe analyze the behavior of the center under Darboux transformations,\nestablishing explicit relationships between the centers of Darboux-equivalent\nweights. These results are illustrated through the study of both reducible and\nirreducible matrix weights, including a detailed analysis of an irreducible\nJacobi-type weight.",
        "We propose an algorithm generating planar networks which structure resembles\na hierarchical structure of desiccation crack patterns.",
        "In scalar-tensor theories beyond Horndeski, the Vainshtein screening\nmechanism is only partially effective inside astrophysical bodies. We\ninvestigate the potential to detect this partial breaking of Vainshtein\nscreening through the tidal response of fluid bodies. Specifically, we\ncalculate the gravitomagnetic tidal Love numbers and analyze how deviations\nfrom general relativity depend on parameters governing the breaking of\nVainshtein screening in the weak-gravity regime. For fixed parameter values,\nthe relative deviations increase with higher multipoles and larger compactness.\nHowever, we demonstrate that these parameters alone are insufficient to fully\ncharacterize the tidal response of relativistic bodies in scalar-tensor\ntheories beyond Horndeski.",
        "We classify the possible elementary amenable fundamental groups of compact\naspherical 4-manifolds with boundary and conclude that they are either\npolycyclic or solvable Baumslag- Solitar. Since these groups are good and\nsatisfy the Farrell-Jones Conjecture, one concludes that such manifolds satisfy\ntopological rigidity: a homotopy equivalence which is a homeomorphism on the\nboundary is homotopic, relative to the boundary, to a homeomorphism. We\nclassify the closed 3-manifolds which arise as the boundary of an compact\naspherical 4-manifold with elementary amenable fundamental group, generalizing\nresults of Freedman and Quinn in the cases of trivial and infinite cyclic\nfundamental groups. Moreover, two such 4-manifolds are homeomorphic if and only\nif their \"enhanced\" peripheral group systems are equivalent, and each such\nmanifold is the boundary connected sum of a compact aspherical 4-manifold with\nprime boundary and a contractible 4-manifold.",
        "In this work, we present higher order div-curl type estimates in the sense of\nCoifman, Lions, Meyer & Semmes in the local setup of elliptic linear\ndifferential operators with smooth coefficients on localizable Hardy spaces.\nOur version implies and extends results obtained for first order operators\nassociated to elliptic systems and complexes of vector fields. As tools, with\nown interest, we develop a new smooth atomic decomposition on localizable\nHardy-Sobolev spaces and a Poincar\\'e type inequality.",
        "The recently introduced resource theory of imaginarity facilitates a\nsystematic investigation into the role of complex numbers in quantum mechanics\nand quantum information theory. In this work, we propose well-defined measures\nof imaginarity using various distance metrics, drawing inspiration from recent\nadvancements in quantum entanglement and coherence. Specifically, we focus on\nquantitatively evaluating imaginarity through measures such as Tsallis relative\n$\\alpha$-entropy, Sandwiched R\\'{e}nyi relative entropy, and Tsallis relative\noperator entropy. Additionally, we analyze the decay rates of these measures.\nOur findings reveal that the Tsallis relative $\\alpha$-entropy of imaginarity\nexhibits higher decay rate under quantum channels compared to other measures.\nFinally, we examine the ordering of single-qubit states under these imaginarity\nmeasures, demonstrating that the order remains invariant under the bit-flip\nchannel for specific parameter ranges. This study enhances our understanding of\nimaginarity as a quantum resource and its potential applications in quantum\ninformation theory.",
        "We show a result inspired by a conjecture by Shulman claiming that\nultracategories as defined by Lurie are normal colax algebras for a certain\npseudo-monad on the category of categories CAT. Such definition allows us to\nregard left and right ultrafunctors as defined by Lurie as instances of\nlax\/colax algebras morphisms",
        "Background and purpose: Upright proton therapy with compact delivery systems\nhas the potential to reduce costs for treatments but could also lead to\nbroadening of the beam penumbra. This study aims at combining upright static\nproton arcs with additional layers of shoot-through (ST) protons to sharpen the\nbeam penumbra and improve plan quality for such systems. An additional\nadvantage of the method is that it provides a straightforward approach for\nrange verification.\n  Methods: We examined various treatment plans for a virtual phantom: 3-beam\nIMPT, static arc (Arc) with\/without ST (Arc+ST), and with\/without collimation\n(+Coll). In the virtual phantom three different targets were utilized to study\nthe effect on conformity index (CI), homogeneity index (HI), robustness and\nmean dose to the phantom volume. The phantom study was complemented with a\nhead-and-neck (H&N) patient case with a similar set of plans. A range\nverification concept that determines residual ranges of the ST protons was\nstudied in simulated scenarios for the H&N case.\n  Results: The Arc+ST plans show superior CI, HI and target robustness compared\nto the Arc+Coll plans. For the Arc plans without ST, the collimated plans\nperform better than the uncollimated plans. For Arc+ST, on the other hand,\ncollimation has little impact on CI, HI and robustness. However, a small\nincrease in the mean dose to the phantom volume is seen without collimation.\nFor the H&N case, similar improvements for Arc+ST can be seen with only a\nmarginal increase of the mean dose to the patient volume. The range\nverification simulation shows that the method is suitable to detect range\nerrors.\n  Conclusions: Combining proton arcs and ST layers can enhance compact upright\nproton solutions by improving plan quality. It is also tailored for the\ninclusion of a fast and straightforward residual range verification method.",
        "Quantum Darwinism recognizes that decoherence imprints redundant records of\npreferred quasi-classical pointer states on the environment. These redundant\nrecords are then accessed by observers. We show how redundancy enables and even\nimplies consensus between observers who use fragments of that decohering\nenvironment to acquire information about systems of interest. We quantify\nconsensus using information-theoretic measures that employ mutual information\nto assess the correlation between the records available to observers from\ndistinct -- hence, independently accessible -- fragments of the environment. We\nprove that when these fragments have enough information about a system,\nobservers that access them will attribute the same pointer state to that\nsystem. Thus, those who know enough about the system agree about what they\nknow. We then test proposed measures of consensus in a solvable model of\ndecoherence as well as in numerical simulations of a many-body system. These\nresults provide detailed understanding of how our classical everyday world\narises from within the fundamentally quantum Universe we inhabit."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study",
    "start_abstract":"Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study"
      ],
      "abstract":[
        "Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "What Drives Cluster Cool-Core Transformations? A Population Level\n  Analysis of TNG-Cluster",
        "A First Look at \"Continuous Spin\" Gravity -- Time Delay Signatures",
        "Unified model of the Hall effect from insulator to overdoped compounds\n  in cuprate superconductors",
        "Every group is the automorphism group of a graph with arbitrarily large\n  genus",
        "Equivalence principles in Weyl transverse gravity",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Semi-Parametric Batched Global Multi-Armed Bandits with Covariates",
        "Topological Data Analysis of Abelian Magnetic Monopoles in Gauge\n  Theories",
        "Two-sided bounds on the point-wise spatial decay of ground states in the\n  renormalized Nelson model with confining potentials",
        "Elusive properties of countably infinite graphs",
        "Quantum stick-slip motion in nanoscaled friction",
        "Constructing Fundamentals for the Theory of Proportions and Symbolic\n  Allusions Applied Interdisciplinarily",
        "Observation of the $W$-annihilation process $D_s^+ \\to \\omega\\rho^+$ and\n  measurement of $D_s^+ \\to \\phi\\rho^+$ in $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$\n  decays",
        "PAH Feature Ratios Around Stellar Clusters and Associations in 19 Nearby\n  Galaxies",
        "Exact collective occupancies of the Moshinsky model in two-dimensional\n  geometry",
        "Semilinear Dynamic Programming: Analysis, Algorithms, and Certainty\n  Equivalence Properties",
        "On the test properties of the Frobenius endomorphism",
        "Possible Explanation of $F_2^n\/F_2^p$ at Large $x$ Using Quantum\n  Statistical Mechanics",
        "Detection and estimation of vertex-wise latent position shifts across\n  networks",
        "Disproving some theorems in Sharma and Chauhan et al. (2018, 2021)",
        "Field Aligned Currents and Aurora During the Terrestrial Alfven Wing\n  State",
        "Linear sigma model with quarks and Polyakov loop in rotation: phase\n  diagrams, Tolman-Ehrenfest law and mechanical properties",
        "On the query complexity of sampling from non-log-concave distributions",
        "Laser induced Compton Scattering to Dark Matter in Effective Field\n  Theory",
        "Tomographic measurement data of states that never existed",
        "Geometric Interpretations of the $k$-Nearest Neighbour Distributions",
        "Complementary signatures of $\\alpha-$attractor inflation in CMB and\n  cosmic string Gravitational Waves",
        "Lattice calculation of the $D_s\\mapsto X \\ell \\bar{\\nu}_\\ell$ inclusive\n  decay rate: an overview"
      ],
      "abstract":[
        "In this study, we examine the frequency and physical drivers of\ntransformations from cool-core (CC) to non-cool-core (NCC) clusters, and vice\nversa, in a sample of 352 massive galaxy clusters (M_vir = 10^14-15.3 M_sun)\nfrom the TNG-Cluster magnetohydrodynamical cosmological simulation of galaxies.\nBy identifying transformations based on the evolution of central entropy and\nfocusing on z<2.5, we find that clusters frequently undergo such events,\ndepending on their assembly and supermassive black hole histories. On average,\nclusters experience 2 to 3 transformations. Transformations can occur in both\ndirections and can be temporary, but those to higher entropy cores, i.e. in the\ndirection from CC to NCC states, are the vast majority. CC phases are shorter\nthan NCC phases, and thus overall the TNG-Cluster population forms with\nlow-entropy cores and moves towards NCC states with time. We study the role\nthat mergers play in driving transformations, and find that mergers within\n~1Gyr prior to a transformation toward higher (but not lower) entropy cores\noccur statistically more often than in a random control sample. Most\nimportantly, we find examples of mergers associated with CC disruption\nregardless of their mass ratio or angular momentum. However, past merger\nactivity is not a good predictor for z=0 CC status, at least based on core\nentropy, even though clusters undergoing more mergers eventually have the\nhighest core entropy values at z=0. We consider the interplay between AGN\nfeedback and evolving cluster core thermodynamics. We find that core\ntransformations are accompanied by an increase in AGN activity, whereby\nfrequent and repeated (kinetic) energy injections from the central SMBHs can\nproduce a collective, long-term impact on central entropy, ultimately heating\ncluster cores. Whether such fast-paced periods of AGN activity are triggered by\nmergers is plausible, but not necessary.",
        "We consider the possibility that gravity is mediated by \"continuous spin\"\nparticles, i.e., massless particles whose invariant spin scale $\\rho_g$ is\nnon-zero. In this case, the primary helicity-2 modes of gravitational radiation\non a Minkowski background mix with a tower of integer-helicity partner modes\nunder boosts, with $\\rho_g$ controlling the degree of mixing. We develop a\nformalism for coupling spinless matter to continuous spin gravity at linearized\nlevel. Using this formalism, we calculate the time delay signatures induced by\ngravitational waves in an idealized laser interferometer detector. The\nfractional deviation from general relativity predictions is $O(\\rho_g\/\\omega)$\nfor gravitational wave frequencies $\\omega >\\rho_g$, and the effects of waves\nwith $\\omega \\lesssim \\rho_g$ are damped. The precision and low frequency\nranges of gravitational wave detectors suggest potential sensitivity to spin\nscales at or below $\\sim 10^{-14}$ eV at ground-based laser interferometers and\n$\\sim 10^{-24}$ eV at pulsar timing arrays, motivating further analysis of\nobservable signatures.",
        "Measurements of the Hall coefficient in La$_{2-x}$Sr$_x$CuO$_4$, ranging from\nthe undoped ($x = p = 0$) Mott insulator to overdoped compounds, exhibit a\ntemperature dependence that offers insights into their electronic structure. We\ninterpret these results using a model based on the theory of phase-separation\n(PS) dynamics, which begins at half-filled ($n = 1$) and at a temperature\n$T_{\\rm PS}(p)$, near the pseudogap temperature $T^*(p)$. The $n = 1$ holes\nhave low mobility and provide the modulations of the charge density waves\n(CDW). As doping increases from $p = 0$, these modulations guide the additional\np holes to occupy alternating CDW domains. This charge inhomogeneity may\nfacilitate the formation of localized superconducting amplitudes below the\ncritical onset temperature $T_{\\rm c}^{\\rm max}(p)$. Using thermal activation\nexpressions, along with quantum tunnelling between the charge domains, we\nsuccessfully reproduce all Hall coefficient measurements $R_{\\rm H}(p,T)$ and\nhighlight the relevant energies of cuprates. The calculations confirm three\nsignificant electronic features: the phase-separating role of the pseudogap\ntemperature, the superconducting state achieved through phase coherence,\n  and the two types of charge carriers whose energies and mobilities become\ncomparable at $p \\approx 0.19$, where\n  $T^*(p) \\approx T_{\\rm c}^{\\rm max}(p)$. This results in a crossover from $n\n= p$ to $n = 1 + p$. These findings, along with the\n  $R_{\\rm H}(p,T)$ calculations from insulating to overdoped compounds,\nunderscore the critical role of the electronic\n  phase separation in the properties of cuprates.",
        "We prove that, to every abstract group $G$, we can associate a sequence of\ngraphs $\\Gamma_n$ such that the automorphism group of $\\Gamma_n$ is isomorphic\nto $G$ and the genus of $\\Gamma_n$ is an unbounded function of $n$.",
        "There exist two consistent theories of massless, self-interacting gravitons,\nwhich differ by their local symmetries: general relativity and Weyl transverse\ngravity. We show that these two theories are also the only two metric\ndescriptions of gravity in 4 spacetime dimensions which obey the equivalence\nprinciple for test gravitational physics. We further analyse how the weaker\nformulations of the equivalence principle are realised in Weyl transverse\ngravity (and its generalisations). The analysis sheds light on the behaviour of\nmatter fields in this theory.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "The multi-armed bandits (MAB) framework is a widely used approach for\nsequential decision-making, where a decision-maker selects an arm in each round\nwith the goal of maximizing long-term rewards. Moreover, in many practical\napplications, such as personalized medicine and recommendation systems,\nfeedback is provided in batches, contextual information is available at the\ntime of decision-making, and rewards from different arms are related rather\nthan independent. We propose a novel semi-parametric framework for batched\nbandits with covariates and a shared parameter across arms, leveraging the\nsingle-index regression (SIR) model to capture relationships between arm\nrewards while balancing interpretability and flexibility. Our algorithm,\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\nemploys a batched successive arm elimination strategy with a dynamic binning\nmechanism guided by the single-index direction. We consider two settings: one\nwhere a pilot direction is available and another where the direction is\nestimated from data, deriving theoretical regret bounds for both cases. When a\npilot direction is available with sufficient accuracy, our approach achieves\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\ncircumventing the curse of dimensionality. Extensive experiments on simulated\nand real-world datasets demonstrate the effectiveness of our algorithm compared\nto the nonparametric batched bandit method introduced by\n\\cite{jiang2024batched}.",
        "Motivated by recent literature on the possible existence of a second\nhigher-temperature phase transition in Quantum Chromodynamics, we revisit the\nproposal that colour confinement is related to the dynamics of magnetic\nmonopoles using methods of Topological Data Analysis, which provides a\nmathematically rigorous characterisation of topological properties of\nquantities defined on a lattice. After introducing persistent homology, one of\nthe main tools in Topological Data Analysis, we shall discuss how this concept\ncan be used to quantitatively analyse the behaviour of monopoles across the\ndeconfinement phase transition. Our approach is first demonstrated for Compact\n$U(1)$ Lattice Gauge Theory, which is known to have a zero-temperature\ndeconfinement phase transition driven by the restoration of the symmetry\nassociated with the conservation of the magnetic charge. For this system, we\nperform a finite-size scaling analysis of observables capturing the homology of\nmagnetic current loops, showing that the expected value of the deconfinement\ncritical coupling is reproduced by our analysis. We then extend our method to\n$SU(3)$ gauge theory, in which Abelian magnetic monopoles are identified after\nprojection in the Maximal Abelian Gauge. A finite-size scaling of our\nhomological observables of Abelian magnetic current loops at temporal size $N_t\n= 4$ provides the expected value of the critical coupling with an accuracy that\nis generally higher than that obtained with conventional thermodynamic\napproaches at comparable statistics, hinting towards the relevance of\ntopological properties of monopole currents for confinement.",
        "We study the renormalized Nelson model for a scalar matter particle in a\ncontinuous confining potential interacting with a possibly massless quantized\nradiation field. When the radiation field is massless we impose a mild infrared\nregularization ensuring that the Nelson Hamiltonian has a non-degenerate ground\nstate in all considered cases. Employing Feynman-Kac representations, we derive\nlower bounds on the point-wise spatial decay of the partial Fock space norms of\nground state eigenvectors. Here the exponential rate function governing the\ndecay is given by the Agmon distance familiar from the analysis of\nSchr\\\"{o}dinger operators. For a large class of confining potentials, our lower\nbounds on the decay of ground state eigenvectors match asymptotically with the\nupper bounds implied by previous work of the present authors.",
        "A graph property is elusive (or evasive) if any algorithm testing it by\nasking questions of the form ''Is there an edge between vertices x and y?''\nmust, in the worst case, examine all pairs of vertices. Elusiveness for\ninfinite vertex sets has been first studied by Csern\\'ak and Soukup, who proved\nthat the long-standing Aanderaa-Karp-Rosenberg Conjecture -- which states that\nevery nontrivial monotone graph property is elusive -- fails for infinite\nvertex sets. We extend their work by giving a closer look to the case when the\nvertex set is countably infinite and the ''algorithm'' terminates after\ninfinitely many steps. Among others, we prove that connectedness is elusive,\nwhich strengthens a result of Csern\\'ak and Soukup. We give counterexamples to\nthe infinite version of the Aanderaa-Karp-Rosenberg Conjecture even if the\n''algorithm'' is required to terminate after infinitely many steps, which\nstrengthens results of Csern\\'ak and Soukup.",
        "Friction in atomistic systems is usually described by the classical\nPrandtl-Tomlinson model suitable for capturing the dragging force of a\nnanoparticle in a periodic potential. Here we consider the quantum mechanical\nversion of this model in which the dissipation is facilitated by releasing heat\nto an external bath reservoir. The time evolution of the system is captured\nwith the Liouville-von Neumann equation through the density matrix of the\nsystem in the Markov approximation. We examine several kinetic and dissipative\nproperties of the nanoparticle by delineating classical vs quantum mechanical\neffects. We find that that the Landau-Zener tunneling is a key factor in the\noverall reduction of the frictional dissipation when compared to the classical\nmotion in which such tunneling is absent. This in-depth study analyzes the\ninterplay between velocity, strength of interaction, and temperature to control\nthe frictional process and provide useful guidelines for experimental data\ninterpretation.",
        "The Theory of Proportions and Symbolic Allusions applied Interdisciplinary\n(TPASAI) is a framework that integrates mathematics, linguistics, psychology,\nand game theory to uncover hidden patterns and proportions in reality. Its\ncentral idea is that numerical encoding of symbols, dates, and language can\nreveal recurring structures and connections that reflect universal principles.\nBy applying fractal analysis, the theory identifies patterns across different\nscales, offering a unifying perspective on the structure of the world. One key\naspect of TPASAI is symbolic analysis, which allows for the reinterpretation of\ntraumatic experiences in psychotherapy. For example, assigning numerical values\nto elements like fingers, dates, or words can help individuals uncover\nmeaningful associations between personal experiences and collective symbols.\nThis approach encourages cognitive flexibility and provides a therapeutic\navenue for recontextualizing emotions. The theory also incorporates principles\nof game theory, which frame reality as a system of symbolic \"codes\" governed by\nrules that can be understood and strategically used. This perspective is\nespecially useful for psychological conditions like obsessive-compulsive\ndisorder (OCD), enabling patients to approach their obsessions as decipherable\npatterns rather than rigid constraints. TPASAI has practical applications in\npsychology, education, and technology. In education, it aids in teaching\nmathematical and linguistic concepts by exploring connections between symbolic\nrepresentations and real-world events. In technology, the methodology can be\nemployed in ciphering and natural language processing. The innovation of TPASAI\nlies in its ability to merge the structured rigor of mathematics with the\ninterpretative flexibility of symbolic analysis, offering a deeper\nunderstanding of events and relationships.",
        "We present the first amplitude analysis and branching fraction measurement of\nthe decay $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$, using $e^+e^-$ collision data\ncollected with the BESIII detector at center-of-mass energies between 4.128 and\n4.226 GeV corresponding to an integrated luminosity of 7.33 fb$^{-1}$, and\nreport the first observation of the pure $W$-annihilation decay $D_s^+ \\to\n\\omega\\rho^+$ with a branching fraction of $(0.99\\pm0.08_{\\rm stat}\\pm0.07_{\\rm\nsyst})\\%$. In comparison to the low significance of the $\\mathcal{D}$ wave in\nthe decay $D_s^+ \\to \\phi\\rho^+$, the dominance of the $\\mathcal{D}$ wave over\nthe $\\mathcal{S}$ and $\\mathcal{P}$ waves, with a fraction of\n$(51.85\\pm7.28_{\\rm stat}\\pm7.90_{\\rm syst})\\%$ observed in the decay, provides\ncrucial information for the``polarization puzzle\", as well as for the\nunderstanding of charm meson decays. The branching fraction of $D^+_s\\to\n\\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$ is measured to be $(4.41\\pm0.15_{\\rm\nstat}\\pm0.13_{\\rm syst})\\%$. Moreover, the branching fraction of $D_s^+ \\to\n\\phi\\rho^+$ is measured to be $(3.98\\pm0.33_{\\rm stat}\\pm0.21_{\\rm syst})\\%$,\nand the $R_{\\phi}= {\\mathcal{B}(\\phi\\to\\pi^+\\pi^-\\pi^0)}\/{\\mathcal{B}(\\phi\\to\nK^+K^-)}$ is determined to be $(0.222\\pm0.019_{\\rm stat}\\pm0.016_{\\rm syst}$),\nwhich is consistent with the previous measurement based on charm meson decays,\nbut deviates from the results from $e^+e^-$ annihilation and $K$-$N$ scattering\nexperiments by more than 3$\\sigma$.",
        "We present a comparison of observed polycyclic aromatic hydrocarbon (PAH)\nfeature ratios in 19 nearby galaxies with a grid of theoretical expectations\nfor near- and mid-infrared dust emission. The PAH feature ratios are drawn from\nCycle 1 JWST observations and are measured for 7224 stellar clusters and 29176\nstellar associations for which we have robust ages and mass estimates from HST\nfive-band photometry. Though there are galaxy-to-galaxy variations, the\nobserved PAH feature ratios largely agree with the theoretical models,\nparticularly those that are skewed toward more ionized and larger PAH size\ndistributions. For each galaxy we also extract PAH feature ratios for 200\npc-wide circular regions in the diffuse interstellar medium, which serve as a\nnon-cluster\/association control sample. Compared to what we find for stellar\nclusters and associations, the 3.3um\/7.7um and 3.3um\/11.3um ratios from the\ndiffuse interstellar medium are $\\sim 0.10-0.15$ dex smaller. When the observed\nPAH feature ratios are compared to the radiation field hardness as probed by\nthe [OIII]\/H$\\beta$ ratio, we find anti-correlations for nearly all galaxies in\nthe sample. These results together suggest that the PAH feature ratios are\ndriven by the shape intensity of the radiation field, and that the smallest\nPAHs -- observed via JWST F335M imaging -- are increasingly 'processed' or\ndestroyed in regions with the most intense and hard radiation fields.",
        "In this paper, we investigate the ground state of $N$ bosonic atoms confined\nin a two-dimensional isotropic harmonic trap, where the atoms interact via a\nharmonic potential. We derive an exact diagonal representation of the\nfirst-order reduced density matrix in polar coordinates, in which the angular\ncomponents of the natural orbitals are eigenstates of the angular momentum\noperator. Furthermore, we present an exact expression for the collective\noccupancy of the natural orbitals with angular momentum $l$, quantifying the\nfraction of particles carrying that angular momentum. The present study\nexplores how the dependence of collective occupancy relies on angular momentum\n$l$ and the control parameters of the system. Building on these findings, we\nexamine boson fragmentation into components with different $l$ and reveal a\nunique feature of the system: the natural orbitals contributing to the\ncorrelations are uniformly distributed across all significant $l$ components.",
        "We consider a broad class of dynamic programming (DP) problems that involve a\npartially linear structure and some positivity properties in their system\nequation and cost function. We address deterministic and stochastic problems,\npossibly with Markov jump parameters. We focus primarily on infinite horizon\nproblems and prove that under our assumptions, the optimal cost function is\nlinear, and that an optimal policy can be computed efficiently with standard DP\nalgorithms. Moreover, we show that forms of certainty equivalence hold for our\nstochastic problems, in analogy with the classical linear quadratic optimal\ncontrol problems.",
        "In this paper, we prove two theorems concerning the test properties of the\nFrobenius endomorphism over commutative Noetherian local rings of prime\ncharacteristic $p$. Our first theorem generalizes a result of Funk-Marley on\nthe vanishing of Ext and Tor modules, while our second theorem generalizes one\nof our previous results on maximal Cohen-Macaulay tensor products. In these\nearlier results, we replace $^{e}R$ with a more general module $^{e}M$, where\n$R$ is a Cohen-Macaulay ring, $M$ is a Cohen-Macaulay $R$-module with full\nsupport, and $^{e}M$ is the module viewed as an $R$-module via the $e$-th\niteration of the Frobenius endomorphism. We also provide examples and present\napplications of our results, yielding new characterizations of the regularity\nof local rings.",
        "The ratio of the neutron to proton structure functions, $F^n_2(x)\/F^p_2(x)$,\nis expected to approach 1\/4 as $x \\to 1$, based on the assumption that $d (x) \/\nu (x)$ vanishes as $x \\to 1$. This expectation is in striking disagreement with\na recent measurement by the Marathon experiment of the scattering of electrons\noff the mirror nuclei $^3$H and $^3$He, showing that $F^n_2(x)\/F^p_2(x)$ is\nlarger than 1\/4 for $ x \\rightarrow 1$. We have examined the consequences of\nthe Pauli Exclusion Principle for the parton distributions in the nucleon when\nthe partons are described by quantum statistical mechanics. We find that the\nrecent experimental result on the $F^n_2(x)\/F^p_2(x)$ over the broad range of\n$x$ can be well described by the quantum statistical approach.",
        "Pairwise network comparison is essential for various applications, including\nneuroscience, disease research, and dynamic network analysis. While existing\nliterature primarily focuses on comparing entire network structures, we address\na vertex-wise comparison problem where two random networks share the same set\nof vertices but allow for structural variations in some vertices, enabling a\nmore detailed and flexible analysis of network differences. In our framework,\nsome vertices retain their latent positions between networks, while others\nundergo shifts. To identify the shifted and unshifted vertices and estimate\ntheir latent position shifts, we propose a method that first derives vertex\nembeddings in a low-rank Euclidean space for each network, then aligns these\nestimated vertex latent positions into a common space to resolve potential\nnon-identifiability, and finally tests whether each vertex is shifted or not\nand estimates the vertex shifts. Our theoretical results establish the test\nstatistic for the algorithms, guide parameter selection, and provide\nperformance guarantees. Simulation studies and real data applications,\nincluding a case-control study in disease research and dynamic network\nanalysis, demonstrate that the proposed algorithms are both computationally\nefficient and effective in extracting meaningful insights from network\ncomparisons.",
        "The main objective of this work is to show, through counterexamples, that\nsome of the theorems presented in the papers of Sharma \\textit{et al.} (2018)\nand Chauhan \\textit{et al.} ( 2021) are incorrect. Although they used these\ntheorems to establish a sufficient condition for a multi-twisted (MT) code to\nbe linear complementary dual (LCD), we show that this condition itself remains\nvalid. We further improve this condition by removing the restrictions on the\nshift constants and relaxing the required coprimality condition. We show that\ncompared to the previous condition, the modified condition is able to identify\nmore LCD MT codes. Furthermore, without the need for a normalized set of\ngenerators, we develop a formula to determine the dimension of any\n$\\rho$-generator MT code.",
        "When sub-Alfvenic (Alfven Mach number M_ < 1) plasmas impact Earth, the\nmagnetosphere develops Alfven wings. A Multiscale Atmosphere Geospace\nEnvironment (MAGE) global simulation of the April 2023 geomagnetic storm,\nvalidated against Active Magnetosphere and Planetary Electrodynamics Response\nExperiment (AMPERE), reveals the mechanism of field-aligned current (FAC)\ngeneration and auroral precipitation for the terrestrial Alfven wings.\nSimulation and observations show northern hemisphere planetward flowing auroral\nelectrons (negative FAC) are predominantly at magnetic local times (MLTs) 8-12.\nJust before the wings formed, solar wind conditions were similar and M_A ~ 1.4,\nyet the same FAC system extended from 9-18 MLT. Flow vorticity drives FACs at\nthe boundary of the Alfven wings and unshocked solar wind. The Alfven wing\nshape presents a different obstacle to the solar wind compared to typical lobe\nfluxes, producing the unique FAC and auroral patterns. New insights about\nAlfven wing FACs will help to understand auroral features for exoplanets inside\ntheir host star's Alfven zone.",
        "We study the effect of rotation on the confining and chiral properties of QCD\nusing the Polyakov-enhanced linear sigma model coupled to quarks. Working in\nthe homogeneous approximation, we obtain the phase diagram at finite\ntemperature, baryon density and angular frequency, taking into account the\ncausality constraint enforced by the spectral boundary conditions at a\ncylindrical surface. We explicitly address various limits with respect to\nsystem size, angular frequency and chemical potential. We demonstrate that, in\nthis model, the critical temperatures of both transitions diminish in response\nto the increasing rotation, being in contradiction with the first-principle\nlattice results. In the limit of large volume, the thermodynamics of the model\nis consistent with the Tolman-Ehrenfest law. We also compute the mechanical\ncharacteristics of rotating plasma such as the moment of inertia and the $K_4$\nshape coefficient.",
        "We study the problem of sampling from a $d$-dimensional distribution with\ndensity $p(x)\\propto e^{-f(x)}$, which does not necessarily satisfy good\nisoperimetric conditions.\n  Specifically, we show that for any $L,M$ satisfying $LM\\ge d\\ge 5$,\n$\\epsilon\\in \\left(0,\\frac{1}{32}\\right)$, and any algorithm with query\naccesses to the value of $f(x)$ and $\\nabla f(x)$, there exists an\n$L$-log-smooth distribution with second moment at most $M$ such that the\nalgorithm requires $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ queries to\ncompute a sample whose distribution is within $\\epsilon$ in total variation\ndistance to the target distribution. We complement the lower bound with an\nalgorithm requiring $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\mathcal O(d)}$\nqueries, thereby characterizing the tight (up to the constant in the exponent)\nquery complexity for sampling from the family of non-log-concave distributions.\n  Our results are in sharp contrast with the recent work of Huang et al.\n(COLT'24), where an algorithm with quasi-polynomial query complexity was\nproposed for sampling from a non-log-concave distribution when\n$M=\\mathtt{poly}(d)$. Their algorithm works under the stronger condition that\nall distributions along the trajectory of the Ornstein-Uhlenbeck process,\nstarting from the target distribution, are $\\mathcal O(1)$-log-smooth. We\ninvestigate this condition and prove that it is strictly stronger than\nrequiring the target distribution to be $\\mathcal O(1)$-log-smooth.\nAdditionally, we study this condition in the context of mixtures of Gaussians.\n  Finally, we place our results within the broader theme of ``sampling versus\noptimization'', as studied in Ma et al. (PNAS'19). We show that for a wide\nrange of parameters, sampling is strictly easier than optimization by a\nsuper-exponential factor in the dimension $d$.",
        "The detection of light dark matter (DM) is a longstanding challenge in\nterrestrial experiments. High-intensity facility of an intense electromagnetic\nfield may provide a plausible strategy to study strong-field particle physics\nand search for light DM. In this work, we propose to search for light DM\nparticle through the nonlinear Compton scattering in the presence of a\nhigh-intense laser field. An ultra-relativistic electron beam collides with an\nintense laser pulse of a number of optical photons and then decays to a pair of\nDM particles. We take into account the Dirac-type fermionic DM in leptophilic\nscenario and the DM-electron interactions in the framework of effective field\ntheory. The decay rates of electron to a DM pair are calculated for effective\nDM operators of different bilinear products. We show the sensitivities of laser\ninduced Compton scattering to the effective cutoff scale for DM lighter than 1\nMeV and compare with direct detection experiments.",
        "Microscopic Schr{\\\"o}dinger cat states are generated from quantum correlated\nfields using a probabilistic heralding photon subtraction event. Subsequent\nquantum state tomography provides complete information about the state with\ntypical photon numbers of the order of one. Another approach strives for a\nlarger number of quantum-correlated photons by conditioning the measurement\nanalysis on events with exactly this number of photons. Here, we present a new\napproach to derive measurement data of quantum correlated states with average\nquantum-correlated photon numbers significantly larger than one. We produce an\nensemble of a heralded, photon-subtracted squeezed vacuum state of light. We\nsplit the states at a balanced beam splitter and simultaneously measure a pair\nof orthogonal field quadratures at the outputs using tomographic `Q-function\nhomodyne detection' (QHD). The final act is probabilistic two-copy data\npost-processing aiming for data from a new state with larger photon number.\nEvaluating the final tomographic data as that of a grown microscopic\nSchr{\\\"o}dinger cat state shows that the probabilistic post-processing\nincreased the photon number of $|\\alpha_0|^2 \\approx 1.2$ to $|\\alpha_2|^2\n\\approx 6.8$. Our concept for obtaining tomographic measurement data of\nmesoscopic non-classical states that never existed might be a turning point in\nmeasurement-based quantum technology.",
        "The $k$-Nearest Neighbour Cumulative Distribution Functions are measures of\nclustering for discrete datasets that are fast and efficient to compute. They\nare significantly more informative than the 2-point correlation function. Their\nconnection to $N$-point correlation functions, void probability functions and\nCounts-in-Cells is known. However, the connections between the CDFs and other\ngeometric and topological spatial summary statistics are yet to be fully\nexplored in the literature. This understanding will be crucial to find\noptimally informative summary statistics to analyse data from stage 4\ncosmological surveys. We explore quantitatively the geometric interpretations\nof the $k$NN CDF summary statistics. We establish an equivalence between the\n1NN CDF at radius $r$ and the volume of spheres with the same radius around the\ndata points. We show that higher $k$NN CDFs are equivalent to the volumes of\nintersections of $\\ge k$ spheres around the data points. We present similar\ngeometric interpretations for the $k$NN cross-correlation joint CDFs. We\nfurther show that the volume, or the CDFs, have information about the angles\nand arc lengths created at the intersections of spheres around the data points,\nwhich can be accessed through the derivatives of the CDF. We show this\ninformation is very similar to that captured by Germ Grain Minkowski\nFunctionals. Using a Fisher analysis we compare the information content and\nconstraining power of various data vectors constructed from the $k$NN CDFs and\nMinkowski Functionals. We find that the CDFs and their derivatives and the\nMinkowski Functionals have nearly identical information content. However, $k$NN\nCDFs are computationally orders of magnitude faster to evaluate. Finally, we\nfind that there is information in the full shape of the CDFs, and therefore\ncaution against using the values of the CDF only at sparsely sampled radii.",
        "When cosmic strings are formed during inflation, they regrow to reach a\nscaling regime, leaving distinct imprints on the stochastic gravitational wave\nbackground (SGWB). Such signatures, associated with specific primordial\nfeatures, can be detected by upcoming gravitational wave observatories, such as\nthe LISA and Einstein Telescope (ET). Our analysis explores scenarios in which\ncosmic strings form either before or during inflation. We examine how the\nnumber of e-folds experienced by cosmic strings during inflation correlates\nwith the predictions of inflationary models observable in cosmic microwave\nbackground (CMB) measurements. This correlation provides a testable link\nbetween inflationary physics and the associated gravitational wave signals in a\ncomplementary manner. Focusing on $\\alpha$-attractor models of inflation, with\nthe Polynomial $\\alpha$-attractor serving as an illustrative example, we find\nconstraints, for instance, on the spectral index $n_s$ to $0.962 \\lesssim n_s\n\\lesssim 0.972$ for polynomial exponent $n=1$, $0.956 \\lesssim n_s \\lesssim\n0.968$ for $n=2$, $0.954 \\lesssim n_s \\lesssim 0.965$ for $n=3$, and $0.963\n\\lesssim n_s \\lesssim 0.964$ for $n=4$, which along with the GW signals from\nLISA, are capable of detecting local cosmic strings that have experienced $\\sim\n34 - 47$ e-folds of inflation consistent with current Planck data and are also\ntestable in upcoming CMB experiments such as LiteBIRD and CMB-S4.",
        "In this talks, on behalf of our collaboration we present an overview of our\nfirst-principle lattice QCD calculation of the $D_s\\mapsto X \\ell\n\\bar{\\nu}_\\ell$ inclusive decay rate. Here we introduce the theoretical\nbackground and focus on the methodological aspects of the calculation. A\ndetailed discussion of our results, including the comparison with the\ncorresponding experimental measurements will soon be presented in a forthcoming\npublication."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study",
    "start_abstract":"Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study"
      ],
      "abstract":[
        "Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Performance of Practical Quantum Oblivious Key Distribution",
        "SCE: Scalable Consistency Ensembles Make Blackbox Large Language Model\n  Generation More Reliable",
        "Heterogenous Macro-Finance Model: A Mean-field Game Approach",
        "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
        "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling",
        "Logistic regression models: practical induced prior specification",
        "Perturbative solutions for compact objects in (2+1)-dimensional\n  Bopp-Podolsky electrodynamics",
        "How to compute the volume in low dimension?",
        "Deeply Optimizing the SAT Solver for the IC3 Algorithm",
        "Remarks on log pluricanonical representations",
        "Dedicated Inference Engine and Binary-Weight Neural Networks for\n  Lightweight Instance Segmentation",
        "Run-and-tumble particles with 1D Coulomb interaction: the active jellium\n  model and the non-reciprocal self-gravitating gas",
        "Revealing the Implicit Noise-based Imprint of Generative Models",
        "Rationality and categorical properties of the moduli of instanton\n  bundles on the projective 3-space",
        "A Spiral Bicycle Track that Can Be Traced by a Unicycle",
        "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
        "A Unified Model of Text and Citations for Topic-Specific Citation\n  Networks",
        "Backtracking for Safety",
        "Improving robot understanding using conversational AI: demonstration and\n  feasibility study",
        "Genus formulas for families of modular curves",
        "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image\n  Enhancement",
        "Learning Orientation Field for OSM-Guided Autonomous Navigation",
        "Improving Multi-Label Contrastive Learning by Leveraging Label\n  Distribution",
        "The class of Aronszajn lines under epimorphisms",
        "Indoor Fusion Positioning Based on \"IMU-Ultrasonic-UWB\" and Factor Graph\n  Optimization Method",
        "Robust Watermarks Leak: Channel-Aware Feature Extraction Enables\n  Adversarial Watermark Manipulation",
        "Impact of Lead Time on Aggregate EV Flexibility for Congestion\n  Management Services",
        "Task-Agnostic Attacks Against Vision Foundation Models",
        "Single-Letter Characterization of the Mismatched Distortion-Rate\n  Function"
      ],
      "abstract":[
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Large language models (LLMs) have demonstrated remarkable performance, yet\ntheir diverse strengths and weaknesses prevent any single LLM from achieving\ndominance across all tasks. Ensembling multiple LLMs is a promising approach to\ngenerate reliable responses but conventional ensembling frameworks suffer from\nhigh computational overheads. This work introduces Scalable Consistency\nEnsemble (SCE), an efficient framework for ensembling LLMs by prompting\nconsistent outputs. The SCE framework systematically evaluates and integrates\noutputs to produce a cohesive result through two core components: SCE-CHECK, a\nmechanism that gauges the consistency between response pairs via semantic\nequivalence; and SCE-FUSION, which adeptly merges the highest-ranked consistent\nresponses from SCE-CHECK, to optimize collective strengths and mitigating\npotential weaknesses. To improve the scalability with multiple inference\nqueries, we further propose ``{You Only Prompt Once}'' (YOPO), a novel\ntechnique that reduces the inference complexity of pairwise comparison from\nquadratic to constant time. We perform extensive empirical evaluations on\ndiverse benchmark datasets to demonstrate \\methodName's effectiveness. Notably,\nthe \\saccheckcomponent outperforms conventional baselines with enhanced\nperformance and a significant reduction in computational overhead.",
        "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.",
        "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.",
        "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.",
        "Bayesian inference for statistical models with a hierarchical structure is\noften characterized by specification of priors for parameters at different\nlevels of the hierarchy. When higher level parameters are functions of the\nlower level parameters, specifying a prior on the lower level parameters leads\nto induced priors on the higher level parameters. However, what are deemed\nuninformative priors for lower level parameters can induce strikingly non-vague\npriors for higher level parameters. Depending on the sample size and specific\nmodel parameterization, these priors can then have unintended effects on the\nposterior distribution of the higher level parameters.\n  Here we focus on Bayesian inference for the Bernoulli distribution parameter\n$\\theta$ which is modeled as a function of covariates via a logistic\nregression, where the coefficients are the lower level parameters for which\npriors are specified. A specific area of interest and application is the\nmodeling of survival probabilities in capture-recapture data and occupancy and\ndetection probabilities in presence-absence data. In particular we propose\nalternative priors for the coefficients that yield specific induced priors for\n$\\theta$. We address three induced prior cases. The simplest is when the\ninduced prior for $\\theta$ is Uniform(0,1). The second case is when the induced\nprior for $\\theta$ is an arbitrary Beta($\\alpha$, $\\beta$) distribution. The\nthird case is one where the intercept in the logistic model is to be treated\ndistinct from the partial slope coefficients; e.g., $E[\\theta]$ equals a\nspecified value on (0,1) when all covariates equal 0. Simulation studies were\ncarried out to evaluate performance of these priors and the methods were\napplied to a real presence\/absence data set and occupancy modelling.",
        "We investigate the space-time geometry generated by compact objects in\n(2+1)-dimensional Bopp-Podolsky electrodynamics. Inspired by previous studies\nwhere the Bopp-Podolsky field acts as a source for spherically symmetric\nsolutions, we revisit this question within the lower-dimensional (2+1)\nframework. Using a perturbative approach, we derive a charged BTZ-like black\nhole solution and compute corrections up to second order in a perturbative\nexpansion valid far from the horizon. Our analysis suggests that the\nnear-horizon and inner structure of the solution remain unaltered, indicating\nthat no new non-black hole objects emerge in this regime. In particular, we do\nnot find evidence of wormhole solutions in the (2+1)-dimensional version of\nthis theory.",
        "Estimating the volume of a convex body is a canonical problem in theoretical\ncomputer science. Its study has led to major advances in randomized algorithms,\nMarkov chain theory, and computational geometry. In particular, determining the\nquery complexity of volume estimation to a membership oracle has been a\nlongstanding open question. Most of the previous work focuses on the\nhigh-dimensional limit. In this work, we tightly characterize the\ndeterministic, randomized and quantum query complexity of this problem in the\nhigh-precision limit, i.e., when the dimension is constant.",
        "The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm\nthat has significantly influenced the field in recent years due to its\nefficiency, scalability, and completeness. It utilizes SAT solvers to solve a\nseries of SAT queries associated with relative induction. In this paper, we\nintroduce several optimizations for the SAT solver in IC3 based on our\nobservations of the unique characteristics of these SAT queries. By observing\nthat SAT queries do not necessarily require decisions on all variables, we\ncompute a subset of variables that need to be decided before each solving\nprocess while ensuring that the result remains unaffected. Additionally, noting\nthat the overhead of binary heap operations in VSIDS is non-negligible, we\nreplace the binary heap with buckets to achieve constant-time operations.\nFurthermore, we support temporary clauses without the need to allocate a new\nactivation variable for each solving process, thereby eliminating the need to\nreset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which\nintegrates these optimizations. A comprehensive evaluation highlights the\nperformance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3\ndemonstrates an average speedup of 3.61 times in solving time compared to the\nIC3 implementation based on MiniSat.",
        "We show the finiteness of log pluricanonical representations under the\nassumption of the existence of a good minimal model.",
        "Reducing computational costs is an important issue for development of\nembedded systems. Binary-weight Neural Networks (BNNs), in which weights are\nbinarized and activations are quantized, are employed to reduce computational\ncosts of various kinds of applications. In this paper, a design methodology of\nhardware architecture for inference engines is proposed to handle modern BNNs\nwith two operation modes. Multiply-Accumulate (MAC) operations can be\nsimplified by replacing multiply operations with bitwise operations. The\nproposed method can effectively reduce the gate count of inference engines by\nremoving a part of computational costs from the hardware system. The\narchitecture of MAC operations can calculate the inference results of BNNs\nefficiently with only 52% of hardware costs compared with the related works. To\nshow that the inference engine can handle practical applications, two\nlightweight networks which combine the backbones of SegNeXt and the decoder of\nSparseInst for instance segmentation are also proposed. The output results of\nthe lightweight networks are computed using only bitwise operations and add\noperations. The proposed inference engine has lower hardware costs than related\nworks. The experimental results show that the proposed inference engine can\nhandle the proposed instance-segmentation networks and achieves higher accuracy\nthan YOLACT on the \"Person\" category although the model size is 77.7$\\times$\nsmaller compared with YOLACT.",
        "Recently we studied $N$ run-and-tumble particles in one dimension - which\nswitch with rate $\\gamma$ between driving velocities $\\pm v_0$ - interacting\nvia the long range 1D Coulomb potential (also called rank interaction), both in\nthe attractive and in the repulsive case, with and without a confining\npotential. We extend this study in two directions. First we consider the same\nsystem, but inside a harmonic confining potential, which we call \"active\njellium\". We obtain a parametric representation of the particle density in the\nstationary state at large $N$, which we analyze in detail. Contrary to the\nlinear potential, there is always a steady-state where the density has a\nbounded support. However, we find that the model still exhibits transitions\nbetween phases with different behaviors of the density at the edges, ranging\nfrom a continuous decay to a jump, or even a shock (i.e. a cluster of\nparticles, which manifests as a delta peak in the density). Notably, the\ninteractions forbid a divergent density at the edges, which may occur in the\nnon-interacting case. In the second part, we consider a non-reciprocal version\nof the rank interaction: the $+$ particles (of velocity $+v_0$) are attracted\ntowards the $-$ particles (of velocity $-v_0$) with a constant force $b\/N$,\nwhile the $-$ particles are repelled by the $+$ particles with a force of same\namplitude. In order for a stationary state to exist we add a linear confining\npotential. We derive an explicit expression for the stationary density at large\n$N$, which exhibits an explicit breaking of the mirror symmetry with respect to\n$x=0$. This again shows the existence of several phases, which differ by the\npresence or absence of a shock at $x=0$, with one phase even exhibiting a\nvanishing density on the whole region $x>0$. Our analytical results are\ncomplemented by numerical simulations for finite $N$.",
        "With the rapid advancement of vision generation models, the potential\nsecurity risks stemming from synthetic visual content have garnered increasing\nattention, posing significant challenges for AI-generated image detection.\nExisting methods suffer from inadequate generalization capabilities, resulting\nin unsatisfactory performance on emerging generative models. To address this\nissue, this paper presents a novel framework that leverages noise-based\nmodel-specific imprint for the detection task. Specifically, we propose a novel\nnoise-based imprint simulator to capture intrinsic patterns imprinted in images\ngenerated by different models. By aggregating imprints from various generative\nmodels, imprints of future models can be extrapolated to expand training data,\nthereby enhancing generalization and robustness. Furthermore, we design a new\npipeline that pioneers the use of noise patterns, derived from a noise-based\nimprint extractor, alongside other visual features for AI-generated image\ndetection, resulting in a significant improvement in performance. Our approach\nachieves state-of-the-art performance across three public benchmarks including\nGenImage, Synthbuster and Chameleon.",
        "We prove the rationality and irreducibility of the moduli space of\nmathematical instanton vector bundles of arbitrary rank and charge on $\\mathbb\nP^3$. In particular, the result applies to the rank-2 case. This problem was\nfirst studied by Barth, Hartshorne, Hirschowitz-Narasimhan in the late 1970s.\nWe also show that the mathematical instantons of variable rank and charge form\na monoidal category. The proof is based on an in-depth analysis of the\nBarth-Hulek monad-construction and on a detailed description of the moduli\nspace of (framed and unframed) stable bundles on Hirzebruch surfaces.",
        "A unibike curve is a track that can be made by either a bicycle or a\nunicycle. More precisely, the end of a unit tangent vector at any point on a\nunibike curve lies on the curve (so the bike's front wheel always lies on the\ntrack made by the rear wheel). David Finn found such a curve in 2002, but it\nloops around itself in an extremely complicated way with many twists and\nself-intersections. Starting with the polar square root curve r = sqrt[t\/(2\npi)] and iterating a simple construction involving a differential equation\napparently leads in the limit to a unibike curve having a spiral shape. The\niteration gets each curve as a rear track of its predecessor. Solving hundreds\nof differential equations numerically, where each depends on the preceding one,\nleads to error buildup, but with some care one can get a curve having unibike\nerror less than 10^-7. The evidence is strong for the conjecture that the limit\nof the iteration exists and is a unibike curve.",
        "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
        "Social scientists analyze citation networks to study how documents influence\nsubsequent work across various domains such as judicial politics and\ninternational relations. However, conventional approaches that summarize\ndocument attributes in citation networks often overlook the diverse semantic\ncontexts in which citations occur. This paper develops the paragraph-citation\ntopic model (PCTM), which analyzes citation networks and document texts\njointly. The PCTM extends conventional topic models by assigning topics to\nparagraphs of citing documents, allowing citations to share topics with their\nembedding paragraphs. Our empirical analysis of U.S. Supreme Court opinions in\nthe privacy issue domain, which includes cases on reproductive rights,\ndemonstrates that citations within individual documents frequently span\nmultiple substantive areas, and citations to individual documents show\nconsiderable topical diversity.",
        "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, but ensuring their safety and alignment with human values\nremains crucial. Current safety alignment methods, such as supervised\nfine-tuning and reinforcement learning-based approaches, can exhibit\nvulnerabilities to adversarial attacks and often result in shallow safety\nalignment, primarily focusing on preventing harmful content in the initial\ntokens of the generated output. While methods like resetting can help recover\nfrom unsafe generations by discarding previous tokens and restarting the\ngeneration process, they are not well-suited for addressing nuanced safety\nviolations like toxicity that may arise within otherwise benign and lengthy\ngenerations. In this paper, we propose a novel backtracking method designed to\naddress these limitations. Our method allows the model to revert to a safer\ngeneration state, not necessarily at the beginning, when safety violations\noccur during generation. This approach enables targeted correction of\nproblematic segments without discarding the entire generated text, thereby\npreserving efficiency. We demonstrate that our method dramatically reduces\ntoxicity appearing through the generation process with minimal impact to\nefficiency.",
        "Explanations constitute an important aspect of successful human robot\ninteractions and can enhance robot understanding. To improve the understanding\nof the robot, we have developed four levels of explanation (LOE) based on two\nquestions: what needs to be explained, and why the robot has made a particular\ndecision. The understandable robot requires a communicative action when there\nis disparity between the human s mental model of the robot and the robots state\nof mind. This communicative action was generated by utilizing a conversational\nAI platform to generate explanations. An adaptive dialog was implemented for\ntransition from one LOE to another. Here, we demonstrate the adaptive dialog in\na collaborative task with errors and provide results of a feasibility study\nwith users.",
        "For each open subgroup $H\\leq \\operatorname{GL}_2(\\widehat{\\mathbb{Z}})$,\nthere is a modular curve $X_H$, defined as a quotient of the full modular curve\n$X(N)$, where $N$ is the level of $H$. The genus formula of a modular curve is\nwell known for $X_0(N)$, $X_1(N)$, $X(N)$, $X_{\\mathrm{sp}}(N)$,\n$X_{\\mathrm{ns}}(N)$, and $X_{S_4}(p)$ for $p$ prime. We explicitly work out\nthe invariants of the genus formulas for $X_{\\mathrm{sp}}^+(N)$,\n$X_{\\mathrm{ns}}^+(N)$, and $X_{\\text{arith},1}(M,MN)$. In Table $1$, we\nprovide the invariants of the genus formulas for all of the modular curves\nlisted.",
        "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https:\/\/github.com\/OUCVisionGroup\/DPF-Net.",
        "OpenStreetMap (OSM) has gained popularity recently in autonomous navigation\ndue to its public accessibility, lower maintenance costs, and broader\ngeographical coverage. However, existing methods often struggle with noisy OSM\ndata and incomplete sensor observations, leading to inaccuracies in trajectory\nplanning. These challenges are particularly evident in complex driving\nscenarios, such as at intersections or facing occlusions. To address these\nchallenges, we propose a robust and explainable two-stage framework to learn an\nOrientation Field (OrField) for robot navigation by integrating LiDAR scans and\nOSM routes. In the first stage, we introduce the novel representation, OrField,\nwhich can provide orientations for each grid on the map, reasoning jointly from\nnoisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep\nneural network by encoding a versatile initial OrField and output an optimized\nOrField. Based on OrField, we propose two trajectory planners for OSM-guided\nrobot navigation, called Field-RRT* and Field-Bezier, respectively, in the\nsecond stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and\nBezier curve to estimate the trajectories. Thanks to the robustness of OrField\nwhich captures both global and local information, Field-RRT* and Field-Bezier\ncan generate accurate and reliable trajectories even in challenging conditions.\nWe validate our approach through experiments on the SemanticKITTI dataset and\nour own campus dataset. The results demonstrate the effectiveness of our\nmethod, achieving superior performance in complex and noisy conditions. Our\ncode for network training and real-world deployment is available at\nhttps:\/\/github.com\/IMRL\/OriField.",
        "In multi-label learning, leveraging contrastive learning to learn better\nrepresentations faces a key challenge: selecting positive and negative samples\nand effectively utilizing label information. Previous studies selected positive\nand negative samples based on the overlap between labels and used them for\nlabel-wise loss balancing. However, these methods suffer from a complex\nselection process and fail to account for the varying importance of different\nlabels. To address these problems, we propose a novel method that improves\nmulti-label contrastive learning through label distribution. Specifically, when\nselecting positive and negative samples, we only need to consider whether there\nis an intersection between labels. To model the relationships between labels,\nwe introduce two methods to recover label distributions from logical labels,\nbased on Radial Basis Function (RBF) and contrastive loss, respectively. We\nevaluate our method on nine widely used multi-label datasets, including image\nand vector datasets. The results demonstrate that our method outperforms\nstate-of-the-art methods in six evaluation metrics.",
        "A linear order $A$ is called strongly surjective if for every non empty\nsuborder $B \\preceq A$, there is an epimorphism from $A$ onto $B$ (denoted by\n$B \\trianglelefteq A$). We show, answering some questions of D\\'aniel T.\nSoukup, that under $\\mathsf{MA}_{\\aleph_{1}}$ there is a strongly surjective\nCountryman line. We also study the general structure of the class of Aronszajn\nlines under $\\trianglelefteq$, and compare it with the well known embeddability\nrelation $\\preceq$. Under $\\mathsf{PFA}$, the class of Aronszajn lines and the\nclass of countable linear orders enjoy similar nice properties when viewed\nunder the embeddability relation; both are well-quasi-ordered and have a finite\nbasis. We show that this analogy does not extend perfectly to the\n$\\trianglelefteq$ relation; while it is known that the countable linear orders\nare still well-quasi-ordered under $\\trianglelefteq$, we show that already in\n$\\mathsf{ZFC}$ the class of Aronszajn lines has an infinite antichain, and\nunder $\\mathsf{MA}_{\\aleph_{1}}$ an infinite decreasing chain as well. We show\nthat some of the analogy survives by proving that under $\\mathsf{PFA}$, for\nsome carefully constructed Countryman line $C$, $C$ and $C^{\\star}$ form a\n$\\trianglelefteq$-basis for the class of Aronszajn lines. Finally we show that\nthis does not extend to all uncountable linear orders by proving that there is\nnever a finite $\\trianglelefteq$-basis for the uncountable real orders.",
        "This paper presents a high-precision positioning system that integrates\nultra-wideband (UWB) time difference of arrival (TDoA) measurements, inertial\nmeasurement unit (IMU) data, and ultrasonic sensors through factor graph\noptimization. To overcome the shortcomings of standalone UWB systems in\nnon-line-of-sight (NLOS) scenarios and the inherent drift associated with\ninertial navigation, we developed a novel hybrid fusion framework. First, a\ndynamic covariance estimation mechanism is incorporated, which automatically\nadjusts measurement weights based on real-time channel conditions. Then, a\ntightly-coupled sensor fusion architecture is employed, utilizing IMU\npre-integration theory for temporal synchronization. Finally, a sliding-window\nfactor graph optimization backend is utilized, incorporating NLOS mitigation\nconstraints. Experimental results in complex indoor environments show a 38\\%\nimprovement in positioning accuracy compared to conventional Kalman\nfilter-based approaches, achieving a 12.3 cm root mean square (RMS) error under\ndynamic motion conditions. The system maintains robust performance even with\nintermittent UWB signal availability, down to a 40\\% packet reception rate,\neffectively suppressing IMU drift through multi-modal constraint fusion. This\nwork offers a practical solution for applications that require reliable indoor\npositioning in GPS-denied environments.",
        "Watermarking plays a key role in the provenance and detection of AI-generated\ncontent. While existing methods prioritize robustness against real-world\ndistortions (e.g., JPEG compression and noise addition), we reveal a\nfundamental tradeoff: such robust watermarks inherently improve the redundancy\nof detectable patterns encoded into images, creating exploitable information\nleakage. To leverage this, we propose an attack framework that extracts leakage\nof watermark patterns through multi-channel feature learning using a\npre-trained vision model. Unlike prior works requiring massive data or detector\naccess, our method achieves both forgery and detection evasion with a single\nwatermarked image. Extensive experiments demonstrate that our method achieves a\n60\\% success rate gain in detection evasion and 51\\% improvement in forgery\naccuracy compared to state-of-the-art methods while maintaining visual\nfidelity. Our work exposes the robustness-stealthiness paradox: current\n\"robust\" watermarks sacrifice security for distortion resistance, providing\ninsights for future watermark design.",
        "Increased electrification of energy end-usage can lead to network congestion\nduring periods of high consumption. Flexibility of loads, such as aggregate\nsmart charging of Electric Vehicles (EVs), is increasingly leveraged to manage\ngrid congestion through various market-based mechanisms. Under such an\narrangement, this paper quantifies the effect of lead time on the aggregate\nflexibility of EV fleets. Simulations using real-world charging transactions\nspanning over different categories of charging stations are performed for two\nflexibility products (redispatch and capacity limitations) when offered along\nwith different business-as-usual (BAU) schedules. Results show that the\nvariation of tradable flexibility depends mainly on the BAU schedules, the\nduration of the requested flexibility, and its start time. Further, the\nimplication of these flexibility products on the average energy costs and\nemissions is also studied for different cases. Simulations show that\nbidirectional (V2G) charging outperforms unidirectional smart charging in all\ncases.",
        "The study of security in machine learning mainly focuses on downstream\ntask-specific attacks, where the adversarial example is obtained by optimizing\na loss function specific to the downstream task. At the same time, it has\nbecome standard practice for machine learning practitioners to adopt publicly\navailable pre-trained vision foundation models, effectively sharing a common\nbackbone architecture across a multitude of applications such as\nclassification, segmentation, depth estimation, retrieval, question-answering\nand more. The study of attacks on such foundation models and their impact to\nmultiple downstream tasks remains vastly unexplored. This work proposes a\ngeneral framework that forges task-agnostic adversarial examples by maximally\ndisrupting the feature representation obtained with foundation models. We\nextensively evaluate the security of the feature representations obtained by\npopular vision foundation models by measuring the impact of this attack on\nmultiple downstream tasks and its transferability between models.",
        "The mismatched distortion-rate problem has remained open since its\nformulation by Lapidoth in 1997. In this paper, we characterize the mismatched\ndistortion-rate function. Our single-letter solution highlights the adequate\nconditional distributions for the encoder and the decoder. The achievability\nresult relies on a time-sharing argument that allows to convexify the upper\nbound of Lapidoth. We show that it is sufficient to consider two regimes, one\nwith a large rate and another one with a small rate. Our main contribution is\nthe converse proof. Suppose that the encoder selects a single-letter\nconditional distribution distinct from the one in the solution, we construct an\nencoding strategy that leads to the same expected cost for both encoder and\ndecoder. This ensures that the encoder cannot gain by changing the\nsingle-letter conditional distribution. This argument relies on a careful\nidentification of the sequence of auxiliary random variables. By building on\nCaratheodory's Theorem we show that the cardinality of the auxiliary random\nvariables is equal to the one of the source alphabet plus three."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Convolutional recurrent neural networks for music classification",
    "start_abstract":"We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A deep representation for invariance and music classification"
      ],
      "abstract":[
        "Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification."
      ],
      "categories":[
        "physics.class-ph"
      ]
    },
    "list":{
      "title":[
        "NeuroPMD: Neural Fields for Density Estimation on Product Manifolds",
        "Superstructure reflexions in tilted perovskites Part 1",
        "Qubit operations using a modular optical system engineered with\n  PyOpticL: a code-to-CAD optical layout tool",
        "Low-energy neutron cross-talk between organic scintillator detectors",
        "Serrin's overdetermined problems on epigraphs",
        "Symbolic Computations of the Two-Colored Diagrams for Central\n  Configurations of the Planar N-vortex Problem",
        "Local perfect chirality at reflection-zeros away from exceptional points\n  in optical whispering gallery microcavity",
        "The Lehmer complex of a Bruhat interval",
        "Theoretical determination of Gilbert damping in reduced dimensions",
        "Pricing American Parisian Options under General Time-Inhomogeneous\n  Markov Models",
        "Algorithms for 2-Solvable Difference Equations",
        "On a formula of the $q$-series $_{2k+4}\\phi_{2k+3}$ and its applications",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "The nexus between disease surveillance, adaptive human behavior and\n  epidemic containment",
        "Orbital Angular Momentum Experimental Bound on the Maximum Predictive\n  Power of Physical Theories in Multi-Dimensional Systems",
        "A multi-component phase-field model for T1 precipitates in Al-Cu-Li\n  alloys",
        "A numerical analysis of Araki-Uhlmann relative entropy in Quantum Field\n  Theory",
        "The Ladder and Readout Cables of Intermediate Silicon Strip Detector for\n  sPHENIX",
        "Consistent Solutions of the Radiation Diffusion Equation in Spherical\n  and Cylindrical Geometries",
        "Demystifying the fusion mechanism in heavy-ion collisions: A\n  six-dimensional Langevin dissipative dynamics approach",
        "Exclusive photoproduction of $\\chi_{c}\\gamma$ pairs",
        "Optimal Response for Hyperbolic Systems by the fast adjoint response\n  method",
        "A trailing lognormal approximation of the Lyman-$\\alpha$ forest:\n  comparison with full hydrodynamic simulations at $2.2\\leq z\\leq 2.7$",
        "Choice Sets and Smart Card Data In Public Transport Route Choice Models:\n  Generated vs. Empirical Sets",
        "Nonlinearity and Quantumness in Thermodynamics: From Principles to\n  Technologies",
        "Computing ternary liquid phase diagrams: Fe-Cu-Ni",
        "Electroweak Scalar Effects Beyond Dimension-6 in SMEFT",
        "A Constant Rate Quantum Computer on a Line",
        "Chemical distance in the Poisson Boolean model with regularly varying\n  diameters"
      ],
      "abstract":[
        "We propose a novel deep neural network methodology for density estimation on\nproduct Riemannian manifold domains. In our approach, the network directly\nparameterizes the unknown density function and is trained using a penalized\nmaximum likelihood framework, with a penalty term formed using manifold\ndifferential operators. The network architecture and estimation algorithm are\ncarefully designed to handle the challenges of high-dimensional product\nmanifold domains, effectively mitigating the curse of dimensionality that\nlimits traditional kernel and basis expansion estimators, as well as overcoming\nthe convergence issues encountered by non-specialized neural network methods.\nExtensive simulations and a real-world application to brain structural\nconnectivity data highlight the clear advantages of our method over the\ncompeting alternatives.",
        "The superstructure spots that appear in diffraction patterns of tilted\nperovskites are well documented and easily calculated using crystallographic\nsoftware. Here, by considering a distortion mode as a perturbation of the\nprototype perovskite structure, we show how the structure factor equation\nyields Boolean conditions for the presence of first order superstructure\nreflexions. A subsequent article describes conditions for second order\nreflexions, which appear only in structures with mixed in-phase and anti-phase\noxygen octahedral tilting. This approach may have some advantages for the\nanalysis of electron diffraction patterns of perovskites.",
        "Complex optical design is hindered by conventional piecewise setup, which\nprevents modularization and therefore abstraction of subsystems at the circuit\nlevel. This limits multiple fields that require complex optics systems,\nincluding quantum computing with atoms and trapped ions, because their optical\nsystems are not scalable. We present an open-source Python library for optical\nlayout (PyOpticL) which uses beam-path simulation and dynamic beam-path routing\nfor quick and easy optical layout by placing optical elements along the beam\npath without a priori specification, enabling dynamic layouts with automatic\nrouting and connectivity. We use PyOpticL to create modular drop-in optical\nbaseplates for common optical subsystems used in atomic and molecular optics\n(AMO) experiments including laser sources, frequency and intensity modulation,\nand locking to an atomic reference for stabilization. We demonstrate this\nminimal working example of a dynamic full laser system for strontium trapped\nions by using it for laser cooling, qubit state detection, and 99.9% fidelity\nsingle-qubit gates with 3D printed baseplates. This enables a new paradigm of\ndesign abstraction layers for engineering optical systems leveraging modular\nbaseplates, as they can be used for any wavelength in the system and enables\nscaling up the underlying optical systems for quantum computers. This new\nopen-source hardware and software code-to-CAD library seeks to foster\nopen-source collaborative hardware and systems design across numerous fields of\nresearch including AMO physics and quantum computing with neutral atoms and\ntrapped ions.",
        "A series of measurements have been performed with low-energy monoenergetic\nneutrons to characterise cross-talk between two organic scintillator detectors.\nCross-talk time-of-flight spectra and probabilities were determined for neutron\nenergies from 1.4 to 15.5 MeV and effective scattering angles ranging from\n$\\sim$50{\\deg} to $\\sim$100{\\deg}. Monte-Carlo simulations incorporating both\nthe active and inactive materials making up the detectors showed reasonable\nagreement with the measurements. Whilst the time-of-flight spectra were very\nwell reproduced, the cross-talk probabilities were only in approximate\nagreement with the measurements, with the most significant discrepancies\n($\\sim$40 %) occurring at the lowest energies. The neutron interaction\nprocesses producing cross-talk at the energies explored here are discussed in\nthe light of these results.",
        "In this work we establish some rigidity results for Serrin's overdetermined\nproblem \\begin{equation*}\n  \\left\\{\n  \\begin{array}{cll}\n  - \\Delta u=f(u) & \\text{in}& \\Omega,\\newline\n  u > 0& \\text{in} & \\Omega,\\newline\n  u=0 & \\text{on} & \\partial \\Omega,\\newline\n  \\dfrac{\\partial u}{\\partial \\eta} = \\mathfrak{c} = const. & \\text{on} &\n\\partial \\Omega,\n  \\end{array}\n  \\right. \\end{equation*}\n  when $\\Omega \\subset \\mathbb{R}^N$ is an epigraph (not necessarily globally\nLipschitz-continuous) and $u$ is a classical solution, possibly unbounded. In\nbroad terms, our main results prove that $\\Omega$ must be an affine half-space\nand $u$ must be one-dimensional, provided the epigraph is bounded from below.\nThese results hold when $f$ is of Allen-Cahn type and $ N \\geq 2$ or,\nalternatively, when $f$ is locally Lipschitz-continuous (with no restriction on\nthe sign of $f(0)$) and $ N \\leq 3$. These results partially answer a question\nraised by Berestycki, Caffarelli and Nirenberg in [1]. Finally, when $f(0) <0$,\nwe also prove a new monotonicity result, valid in any dimension $ N \\geq 2$.",
        "We apply the singular sequence method to investigate the finiteness problem\nfor stationary configurations of the planar N-vortex problem. The initial step\nof the singular sequence method involves identifying all two-colored diagrams.\nThese diagrams represent potential scenarios where finiteness may fail. We\ndevelop a symbolic computation algorithm to determine all two-colored diagrams\nfor central configurations of the planar N-vortex problem.",
        "Recently, a local and imperfect chirality of the resonant eigenmode at the\nexceptional point (EP) has been reported in the optical whispering gallery\nmicrocavity system perturbed by two strong nanoscatterers [Phys. Rev. A 108,\nL041501 (2023)]. Here, we discover a local perfect chirality of the resonant\neigenmode away from the EP in the parameter space of the strongly perturbed\nmicrocavity system. By considering the multiple scattering process of the\nazimuthally propagating modes (APMs) at the nanoscatterers with a\nfirst-principles-based model, the local perfect chirality is predicted to\nresult from the unidirectional reflectionlessness, i.e., the reflection-zero\n(R-zero) of the APMs at the two nanoscatterers. Numerical results and model\npredictions consistently show that the structural parameters of the R-zero\ntypically deviate from those of the EP, which means that the pair of split\nresonant eigenmodes at the R-zero have different complex resonance frequencies\nand electromagnetic fields. In general, only one of the pair of split\neigenmodes exhibits a local perfect chirality within the local azimuthal range\ndivided by the two nanoscatterers. With the decrease of the two nanoscatterers'\nsizes or their relative azimuthal angle, the R-zero tends to coincide with the\nEP.",
        "We introduce Lehmer codes, with immersions in the Bruhat order, for several\nfinite Coxeter groups, including all the classical Weyl groups. This allows to\nassociate to each lower Bruhat interval of these groups a multicomplex whose\nf-polynomial is the Poincar\\'e polynomial of the interval. Via a general\nconstruction, we prove that these polynomials are h-polynomials of\nvertex-decomposable simplicial complexes. Moreover we provide a classification,\nin terms of unimodal permutations, of Poincar\\'e polynomials of smooth Schubert\nvarieties in flag manifolds.",
        "An ab initio scheme based on the linear response theory of exchange torque\ncorrelation is presented to calculate intrinsic Gilbert damping parameters in\nmagnets of reduced dimensions. The method implemented into the real-space\nKorringa-Kohn-Rostoker (RS-KKR) Greens' function framework enables to obtain\ndiagonal elements of the atomic-site-dependent on-site and non-local Gilbert\ndamping tensor. Going from the 3D bulk and surfaces of iron and cobalt\nferromagnets addressed in our previous work [Phys. Rev. B 109, 094417 (2024)],\nin the present paper monolayers of Fe and Co on (001)- and (111)-oriented Cu,\nAg, and Au substrates are studied, and particularly the substrate-dependent\ntrends are compared. Furthermore, the Gilbert damping parameters are calculated\nfor Fe and Co adatoms and dimers on (001)-oriented substrates. It is\ninvestigated how the damping parameter of single adatoms depends on their\nvertical position. This dependence is quantified in relation to the adatoms'\ndensity of states at the Fermi energy showing a non-monotonic behavior. By\nrotating the spin moment of the adatoms and collinear magnetic dimers, an\nanisotropic behavior of the damping is revealed. Finally, a significant, three-\nto ten-times increase of the on-site Gilbert damping is found in\nantiferromagnetic dimers in comparison to the ferromagnetic ones, whilst the\ninter-site damping is even more enhanced.",
        "This paper develops general approaches for pricing various types of\nAmerican-style Parisian options (down-in\/-out, perpetual\/finite-maturity) with\ngeneral payoff functions based on continuous-time Markov chain (CTMC)\napproximation under general 1D time-inhomogeneous Markov models. For the\ndown-in types, by conditioning on the Parisian stopping time, we reduce the\npricing problem to that of a series of vanilla American options with different\nmaturities and their prices integrated with the distribution function of the\nParisian stopping time yield the American Parisian down-in option price. This\nfacilitates an efficient application of CTMC approximation to obtain the\napproximate option price by calculating the required quantities. For the\nperpetual down-in cases under time-homogeneous models, significant\ncomputational cost can be reduced. The down-out cases are more complicated, for\nwhich we use the state augmentation approach to record the excursion duration\nand then the approximate option price is obtained by solving a series of\nvariational inequalities recursively with the Lemke's pivoting method. We show\nthe convergence of CTMC approximation for all the types of American Parisian\noptions under general time-inhomogeneous Markov models, and the accuracy and\nefficiency of our algorithms are confirmed with extensive numerical\nexperiments.",
        "Our paper \"Solving Third Order Linear Difference Equations in Terms of Second\nOrder Equations\" gave two algorithms for solving difference equations in terms\nof lower order equations: an algorithm for absolute factorization, and an\nalgorithm for solving third order equations in terms of second order. Here we\nimprove the efficiency for absolute factorization, and extend the other\nalgorithm to order four.",
        "In this paper we apply a formula of the very-well poised $_{2k+4}\\phi_{2k+3}$\nto write a $k$-tuple sum of $q$-series as a linear combination of terms wherein\neach term is a product of expressions of the form $\\frac{1}{(qy,\nqy^{-1};q)_\\infty}$. As an application, we shall express a variety of sums and\ndouble sums of $q$-series as linear combinations of infinite products. Our\nformulas are motivated by their connection to overpartition pairs.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Epidemics exhibit interconnected processes that operate at multiple time and\norganizational scales, a hallmark of complex adaptive systems. Modern\nepidemiological modeling frameworks incorporate feedback between\nindividual-level behavioral choices and centralized interventions. Nonetheless,\nthe realistic operational course for disease detection, planning, and response\nis often overlooked. Disease detection is a dynamic challenge, shaped by the\ninterplay between surveillance efforts and transmission characteristics. It\nserves as a tipping point that triggers emergency declarations, information\ndissemination, adaptive behavioral responses, and the deployment of public\nhealth interventions. Evaluating the impact of disease surveillance systems as\ntriggers for adaptive behavior and public health interventions is key to\ndesigning effective control policies.\n  We examine the multiple behavioral and epidemiological dynamics generated by\nthe feedback between disease surveillance and the intertwined dynamics of\ninformation and disease propagation. Specifically, we study the intertwined\ndynamics between: $(i)$ disease surveillance triggering health emergency\ndeclarations, $(ii)$ risk information dissemination producing decentralized\nbehavioral responses, and $(iii)$ centralized interventions. Our results show\nthat robust surveillance systems that quickly detect a disease outbreak can\ntrigger an early response from the population, leading to large epidemic sizes.\nThe key result is that the response scenarios that minimize the final epidemic\nsize are determined by the trade-off between the risk information dissemination\nand disease transmission, with the triggering effect of surveillance mediating\nthis trade-off. Finally, our results confirm that behavioral adaptation can\ncreate a hysteresis-like effect on the final epidemic size.",
        "The completeness of quantum mechanics in predictive power is a central\nquestion in its foundational study. While most investigations focus on\ntwo-dimensional systems, high-dimensional systems are more general and widely\napplicable. Building on the non-extensibility theorem by Colbeck and Renner\n[Phys. Rev. Lett. 101, 050403 (2008)], which established that no higher theory\ncan enhance the predictive power of quantum mechanics for two-dimensional\nsystems, we extend this result to arbitrarily dimensional systems. We connect\nmaximum potential predictive power achievable by any alternative theory to\nexperimentally observable correlations, and establish optimal experimental\nbounds across varying dimensions by exploiting two-photon orbital angular\nmomentum entangled states with entanglement concentration. These bounds falsify\na broader class of alternative theories, including Bell's and Leggett's models,\nand those that remain theoretically ambiguous or experimentally unverified. Our\nfindings not only deepen the foundational understanding of quantum mechanics\nbut also hold significant potential for high-dimensional quantum cryptography.",
        "In this study, the role of elastic and interfacial energies in the shape\nevolution of T1 precipitates in Al-Cu-Li alloys is investigated using\nphase-field modeling. We employ a formulation considering the stoichiometric\nnature of the precipitate phase explicitly, including coupled equation systems\nfor various order parameters. Inputs such as elastic properties are derived\nfrom DFT calculations, while chemical potentials are obtained from CALPHAD\ndatabases. This methodology provides a framework that is consistent with the\nderived chemical potentials to study the interplay of thermodynamic, kinetic,\nand elastic effects on T1 precipitate evolution in Al-Cu-Li alloys. It is shown\nthat diffusion-controlled lengthening and interface-controlled thickening are\nimportant mechanisms to describe the growth of T1 precipitates. Furthermore,\nthe study illustrates that the precipitate shape is significantly influenced by\nthe anisotropy in interfacial energy and linear reaction rate, however, elastic\neffects only play a minor role.",
        "We numerically investigate the Araki-Uhlmann relative entropy in Quantum\nField Theory, focusing on a free massive scalar field in 1+1-dimensional\nMinkowski spacetime. Using Tomita-Takesaki modular theory, we analyze the\nrelative entropy between a coherent state and the vacuum state, with several\ntypes of test functions localized in the right Rindler wedge. Our results\nconfirm that relative entropy decreases with increasing mass and grows with the\nsize of the spacetime region, aligning with theoretical expectations.",
        "A new silicon-strip-type detector was developed for precise charged-particle\ntracking in the central rapidity region of heavy ion collisions. A new detector\nand collaboration at the Relativistic Heavy Ion Collider at Brookhaven National\nLaboratory is sPHENIX, which is a major upgrade of the PHENIX detector. The\nintermediate tracker (INTT) is part of the advanced tracking system of the\nsPHENIX detector complex together with a CMOS monolithic-active-pixel-sensor\nbased silicon-pixel vertex detector, a time-projection chamber, and a\nmicromegas-based detector. The INTT detector is barrel shaped and comprises 56\nsilicon ladders. Two different types of strip sensors of 78~$\\mu m$ pitch and\n320~$\\mu m$ thick are mounted on each half of a silicon ladder. Each strip\nsensor is segmented into 8$\\times$2 and 5$\\times$2 blocks with lengths of 16\nand 20 mm. Strips are read out with a silicon strip-readout (FPHX) chip. In\norder to transmit massive data from the FPHX to the down stream readout\nelectronics card (ROC), a series of long and high speed readout cables were\ndeveloped. This document focuses on the silicon ladder, the readout cables, and\nthe ROC of the INTT. The radiation hardness is studied for some parts of the\nINTT devices in the last part of this document, since the INTT employed some\nmaterials from the technology frontier of the industry whose radiation hardness\nis not necessarily well known.",
        "We have extended the radiation diffusion model of Hammer and Rosen to\ndiverging spherical and cylindrical geometries. The effect of curvilinear\ngeometry on the supersonic, expanding wavefront increases as the internal\nradius of a spherical or cylindrical shell approaches zero. Small spherical\ngeometries are important for modeling systems at the size scale of ICF\ncapsules, at these scales existing quasi-analytic models for planar geometry\nsignificantly disagree with the results of simulation. With this method, the\nbenefits of rapid iteration can be applied to common spherical systems at much\nsmaller length scales. We present comparisons between numerical diffusion\nsolutions and the analytic model to give ranges of applicability for the model.",
        "We present an in-depth investigation of heavy-ion fusion dynamics using a\nsix-dimensional Langevin framework that enables unrestricted motion of the\nasymmetry parameter. The stochastic formalism naturally incorporates friction\neffects and energy fluctuations, providing a detailed understanding of the\nfusion process. The dynamics transition into the overdamped regime,\nfacilitating rapid neck stabilization while effectively capturing the interplay\nbetween shape and rotational degrees of freedom. This approach achieves\nexcellent agreement with experimental spin distributions and fusion\ncross-sections, establishing a robust foundation for forthcoming studies on the\nsynthesis of superheavy elements and the exploration of the enigmatic fusion\nhindrance mechanism.",
        "In this manuscript we study the exclusive photoproduction of $\\chi_{c}\\gamma$\npairs in the collinear factorization framework. We found the coefficient\nfunctions for all possible spins and helicity projections of $\\chi_{c}$ mesons\nand final-state photons in the leading order in the strong coupling\n$\\alpha_{s}$. In our analysis we focused on the contribution of the leading\ntwist chiral even GPDs, and found that for all spin states of $\\chi_{c}$ the\nsuggested process is determined by the behavior of the gluon GPDs $H_{g}$ in\nthe ERBL kinematics. Using the phenomenological parametrizations of the GPDs\nfrom the literature, we estimated numerically the photoproduction\ncross-sections and the expected counting rates in the kinematics of\nmiddle-energy photoproduction experiments that could be realized at the\nElectron-Ion Collider. We observed that the $\\chi_{c1}$, $\\chi_{c2}$ mesons are\nproduced predominantly with the same polarization as the incoming photon, and\nthe expected counting rates of $\\chi_{c1}\\gamma$ and $\\chi_{c2}\\gamma$ pairs\nare sufficiently large for their experimental study. We also analyzed the\nangular distribution of $\\chi_{c}$ mesons in electroproduction experiments, and\nnoticed that for some helicity components there are sizable angular\nasymmetries, which can be used as complementary observables for experimental\nstudy. Finally, we estimated the role of this process as a potential background\nto $\\chi_{c}$ photoproduction, which has been recently suggested as a tool for\nstudies of odderons. We found that the contribution of $\\chi_{c}\\gamma$ (with\nundetected photon) is on par with expected contributions of odderons and\nPrimakoff mechanisms in the kinematics of small momentum transfer\n$|t|\\lesssim1$ GeV$^{2}$, but becomes negligible at larger $|t|$.",
        "In a uniformly hyperbolic system, we consider the problem of finding the\noptimal infinitesimal perturbation to apply to the system, from a certain set\n$P$ of feasible ones, to maximally increase the expectation of a given\nobservation function. We perturb the system both by composing with a\ndiffeomorphism near the identity or by adding a deterministic perturbation to\nthe dynamics. In both cases, using the fast adjoint response formula, we show\nthat the linear response operator, which associates the response of the\nexpectation to the perturbation on the dynamics, is bounded in terms of the\n$C^{1,\\alpha}$ norm of the perturbation. Under the assumption that $P$ is a\nstrictly convex, closed subset of a Hilbert space $\\cH$ that can be\ncontinuously mapped in the space of $C^3$ vector fields on our phase space, we\nshow that there is a unique optimal perturbation in $P$ that maximizes the\nincrease of the given observation function. Furthermore since the response\noperator is represented by a certain element $v$ of $\\cH$, when the feasible\nset $P$ is the unit ball of $\\cH$, the optimal perturbation is $v\/||v||_{\\cH}$.\nWe also show how to compute the Fourier expansion $v$ in different cases. Our\napproach can work even on high dimensional systems. We demonstrate our method\non numerical examples in dimensions 2, 3, and 21.",
        "Lyman-$\\alpha$(Ly$\\alpha$) forest in the spectra of distant quasars encodes\nthe information of the underlying cosmic density field at smallest scales. The\nmodelling of the upcoming large and high-fidelity forest data using\ncosmological hydrodynamical simulations is computationally challenging and\ntherefore, requires accurate semi-analytical techniques. One such approach is\nbased on the assumption that baryonic density fields in the intergalactic\nmedium (IGM) follow lognormal distribution. Keeping this in mind, we extend our\nearlier work to improve the lognormal model of the Ly$\\alpha$ forest in\nrecovering the parameters characterizing IGM state, particularly the hydrogen\nphotoionization rate ($\\Gamma_{12}$), between $2.2 \\leq z \\leq 2.7$, by\nsimulating the model spectra at a slightly lower redshift than the Sherwood\nsmooth particle hydrodynamical simulations (SPH) data. The recovery of thermal\nparameters, namely, the mean-density IGM temperature ($T_0$) and the slope of\nthe temperature-density relation ($\\gamma$) is also alleviated. These\nparameters are estimated through a Markov Chain Monte Carlo (MCMC) technique,\nusing the mean and power spectrum of the transmitted flux. We find that the\nusual lognormal distribution of IGM densities tend to over-predict the number\nof Ly$\\alpha$ absorbers seen in SPH simulation. A lognormal model simulated at\na lower redshift than SPH data can address this limitation to a certain extent.\nWe show that with such a \"trailing\" model of lognormal distribution, values of\n$\\Gamma_{12}$ are recovered at $\\lesssim 1-\\sigma$. We argue that this model\ncan be useful for constraining cosmological parameters.",
        "This study evaluates path sets generation for route choice models in\nmultimodal public transportation networks, using both conventional (network\nalgorithms) and empirical (smart card data driven) methods. While the empirical\napproach can present limitations with a short observation period, it improves\nsubstantially with more data, offering a computational efficiency advantage\nover conventional methods. In such approach, while incorporating real-world\ndelays increased travel time variability, it still aligned with planned travel\ntimes, and relaxing access\/egress assumptions further enhanced coverage. Work\nis undergoing on the evaluation of the impact of different choice sets in bias\nand efficiency of route choice parameter estimates.",
        "The impact of quantum mechanics on thermodynamics, particularly on the\nprinciples and designs of heat machines (HM), has been limited by the\nincompatibility of quantum coherent evolution with the dissipative, open-system\nnature of all existing HM and their basic structure, which has not been\nradically changed since Carnot. We have recently proposed a paradigm change\nwhereby conventional HM functionality is replaced by that of few-mode coherent,\nclosed systems with nonlinear, e.g. cross-Kerr, inter-mode couplings. These\ncouplings allow us to coherently filter incident thermal noise, transforming it\ninto a resource of work and information. Current technological advances enable\nheat engines, noise sensors or microscopes based on such designs to operate\nwith thermal noise sources of few photons. This paradigm shift opens a path\ntowards radically new understanding and exploitation of the relation between\ncoherent, quantum or classical, evolution and thermodynamic behavior.",
        "We compute the phase separation of the immiscible liquid alloy Fe-Cu-Ni. Our\ncomputational approach uses a virtual semigrand canonical Widom approach to\ndetermine differences in excess chemical potentials between different species.\nUsing an embedded atom potential for Fe-Cu-Ni, we simulate liquid states over a\nrange of compositions and temperatures. This raw data is then fit to\nRedlich-Kister polynomials for the Gibbs free energy with a simple temperature\ndependence. Using the analytic form, we can determine the phase diagram for the\nternary liquid, compute the miscibility gap and spinodal decomposition as a\nfunction of temperature for this EAM potential. In addition, we compute density\nas a function of composition and temperature, and predict pair correlation\nfunctions. We use static structure factors to estimate the second derivative of\nthe Gibbs free energy (the $S^0$ method) and compare with our fit Gibbs free\nenergy. Finally, using a nonequilibrium Hamiltonian integration method, we\nseparately compute absolute Gibbs free energies for the pure liquid states;\nthis shows that our endpoints are accurate to within 1 meV for our ternary\nGibbs free energy, as well as the absolute Gibbs free energy for the ternary\nliquid.",
        "The Standard Model Effective Field Theory (SMEFT) provides a robust framework\nfor probing deviations in the couplings of Standard Model particles from their\ntheoretical predictions. This framework relies on an expansion in\nhigher-dimensional operators, often truncated at dimension-six. In this work,\nwe compute the effective dimension-eight operators generated by integrating out\nheavy scalar fields at one-loop order in the Green's basis within two extended\nscalar sector models: the Two Higgs Doublet Model and the Complex Triplet\nScalar Model. We also investigate the impact of heavy scalar fields on the\nfermion sector, deriving the fermionic effective operators up to dimension\neight for these models, and detail how contributions can be mapped onto\nnon-redundant bases. To assess the importance of higher-order contributions in\nthe SMEFT expansion, we analyze the dimension-eight effects for electroweak\nprecision observables at the next frontier of precision lepton machines such as\nGigaZ.",
        "We prove by construction that the Bravyi-Poulin-Terhal bound on the spatial\ndensity of stabilizer codes does not generalize to stabilizer circuits. To do\nso, we construct a fault tolerant quantum computer with a coding rate above 5%\nand quasi-polylog time overhead, out of a line of qubits with nearest-neighbor\nconnectivity, and prove it has a threshold. The construction is based on\nmodifications to the tower of Hamming codes of Yamasaki and Koashi (Nature\nPhysics, 2024), with operators measured using a variant of Shor's measurement\ngadget.",
        "We study the Poisson Boolean model with convex bodies which are\nrotation-invariant distributed. We assume that the convex bodies have regularly\nvarying diameters with indices $-\\alpha_1\\geq \\dots\\geq-\\alpha_d$ where\n$\\alpha_k >0$ for all $k\\in\\{1,\\dots,d\\}.$ It is known that a sufficient\ncondition for the robustness of the model, i.e. the union of the convex bodies\nhas an unbounded connected component no matter what the intensity of the\nunderlying Poisson process is, is that there exists some $k\\in\\{1,\\dots,d\\}$\nsuch that $\\alpha_k<\\min\\{2k,d\\}$. To avoid that this connected component\ncovers all of $\\mathbb{R}^d$ almost surely we also require $\\alpha_k> k$ for\nall $k\\in\\{1,\\dots,d\\}$. We show that under these assumptions, the chemical\ndistance of two far apart vertices $\\mathbf{x}$ and $\\mathbf{y}$ behaves like\n$c\\log\\log|x-y|$ as $|x-y|\\rightarrow \\infty$, with an explicit and very\nsurprising constant $c$ that depends only on the model parameters. We\nfurthermore show that if there exists $k$ such that $\\alpha_k\\leq k$, the\nchemical distance is smaller than $c\\log\\log|x-y|$ for all $c>0$ and that if\n$\\alpha_k\\geq\\min\\{2k,d\\}$ for all $k$, it is bigger than $c\\log\\log|x-y|$ for\nall $c>0$."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A deep representation for invariance and music classification",
    "start_abstract":"Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification.",
    "start_categories":[
      "physics.class-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Convolutional recurrent neural networks for music classification"
      ],
      "abstract":[
        "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "A shape-optimization approach for inverse diffusion problems using a\n  single boundary measurement",
        "Hydrated Cable Bacteria Exhibit Protonic Conductivity Over Long\n  Distances",
        "Torsion models for tensor-triangulated categories",
        "RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks",
        "Localization of critical points in annular conical sets via the method\n  of Nehari manifold",
        "Temporal Preference Optimization for Long-Form Video Understanding",
        "Detecting APT Malware Command and Control over HTTP(S) Using Contextual\n  Summaries",
        "PCSI -- The Platform for Content-Structure Inference",
        "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays",
        "Scalable intensity-based photonic matrix-vector multiplication processor\n  using single-wavelength time-division-multiplexed signals",
        "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an\n  AI-Driven Era",
        "Algorithms and Hardness Results for the $(k,\\ell)$-Cover Problem",
        "A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization",
        "Spread them Apart: Towards Robust Watermarking of Generated Content",
        "Uncertainty Expression for Human-Robot Task Communication",
        "Spatial Context-Driven Positive Pair Sampling for Enhanced\n  Histopathology Image Classification",
        "Resurrecting saturated LLM benchmarks with adversarial encoding",
        "Evolving Performance Practices in Beethoven's Cello Sonatas: Tempo,\n  Portamento, and Historical Interpretation of the First Movements",
        "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with\n  360$^\\circ$ Cameras",
        "A Binary Classification Social Network Dataset for Graph Machine\n  Learning",
        "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation",
        "$k$-SVD with Gradient Descent",
        "Efficient and Privacy-Preserved Link Prediction via Condensed Graphs",
        "DeepSolution: Boosting Complex Engineering Solution Design via\n  Tree-based Exploration and Bi-point Thinking",
        "Supervised Learning with Evolving Tasks and Performance Guarantees",
        "On Statistical Estimation of Edge-Reinforced Random Walks",
        "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
        "Semantically Cohesive Word Grouping in Indian Languages",
        "PropNet: a White-Box and Human-Like Network for Sentence Representation"
      ],
      "abstract":[
        "This paper explores the reconstruction of a space-dependent parameter in\ninverse diffusion problems, proposing a shape-optimization-based approach. The\nmain objective is to recover the absorption coefficient from a single boundary\nmeasurement. While conventional gradient-based methods rely on the Fr\\'{e}chet\nderivative of a cost functional with respect to the unknown parameter, we also\nutilize its shape derivative with respect to the unknown boundary interface for\nrecovery. This non-conventional approach addresses the problem of parameter\nrecovery from a single measurement, which represents the key innovation of this\nwork. Numerical experiments confirm the effectiveness of the proposed method,\neven for intricate and non-convex boundary interfaces.",
        "This study presents the direct measurement of proton transport along\nfilamentous Desulfobulbaceae, or cable bacteria. Cable bacteria are filamentous\nmulticellular microorganisms that have garnered much interest due to their\nability to serve as electrical conduits, transferring electrons over several\nmillimeters. Our results indicate that cable bacteria can also function as\nprotonic conduits because they contain proton wires that transport protons at\ndistances greater than 100 um. We find that protonic conductivity ({\\sigma}P)\nalong cable bacteria varies between samples and is measured as high as 114 +\/-\n28 uS cm^-1 at 25-degrees C and 70-percent relative humidity (RH). For cable\nbacteria, the protonic conductance (GP) and {\\sigma}P are dependent upon the\nRH, increasing by as much as 26-fold between 60-percent and 80-percent RH. This\nobservation implies that proton transport occurs via the Grotthuss mechanism\nalong water associated with cable bacteria, forming proton wires. In order to\ndetermine {\\sigma}P and GP along cable bacteria, we implemented a protocol\nusing a modified transfer-printing technique to deposit either palladium\ninterdigitated protodes (IDP), palladium transfer length method (TLM) protodes,\nor gold interdigitated electrodes(IDE) on top of cable bacteria. Due to the\nrelatively mild nature of the transfer-printing technique, this method should\nbe applicable to a broad array of biological samples and curved materials. The\nobservation of protonic conductivity in cable bacteria presents possibilities\nfor investigating the importance of long-distance proton transport in microbial\necosystems and to potentially build biotic or biomimetic scaffolds to interface\nwith materials via proton-mediated gateways or channels.",
        "Given a rigidly-compactly generated tensor-triangulated category whose Balmer\nspectrum is finite dimensional and Noetherian, we construct a torsion model for\nit, which is equivalent to the original tensor-triangulated category. The\ntorsion model is determined in an adelic fashion by objects with singleton\nsupports. This categorifies the Cousin complex from algebra, and the process of\nreconstructing a spectrum from its monochromatic layers in chromatic stable\nhomotopy theory. This model is inspired by work of the second author in\nrational equivariant stable homotopy theory, and extends previous work of the\nauthors from the one-dimensional setting.",
        "The advance towards 6G networks comes with the promise of unprecedented\nperformance in sensing and communication capabilities. The feat of achieving\nthose, while satisfying the ever-growing demands placed on wireless networks,\npromises revolutionary advancements in sensing and communication technologies.\nAs 6G aims to cater to the growing demands of wireless network users, the\nimplementation of intelligent and efficient solutions becomes essential. In\nparticular, reconfigurable intelligent surfaces (RISs), also known as Smart\nSurfaces, are envisioned as a transformative technology for future 6G networks.\nThe performance of RISs when used to augment existing devices is nevertheless\nlargely affected by their precise location. Suboptimal deployments are also\ncostly to correct, negating their low-cost benefits. This paper investigates\nthe topic of optimal RISs diffusion, taking into account the improvement they\nprovide both for the sensing and communication capabilities of the\ninfrastructure while working with other antennas and sensors. We develop a\ncombined metric that takes into account the properties and location of the\nindividual devices to compute the performance of the entire infrastructure. We\nthen use it as a foundation to build a reinforcement learning architecture that\nsolves the RIS deployment problem. Since our metric measures the surface where\ngiven localization thresholds are achieved and the communication coverage of\nthe area of interest, the novel framework we provide is able to seamlessly\nbalance sensing and communication, showing its performance gain against\nreference solutions, where it achieves simultaneously almost the reference\nperformance for communication and the reference performance for localization.",
        "Using the Nehari manifold method, we establish sufficient conditions such\nthat a smooth functional attains a ground state within an annular domain of a\nclosed cone. The localization we obtain immediately allows for multiplicity\nwhen applied to disjoint conical sets. To illustrate our results, we consider a\ntwo-point boundary value problem and obtain a solution within a shell of a\nclosed cone, defined in terms of a Harnack inequality with respect to the\nenergy norm. The conditions imposed on the nonlinear term naturally extend\nthose from classical examples in the literature which were derived using the\nmethod of Nehari manifold on the entire domain.",
        "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps:\/\/ruili33.github.io\/tpo_website.",
        "Advanced Persistent Threats (APTs) are among the most sophisticated threats\nfacing critical organizations worldwide. APTs employ specific tactics,\ntechniques, and procedures (TTPs) which make them difficult to detect in\ncomparison to frequent and aggressive attacks. In fact, current network\nintrusion detection systems struggle to detect APTs communications, allowing\nsuch threats to persist unnoticed on victims' machines for months or even\nyears. In this paper, we present EarlyCrow, an approach to detect APT malware\ncommand and control over HTTP(S) using contextual summaries.\n  The design of EarlyCrow is informed by a novel threat model focused on TTPs\npresent in traffic generated by tools recently used as part of APT campaigns.\nThe threat model highlights the importance of the context around the malicious\nconnections, and suggests traffic attributes which help APT detection.\nEarlyCrow defines a novel multipurpose network flow format called PairFlow,\nwhich is leveraged to build the contextual summary of a PCAP capture,\nrepresenting key behavioral, statistical and protocol information relevant to\nAPT TTPs. We evaluate the effectiveness of EarlyCrow on unseen APTs obtaining a\nheadline macro average F1-score of 93.02% with FPR of $0.74%.",
        "The Platform for Content-Structure Inference (PCSI, pronounced \"pixie\")\nfacilitates the sharing of information about the process of converting Web\nresources into structured content objects that conform to a predefined format.\nPCSI records encode methods for deriving structured content from classes of\nURLs, and report the results of applying particular methods to particular URLs.\nThe methods are scripts written in Hex, a variant of Awk with facilities for\ntraversing the HTML DOM.",
        "Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.",
        "Photonic integrated circuits provide a compact platform for ultrafast and\nenergy-efficient matrix-vector multiplications (MVMs) in the optical domain.\nRecently, schemes based on time-division multiplexing (TDM) have been proposed\nas scalable approaches for realizing large-scale photonic MVM processors.\nHowever, existing demonstrations rely on coherent detection or multiple\nwavelengths, both of which complicate their operations. In this work, we\ndemonstrate a scalable TDM-based photonic MVM processor that uses only\nsingle-wavelength intensity-modulated optical signals, thereby avoiding\ncoherent detection and enabling simplified operations. A 32-channel processor\nis fabricated on a Si-on-insulator (SOI) platform and used to experimentally\nperform convolution operations in a convolutional neural network (CNN) for\nhandwritten digit recognition, achieving a classification accuracy of 93.47%\nfor 1500 images.",
        "Software developers balance a variety of different tasks in a workweek, yet\nthe allocation of time often differs from what they consider ideal. Identifying\nand addressing these deviations is crucial for organizations aiming to enhance\nthe productivity and well-being of the developers. In this paper, we present\nthe findings from a survey of 484 software developers at Microsoft, which aims\nto identify the key differences between how developers would like to allocate\ntheir time during an ideal workweek versus their actual workweek. Our analysis\nreveals significant deviations between a developer's ideal workweek and their\nactual workweek, with a clear correlation: as the gap between these two\nworkweeks widens, we observe a decline in both productivity and satisfaction.\nBy examining these deviations in specific activities, we assess their direct\nimpact on the developers' satisfaction and productivity. Additionally, given\nthe growing adoption of AI tools in software engineering, both in the industry\nand academia, we identify specific tasks and areas that could be strong\ncandidates for automation. In this paper, we make three key contributions: 1)\nWe quantify the impact of workweek deviations on developer productivity and\nsatisfaction 2) We identify individual tasks that disproportionately affect\nsatisfaction and productivity 3) We provide actual data-driven insights to\nguide future AI automation efforts in software engineering, aligning them with\nthe developers' requirements and ideal workflows for maximizing their\nproductivity and satisfaction.",
        "A connected graph has a $(k,\\ell)$-cover if each of its edges is contained in\nat least $\\ell$ cliques of order $k$. Motivated by recent advances in extremal\ncombinatorics and the literature on edge modification problems, we study the\nalgorithmic version of the $(k,\\ell)$-cover problem. Given a connected graph\n$G$, the $(k, \\ell)$-cover problem is to identify the smallest subset of\nnon-edges of $G$ such that their addition to $G$ results in a graph with a $(k,\n\\ell)$-cover. For every constant $k\\geq3$, we show that the $(k,1)$-cover\nproblem is $\\mathbb{NP}$-complete for general graphs. Moreover, we show that\nfor every constant $k\\geq 3$, the $(k,1)$-cover problem admits no\npolynomial-time constant-factor approximation algorithm unless\n$\\mathbb{P}=\\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can\nbe solved in polynomial time when the input graph is chordal. For the class of\ntrees and general values of $k$, we show that the $(k,1)$-cover problem is\n$\\mathbb{NP}$-hard even for spiders. However, we show that for every $k\\geq4$,\nthe $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor\napproximable when the input graph is a tree.",
        "The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.",
        "Generative models that can produce realistic images have improved\nsignificantly in recent years. The quality of the generated content has\nincreased drastically, so sometimes it is very difficult to distinguish between\nthe real images and the generated ones. Such an improvement comes at a price of\nethical concerns about the usage of the generative models: the users of\ngenerative models can improperly claim ownership of the generated content\nprotected by a license. In this paper, we propose an approach to embed\nwatermarks into the generated content to allow future detection of the\ngenerated content and identification of the user who generated it. The\nwatermark is embedded during the inference of the model, so the proposed\napproach does not require the retraining of the latter. We prove that\nwatermarks embedded are guaranteed to be robust against additive perturbations\nof a bounded magnitude. We apply our method to watermark diffusion models and\nshow that it matches state-of-the-art watermarking schemes in terms of\nrobustness to different types of synthetic watermark removal attacks.",
        "An underlying assumption of many existing approaches to human-robot task\ncommunication is that the robot possesses a sufficient amount of environmental\ndomain knowledge, including the locations of task-critical objects. This\nassumption is unrealistic if the locations of known objects change or have not\nyet been discovered by the robot. In this work, our key insight is that in many\nscenarios, robot end users possess more scene insight than the robot and need\nways to express it. Presently, there is a lack of research on how solutions for\ncollecting end-user scene insight should be designed. We thereby created an\nUncertainty Expression System (UES) to investigate how best to elicit end-user\nscene insight. The UES allows end users to convey their knowledge of object\nuncertainty using either: (1) a precision interface that allows meticulous\nexpression of scene insight; (2) a painting interface by which users create a\nheat map of possible object locations; and (3) a ranking interface by which end\nusers express object locations via an ordered list. We then conducted a user\nstudy to compare the effectiveness of these approaches based on the accuracy of\nscene insight conveyed to the robot, the efficiency at which end users are able\nto express this scene insight, and both usability and task load. Results\nindicate that the rank interface is more user friendly and efficient than the\nprecision interface, and that the paint interface is the least accurate.",
        "Deep learning has demonstrated great promise in cancer classification from\nwhole-slide images (WSIs) but remains constrained by the need for extensive\nannotations. Annotation-free methods, such as multiple instance learning (MIL)\nand self-supervised learning (SSL), have emerged to address this challenge;\nhowever, current SSL techniques often depend on synthetic augmentations or\ntemporal context, which may not adequately capture the intricate spatial\nrelationships inherent to histopathology. In this work, we introduce a novel\nspatial context-driven positive pair sampling strategy for SSL that leverages\nthe natural coherence of adjacent patches in WSIs. By constructing biologically\nrelevant positive pairs from spatially proximate patches, our approach\nharnesses inherent spatial coherence to enhance patch-level representations,\nultimately boosting slide-level classification performance. Experiments on\nmultiple datasets reveal that our strategy improves classification accuracy by\n5\\% to 10\\% over the standard method, paving the way for more clinically\nrelevant AI models in cancer diagnosis. The code is available at\nhttps:\/\/anonymous.4open.science\/r\/contextual-pairs-E72F\/.",
        "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.",
        "This paper examines the evolving performance practices of Ludwig van\nBeethoven's cello sonatas, with a particular focus on tempo and portamento\nbetween 1930 and 2012. It integrates analyses of 22 historical recordings,\nadvancements in recording technology to shed light on changes in interpretative\napproaches. By comparing Beethoven's metronome markings, as understood through\ncontemporaries such as Czerny and Moscheles, with their application in modern\nperformances, my research highlights notable deviations. These differences\nprove the challenges performers face in reconciling historical tempos with the\ndemands of contemporary performance practice. My study pays special attention\nto the diminishing use of audible portamento in the latter half of the 20th\ncentury, contrasted with a gradual increase in tempo after 1970. This\ndevelopment is linked to broader cultural and pedagogical shifts, including the\nadoption of fingering techniques that reduce hand shifts, thereby facilitating\ngreater technical precision at faster tempos. Nonetheless, my study identifies\nthe persistence of 'silent portamento' as an expressive device, allowing\nperformers to retain stylistic expression without compromising rhythmic\nintegrity. My paper offers valuable insights for performers and scholars alike,\nadvocating a critical reassessment of Beethoven's tempo markings and the\nnuanced application of portamento in modern performance practice.",
        "We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D\nmapping and rendering of indoor environments. Traditional Structure-from-Motion\n(SfM) methods may not work well in large-scale indoor scenes due to the\nprevalence of textureless and repetitive regions. To overcome these challenges,\nour approach (IM360) leverages the wide field of view of omnidirectional images\nand integrates the spherical camera model into every core component of the SfM\npipeline. In order to develop a comprehensive 3D reconstruction solution, we\nintegrate a neural implicit surface reconstruction technique to generate\nhigh-quality surfaces from sparse input data. Additionally, we utilize a\nmesh-based neural rendering approach to refine texture maps and accurately\ncapture view-dependent properties by combining diffuse and specular components.\nWe evaluate our pipeline on large-scale indoor scenes from the Matterport3D and\nStanford2D3D datasets. In practice, IM360 demonstrate superior performance in\nterms of textured mesh reconstruction over SOTA. We observe accuracy\nimprovements in terms of camera localization and registration as well as\nrendering high frequency details.",
        "Social networks have a vast range of applications with graphs. The available\nbenchmark datasets are citation, co-occurrence, e-commerce networks, etc, with\nclasses ranging from 3 to 15. However, there is no benchmark classification\nsocial network dataset for graph machine learning. This paper fills the gap and\npresents the Binary Classification Social Network Dataset (\\textit{BiSND}),\ndesigned for graph machine learning applications to predict binary classes. We\npresent the BiSND in \\textit{tabular and graph} formats to verify its\nrobustness across classical and advanced machine learning. We employ a diverse\nset of classifiers, including four traditional machine learning algorithms\n(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural\nNetwork (multi-layer perceptrons), one Graph Neural Network (Graph\nConvolutional Network), and three state-of-the-art Graph Contrastive Learning\nmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for\nclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicating\npromising avenues for future enhancements.",
        "Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.",
        "We show that a gradient-descent with a simple, universal rule for step-size\nselection provably finds $k$-SVD, i.e., the $k\\geq 1$ largest singular values\nand corresponding vectors, of any matrix, despite nonconvexity. There has been\nsubstantial progress towards this in the past few years where existing results\nare able to establish such guarantees for the \\emph{exact-parameterized} and\n\\emph{over-parameterized} settings, with choice of oracle-provided step size.\nBut guarantees for generic setting with a step size selection that does not\nrequire oracle-provided information has remained a challenge. We overcome this\nchallenge and establish that gradient descent with an appealingly simple\nadaptive step size (akin to preconditioning) and random initialization enjoys\nglobal linear convergence for generic setting. Our convergence analysis reveals\nthat the gradient method has an attracting region, and within this attracting\nregion, the method behaves like Heron's method (a.k.a. the Babylonian method).\nEmpirically, we validate the theoretical results. The emergence of modern\ncompute infrastructure for iterative optimization coupled with this work is\nlikely to provide means to solve $k$-SVD for very large matrices.",
        "Link prediction is crucial for uncovering hidden connections within complex\nnetworks, enabling applications such as identifying potential customers and\nproducts. However, this research faces significant challenges, including\nconcerns about data privacy, as well as high computational and storage costs,\nespecially when dealing with large-scale networks. Condensed graphs, which are\nmuch smaller than the original graphs while retaining essential information,\nhas become an effective solution to both maintain data utility and preserve\nprivacy. Existing methods, however, initialize synthetic graphs through random\nnode selection without considering node connectivity, and are mainly designed\nfor node classification tasks. As a result, their potential for\nprivacy-preserving link prediction remains largely unexplored. We introduce\nHyDRO\\textsuperscript{+}, a graph condensation method guided by algebraic\nJaccard similarity, which leverages local connectivity information to optimize\ncondensed graph structures. Extensive experiments on four real-world networks\nshow that our method outperforms state-of-the-art methods and even the original\nnetworks in balancing link prediction accuracy and privacy preservation.\nMoreover, our method achieves nearly 20* faster training and reduces storage\nrequirements by 452*, as demonstrated on the Computers dataset, compared to\nlink prediction on the original networks. This work represents the first\nattempt to leverage condensed graphs for privacy-preserving link prediction\ninformation sharing in real-world complex networks. It offers a promising\npathway for preserving link prediction information while safeguarding privacy,\nadvancing the use of graph condensation in large-scale networks with privacy\nconcerns.",
        "Designing solutions for complex engineering challenges is crucial in human\nproduction activities. However, previous research in the retrieval-augmented\ngeneration (RAG) field has not sufficiently addressed tasks related to the\ndesign of complex engineering solutions. To fill this gap, we introduce a new\nbenchmark, SolutionBench, to evaluate a system's ability to generate complete\nand feasible solutions for engineering problems with multiple complex\nconstraints. To further advance the design of complex engineering solutions, we\npropose a novel system, SolutionRAG, that leverages the tree-based exploration\nand bi-point thinking mechanism to generate reliable solutions. Extensive\nexperimental results demonstrate that SolutionRAG achieves state-of-the-art\n(SOTA) performance on the SolutionBench, highlighting its potential to enhance\nthe automation and reliability of complex engineering solution design in\nreal-world applications.",
        "Multiple supervised learning scenarios are composed by a sequence of\nclassification tasks. For instance, multi-task learning and continual learning\naim to learn a sequence of tasks that is either fixed or grows over time.\nExisting techniques for learning tasks that are in a sequence are tailored to\nspecific scenarios, lacking adaptability to others. In addition, most of\nexisting techniques consider situations in which the order of the tasks in the\nsequence is not relevant. However, it is common that tasks in a sequence are\nevolving in the sense that consecutive tasks often have a higher similarity.\nThis paper presents a learning methodology that is applicable to multiple\nsupervised learning scenarios and adapts to evolving tasks. Differently from\nexisting techniques, we provide computable tight performance guarantees and\nanalytically characterize the increase in the effective sample size.\nExperiments on benchmark datasets show the performance improvement of the\nproposed methodology in multiple scenarios and the reliability of the presented\nperformance guarantees.",
        "Reinforced random walks (RRWs), including vertex-reinforced random walks\n(VRRWs) and edge-reinforced random walks (ERRWs), model random walks where the\ntransition probabilities evolve based on prior visitation history~\\cite{mgr,\nfmk, tarres, volkov}. These models have found applications in various areas,\nsuch as network representation learning~\\cite{xzzs}, reinforced\nPageRank~\\cite{gly}, and modeling animal behaviors~\\cite{smouse}, among others.\nHowever, statistical estimation of the parameters governing RRWs remains\nunderexplored. This work focuses on estimating the initial edge weights of\nERRWs using observed trajectory data. Leveraging the connections between an\nERRW and a random walk in a random environment (RWRE)~\\cite{mr, mr2}, as given\nby the so-called \"magic formula\", we propose an estimator based on the\ngeneralized method of moments. To analyze the sample complexity of our\nestimator, we exploit the hyperbolic Gaussian structure embedded in the random\nenvironment to bound the fluctuations of the underlying random edge\nconductances.",
        "Large language models (LLMs) often retain outdated or incorrect information\nfrom pre-training, which undermines their reliability. While model editing\nmethods have been developed to address such errors without full re-training,\nthey frequently suffer from knowledge conflicts, where outdated information\ninterferes with new knowledge. In this work, we propose Conflict-free Model\nEditing (CoME), a novel framework that enhances the accuracy of knowledge\nupdates in LLMs by selectively removing outdated knowledge. CoME leverages\nunlearning to mitigate knowledge interference, allowing new information to be\nintegrated without compromising relevant linguistic features. Through\nexperiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we\ndemonstrate that CoME improves both editing accuracy and model reliability when\napplied to existing editing methods. Our results highlight that the targeted\nremoval of outdated knowledge is crucial for enhancing model editing\neffectiveness and maintaining the model's generative performance.",
        "Indian languages are inflectional and agglutinative and typically follow\nclause-free word order. The structure of sentences across most major Indian\nlanguages are similar when their dependency parse trees are considered. While\nsome differences in the parsing structure occur due to peculiarities of a\nlanguage or its preferred natural way of conveying meaning, several apparent\ndifferences are simply due to the granularity of representation of the smallest\nsemantic unit of processing in a sentence. The semantic unit is typically a\nword, typographically separated by whitespaces. A single whitespace-separated\nword in one language may correspond to a group of words in another. Hence,\ngrouping of words based on semantics helps unify the parsing structure of\nparallel sentences across languages and, in the process, morphology. In this\nwork, we propose word grouping as a major preprocessing step for any\ncomputational or linguistic processing of sentences for Indian languages. Among\nIndian languages, since Hindi is one of the least agglutinative, we expect it\nto benefit the most from word-grouping. Hence, in this paper, we focus on Hindi\nto study the effects of grouping. We perform quantitative assessment of our\nproposal with an intrinsic method that perturbs sentences by shuffling words as\nwell as an extrinsic evaluation that verifies the importance of word grouping\nfor the task of Machine Translation (MT) using decomposed prompting. We also\nqualitatively analyze certain aspects of the syntactic structure of sentences.\nOur experiments and analyses show that the proposed grouping technique brings\nuniformity in the syntactic structures, as well as aids underlying NLP tasks.",
        "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification",
    "start_abstract":"Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "LoRA: Low-Rank Adaptation of Large Language Models"
      ],
      "abstract":[
        "An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Probing electric field tunable multiband superconductivity in\n  alternating twisted quadralayer graphene",
        "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist\n  Content",
        "Online Meta-learning for AutoML in Real-time (OnMAR)",
        "Correlated vibration-solvent and Duschinsky effects on optical\n  spectroscopy",
        "Rise of the Community Champions: From Reviewer Crunch to Community Power",
        "On the diagonals of rational functions: the minimal number of variables\n  (unabridged version)",
        "Magnetic moments in the Poynting theorem, Maxwell equations, Dirac\n  equation, and QED",
        "Analysis and Extension of Noisy-target Training for Unsupervised Target\n  Signal Enhancement",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "AI-Driven Solutions for Falcon Disease Classification: Concatenated\n  ConvNeXt cum EfficientNet AI Model Approach",
        "Memory-dependent abstractions of stochastic systems through the lens of\n  transfer operators",
        "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face\n  Obfuscation Methods",
        "Invariance properties of the solution operator for measure-valued\n  semilinear transport equations",
        "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
        "Extraction of HI gas with bulk motions in the disk of galaxies",
        "Improved $L^p$ bounds for the strong spherical maximal operator",
        "The Gas-to-Dust Ratio Investigation in the Massive Star-Forming region\n  M17",
        "Open-Source Factor Graph Optimization Package for GNSS: Examples and\n  Applications",
        "Identifying Likely-Reputable Blockchain Projects on Ethereum",
        "A Distributional Perspective on Word Learning in Neural Language Models",
        "Experience-replay Innovative Dynamics",
        "Decoding-based Regression",
        "Weakly-Constrained 4D Var for Downscaling with Uncertainty using\n  Data-Driven Surrogate Models",
        "Coherence of a hole spin flopping-mode qubit in a circuit quantum\n  electrodynamics environment",
        "What Influences the Field Goal Attempts of Professional Players?\n  Analysis of Basketball Shot Charts via Log Gaussian Cox Processes with\n  Spatially Varying Coefficients",
        "Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver\n  Assistance Systems",
        "Development and validation of a high-fidelity full-spectrum Monte Carlo\n  model for the Swiss airborne gamma-ray spectrometry system",
        "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical\n  Study",
        "Development of an uncertainty-aware equation of state for gold"
      ],
      "abstract":[
        "Alternating twisted multilayer graphene presents a compelling multiband\nsystem for exploring superconductivity. Here we investigate robust\nsuperconductivity in alternating twisted quadralayer graphene, elucidating\ncarrier contributions from both flat and dispersive bands. The\nsuperconductivity is robust, with a strong electrical field tunability, a\nmaximum BKT transition temperature of 1.6 K, and high critical magnetic fields\nbeyond the Pauli limit. We disentangle the carrier density of Dirac bands and\nflat bands from the Landau fan diagram. Moreover, we could estimate the\nflatband Fermi velocity from the obtained high critical current near half\nfilling when superconductivity is killed at finite magnetic fields, and further\nquantify the superfluid stiffness from the low critical current in the\nsuperconducting regime. Our results exhibit the electric field tunable coupling\nstrength within the superconducting phase, revealing unconventional properties\nwith vanishing Fermi velocity and large superfluid stiffness. These phenomena,\nattributed to substantial quantum metric contributions, offer new insights into\nthe mechanisms underlying unconventional superconductivity in moire systems.",
        "Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated\nextremist content, including photorealistic images and text, which can be used\nto bypass safety mechanisms and generate harmful outputs. However, existing\ndatasets for evaluating LMM robustness offer limited exploration of extremist\ncontent, often lacking AI-generated images, diverse image generation models,\nand comprehensive coverage of historical events, which hinders a complete\nassessment of model vulnerabilities. To fill this gap, we introduce\nExtremeAIGC, a benchmark dataset and evaluation framework designed to assess\nLMM vulnerabilities against such content. ExtremeAIGC simulates real-world\nevents and malicious use cases by curating diverse text- and image-based\nexamples crafted using state-of-the-art image generation techniques. Our study\nreveals alarming weaknesses in LMMs, demonstrating that even cutting-edge\nsafety measures fail to prevent the generation of extremist material. We\nsystematically quantify the success rates of various attack strategies,\nexposing critical gaps in current defenses and emphasizing the need for more\nrobust mitigation strategies.",
        "Automated machine learning (AutoML) is a research area focusing on using\noptimisation techniques to design machine learning (ML) algorithms, alleviating\nthe need for a human to perform manual algorithm design. Real-time AutoML\nenables the design process to happen while the ML algorithm is being applied to\na task. Real-time AutoML is an emerging research area, as such existing\nreal-time AutoML techniques need improvement with respect to the quality of\ndesigns and time taken to create designs. To address these issues, this study\nproposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach.\nMeta-learning gathers information about the optimisation process undertaken by\nthe ML algorithm in the form of meta-features. Meta-features are used in\nconjunction with a meta-learner to optimise the optimisation process. The OnMAR\napproach uses a meta-learner to predict the accuracy of an ML design. If the\naccuracy predicted by the meta-learner is sufficient, the design is used, and\nif the predicted accuracy is low, an optimisation technique creates a new\ndesign. A genetic algorithm (GA) is the optimisation technique used as part of\nthe OnMAR approach. Different meta-learners (k-nearest neighbours, random\nforest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. not\nspecific to a single real-time AutoML application) and therefore evaluated on\nthree different real-time AutoML applications, namely: composing an image\nclustering algorithm, configuring the hyper-parameters of a convolutional\nneural network, and configuring a video classification pipeline. The OnMAR\napproach is effective, matching or outperforming existing real-time AutoML\napproaches, with the added benefit of a faster runtime.",
        "Understanding the role of vibrations in optical spectroscopies is essential\nfor the precise interpretation of spectroscopic behavior, especially in systems\nwith complex solvation effects. This workstudies the correlated Duschinsky and\nsolvent effects on the optical spectra using the extended\ndissipaton-equation-of-motion (ext-DEOM) approach, which is an exact and\nnon-Markovian, nonperturbative approach for nonlinear environmental couplings.\nIn the paper, the environment (bath) is composed of the solvent and\nintramolecular vibrational modes whose Duschinsky rotations constitute the\nquardratic couplings to the electronic states. To apply the ext-DEOM, one key\nstep is to obtain the bath coupling descriptors, which is elaborated. As an\naccurate description of solvated molecular systems, the simulating results\ndemonstrate how the above factors affect the position and shape of spectral\nbands.",
        "Academic publishing is facing a crisis driven by exponential growth in\nsubmissions and an overwhelmed peer review system, leading to inconsistent\ndecisions and a severe reviewer shortage. This paper introduces Panvas, a\nplatform that reimagines academic publishing as a continuous, community-driven\nprocess. Panvas addresses these systemic failures with a novel combination of\neconomic incentives (paid reviews) and rich interaction mechanisms\n(multi-dimensional ratings, threaded discussions, and expert-led reviews). By\nmoving beyond the traditional accept\/reject paradigm and integrating paper\nhosting with code\/data repositories and social networking, Panvas fosters a\nmeritocratic environment for scholarly communication and presents a radical\nrethinking of how we evaluate and disseminate scientific knowledge. We present\nthe system design, development roadmap, and a user study plan to evaluate its\neffectiveness.",
        "From some observations on the linear differential operators occurring in the\nLattice Green function of the d-dimensional face centred and simple cubic\nlattices, and on the linear differential operators occurring in the n-particle\ncontributions\n  to the magnetic susceptibility of the square Ising model, we forward some\nconjectures on the diagonals of rational functions. These conjectures are also\nin agreement with exact results we obtain for many Calabi-Yau operators, and\nmany other examples related, or not related to physics.\n  Consider a globally bounded power series which is the diagonal of rational\nfunctions of a certain number of variables, annihilated by an irreducible\nminimal order linear differential operator homomorphic to its adjoint. Among\nthe logarithmic formal series solutions, at the origin, of this operator, call\nn the highest power of the logarithm. We conjecture that this diagonal series\ncan be represented as a diagonal of a rational function with a minimal number\nof variables N_v related to this highest power n by the relation N_v = n +2.\n  Since the operator is homomorphic to its adjoint, its differential Galois\ngroup is symplectic or orthogonal. We also conjecture that the symplectic or\northogonal character of the differential Galois group is related to the parity\nof the highest power n, namely symplectic for n odd and orthogonal for n even.\n  We also sketch the case where the denominator of the rational function is not\nirreducible and is the product of, for instance, two polynomials. The analysis\nof the linear differential operators annihilating the diagonal of rational\nfunction where the denominator is the product of two polynomials, sheds some\nlight on the emergence of such mixture of direct sums and products of factors.\n  The conjecture N_v = n +2 still holds for such reducible linear differential\noperators.",
        "The role of magnetic moments in electrodynamics is examined in this work. The\neffects are described in the context of conventional quantum electrodynamics\nexpressed in terms of the electromagnetic fields or in the context of an\nextended Poynting theorem and extended Maxwell equations. These extensions take\ninto account the energetics of interaction of magnetic moments with\ninhomogeneous magnetic fields. We show how magnetic moment effects are included\nin either version of electrodynamics and that these apparently different\nformulations can give consistent results. In either case, we express the\ninteractions in terms of electromagnetic fields only, avoiding use of a vector\npotential.",
        "Deep neural network-based target signal enhancement (TSE) is usually trained\nin a supervised manner using clean target signals. However, collecting clean\ntarget signals is costly and such signals are not always available. Thus, it is\ndesirable to develop an unsupervised method that does not rely on clean target\nsignals. Among various studies on unsupervised TSE methods, Noisy-target\nTraining (NyTT) has been established as a fundamental method. NyTT simply\nreplaces clean target signals with noisy ones in the typical supervised\ntraining, and it has been experimentally shown to achieve TSE. Despite its\neffectiveness and simplicity, its mechanism and detailed behavior are still\nunclear. In this paper, to advance NyTT and, thus, unsupervised methods as a\nwhole, we analyze NyTT from various perspectives. We experimentally demonstrate\nthe mechanism of NyTT, the desirable conditions, and the effectiveness of\nutilizing noisy signals in situations where a small number of clean target\nsignals are available. Furthermore, we propose an improved version of NyTT\nbased on its properties and explore its capabilities in the dereverberation and\ndeclipping tasks, beyond the denoising task.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Falconry, an ancient practice of training and hunting with falcons,\nemphasizes the need for vigilant health monitoring to ensure the well-being of\nthese highly valued birds, especially during hunting activities. This research\npaper introduces a cutting-edge approach, which leverages the power of\nConcatenated ConvNeXt and EfficientNet AI models for falcon disease\nclassification. Focused on distinguishing 'Normal,' 'Liver,' and\n'Aspergillosis' cases, the study employs a comprehensive dataset for model\ntraining and evaluation, utilizing metrics such as accuracy, precision, recall,\nand f1-score. Through rigorous experimentation and evaluation, we demonstrate\nthe superior performance of the concatenated AI model compared to traditional\nmethods and standalone architectures. This novel approach contributes to\naccurate falcon disease classification, laying the groundwork for further\nadvancements in avian veterinary AI applications.",
        "With the increasing ubiquity of safety-critical autonomous systems operating\nin uncertain environments, there is a need for mathematical methods for formal\nverification of stochastic models. Towards formally verifying properties of\nstochastic systems, methods based on discrete, finite Markov approximations --\nabstractions -- thereof have surged in recent years. These are found in\ncontexts where: either a) one only has partial, discrete observations of the\nunderlying continuous stochastic process, or b) the original system is too\ncomplex to analyze, so one partitions the continuous state-space of the\noriginal system to construct a handleable, finite-state model thereof. In both\ncases, the abstraction is an approximation of the discrete stochastic process\nthat arises precisely from the discretization of the underlying continuous\nprocess. The fact that the abstraction is Markov and the discrete process is\nnot (even though the original one is) leads to approximation errors. Towards\naccounting for non-Markovianity, we introduce memory-dependent abstractions for\nstochastic systems, capturing dynamics with memory effects. Our contribution is\ntwofold. First, we provide a formalism for memory-dependent abstractions based\non transfer operators. Second, we quantify the approximation error by upper\nbounding the total variation distance between the true continuous state\ndistribution and its discrete approximation.",
        "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
        "We provide conditions under which we prove for measure-valued transport\nequations with non-linear reaction term in the space of finite signed Radon\nmeasures, that positivity is preserved, as well as absolute continuity with\nrespect to Lebesgue measure, if the initial condition has that property.\nMoreover, if the initial condition has $L^p$ regular density, then the solution\nhas the same property.",
        "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
        "We propose a new method for extracting bulk motion gases in the disk of a\ngalaxy from HI data cubes, offering improvements over classical techniques like\nmoment analysis and line profile fitting. Our approach decomposes the\nline-of-sight velocity profiles into multiple Gaussian components, which are\nthen classified into (underlying and dominant) bulk and non-bulk motion gases\nbased on criteria such as HI surface density, velocity dispersion, kinetic\nenergy, and rotation velocity. A 2D tilted-ring analysis is employed to refine\nthe kinematical parametres of the galaxy disk, ensuring robust extraction of\nthe bulk motion gases. We demonstrate the effectiveness of this method using\nthe HI data cubes of NGC 4559 from the WSRT-HALOGAS survey, distinguishing\nbetween bulk and non-bulk gas components. From this, we find that approximately\n50% of the HI gas in NGC 4559 is classified as non-bulk, possibly linked to\nprocesses such as stellar feedback. This work provides a robust framework for\nanalysing HI kinematics of galaxies from high sensitivity HI observations of\ngalaxies like MeerKAT-MHONGOOSE and FAST-FEASTS and allows us to best exploit\nthe kinematic information of the complex gas dynamics within galaxy disks.",
        "We study the $L^p$ mapping properties of the strong spherical maximal\nfunction, which is a multiparameter generalisation of Stein's spherical maximal\nfunction. We show that this operator is bounded on $L^p$ for $p > 2$ in all\ndimensions $n \\geq 3$. This matches the conjectured sharp range $p>(n+1)\/(n-1)$\nwhen $n=3$. For $n=2$ the analogous estimate was recently proved by Chen, Guo\nand Yang.\n  Our result builds upon and improves an earlier bound of Lee, Lee and Oh. The\nmain novelty is an estimate in discretised incidence geometry that bounds the\nvolume of the intersection of thin neighbourhoods of axis-parallel ellipsoids.\nThis estimate is then interpolated with the Fourier analytic $L^p$-Sobolev\nestimates of Lee, Lee and Oh.",
        "M17 is a well-known massive star-forming region, and its Gas-to-Dust Ratio\n(GDR) may vary significantly compared to the other areas. The mass of gas can\nbe traced by the ${\\rm CO}$ emission observed in the \\emph{Milky Way Imaging\nScroll Painting (MWISP) project}. The dust mass can be traced by analyzing the\ninterstellar extinction magnitude obtained from the \\emph{United Kingdom\nInfrared Telescope (UKIRT)}. We computed the ratio ${W({\\rm CO})\/A_V}$: for\n${A_V \\le }$ 10 mag, ${{ W(^{12}{\\rm CO})\/ A_V}= (6.27 \\pm 0.19)}$ ${\\mathrm{{K\n\\cdot km\/s} \\cdot mag^{-1}}}$ and ${{ W(^{13}{\\rm CO})\/ A_V} = (0.75 \\pm\n0.72)}$ ${ \\mathrm{{K \\cdot km\/s} \\cdot mag^{-1}}}$; whereas for ${{A_V} \\ge\n10}$ mag, ${{ W(^{12}{\\rm CO})\/ A_V} = (15.8 \\pm 0.06) }$ ${\\mathrm{{K \\cdot\nkm\/s} \\cdot mag^{-1}}}$ and ${{ W(^{13}{\\rm CO})\/ A_V} = (3.11 \\pm 0.25)}$ ${\n\\mathrm{{K \\cdot km\/s} \\cdot mag^{-1}}}$. Then, we converted the ${W({\\rm\nCO})\/A_V}$ into ${N(\\rm H)\/A_V}$. Using the WD01 model, we derived the GDR: for\n${A_V \\le }$ 10 mag, the GDRs were ${118 \\pm 9}$ for ${^{12}{\\rm CO}}$ and ${83\n\\pm 62}$ for ${^{13}{\\rm CO}}$, comparable to those of the Milky Way; however,\nfor ${A_V \\ge }$ 10 mag, the GDRs increased significantly to ${296 \\pm 3}$ for\n${^{12}{\\rm CO}}$ and ${387 \\pm 40}$ for ${^{13}{\\rm CO}}$, approximately three\ntimes higher than those of the Milky Way. In the discussion, we compared the\nresults of this work with previous studies and provided a detailed discussion\nof the influence of massive stars and other factors on GDR.",
        "State estimation methods using factor graph optimization (FGO) have garnered\nsignificant attention in global navigation satellite system (GNSS) research.\nFGO exhibits superior estimation accuracy compared with traditional state\nestimation methods that rely on least-squares or Kalman filters. However, only\na few FGO libraries are specialized for GNSS observations. This paper\nintroduces an open-source GNSS FGO package named gtsam\\_gnss, which has a\nsimple structure and can be easily applied to GNSS research and development.\nThis package separates the preprocessing of GNSS observations from factor\noptimization. Moreover, it describes the error function of the GNSS factor in a\nstraightforward manner, allowing for general-purpose inputs. This design\nfacilitates the transition from ordinary least-squares-based positioning to FGO\nand supports user-specific GNSS research. In addition, gtsam\\_gnss includes\nanalytical examples involving various factors using GNSS data in real urban\nenvironments. This paper presents three application examples: the use of a\nrobust error model, estimation of integer ambiguity in the carrier phase, and\ncombination of GNSS and inertial measurements from smartphones. The proposed\nframework demonstrates excellent state estimation performance across all use\ncases.",
        "Identifying reputable Ethereum projects remains a critical challenge within\nthe expanding blockchain ecosystem. The ability to distinguish between\nlegitimate initiatives and potentially fraudulent schemes is non-trivial. This\nwork presents a systematic approach that integrates multiple data sources with\nadvanced analytics to evaluate credibility, transparency, and overall\ntrustworthiness. The methodology applies machine learning techniques to analyse\ntransaction histories on the Ethereum blockchain.\n  The study classifies accounts based on a dataset comprising 2,179 entities\nlinked to illicit activities and 3,977 associated with reputable projects.\nUsing the LightGBM algorithm, the approach achieves an average accuracy of\n0.984 and an average AUC of 0.999, validated through 10-fold cross-validation.\nKey influential factors include time differences between transactions and\nreceived_tnx.\n  The proposed methodology provides a robust mechanism for identifying\nreputable Ethereum projects, fostering a more secure and transparent investment\nenvironment. By equipping stakeholders with data-driven insights, this research\nenables more informed decision-making, risk mitigation, and the promotion of\nlegitimate blockchain initiatives. Furthermore, it lays the foundation for\nfuture advancements in trust assessment methodologies, contributing to the\ncontinued development and maturity of the Ethereum ecosystem.",
        "Language models (LMs) are increasingly being studied as models of human\nlanguage learners. Due to the nascency of the field, it is not well-established\nwhether LMs exhibit similar learning dynamics to humans, and there are few\ndirect comparisons between learning trajectories in humans and models. Word\nlearning trajectories for children are relatively well-documented, and recent\nwork has tried to extend these investigations to language models. However,\nthere are no widely agreed-upon metrics for word learning in language models.\nWe take a distributional approach to this problem, defining lexical knowledge\nin terms of properties of the learned distribution for a target word. We argue\nthat distributional signatures studied in prior work fail to capture key\ndistributional information. Thus, we propose an array of signatures that\nimprove on earlier approaches by capturing knowledge of both where the target\nword can and cannot occur as well as gradient preferences about the word's\nappropriateness. We obtain learning trajectories for a selection of small\nlanguage models we train from scratch, study the relationship between different\ndistributional signatures, compare how well they align with human word learning\ntrajectories and interpretable lexical features, and address basic\nmethodological questions about estimating these distributional signatures. Our\nmetrics largely capture complementary information, suggesting that it is\nimportant not to rely on a single metric. However, across all metrics, language\nmodels' learning trajectories fail to correlate with those of children.",
        "Despite its groundbreaking success, multi-agent reinforcement learning (MARL)\nstill suffers from instability and nonstationarity. Replicator dynamics, the\nmost well-known model from evolutionary game theory (EGT), provide a\ntheoretical framework for the convergence of the trajectories to Nash\nequilibria and, as a result, have been used to ensure formal guarantees for\nMARL algorithms in stable game settings. However, they exhibit the opposite\nbehavior in other settings, which poses the problem of finding alternatives to\nensure convergence. In contrast, innovative dynamics, such as the Brown-von\nNeumann-Nash (BNN) or Smith, result in periodic trajectories with the potential\nto approximate Nash equilibria. Yet, no MARL algorithms based on these dynamics\nhave been proposed. In response to this challenge, we develop a novel\nexperience replay-based MARL algorithm that incorporates revision protocols as\ntunable hyperparameters. We demonstrate, by appropriately adjusting the\nrevision protocols, that the behavior of our algorithm mirrors the trajectories\nresulting from these dynamics. Importantly, our contribution provides a\nframework capable of extending the theoretical guarantees of MARL algorithms\nbeyond replicator dynamics. Finally, we corroborate our theoretical findings\nwith empirical results.",
        "Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.",
        "Dynamic downscaling typically involves using numerical weather prediction\n(NWP) solvers to refine coarse data to higher spatial resolutions. Data-driven\nmodels such as FourCastNet have emerged as a promising alternative to the\ntraditional NWP models for forecasting. Once these models are trained, they are\ncapable of delivering forecasts in a few seconds, thousands of times faster\ncompared to classical NWP models. However, as the lead times, and, therefore,\ntheir forecast window, increase, these models show instability in that they\ntend to diverge from reality. In this paper, we propose to use data\nassimilation approaches to stabilize them when used for downscaling tasks. Data\nassimilation uses information from three different sources, namely an imperfect\ncomputational model based on partial differential equations (PDE), from noisy\nobservations, and from an uncertainty-reflecting prior. In this work, when\ncarrying out dynamic downscaling, we replace the computationally expensive\nPDE-based NWP models with FourCastNet in a ``weak-constrained 4DVar framework\"\nthat accounts for the implied model errors. We demonstrate the efficacy of this\napproach for a hurricane-tracking problem; moreover, the 4DVar framework\nnaturally allows the expression and quantification of uncertainty. We\ndemonstrate, using ERA5 data, that our approach performs better than the\nensemble Kalman filter (EnKF) and the unstabilized FourCastNet model, both in\nterms of forecast accuracy and forecast uncertainty.",
        "The entanglement of microwave photons and spin qubits in silicon represents a\npivotal step forward for quantum information processing utilizing semiconductor\nquantum dots. Such hybrid spin circuit quantum electrodynamics (cQED) has been\nachieved by granting a substantial electric dipole moment to a spin by\nde-localizing it in a double quantum dot under spin-orbit interaction, thereby\nforming a flopping-mode (FM) spin qubit. Despite its promise, the coherence\nproperties demonstrated to date remain insufficient to envision FM spin qubits\nas practical single qubits. Here, we present a FM hole spin qubit in a silicon\nnanowire coupled to a high-impedance niobium nitride microwave resonator for\nreadout. We report Rabi frequencies exceeding 100 MHz and coherence times in\nthe microsecond range, resulting in a high single gate quality factor of 380.\nThis establishes FM spin qubits as fast and reliable qubits. Moreover, using\nthe large frequency tunability of the FM qubit, we reveal for the first time\nthat photonic effects predominantly limit coherence, with radiative decay being\nthe main relaxation channel and photon shot-noise inducing dephasing. These\nresults highlight that optimized microwave engineering can unlock the potential\nof FM spin qubits in hybrid cQED architectures, offering a scalable and robust\nplatform for fast and coherent spin qubits with strong coupling to microwave\nphotons.",
        "Basketball shot charts provide valuable information regarding local patterns\nof in-game performance to coaches, players, sports analysts, and statisticians.\nThe spatial patterns of where shots were attempted and whether the shots were\nsuccessful suggest options for offensive and defensive strategies as well as\nhistorical summaries of performance against particular teams and players. The\ndata represent a marked spatio-temporal point process with locations\nrepresenting locations of attempted shots and an associated mark representing\nthe shot's outcome (made\/missed). Here, we develop a Bayesian log Gaussian Cox\nprocess model allowing joint analysis of the spatial pattern of locations and\noutcomes of shots across multiple games. We build a hierarchical model for the\nlog intensity function using Gaussian processes, and allow spatially varying\neffects for various game-specific covariates. We aim to model the spatial\nrelative risk under different covariate values. For inference via posterior\nsimulation, we design a Markov chain Monte Carlo (MCMC) algorithm based on a\nkernel convolution approach. We illustrate the proposed method using extensive\nsimulation studies. A case study analyzing the shot data of NBA legends Stephen\nCurry, LeBron James, and Michael Jordan highlights the effectiveness of our\napproach in real-world scenarios and provides practical insights into\noptimizing shooting strategies by examining how different playing conditions,\ngame locations, and opposing team strengths impact shooting efficiency.",
        "Accurate environmental perception is critical for advanced driver assistance\nsystems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role\nin ADAS; they can reliably detect obstacles and help ensure traffic safety.\nExisting research on LiDAR sensing has demonstrated that adapting the LiDAR's\nresolution and range based on environmental characteristics can improve machine\nperception. However, current adaptive LiDAR approaches for ADAS have not\nexplored the possibility of combining the perception abilities of the vehicle\nand the human driver, which can potentially further enhance the detection\nperformance. In this paper, we propose a novel system that adapts LiDAR\ncharacteristics to human driver's visual perception to enhance LiDAR sensing\noutside human's field of view. We develop a proof-of-concept prototype of the\nsystem in the virtual environment CARLA. Our system integrates real-time data\non the driver's gaze to identify regions in the environment that the driver is\nmonitoring. This allows the system to optimize LiDAR resources by dynamically\nincreasing the LiDAR's range and resolution in peripheral areas that the driver\nmay not be attending to. Our simulations show that this gaze-aware LiDAR\nenhances detection performance compared to a baseline standalone LiDAR,\nparticularly in challenging environmental conditions like fog. Our hybrid\nhuman-machine sensing approach potentially offers improved safety and\nsituational awareness in real-time driving scenarios for ADAS applications.",
        "Airborne Gamma-Ray Spectrometry (AGRS) is a critical tool for radiological\nemergency response, enabling the rapid identification and quantification of\nhazardous terrestrial radionuclides over large areas. However, existing\ncalibration methods are limited to a few gamma-ray sources, excluding most\nradionuclides released in severe nuclear accidents and nuclear weapon\ndetonations, compromising effective response and risk assessment. Here, we\npresent a high-fidelity Monte Carlo model that overcomes these limitations,\noffering full-spectrum calibration for any gamma-ray source. Unlike previous\napproaches, our model integrates a detailed mass model of the aircraft and a\ncalibrated non-proportional scintillation model, enabling accurate\nevent-by-event predictions of the spectrometer's response to arbitrarily\ncomplex gamma-ray fields. Validation in near-, mid-, and far-field scenarios\ndemonstrates that the model not only addresses major deficiencies of previous\napproaches but also achieves the accuracy required to supersede empirical\ncalibration methods. This advancement enables high-fidelity spectral signature\ngeneration for any gamma-ray source, reduces calibration time and costs,\nminimizes reliance on high-intensity sources, and eliminates related\nradioactive waste. The approach presented here is a critical step toward\nintegrating advanced full-spectrum data reduction methods for AGRS, unlocking\nnew capabilities beyond emergency response, such as atmospheric cosmic-ray flux\nquantification for geophysics and trace-level airborne radionuclide\nidentification for nuclear security.",
        "Reasoning capabilities have significantly improved the performance of\nvision-language models (VLMs) in domains such as mathematical problem-solving,\ncoding, and visual question-answering. However, their impact on real-world\napplications remains unclear. This paper presents the first empirical study on\nthe effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that\nrequires interpreting complex screen layouts, understanding user instructions,\nand executing multi-turn interactions. We evaluate two pairs of commercial\nmodels--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and\nreasoning-enhanced versions across two static benchmarks (ScreenSpot and\nAndroidControl) and one interactive environment (AndroidWorld). We surprisingly\nfind the Claude 3.7 Sonnet reasoning model achieves state-of-the-art\nperformance on AndroidWorld. However, reasoning VLMs generally offer marginal\nimprovements over non-reasoning models on static benchmarks and even degrade\nperformance in some agent setups. Notably, reasoning and non-reasoning VLMs\nfail on different sets of tasks, suggesting that reasoning does have an impact,\nbut its benefits and drawbacks counterbalance each other. We attribute these\ninconsistencies to the limitations of benchmarks and VLMs. Based on the\nfindings, we provide insights for further enhancing mobile GUI agents in terms\nof benchmarks, VLMs, and their adaptability in dynamically invoking reasoning\nVLMs. The experimental data are publicly available at\nhttps:\/\/github.com\/LlamaTouch\/VLM-Reasoning-Traces.",
        "This study introduces a framework that employs Gaussian Processes (GPs) to\ndevelop high-fidelity equation of state (EOS) tables, essential for modeling\nmaterial properties across varying temperatures and pressures. GPs offer a\nrobust predictive modeling approach and are especially adept at handling\nuncertainties systematically. By integrating Error-in-Variables (EIV) into the\nGP model, we adeptly navigate uncertainties in both input parameters (like\ntemperature and density) and output variables (including pressure and other\nthermodynamic properties). Our methodology is demonstrated using\nfirst-principles density functional theory (DFT) data for gold, observing its\nproperties over maximum density compression (up to 100 g\/cc) and extreme\ntemperatures within the warm dense matter region (reaching 300 eV).\nFurthermore, we assess the resilience of our uncertainty propagation within the\nresultant EOS tables under various conditions, including data scarcity and the\nintrinsic noise of experiments and simulations."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"LoRA: Low-Rank Adaptation of Large Language Models",
    "start_abstract":"An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification"
      ],
      "abstract":[
        "Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Nanostructured thin films of indium oxide nanocrystals confined in\n  alumina matrixes",
        "Steady-state coherence in multipartite quantum systems: its connection\n  with thermodynamic quantities and impact on quantum thermal machines",
        "Solving Superconformal Ward Identities in Mellin Space",
        "Footprint in fitting $B\\to D$ vector form factor and determination for\n  $D$-meson leading-twist LCDA",
        "SU(4) gate design via unitary process tomography: its application to\n  cross-resonance based superconducting quantum devices",
        "Constrained mean-field control with singular control: Existence,\n  stochastic maximum principle and constrained FBSDE",
        "Jet rates in Higgs boson decay at third order in QCD",
        "On (in)consistency of M-estimators under contamination",
        "Score Matching Riemannian Diffusion Means",
        "Lower bound on the radii of circular orbits in the extremal Kerr\n  black-hole spacetime",
        "Cartan Quantum Metrology",
        "GPU Accelerated Image Quality Assessment-Based Software for Transient\n  Detection",
        "Multiaccuracy and Multicalibration via Proxy Groups",
        "Federated Variational Inference for Bayesian Mixture Models",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$",
        "COSMOS-Web: The emergence of the Hubble Sequence",
        "Metastability and Ostwald Step Rule in the Crystallisation of Diamond\n  and Graphite from Molten Carbon",
        "Radii of light nuclei from the Jacobi No-Core Shell Model",
        "It's Not All Black and White: Degree of Truthfulness for Risk-Avoiding\n  Agents",
        "Beyond Fishing: The Value of Maritime Cultural Heritage in Germany",
        "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for\n  Credit Bond Recommendation",
        "Electromagnetic System Conceptual Design for a Negative Triangularity\n  Tokamak",
        "Long-distance genuine multipartite Entanglement between Magnetic Defects\n  in Spin Chains",
        "Radiation sputtering of hydrocarbon ices at Europa-relevant temperatures",
        "Central Velocity Dispersion being the Primary Driver of Abundance\n  Patterns in Quenched Galaxies",
        "Adaptive Observation Cost Control for Variational Quantum Eigensolvers",
        "Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud\n  Offloading",
        "Study of $1^{--}$ P wave charmoniumlike and bottomoniumlike tetraquark\n  spectroscopy",
        "Holographic QCD phase diagram for a rotating plasma in the Hawking-Page\n  approach"
      ],
      "abstract":[
        "Nanocrystals of indium oxide (In$_2$O$_3$) with sizes below 10 nm were\nprepared in alumina matrixes by using a co-pulverization method. The used\nsubstrates such as borosilicate glasses or (100) silicon as well as the\nsubstrate temperatures during the deposition process were modified and their\neffects characterized on the structural and physical properties of\nalumina-In$_2$O$_3$ films. Complementary investigation methods including X-ray\ndiffraction, optical transmittance in the range 250-1100 nm and transmission\nelectron microscopy were used to analyze the nanostructured films. The\ncrystalline order, morphology and optical responses were monitored as function\nof the deposition parameters and the post-synthesis annealing. The optimal\nconditions were found and allow realizing suitable nanostructured films with a\nmajor crystalline order of cubic phase for the In$_2$O$_3$ nanocrystals. The\noptical properties of the films were analyzed and the key parameters such as\ndirect and indirect band gaps were evaluated as function of the synthesis\nconditions and the crystalline quality of the films.",
        "Understanding how coherence of quantum systems affects thermodynamic\nquantities, such as work and heat, is essential for harnessing quantumness\neffectively in thermal quantum technologies. Here, we study the unique\ncontributions of quantum coherence among different subsystems of a multipartite\nsystem, specifically in non-equilibrium steady states, to work and heat\ncurrents. Our system comprises two coupled ensembles, each consisting of $N$\nparticles, interacting with two baths of different temperatures, respectively.\nThe particles in an ensemble interact with their bath either simultaneously or\nsequentially, leading to non-local dissipation and enabling the decomposition\nof work and heat currents into local and non-local components.We find that the\nnon-local heat current, as well as both the local and non-local work\ncurrents,are linked to the system quantum coherence. We provide explicit\nexpressions of coherence-related quantities that determine the work currents\nunder various intrasystem interactions.Our scheme is versatile, capable of\nfunctioning as a refrigerator, an engine, and an accelerator, with its\nperformance being highly sensitive to the configuration settings. These\nfindings establish a connection between thermodynamic quantities and quantum\ncoherence, supplying valuable insights for the design of quantum thermal\nmachines.",
        "We study four-point correlators in superconformal theories in various\ndimensions. We develop an efficient method to solve the superconformal Ward\nidentities in Mellin space. For 4d $\\mathcal{N}=4$ SYM and the 6d\n$\\mathcal{N}=(2,0)$ theory, our method reproduces the known solutions. As novel\napplications of this method, we also derive solutions in 3d $\\mathcal{N} = 8$\nABJM, and in 4d $\\mathcal{N} = 4$ SYM with line defects.",
        "In this paper, we fit the $B\\to D$ vector transition form factor (TFF) by\nusing the data measured by BABAR and Belle Collaborations within Monte Carlo\n(MC) method. Meanwhile, the $B\\to D$ TFF is also calculated by using the QCD\nlight-cone sum rules approach (LCSRs) within right-handed chiral current\ncorrelation function. In which, the $D$-meson leading-twist light-cone\ndistribution amplitude (LCDA) serves as crucial input parameter is\nreconstructed with light-cone harmonic oscillator model where its longitudinal\nbehavior primarily determined by the model-free parameter $B_{2;D}$. After\nmatching the TFF with two scenarios from MC and LCSRs, we have $B_{2;D}=0.17$.\nThen, we present the curve of $D$-meson leading-twist LCDA in comparison with\nother theoretical approaches. Subsequently, the $B\\to D$ TFF $f_{+}^{BD}(q^2)$\nat the large recoil region is $f_{+}^{BD}(0)=0.625^{+0.087}_{-0.113}$, which is\ncompared in detail with theoretical estimates and experimental measurements.\nFurthermore, we calculate the decay width and branching ratio of the\nCabibbo-favored semileptonic decays $B\\to D\\ell \\bar{\\nu}_{\\ell}$, which lead\nto the results $\\mathcal{B}(B^0\\to D^-\\ell ^+\\nu _{\\ell})\n=(1.96_{-0.55}^{+0.51})\\times 10^{-2}$ and $\\mathcal{B}(B^+\\to \\bar{D}^0\\ell\n^+\\nu _{\\ell}) =(2.12_{-0.59}^{+0.55})\\times 10^{-2}$. Finally, we predict the\nCKM matrix element with two scenarios $|V_{cb}|_{\\rm\nSR}=42.97_{-2.57}^{+2.42}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=42.82_{-1.29}^{+1.07}\\times 10^{-3}$ from $B^0\\to D^-\\ell^+\\nu_{\\ell}$,\n$|V_{cb}|_{\\rm SR}=41.93_{-1.05}^{+1.03}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=41.82_{-0.25}^{+0.23}\\times 10^{-3}$ from $B^+\\to\n\\bar{D}^0\\ell^+\\nu_{\\ell}$ which are in good agreement with theoretical and\nexperimental predictions.",
        "We present a novel approach for implementing pulse-efficient SU(4) gates on\ncross resonance (CR)-based superconducting quantum devices. Our method\nintroduces a parameterized unitary derived from the CR-Hamiltonian propagator,\nwhich accounts for static-$ZZ$ interactions. Leveraging the Weyl chamber's\ngeometric structure, we successfully realize a continuous 2-qubit basis gate,\n$R_{ZZ}(\\theta)$, as an echo-free pulse schedule on the IBM Quantum device\nibm_kawasaki. We evaluate the average fidelity and gate time of various SU(4)\ngates generated using the $R_{ZZ}(\\theta)$ to confirm the advantages of our\nimplementation.",
        "This paper studies some mean-field control (MFC) problems with singular\ncontrol under general dynamic state-control-law constraints. We first propose a\ncustomized relaxed control formulation to cope with the dynamic mixed\nconstraints and establish the existence of an optimal control using some\ncompactification arguments in the proper canonical spaces to accommodate the\nsingular control. To characterize the optimal pair of regular and singular\ncontrols, we treat the controlled McKean-Vlasov process as an\ninfinite-dimensional equality constraint and recast the MFC problem as an\noptimization problem on canonical spaces with constraints on Banach space,\nallowing us to derive the stochastic maximum principle (SMP) and a constrained\nBSDE using a novel Lagrange multipliers method. In addition, we further\ninvestigate the uniqueness and the stability result of the solution to the\nconstrained FBSDE associated to the constrained MFC problem with singular\ncontrol.",
        "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables.",
        "We consider robust location-scale estimators under contamination. We show\nthat commonly used robust estimators such as the median and the Huber estimator\nare inconsistent under asymmetric contamination, while the Tukey estimator is\nconsistent. In order to make nuisance parameter free inference based on the\nTukey estimator a consistent scale estimator is required. However, standard\nrobust scale estimators such as the interquartile range and the median absolute\ndeviation are inconsistent under contamination.",
        "Estimating means on Riemannian manifolds is generally computationally\nexpensive because the Riemannian distance function is not known in closed-form\nfor most manifolds. To overcome this, we show that Riemannian diffusion means\ncan be efficiently estimated using score matching with the gradient of Brownian\nmotion transition densities using the same principle as in Riemannian diffusion\nmodels. Empirically, we show that this is more efficient than Monte Carlo\nsimulation while retaining accuracy and is also applicable to learned\nmanifolds. Our method, furthermore, extends to computing the Fr\\'echet mean and\nthe logarithmic map for general Riemannian manifolds. We illustrate the\napplicability of the estimation of diffusion mean by efficiently extending\nEuclidean algorithms to general Riemannian manifolds with a Riemannian\n$k$-means algorithm and maximum likelihood Riemannian regression.",
        "It is often stated in the physics literature that maximally-spinning Kerr\nblack-hole spacetimes are characterized by near-horizon co-rotating circular\ngeodesics of radius $r_{\\text{circular}}$ with the property\n$r_{\\text{circular}}\\to r^+_{\\text{H}}$, where $r_{\\text{H}}$ is the horizon\nradius of the extremal black hole. Based on the famous Thorne hoop conjecture,\nin the present compact paper we provide evidence for the existence of a\nnon-trivial lower bound\n${{r_{\\text{circular}}-r_{\\text{H}}}\\over{r_{\\text{H}}}}\\gtrsim (\\mu\/M)^{1\/2}$\non the radii of circular orbits in the extremal Kerr black-hole spacetime,\nwhere $\\mu\/M$ is the dimensionless mass ratio which characterizes the composed\nblack-hole-orbiting-particle system.",
        "We address the characterization of two-qubit gates, focusing on bounds to\nprecision in the joint estimation of the three parameters that define their\nCartan decomposition. We derive the optimal probe states that jointly maximize\nprecision, minimize sloppiness, and eliminate quantum incompatibility.\nAdditionally, we analyze the properties of the set of optimal probes and\nevaluate their robustness against noise.",
        "Fast imaging localises celestial transients using source finders in the image\ndomain. The need for high computational throughput in this process is driven by\nnext-generation telescopes such as Square Kilometre Array (SKA), which, upon\ncompletion, will be the world's largest aperture synthesis radio telescope. It\nwill collect data at unprecedented velocity and volume. Due to the vast amounts\nof data the SKA will produce, current source finders based on source extraction\nmay be inefficient in a wide-field search. In this paper, we focus on the\nsoftware development of GPU-accelerated transient finders based on Image\nQuality Assessment (IQA) methods -- Low-Information Similarity Index (LISI) and\naugmented LISI (augLISI). We accelerate the algorithms using GPUs, achieving\nkernel time of approximately 0.1 milliseconds for transient finding in\n2048X2048 images.",
        "As the use of predictive machine learning algorithms increases in high-stakes\ndecision-making, it is imperative that these algorithms are fair across\nsensitive groups. Unfortunately, measuring and enforcing fairness in real-world\napplications can be challenging due to missing or incomplete sensitive group\ndata. Proxy-sensitive attributes have been proposed as a practical and\neffective solution in these settings, but only for parity-based fairness\nnotions. Knowing how to evaluate and control for fairness with missing\nsensitive group data for newer and more flexible frameworks, such as\nmultiaccuracy and multicalibration, remains unexplored. In this work, we\naddress this gap by demonstrating that in the absence of sensitive group data,\nproxy-sensitive attributes can provably be used to derive actionable upper\nbounds on the true multiaccuracy and multicalibration, providing insights into\na model's potential worst-case fairness violations. Additionally, we show that\nadjusting models to satisfy multiaccuracy and multicalibration across\nproxy-sensitive attributes can significantly mitigate these violations for the\ntrue, but unknown, sensitive groups. Through several experiments on real-world\ndatasets, we illustrate that approximate multiaccuracy and multicalibration can\nbe achieved even when sensitive group information is incomplete or unavailable.",
        "We present a federated learning approach for Bayesian model-based clustering\nof large-scale binary and categorical datasets. We introduce a principled\n'divide and conquer' inference procedure using variational inference with local\nmerge and delete moves within batches of the data in parallel, followed by\n'global' merge moves across batches to find global clustering structures. We\nshow that these merge moves require only summaries of the data in each batch,\nenabling federated learning across local nodes without requiring the full\ndataset to be shared. Empirical results on simulated and benchmark datasets\ndemonstrate that our method performs well in comparison to existing clustering\nalgorithms. We validate the practical utility of the method by applying it to\nlarge scale electronic health record (EHR) data.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$.",
        "Leveraging the wide area coverage of the COSMOS-Web survey, we quantify the\nabundance of different morphological types from $z\\sim 7$ with unprecedented\nstatistics and establish robust constraints on the epoch of emergence of the\nHubble sequence. We measure the global (spheroids, disk-dominated,\nbulge-dominated, peculiar) and resolved (stellar bars) morphologies for about\n400,000 galaxies down to F150W=27 using deep learning, representing a\ntwo-orders-of-magnitude increase over previous studies. We then provide\nreference Stellar Mass Functions (SMFs) of different morphologies between\n$z\\sim 0.2$ and $z\\sim 7$ and best-fit parameters to inform models of galaxy\nformation. All catalogs and data are made publicly available. (a)At redshift z\n> 4.5, the massive galaxy population ($\\log M_*\/M_\\odot>10$) is dominated by\ndisturbed morphologies (~70%) -- even in the optical rest frame -- and very\ncompact objects (~30%) with effective radii smaller than ~500pc. This confirms\nthat a significant fraction of the star formation at cosmic dawn occurs in very\ndense regions, although the stellar mass for these systems could be\noverestimated.(b)Galaxies with Hubble-type morphologies -- including bulge and\ndisk-dominated galaxies -- arose rapidly around $z\\sim 4$ and dominate the\nmorphological diversity of massive galaxies as early as $z\\sim 3$. (c)Using\nstellar bars as a proxy, we speculate that stellar disks in massive galaxies\nmight have been common (>50%) among the star-forming population since cosmic\nnoon ($z\\sim2$-2.5) and formed as early as $z\\sim 7$ (d)Massive quenched\ngalaxies are predominantly bulge-dominated from z~4 onward, suggesting that\nmorphological transformations briefly precede or are simultaneous to quenching\nmechanisms at the high-mass end. (e) Low-mass ($\\log M_*\/M_\\odot<10$) quenched\ngalaxies are typically disk-dominated, pointing to different quenching routes\nin the two ends of the stellar mass spectrum from cosmic dawn.",
        "The crystallisation of carbon from the melt under extreme conditions is\nhighly relevant to earth and planetary science, materials manufacturing, and\nnuclear fusion research. The thermodynamic conditions near the\ngraphite-diamond-liquid (GDL) triple point are especially of interest for\ngeological and technological applications, but high-pressure flash heating\nexperiments aiming to resolve this region of the phase diagram of carbon\nexhibit large discrepancies. Experimental challenges are often related to the\npersistence of metastable crystalline or glassy phases, superheated crystals,\nor supercooled liquids. A deeper understanding of the crystallisation kinetics\nof diamond and graphite is crucial for effectively interpreting the outcomes of\nthese experiments. Here, we reveal the microscopic mechanisms of diamond and\ngraphite nucleation from liquid carbon through molecular simulations with\nfirst-principles machine learning potentials. Our simulations accurately\nreproduce the experimental phase diagram of carbon in the region around the GDL\ntriple point and show that liquid carbon crystallises spontaneously upon\ncooling at constant pressure. Surprisingly, metastable graphite crystallises in\nthe domain of diamond thermodynamic stability at pressures above the triple\npoint. Furthermore, whereas diamond crystallises through a classical nucleation\npathway, graphite follows a two-step process in which low-density fluctuations\nforego ordering. Calculations of the nucleation rates of the two competing\nphases confirm this result and reveal a manifestation of Ostwald's step rule\nwhere the strong metastability of graphite hinders the transformation to the\nstable diamond phase. Our results provide a new key to interpreting melting and\nrecrystallisation experiments and shed light on nucleation kinetics in\npolymorphic materials with deep metastable states.",
        "Accurately determining the size of the atomic nucleus with realistic nuclear\nforces is a long outstanding issue of nuclear physics. The no-core shell model\n(NCSM), one of the powerful ab initio methods for nuclear structure, can\nachieve accurate energies of light nuclei. The extraction of converged radii is\nmore difficult. In this work, we present a novel method to effectively extract\nthe radius of light nuclei by restoring the long-range behavior of densities\nfrom NCSM calculations. The correct large distance asymptotic of two-body\nrelative densities are deduced based on the NCSM densities in limited basis\nsize. The resulting radii using the corrected densities show a nice\nconvergence. The root-mean-square matter and charge radii of $^{4,6,8}$He and\n$^{6,7,8}$Li can be accurately obtained based on Jacobi-NCSM calculations with\nthe high-precision chiral two-nucleon and three-nucleon forces combined with\nthis new method. Our method can be straightforwardly extended to other ab\ninitio calculations, potentially providing a better description of nuclear\nsizes with realistic nuclear forces.",
        "The classic notion of truthfulness requires that no agent has a profitable\nmanipulation -- an untruthful report that, for some combination of reports of\nthe other agents, increases her utility. This strong notion implicitly assumes\nthat the manipulating agent either knows what all other agents are going to\nreport, or is willing to take the risk and act as-if she knows their reports.\n  Without knowledge of the others' reports, most manipulations are risky --\nthey might decrease the manipulator's utility for some other combinations of\nreports by the other agents. Accordingly, a recent paper (Bu, Song and Tao,\n``On the existence of truthful fair cake cutting mechanisms'', Artificial\nIntelligence 319 (2023), 103904) suggests a relaxed notion, which we refer to\nas risk-avoiding truthfulness (RAT), which requires only that no agent can gain\nfrom a safe manipulation -- one that is sometimes beneficial and never harmful.\n  Truthfulness and RAT are two extremes: the former considers manipulators with\ncomplete knowledge of others, whereas the latter considers manipulators with no\nknowledge at all. In reality, agents often know about some -- but not all -- of\nthe other agents. This paper introduces the RAT-degree of a mechanism, defined\nas the smallest number of agents whose reports, if known, may allow another\nagent to safely manipulate, or $n$ if there is no such number. This notion\ninterpolates between classic truthfulness (degree $n$) and RAT (degree at least\n$1$): a mechanism with a higher RAT-degree is harder to manipulate safely.\n  To illustrate the generality and applicability of this concept, we analyze\nthe RAT-degree of prominent mechanisms across various social choice settings,\nincluding auctions, indivisible goods allocations, cake-cutting, voting, and\nstable matchings.",
        "The importance of maritime heritage in providing benefits such as a sense of\nplace and identity has been widely discussed. However, there remains a lack of\ncomprehensive quantitative analysis, particularly regarding monetary valuation\nand its impact on people's preferences. In this study, I present the results of\na choice experiment that assesses the value of the maritime cultural heritage\nassociated with shrimp fishing through seafood consumption preferences in\nGermany. Additionally, I investigate people's attitudes toward cultural\nheritage and examine how these attitudes affect their stated preferences. I\nfind that these attitudes are significantly stronger in towns where local\nfishermen led a prominent awareness campaign on fishing culture during the\nstudy period. Moreover, I observe a positive willingness to pay for a cultural\nheritage attribute in shrimp dishes, which varies depending on individuals'\nattitudes toward cultural heritage.",
        "Graph Neural Networks have significantly advanced research in recommender\nsystems over the past few years. These methods typically capture global\ninterests using aggregated past interactions and rely on static embeddings of\nusers and items over extended periods of time. While effective in some domains,\nthese methods fall short in many real-world scenarios, especially in finance,\nwhere user interests and item popularity evolve rapidly over time. To address\nthese challenges, we introduce a novel extension to Light Graph Convolutional\nNetwork (LightGCN) designed to learn temporal node embeddings that capture\ndynamic interests. Our approach employs causal convolution to maintain a\nforward-looking model architecture. By preserving the chronological order of\nuser-item interactions and introducing a dynamic update mechanism for\nembeddings through a sliding window, the proposed model generates well-timed\nand contextually relevant recommendations. Extensive experiments on a\nreal-world dataset from BNP Paribas demonstrate that our approach significantly\nenhances the performance of LightGCN while maintaining the simplicity and\nefficiency of its architecture. Our findings provide new insights into\ndesigning graph-based recommender systems in time-sensitive applications,\nparticularly for financial product recommendations.",
        "Negative triangularity (NT) tokamak configurations have several key benefits\nincluding sufficient core confinement, improved power handling, and reduced\nedge pressure gradients that allow for edge-localized mode (ELM) free\noperation. We present the design of a compact NT device for testing\nsophisticated simulation and control software, with the aim of demonstrating NT\ncontrollability and informing power plant operation. The TokaMaker code is used\nto develop the basic electromagnetic system of the $R_0$ = 1 m, $a$ = 0.27 m,\n$B_t$ = 3 T, $I_p$ = 0.75 MA tokamak. The proposed design utilizes eight\npoloidal field coils with maximum currents of 1 MA to achieve a wide range of\nplasma geometries with $-0.7 < \\delta < -0.3$ and $1.5 < \\kappa < 1.9$.\nScenarios with strong negative triangularity and high elongation are\nparticularly susceptible to vertical instability, necessitating the inclusion\nof high-field side and\/or low-field side passive stabilizing plates which\ntogether reduce vertical instability growth rates by $\\approx$75%. Upper limits\nfor the forces on poloidal and toroidal field coils are predicted and\nmechanical loads on passive structures during current quench events are\nassessed. The 3 T on-axis toroidal field is achieved with 16 demountable copper\ntoroidal field coils, allowing for easy maintenance of the vacuum vessel and\npoloidal field coils. This pre-conceptual design study demonstrates that the\nkey capabilities required of a dedicated NT tokamak experiment can be realized\nwith existing copper magnet technologies.",
        "We investigate the emergence and properties of long-distance genuine\nmultipartite entanglement, induced via three localized magnetic defects, in a\none-dimensional transverse-field XX spin-$1\/2$ chain. Using both analytical and\nnumerical techniques, we determine the conditions for the existence of bound\nstates localized at the defects. We find that the reduced density matrix (RDM)\nof the defects exhibits long-distance genuine multipartite entanglement (GME)\nacross the whole range of the Hamiltonian parameter space, including regions\nwhere the two-qubit concurrence is zero. We quantify the entanglement by using\nnumerical lower bounds for the GME concurrence, as well as by analytically\nderiving the GME concurrence in regions where the RDM is of rank two. Our work\nprovides insights into generating multipartite entanglement in many-body\nquantum systems via local control techniques.",
        "The surfaces of some icy moons, such as Jupiter's moon Europa, are heavily\nbombarded by energetic particles that can alter the surface materials and\naffect the composition of its exosphere. Detection of CO2 on Europa's surface\nindicate that Europa's interior may be transporting freshly exposed\ncarbon-containing material to the surface. It is unknown whether this CO2 is a\nproduct of radiation of carbon-containing precursors or whether it is present\nin the initial deposits. Regardless, further radiolysis by high-energy\nelectrons or ions can sputter CO2 (and organic fragments if present) into\nEuropa's exosphere. In this study, we investigate the radiation sputtering of\nCO2 and organic fragments from hydrocarbon water ice mixtures at different\nEuropa-relevant surface temperatures to identify how its sputtering products\nevolve over time. This study shows that the sputtering of hydrocarbon water ice\nleads to the production of mostly CO2, CO, and fragmented hydrocarbons. The\nonset of sputtered hydrocarbons is immediate, and quickly reaches a steady\nstate, whereas CO2 and CO are formed more gradually. It is found that higher\ntemperatures cause more sputtering, and that there are some notable differences\nin the distribution of species that are sputtered at different temperatures,\nindicating local heterogeneity of sputtering yields depending on the surface\ntemperature.",
        "The element abundances of galaxies provide crucial insights into their\nformation and evolution. Using high-resolution IFU data from the MaNGA survey,\nwe analyze the central spectra (0-0.5 $R_{\\rm e}$) of 1,185 quenched galaxies\n($z = 0.012-0.15$) to study their element abundances and stellar populations.\nWe employ the full-spectrum fitting code {\\tt alf} to derive stellar ages and\nelement abundances from synthetic spectra and empirical libraries. Our key\nfindings are: (1) Central velocity dispersion ($\\sigma_*$) is the most\neffective parameter correlating with (relative) element abundances, especially\n[Na\/Fe], [Mg\/Fe], [C\/Fe], and [N\/Fe], outperforming $M_\\ast$ and $M_\\ast\/R_{\\rm\ne}$. (2) When binned by $\\sigma_*$, the relative abundances of Na, Mg, C, and N\nremain stable across different formation times ($T_{\\rm form}$), suggesting\nthese elements are primarily influenced by the burstiness of star formation\n(traced by $\\sigma_*$) rather than prolonged evolutionary processes. (3) Fe and\nCa show little variation with $\\sigma_*$, indicating weaker sensitivity to\n$\\sigma_*$-driven processes. However, $T_{\\rm form}$ has a global influence on\nall elements, contributing to their overall chemical evolution, albeit\nsecondary to $\\sigma_*$ for most elements. These results support the primary\nrole of $\\sigma_*$ in shaping the abundance patterns, likely stemming from the\nconnection between central massive black holes and possibly dark matter halos,\nwhich influences the burstiness of star formation histories.",
        "The objective to be minimized in the variational quantum eigensolver (VQE)\nhas a restricted form, which allows a specialized sequential minimal\noptimization (SMO) that requires only a few observations in each iteration.\nHowever, the SMO iteration is still costly due to the observation noise -- one\nobservation at a point typically requires averaging over hundreds to thousands\nof repeated quantum measurement shots for achieving a reasonable noise level.\nIn this paper, we propose an adaptive cost control method, named subspace in\nconfident region (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP)\nsurrogate, and requires it to have low uncertainty over the subspace being\nupdated, so that optimization in each iteration is performed with guaranteed\naccuracy. The adaptive cost control is performed by first setting the required\naccuracy according to the progress of the optimization, and then choosing the\nminimum number of measurement shots and their distribution such that the\nrequired accuracy is satisfied. We demonstrate that SubsCoRe significantly\nimproves the efficiency of SMO, and outperforms the state-of-the-art methods.",
        "Consider an edge computing setting in which a user submits queries for the\nsolution of a linear system to an edge processor, which is subject to\ntime-varying computing availability. The edge processor applies a probabilistic\nlinear solver (PLS) so as to be able to respond to the user's query within the\nallotted time and computing budget. Feedback to the user is in the form of an\nuncertainty set. Due to model misspecification, the uncertainty set obtained\nvia a direct application of PLS does not come with coverage guarantees with\nrespect to the true solution of the linear system. This work introduces a new\nmethod to calibrate the uncertainty sets produced by PLS with the aim of\nguaranteeing long-term coverage requirements. The proposed method, referred to\nas online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from\ncloud to edge. This enables the online calibration of uncertainty thresholds\nvia online conformal prediction (OCP), an online optimization method previously\nstudied in the context of prediction models. The validity of OCP-PLS is\nverified via experiments that bring insights into trade-offs between coverage,\nprediction set size, and cloud usage.",
        "The masses of $1^{--}$ P-wave charmonium-like and bottomonium-like tetraquark\nstates are calculated in a constituent quark model (CQM) where the Cornell-like\npotential and Breit-Fermi interaction are employed. All model parameters were\nimported from previous work, and predetermined by studying the low-lying\nconventional S- and P-wave light, charmed, bottom, charmonium, and bottomonium\nmeson mass spectra. The decay widths of $1^{--}$ P-wave tetraquark states are\ncalculated for possible two-body strong decay channels within the rearrangement\nmechanism, including $\\omega \\chi_{cJ}$, $\\eta J\/\\psi$, and $\\rho J\/\\psi$ for\ncharmonium-like tetraquarks, and $\\omega \\chi_{bJ}$ for bottomonium-like\ntetraquarks. The tetraquark theoretical results are compared with the selected\nexotic states, also known as Y states, and tentative assignments are suggested.\nThis study suggests that $\\psi(4230)$, $\\psi(4360)$, $\\psi(4660)$, and\n$\\Upsilon$(10753) may be P-wave tetraquark states and that multiple states\nmight exist around 4.23 GeV and 4.36 GeV.",
        "We investigate the combined effect of rotation and finite chemical potential\nin the confinement\/deconfinement transition of strongly interacting matter. The\nholographic description consists of a five-dimensional geometry that contains a\nblack hole (BH) in the deconfined (plasma) phase. The geometry is equipped with\nsome cut-off that introduces an infrared energy scale. We consider two\npossibilities: the so-called hard wall and soft wall AdS\/QCD models. The\ntransition between the plasma and hadronic phases is represented\nholographically as a Hawking-Page transition between geometries with and\nwithout a black hole. The gravitational dual of the rotating plasma at finite\ndensity is given by a Reissner-Nordstr\\\"om (RN) charged anti-de Sitter (AdS) BH\nwith non-zero angular momentum. This analysis provides the critical temperature\nof deconfinement as a function of the quark chemical potential and the plasma\nrotational velocity. For the particular case of very low temperatures and high\ndensities, it is found that the critical value of the chemical potential for\nthe transition to occur at $ T \\to 0 $ does not depend on the plasma vorticity,\nsince the effects of rotation and quark density on the critical temperature are\nshown to be independent."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project",
    "start_abstract":"The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints"
      ],
      "abstract":[
        "An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks",
        "Optimization Landscapes Learned: Proxy Networks Boost Convergence in\n  Physics-based Inverse Problems",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "How do recollimation-induced instabilities shape the propagation of\n  hydrodynamic relativistic jets?",
        "Structural Perturbation in Large Language Model Representations through\n  Recursive Symbolic Regeneration",
        "Understanding and Evaluating Hallucinations in 3D Visual Language Models",
        "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach",
        "Comparative Analysis of Control Strategies for Position Regulation in DC\n  Servo Motors",
        "ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting",
        "Two-stage Deep Denoising with Self-guided Noise Attention for Multimodal\n  Medical Images",
        "On Choquard-Kirchhoff Type Critical Multiphase Problem",
        "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
        "Primordial Origin of Methane on Eris and Makemake Supported by D\/H\n  Ratios",
        "Quarkonia and Deconfined Quark-Gluon Matter in Heavy-Ion Collisions",
        "Quantum-Inspired Fidelity-based Divergence",
        "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical\n  Flow Estimation",
        "Towards Efficient Parametric State Estimation in Circulating Fuel\n  Reactors with Shallow Recurrent Decoder Networks",
        "Exploring and Evaluating Multimodal Knowledge Reasoning Consistency of\n  Multimodal Large Language Models",
        "exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem",
        "Rotational effects on the small-scale dynamo",
        "Curvature perturbations from vacuum transition during inflation",
        "Hierarchical RL-MPC for Demand Response Scheduling",
        "An exponential integrator multicontinuum homogenization method for\n  fractional diffusion problem with multiscale coefficients",
        "Foundation Models for Spatio-Temporal Data Science: A Tutorial and\n  Survey",
        "Matrix weighted inequalities for fractional type integrals associated to\n  operators with new classes of weights",
        "Vexed by VEX tools: Consistency evaluation of container vulnerability\n  scanners",
        "Drone Detection and Tracking with YOLO and a Rule-based Method",
        "Asymptotic approximations for convection onset with Ekman pumping at low\n  wavenumbers",
        "Optimizing 2D+1 Packing in Constrained Environments Using Deep\n  Reinforcement Learning"
      ],
      "abstract":[
        "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation",
        "Solving inverse problems in physics is central to understanding complex\nsystems and advancing technologies in various fields. Iterative optimization\nalgorithms, commonly used to solve these problems, often encounter local\nminima, chaos, or regions with zero gradients. This is due to their\noverreliance on local information and highly chaotic inverse loss landscapes\ngoverned by underlying partial differential equations (PDEs). In this work, we\nshow that deep neural networks successfully replicate such complex loss\nlandscapes through spatio-temporal trajectory inputs. They also offer the\npotential to control the underlying complexity of these chaotic loss landscapes\nduring training through various regularization methods. We show that optimizing\non network-smoothened loss landscapes leads to improved convergence in\npredicting optimum inverse parameters over conventional momentum-based\noptimizers such as BFGS on multiple challenging problems.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "Recollimation is a phenomenon of particular importance in the dynamic\nevolution of jets and in the emission of high-energy radiation. Additionally,\nthe full comprehension of this phenomenon provides insights into fundamental\nproperties of jets in the vicinity of the Active Galactic Nucleus (AGN).\nThree-dimensional (magneto-)hydrodynamic simulations revealed that the jet\nconditions at recollimation favor the growth of strong instabilities,\nchallenging the traditional view-supported from two-dimensional simulations-of\nconfined jets undergoing a series of recollimation and reflection shocks. To\ninvestigate the stability of relativistic jets in AGNs at recollimation sites,\nwe perform a set of long duration three-dimensional relativistic hydrodynamic\nsimulations with the state-of-the-art PLUTO code, to focus on the development\nof hydrodynamical instabilities. We explore the non-linear growth of the\ninstabilities and their effects on the physical jet properties as a function of\nthe initial jet parameters: jet Lorentz factor, temperature, opening angle and\njet-environment density-contrast. The parameter space is designed to describe\nlow-power, weakly magnetized jets at small distances from the core (around the\nparsec scale). All collimating jets we simulated develop instabilities.\nRecollimation instabilities decelerate the jet, heat it, entrain external\nmaterial, and move the recollimation point to shorter distances from the core.\nThis is true for both conical and cylindrical jets. The instabilities, that are\nfirst triggered by the centrifugal instability, appear to be less disruptive in\nthe case of narrower, denser, more relativistic, and warmer jets. These results\nprovide valuable insights into the complex processes governing AGN jets and\ncould be used to model the properties of low-power, weakly magnetized jetted\nAGNs.",
        "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation.",
        "Recently, 3D-LLMs, which combine point-cloud encoders with large models, have\nbeen proposed to tackle complex tasks in embodied intelligence and scene\nunderstanding. In addition to showing promising results on 3D tasks, we found\nthat they are significantly affected by hallucinations. For instance, they may\ngenerate objects that do not exist in the scene or produce incorrect\nrelationships between objects. To investigate this issue, this work presents\nthe first systematic study of hallucinations in 3D-LLMs. We begin by quickly\nevaluating hallucinations in several representative 3D-LLMs and reveal that\nthey are all significantly affected by hallucinations. We then define\nhallucinations in 3D scenes and, through a detailed analysis of datasets,\nuncover the underlying causes of these hallucinations. We find three main\ncauses: (1) Uneven frequency distribution of objects in the dataset. (2) Strong\ncorrelations between objects. (3) Limited diversity in object attributes.\nAdditionally, we propose new evaluation metrics for hallucinations, including\nRandom Point Cloud Pair and Opposite Question Evaluations, to assess whether\nthe model generates responses based on visual information and aligns it with\nthe text's meaning.",
        "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.",
        "A servomotor is a closed-loop system designed for precise movement control,\nutilizing position feedback to achieve accurate final positions. Due to the\nability to deliver higher power output and operate at enhanced speeds, DC servo\nmotors are considered ideal for applications requiring precision and\nperformance. This research aims to design, simulate, and compare various\ncontrol strategies for precise position control in DC servo motors (DSM). The\ncontrollers evaluated in this study include proportional (P),\nproportional-integral (PI), proportional-integral-derivative (PID),\nstate-feedback controllers (SFC), and state-feedback controllers augmented with\nintegral action (SFCIA). The performance of these controllers was evaluated\nusing MATLAB simulations, characterized by overshoot, settling time,\nsteady-state error, rise time, and peak time. The results indicate that the\nstate-feedback controller with integral action (SFCIA) surpasses other control\nstrategies by achieving zero steady-state error, minimal overshoot, the\nshortest settling time, and optimized rise and peak times. These findings\nhighlight the effectiveness of SFCIA for tasks requiring high levels of\nstability, precision, and dynamic performance.",
        "We address the challenge of task-oriented navigation in unstructured and\nunknown environments, where robots must incrementally build and reason on rich,\nmetric-semantic maps in real time. Since tasks may require clarification or\nre-specification, it is necessary for the information in the map to be rich\nenough to enable generalization across a wide range of tasks. To effectively\nexecute tasks specified in natural language, we propose a hierarchical\nrepresentation built on language-embedded Gaussian splatting that enables both\nsparse semantic planning that lends itself to online operation and dense\ngeometric representation for collision-free navigation. We validate the\neffectiveness of our method through real-world robot experiments conducted in\nboth cluttered indoor and kilometer-scale outdoor environments, with a\ncompetitive ratio of about 60% against privileged baselines. Experiment videos\nand more details can be found on our project page: https:\/\/atlasnav.github.io",
        "Medical image denoising is considered among the most challenging vision\ntasks. Despite the real-world implications, existing denoising methods have\nnotable drawbacks as they often generate visual artifacts when applied to\nheterogeneous medical images. This study addresses the limitation of the\ncontemporary denoising methods with an artificial intelligence (AI)-driven\ntwo-stage learning strategy. The proposed method learns to estimate the\nresidual noise from the noisy images. Later, it incorporates a novel noise\nattention mechanism to correlate estimated residual noise with noisy inputs to\nperform denoising in a course-to-refine manner. This study also proposes to\nleverage a multi-modal learning strategy to generalize the denoising among\nmedical image modalities and multiple noise patterns for widespread\napplications. The practicability of the proposed method has been evaluated with\ndense experiments. The experimental results demonstrated that the proposed\nmethod achieved state-of-the-art performance by significantly outperforming the\nexisting medical image denoising methods in quantitative and qualitative\ncomparisons. Overall, it illustrates a performance gain of 7.64 in Peak\nSignal-to-Noise Ratio (PSNR), 0.1021 in Structural Similarity Index (SSIM),\n0.80 in DeltaE ($\\Delta E$), 0.1855 in Visual Information Fidelity Pixel-wise\n(VIFP), and 18.54 in Mean Squared Error (MSE) metrics.",
        "In this paper, we obtain the existence of weak solutions to the\nChoquard-Kirchhoff type critical multiphase problem: \\begin{equation*}\n\\left\\{\\begin{array}{cc}\n  &-M(\\varphi_{\\h}(\\lvert{\\nabla u}\\rvert))div(\\lvert{\\nabla\nu}\\rvert^{p(x)-2}\\nabla u+a_1(x)\\lvert{\\nabla u}\\rvert^{q(x)-2}\\nabla\nu+a_2(x)\\lvert{\\nabla u}\\rvert^{r(x)-2}\\nabla u)\n  & =\\lambda g(x)\\lvert{u}\\rvert^{\\gamma(x)-2}u+\\theta B(x,u)+\\kappa\n\\left(\\int_{\\q}\\frac{F(y,u(y))}{\\lvert{x-y}\\rvert^{d(x,y)}}\\, dy\\right) f(x,u)\n\\ \\text{in} \\ \\Omega,\n  & u=0 \\ \\text{on} \\ {\\partial \\Omega}. \\end{array}\\right. \\end{equation*}\n  The term $B(x,u)$ on the right-hand side generalizes the critical growth. We\nobtain existence and multiplicity results by establishing certain embedding\nresults and concentration compactness principle along with the\nHardy-Littlewood-Sobolev type inequality for the Musielak Orlicz Sobolev space\n$ W^{1,\\mathcal{T}}(\\q)$.",
        "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly detection models often prioritize accuracy while neglecting\nthe critical aspect of privacy. In this work, we propose a hybrid ensemble\nmodel that incorporates privacy-preserving techniques to address both detection\naccuracy and data protection. Our model combines the strengths of several\nmachine learning algorithms, including K-Nearest Neighbors (KNN), Support\nVector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create\na robust system capable of identifying network anomalies while ensuring\nprivacy. The proposed approach integrates advanced preprocessing techniques\nthat enhance data quality and address the challenges of small sample sizes and\nimbalanced datasets. By embedding privacy measures into the model design, our\nsolution offers a significant advancement over existing methods, ensuring both\nenhanced detection performance and strong privacy safeguards.",
        "Deuterium, a heavy isotope of hydrogen, is a key tracer of the formation of\nthe Solar System. Recent JWST observations have expanded the dataset of D\/H\nratios in methane on the KBOs Eris and Makemake, providing new insights into\ntheir origins. This study examines the elevated D\/H ratios in methane on these\nKBOs in the context of protosolar nebula dynamics and chemistry, and proposes a\nprimordial origin for the methane, in contrast to previous hypotheses\nsuggesting abiotic production by internal heating. A time-dependent disk model\ncoupled with a deuterium chemistry module was used to simulate the isotopic\nexchange between methane and hydrogen. Observational constraints, including the\nD\/H ratio measured in methane in comet 67P\/Churyumov-Gerasimenko, were used to\nrefine the primordial D\/H abundance. The simulations show that the observed D\/H\nratios in methane on Eris and Makemake are consistent with a primordial origin.\nThe results suggest that methane on these KBOs likely originates from the\nprotosolar nebula, similar to cometary methane, and was sequestered in solid\nform -- either as pure condensates or clathrates -- within their building\nblocks prior to accretion. These results provide a { simple} explanation for\nthe high D\/H ratios in methane on Eris and Makemake, without the need to invoke\ninternal production mechanisms.",
        "In this report, we present an experimental overview of quarkonium results\nobtained in nucleus-nucleus heavy-ion collisions, with a focus on the data\ncollected at the LHC. We discuss the current understanding of charmonium and\nbottomonium behavior in the deconfined medium produced in such collisions,\ncomparing the various observables now accessible to state-of-the-art\ntheoretical models. We also discuss the open points and how future heavy-ion\nexperiments aim to clarify these aspects.",
        "Kullback--Leibler (KL) divergence is a fundamental measure of the\ndissimilarity between two probability distributions, but it can become unstable\nin high-dimensional settings due to its sensitivity to mismatches in\ndistributional support. To address robustness limitations, we propose a novel\nQuantum-Inspired Fidelity-based Divergence (QIF), leveraging quantum\ninformation principles yet efficiently computable on classical hardware.\nCompared to KL divergence, QIF demonstrates improved numerical stability under\npartial or near-disjoint support conditions, thereby reducing the need for\nextensive regularization in specific scenarios. Moreover, QIF admits\nwell-defined theoretical bounds and continuous similarity measures. Building on\nthis, we introduce a novel regularization method, QR-Drop, which utilizes QIF\nto improve generalization in machine learning models. Empirical results show\nthat QR-Drop effectively mitigates overfitting and outperforms state-of-the-art\nmethods.",
        "Spiking Neural Networks (SNNs) have emerged as a promising tool for\nevent-based optical flow estimation tasks due to their ability to leverage\nspatio-temporal information and low-power capabilities. However, the\nperformance of SNN models is often constrained, limiting their application in\nreal-world scenarios. In this work, we address this gap by proposing a novel\nneural network architecture, ST-FlowNet, specifically tailored for optical flow\nestimation from event-based data. The ST-FlowNet architecture integrates\nConvGRU modules to facilitate cross-modal feature augmentation and temporal\nalignment of the predicted optical flow, improving the network's ability to\ncapture complex motion dynamics. Additionally, to overcome the challenges\nassociated with training SNNs, we introduce a novel approach to derive SNN\nmodels from pre-trained artificial neural networks (ANNs) through ANN-to-SNN\nconversion or our proposed BISNN method. Notably, the BISNN method alleviates\nthe complexities involved in biological parameter selection, further enhancing\nthe robustness of SNNs in optical flow estimation tasks. Extensive evaluations\non three benchmark event-based datasets demonstrate that the SNN-based\nST-FlowNet model outperforms state-of-the-art methods, delivering superior\nperformance in accurate optical flow estimation across a diverse range of\ndynamic visual scenes. Furthermore, the inherent energy efficiency of SNN\nmodels is highlighted, establishing a compelling advantage for their practical\ndeployment. Overall, our work presents a novel framework for optical flow\nestimation using SNNs and event-based data, contributing to the advancement of\nneuromorphic vision applications.",
        "The recent developments in data-driven methods have paved the way to new\nmethodologies to provide accurate state reconstruction of engineering systems;\nnuclear reactors represent particularly challenging applications for this task\ndue to the complexity of the strongly coupled physics involved and the\nextremely harsh and hostile environments, especially for new technologies such\nas Generation-IV reactors. Data-driven techniques can combine different sources\nof information, including computational proxy models and local noisy\nmeasurements on the system, to robustly estimate the state. This work leverages\nthe novel Shallow Recurrent Decoder architecture to infer the entire state\nvector (including neutron fluxes, precursors concentrations, temperature,\npressure and velocity) of a reactor from three out-of-core time-series neutron\nflux measurements alone. In particular, this work extends the standard\narchitecture to treat parametric time-series data, ensuring the possibility of\ninvestigating different accidental scenarios and showing the capabilities of\nthis approach to provide an accurate state estimation in various operating\nconditions. This paper considers as a test case the Molten Salt Fast Reactor\n(MSFR), a Generation-IV reactor concept, characterised by strong coupling\nbetween the neutronics and the thermal hydraulics due to the liquid nature of\nthe fuel. The promising results of this work are further strengthened by the\npossibility of quantifying the uncertainty associated with the state\nestimation, due to the considerably low training cost. The accurate\nreconstruction of every characteristic field in real-time makes this approach\nsuitable for monitoring and control purposes in the framework of a reactor\ndigital twin.",
        "In recent years, multimodal large language models (MLLMs) have achieved\nsignificant breakthroughs, enhancing understanding across text and vision.\nHowever, current MLLMs still face challenges in effectively integrating\nknowledge across these modalities during multimodal knowledge reasoning,\nleading to inconsistencies in reasoning outcomes. To systematically explore\nthis issue, we propose four evaluation tasks and construct a new dataset. We\nconduct a series of experiments on this dataset to analyze and compare the\nextent of consistency degradation in multimodal knowledge reasoning within\nMLLMs. Based on the experimental results, we identify factors contributing to\nthe observed degradation in consistency. Our research provides new insights\ninto the challenges of multimodal knowledge reasoning and offers valuable\nguidance for future efforts aimed at improving MLLMs.",
        "The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.",
        "Using direct numerical simulations of forced rotating turbulence, we study\nthe effect of rotation on the growth rate and the saturation level of the\nsmall-scale dynamo. For slow rotation rates, increasing the rotation rate\nreduces both the growth rate and the saturation level. Once the rotation rate\ncrosses a threshold, large-scale vortices are formed which enhance the growth\nrate and the saturation level. Below this threshold, the suppression of the\nsmall-scale dynamo with increasing rotation is explained by the fact that at\nscales close to, but smaller than, the forcing scale, rotating turbulence is\none-dimensionalized, with the velocity component along the rotation axis being\nlarger than the other two components. This is due to the rotational\ndestabilization of vortices produced by the forcing function. While the\nrotational effect on the growth rate becomes small at high Re, the ratio of the\nsteady-state magnetic to kinetic energies remains suppressed by up to 35% as\ncompared to the non-rotating case.",
        "We demonstrate that in the presence of a light scalar spectator field, vacuum\ntransitions taking place during inflation can produce large, potentially\ndetectable non-Gaussian signatures in the primordial curvature perturbation.\nSuch transitions are common in theories with multiple scalar fields when the\npotential has several minima. Our computation proceeds by numerically finding\nthe instanton solution that describes quantum tunnelling between vacuum states\nin a de Sitter background, calculating its dependence on the spectator field\nand, thereby, its effect on the expansion of space. For a scenario with Higgs\ninflation, we obtain the non-Gaussianity parameter $f_\\mathrm{NL} \\sim O(10)$\nand study its parameter dependence.",
        "This paper presents a hierarchical framework for demand response optimization\nin air separation units (ASUs) that combines reinforcement learning (RL) with\nlinear model predictive control (LMPC). We investigate two control\narchitectures: a direct RL approach and a control-informed methodology where an\nRL agent provides setpoints to a lower-level LMPC. The proposed RL-LMPC\nframework demonstrates improved sample efficiency during training and better\nconstraint satisfaction compared to direct RL control. Using an industrial ASU\ncase study, we show that our approach successfully manages operational\nconstraints while optimizing electricity costs under time-varying pricing.\nResults indicate that the RL-LMPC architecture achieves comparable economic\nperformance to direct RL while providing better robustness and requiring fewer\ntraining samples to converge. The framework offers a practical solution for\nimplementing flexible operation strategies in process industries, bridging the\ngap between data-driven methods and traditional control approaches.",
        "In this paper, we present a robust and fully discretized method for solving\nthe time fractional diffusion equation with high-contrast multiscale\ncoefficients. We establish the homogenized equation in a coarse mesh using a\nmulticontinuum approach and employ the exponential integrator method for time\ndiscretization. The multicontinuum upscaled model captures the physical\ncharacteristics of the solution for the high-contrast multiscale problem,\nincluding averages and gradient effects in each continuum at the coarse scale.\nWe use the exponential integration method to address the nonlocality induced by\nthe time fractional derivative and the stiffness from the multiscale\ncoefficients in the semi-discretized problem. Convergence analysis of the\nnumerical scheme is provided, along with illustrative numerical examples. Our\nresults demonstrate the accuracy, efficiency, and improved stability for\nvarying order of fractional derivatives.",
        "Spatio-Temporal (ST) data science, which includes sensing, managing, and\nmining large-scale data across space and time, is fundamental to understanding\ncomplex systems in domains such as urban computing, climate science, and\nintelligent transportation. Traditional deep learning approaches have\nsignificantly advanced this field, particularly in the stage of ST data mining.\nHowever, these models remain task-specific and often require extensive labeled\ndata. Inspired by the success of Foundation Models (FM), especially large\nlanguage models, researchers have begun exploring the concept of\nSpatio-Temporal Foundation Models (STFMs) to enhance adaptability and\ngeneralization across diverse ST tasks. Unlike prior architectures, STFMs\nempower the entire workflow of ST data science, ranging from data sensing,\nmanagement, to mining, thereby offering a more holistic and scalable approach.\nDespite rapid progress, a systematic study of STFMs for ST data science remains\nlacking. This survey aims to provide a comprehensive review of STFMs,\ncategorizing existing methodologies and identifying key research directions to\nadvance ST general intelligence.",
        "Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a\nnon-negative self-adjoint operator on $L^2(\\mathbb{R}^d)$. Assume that the\nkernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for\nall $N>0$, there are constants $c,C>0$ such that \\begin{align}\\label{upper\nbound}\n|p_t(x,y)|\\leq\\frac{C}{t^{d\/2}}e^{-\\frac{|x-y|^2}{ct}}\\Big(1+\\frac{\\sqrt{t}}{\\rho(x)}+\n\\frac{\\sqrt{t}}{\\rho(y)}\\Big)^{-N} \\end{align} holds for all\n$x,y\\in\\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix\nweighted inequalities for fractional type integrals associated to $L$ with new\nclasses of matrix weights, which are nontrivial extension of the results\nestablished by Li, Rahm and Wick [23]. Next, we give new two-weight bump\nconditions with Young functions satisfying wider conditions for fractional type\nintegrals associated to $L$, which cover the result obtained by Cruz-Uribe,\nIsralowitz and Moen [6]. We point out that the new classes of matrix weights\nand bump conditions are larger and weaker than the classical ones given in [17]\nand [6], respectively. As applications, our results can be applied to settings\nof magnetic Schr\\\"{o}dinger operator, Laguerre operators, etc.",
        "This paper presents a study that analyzed state-of-the-art vulnerability\nscanning tools applied to containers. We have focused the work on tools\nfollowing the Vulnerability Exploitability eXchange (VEX) format, which has\nbeen introduced to complement Software Bills of Material (SBOM) with security\nadvisories of known vulnerabilities. Being able to get an accurate\nunderstanding of vulnerabilities found in the dependencies of third-party\nsoftware is critical for secure software development and risk analysis.\nAccepting the overwhelming challenge of estimating the precise accuracy and\nprecision of a vulnerability scanner, we have in this study instead set out to\nexplore how consistently different tools perform. By doing this, we aim to\nassess the maturity of the VEX tool field as a whole (rather than any\nparticular tool). We have used the Jaccard and Tversky indices to produce\nsimilarity scores of tool performance for several different datasets created\nfrom container images. Overall, our results show a low level of consistency\namong the tools, thus indicating a low level of maturity in VEX tool space. We\nhave performed a number of experiments to find and explanation to our results,\nbut largely they are inconclusive and further research is needed to understand\nthe underlying causalities of our findings.",
        "Drones or unmanned aerial vehicles are traditionally used for military\nmissions, warfare, and espionage. However, the usage of drones has\nsignificantly increased due to multiple industrial applications involving\nsecurity and inspection, transportation, research purposes, and recreational\ndrone flying. Such an increased volume of drone activity in public spaces\nrequires regulatory actions for purposes of privacy protection and safety.\nHence, detection of illegal drone activities such as boundary encroachment\nbecomes a necessity. Such detection tasks are usually automated and performed\nby deep learning models which are trained on annotated image datasets. This\npaper builds on a previous work and extends an already published open source\ndataset. A description and analysis of the entire dataset is provided. The\ndataset is used to train the YOLOv7 deep learning model and some of its minor\nvariants and the results are provided. Since the detection models are based on\na single image input, a simple cross-correlation based tracker is used to\nreduce detection drops and improve tracking performance in videos. Finally, the\nentire drone detection system is summarized.",
        "Ekman pumping is a phenomenon induced by no-slip boundary conditions in\nrotating fluids. In the context of Rayleigh-B\\'enard convection, Ekman pumping\ncauses a significant change in the linear stability of the system compared to\nwhen it is not present (that is, stress-free). Motivated by numerical solutions\nto the marginal stability problem of the incompressible Navier-Stokes (iNSE)\nsystem, we seek analytical asymptotic solutions which describe the departure of\nthe no-slip solution from the stress-free. The substitution of normal modes\ninto a reduced asymptotic model yields a linear system for which we explore\nanalytical solutions for various scalings of wavenumber. We find very good\nagreement between the analytical asymptotic solutions and the numerical\nsolutions to the iNSE linear stability problem with no-slip boundary\nconditions.",
        "This paper proposes a novel approach based on deep reinforcement learning\n(DRL) for the 2D+1 packing problem with spatial constraints. This problem is an\nextension of the traditional 2D packing problem, incorporating an additional\nconstraint on the height dimension. Therefore, a simulator using the OpenAI Gym\nframework has been developed to efficiently simulate the packing of rectangular\npieces onto two boards with height constraints. Furthermore, the simulator\nsupports multidiscrete actions, enabling the selection of a position on either\nboard and the type of piece to place. Finally, two DRL-based methods (Proximal\nPolicy Optimization -- PPO and the Advantage Actor-Critic -- A2C) have been\nemployed to learn a packing strategy and demonstrate its performance compared\nto a well-known heuristic baseline (MaxRect-BL). In the experiments carried\nout, the PPO-based approach proved to be a good solution for solving complex\npackaging problems and highlighted its potential to optimize resource\nutilization in various industrial applications, such as the manufacturing of\naerospace composites."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints",
    "start_abstract":"An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project"
      ],
      "abstract":[
        "The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate"
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "The Relativity of Causal Knowledge",
        "Meta-Learning-Based People Counting and Localization Models Employing\n  CSI from Commodity WiFi NICs",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator",
        "Semadeni derivative of Banach spaces and functions on nonmetrizable\n  rectangles",
        "Minimum numbers of Dehn colors of knots and $\\mathcal{R}$-palette graphs",
        "The Third Generation of Nanogenerators: The Irreplaceable Potential\n  Source Enabled by the Flexoelectric Nanogenerator",
        "Simplicial effects and weakly associative partial groups",
        "Vectorial Kato inequality for $p$-harmonic maps with optimal constant",
        "Uniform-in-time error estimate of random batch method with replacement\n  for the Cucker-Smale model",
        "Electrical Control of the Exchange Bias Effect at\n  Ferromagnet-Altermagnet Junctions",
        "Hybrid Quantum-Classical Optimisation of Traveling Salesperson Problem",
        "Limited attention and models of choice: A behavioral equivalence",
        "A Pathwise Coordinate Descent Algorithm for LASSO Penalized Quantile\n  Regression",
        "Precompactness in bivariate metric semigroup-valued bounded variation\n  spaces",
        "Thomas-Wigner rotation via Clifford algebras",
        "Analysing the flux stability of stellar calibrator candidates with TESS",
        "Balanced cross-Kerr coupling for superconducting qubit readout",
        "Nano-topographical changes in latent fingerprint due to degradation over\n  time studied by Atomic force microscopy -- option to set a timeline?",
        "On singular problems associated with mixed operators under mixed\n  boundary conditions",
        "On total transitivity of graphs",
        "Stochastic Stefan problem on moving hypersurfaces: an approach by a new\n  framework of nonhomogeneous monotonicity",
        "Weak-emission-line quasars: A new clue from their optical variability",
        "On the global solvability of the generalised Navier-Stokes system in\n  critical Besov spaces",
        "Role of the Short-Range Dynamics in the Formation of $D^{(*)}\\bar\n  D^{(*)}\/B^{(*)}\\bar B^{(*)} $ Hadronic Molecules",
        "Orbits on a product of two flags and a line and the Bruhat Order, I",
        "Extended circular nim",
        "A Dimension-Reduced Multivariate Spatial Model for Extreme Events:\n  Balancing Flexibility and Scalability",
        "Cavity quantum electrodynamics of photonic temporal crystals",
        "Effect of interface on magnetic exchange coupling in Co\/Ru\/Co trilayer:\n  from ab-initio simulations to micromagnetics"
      ],
      "abstract":[
        "Recent advances in artificial intelligence reveal the limits of purely\npredictive systems and call for a shift toward causal and collaborative\nreasoning. Drawing inspiration from the revolution of Grothendieck in\nmathematics, we introduce the relativity of causal knowledge, which posits\nstructural causal models (SCMs) are inherently imperfect, subjective\nrepresentations embedded within networks of relationships. By leveraging\ncategory theory, we arrange SCMs into a functor category and show that their\nobservational and interventional probability measures naturally form convex\nstructures. This result allows us to encode non-intervened SCMs with convex\nspaces of probability measures. Next, using sheaf theory, we construct the\nnetwork sheaf and cosheaf of causal knowledge. These structures enable the\ntransfer of causal knowledge across the network while incorporating\ninterventional consistency and the perspective of the subjects, ultimately\nleading to the formal, mathematical definition of relative causal knowledge.",
        "In this paper, we consider people counting and localization systems\nexploiting channel state information (CSI) measured from commodity WiFi network\ninterface cards (NICs). While CSI has useful information of amplitude and phase\nto describe signal propagation in a measurement environment of interest, CSI\nmeasurement suffers from offsets due to various uncertainties. Moreover, an\nuncontrollable external environment where other WiFi devices communicate each\nother induces interfering signals, resulting in erroneous CSI captured at a\nreceiver. In this paper, preprocessing of CSI is first proposed for offset\nremoval, and it guarantees low-latency operation without any filtering process.\nAfterwards, we design people counting and localization models based on\npre-training. To be adaptive to different measurement environments,\nmeta-learning-based people counting and localization models are also proposed.\nNumerical results show that the proposed meta-learning-based people counting\nand localization models can achieve high sensing accuracy, compared to other\nlearning schemes that follow simple training and test procedures.",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator is\ninvestigated both theoretically and experimentally. When the Kerr nonlinear\nresonator is driven strongly such that the induced Rabi frequency is comparable\nto or larger than the Kerr nonlinearity, the system cannot be approximated as a\ntwo-level system. We theoretically derive characteristic features in the\nfluorescence spectra such as the decrease of the center-peak intensity and the\nasymmetric sideband peaks in the presence of finite dephasing. Those features\nare consistently explained by the population of the initial dressed state and\nits transition matrix element to the final dressed state of the transition\ncorresponding to each peak. Finally, we experimentally measure the resonance\nfluorescence spectra of a driven superconducting Kerr nonlinear resonator and\nfind a quantitative agreement with our theory.",
        "We study Banach spaces $C(K)$ of real-valued continuous functions from the\nfinite product of compact lines. It turns out that the topological character of\nthese compact lines can be used to distinguish whether two spaces of continuous\nfunctions on products are isomorphic or embeddable to each other. In\nparticular, for compact lines $K_1, \\dots, K_n, L_1, \\dots, L_k$ of uncountable\ncharacter and $k \\neq n$, we claim that Banach spaces $C(\\prod_{i=1}^n K_i)$\nand $C(\\prod_{j=1}^k L_j)$ are not isomorphic.",
        "In this paper, we consider minimum numbers of colors of knots for Dehn\ncolorings. In particular, we will show that for any odd prime number $p$ and\nany Dehn $p$-colorable knot $K$, the minimum number of colors for $K$ is at\nleast $\\lfloor \\log_2 p \\rfloor +2$. Moreover, we will define the $\\R$-palette\ngraph for a set of colors. The $\\R$-palette graphs are quite useful to give\ncandidates of sets of colors which might realize a nontrivially Dehn\n$p$-colored diagram. In Appendix, we also prove that for Dehn $5$-colorable\nknot, the minimum number of colors is $4$.",
        "The electroneutrality assumption has long been adopted by scholars; however,\nthis assumption may lead to an oversight of certain physical effects. Using\nderivations from a discontinuous medium, we have obtained an expression for the\npotential and energy of a many-body unipolar charge system, which corresponds\nwell to its counterpart in a continuous medium. The compressed form of this\nexpression suggests that compressing a macroscale charged body to the nanoscale\ncan yield an enormous electric potential and energy, thereby establishing a\nconcrete research framework for third-generation nanogenerators. This effect\nmay serve as a crucial reference for understanding anomalous spatial\nelectromagnetic distributions and divergent energy fields.",
        "In this paper, we introduce a new category of simplicial effects that extends\nthe categories of effect algebras and their multi-object counterpart, effect\nalgebroids. Our approach is based on relaxing the associativity condition\nsatisfied by effect algebras and, more generally, partial monoids. Within this\nframework, simplicial effects and weakly associative partial groups arise as\ntwo extreme cases in the category of weak partial monoids. Our motivation is to\ncapture simplicial structures from the theory of simplicial distributions and\nmeasurements that behave like effects.",
        "We derive the sharp vectorial Kato inequality for $p$-harmonic mappings.\nSurprisingly, the optimal constant differs from the one obtained for scalar\nvalued $p$-harmonic functions by Chang, Chen, and Wei. As an application we\ndemonstrate how this inequality can be used in the study of regularity of\n$p$-harmonic maps. Furthermore, in the case of $p$-harmonic maps from $B^3$ to\n$\\mathbb{S}^3$, we enhance the known range of $p$ values for which regularity\nis achieved. Specifically, we establish that for $p \\in [2, 2.642]$, minimizing\n$p$-harmonic maps must be regular.",
        "The Random Batch Method (RBM), proposed by Jin et al. in 2020, is an\nefficient algorithm for simulating interacting particle systems. The\nuniform-in-time error estimates of the RBM without replacement have been\nobtained for various interacting particle systems, while the analysis of the\nRBM with replacement is just considered in (Cai et al., 2024) recently for the\nfirst-order systems governed by Langevin dynamics. In this work, we present the\nerror estimate for the RBM with replacement applied to a second-order system\nknown as the Cucker-Smale model. By introducing a crucial auxiliary system and\nleveraging the intrinsic characteristics of the Cucker-Smale model, we derive\nan estimate that is uniform in both time and particle numbers. Additionally, we\nprovide numerical simulations to validate the analytical results.",
        "This work analyzes the behavior of the interface between a ferromagnetic\nmaterial and an alter-magnet. We use a well-established line of arguments based\non electronic mean-field calculations to show that new surface phenomena that\nlead to altermagnetic materials induce an exchange bias effect on the nearby\nferromagnet. We reveal the physical mechanisms behind this phenomenon that lead\nto quantitative control over its strength. Interestingly, we predict exotic\nelectric-field-induced phenomena. This is an analogy to the relationship\nbetween exchange bias and the injection of spin currents in\nspin-transfer-dominated scenarios, which has been reported earlier in the\ntraditional antiferromagnetic\/ferromagnetic junction.",
        "The Traveling Salesperson Problem (TSP) is a fundamental NP-hard optimisation\nchallenge with widespread applications in logistics, operations research, and\nnetwork design. While classical algorithms effectively solve small to\nmedium-sized instances, they struggle with scalability due to exponential\ncomplexity. In this work, we present a hybrid quantum-classical approach that\nleverages IBM's Qiskit Runtime to integrate quantum optimisation techniques\nwith classical machine learning methods, specifically K-Means clustering and\nRandom Forest classifiers. These machine learning components aid in problem\ndecomposition and noise mitigation, improving the quality of quantum solutions.\nExperimental results for TSP instances ranging from 4 to 8 cities reveal that\nthe quantum-only approach produces solutions up to 21.7% worse than the\nclassical baseline, while the hybrid method reduces this cost increase to 11.3%\nfor 8 cities. This demonstrates that hybrid approaches improve solution quality\ncompared to purely quantum methods but remain suboptimal compared to classical\nsolvers. Despite current hardware limitations, these results highlight the\npotential of quantum-enhanced methods for combinatorial optimisation, paving\nthe way for future advancements in scalable quantum computing frameworks.",
        "We show that many models of choice can be alternatively represented as\nspecial cases of choice with limited attention (Masatlioglu, Nakajima, and\nOzbay, 2012), and the properties of the unobserved attention filters that\nexplain the observed choices are singled out. Moreover, for each specification,\nwe infer information about the DM's attention and preference from irrational\nfeatures of choice data.",
        "$\\ell_1$ penalized quantile regression is used in many fields as an\nalternative to penalized least squares regressions for high-dimensional data\nanalysis. Existing algorithms for penalized quantile regression either use\nlinear programming, which does not scale well in high dimension, or an\napproximate coordinate descent (CD) which does not solve for exact\ncoordinatewise minimum of the nonsmooth loss function. Further, neither\napproaches build fast, pathwise algorithms commonly used in high-dimensional\nstatistics to leverage sparsity structure of the problem in large-scale data\nsets. To avoid the computational challenges associated with the nonsmooth\nquantile loss, some recent works have even advocated using smooth\napproximations to the exact problem. In this work, we develop a fast, pathwise\ncoordinate descent algorithm to compute exact $\\ell_1$ penalized quantile\nregression estimates for high-dimensional data. We derive an easy-to-compute\nexact solution for the coordinatewise nonsmooth loss minimization, which, to\nthe best of our knowledge, has not been reported in the literature. We also\nemploy a random perturbation strategy to help the algorithm avoid getting stuck\nalong the regularization path. In simulated data sets, we show that our\nalgorithm runs substantially faster than existing alternatives based on\napproximate CD and linear program, while retaining the same level of estimation\naccuracy.",
        "In this paper, we show that if a set in bivariate metric semigroups-valued\nbounded variation spaces is pointwise totally bounded and joint equivariated\nthen it is precompact. These spaces include bounded Jordan variation spaces,\nbounded Wiener variation spaces, bounded Waterman variation spaces, bounded\nRiesz variation spaces and bounded Korenblum variation spaces. To do so, we\nintroduce the concept of equimetric set.",
        "We derive Macfarlane's formula for the Thomas-Wigner angle of rotation using\nClifford-algebra methods. The presentation is pedagogical and elementary,\nsuitable for students with some basic knowledge of special relativity; no prior\nknowledge of Clifford algebras is required.",
        "The ESA space mission Ariel requires bright sources that are stable at the\nlevel of 100ppm over 6 hours in order to accurately measure exoplanet\natmospheres through transmission spectroscopy. To ensure this, in-flight\ninstrument calibration can be performed by observing stellar calibrators. In\nthis study, a stellar calibrator candidate list distributed over the sky is\ncreated and a flux variability analysis is performed to identify the best\nstellar calibrators for transit spectroscopy of exoplanet atmospheres with\nAriel. A starting candidate sample of 1937 solar-type stars is created using\nthe all-sky surveys Two Micron All Sky Survey and Gaia. Using stellar light\ncurves from the Transit Exoplanet Survey Satellite (TESS), the flux variability\nof each star is characterised by computing its Lomb-Scargle periodogram and\nreduced chi-squared. This enables the elimination of stars with detectable\nvariability from the sample. Approximately 22.2% of stars from the starting\nsample pass the selection as potential calibrators. These do not all\nnecessarily meet Ariel's stability requirement, although some will. No\ncorrelation between flux stability and stellar properties is found, as long as\nthe correct value ranges for the parameters are chosen, like a surface\ntemperature between 5000 and 6300K. The only exception is stellar magnitude:\nNoise in TESS data increases as stars get dimmer, so, a high percentage of\nfaint stars passes the selection since their variability is more likely hidden\nwithin the inherent TESS noise. Contrarily, stars brighter than 5mag cannot be\nused as calibrators. A list of 430 promising bright calibration targets\ndistributed over the sky has been selected. These can potentially be used as\nstellar calibrators for the Ariel mission. Targets from this list will have to\nbe further studied to determine which ones possess a flux stability better than\n100ppm over 6 hours.",
        "Dispersive readout, the standard method for measuring superconducting qubits,\nis limited by multiphoton qubit-resonator processes arising even at moderate\ndrive powers. These processes degrade performance, causing dispersive readout\nto lag behind single- and two-qubit gates in both speed and fidelity. In this\nwork, we propose a novel readout method, termed \"junction readout\". Junction\nreadout leverages the nonperturbative cross-Kerr interaction resulting from\ncoupling a qubit and a resonator via a Josephson junction. Furthermore, by\nadding a capacitive coupling in parallel to the junction, Purcell decay can be\nsuppressed without the need for a Purcell filter. We also show that junction\nreadout is more robust against deleterious multiphoton processes, and offers\ngreater flexibility for resonator frequency allocation. Crucially, junction\nreadout achieves superior performance compared to dispersive readout while\nmaintaining similar hardware overhead. Numerical simulations show that junction\nreadout can achieve fidelities exceeding $99.99\\%$ in under $30$ ns, making it\na promising alternative for superconducting qubit readout with current\nhardware.",
        "Latent fingerprints, if present, are crucial in identifying the suspect who\nwas at the crime scene. If there are many latent fingerprints or the suspect is\nfrom the same household, crime investigators may have difficulty identifying\nwhose latent fingerprints are time-related to the crime. Here, we report\nchanges in the nanoscale topography of latent fingerprints, which may serve as\na timeline and could help estimate when the latent fingerprint was imprinted.\nOn the latent fingerprint of an adolescent, we observed a change in\nnano-topography over time, specifically the formation of nano-chain structures\nin space between the imprinted papillary ridges. We consequently compared this\nobservation with the decomposition of the latent fingerprints of a child and\nadult. We observed a significant difference in the time change in\nnano-topography of latent fingerprints of a child, adolescent, and young adult.\nThe nano-topographical changes of latent fingerprints were studied by atomic\nforce microscopy over 70 days. In the case of child's and adolescent's latent\nfingerprints, the first nano-chains were observed already 24 hours after\nimprinting of the latent fingerprint, and the number of nano-chains increased\nsteadily up to 21 days, then we observed that another organic material covered\nthe nano-chains, and they started slowly deteriorating; nevertheless, the\nnano-chains were still present on the 70th day.",
        "In this paper, we study the following singular problem associated with mixed\noperators (the combination of the classical Laplace operator and the fractional\nLaplace operator) under mixed boundary conditions \\begin{equation*} \\label{1}\n  \\left\\{\n  \\begin{aligned}\n  \\mathcal{L}u &= g(u), \\quad u > 0 \\quad \\text{in} \\quad \\Omega,\n  u &= 0 \\quad \\text{in} \\quad U^c,\n  \\mathcal{N}_s(u) &= 0 \\quad \\text{in} \\quad \\mathcal{N},\n  \\frac{\\partial u}{\\partial \\nu} &= 0 \\quad \\text{in} \\quad \\partial \\Omega\n\\cap \\overline{\\mathcal{N}},\n  \\end{aligned}\n  \\right.\n  \\tag{$P_\\lambda$} \\end{equation*}\n  where $U= (\\Omega \\cup {\\mathcal{N}} \\cup\n(\\partial\\Omega\\cap\\overline{\\mathcal{N}}))$, $\\Omega \\subseteq \\mathbb{R}^N$\nis a non empty open set, $\\mathcal{D}$, $\\mathcal{N}$ are open subsets of\n$\\mathbb{R}^N\\setminus{\\bar{\\Omega }}$ such that ${\\mathcal{D}} \\cup\n{\\mathcal{N}}= \\mathbb{R}^N\\setminus{\\bar{\\Omega}}$, $\\mathcal{D} \\cap\n{\\mathcal{N}}= \\emptyset $ and $\\Omega\\cup \\mathcal{N}$ is a bounded set with\nsmooth boundary, $\\lambda >0$ is a real parameter and\n  $\\mathcal{L}= -\\Delta+(-\\Delta)^{s},~ \\text{for}~s \\in (0, 1).$\n  Here $g(u)=u^{-q}$ or $g(u)= \\lambda u^{-q}+ u^p$ with $0<q<1<p\\leq 2^*-1$.\nWe study $(P_\\lambda)$ to derive the existence of weak solutions along with its\n$L^\\infty$-regularity. Moreover, some Sobolev-type variational inequalities\nassociated with these weak solutions are established.",
        "Let $G=(V, E)$ be a graph where $V$ and $E$ are the vertex and edge sets,\nrespectively. For two disjoint subsets $A$ and $B$ of $V$, we say $A$\n\\emph{dominates} $B$ if every vertex of $B$ is adjacent to at least one vertex\nof $A$. A vertex partition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a\n\\emph{transitive partition} of size $k$ if $V_i$ dominates $V_j$ for all $1\\leq\ni<j\\leq k$. In this article, we study a variation of transitive partition,\nnamely \\emph{total transitive partition}. The total transitivity $Tr_t(G)$ is\nequal to the maximum order of a vertex partition $\\pi = \\{V_1, V_2, \\ldots,\nV_k\\}$ of $G$ obtained by repeatedly removing a total dominating set from $G$,\nuntil no vertices remain. Thus, $V_1$ is a total dominating set of $G$, $V_2$\nis a total dominating set of the graph $G_1=G-V_1$, and, in general, $V_{i+1}$\nis a total dominating set in the graph $G_i=G-\\bigcup_{j=1}^i V_i$. A vertex\npartition of order $Tr_t(G)$ is called $Tr_t$-partition. The \\textsc{Maximum\nTotal Transitivity Problem} is to find a total transitive partition of a given\ngraph with the maximum number of parts. First, we characterize split graphs\nwith total transitivity equal to $1$ and $\\omega(G)-1$. Moreover, for the split\ngraph $G$ and $1\\leq p\\leq \\omega(G)-1$, we give some necessary conditions for\n$Tr_t(G)=p$. Furthermore, we show that the decision version of this problem is\nNP-complete for bipartite graphs. On the positive side, we design a\npolynomial-time algorithm to solve \\textsc{Maximum Total Transitivity Problem}\nin trees.",
        "The purpose of this paper is to establish the well-posedness of the\nstochastic Stefan problem on moving hypersurfaces. Through a specially designed\ntransformation, it turns out we need to solve stochastic partial differential\nequations on a fixed hypersurface with a new kind of nonhomogeneous\nmonotonicity involving a family of time-dependent operators. This new class of\nSPDEs is of independent interest and can also be applied to solve many other\ninteresting models such as the stochastic $p$-Laplacian equations, stochastic\nAllen-Cahn equation and stochastic heat equations on time-dependent domains or\nhypersurfaces. (Monotone) Operator-valued calculus and geometric analysis of\nmoving hypersurfaces play important roles in the study. Moreover, a forthcoming\nresult on the well-posedness of stochastic 2D Navier-Stokes equation on moving\ndomains is also based on our framework.",
        "Weak-emission-line QSOs (WLQs) are an enigmatic subclass of the QSO\npopulation, as their optical\/UV spectra are marked by abnormally weak (or\nabsent) emission lines. To obtain much-needed additional clues to the origin of\nthis and other known peculiarities of WLQs, we have determined the 'ensemble'\noptical variability characteristics for a large, well-defined sample of 76\nradio-quiet WLQs and also for a matched control sample comprising 603 normal\nradio-quiet QSOs. This analysis was done using their light-curves recorded in\nthe $g$ and $r$ bands, under the ZTF survey during 2018-2024, with a typical\ncadence of 3 days. We find that, compared to normal QSOs, WLQs exhibit\nsystematically milder optical variability on month\/year-like time scales (by a\nfactor of $\\sim$ 1.76$\\pm$0.05 in amplitude). We have independently verified\nthis by carrying out an equivalent analysis of the V-band light-curves acquired\nunder the CRTS during 2007- 2014, with a typical cadence of 10 days. This new\nobservational differentiator between WLQs and normal QSOs may provide clues to\nunderstanding the intriguing nature of WLQs. It is proposed that the clumpiness\nof the torus material flowing into the central engine may play a key role in\nexplaining the observed differences between the WLQs and normal QSOs.",
        "This paper is devoted to the global solvability of the Navier-Stokes system\nwith fractional Laplacian $(-\\Delta)^{\\alpha}$ in $\\mathbb{R}^{n}$ for\n$n\\geq2$, where the convective term has the form $(|u|^{m-1}u)\\cdot\\nabla u$\nfor $m\\geq1$. By establishing the estimates for the difference\n$|u_{1}|^{m-1}u_{1}-|u_{2}|^{m-1}u_{2}$ in homogeneous Besov spaces, and\nemploying the maximal regularity property of $(-\\Delta)^{\\alpha}$ in Lorentz\nspaces, we prove global existence and uniqueness of the strong solution of the\nNavier-Stokes in critical Besov spaces for both $m=1$ and $m>1$",
        "We investigate potential hadronic molecular states in the $D^{(*)}\\bar\nD^{(*)}$ and $B^{(*)}\\bar B^{*}$ systems using light meson exchange\ninteractions. Our analysis focuses on coupled-channel systems with spin-parity\nquantum numbers $J^{PC}=0^{++}$, $1^{+\\pm}$ and $2^{++}$, examining how the\n$\\delta(r)$ potential affects states near threshold. Using coupled-channel\nanalysis, we reproduce the $X(3872)$ mass with a given cutoff for the\n$(I)J^{PC}=(0)1^{++}$ state, finding a minimal impact from the $\\delta(r)$\nterm. At this cutoff, both the $(0)0^{++}$ state near the $D\\bar D$ threshold\nand the $(0)1^{+-}$ state near the $D\\bar D^*$ threshold show less sensitivity\nto the $\\delta(r)$ term compared to the three states-$(0)0^{++}$, $(0)1^{+-}$,\nand $(0)2^{++}$-near the $D^*\\bar D^*$ threshold. As anticipated, the\n$B^{(*)}\\bar B^{*}$ systems exhibit similar behavior but with stronger binding\ndue to their larger reduced mass. These findings suggest promising directions\nfor future experimental searches, particularly in the isoscalar sector, which\ncould substantially advance our understanding of exotic tetraquark states.",
        "Let $G=GL(n)$ be the $n\\times n$ complex general linear group and let\n$\\mathcal{B}_{n}$ be its flag variety. The standard Borel subgroup $B$ of upper\ntriangular matrices acts on the product $\\mathcal{B}_{n}\\times\n\\mathbb{P}^{n-1}$ with finitely many orbits. In this paper, we study the\n$B$-orbits on the subvarieties $\\mathcal{B}_{n}\\times \\mathcal{O}_{i}$, where\n$\\mathcal{O}_{i}$ is the $B$-orbit on $\\mathbb{P}^{n-1}$ containing the line\nthrough the origin in the direction of the $i$-th standard basis vector of\n$\\mathbb{C}^{n}$. For each $i=1,\\dots, n$, we construct a bijection between\n$B$-orbits on $\\mathcal{B}_{n}\\times\\mathcal{O}_{i}$ and certain pairs of\nSchubert cells in $\\mathcal{B}_{n}\\times\\mathcal{B}_{n}$. We also show that\nthis bijection can be used to understand the Richardson-Springer monoid action\non such $B$-orbits in terms of the classical monoid action of the symmetric\ngroup on itself. We also develop combinatorial models of these orbits and use\nthese models to compute exponential generating functions for the sequences\n$\\{|B\\backslash(\\mathcal{B}_{n}\\times\\mathcal{O}_{i})|\\}_{n\\geq 1}$ and\n$\\{|B\\backslash (\\mathcal{B}_{n}\\times \\mathbb{P}^{n-1})|\\}_{n\\geq 1}$. In the\nsequel to this paper, we use the results of this paper to construct a\ncorrespondence between $B$-orbits on $\\mathcal{B}_{n}\\times\\mathbb{P}^{n-1}$\nand a collection of $B$-orbits on the flag variety $\\mathcal{B}_{n+1}$ of\n$GL(n+1)$ and show that this correspondence respects closures relations and\npreserves monoid actions. As a consequence both closure relations and monoid\nactions for all $B$-orbits on $\\mathcal{B}_{n}\\times\\mathbb{P}^{n-1}$ can be\nunderstood via the Bruhat order by using our results in [CE].",
        "Circular nim $CN(m, k)$ is a variant of nim, in which there are $m$ piles of\ntokens arranged in a circle and each player, in their turn, chooses at most $k$\nconsecutive piles in the circle and removes an arbitrary number of tokens from\neach pile. The player must remove at least one token in total. For some cases\nof $m$ and $k$, closed formulas to determine which player has a winning\nstrategy have been found. Almost all cases are still open problems. In this\npaper, we consider a variant of circular nim, extended circular nim. In\nextended circular nim $ECN(m_S, k)$, there are $m$ piles of tokes arranged in a\ncircle. $S$ is a set of positive integers less than or equal to half of $m$.\nEach player, in their turn, chooses at most $k$ piles selected every $s$-th\npile in a circle for an $s \\in S$. We find some closed formulas to determine\nwhich player has a winning strategy for the cases where the number of piles is\nno more than eight.",
        "Modeling extreme precipitation and temperature is vital for understanding the\nimpacts of climate change, as hazards like intense rainfall and record-breaking\ntemperatures can result in severe consequences, including floods, droughts, and\nwildfires. Gaining insight into the spatial variation and interactions between\nthese extremes is critical for effective risk management, early warning\nsystems, and informed policy-making. However, challenges such as the rarity of\nextreme events, spatial dependencies, and complex cross-variable interactions\nhinder accurate modeling. We introduce a novel framework for modeling spatial\nextremes, building upon spatial generalized extreme value (GEV) models. Our\napproach incorporates a dimension-reduced latent spatial process to improve\nscalability and flexibility, particularly in capturing asymmetry in\ncross-covariance structures. This Joint Latent Spatial GEV model (JLS-GEV)\novercomes key limitations of existing methods by providing a more flexible\nframework for inter-variable dependencies. In addition to addressing event\nrarity, spatial dependence and cross-variable interactions, JLS-GEV supports\nnonstationary spatial behaviors and independently collected data sources, while\nmaintaining practical fitting times through dimension reduction. We validate\nJLS-GEV through extensive simulation studies, demonstrating its superior\nperformance in capturing spatial extremes compared to baseline modeling\napproaches. Application to real-world data on extreme precipitation and\ntemperature in the southeastern United States highlights its practical utility.\nWhile primarily motivated by environmental challenges, this framework is\nbroadly applicable to interdisciplinary studies of spatial extremes in\ninterdependent natural processes.",
        "Photonic temporal crystals host a variety of intriguing phenomena, from wave\namplification and mixing to exotic band structures, all stemming from the\ntime-periodic modulation of optical properties. While these features have been\nwell described classically, their quantum manifestation has remained elusive.\nHere, we introduce a quantum electrodynamical model of PTCs that reveals a\ndeeper connection between classical and quantum pictures: the classical\nmomentum gap arises from a localization-delocalization quantum phase transition\nin a Floquet-photonic synthetic lattice. Leveraging an effective Hamiltonian\nperspective, we pinpoint the critical momenta and highlight how classical\nexponential field growth manifests itself as wave-packet acceleration in the\nquantum synthetic space. Remarkably, when a two-level atom is embedded in such\na cavity, its Rabi oscillations undergo irreversible decay to a half-and-half\nmixed state-a previously unobserved phenomenon driven by photonic\ndelocalization within the momentum gap, even with just a single frequency mode.\nOur findings establish photonic temporal crystals as versatile platforms for\nstudying nonequilibrium quantum photonics and suggest new avenues for\ncontrolling light matter interactions through time domain engineering.",
        "Interfaces play a substantial role for the functional properties of\nstructured magnetic materials and magnetic multilayers. Modeling the functional\nbehavior of magnetic materials requires the treatment of the relevant phenomena\nat the device level. Properties predicted from the electronic structure and\nspin dynamics at the atomistic level have to be properly transferred into a\ncontinuum level treatment. In this work we show how Co\/Ru\/Co three layers can\nbe simulated with the continuum theory of micromagnetism, with interface\ncoupling energies and bulk intrinsic properties properly derived from the\nresults of \\emph{ab initio} and spin dynamics simulations at different\ntemperatures."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry",
    "start_abstract":"Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "A ConvNet for the 2020s"
      ],
      "abstract":[
        "The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Intrinsic Donaldson-Thomas theory. I. Component lattices of stacks",
        "Ferromagnetic Resonance in a Magnetically Dilute Percolating\n  Ferromagnet: An Experimental and Theoretical Study",
        "Towards Enterprise-Ready Computer Using Generalist Agent",
        "Maps from Grassmannians of 2-planes to projective spaces",
        "Approaching the Limits to EFL Writing Enhancement with AI-generated Text\n  and Diverse Learners",
        "Sensitivity of Double Deeply Virtual Compton Scattering observables to\n  Generalized Parton Distributions",
        "The EnviroMapper Toolkit: an Input Physicalisation that Captures the\n  Situated Experience of Environmental Comfort in Offices",
        "VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from\n  Sparse Matrix Decomposition",
        "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study",
        "Post-Quantum Stealth Address Protocols",
        "MinD the gap: Membrane proteins form 3D patterns in a suspension of\n  liposomes",
        "Causes and Strategies in Multiagent Systems",
        "Hybrid MIMO in the Upper Mid-Band: Architectures, Processing, and Energy\n  Efficiency",
        "Unit Edge-Length Rectilinear Drawings with Crossings and Rectangular\n  Faces",
        "DBRouting: Routing End User Queries to Databases for Answerability",
        "Towards Unified Structured Light Optimization",
        "AI\/ML Based Detection and Categorization of Covert Communication in IPv6\n  Network",
        "DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single\n  Image",
        "The Role of Legal Frameworks in Shaping Ethical Artificial Intelligence\n  Use in Corporate Governance",
        "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks",
        "Quantum interference with time-frequency modes and multiple-photons\n  generated by a silicon nitride microresonator",
        "Split-Aperture Phased Array Radar Resource Management for Tracking Tasks",
        "The Battling Influencers Game: Nash Equilibria Structure of a Potential\n  Game and Implications to Value Alignment",
        "Hierarchical Neuro-Symbolic Decision Transformer",
        "Simultaneous Energy Harvesting and Bearing Fault Detection using\n  Piezoelectric Cantilevers",
        "Data Fusion for Full-Range Response Reconstruction via Diffusion Models",
        "A Non-Monotone Line-Search Method for Minimizing Functions with Spurious\n  Local Minima",
        "Adaptive finite element approximation for quasi-static crack growth",
        "A Fresh Perspective on Water Dynamics in Aqueous Salt Solutions"
      ],
      "abstract":[
        "This is the first paper in a series on intrinsic Donaldson-Thomas theory, a\ngeneralization of Donaldson-Thomas theory from the linear case, or the case of\nmoduli stacks of objects in $3$-Calabi-Yau abelian categories, to the\nnon-linear case of general $(-1)$-shifted symplectic stacks. This is done by\ndeveloping a new framework for studying the enumerative geometry of general\nalgebraic stacks, and we expect that this framework can also be applied to\nextending other types of enumerative theories for linear stacks to the\nnon-linear case.\n  In this paper, we establish the foundations of our framework. We introduce\nthe component lattice of an algebraic stack, which is the key combinatorial\nobject in our theory. It generalizes and globalizes the cocharacter lattice and\nthe Weyl group of an algebraic group, and is defined as the set of connected\ncomponents of the stack of graded points of the original stack.\n  We prove several results on the structure of graded and filtered points of a\nstack using the component lattice. The first is the constancy theorem, which\nstates that there is a wall-and-chamber structure on the component lattice,\nsuch that the isomorphism types of connected components of the stacks of graded\nand filtered points stay constant within each chamber. The second is the\nfiniteness theorem, providing a criterion for the finiteness of the number of\npossible isomorphism types of these components. The third is the associativity\ntheorem, generalizing the structure of Hall algebras from linear stacks to\ngeneral stacks, involving a notion of Hall categories.\n  Finally, we discuss some applications of these results outside\nDonaldson-Thomas theory, including a construction of stacks of real-weighted\nfiltrations, and a generalization of the semistable reduction theorem to\nreal-weighted filtrations.",
        "Ferromagnetic resonance (FMR) serves as a powerful probe of magnetization\ndynamics and anisotropy in percolating ferromagnets, where short-range\ninteractions govern long-range magnetic order. We apply this approach to\nGa$_{1-x}$Mn$_x$N ($x \\simeq 8$), a dilute ferromagnetic semiconductor,\ncombining FMR and superconducting quantum interference device magnetometry. Our\nresults confirm the percolative nature of ferromagnetism in (Ga,Mn)N, with a\nCurie temperature $T_{\\mathrm{C}} = 12$ K, and reveal that despite magnetic\ndilution, key features of conventional ferromagnets are retained. FMR\nmeasurements establish a robust uniaxial anisotropy, dictated by Mn$^{3+}$\nsingle-ion anisotropy, with an easy-plane character at low Mn content. While\nexcessive line broadening suppresses FMR signals below 9 K, they persist up to\n70 K, indicating the presence of non-percolating ferromagnetic clusters well\nabove $T_{\\mathrm{C}}$. The temperature dependence of the FMR intensity follows\nthat of the magnetization, underscoring the stability of these clusters.\nAnalysis of the FMR linewidth provides insights into relaxation processes,\nrevealing large Gilbert damping due to the low magnetization of the system.\nStrikingly, atomistic spin model simulations reproduce the experimentally\nobserved resonance fields, anisotropy trends, and linewidth evolution with\nremarkable accuracy. This agreement underscores the predictive power of our\nmodeling approach in describing percolating ferromagnets. This study advances\nthe understanding of percolating ferromagnetic systems, demonstrating that FMR\nis a key technique for probing their unique dynamic and anisotropic properties.\nOur findings contribute to the broader exploration of dilute ferromagnets and\nprovide new insights into percolating ferromagnetic systems, which will be\nrelevant for spintronic opportunities.",
        "This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.",
        "Using quaternions and octonions, we construct some maps from the Grassmannian\nof 2-dimensional planes of $\\mathbb{R}^n$, $\\mathrm{Gr}_2(\\mathbb{R}^n)$, to\nthe projective space $\\mathbb{R}\\mathrm{P}^k$, for certain values of $n$ and\n$k$. All of our maps induce an isomorphism at the level of fundamental groups,\nand two of them are shown to be submersions. As an application, we obtain new\nestimates of the Lusternik-Schnirelmann category of\n$\\mathrm{Gr}_2(\\mathbb{R}^n)$ for specific values of $n$.",
        "Generative artificial intelligence (AI) chatbots, such as ChatGPT, are\nreshaping how English as a foreign language (EFL) students write since students\ncan compose texts by integrating their own words with AI-generated text. This\nstudy investigated how 59 Hong Kong secondary school students with varying\nlevels of academic achievement interacted with AI-generated text to compose a\nfeature article, exploring whether any interaction patterns benefited the\noverall quality of the article. Through content analysis, multiple linear\nregression and cluster analysis, we found the overall number of words --\nwhether AI- or human-generated -- is the main predictor of writing quality.\nHowever, the impact varies by students' competence to write independently, for\ninstance, by using their own words accurately and coherently to compose a text,\nand to follow specific interaction patterns with AI-generated text. Therefore,\nalthough composing texts with human words and AI-generated text may become\nprevalent in EFL writing classrooms, without educators' careful attention to\nEFL writing pedagogy and AI literacy, high-achieving students stand to benefit\nmore from using AI-generated text than low-achieving students.",
        "Double Deeply Virtual Compton Scattering (DDVCS) is a promising channel for\nGeneralized Parton Distribution (GPD) studies as it is a generalization of the\nDeeply Virtual Compton Scattering (DVCS) and Timelike Compton Scattering (TCS)\nprocesses. Contrary to DVCS and TCS, the GPD phase space accessed through DDVCS\nis not constrained by on-shell conditions on the incoming and outgoing photons\nthus allowing unrestricted GPD extraction from experimental observables.\nConsidering polarized electron and positron beams directed to a polarized\nproton target, we study the sensitivity of the DDVCS cross-section asymmetries\nto the chiral-even proton GPDs from different model predictions. The\nfeasibility of such measurements is further investigated in the context of the\nCLAS and SoLID spectrometers at the Thomas Jefferson National Accelerator\nFacility and the future Electron-Ion Collider at the Brookhaven National\nLaboratory.",
        "The environmental comfort in offices is traditionally captured by surveying\nan entire workforce simultaneously, which yet fails to capture the situatedness\nof the different personal experiences. To address this limitation, we developed\nthe EnviroMapper Toolkit, a data physicalisation toolkit that allows individual\noffice workers to record their personal experiences of environmental comfort by\nmapping the actual moments and locations these occurred. By analysing two\nin-the-wild studies in existing open-plan office environments (N=14), we\ndemonstrate how this toolkit acts like a situated input visualisation that can\nbe interpreted by domain experts who were not present during its construction.\nThis study therefore offers four key contributions: (1) the iterative design\nprocess of the physicalisation toolkit; (2) its preliminary deployment in two\nreal-world office contexts; (3) the decoding of the resulting artefacts by\ndomain experts; and (4) design considerations to support future input\nphysicalisation and visualisation constructions that capture and synthesise\ndata from multiple individuals.",
        "Securely computing graph convolutional networks (GCNs) is critical for\napplying their analytical capabilities to privacy-sensitive data like\nsocial\/credit networks. Multiplying a sparse yet large adjacency matrix of a\ngraph in GCN--a core operation in training\/inference--poses a performance\nbottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it\nincurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and\nleveraging the monotonicity of non-zero entry locations, we propose a co-design\nharmonizing secure multi-party computation (MPC) with matrix sparsity. Our\nsparse matrix decomposition transforms an arbitrary sparse matrix into a\nproduct of structured matrices. Specialized MPC protocols for oblivious\npermutation and selection multiplication are then tailored, enabling our secure\nsparse matrix multiplication ($(SM)^2$) protocol, optimized for secure\nmultiplication of these structured matrices. Together, these techniques take\n$O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present\nVirgos, a secure 2-party framework that is communication-efficient and\nmemory-friendly on standard vertically-partitioned graph datasets. Performance\nof Virgos has been empirically validated across diverse network conditions.",
        "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
        "The Stealth Address Protocol (SAP) allows users to receive assets through\nstealth addresses that are unlinkable to their stealth meta-addresses. The most\nwidely used SAP, Dual-Key SAP (DKSAP), and the most performant SAP, Elliptic\nCurve Pairing Dual-Key SAP (ECPDKSAP), are based on elliptic curve\ncryptography, which is vulnerable to quantum attacks. These protocols depend on\nthe elliptic curve discrete logarithm problem, which could be efficiently\nsolved on a sufficiently powerful quantum computer using the Shor algorithm. In\nthis paper three novel post-quantum SAPs based on lattice-based cryptography\nare presented: LWE SAP, Ring-LWE SAP and Module-LWE SAP. These protocols\nleverage Learning With Errors (LWE) problem to ensure quantum-resistant\nprivacy. Among them, Module-LWE SAP, which is based on the Kyber key\nencapsulation mechanism, achieves the best performance and outperforms ECPDKSAP\nby approximately 66.8% in the scan time of the ephemeral public key registry.",
        "The self-organization of pattern-forming systems depends not only on the\nchemical but also physical properties of their components. In this work, we\nfragmented and dispersed the MinDE protein system's lipid substrate into\ndiffusive sub-micrometer-sized liposomes, and report that the ATP-fueled\nprotein-protein interactions continue to drive spatially extended patterns at\nscales well separated from those of the requisite liposomes, despite the\ncomplete loss of membrane continuity. The patterns form in three-dimensions\nbecause the membrane is dispersed in a volume. By varying protein\nconcentration, liposome size distribution, and density, we observed and\ncharacterized rich 3D dynamical patterns at steady state, including traveling\nwaves, dynamical spirals and a mixed phase where both patterns coexist.\nSimulations and linear stability analysis of a coarse-grained model reveal that\nthe dispersed membranes's physical properties effectively rescale two key\nfactors that govern pattern formation and wavelength selection:\nprotein-membrane binding rates and diffusion. This work highlights the\nrobustness of pattern formation in membrane-bulk systems despite membrane\nfragmentation. It suggests that biological protein systems have the potential\nto serve as adaptable templates for out-of-equilibrium self-organization in 3D,\nbeyond in vivo biological contexts.",
        "Causality plays an important role in daily processes, human reasoning, and\nartificial intelligence. There has however not been much research on causality\nin multi-agent strategic settings. In this work, we introduce a systematic way\nto build a multi-agent system model, represented as a concurrent game\nstructure, for a given structural causal model. In the obtained so-called\ncausal concurrent game structure, transitions correspond to interventions on\nagent variables of the given causal model. The Halpern and Pearl framework of\ncausality is used to determine the effects of a certain value for an agent\nvariable on other variables. The causal concurrent game structure allows us to\nanalyse and reason about causal effects of agents' strategic decisions. We\nformally investigate the relation between causal concurrent game structures and\nthe original structural causal models.",
        "As 6G networks evolve, the upper mid-band spectrum (7 GHz to 24 GHz), or\nfrequency range 3 (FR3), is emerging as a promising balance between the\ncoverage offered by sub-6 GHz bands and the high-capacity of millimeter wave\n(mmWave) frequencies. This paper explores the structure of FR3 hybrid MIMO\nsystems and proposes two architectural classes: Frequency Integrated (FI) and\nFrequency Partitioned (FP). FI architectures enhance spectral efficiency by\nexploiting multiple sub-bands parallelism, while FP architectures dynamically\nallocate sub-band access according to specific application requirements.\nAdditionally, two approaches, fully digital (FD) and hybrid analog-digital\n(HAD), are considered, comparing shared (SRF) versus dedicated RF (DRF) chain\nconfigurations. Herein signal processing solutions are investigated,\nparticularly for an uplink multi-user scenario with power control optimization.\n  Results demonstrate that SRF and DRF architectures achieve comparable\nspectral efficiency; however, SRF structures consume nearly half the power of\nDRF in the considered setup. While FD architectures provide higher spectral\nefficiency, they do so at the cost of increased power consumption compared to\nHAD. Additionally, FI architectures show slightly greater power consumption\ncompared to FP; however, they provide a significant benefit in spectral\nefficiency (over 4 x), emphasizing an important trade-off in FR3 engineering.",
        "Unit edge-length drawings, rectilinear drawings (where each edge is either a\nhorizontal or a vertical segment), and rectangular face drawings are among the\nmost studied subjects in Graph Drawing. However, most of the literature on\nthese topics refers to planar graphs and planar drawings. In this paper we\nstudy drawings with all the above nice properties but that can have edge\ncrossings; we call them Unit Edge length Rectilinear drawings with Rectangular\nFaces (UER-RF drawings). We consider crossings as dummy vertices and apply the\nunit edge-length convention to the edge segments connecting any two (real or\ndummy) vertices. Note that UER-RF drawings are grid drawings (vertices are\nplaced at distinct integer coordinates), which is another classical requirement\nof graph visualizations. We present several efficient and easily implementable\nalgorithms for recognizing graphs that admit UER-RF drawings and for\nconstructing such drawings if they exist. We consider restrictions on the\ndegree of the vertices or on the size of the faces. For each type of\nrestriction, we consider both the general unconstrained setting and a setting\nin which either the external boundary of the drawing is fixed or the rotation\nsystem of the graph is fixed as part of the input.",
        "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.",
        "Structured light (SL) 3D reconstruction captures the precise surface shape of\nobjects, providing high-accuracy 3D data essential for industrial inspection\nand robotic vision systems. However, current research on optimizing projection\npatterns in SL 3D reconstruction faces two main limitations: each scene\nrequires separate training of calibration parameters, and optimization is\nrestricted to specific types of SL, which restricts their application range. To\ntackle these limitations, we present a unified framework for SL optimization,\nadaptable to diverse lighting conditions, object types, and different types of\nSL. Our framework quickly determines the optimal projection pattern using only\na single projected image. Key contributions include a novel global matching\nmethod for projectors, enabling precise projector-camera alignment with just\none projected image, and a new projection compensation model with a photometric\nadjustment module to reduce artifacts from out-of-gamut clipping. Experimental\nresults show our method achieves superior decoding accuracy across various\nobjects, SL patterns, and lighting conditions, significantly outperforming\nprevious methods.",
        "The flexibility and complexity of IPv6 extension headers allow attackers to\ncreate covert channels or bypass security mechanisms, leading to potential data\nbreaches or system compromises. The mature development of machine learning has\nbecome the primary detection technology option used to mitigate covert\ncommunication threats. However, the complexity of detecting covert\ncommunication, evolving injection techniques, and scarcity of data make\nbuilding machine-learning models challenging. In previous related research,\nmachine learning has shown good performance in detecting covert communications,\nbut oversimplified attack scenario assumptions cannot represent the complexity\nof modern covert technologies and make it easier for machine learning models to\ndetect covert communications. To bridge this gap, in this study, we analyzed\nthe packet structure and network traffic behavior of IPv6, used encryption\nalgorithms, and performed covert communication injection without changing\nnetwork packet behavior to get closer to real attack scenarios. In addition to\nanalyzing and injecting methods for covert communications, this study also uses\ncomprehensive machine learning techniques to train the model proposed in this\nstudy to detect threats, including traditional decision trees such as random\nforests and gradient boosting, as well as complex neural network architectures\nsuch as CNNs and LSTMs, to achieve detection accuracy of over 90\\%. This study\ndetails the methods used for dataset augmentation and the comparative\nperformance of the applied models, reinforcing insights into the adaptability\nand resilience of the machine learning application in IPv6 covert\ncommunication. In addition, we also proposed a Generative AI-assisted\ninterpretation concept based on prompt engineering as a preliminary study of\nthe role of Generative AI agents in covert communication.",
        "Recent developments in generative diffusion models have turned many dreams\ninto realities. For video object insertion, existing methods typically require\nadditional information, such as a reference video or a 3D asset of the object,\nto generate the synthetic motion. However, inserting an object from a single\nreference photo into a target background video remains an uncharted area due to\nthe lack of unseen motion information. We propose DreamInsert, which achieves\nImage-to-Video Object Insertion in a training-free manner for the first time.\nBy incorporating the trajectory of the object into consideration, DreamInsert\ncan predict the unseen object movement, fuse it harmoniously with the\nbackground video, and generate the desired video seamlessly. More\nsignificantly, DreamInsert is both simple and effective, achieving zero-shot\ninsertion without end-to-end training or additional fine-tuning on\nwell-designed image-video data pairs. We demonstrated the effectiveness of\nDreamInsert through a variety of experiments. Leveraging this capability, we\npresent the first results for Image-to-Video object insertion in a\ntraining-free manner, paving exciting new directions for future content\ncreation and synthesis. The code will be released soon.",
        "This article examines the evolving role of legal frameworks in shaping\nethical artificial intelligence (AI) use in corporate governance. As AI systems\nbecome increasingly prevalent in business operations and decision-making, there\nis a growing need for robust governance structures to ensure their responsible\ndevelopment and deployment. Through analysis of recent legislative initiatives,\nindustry standards, and scholarly perspectives, this paper explores key legal\nand regulatory approaches aimed at promoting transparency, accountability, and\nfairness in corporate AI applications. It evaluates the strengths and\nlimitations of current frameworks, identifies emerging best practices, and\noffers recommendations for developing more comprehensive and effective AI\ngovernance regimes. The findings highlight the importance of adaptable,\nprinciple-based regulations coupled with sector-specific guidance to address\nthe unique challenges posed by AI technologies in the corporate sphere.",
        "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.",
        "We demonstrate bipartite gaussian boson sampling with squeezed light in 6\nmixed time-frequency modes. Non-degenerate two-mode squeezing is generated in\ntwo time-bins from a silicon nitride microresonator with simultaneous high\nspectral purity (>0.86(3)) and indistinguishability (0.985(2)). An unbalanced\ninterferometer embedding electro-optic modulators, which is stabilized by\nexploiting the continuous energy-time entanglement of the generated photon\npairs, controls time and frequency-bin modes. We measure 144 collision-free\nevents with 4 photons at the output, achieving a fidelity >0.98 with the\ntheoretical probability distribution. We use this result to identify the\nsimilarity between families of isomorphic graphs with 6 vertices, and present\nan approach for the realization of universal operations on time-frequency\nmodes.",
        "The next generation of radar systems will include advanced digital front-end\ntechnology in the apertures allowing for spatially subdividing radar tasks over\nthe array, the so-called split-aperture phased array (SAPA) concept. The goal\nof this paper is to introduce radar resource management for the SAPA concept\nand to demonstrate the added benefit of the SAPA concept for active tracking\ntasks. To do so, the radar resource management problem is formulated and solved\nby employing the quality of service based resource allocation model (Q-RAM)\nframework. As active tracking tasks may be scheduled simultaneously, the\nresource allocation of tasks becomes dependent on the other tasks. The solution\nto the resource allocation problem is obtained by introducing the adaptive fast\ntraversal algorithm combined with a three dimensional strip packing algorithm\nto handle task dependencies. It will be demonstrated by a simulation example\nthat the SAPA concept can significantly increase the number of active tracks of\na multifunction radar system compared to scheduling tasks sequentially.",
        "When multiple influencers attempt to compete for a receiver's attention,\ntheir influencing strategies must account for the presence of one another. We\nintroduce the Battling Influencers Game (BIG), a multi-player simultaneous-move\ngeneral-sum game, to provide a game-theoretic characterization of this social\nphenomenon. We prove that BIG is a potential game, that it has either one or an\ninfinite number of pure Nash equilibria (NEs), and these pure NEs can be found\nby convex optimization. Interestingly, we also prove that at any pure NE, all\n(except at most one) influencers must exaggerate their actions to the maximum\nextent. In other words, it is rational for the influencers to be non-truthful\nand extreme because they anticipate other influencers to cancel out part of\ntheir influence. We discuss the implications of BIG to value alignment.",
        "We present a hierarchical neuro-symbolic control framework that couples\nclassical symbolic planning with transformer-based policies to address complex,\nlong-horizon decision-making tasks. At the high level, a symbolic planner\nconstructs an interpretable sequence of operators based on logical\npropositions, ensuring systematic adherence to global constraints and goals. At\nthe low level, each symbolic operator is translated into a sub-goal token that\nconditions a decision transformer to generate a fine-grained sequence of\nactions in uncertain, high-dimensional environments. We provide theoretical\nanalysis showing how approximation errors from both the symbolic planner and\nthe neural execution layer accumulate. Empirical evaluations in grid-worlds\nwith multiple keys, locked doors, and item-collection tasks show that our\nhierarchical approach outperforms purely end-to-end neural approach in success\nrates and policy efficiency.",
        "Bearings are critical components in industrial machinery, yet their\nvulnerability to faults often leads to costly breakdowns. Conventional fault\ndetection methods depend on continuous, high-frequency vibration sensing,\ndigitising, and wireless transmission to the cloud-an approach that\nsignificantly drains the limited energy reserves of battery-powered sensors,\naccelerating their depletion and increasing maintenance costs. This work\nproposes a fundamentally different approach: rather than using instantaneous\nvibration data, we employ piezoelectric energy harvesters (PEHs) tuned to\nspecific frequencies and leverage the cumulative harvested energy over time as\nthe key diagnostic feature. By directly utilising the energy generated from the\nmachinery's vibrations, we eliminate the need for frequent analog-to-digital\nconversions and data transmission, thereby reducing energy consumption at the\nsensor node and extending its operational lifetime. To validate this approach,\nwe use a numerical PEH model and publicly available acceleration datasets,\nexamining various PEH designs with different natural frequencies. We also\nconsider the influence of the classification algorithm, the number of devices,\nand the observation window duration. The results demonstrate that the harvested\nenergy reliably indicates bearing faults across a range of conditions and\nseverities. By converting vibration energy into both a power source and a\ndiagnostic feature, our solution offers a more sustainable, low-maintenance\nstrategy for fault detection in smart machinery.",
        "Accurately capturing the full-range response of structures is crucial in\nstructural health monitoring (SHM) for ensuring safety and operational\nintegrity. However, limited sensor deployment due to cost, accessibility, or\nscale often hinders comprehensive monitoring. This paper presents a novel data\nfusion framework utilizing diffusion models to reconstruct the full-range\nstructural response from sparse and heterogeneous sensor measurements. We\nincorporate Diffusion Posterior Sampling (DPS) into the reconstruction\nframework, using sensor measurements as probabilistic constraints to guide the\nsampling process. A lightweight neural network serves as the surrogate forward\nmodel within the DPS algorithm, which maps full-range structural responses to\nlocal sensor data. This approach enables flexibility in sensor configurations\nwhile reducing computational costs. The proposed framework is validated on a\nsteel plate shear wall exhibiting nonlinear responses. Comparative experiments\nare conducted with three forward models. Among these, the neural network\nsurrogate model achieves a desirable reconstruction accuracy, with a weighted\nmean absolute percentage error (WMAPE) as low as 1.57%, while also\ndemonstrating superior adaptability and computational efficiency. Additional\nexperiments explore the impact of sensor placement strategies and noise levels.\nResults show that even under sparse measurements or high noise conditions, the\nWMAPE remains capped at 15%, demonstrating the robustness in challenging\nscenarios. The proposed framework shows new possibilities for probabilistic\nmodeling and decision-making in SHM, offering a novel data fusion approach for\nfull-range monitoring of structures.",
        "In this paper, we propose a new non-monotone line-search method for smooth\nunconstrained optimization problems with objective functions that have many\nnon-global local minimizers. The method is based on a relaxed Armijo condition\nthat allows a controllable increase in the objective function between\nconsecutive iterations. This property helps the iterates escape from nearby\nlocal minimizers in the early iterations. For objective functions with\nLipschitz continuous gradients, we derive worst-case complexity estimates on\nthe number of iterations needed for the method to find approximate stationary\npoints. Numerical results are presented, showing that the new method can\nsignificantly outperform other non-monotone methods on functions with spurious\nlocal minima.",
        "We provide an adaptive finite element approximation for a model of\nquasi-static crack growth in dimension two. The discrete setting consists of\nintegral functionals that are defined on continuous, piecewise affine\nfunctions, where the triangulation is a part of the unknown of the problem and\nadaptive in each minimization step. The limit passage is conducted\nsimultaneously in the vanishing mesh size and discretized time step, and\nresults in an evolution for the continuum Griffith model of brittle fracture\nwith isotropic surface energy [FriedrichSolombrino16] which is characterized by\nan irreversibility condition, a global stability, and an energy balance. Our\nresult corresponds to an evolutionary counterpart of the static\nGamma-convergence result in [BabadjianBonhomme23] for which, as a byproduct, we\nprovide an alternative proof.",
        "Molecular dynamics in pure water and aqueous salt solutions remain\nincompletely understood, partly due to the apparent contradictions between\nresults from different spectroscopic techniques. In this work, we demonstrate,\nby detailed comparison of light scattering and dielectric spectroscopy data for\npure water and aqueous lithium chloride solutions, that these apparent\ncontradictions can be resolved by accounting for orientational\ncross-correlations of neighboring molecules. Remarkably, a single structural\nrelaxation mode with largely temperature- and concentration-independent shape\ncan be identified in all spectra, from room temperature down to the deeply\nsupercooled regime. These results provide a new perspective for the study of\nmolecular dynamics in aqueous salt solutions."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"A ConvNet for the 2020s",
    "start_abstract":"The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry"
      ],
      "abstract":[
        "Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "From Site Response to Site-city Interaction: a Case Study in the Tokyo\n  Area",
        "Well-to-Tank Carbon Intensity Variability of Fossil Marine Fuels: A\n  Country-Level Assessment",
        "The IDEA detector concept for FCC-ee",
        "A simple magnetic field stabilization technique for atomic Bose-Einstein\n  condensate experiments",
        "Terahertz Magnon Excitations and Switching in Non-Collinear\n  Antiferromagnets",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Diffusion Approximation for Slow-Fast SDEs with State-Dependent\n  Switching",
        "Direct Nucleation of Hierarchical Nanostructures on Plasmonic Fiber\n  Optics Enables Enhanced SERS Performance",
        "Periodic Variability of the Central Stars of Planetary Nebulae Surveyed\n  through the Zwicky Transient Facility",
        "Further results on relative, divergence measures based on extropy and\n  their applications",
        "On recurrence and entropy in hyperspace of continua in dimension one",
        "Emerging Excess Consistent with a Narrow Resonance at 152 GeV in\n  High-Energy Proton-Proton Collisions",
        "Fibonacci-Modulation-Induced Multiple Topological Anderson Insulators",
        "The route of shear to Ising superconductivity in bilayer graphene",
        "Detection of Physiological Data Tampering Attacks with Quantum Machine\n  Learning",
        "Functional equation arising in behavioral sciences: solvability and\n  collocation scheme in H\\\"older spaces",
        "Large $N$ limits of supersymmetric quantum field theories: A pedagogical\n  overview",
        "A note on the differential spectrum of a class of locally APN functions",
        "Limiting distributions of generalized money exchange models",
        "Integrals of Legendre polynomials and approximations",
        "Four regimes of primary radiation damage in tungsten",
        "Manipulation of topological phase transitions and the mechanism of\n  magnetic interactions in Eu-based Zintl-phase materials",
        "Group distance magic cubic graphs",
        "An abelian formula for the quantum Weyl group action of the coroot\n  lattice",
        "PDRs4All. XII. FUV-driven formation of hydrocarbon radicals and their\n  relation with PAHs",
        "Weak-strong uniqueness for solutions to mean-field games",
        "Strong law of large numbers for a branching random walk among Bernoulli\n  traps",
        "On termination of minimal model program for log canonical pairs on\n  complex analytic spaces",
        "Photo-induced spall failure of (111) twist grain boundaries in Ni\n  bicrystals"
      ],
      "abstract":[
        "Considering the purpose of the session relating early engineering\ndevelopments in site response and soil-structure interaction, this paper\nfocuses on the development of studies regarding site-city interaction following\nthe striking site response observations obtained in Mexico City during the 1985\nGuerrero-Michoacan event, The first part presents an overview of the\ninvestigations on multiple structure-soil-structure interaction, starting with\nMexico-city like environments with dense urbanization on soft soils, which\nlater evolved with the concept of metamaterials. Up to now, such investigations\nhave been largely relying on numerical simulations in 2D and 3D media, coupling\nsoft surface soil layers and simplified building models, including also some\ntheoretical developments using various mechanical concepts. They also relied on\na number of laboratory experiments on reduced-scale mock-ups with diverse\nvibratory sources (shaking table, acoustic devices). The latest studies coupled\nfull-scale experiments on mechanical analogs such as forests or wind turbine\nfarms involving sets of resonators with similar frequencies, and numerical\nsimulation to investigate their impact on the propagation of surface (Rayleigh)\nwaves. Almost all such studies converge in predicting lower ground motion\namplitude for sites located within the ''urbanized'' area, but none of them can\nbe considered a ''groundtruth'' proof for a real earthquake in a real city. The\nsecond part thus takes advantage of the long duration of strong motion\nobservations in the Kanto area thanks to the KiK-net, K-NET and JMA\n(Shin-dokei) networks, to investigate the possible changes in site response\nwith time. The first results obtained with the event-specific site terms\nderived from Generalized Inversion Techniques (Nakano et al., 2015) indicate a\nsystematic reduction of the low frequency (0.2 -1 Hz) site amplification, in\nthe central-south Tokyo area. As this frequency band corresponds both to the\nsite frequency (very thick deposits) and to the high-rise buildings, the\ndiscussion focuses on the possible relation with the extensive construction in\nsome areas of downtown Tokyo over the last 2 decades.",
        "The transition toward a low-carbon maritime transportation requires\nunderstanding lifecycle carbon intensity (CI) of marine fuels. While\nwell-to-tank emissions significantly contribute to total greenhouse gas\nemissions, many studies lack global perspective in accounting for upstream\noperations, transportation, refining, and distribution. This study evaluates\nwell-to-tank CI of High Sulphur Fuel Oil (HSFO) and well-to-refinery exit CI of\nLiquefied Petroleum Gas (LPG) worldwide at asset level. HSFO represents\ntraditional marine fuel, while LPG serves as potential transition fuel due to\nlower tank-to-wake emissions and compatibility with low-carbon fuels. Using\nOPGEE and PRELIM tools with R-based geospatial methods, we derive country-level\nCI values for 72 countries (HSFO) and 74 countries (LPG), covering 98% of\nglobal production. Results show significant variation in climate impacts\nglobally. HSFO upstream CI ranges 1-22.7 gCO2e\/MJ, refining CI 1.2-12.6\ngCO2e\/MJ, with global volume-weighted-average well-to-tank CI of 12.4 gCO2e\/MJ.\nUpstream and refining account for 55% and 32% of HSFO well-to-tank CI, with\nlarge exporters and intensive refining practices showing higher emissions. For\nLPG, upstream CI ranges 0.9-22.7 gCO2e\/MJ, refining CI 2.8-13.9 gCO2e\/MJ, with\nvolume-weighted-average well-to-refinery CI of 15.6 gCO2e\/MJ. Refining\ncomprises 49% of LPG well-to-refinery CI, while upstream and transport\nrepresent 44% and 6%. Major players include China, United States and Russia.\nThese findings reveal significant CI variability across countries and supply\nchains, offering opportunities for targeted emission reduction policies.",
        "A detector concept, named IDEA, optimized for the physics and running\nconditions at the FCC-ee is presented. After discussing the expected running\nconditions and the main physics drivers, a detailed description of the\nindividual sub-detectors is given. These include: a very light tracking system\nwith a powerful vertex detector inside a large drift chamber surrounded by a\nsilicon wrapper, a high resolution dual readout crystal electromagnetic\ncalorimeter, an HTS based superconducting solenoid, a dual readout fiber\ncalorimeter and three layers of muon chambers embedded in the magnet flux\nreturn yoke. Some examples of the expected detector performance, based on fast\nand full simulation, are also given.",
        "We demonstrate a simple magnetic field stabilization technique in a\nBose-Einstein condensate experiment. Our technique is based on the precise\nmeasurement of the current fluctuations in the main magnetic field coils and\namounts to their compensation using an auxiliary coil. It has the advantage of\nsimplicity as compensation is done using a low inductance coil that can be\nstraightforwardly driven at the relevant frequencies (1 kHz). The performances\nof the different components (power supply, current transducer, electronics...)\nare precisely characterized. In addition, for optimal stability the ambient\nmagnetic field is also measured and compensated. The magnetic field stability\naround 57 G is measured by Ramsey spectroscopy of magnetic field sensitive\nradiofrequency transition between two spin states of potassium 39 and the\nshot-to-shot fluctuations are reduced to 64(7) $\\mu$G rms, i.e. at the 1 x 10\n-6 level. In the context of our experiment, this result opens interesting\nprospects for the study of three-body interactions in Bose-Einstein condensate\npotassium spin mixtures.",
        "We investigate how spatiotemporal spin polarized current can lead to\nterahertz frequency excitations in non-collinear antiferromagnets. By solving\nthe Landau-Lifshitz-Gilbert equation numerically for non-collinear\nantiferromagnet, we show that the magnon frequency spectrum exhibits standing\nspin wave modes and depends on the thickness of Mn$_3$Ge in heterostructure\nFe|Au|Mn$_3$Ge. Also, we analyze the switching process of ground state as a\nfunction of a spin current. We show a switching phase diagram, which contains\nswitching and non-switching regions. Our work suggests non-collinear\nantiferromagnets as an efficient platform for terahertz magnonics and ultrafast\nmemory devices.",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "In this paper, we study the diffusion approximation for slow-fast stochastic\ndifferential equations with state-dependent switching, where the slow component\n$X^{\\varepsilon}$ is the solution of a stochastic differential equation with\nadditional homogenization term, while the fast component $\\alpha^{\\varepsilon}$\nis a switching process. We first prove the weak convergence of\n$\\{X^\\varepsilon\\}_{0<\\varepsilon\\leq 1}$ to $\\bar{X}$ in the space of\ncontinuous functions, as $\\varepsilon\\rightarrow 0$. Using the martingale\nproblem approach and Poisson equation associated with a Markov chain, we\nidentify this weak limiting process as the unique solution $\\bar{X}$ of a new\nstochastic differential equation, which has new drift and diffusion terms that\ndiffer from those in the original equation. Next, we prove the order $1\/2$ of\nweak convergence of $X^{\\varepsilon}_t$ to $\\bar{X}_t$ by applying suitable\ntest functions $\\phi$, for any $t\\in [0, T]$. Additionally, we provide an\nexample to illustrate that the order we achieve is optimal.",
        "We present an innovative fabrication method to achieve bottom-up in situ\nsurface-overstructured Au nanoislands (NIs) with tunable grades of surface\ncoverage, elongation, and branching, directly on micro-optical fibers for\nsensing applications. These all-in-gold hierarchical nanostructures consist of\nNIs coated with surface protrusions of various morphologies. They are created\nin solution using a selective seeded growth approach, whereby additional gold\ngrowth is achieved over Au NIs formerly developed on the fiber facet by a\nsolid-state dewetting approach. The morphology of nanosized surface-NI\noverstructuring can be adjusted from multi-dot-decorated Au NIs to\nmulti-arm-decorated Au NIs. This engineering of optical fibers allows for\nimproved remote surface-enhanced Raman spectroscopy (SERS) molecular detection.\nBy combining solid-state dewetting and wet-chemical approaches, we achieve\nstable in-contact deposition of surface-overstructured NIs with the optical\nfiber solid substrate, alongside precise control over branching morphology and\nanisotropy extent. The fiber optic probes engineered by surface-overstructured\nNIs exhibit outstanding sensing performance in an instant and through-fiber\ndetection scheme, achieving a remarkable detection limit at 10-7 M for the R6G\naqueous solution. These engineered probes demonstrate an improved detection\nlimit by one order of magnitude and enhanced peak prominence compared to\ndevices solely decorated with pristine NIs.",
        "A consensus has been reached in recent years that binarity plays an important\nrole in the formation and evolution of a significant fraction of planetary\nnebulae (PNe). Utilizing the archived photometric data from the Zwicky\nTransient Facility survey, we conducted a comprehensive data mining in search\nfor brightness variations in a large sample of Galactic PNe. This effort leads\nto identification of 39 PNe, whose central stars exhibit periodic variation in\nlight curves. Among these objects, 20 are known binary central stars of PNe,\nwhile the remaining 19 are new discoveries. Additionally, we identified 14 PNe\nwith central stars displaying anomalous variation in light curves, as well as\neight variables based on the high-cadence photometric data. Among the new\ndiscoveries of periodicity, we found compelling evidence of binary systems at\nthe centres of two archetypal quadrupolar PNe. We also report on very peculiar\nbrightness variation observed in the central core of the compact PN NGC6833.\nSeveral PNe in our sample deserve follow-up observations, both high-dispersion\nspectroscopy and high-precision photometry, to reveal the true nature of their\ncentral binarity or even multiplicity.",
        "This study explores information measures based on extropy, introducing\ndynamic relative extropy measures for residual and past lifetimes, and\ninvestigating their various properties. Furthermore, the study analyzes the\nrelationships between extropy-based divergence with dynamic relative extropy\nand other extropy measures. A nonparametric estimator for relative extropy is\ndeveloped, and its performance is assessed through numerical simulation\nstudies. The practical applicability of the relative extropy is demonstrated\nthrough some real-life data sets.",
        "We show that if $G$ is a topological graph, and $f$ is continuous map, then\nthe induced map $\\tilde{f}$ acting on the hyperspace $C(G)$ of all connected\nsubsets of $G$ by natural formula $\\tilde{f}(C)=f(C)$ carries the same entropy\nas $f$.\n  This is well known that it does not hold on the larger hyperspace of all\ncompact subsets. Also negative examples were given for the hyperspace $C(X)$ on\nsome continua $X$, including dendrites.\n  Our work extends previous positive results obtained first for much simpler\ncase of compact interval by completely different tools.",
        "The Higgs boson discovery at the Large Hadron Collider (LHC) at CERN\nconfirmed the existence of the last missing particle of the Standard Model\n(SM). The existence of new fundamental constituents of matter beyond the SM is\nof great importance for our understanding of Nature. In this context, indirect\n(non-resonant) indications for new scalar bosons were found in the data from\nthe first run of the LHC, taken between 2010 and 2012 at CERN: an excess in the\ninvariant mass of muon-electron pairs, consistent with a new Higgs boson ($S$)\nwith a mass of $150\\pm5$ GeV. Other processes with multiple leptons in the\nfinal state, moderate missing energy, and possibly (bottom quark) jets exhibit\ndeviations from the SM predictions. These anomalies can be explained within a\nsimplified model in which a new heavy Higgs boson $H$ decays into two lighter\nHiggses $S$. This lighter Higgs $S$ subsequently decays to $W$ bosons, bottom\nquarks and has also an invisible decay mode.\n  Here, we demonstrate that using this model we can identify narrow excesses in\ndi-photon and $Z$-photon spectra around 152 GeV. By incorporating the latest\nmeasurements of di-photons in association with leptons, we obtain a combined\nglobal significance of $5.4\\sigma$. This represents the highest significance\never reported for an excess consistent with a narrow resonance beyond the SM\n(BSM) in high-energy proton-proton collision data at the LHC. Such findings\nhave the potential to usher in a new era in particle physics - the BSM epoch -\noffering crucial insights into unresolved puzzles of nature.",
        "We uncover the emergence of multiple topological Anderson insulators (TAIs)\nin a 1D spin-orbit coupled (SOC) chain driven by Fibonacci modulation,\ntransforming a trivial band structure into a cascade of topologically\nnontrivial phases. This intriguing phenomenon is marked by the appearance of\nzero-energy modes and transitions in the $\\mathcal{Z}_2$ topological quantum\nnumber. Strikingly, as the SOC amplitude decreases, the number of TAI phases\ngrows, a behavior intricately linked to the fractal structure of the energy\nspectrum induced by Fibonacci modulation. Unlike conventional TAI phases, which\nexhibit fully localized eigenstates, the wave functions in the\nFibonacci-modulated TAI phases exhibit multifractal behavior. Furthermore, this\nmodel can be experimentally realized in a Bose-Einstein condensate along the\nmomentum lattice, where its topological transitions and multifractal properties\ncan be probed through quench dynamics. Our findings open new avenues for\nexploring exotic disorder-induced topological phases and their intricate\nmultifractal nature.",
        "We show that the sheared graphene bilayers can be tuned to have flat\nlow-energy bands for sufficiently large size of their moir\\'e supercell. In\nthis regime, the interacting system becomes prone to develop broken-symmetry\nphases, with valley symmetry breaking as the dominant pattern. The strong\nsignal of symmetry breaking favors the onset of a pairing instability in which\nthe electrons with opposite spin projection in the Cooper pairs live in\ndifferent valleys. The Fermi lines become distorted due to the repulsive\nCoulomb interaction, which makes the screening highly anisotropic, leading\neasily to attraction in some of the interaction channels. We also show that the\nsheared graphene bilayers offer the possibility to realize the combined\nbreakdown of parity and valley symmetry, making them very suitable to study the\ninterplay between correlations and topology in a two-dimensional electron\nsystem.",
        "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases.",
        "We consider a generalization of a functional equation that models the\nlearning process in various animal species. The equation can be considered\nnonlocal, as it is built with a convex combination of the unknown function\nevaluated at mixed arguments. This makes the equation contain two terms with\nvanishing delays. We prove the existence and uniqueness of the solution in the\nH\\\"older space which is a natural function space to consider. In the second\npart of the paper, we devise an efficient numerical collocation method used to\nfind an approximation to the main problem. We prove the convergence of the\nscheme and, in passing, several properties of the linear interpolation operator\nacting on the H\\\"older space. Numerical simulations verify that the order of\nconvergence of the method (measured in the supremum norm) is equal to the order\nof H\\\"older continuity.",
        "The different large $N$ limits of supersymmetric quantum field theories in\nthree, four, and five dimensions are reviewed. We distinguish between the\nplanar limit of SQCD theories, the M-theory limit suited in three and five\ndimensions, and the long quiver limit. The method to solve exactly the sphere\npartition functions in each type of limit is spelled out in a pedagogical way.\nAfter a comprehensive general treatment of the saddle point approximation in\nthe large $N$ limit, we present an extensive list of examples and detail the\ncalculations. The scope of this overview is to provide an entry-level,\ncomputation-oriented understanding of the techniques featured in the field\ntheory side of the AdS\/CFT correspondence.",
        "Let $\\gf_{p^n}$ denote the finite field containing $p^n$ elements, where $n$\nis a positive integer and $p$ is a prime. The function\n$f_u(x)=x^{\\frac{p^n+3}{2}}+ux^2$ over $\\gf_{p^n}[x]$ with\n$u\\in\\gf_{p^n}\\setminus\\{0,\\pm1\\}$ was recently studied by Budaghyan and Pal in\n\\cite{Budaghyan2024ArithmetizationorientedAP}, whose differential uniformity is\nat most $5$ when $p^n\\equiv3~(mod~4)$. In this paper, we study the differential\nuniformity and the differential spectrum of $f_u$ for $u=\\pm1$. We first give\nsome properties of the differential spectrum of any cryptographic function.\nMoreover, by solving some systems of equations over finite fields, we express\nthe differential spectrum of $f_{\\pm1}$ in terms of the quadratic character\nsums.",
        "The \"Money Exchange Model\" is a type of agent-based simulation model used to\nstudy how wealth distribution and inequality evolve through monetary exchanges\nbetween individuals. The primary focus of this model is to identify the\nlimiting wealth distributions that emerge at the macroscopic level, given the\nmicroscopic rules governing the exchanges among agents. In this paper, we\nformulate generalized versions of the immediate exchange model and the uniform\nsaving model both of which are types of money exchange models, as discrete-time\ninteracting particle systems and characterize their stationary distributions.\nFurthermore, we prove that under appropriate scaling, the asymptotic wealth\ndistribution converges to a Gamma distribution or an exponential distribution\nfor both models. The limiting distribution depends on the weight function that\naffects the probability distribution of the number of coins exchanged by each\nagent. In particular, our results provide a mathematically rigorous formulation\nand generalization of the assertions previously predicted in studies based on\nnumerical simulations and heuristic arguments.",
        "We derive some identities and relations and extremal problems and\nminimization and Fourier development involving of integral Legendre\npolynomials.",
        "We observe for the first time in silico the transition to a linear regime in\nthe primary damage production in tungsten. As the critical plasma-facing\nmaterial in fusion reactors, radiation damage in tungsten has been studied\nextensively in experiments and simulations. Irradiation experiments routinely\nproduce recoils in the MeV range while full atomistic modelling has been\nlimited to a few hundred keV. Here we bridge these scales with extremely\nlarge-scale and accurate machine-learning-driven molecular dynamics simulations\nwith recoil energies up to 2 MeV in systems up to one billion atoms. We reveal\nfour regimes of primary damage as a function of damage energy, with a\ntransition to a high-energy regime that deviates from all previous models.\nCuriously, the start of the high-energy regime coincides with the highest\npossible recoil energy to tungsten atoms from fusion-emitted neutrons (300\nkeV).",
        "Various topological phases, including topological insulators, topological\nsemimetals, and topological superconductors, along with the controllable\ntopological phase transitions, have attracted considerable attention due to\ntheir promising applications in spintronics and quantum computing. In this\nwork, we propose two distinct methods for manipulating topological phase\ntransitions in magnetic materials. First, by varying the strength of electron\ncorrelation effects, we induce a series of topological state transitions within\nthe EuM$_2$X$_2$ (M = Zn, Cd; X = P, As, Sb) family of Zintl materials,\nincluding magnetic topological crystalline insulators (TCIs) and magnetic Dirac\nsemimetals. Our findings indicate that strong electron correlation effects tend\nto influence the emergence of topological phases. Second, by reducing the\nelectronegativity of the pnictogen X (from P to As and Sb), we observe a\nsimilar transition from trivial insulator to magnetic Dirac semimetal or\nmagnetic TCI. This suggests that weaker electronegativity favors the emergence\nof topological phases. Furthermore, we establish a Heisenberg model to describe\nthe magnetic interactions of the EuM$_2$X$_2$ system, based on which we perform\nMonte Carlo simulations of specific heat and magnetic susceptibility, yielding\nN\\'eel temperatures that perfectly match experimental data. This suggests that\nthe local magnetic moment framework provides an accurate description of the\nmagnetization behavior in this family of materials. This work provides the\npotential for the experimental manipulation of topological phase transitions\nand their possible applications, while also enhancing the understanding of the\nmagnetic interactions within the EuM$_2$X$_2$ system and offering a theoretical\nfoundation for future applications in magnetism.",
        "A $\\Gamma$\\emph{-distance magic labeling} of a graph $G = (V, E)$ with $|V| =\nn$ is a bijection $\\ell$ from $V$ to an Abelian group $\\Gamma$ of order $n$,\nfor which there exists $\\mu \\in \\Gamma$, such that the weight $w(x) =\\sum_{y\\in\nN(x)}\\ell(y)$ of every vertex $x \\in V$ is equal to $\\mu$. In this case, the\nelement $\\mu$ is called the \\emph{magic constant of} $G$. A graph $G$ is called\na \\emph{group distance magic} if there exists a $\\Gamma$-distance magic\nlabeling of $G$ for every Abelian group $\\Gamma$ of order $n$.\n  In this paper, we focused on cubic $\\Gamma$-distance magic graphs as well as\nsome properties of such graphs.",
        "Let g be a complex simple Lie algebra and Uq(Lg) its quantum loop algebra,\nwhere q is not a root of unity. We give an explicit formula for the quantum\nWeyl group action of the coroot lattice Q of g on finite-dimensional\nrepresentations of Uq(Lg) in terms of its commuting generators. The answer is\nexpressed in terms of the Chari-Pressley series, whose evaluation on highest\nweight vectors gives rise to Drinfeld polynomials. It hinges on a strong\nrationality result for that series, which is derived in the present paper. As\nan application, we identify the action of Q on the equivariant K-theory of\nNakajima quiver varieties with that of explicitly given determinant line\nbundles.",
        "We present subarcsecond-resolution ALMA mosaics of the Orion Bar PDR in [CI]\n609um, C2H (4-3), and C18O (3-2) emission lines complemented by JWST images of\nH2 and aromatic infrared band (AIB) emission. The rim of the Bar shows very\ncorrugated structures made of small-scale H2 dissociation fronts (DFs). The\n[CI] 609 um emission peaks very close (~0.002 pc) to the main H2-emitting DFs,\nsuggesting the presence of gas density gradients. These DFs are also bright and\nremarkably similar in C2H emission, which traces \"hydrocarbon radical peaks\"\ncharacterized by very high C2H abundances, reaching up to several x10^-7. The\nhigh abundance of C2H and of related hydrocarbon radicals, such as CH3, CH2,\nand CH, can be attributed to gas-phase reactions driven by elevated\ntemperatures, the presence of C+ and C, and the reactivity of FUV-pumped H2.\nThe hydrocarbon radical peaks roughly coincide with maxima of the 3.4\/3.3 um\nAIB intensity ratio, a proxy for the aliphatic-to-aromatic content of PAHs.\nThis implies that the conditions triggering the formation of simple\nhydrocarbons also favor the formation (and survival) of PAHs with aliphatic\nside groups, potentially via the contribution of bottom-up processes in which\nabundant hydrocarbon radicals react in situ with PAHs. Ahead of the DFs, in the\natomic PDR zone (where [H]>>[H2]), the AIB emission is the brightest, but small\nPAHs and carbonaceous grains undergo photo-processing due to the stronger FUV\nfield. Our detection of trace amounts of C2H in this zone may result from the\nphotoerosion of these species. This study provides a spatially resolved view of\nthe chemical stratification of key carbon carriers in a PDR. Overall, both\nbottom-up and top-down processes appear to link simple hydrocarbon molecules\nwith PAHs in molecular clouds; however, the exact chemical pathways and their\nrelative contributions remain to be quantified.",
        "This paper addresses the crucial question of solution uniqueness in\nstationary first-order Mean-Field Games (MFGs). Despite well-established\nexistence results, establishing uniqueness, particularly for weaker solutions\nin the sense of monotone operators, remains an open challenge. Building upon\nthe framework of monotonicity methods, we introduce a linearization method that\nenables us to prove a weak-strong uniqueness result for stationary MFG systems\non the d-dimensional torus. In particular, we give explicit conditions under\nwhich this uniqueness holds.",
        "We study a $d$-dimensional branching random walk (BRW) in an i.i.d. random\nenvironment on $\\mathbb{Z}^d$ in discrete time. A Bernoulli trap field is\nattached to $\\mathbb{Z}^d$, where each site, independently of the others, is a\ntrap with a fixed probability. The interaction between the BRW and the trap\nfield is given by the hard killing rule. Given a realization of the\nenvironment, over each time step, each particle first moves according to a\nsimple symmetric random walk to a nearest neighbor, and immediately afterwards,\nsplits into two particles if the new site is not a trap or is killed instantly\nif the new site is a trap. Conditional on the ultimate survival of the BRW, we\nprove a strong law of large numbers for the total mass of the process. Our\nresult is quenched, that is, it holds in almost every environment in which the\nstarting point of the BRW is inside the infinite connected component of\ntrap-free sites.",
        "We study the termination of minimal model programs for log canonical pairs in\nthe complex analytic setting. By using the termination, we prove a relation\nbetween the minimal model theory for projective log canonical pairs and that\nfor log canonical pairs in the complex analytic setting. The minimal model\nprograms for algebraic stacks and analytic stacks are also discussed.",
        "Spall failure, a complex failure mechanism driven by tensile stress wave\ninteractions, has been extensively studied in single-crystal FCC metals,\nrevealing a precursor stage involving dislocation emission along closed-packed\ndirections. Here we investigate the photo-induced spall failure of Ni\nbicrystals under a two-pulse laser configuration, exploring various\nmisorientation angles through two-temperature molecular dynamics (MD)\nsimulations including electronic effects to simulate light-matter interaction.\nOur findings demonstrate that light-matter interactions can induce spall\nfailure at the sample center, similar to conventional plate-impact methods,\nwhen two laser-pulses are applied to the front and back surfaces of the sample.\nThe study reveals the significant influence of misorientation angles on\ndislocation activity and spall behavior, where grain boundaries (GBs) play\npivotal roles, either promoting or impeding dislocation interactions.\nFurthermore, our work highlights the potential for enhancing spall resistance\nby tailoring materials through misorientation angle variation."
      ]
    }
  },
  {
    "id":2412.11399,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Forecasting the inevitable: A review on the impacts of climate change on renewable energy resources",
    "start_abstract":"Understanding the relationship and quantifying impacts of climate change on energy production is key to meeting our objectives achieving a sustainable future. Here we review current state art methodologies forecast future climate, potential changes in renewable main findings regarding role renewables decarbonisation supply. Most studies used model power equations estimate output. The largest variation estimated was for long-term scenarios, with non-significant variations reported short-term. highest variability found wind followed by hydro, both long-term, overall low solar any period. Additionally, efforts point investments as one pillars reducing fossil fuel dependency. Current knowledge gaps about uncertainty modelling results combined effects resources. Future should focus increasing resolution models improving input data, well assess entire electricity system not concentrate single source, which will aid defining strategies.",
    "start_categories":[
      "Physical Geography"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "High-Resolution Image Synthesis with Latent Diffusion Models"
      ],
      "abstract":[
        "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on data and beyond. Additionally, their formulation allows for guiding mechanism to control generation without retraining. However, since these typically operate directly in pixel space, optimization powerful DMs often consumes hundreds GPU days inference is expensive due evaluations. To enable DM training limited computational resources while retaining quality flexibility, we apply them latent space pretrained autoencoders. In contrast previous work, such representation first time reach near-optimal point between complexity reduction detail preservation, greatly boosting visual fidelity. introducing cross-attention layers model architecture, turn flexible generators general conditioning inputs as text or bounding boxes high-resolution becomes possible convolutional manner. Our (LDMs) new state art scores inpainting class-conditional highly competitive performance various tasks, including unconditional generation, text-to-image synthesis, super-resolution, significantly reducing requirements compared pixel-based DMs."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "METAMON: Finding Inconsistencies between Program Documentation and\n  Behavior using Metamorphic LLM Queries",
        "SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language\n  Models Tackling Knowledge-based Reasoning Tasks",
        "A Population Synthesis Study on the Formation of Cold Jupiters from\n  Truncated Planetesimal Disks",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading",
        "BASIC: Bipartite Assisted Spectral-clustering for Identifying\n  Communities in Large-scale Networks",
        "Fractional differential equations of a reaction-diffusion SIR model\n  involving the Caputo-fractional time-derivative and a nonlinear diffusion\n  operator",
        "The best two-term underapproximation using numbers from Fibonacci-type\n  sequences",
        "Deep Lossless Image Compression via Masked Sampling and Coarse-to-Fine\n  Auto-Regression",
        "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face\n  Obfuscation Methods",
        "Integrated Sensing, Communication, and Powering (ISCAP) for IoT: A Joint\n  Beamforming Design",
        "Optimizing Sequence Alignment with Scored NFAs",
        "Observation of the charged-particle multiplicity dependence of\n  $\\sigma_{\\psi(2S)}\/\\sigma_{\\text{J}\/\\psi}$ in pPb collisions at 8.16 TeV",
        "Design Optimization of Musculoskeletal Humanoids with Maximization of\n  Redundancy to Compensate for Muscle Rupture",
        "Evidence for Variable Accretion onto PDS 70 c and Implications for\n  Protoplanet Detections",
        "Athermal creep deformation of ultrastable amorphous solids",
        "BiRating -- Iterative averaging on a bipartite graph of Beat Saber\n  scores, player skills, and map difficulties",
        "Gravitational form factors in the perturbative limit",
        "Ladder Operator Block-Encoding",
        "Integrally Hilbertian rings and the polynomial Schinzel hypothesis",
        "Stackelberg Game Based Performance Optimization in Digital Twin Assisted\n  Federated Learning over NOMA Networks",
        "Why a Bose-Einstein condensate cannot exist in a system of interacting\n  bosons at ultrahigh temperatures",
        "Frequency domain identification for multivariable motion control\n  systems: Applied to a prototype wafer stage",
        "Strict Erd\\H{o}s-Ko-Rado theorems for simplicial complexes",
        "Exploring Large Language Models (LLMs) through interactive Python\n  activities",
        "Multilingual Language Model Pretraining using Machine-translated Data",
        "Langevin Multiplicative Weights Update with Applications in Polynomial\n  Portfolio Management",
        "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
        "Strong positive recurrence and exponential mixing for diffeomorphisms"
      ],
      "abstract":[
        "Code documentation can, if written precisely, help developers better\nunderstand the code they accompany. However, unlike code, code documentation\ncannot be automatically verified via execution, potentially leading to\ninconsistencies between documentation and the actual behavior. While such\ninconsistencies can be harmful for the developer's understanding of the code,\nchecking and finding them remains a costly task due to the involvement of human\nengineers. This paper proposes METAMON, which uses an existing search-based\ntest generation technique to capture the current program behavior in the form\nof test cases, and subsequently uses LLM-based code reasoning to identify the\ngenerated regression test oracles that are not consistent with the program\nspecifications in the documentation. METAMON is supported in this task by\nmetamorphic testing and self-consistency. An empirical evaluation against 9,482\npairs of code documentation and code snippets, generated using five open-source\nprojects from Defects4J v2.0.1, shows that METAMON can classify the\ncode-and-documentation inconsistencies with a precision of 0.72 and a recall of\n0.48.",
        "Deductive reasoning is a crucial logical capability that assists us in\nsolving complex problems based on existing knowledge. Although augmented by\nChain-of-Thought prompts, Large Language Models (LLMs) might not follow the\ncorrect reasoning paths. Enhancing the deductive reasoning abilities of LLMs,\nand leveraging their extensive built-in knowledge for various reasoning tasks,\nremains an open question. Attempting to mimic the human deductive reasoning\nparadigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought\n(SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle\ncomplex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the\nquestion and then uses the interpretation and the original question to propose\na suitable major premise. It proceeds by generating and answering minor premise\nquestions in two stages to match the minor premises. Finally, it guides LLMs to\nuse the previously generated major and minor premises to perform syllogistic\ndeductive reasoning to derive the answer to the original question. Extensive\nand thorough experiments on knowledge-based reasoning tasks have demonstrated\nthe effectiveness and advantages of our SR-FoT.",
        "The occurrence rate of giant planets increases with orbital period and turns\nover at a location that roughly corresponds to the snow line of solar-type\nstars. Further, the density distribution of cold Jupiters (CJs) on the\nsemi-major axis - mass diagram shows a relatively steep inner boundary, shaping\nthe desert of warm Jupiters. The eccentricities of CJs show a broad\ndistribution with a decreasing number density towards the larger end. Previous\nplanet formation models fail to reproduce all these features at the same time.\nWe use a planet population synthesis (PPS) model with truncated initial\nplanetesimal distribution and compare the mass and orbital distribution of the\nsimulated planets with the observation. We show that the occurrence of CJs with\nrespect to the orbital period, the slope of the inner boundary of CJs on the\nsemi-major axis - mass diagram, and the eccentricity distribution of CJs agree\nreasonably well with observation, if CJs form from truncated planetesimal disks\nof 10 au or wider with suppressed migration. While PPS simulations generally\noverestimate the fraction of giants with eccentricity below 0.2, $N$-body\nsimulations produce a more consistent eccentricity distribution with\nobservation. While the fraction of high-eccentricity planets can be increased\nby widening the planetesimal disk or reducing the migration speed, a deficit of\ngiants with eccentricity between 0.2-0.4 exists regardless of the choices of\nparameters. Our results indicate that CJs are more likely born in truncated\ndisks near the snow line than in classical uniform disks.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs.",
        "Community detection, which focuses on recovering the group structure within\nnetworks, is a crucial and fundamental task in network analysis. However, the\ndetection process can be quite challenging and unstable when community signals\nare weak. Motivated by a newly collected large-scale academic network dataset\nfrom the Web of Science, which includes multi-layer network information, we\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\nCommunities (BASIC), which incorporates the bipartite network information into\nthe community structure learning of the primary network. The accuracy and\nstability enhancement of BASIC is validated theoretically on the basis of the\ndegree-corrected stochastic block model framework, as well as numerically\nthrough extensive simulation studies. We rigorously study the convergence rate\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\nupper error bound than that based on the primary network information alone. We\nutilize the proposed BASIC method to analyze the newly collected large-scale\nacademic network dataset from statistical papers. During the author\ncollaboration network structure learning, we incorporate the bipartite network\ninformation from author-paper, author-institution, and author-region\nrelationships. From both statistical and interpretative perspectives, these\nbipartite networks greatly aid in identifying communities within the primary\ncollaboration network.",
        "The main aim of this study is to analyze a fractional parabolic SIR epidemic\nmodel of a reaction-diffusion, by using the nonlocal Caputo fractional\ntime-fractional derivative and employing the $p$-Laplacian operator. The\nimmunity is imposed through the vaccination program, which is regarded as a\ncontrol variable. Finding the optimal control pair that reduces the number of\nsick people, the associated vaccination, and treatment expenses across a\nconstrained time and space is our main study. The existence and uniqueness of\nthe nonnegative solution for the spatiotemporal SIR model are established. It\nis also demonstrated that an optimal control exists. In addition, we obtain a\ndescription of the optimal control in terms of state and adjoint functions.\nThen, the optimality system is resolved by a discrete iterative scheme that\nconverges after an appropriate test, similar to the forward-backward sweep\nmethod. Finally, numerical approximations are given to show the effectiveness\nof the proposed control program, which provides meaningful results using\ndifferent values of the fractional order and $p$, respectively the order of the\nCaputo derivative and the $p$-Laplacian operators.",
        "This paper studies the greedy two-term underapproximation of $\\theta\\in\n(0,1]$ using reciprocals of numbers from a Fibonacci-type sequence\n$(c_n)_{n=1}^\\infty$. We find the set of $\\theta$ whose greedy two-term\nunderapproximation is the best among all two-term underapproximations using\n$1\/c_n$'s. We then derive a neat description of the set when\n$(c_n)_{n=1}^\\infty$ is the Fibonacci sequence or the Lucas sequence.",
        "Learning-based lossless image compression employs pixel-based or\nsubimage-based auto-regression for probability estimation, which achieves\ndesirable performances. However, the existing works only consider context\ndependencies in one direction, namely, those symbols that appear before the\ncurrent symbol in raster order. We believe that the dependencies between the\ncurrent and future symbols should be further considered. In this work, we\npropose a deep lossless image compression via masked sampling and\ncoarse-to-fine auto-regression. It combines lossy reconstruction and\nprogressive residual compression, which fuses contexts from various directions\nand is more consistent with human perception. Specifically,\n  the residuals are decomposed via $T$ iterative masked sampling, and each\nsampling consists of three steps: 1) probability estimation, 2) mask\ncomputation, and 3) arithmetic coding. The iterative process progressively\nrefines our prediction and gradually presents a real image. Extensive\nexperimental results show that compared with the existing traditional and\nlearned lossless compression, our method achieves comparable compression\nperformance on extensive datasets with competitive coding speed and more\nflexibility.",
        "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
        "This paper studies Integrated Sensing, Communication, and Powering (ISCAP) as\na novel framework designed to enhance Internet of Things (IoT) applications\nwithin sixth-generation wireless networks. In these applications, in addition\nto IoT devices requiring an energy supply and receiving information or control\ndata to perform their tasks, the base station serving them must sense the\ndevices and their environment to localize them, thereby improving data\ntransmission and enabling simultaneous power delivery. In our multi-node ISCAP\nIoT system, we optimize base station beamforming alongside the receiver's\npower-splitting factor to maximize energy harvesting while adhering to strict\ncommunication and sensing constraints. To effectively tackle this non-convex\noptimization problem, we decompose it into three manageable subproblems and\nemploy several techniques such as semidefinite relaxation and Rayleigh quotient\nmethods to find an efficient solution. Simulation results demonstrate the\neffectiveness of the proposed design, highlighting performance trade-offs among\nsensing accuracy, communication reliability, and power transfer efficiency.",
        "The rapid increase in symbolic data has underscored the significance of\npattern matching and regular expression processing. While nondeterministic\nfinite automata (NFA) are commonly used for these tasks, they are limited to\ndetecting matches without determining the optimal one. This research expands on\nthe NAPOLY pattern-matching accelerator by introducing NAPOLY+, which adds\nregisters to each processing element to store variables like scores, weights,\nor edge costs. This enhancement allows NAPOLY+ to identify the highest score\ncorresponding to the best match in sequence alignment tasks through the\nnew-added arithmetic unit in each processor element. The design was evaluated\nagainst the original NAPOLY, with results showing that NAPOLY+ offers superior\nfunctionality and improved performance in identifying the best match. The\ndesign was implemented and tested on zynq102 and zynq104 FPGA devices, with\nperformance metrics compared across array sizes from 1K to 64K processing\nelements. The results showed that memory usage increased proportionally with\narray size with Fmax decreasing as the array size grew on both platforms. The\nreported findings focus specifically on the core array, excluding the impact of\nbuffers and DRAMs.",
        "Bound states of charm and anticharm quarks, known as charmonia, have a rich\nspectroscopic structure that can be used to probe the dynamics of hadron\nproduction in high-energy hadron collisions. Here, the cross section ratio of\nexcited ($\\psi$(2S)) and ground state (J\/$\\psi$) vector mesons is measured as a\nfunction of the charged-particle multiplicity in proton-lead (pPb) collisions\nat a center-of-mass (CM) energy per nucleon pair of 8.16 TeV. The data\ncorresponding to an integrated luminosity of 175 nb$^{-1}$ were collected using\nthe CMS detector. The ratio is measured separately for prompt and nonprompt\ncharmonia in the transverse momentum range 6.5 $\\lt$ $p_\\text{T}$ $\\lt$ 30 GeV\nand in four rapidity ranges spanning $-$2.865 $\\lt$ $y_\\text{CM}$ $\\lt$ 1.935.\nFor the first time, a statistically significant multiplicity dependence of the\nprompt cross section ratio is observed in proton-nucleus collisions. There is\nno clear rapidity dependence in the ratio. The prompt measurements are compared\nwith a theoretical model which includes interactions with nearby particles\nduring the evolution of the system. These results provide additional\nconstraints on hadronization models of heavy quarks in nuclear collisions.",
        "Musculoskeletal humanoids have various biomimetic advantages, and the\nredundant muscle arrangement allowing for variable stiffness control is one of\nthe most important. In this study, we focus on one feature of the redundancy,\nwhich enables the humanoid to keep moving even if one of its muscles breaks, an\nadvantage that has not been dealt with in many studies. In order to make the\nmost of this advantage, the design of muscle arrangement is optimized by\nconsidering the maximization of minimum available torque that can be exerted\nwhen one muscle breaks. This method is applied to the elbow of a\nmusculoskeletal humanoid Musashi with simulations, the design policy is\nextracted from the optimization results, and its effectiveness is confirmed\nwith the actual robot.",
        "Understanding the processes of planet formation and accretion in young\nsystems is essential to unraveling the initial conditions of planetary systems.\nThe PDS 70 system, which hosts two directly imaged protoplanets, provides a\nunique laboratory for studying these phenomena, particularly through H-alpha\nemission a commonly used accretion tracer. We present multi-epoch observations\nand examine the variability in accretion signatures within this system,\nfocusing on PDS 70 b and c. Using Hubble Space Telescope narrowband H-alpha\nimaging from 2020 and 2024, we achieve high signal-to-noise ratio detections of\nthese planets and reveal significant changes in H-alpha flux. For PDS 70 c, the\nH-alpha flux more than doubled between 2020 and 2024. The trend is consistent\nwith the one identified in recently published MagAO-X data, further confirming\nthat PDS 70 c has become significantly brighter in H between 2023 March and\n2024 May. The observed variability suggests dynamic accretion processes,\npossibly modulated by circumplanetary disk properties or transient accretion\nbursts. High-amplitude variability in PDS 70 c motivates simultaneous\nmonitoring of multiple accretion tracers to probe the mechanisms of mass growth\nof gas giant planets. We quantify the impact of variability on the\ndetectability of protoplanets in imaging surveys and emphasize the need for\ncontinued and regular monitoring to accurately assess the occurrence and\ncharacteristics of young, forming planets.",
        "We numerically investigate the athermal creep deformation of amorphous\nmaterials having a wide range of stability. The imposed shear stress serves as\nthe control parameter, allowing us to examine the time-dependent transient\nresponse through both the macroscopic strain and microscopic observables. Least\nstable samples exhibit monotonicity in the transient strain rate versus time,\nwhile more stable samples display a pronounced non-monotonic S-shaped curve,\ncorresponding to failure by sharp shear band formation. We identify a diverging\ntimescale associated with the fluidization process and extract the\ncorresponding critical exponents. Our results are compared with predictions\nfrom existing scaling theories relevant to soft matter systems. The numerical\nfindings for stable, brittle-like materials represent a challenge for\ntheoretical descriptions. We monitor the microscopic initiation of shear bands\nduring creep responses. Our study encompasses creep deformation across a\nvariety of materials ranging from ductile soft matter to brittle metallic and\noxide glasses, all within the same numerical framework.",
        "Difficulty estimation of Beat Saber maps is an interesting data analysis\nproblem and valuable to the Beat Saber competitive scene. We present a simple\nalgorithm that iteratively averages player skill and map difficulty estimations\nin a bipartite graph of players and maps, connected by scores, using scores\nonly as input. This approach simultaneously estimates player skills and map\ndifficulties, exploiting each of them to improve the estimation of the other,\nexploitng the relation of multiple scores by different players on the same map,\nor on different maps by the same player. While we have been unable to prove or\ncharacterize theoretical convergence, the implementation exhibits convergent\nbehaviour to low estimation error in all instances, producing accurate results.\nAn informal qualitative evaluation involving experienced Beat Saber community\nmembers was carried out, comparing the difficulty estimations output by our\nalgorithm with their personal perspectives on the difficulties of different\nmaps. There was a significant alignment with player perceived perceptions of\ndifficulty and with other existing methods for estimating difficulty. Our\napproach showed significant improvement over existing methods in certain known\nproblematic maps that are not typically accurately estimated, but also produces\nproblematic estimations for certain families of maps where the assumptions on\nthe meaning of scores were inadequate (e.g. not enough scores, or scores over\noptimized by players). The algorithm has important limitations, related to data\nquality and meaningfulness, assumptions on the domain problem, and theoretical\nconvergence of the algorithm. Future work would significantly benefit from a\nbetter understanding of adequate ways to quantify map difficulty in Beat Saber,\nincluding multidimensionality of skill and difficulty, and the systematic\nbiases present in score data.",
        "The generalized distribution amplitudes (GDAs) have been paid attention in\nrecent years because of their relation with the energy momentum tensor (EMT)\nform factors (FFs). The GDAs can be experimentally accessed through the study\nof amplitudes in $\\gamma^{\\ast} \\gamma \\to M_1 M_2$ and $\\gamma^{\\ast} \\to M_1\nM_2 \\gamma$, where $M_1M_2$ is a pseudoscalar meson pair such as $\\pi \\eta $\nand $\\eta \\eta^{\\prime}$. In this paper we calculate these amplitudes in the\nperturbative limit where the $M_1M_2$ GDAs are expressed in terms of meson\ndistribution amplitudes which have been constrained in the past experiments.\nOur explicit calculation verifies the existence of a new EMT FF which breaks\nthe conservation law of EMT for each quark flavor in its hadronic matrix\nelement. In addition, our result shows that the $M_1M_2$ GDAs are identical\nbetween $\\gamma^{\\ast} \\gamma \\to M_1 M_2$ and $\\gamma^{\\ast} \\to M_1 M_2\n\\gamma$, which confirms the universality of GDAs. In future, the GDAs and EMT\nFFs studied in this paper can be investigated at Belle II. Our study enhances\nthe accessibility to the $P$-wave GDAs in $\\gamma^{\\ast} \\gamma \\to M_1 M_2$\nand $\\gamma^{\\ast} \\to M_1 M_2 \\gamma$ and gives a promising way to search for\nexotic hybrid mesons in the future experiment.",
        "We describe and analyze LOBE (Ladder Operator Block-Encoding), a framework\nfor block-encoding second-quantized ladder operators that act upon fermionic\nand bosonic modes. We numerically benchmark these constructions using models\narising in quantum field theories including the quartic oscillator, and phi4\nand Yukawa Hamiltonians on the light front. These benchmarks show that LOBE\nproduces block-encodings with fewer non-Clifford operations, fewer\nblock-encoding ancillae and overall number of qubits, and lower rescaling\nfactors for various second-quantized operators as compared to block-encoding\nframeworks that expand the ladder operators in the Pauli basis. The LOBE\nconstructions also demonstrate favorable scaling with respect to key\nparameters, including the maximum occupation of bosonic modes, the total number\nof fermionic and bosonic modes, and the locality of the operators. LOBE is\nimplemented as an open source python package to enable further applications.",
        "We prove that all rings of integers of number fields are ``integrally\nHilbertian''. That is, i.e. they satisfy an ``integral'' version of the\nclassical Hilbert irreducibility property, to the effect that, under\nappropriate irreducibility assumptions on given multivariate polynomials with\ncoefficients in such a ring ${\\mathcal Z}$, one can specialize in the ring\n${\\mathcal Z}$ some of the variables in such a way that irreducibility over the\nring ${\\mathcal Z}$ is preserved for the specialized polynomials. We identify\nan intermediate type of ring, which we call ``Schinzel ring'', that is central\nin the study of this new Hilbert property. Dedekind domains and Unique\nFactorization Domains are shown to be Schinzel rings, leading to other examples\nof integrally Hilbertian rings. A main application is a polynomial version, for\nthe ring ${\\mathcal Z}[Y_1,\\ldots,Y_n]$, of the Schinzel Hypothesis about\nprimes in value sets of polynomials. Some consequences for the ring of integers\nitself are also deduced.",
        "Despite the advantage of preserving data privacy, federated learning (FL)\nstill suffers from the straggler issue due to the limited computing resources\nof distributed clients and the unreliable wireless communication environment.\nBy effectively imitating the distributed resources, digital twin (DT) shows\ngreat potential in alleviating this issue. In this paper, we leverage DT in the\nFL framework over non-orthogonal multiple access (NOMA) network to assist FL\ntraining process, considering malicious attacks on model updates from clients.\nA reputationbased client selection scheme is proposed, which accounts for\nclient heterogeneity in multiple aspects and effectively mitigates the risks of\npoisoning attacks in FL systems. To minimize the total latency and energy\nconsumption in the proposed system, we then formulate a Stackelberg game by\nconsidering clients and the server as the leader and the follower,\nrespectively. Specifically, the leader aims to minimize the energy consumption\nwhile the objective of the follower is to minimize the total latency during FL\ntraining. The Stackelberg equilibrium is achieved to obtain the optimal\nsolutions. We first derive the strategies for the followerlevel problem and\ninclude them in the leader-level problem which is then solved via problem\ndecomposition. Simulation results verify the superior performance of the\nproposed scheme.",
        "It is well known that a Bose-Einstein (BE) condensate of atoms exists in a\nsystem of interacting Bose atoms at $T\\lesssim T^{(i)}_{c}$, where\n$T^{(i)}_{c}$ is the BE condensation temperature of an ideal gas. It is also\ngenerally accepted that BE condensation is impossible at ``ultrahigh''\ntemperatures $T\\gg T^{(i)}_{c}$. While the latter property has been\ntheoretically proven for an ideal gas, no such proof exists for an interacting\nsystem, to our knowledge. In this paper, we propose an approximate mathematical\nproof for a finite, nonrelativistic, periodic system of $N$ spinless\ninteracting bosons. The key point is that, at $T\\gg T^{(i)}_{c}$, the main\ncontribution to the occupation number\n$N_{0}=\\frac{1}{Z}\\sum_{\\wp}e^{-E_{\\wp}\/k_{B}T}\\langle\n\\Psi_{\\wp}|\\hat{a}^{+}_{\\mathbf{0}}\\hat{a}_{\\mathbf{0}}|\\Psi_{\\wp}\\rangle$,\ncorresponding to atoms with zero momentum, originates from the states\ncontaining $N$ elementary quasiparticles. These states do not contain the BE\ncondensate of zero-momentum atoms, implying that an ultrahigh temperature\nshould ``blur'' such a condensate.",
        "Multivariable parametric models are essential for optimizing the performance\nof high-tech systems. The main objective of this paper is to develop an\nidentification strategy that provides accurate parametric models for complex\nmultivariable systems. To achieve this, an additive model structure is adopted,\noffering advantages over traditional black-box model structures when\nconsidering physical systems. The introduced method minimizes a weighted\nleast-squares criterion and uses an iterative linear regression algorithm to\nsolve the estimation problem, achieving local optimality upon convergence.\nExperimental validation is conducted on a prototype wafer-stage system,\nfeaturing a large number of spatially distributed actuators and sensors and\nexhibiting complex flexible dynamic behavior, to evaluate performance and\ndemonstrate the effectiveness of the proposed method.",
        "We show that if a simplicial complex is a near-cone of sufficiently high\ndepth, then the only maximum families of small pairwise intersecting faces are\nthose with a common intersection. Thus, near-cones of sufficiently high depth\nsatisfy the strict Erd\\H{o}s-Ko-Rado property conjectured by Holroyd and Talbot\nand by Borg. One consequence is a strict Erd\\H{o}s-Ko-Rado theorem for\nindependence complexes of chordal graphs with an isolated vertex. Under\nstronger shiftedness conditions, we prove a sharper stability theorem of\nHilton-Milner type, as well as two cross-intersecting theorems.",
        "This paper presents an approach to introduce physics students to the basic\nconcepts of Large Language Models (LLMs) using Python-based activities in\nGoogle Colab. The teaching strategy integrates active learning strategies and\ncombines theoretical ideas with practical, physics-related examples. Students\nengage with key technical concepts, such as word embeddings, through hands-on\nexploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot\nof attention in 2019 for its ability to generate coherent and plausible text\nfrom simple prompts.\n  The activities highlight how words acquire meaning and how LLMs predict\nsubsequent tokens by simulating simplified scenarios related to physics. By\nfocusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles\nunderlying modern LLMs, such as semantic representation and contextual\nprediction. Through interactive experimenting in Google Colab, students observe\nthe relationship between model parameters (such as temperature) in GPT-2 and\noutput behaviour, understand scaling laws relating data quantity to model\nperformance, and gain practical insights into the predictive capabilities of\nLLMs. This approach allows students to begin to understand how these systems\nwork by linking them to physics concepts - systems that will shape their\nacademic studies, professional careers and roles in society.",
        "High-resource languages such as English, enables the pretraining of\nhigh-quality large language models (LLMs). The same can not be said for most\nother languages as LLMs still underperform for non-English languages, likely\ndue to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated texts from a\nsingle high-quality source language can contribute significantly to the\npretraining quality of multilingual LLMs. We translate FineWeb-Edu, a\nhigh-quality English web dataset, into nine languages, resulting in a\n1.7-trillion-token dataset, which we call TransWebEdu and pretrain a\n1.3B-parameter model, TransWebLLM, from scratch on this dataset. Across nine\nnon-English reasoning tasks, we show that TransWebLLM matches or outperforms\nstate-of-the-art multilingual models trained using closed data, such as\nLlama3.2, Qwen2.5, and Gemma, despite using an order of magnitude less data. We\ndemonstrate that adding less than 5% of TransWebEdu as domain-specific\npretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian,\nSwahili, and Welsh understanding and commonsense reasoning tasks. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nOpen Source Initiative-approved licenses.",
        "We consider nonconvex optimization problem over simplex, and more generally,\na product of simplices. We provide an algorithm, Langevin Multiplicative\nWeights Update (LMWU) for solving global optimization problems by adding a\nnoise scaling with the non-Euclidean geometry in the simplex. Non-convex\noptimization has been extensively studied by machine learning community due to\nits application in various scenarios such as neural network approximation and\nfinding Nash equilibrium. Despite recent progresses on provable guarantee of\nescaping and avoiding saddle point (convergence to local minima) and global\nconvergence of Langevin gradient based method without constraints, the global\noptimization with constraints is less studied. We show that LMWU algorithm is\nprovably convergent to interior global minima with a non-asymptotic convergence\nanalysis. We verify the efficiency of the proposed algorithm in real data set\nfrom polynomial portfolio management, where optimization of a highly non-linear\nobjective function plays a crucial role.",
        "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps:\/\/github.com\/Gen-Verse\/WideRange4D",
        "We introduce the strong positive recurrence (SPR) property for\ndiffeomorphisms on closed manifolds with arbitrary dimension, and show that it\nhas many consequences and holds in many cases. SPR diffeomorphisms can be coded\nby countable state Markov shifts whose transition matrices act with a spectral\ngap on a large Banach space, and this implies exponential decay of\ncorrelations, almost sure invariance principle, large deviations, among other\nproperties of the ergodic measures of maximal entropy. Any $C^\\infty$ smooth\nsurface diffeomorphism with positive entropy is SPR, and there are many other\nexamples with lesser regularity, or in higher dimension."
      ]
    }
  },
  {
    "id":2412.11399,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"High-Resolution Image Synthesis with Latent Diffusion Models",
    "start_abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on data and beyond. Additionally, their formulation allows for guiding mechanism to control generation without retraining. However, since these typically operate directly in pixel space, optimization powerful DMs often consumes hundreds GPU days inference is expensive due evaluations. To enable DM training limited computational resources while retaining quality flexibility, we apply them latent space pretrained autoencoders. In contrast previous work, such representation first time reach near-optimal point between complexity reduction detail preservation, greatly boosting visual fidelity. introducing cross-attention layers model architecture, turn flexible generators general conditioning inputs as text or bounding boxes high-resolution becomes possible convolutional manner. Our (LDMs) new state art scores inpainting class-conditional highly competitive performance various tasks, including unconditional generation, text-to-image synthesis, super-resolution, significantly reducing requirements compared pixel-based DMs.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Forecasting the inevitable: A review on the impacts of climate change on renewable energy resources"
      ],
      "abstract":[
        "Understanding the relationship and quantifying impacts of climate change on energy production is key to meeting our objectives achieving a sustainable future. Here we review current state art methodologies forecast future climate, potential changes in renewable main findings regarding role renewables decarbonisation supply. Most studies used model power equations estimate output. The largest variation estimated was for long-term scenarios, with non-significant variations reported short-term. highest variability found wind followed by hydro, both long-term, overall low solar any period. Additionally, efforts point investments as one pillars reducing fossil fuel dependency. Current knowledge gaps about uncertainty modelling results combined effects resources. Future should focus increasing resolution models improving input data, well assess entire electricity system not concentrate single source, which will aid defining strategies."
      ],
      "categories":[
        "Physical Geography"
      ]
    },
    "list":{
      "title":[
        "Computing theta-dependent mass spectrum of the 2-flavor Schwinger model\n  in the Hamiltonian formalism",
        "Semi-Parametric Batched Global Multi-Armed Bandits with Covariates",
        "Polarization-induced Quantum Spin Hall Insulator and Topological Devices\n  in InAs Quantum Wells",
        "On the module of derivations of a line arrangement",
        "Quantum Entanglement Response to Step-like Gate Modulation",
        "Graph Counterfactual Explainable AI via Latent Space Traversal",
        "Galaxy populations of ProtoClusters in cosmological hydrodynamical\n  simulations",
        "Flipping operators and locally harmonic Maass forms",
        "Convergence of body-orders in linear atomic cluster expansions",
        "Thermal Stability of Skyrmion Tubes in Nanostructured Cuboids",
        "Direct derivation of anisotropic atomic displacement parameters from\n  molecular dynamics simulations demonstrated in thermoelectric materials",
        "Kontsevich graphs act on Nambu--Poisson brackets, IV. Resilience of the\n  graph calculus in the dimensional shift $d\\mapsto d+1$",
        "Optimal multi-time-scale estimates for diluted autocatalytic chemical\n  networks. (1) Introduction and $\\sigma^*$-dominant case",
        "Siamese Foundation Models for Crystal Structure Prediction",
        "A variational approach to the analysis of the continuous space-time FEM\n  for the wave equation",
        "Homoclinic orbits, Reeb chords and nice Birkhoff sections for Reeb flows\n  in 3D",
        "Line-of-sight shear in SLACS strong lenses",
        "Duality of Selmer groups of an abelian variety over a number field",
        "The Kaluza-Klein AdS Virasoro-Shapiro Amplitude near Flat Space",
        "Dynamics of Supersolid state: normal fluid, superfluid, and supersolid\n  velocities",
        "Self-assembly of Dipolar Crystals from Magnetic Colloids",
        "Combinatorial Calabi flow for ideal circle pattern",
        "Statistical Uncertainties of Limit Cycle Systems in Langevin Bath",
        "Wave Decay with Singular Damping",
        "Modulation of superconductivity across a Lifshitz transition in\n  alternating-angle twisted quadrilayer graphene",
        "Twisting $\\mathcal{O}$-operators by $(2,3)$-Cocycle of Hom-Lie-Yamaguti\n  Algebras with Representations",
        "An automated geometric space curve approach for designing dynamically\n  corrected gates",
        "State-Agnostic Approach to Certifying Electron-Photon Entanglement in\n  Electron Microscopy",
        "On the linear independence condition for the Bobkov-Tanaka first\n  eigenvalue of the double-phase operator"
      ],
      "abstract":[
        "We compute the $\\theta$-dependent mass spectrum of the 2-flavor Schwingr\nmodel using the tensor network (DMRG) in the Hamiltonian formalism. The pion\nand the sigma meson are identified as stable particles of the model for nonzero\n$\\theta$ whereas the eta meson becomes unstable. The meson masses are obtained\nfrom the one-point functions, using the meson operators defined by\ndiagonalizing the correlation matrix to deal with the operator mixing. We also\ncompute the dispersion relation directly by measuring the energy and momentum\nof the excited states, where the mesons are distinguished by the isospin\nquantum number. We confirmed that the meson masses computed by these methods\nagree with each other and are consistent with the calculation by the bosonized\nmodel. Our methods are free from the sign problem and show a significant\nimprovement in accuracy compared to the conventional Monte Carlo methods.\nFurthermore, at the critical point $\\theta = \\pi$, the mesons become almost\nmassless, and the one-point functions reproduce the expected CFT-like behavior.",
        "The multi-armed bandits (MAB) framework is a widely used approach for\nsequential decision-making, where a decision-maker selects an arm in each round\nwith the goal of maximizing long-term rewards. Moreover, in many practical\napplications, such as personalized medicine and recommendation systems,\nfeedback is provided in batches, contextual information is available at the\ntime of decision-making, and rewards from different arms are related rather\nthan independent. We propose a novel semi-parametric framework for batched\nbandits with covariates and a shared parameter across arms, leveraging the\nsingle-index regression (SIR) model to capture relationships between arm\nrewards while balancing interpretability and flexibility. Our algorithm,\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\nemploys a batched successive arm elimination strategy with a dynamic binning\nmechanism guided by the single-index direction. We consider two settings: one\nwhere a pilot direction is available and another where the direction is\nestimated from data, deriving theoretical regret bounds for both cases. When a\npilot direction is available with sufficient accuracy, our approach achieves\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\ncircumventing the curse of dimensionality. Extensive experiments on simulated\nand real-world datasets demonstrate the effectiveness of our algorithm compared\nto the nonparametric batched bandit method introduced by\n\\cite{jiang2024batched}.",
        "In this work, we predict the emergence of a quantum spin Hall insulator\n(QSHI) in conventional semiconductors, specifically InAs quantum wells, driven\nby a built-in polarization field. We propose QSHI InAs quantum wells as a\nplatform to engineer topological field effect devices. More precisely, we first\npresent a novel topological logic device that operates without a topological\nphase transition. Subsequently, we design a high-performance topological\ntransistor due to the presence of edge states. Our approach provides a\npotential framework for harnessing the unique features of QSHI in device\ndesign, paving the way for future topological devices.",
        "To each multiple point $p$ in a line arrangement $\\mathcal A$ in the complex\nprojective plane we associate a derivation $\\tilde D_p \\in D_0( \\mathcal A)$.\nWe show first that these derivations span a large subspace of $D_0(\\mathcal\nA)$. To each such derivation $\\tilde D_p \\in D_0(\\mathcal A)$ we associate a\npolynomial $g_p$ which seems to play a key role in the characterization of the\nfreeness of $\\mathcal A$, as well as in the study of the position of the\nmultiple points of $\\mathcal A$ with respect to unions of lines.",
        "We examine the influence of a step-like gate voltage on the entanglement\nformation of two interacting charge qubits, where charge is injected on demand\ninto the qubits. The gate voltage modulates the tunnel coupling between the\nqubits and two electronic reservoirs (leads), which supply the initial charges\nto the system. The qubits interact capacitively through Coulomb repulsion, and\nthe interplay between Coulomb interactions and hopping processes leads to the\nformation of entangled states. Our analysis focuses on how the physical\nparameters of the gate pulse affect the degree of entanglement. In pursuit of\nthis aim, we calculate fidelity, linear entropy, and negativity within the\nframework of density matrix formalism. Our analysis demonstrate how to optimize\nthe gate pulse to reach a ``sweet spot'' that maximizes entanglement, even in\nthe presence of additional dephasing sources. These results could contribute to\nthe future experimental realization of entanglement in interacting charge\nqubits.",
        "Explaining the predictions of a deep neural network is a nontrivial task, yet\nhigh-quality explanations for predictions are often a prerequisite for\npractitioners to trust these models. Counterfactual explanations aim to explain\npredictions by finding the ''nearest'' in-distribution alternative input whose\nprediction changes in a pre-specified way. However, it remains an open question\nhow to define this nearest alternative input, whose solution depends on both\nthe domain (e.g. images, graphs, tabular data, etc.) and the specific\napplication considered. For graphs, this problem is complicated i) by their\ndiscrete nature, as opposed to the continuous nature of state-of-the-art graph\nclassifiers; and ii) by the node permutation group acting on the graphs. We\npropose a method to generate counterfactual explanations for any differentiable\nblack-box graph classifier, utilizing a case-specific permutation equivariant\ngraph variational autoencoder. We generate counterfactual explanations in a\ncontinuous fashion by traversing the latent space of the autoencoder across the\nclassification boundary of the classifier, allowing for seamless integration of\ndiscrete graph structure and continuous graph attributes. We empirically\nvalidate the approach on three graph datasets, showing that our model is\nconsistently high-performing and more robust than the baselines.",
        "The study of protoclusters at cosmic noon is essential to understand the\nimpact on galaxies of the environment and of the transformational processes\noccurring in this epoch. This work tests the predictions of the DIANOGA\ncosmological hydrodynamical simulations of cluster progenitors at z=2.2,\ncomparing them with observations, and investigates the environmental effects on\ngalaxies by comparing protoclusters with an average volume of the Universe. We\nanalyze 14 protoclusters and a cosmological box of 49 cMpc\/h per side. We\ncompare predictions and observations of the galaxy properties, including colors\nof galaxies obtained with radiative transfer, to analyze UVJ diagrams. We\nshowed that the DIANOGA simulations produce a galaxy stellar mass function in\nbroad agreement with observations, with a higher fraction of high-mass galaxies\n($M_{\\ast}>10^{10} \\ M_{\\odot}$) in massive halos in protoclusters, compared to\nthe box. The same signal, with lower significance, is also observed in the\nwide-field protocluster structures, indicating an accelerated evolution of\ngalaxies before their infall into massive halos. Our simulations underestimate\nSFRs of galaxies both in protoclusters and in the box, compared to\nobservations, due to low gas reservoirs. We find a weak suppression of SFRs in\nprotocluster galaxies (~0.05 dex), compared to the box, increasing up to ~0.25\ndex in massive halos. The quenched galaxy fraction varies significantly across\ndifferent protocluster halos, consistent with observations. The simulations\nshow a strong dependence of quenched fractions on halo mass and an excess of\nquenched galaxies in the wide-field protocluster region, compared to the\ncosmological box. UVJ diagram analysis shows qualitative agreement with\nobserved color distributions of star-forming and quenched galaxies, except for\nfew massive galaxies with steeper reddening vectors than typically assumed in\nobservations.",
        "In the theory of integral weight harmonic Maass forms of manageable growth,\ntwo key differential operators, the Bol operator and the shadow operator, play\na fundamental role. Harmonic Maass forms of manageable growth canonically split\ninto two parts, and each operator controls one of these parts. A third\noperator, called the flipping operator, exchanges the role of these two parts.\nMaass--Poincar\\'e series (of parabolic type) form a convenient basis of\nnegative weight harmonic Maass forms of manageable growth, and flipping has the\neffect of negating an index. Recently, there has been much interest in locally\nharmonic Maass forms defined by the first author, Kane, and Kohnen. These are\nlifts of Poincar\\'e series of hyperbolic type, and are intimately related to\nthe Shimura and Shintani lifts. In this note, we prove that a similar property\nholds for the flipping operator applied to these Poincar\\'e series.",
        "We study the convergence of a linear atomic cluster expansion (ACE) potential\nwith respect to its basis functions, in terms of the effective two-body\ninteractions of elemental Carbon systems. We build ACE potentials with\ndescriptor sets truncated at body-orders $K=2,3,4$ trained on pure dimers, or\non large datasets of different diversities but without any dimers. Potentials\ntrained on a more diverse dataset fare better in validation and result in a\nnontrivial dimer curve, but still very far from the theoretical two-body\ninteraction calculated by DFT. Moreover, dimer curves between descriptor sets\nclipped at different $K$ do not seem to converge to a universal function for a\ngiven dataset. We conclude that machine learning potentials employing atomic\ncluster expansions optimize losses at low $K$ but fail to generalize and\nconverge properties described by two-body interactions.",
        "Magnetic skyrmions in bulk materials are typically regarded as\ntwo-dimensional structures. However, they also exhibit three-dimensional\nconfigurations, known as skyrmion tubes, which elongate and extend in-depth.\nUnderstanding the configurations and stabilization mechanism of skyrmion tubes\nis crucial for the development of advanced spintronic devices. However, the\ngeneration and annihilation of skyrmion tubes in confined geometries are still\nrarely reported. Here, we present direct imaging of skyrmion tubes in\nnanostructured cuboids of a chiral magnet FeGe using Lorentz transmission\nelectronic microscopy (TEM), while applying an in-plane magnetic field. It is\nobserved that skyrmion tubes stabilize in a narrow field-temperature region\nnear the Curie temperature (Tc). Through a field cooling process, metastable\nskyrmion tubes can exist in a larger region of the field-temperature diagram.\nCombining these experimental findings with micromagnetic simulations, we\nattribute these phenomena to energy differences and thermal fluctuations. Our\nresults could promote topological spintronic devices based on skyrmion tubes.",
        "Atomic displacement parameters (ADPs) are crystallographic information that\ndescribe the statistical distribution of atoms around an atom site. Direct\nderivation of anisotropic ADPs by atom from molecular dynamics (MD)\nsimulations, where the (co)valences of atom positions are taken over recordings\nat different time steps in a single MD simulation, was demonstrated on three\nthermoelectric materials, Ag8SnSe6, Na2In2Sn4, and BaCu1.14In0.86P2. Unlike the\nvery frequently used lattice dynamics approach, the MD approach can obtain ADPs\nin disordered crystals and at finite temperature, but not under conditions\nwhere atoms migrate in the crystal. ADPs from MD simulations would act as a\ntool complementing experimental efforts to understand the crystal structure\nincluding the distribution of atoms around atom sites.",
        "We examine whether the Kontsevich flows $\\dot{P}=Q^\\gamma_d(P)$ of\nNambu--Poisson structures $P$ on $\\mathbb{R}^d$ are Poisson coboundaries, for\n$\\gamma$ some suitable cocycle in the Kontsevich graph complex. That is, we\ninspect the existence of a vector field $\\vec{X}^\\gamma_d(P)$ such that\n$Q^\\gamma_d(P)=[[ P,\\vec{X}^\\gamma_d(P)]]$, where $[[\\cdot,\\cdot]]$ is the\nSchouten bracket of multivector fields (the generalised Lie bracket). To tackle\nthis class of problems in dimensions $d\\geq3$, we introduced a series of\nsimplications in paper II (arXiv:2409.12555); here, we present a series of\nresults regarding the down-up behaviour of solutions $\\vec{X}^\\gamma_d(P)$ and\nvanishing micro-graphs in the course of dimension shift $d\\mapsto d+1$.",
        "Autocatalytic chemical networks are dynamical systems whose linearization\naround zero has a positive Lyapunov exponent; this exponent gives the growth\nrate of the system in the diluted regime, i.e. for near-zero concentrations.\nThe generator of the dynamics in the kinetic limit is then a Perron-Frobenius\nmatrix, suggesting the use of Markov chain techniques to get long-time\nasymptotics. This series of works introduces a new, general procedure providing\nprecise quantitative information about such asymptotics, based on estimates for\nthe Lyapunov eigenvalue and eigenvector. The algorithm, inspired from Wilson's\nrenormalization group method in quantum field theory, is based on a downward\nrecursion on kinetic scales, starting from the fastest, and terminating with\nthe slowest rates. Estimates take on the form of simple rational functions of\nkinetic rates. They are accurate under a separation of scales hypothesis,\nloosely stating that kinetic rates span many orders of magnitude. We provide\nhere a brief general motivation and introduction to the method, present some\nsimple examples, and derive a number of preliminary results, in particular the\nestimation of Lyapunov data for a subclass of so-called $\\sigma^*$-dominant\ngraphs.",
        "Crystal Structure Prediction (CSP), which aims to generate stable crystal\nstructures from compositions, represents a critical pathway for discovering\nnovel materials. While structure prediction tasks in other domains, such as\nproteins, have seen remarkable progress, CSP remains a relatively underexplored\narea due to the more complex geometries inherent in crystal structures. In this\npaper, we propose Siamese foundation models specifically designed to address\nCSP. Our pretrain-finetune framework, named DAO, comprises two complementary\nfoundation models: DAO-G for structure generation and DAO-P for energy\nprediction. Experiments on CSP benchmarks (MP-20 and MPTS-52) demonstrate that\nour DAO-G significantly surpasses state-of-the-art (SOTA) methods across all\nmetrics. Extensive ablation studies further confirm that DAO-G excels in\ngenerating diverse polymorphic structures, and the dataset relaxation and\nenergy guidance provided by DAO-P are essential for enhancing DAO-G's\nperformance. When applied to three real-world superconductors\n($\\text{CsV}_3\\text{Sb}_5$, $ \\text{Zr}_{16}\\text{Rh}_8\\text{O}_4$ and\n$\\text{Zr}_{16}\\text{Pd}_8\\text{O}_4$) that are known to be challenging to\nanalyze, our foundation models achieve accurate critical temperature\npredictions and structure generations. For instance, on\n$\\text{CsV}_3\\text{Sb}_5$, DAO-G generates a structure close to the\nexperimental one with an RMSE of 0.0085; DAO-P predicts the $T_c$ value with\nhigh accuracy (2.26 K vs. the ground-truth value of 2.30 K). In contrast,\nconventional DFT calculators like Quantum Espresso only successfully derive the\nstructure of the first superconductor within an acceptable time, while the RMSE\nis nearly 8 times larger, and the computation speed is more than 1000 times\nslower. These compelling results collectively highlight the potential of our\napproach for advancing materials science research and development.",
        "We present a stability and convergence analysis of the space-time continuous\nfinite element method for the Hamiltonian formulation of the wave equation.\nMore precisely, we prove a continuous dependence of the discrete solution on\nthe data in a $C^0([0, T]; X)$-type energy norm, which does not require any\nrestriction on the meshsize or the time steps. Such stability estimates are\nthen used to derive a priori error estimates with quasi-optimal convergence\nrates, where a suitable treatment of possible nonhomogeneous Dirichlet boundary\nconditions is pivotal to avoid loss of accuracy. Moreover, based on the\nproperties of a postprocessed approximation, we derive a constant-free,\nreliable a posteriori error estimate in the $C^0([0, T]; L^2(\\Omega))$-norm for\nthe semidiscrete-in-time formulation. Several numerical experiments are\npresented to validate our theoretical findings.",
        "We prove that for a $C^\\infty$-generic contact form defining a given\nco-oriented contact structure on a closed $3$-manifold, every hyperbolic\nperiodic Reeb orbit admits a transverse homoclinic connection in each of the\nbranches of its stable and unstable manifolds. We exploit this result to prove\nthat for a $C^\\infty$-generic contact form defining a given co-oriented contact\nstructure, given any finite collection $\\Gamma$ of periodic Reeb orbits and any\nLegendrian link $L$, there exists a global surface of section (embedded\nBirkhoff section) for the Reeb flow that contains $\\Gamma$ in its boundary, and\nthat contains in its interior a Legendrian link that is Legendrian isotopic to\n$L$ by a $C^0$-small isotopy. Finally we prove that if the Reeb vector field\nadmits a $\\partial$-strong Birkhoff section then every Legendrian knot has\ninfinitely many geometrically distinct Reeb chords, except possibly when the\nambient manifold is a lens space or the sphere and the Reeb flow has exactly\ntwo periodic orbits. In particular, $C^\\infty$-generically on the contact form\nthere are infinitely many geometrically distinct Reeb chords for every\nLegendrian knot. In the case of geodesic flows, every Legendrian knot has\ninfinitely many disjoint chords, without any further assumptions.",
        "Inhomogeneities along the line of sight in strong gravitational lensing\ndistort the images produced, in an effect called shear. If measurable, this\nshear may provide independent constraints on cosmological parameters,\ncomplementary to traditional cosmic shear. We model 50 strong gravitational\nlenses from the Sloan Lens ACS (SLACS) catalogue with the aim of measuring the\nline-of-sight (LOS) shear for the first time. We use the `minimal model' for\nthe LOS shear, which has been shown to be theoretically safe from degeneracies\nwith lens model parameters, a finding which has been confirmed using mock data.\nWe use the dolphin automated modelling pipeline, which uses the lenstronomy\nsoftware as a modelling engine, to model our selected lenses. We model the main\ndeflector with an elliptical power law profile, the lens light with elliptical\nS\\'ersic profiles and the source with a basis set of shapelets and an\nelliptical S\\'ersic profile. We successfully obtain a line-of-sight shear\nmeasurement from 18 of the 50 lenses. We find that these LOS shear measurements\nare consistent with external shears measured in recent works using a simpler\nshear model, which are larger than those expected from weak lensing. Neglecting\nthe post-Born correction to the potential of the main deflector due to\nforeground shear leads to a propagation of degeneracies to the LOS shear\nmeasurement, and the same effect is seen if a prior is used to connect the lens\nmass and light ellipticities. The inclusion of an octupole moment in the lens\nmass profile does not lead to shear measurements that are in better agreement\nwith the expectations from weak lensing.",
        "Let $A$ be an abelian variety defined over a number field $K$ and let\n$A^{\\vee}$ be the dual abelian variety. For an odd prime $p$, we consider two\nSelmer groups attached to $A[p]$ and relate the orders of these groups along\nwith those of their corresponding duals to the order of the component groups of\n$A^{\\vee}$ at primes $v$.",
        "We bootstrap the first-order correction in the curvature expansion of the\nVirasoro-Shapiro amplitude in AdS spacetime, for arbitrary Kaluza-Klein charges\nof external operators. By constructing a universal ansatz based on\nsingle-valued multiple polylogarithms as well as an AdS$\\times$S formalism, and\nmatching it with the low-lying result, we derive a unified formula in terms of\nworld-sheet integrals. Our result predicts an infinite number of Wilson\ncoefficients that were not available in previous literature.",
        "Landau's excitation-based argument for superfluids -- that at temperature\n$T=0$ the normal fluid density $\\rho_{n}$ is zero -- should also apply to\nsupersolids. Further, for a total mass density $\\rho$, Leggett argues that the\nsuperfluid fraction $\\rho_{s}\/\\rho<1$. These arguments imply that there is a\nmissing mass. We attribute this to a supersolid density $\\rho_{L}$, with\n$\\rho_{L}\\equiv \\rho-\\rho_{s}-\\rho_{n}$, and a momentum-bearing supersolid\nvelocity $v_{Li}$. Using Onsager's irreversible thermodynamics we derive the\nmacroscopic dynamical equations for this system. We find that $v_{Li}$ is\nsubject to the force of elasticity, to the negative gradient of the chemical\npotential per mass $\\mu$ (as for the superfluid velocity $v_{si}$), and to drag\nagainst the normal fluid (leading to the interpretation of $L$ as lattice).\nThus both the superfluid and supersolid components are associated with the\nground state. The normal modes for such a system have a crossover in frequency,\nabove which the normal fluid velocity $v_{ni}$ is an independent variable and\nbelow which it is locked to $v_{Li}$. For an isotropic lattice we study both\nthe transverse response and longitudinal response. The ring geometry for atomic\ngas supersolid states may provide a geometry for testing these predictions.",
        "We study the self-assembly of magnetic colloids using the Stockmayer (SM)\nmodel characterized by short-range Lennard-Jones interactions and long-range\ndipole-dipole interactions. Using molecular dynamics simulations, we design\ncooling protocols that yield perfectly assembled single-domain magnetic\ncrystals. We identify cooling rates at which the system transforms from an\namorphous glass to a crystal, where magnetic ordering promotes crystalline\norder. Remarkably, we observe that the latter develops via a spontaneous\ntransition rather than through the traditional nucleation and growth mechanism.\nFor a weakly dipolar fluid ($\\mu=1$), this self-assembly results in a\nface-centered cubic (FCC) colloidal crystal with dipole moments chained along\nthe (111) direction. For fluids with higher dipole moment ($\\mu = 2.5$), the\ncrystal structure shifts towards a body-centered orthorhombic (BCO) arrangement\ndue to the compression of chains from strong dipolar attractions. These results\nprovide valuable insights into the mechanisms driving crystallization in\nmagnetic fluids, opening new avenues for understanding the formation of\nmagnetically responsive colloidal magnetic crystals with promising\napplications.",
        "We study the combinatorial Calabi flow for ideal circle patterns in both\nhyperbolic and Euclidean background geometry. We prove that the flow exists for\nall time and converges exponentially fast to an ideal circle pattern metric on\nsurfaces with prescribed attainable curvatures. As a consequence, we provide an\nalgorithm to find the desired ideal circle patterns.",
        "We show that limit cycle systems in Langevin bath exhibit uncertainty in\nobservables that define the limit-cycle plane, and maintain a positive lower\nbound. The uncertainty-bound depends on the parameters that determine the shape\nand periodicity of the limit cycle. In one dimension, we use the framework of\ncanonical dissipative systems to construct the limit cycle, whereas in two\ndimensions, particle in central potentials with radial-dissipation provide us\nnatural examples. We show that, the position-momenta uncertainty of particle in\na central potential is larger than half the magnitude of the angular momentum\n(conserved) of the particle. We also investigate how uncertainties, which are\nabsent in deterministic systems, increase with time when the systems are\nattached to a bath and eventually cross the lower bound before reaching the\nsteady state.",
        "We consider the stabilization problem on a manifold with boundary for a wave\nequation with measure-valued linear damping. For a wide class of measures,\ncontaining Dirac masses on hypersurfaces as well as measures with fractal\nsupport, we establish an abstract energy decay result.",
        "We report electric field-controlled modulation of the Fermi surface topology\nand explore its effects on the superconducting state in alternating-angle\ntwisted quadrilayer graphene (TQG). The unique combination of flat and\ndispersive bands in TQG allows us to simultaneously tune the band structure\nthrough carrier density, $n$, and displacement field, $D$. From\ndensity-dependent Shubnikov-de Haas quantum oscillations and Hall measurements,\nwe quantify the $D$-dependent bandwidth of the flat and dispersive bands and\ntheir hybridization. In the high $|D|$ regime, the increased bandwidth favors\nthe single particle bands, which coincides exactly with the vanishing of the\nsuperconducting transition temperature $T_c$, showing that superconductivity in\nTQG is strongly bound to the symmetry-broken state. For a range of lower $|D|$\nvalues, a Lifshitz transition occurs when the flat and dispersive band Fermi\nsurfaces merge within the $\\nu=+2$ symmetry-broken state. The superconducting\nstate correspondingly shows an enhanced $T_c$, suggesting that the\nsuperconducting condensate is strongly dependent on the Fermi surface topology\nand density of states within this symmetry-broken state.",
        "In this paper, we first introduce the notion of twisted $\\mathcal\nO$-operators on a Hom-Lie-Yamaguti algebra by a given $(2,3)$-cocycle with\ncoefficients in a representation. We show that a twisted $\\mathcal O$-operator\ninduces a Hom-Lie-Yamaguti structure. We also introduce the notion of a\nweighted Reynolds operator on a Hom-Lie-Yamaguti algebra, which can serve as a\nspecial case of twisted $\\mathcal O$-operators on Hom-Lie-Yamaguti algebras.\nThen, we define a cohomology of twisted $\\mathcal O$-operator on\nHom-Lie-Yamaguiti algebras with coefficients in a representation. Furthermore,\nwe introduce and study the Hom-NS-Lie-Yamaguti algebras as the underlying\nstructure of the twisted $\\mathcal O$-operator on Hom-Lie-Yamaguti algebras.\nFinally, we investigate the twisted $\\mathcal O$-operator on Hom-Lie-Yamaguti\nalgebras induced by the twisted $\\mathcal O$-operator on a Hom-Lie algebras.",
        "The noisy nature of quantum hardware necessitates the implementation of\nhigh-fidelity quantum gates in a noise-insensitive manner. While there exist\nmany powerful methods for designing dynamically corrected gates, they typically\ninvolve an exploration across a large-dimensional landscape filled with\nsolutions that are only locally optimal, making it challenging to find globally\noptimal ones. Moreover, these methods often use a single cost function to try\nto accomplish the two disparate goals of achieving a target gate and\nsuppressing noise, and this can lead to unnecessary tradeoffs between the two\nand, consequently, lower fidelities. Here, we present a method for designing\ndynamically corrected gates called B\\'ezier Ansatz for Robust Quantum (BARQ)\ncontrol to address these challenges. Rather than numerically optimizing the\ncontrols directly, BARQ instead makes use of the Space Curve Quantum Control\nformalism in which the quantum evolution is mapped to a geometric space curve.\nIn the formulation used by BARQ, the boundary conditions of the space curve\ndetermine the target gate, while its shape determines the noise robustness of\nthe corresponding gate. This allows the target gate to be fixed upfront, so\nthat numerical optimization is only needed to achieve noise-robustness, and\nthis is performed efficiently using a control-point parameterization of the\nspace curve. In this way, BARQ eliminates the gate-fixing and noise-robustness\ntradeoff while also providing a global perspective into the control landscape,\nand allows for ample freedom to design experimentally friendly and robust\ncontrol pulses. The pulse design is facilitated through the developed software\npackage qurveros.",
        "Transmission electron microscopes (TEMs) enable atomic-scale imaging and\ncharacterisation, driving advances across fields from materials science to\nbiology. Quantum correlations, specifically entanglement, may provide a basis\nfor novel hybrid sensing techniques to make TEMs compatible with sensitive\nsamples prone to radiation damage. We present a protocol to certify\nentanglement between electrons and photons naturally arising from certain\ncoherent cathodoluminescence processes. Using mutually unbiased bases in\nposition and momentum, our method allows robust, state-agnostic entanglement\nverification and provides a lower bound on the entanglement of formation,\nenabling quantitative comparisons across platforms. Simulations under\nexperiment-inspired conditions and preliminary experimental data highlight the\nfeasibility of implementing this approach in modern TEM systems with optical\nspecimen access. Our work integrates photonic quantum information techniques\nwith electron microscopy. It establishes a foundation for entanglement-based\nimaging at the atomic scale, offering a potential pathway to reduce radiation\nexposure.",
        "The paper investigates a pivotal condition for the Bobkov-Tanaka type\nspectrum for double-phase operators. This condition is satisfied if either the\nweight $w$ driving the double-phase operator is strictly positive in the whole\ndomain or the domain is convex and fulfils a suitable symmetry condition."
      ]
    }
  },
  {
    "id":2412.18649,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"A Review of Surface Haptics: Enabling Tactile Effects on Touch Surfaces",
    "start_abstract":"In this article, we review the current technology underlying surface haptics that converts passive touch surfaces to active ones (machine haptics), our perception of tactile stimuli displayed through (human their potential applications (human-machine interaction), and finally, challenges ahead us in making them available commercial systems. This article primarily covers interactions human fingers or hands with surface-haptics displays by focusing on three most popular actuation methods: vibrotactile, electrostatic, ultrasonic.",
    "start_categories":[
      "cs.HC"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Biophysical properties of the human finger for touch comprehension: influences of ageing and gender"
      ],
      "abstract":[
        "The human finger plays an extremely important role in tactile perception, but little is known about how age and gender affect its biophysical properties their perception. We combined studies on contact characteristics, mechanical surface topography to understand effects the finger. values obtained regarding characteristics (i.e. adhesive force) were significantly higher for women than men. As Young's modulus E), a significant positive correlation with was observed found be women. A between arithmetic mean of roughness However, inverse effect highlighted have never been reported previously literature. These results open new perspectives understanding weakening perception across ages it differs men"
      ],
      "categories":[
        "Biodynamic"
      ]
    },
    "list":{
      "title":[
        "Spatiotemporal steering of non-diffracting wavepackets",
        "Regularizing effect of the spatially homogeneous Landau equation with\n  soft potential",
        "A Note on Strongly $\\pi$-Regular Elements",
        "Foundations of Digital Circuits: Denotation, Operational, and Algebraic\n  Semantics",
        "Looking for the {\\gamma}-Ray Cascades of the KM3-230213A Neutrino Source",
        "Optimizing Input Data Collection for Ranking and Selection",
        "Modeling cell differentiation in neuroblastoma: insights into\n  development, malignancy, and treatment relapse",
        "Metasurface Dome for Above-the-Horizon Grating Lobes Reduction in 5G-NR\n  Systems",
        "A multi-wavelength investigation of spiral structures in $z > 1$\n  galaxies with JWST",
        "A note on goodness of fit testing for the Poisson distribution",
        "Interface conditions for Maxwell's equations by homogenization of thin\n  inclusions: transmission, reflection or polarization",
        "Utilizing Quantum Fingerprints in Plant Cells to Evaluate Plant\n  productivity",
        "Galaxies in the simulated cosmic web: I. Filament identification and\n  their properties",
        "Topological Casimir effect for fermionic condensate in AdS spacetime\n  with compact dimensions",
        "Smoothing properties of the Wilson flow and the topological charge",
        "Controllability and Displacement Analysis of a Three-Link Elastic\n  Microswimmer: A Geometric Control Approach",
        "LU Decomposition and Generalized Autoone-Takagi Decomposition of Dual\n  Matrices and their Applications",
        "New constraints on the galactic ionizing efficiency and escape fraction\n  at 2.5 < z < 6 based on quasar absorption spectra",
        "Optimal ANOVA-based emulators of models with(out) derivatives",
        "Model-independent forecasts for the cosmological anisotropic stress",
        "Cubic norm pairs and hermitian cubic norm structures",
        "Weak electronic correlations in the cobalt oxychalcogenide\n  superconductor Na2CoSe2O",
        "Multi-photon enhanced resolution for Superconducting Nanowire\n  Single-Photon Detector-based Time-of-Flight lidar systems",
        "Bikernels by monochromatic paths",
        "Hard Lefschetz Condition on symplectic non-K\\\"ahler solvmanifolds",
        "Stories that (are) Move(d by) Markets: A Causal Exploration of Market\n  Shocks and Semantic Shifts across Different Partisan Groups",
        "A convenient characterisation of convergent upper transition operators",
        "Decay estimates for solutions to non-autonomous critical p-Laplace\n  problems",
        "Parselets: An Abstraction for Fast, General-Purpose Algorithmic\n  Information Calculus"
      ],
      "abstract":[
        "We study the dynamics of space-time non-diffracting wavepackets, commonly\nknown as light bullets, in a spatiotemporally varying medium. We show that by\nspatiotemporal refraction, a monochromatic focused beam can be converted to a\nlight bullet that propagates at a given velocity. By further designing the\nindex profile of the spatiotemporal boundary, the group velocity and the\npropagation direction of the light bullet can be engineered in a programmable\nway. All effects mentioned above cannot be achieved by spatial or temporal\nboundaries, and are only possible with spatiotemporal boundaries. These\nfindings provide unique ways to engineer the dynamics of electromagnetic\nwavepackets in space-time. Such wavepackets with engineered spacetime\ntrajectories may find potential applications in the spatiotemporal control of\nmaterial properties or particles, or for use as a way to emulate relativistic\nphysics in the laboratory.",
        "This paper investigates the Cauchy problem of the spatially homogeneous\nLandau equation with soft potential under the perturbation framework to global\nequilibrium. We prove that the solution to the Cauchy problem exhibits\nanalyticity in the time variable and the Gelfand-Shilov regularizing effect in\nthe velocity variables.",
        "Let $A \\in \\mathbb{M}_m(S)$, where $S$ is a commutative ring and $m>1$. For a\npositive integer $n$ if $A^n \\mathbb{M}_m(S)= A^{n+1}\\mathbb{M}_m(S)$, then we\nprove that $\\mathbb{M}_m(S)A^n = \\mathbb{M}_m(S)A^{n+1}$. For $n = 1$ this\nvalidates a recent conjecture [3, Conjecture 3.7].",
        "This thesis details a project to define a fully compositional theory of\nsynchronous sequential circuits built from primitive components, motivated by\napplying techniques successfully used in programming languages to hardware.\n  The first part of the thesis defines the syntactic foundations of sequential\ncircuit morphisms, and then builds three different semantic theories:\ndenotational, operational and algebraic. We characterise the denotational\nsemantics of sequential circuits as certain causal stream functions, as well as\nproviding a link to existing circuit methodologies by mapping between circuit\nmorphisms, stream functions and Mealy machines. The operational semantics is\ndefined as a strategy for applying some global transformations followed by\nlocal reductions to demonstrate how a circuit processes a value, leading to a\nnotion of observational equivalence. The algebraic semantics consists of\nequations for bringing circuits into a pseudo-normal form, and then encoding\nbetween different state sets. This part of the thesis concludes with a\ndiscussion of some novel applications, such as those for using partial\nevaluation for digital circuits.\n  While mathematically rigorous, the categorical string diagram formalism is\nnot suited for reasoning computationally. The second part of this thesis\ndetails an extension of string diagram rewriting with hypergraphs so that it is\ncompatible with the traced comonoid structure present in the category of\ndigital circuits. We identify the properties that characterise cospans of\nhypergraphs corresponding to traced comonoid terms, and demonstrate how to\nidentify rewriting contexts valid for rewriting modulo traced comonoid\nstructure. We apply the graph rewriting framework to fixed point operators as\nwell as the operational semantics from the first part, and present a new\nhardware description language based on these theoretical developments.",
        "The extreme energy of the KM3-230213A event could transform our understanding\nof the most energetic sources in the Universe. However, it also reveals an\ninconsistency between the KM3NeT detection and strong IceCube constraints on\nthe ultra-high energy neutrino flux. The most congruous explanation for the\nKM3NeT and IceCube data requires KM3-230213A to be produced by a (potentially\ntransient) source fortuitously located in a region where the KM3NeT acceptance\nis maximized. In hadronic models of ultra-high-energy neutrino production, such\na source would also produce a bright {\\gamma}-ray signal, which would cascade\nto GeV--TeV energies due to interactions with extragalactic background light.\nWe utilize the {\\gamma}-Cascade package to model the spectrum, spatial\nextension, and time-delay of such a source, and scan a region surrounding the\nKM3NeT event to search for a consistent {\\gamma}-ray signal. We find no\nconvincing evidence for a comparable \\textit{Fermi}-LAT source and place\nconstraints on a combination of the source redshift and the intergalactic\nmagnetic field strength between the source and Earth.",
        "We study a ranking and selection (R&S) problem when all solutions share\ncommon parametric Bayesian input models updated with the data collected from\nmultiple independent data-generating sources. Our objective is to identify the\nbest system by designing a sequential sampling algorithm that collects input\nand simulation data given a budget. We adopt the most probable best (MPB) as\nthe estimator of the optimum and show that its posterior probability of\noptimality converges to one at an exponential rate as the sampling budget\nincreases. Assuming that the input parameters belong to a finite set, we\ncharacterize the $\\epsilon$-optimal static sampling ratios for input and\nsimulation data that maximize the convergence rate. Using these ratios as\nguidance, we propose the optimal sampling algorithm for R&S (OSAR) that\nachieves the $\\epsilon$-optimal ratios almost surely in the limit. We further\nextend OSAR by adopting the kernel ridge regression to improve the simulation\noutput mean prediction. This not only improves OSAR's finite-sample\nperformance, but also lets us tackle the case where the input parameters lie in\na continuous space with a strong consistency guarantee for finding the optimum.\nWe numerically demonstrate that OSAR outperforms a state-of-the-art competitor.",
        "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment.",
        "The use of 5G New Radio (NR) spectrum around 26 GHz is currently raising the\nquest on its compatibility with the well-established Earth\nExploration-Satellite Service (EESS), which may be blinded by the spurious\nradiation emitted Above-the-Horizon (AtH) by Base Station (BS) antennas.\nIndeed, AtH grating lobes are often present during cell scanning due to the\nlarge inter-element spacing in BS array antennas for achieving higher gains\nwith a reduced number of RF chains. In this letter, we propose an approach\nbased on an electrically thin metasurface-based dome for the reduction of AtH\ngrating lobes in 5G-NR BS antennas. The proposed scanning range shifting\napproach exploits the natural lower amplitude of the grating lobes when the\nantenna array scans in an angular region closer to the broadside direction. The\ngrating lobe reduction is here demonstrated considering a 1x4 phased linear\nantenna array operating under dual-liner 45deg-slant polarization. A simple\ndesign procedure for designing the metasurface dome is reported, together with\nthe antenna performances, evaluated through a proper set of numerical\nexperiments. It is shown that the grating lobe radiation towards the satellite\nregion is significantly reduced, whereas the overall insertion loss is\nmoderate.",
        "Recent JWST observations have revealed the prevalence of spiral structures at\n$z > 1$. Unlike in the local Universe, the origin and the consequence of\nspirals at this epoch remain unexplored. We use public JWST\/NIRCam data from\nthe COSMOS-Web survey to map spiral structures in eight massive ($>\n10^{10.5}\\,\\rm M_{\\odot}$) star-forming galaxies at $z_{\\rm spec} \\sim 1.5$. We\npresent a method for systematically quantifying spiral arms at $z>1$, enabling\ndirect measurements of flux distributions. Using rest-frame near-IR images, we\nconstruct morphological models accurately tracing spiral arms. We detect\noffsets ($\\sim 0.2 - 0.8\\,\\rm kpc$) between the rest-frame optical and near-IR\nflux distributions across most arms. Drawing parallels to the local Universe,\nwe conclude that these offsets reflect the presence of density waves. For nine\nout of eighteen arms, the offsets indicate spiral shocks triggered by density\nwaves. Five arms have offsets in the opposite direction and are likely\nassociated with tidal interactions. For the remaining cases with no detected\noffsets, we suggest that stochastic 'clumpy' star formation is the primary\ndriver of their formation. In conclusion, we find a multi-faceted nature of\nspiral arms at $z > 1$, similar to that in the local Universe.",
        "Since its introduction in 1950, Fisher's dispersion test has become a\nstandard means of deciding whether or not count data follow the Poisson\ndistribution. The test is based on a characteristic property of the Poisson\ndistribution, and discriminates well between the Poisson and the natural\nalternative hypotheses of binomial and negative binomial distributions.\n  While the test is commonly used to test for general deviations from\nPoissonity, its performance against more general alternatives has not been\nwidely investigated. This paper presents realistic alternative hypotheses for\nwhich general goodness of fit tests perform much better than the Fisher\ndispersion test.",
        "We consider the time-harmonic Maxwell equations in a complex geometry. We are\ninterested in geometries that model polarization filters or Faraday cages. We\nstudy the situation that the underlying domain contains perfectly conducting\ninclusions, the inclusions are distributed in a periodic fashion along a\nsurface. The periodicity is $\\eta>0$ and the typical scale of the inclusion is\n$\\eta$, but we allow also the presence of even smaller scales, e.g. when thin\nwires are analyzed. We are interested in the limit $\\eta\\to 0$ and in effective\nequations. Depending on geometric properties of the inclusions, the effective\nsystem can imply perfect transmission, perfect reflection or polarization.",
        "Overcoming the strong chlorophyll background poses a significant challenge\nfor measuring and optimizing plant growth. This research investigates the novel\napplication of specialized quantum light emitters introduced into intact leaves\nof tobacco (Nicotiana tabacum), a well-characterized model plant system for\nstudies of plant health and productivity. Leaves were harvested from plants\ncultivated under two distinct conditions: low light (LL), representing\nunhealthy leaves with reduced photosynthesis. and high light (HL), representing\nhealthy leaves with highly active photosynthesis. Higher-order correlation data\nwere collected and analyzed using machine learning (ML) techniques,\nspecifically a Convolutional Neural Network (CNN), to classify the photon\nemitter states. This CNN efficiently identified unique patterns and created\ndistinct fingerprints for Nicotiana leaves grown under LL and HL, demonstrating\nsignificantly different quantum profiles between the two conditions. These\nquantum fingerprints serve as a foundation for a novel unified analysis of\nplant growth parameters associated with different photosynthetic states. By\nemploying CNN, the emitter profiles were able to reproducibly classify the\nleaves as healthy or unhealthy. This model achieved high probability values for\neach classification, confirming its accuracy and reliability. The findings of\nthis study pave the way for broader applications, including the application of\nadvanced quantum and machine learning technologies in plant health monitoring\nsystems.",
        "As the environment harbouring the majority of galaxies, filaments are thought\nto play a key role in the co-evolution of galaxies and the cosmic web. In this\nfirst part of a series to understand the link between galaxies and filaments\nthrough cosmological simulations, we address two major current obstacles on\nthis path: the difficulty of meaningful filament identification, and their\npoorly constrained properties and internal structure. We use the public EAGLE\nand TNG100 simulations to build physically motivated filament catalogues with\nthe DisPerSE algorithm, based on the dark matter (DM) field at redshift z = 0\nand z = 2, explicitly accounting for the multi-scale nature of filaments and\nwith careful validation of results. Filament widths, lengths, and densities\nvary by factors ~5-100 in both simulations, highlighting the heterogeneous\nnature of filaments as a cosmic environment. All filaments are relatively thin,\nwith overdensity profiles of galaxies, DM, and gas dropping to the cosmic mean\nwithin <3 Mpc from their spines. Contrary to groups and clusters, filament\ncores are highly substructure dominated, by as much as ~80 per cent. Filament\ngas maps reveal rich temperature and density structures that limit the\napplicability of simple cylindrically symmetric models. EAGLE and TNG100 agree\nthat z = 2 filament spines are traced by overdense cool gas in pressure\nequilibrium with a >10x hotter envelope. However, significant differences in\ndetail between their predicted gas property maps imply that individual\nsimulations cannot yet describe the baryon structure of filaments with\ncertainty. Finally, we compare our fiducial filament network to one constructed\nfrom galaxies. The two differ in many aspects, but the distance of a galaxy to\nits nearest galaxy-based filament still serves as a statistical proxy for its\ntrue environment.",
        "We investigate combined effects of gravitational field and spatial topology\non the fermionic condensate (FC) for a massive Dirac field in locally anti-de\nSitter (AdS) spacetime with a part of spatial dimensions compactified to a\ntorus. For general phases in the periodicity conditions along compact\ndimensions the topological Casimir contribution is explicitly extracted and the\nrenormalization is reduced to the one for purely AdS spacetime. The FC is an\neven periodic function of the magnetic flux enclosed by compact dimension with\nthe period of flux quantum. The topological contribution vanishes on the AdS\nboundary and dominates in the total FC in the region near the AdS horizon. For\nproper lengths of compact dimensions smaller than the AdS curvature radius the\ninfluence of the gravitational field is weak and the leading term in the\ncorresponding expansion coincides with the FC for a locally Minkowski spacetime\nwith compact dimensions. For large proper lengths the decay of the topological\nFC follows a power law for both massless and massive field, in contrast to an\nexponential decay in Minkowski bulk for massive fields. Applications are\ndiscussed for 2D Dirac materials.",
        "We study SU$(N_C)$ gauge theories with a single fermion in the two-index\nantisymmetric representation to predict the mesonic spectrum of supersymmetric\n$\\mathcal{N}=1$ SYM theories. Using gradient flow methods, we investigate\nfractional topological charges in $N_C = 4$ ensembles with varying lattice\nspacings. We show that the use of overimproved gauge actions (specifically the\nDBW2 action) in the smearing kernel stabilises the values of the topological\ncharge already at moderate values of the flow time, while this is not the case\nfor the standard Wilson flow.",
        "This study investigates the dynamics and controllability of a Purcell\nthree-link microswimmer equipped with passive elastic torsional coils at its\njoints. By controlling the spontaneous curvature, we analyse the swimmers\nmotion using both linear and weakly nonlinear approaches. Linear analysis\nreveals steady harmonic solutions for small-amplitude controls but does not\npredict any net displacement, whereas weakly nonlinear analysis predicts\ntranslation along the orientation of the central link. Using geometric control\ntheory, we prove that the system is small time locally controllable near\nequilibrium and derive displacement estimates for periodic piecewise constant\ncontrols, which are validated through numerical simulations. These findings\nindicate that oscillatory controls can enable motion in all directions near\nequilibrium. This work offers foundational insights into the controllability of\nelastic microswimmers, paving the way for advanced motion planning and control\nstrategies.",
        "This paper uses matrix transformations to provide the Autoone-Takagi\ndecomposition of dual complex symmetric matrices and extends it to dual\nquaternion $\\eta$-Hermitian matrices. The LU decomposition of dual matrices is\ngiven using the general solution of the Sylvester equation, and its equivalence\nto the existence of rank-k decomposition and dual Moore-Penrose generalized\ninverse (DMPGI) is proved. Similar methods are then used to provide the\nCholesky decomposition of dual real symmetric positive definite matrices. Both\nof our decompositions are driven by applications in numerical linear algebra.",
        "Measurements of the ionization state of the intergalactic medium (IGM) can\nprobe the sources of the extragalactic ionizing background. We provide new\nmeasurements of the ionizing emissivity of galaxies using measurements of the\nionizing background and ionizing photon mean free path from high-redshift\nquasar spectra at $2.5 < z < 6$. Unlike most prior works, we account for\nradiative-transfer effects and possible neutral islands from the tail of\nreionization at $z > 5$. We combine our results with measurements of the UV\nluminosity function to constrain the average escaping ionizing efficiency of\ngalaxies, $\\langle f_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}$. Assuming\ngalaxies with $M_{\\rm UV} < -11$ emit ionizing photons, we find $\\log (\\langle\nf_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}\/{\\rm erg^{-1}Hz}) =\n24.47_{-0.17}^{+0.09}$ and $24.75_{-0.28}^{+0.15}$ at $z=5$ and $6$, and\n$1\\sigma$ upper limits of $24.48$ and $24.31$ at $z = 2.5$ and $4$,\nrespectively. We also estimate the population-averaged $f_{\\rm esc}$ using\nmeasurements of intrinsic ionizing efficiency from JWST. We find $\\langle\nf_{\\rm esc} \\rangle = 0.126_{-0.041}^{+0.034}$ and $0.224_{-0.108}^{+0.098}$ at\n$z=5$ and $6$, and $1\\sigma$ upper limits of $f_{\\rm esc}< 0.138$ and $0.096$\nat $z=2.5$ and $4$, respectively, for $M_{\\rm UV} < -11$. Our findings are\nconsistent with prior measurements of $f_{\\rm esc} \\lesssim 10\\%$ at $z \\leq\n4$, but indicate a factor of several increase between $z = 4$ and $6$. The\nsteepness of this evolution is sensitive to the highly uncertain mean free path\nand ionizing background intensity at $z>5$. Lastly, we find\n$1.10^{+0.21}_{-0.39}$ photons per H atom are emitted into the IGM between\n$z=6$ and $=5.3$. This is $\\approx 4\\times$ more than needed to complete the\nlast $20\\%$ of reionization absent recombinations, suggesting that\nreionization's end was likely absorption-dominated.",
        "This paper proposes new ANOVA-based approximations of functions and emulators\nof high-dimensional models using either available derivatives or local\nstochastic evaluations of such models. Our approach makes use of sensitivity\nindices to design adequate structures of emulators. For high-dimensional models\nwith available derivatives, our derivative-based emulators reach dimension-free\nmean squared errors (MSEs) and parametric rate of convergence (i.e.,\n$\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model\n(without available derivatives) by deriving global emulators that account for\nthe local properties of models or simulators. Such generic emulators enjoy\ndimension-free biases, parametric rates of convergence and MSEs that depend on\nthe dimensionality. Dimension-free MSEs are obtained for high-dimensional\nmodels with particular inputs' distributions. Our emulators are also\ncompetitive in dealing with different distributions of the input variables and\nfor selecting inputs and interactions. Simulations show the efficiency of our\napproach.",
        "The effective anisotropic stress $\\eta$ is a key variable in the\ncharacterization of many classes of modified gravity theories, as it allows the\ntesting for a long-range force additional to gravity. In this paper we forecast\nthe precision with which future large surveys can determine $\\eta$ in a way\nthat only relies on directly observable quantities obtained from the\nspectroscopic measurements of the clustering of galaxies and the photometric\nbased observation of the projected lensing and galaxy clustering correlations\nand their cross signal. Our method does not require further assumptions about\nthe initial power spectrum, the modified gravity model, the expansion rate, or\nthe bias. We consider various cases: $\\eta$ free to vary in space and time, or\nwith only redshift dependence, or constant. We take as a reference\nspecifications that approximate a Euclid-like photometric or a combined one\nwith a DESI-like spectroscopic survey. Among our results, we find that a future\nlarge-scale lensing and clustering survey can constrain $\\eta$ to at least 30\\%\nif $z$, $k$ independent, and to less than 10\\% on average for the $z$\ndependence only, to finally reach 5\\% values in the constant case.",
        "We generalize cubic norm structures to cubic norm pairs and extend hermitian\ncubic norm structures to arbitrary commutative unital rings. For the associated\n``skew dimension one structurable algebra\" of these pairs, we construct a\ncorresponding Lie algebra and a group of automorphisms of the Lie algebra.\nUsing the structure of this automorphism group, we also prove that each\nhermitian cubic norm structure induces an operator Kantor pair.",
        "Motivated by the newly discovered Co-based superconductor Na2CoSe2O, we\nperformed systematic calculations of its electronic band structures using the\ndensity functional theory (DFT) plus the dynamical mean-field theory (DMFT)\napproaches. Our comparative studies reveal weakly correlated and itinerant\nnature of the Co-3d electrons and show no sign of fluctuating local moments as\nexpected in many other unconventional superconductors, although the Co eg\norbitals are close to half filling. These suggest that Na2CoSe2O is a normal\nparamagnetic metal and its superconductivity might not be of strongly\ncorrelated nature, contrary to the initial speculation. We suggest future\ninvestigations of electron-phonon interactions to clarify its pairing\nmechanism.",
        "Superconducting nanowire single photon detectors (SNSPDs) emerged in the last\ndecade as a disruptive technology that features performance characteristics,\nsuch as high sensitivity, dynamic range and temporal accuracy, which are\nideally suited for light detection and ranging (lidar) applications. Here, we\nreport a time-of-flight (TOF) lidar system based on waveguide-integrated SNSPDs\nthat excels in temporal accuracy, which translates into high range resolution.\nFor single-shot measurements, we find resolution in the millimeter regime,\nresulting from the jitter of the time-of-flight signal of 21$\\,$ps for low\nphoton numbers. We further decrease this signal jitter to 11$\\,$ps by driving\nthe SNSPD into a multiphoton detection regime, utilizing laser pulses of higher\nintensity, thus improving range resolution. For multi-shot measurements we find\nsub-millimeter range-accuracy of 0.75$\\,$mm and reveal additional surface\ninformation of scanned objects by visualizing the number of reflected photons\nand their temporal spread with the acquired range data in a combined\nrepresentation. Our realization of a lidar receiver exploits favorable timing\naccuracy of waveguide-integrated SNSPDs and extends their operation to the\nmultiphoton regime, which benefits a wide range of remote sensing applications.",
        "In this paper, we introduce the concept of bikernel by monochromatic paths of\na bicolored digraph. This concept is strongly motivated by the existing notions\nof kernels, kernels by monochromatic paths, and double stable augmented\ncategories. We establish sufficient and necessary conditions for several\nfamilies of bicolored digraphs to have a bikernel by monochromatic paths. Also,\nwe characterize bicolored digraphs without monochromatic cycles that possess a\nbikernel by monochromatic paths. Similarly, we characterize bicolored digraphs\nwith monochromatic cycles that also have a bikernel by monochromatic paths.\nFurthermore, we prove sufficient and necessary conditions for some families of\ndigraphs to be $BK$-colorable. This means that a bicoloration of the digraph\nexists where the resulting bicolored digraph has a bikernel.",
        "We provide new families of compact complex manifolds with no K\\\"ahler\nstructure carrying symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. These examples are obtained as compact quotients of the solvable\nLie group $\\mathbb{C}^{2n} \\ltimes_{\\rho} \\mathbb{C}^{2m}$, for which we\nconstruct explicit lattices. By cohomological computations we prove that such\nmanifolds carry symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. Furthermore, we compute the Kodaira dimension of an almost-K\\\"ahler\nstructure and generators for the de Rham and Dolbeault cohomologies.",
        "Macroeconomic fluctuations and the narratives that shape them form a mutually\nreinforcing cycle: public discourse can spur behavioural changes leading to\neconomic shifts, which then result in changes in the stories that propagate. We\nshow that shifts in semantic embedding space can be causally linked to\nfinancial market shocks -- deviations from the expected market behaviour.\nFurthermore, we show how partisanship can influence the predictive power of\ntext for market fluctuations and shape reactions to those same shocks. We also\nprovide some evidence that text-based signals are particularly salient during\nunexpected events such as COVID-19, highlighting the value of language data as\nan exogenous variable in economic forecasting. Our findings underscore the\nbidirectional relationship between news outlets and market shocks, offering a\nnovel empirical approach to studying their effect on each other.",
        "Motivated by its connection to the limit behaviour of imprecise Markov\nchains, we introduce and study the so-called convergence of upper transition\noperators: the condition that for any function, the orbit resulting from\niterated application of this operator converges. In contrast, the existing\nnotion of `ergodicity' requires convergence of the orbit to a constant. We\nderive a very general (and practically verifiable) sufficient condition for\nconvergence in terms of accessibility and lower reachability, and prove that\nthis sufficient condition is also necessary whenever (i) all transient states\nare absorbed or (ii) the upper transition operator is finitely generated.",
        "We prove optimal decay estimates for positive solutions to elliptic\np-Laplacian problems in the entire Euclidean space, when a critical\nnonlinearity with a decaying source term is considered. Also gradient decay\nestimates are furnished. Our results extend previous theorems in the\nliterature, in which a purely critical reaction is treated. The technique is\nbased on a priori estimates, regularity results, and rescaling arguments,\ncombined with the doubling lemma.",
        "This work describes the principled design of a theoretical framework leading\nto fast and accurate algorithmic information measures on finite multisets of\nfinite strings by means of compression. One distinctive feature of our approach\nis to manipulate {\\em reified}, explicit representations of the very entities\nand quantities of the theory itself: compressed strings, models,\nrate-distortion states, minimal sufficient models, joint and relative\ncomplexity. To do so, a programmable, recursive data structure called a {\\em\nparselet} essentially provides modeling of a string as a concatenation of\nparameterized instantiations from sets of finite strings that encode the\nregular part of the data. This supports another distinctive feature of this\nwork, which is the native embodiment of Epicurus' Principle on top of Occam's\nRazor, so as to produce both a most-significant and most-general explicit model\nfor the data. This model is iteratively evolved through the Principle of\nMinimal Change to reach the so-called minimal sufficient model of the data.\nParselets may also be used to compute a compression score to any arbitrary\nhypothesis about the data. A lossless, rate-distortion oriented, compressed\nrepresentation is proposed, that allows immediate reusability of the costly\ncomputations stored on disk for their fast merging as our core routine for\ninformation calculus. Two information measures are deduced: one is exact\nbecause it is purely combinatorial, and the other may occasionally incur slight\nnumerical inaccuracies because it is an approximation of the Kolmogorov\ncomplexity of the minimal sufficient model. Symmetry of information is enforced\nat the bit level. Whenever possible, parselets are compared with off-the-shelf\ncompressors on real data. Some other applications just get enabled by\nparselets."
      ]
    }
  },
  {
    "id":2412.18649,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Biophysical properties of the human finger for touch comprehension: influences of ageing and gender",
    "start_abstract":"The human finger plays an extremely important role in tactile perception, but little is known about how age and gender affect its biophysical properties their perception. We combined studies on contact characteristics, mechanical surface topography to understand effects the finger. values obtained regarding characteristics (i.e. adhesive force) were significantly higher for women than men. As Young's modulus E), a significant positive correlation with was observed found be women. A between arithmetic mean of roughness However, inverse effect highlighted have never been reported previously literature. These results open new perspectives understanding weakening perception across ages it differs men",
    "start_categories":[
      "Biodynamic"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "A Review of Surface Haptics: Enabling Tactile Effects on Touch Surfaces"
      ],
      "abstract":[
        "In this article, we review the current technology underlying surface haptics that converts passive touch surfaces to active ones (machine haptics), our perception of tactile stimuli displayed through (human their potential applications (human-machine interaction), and finally, challenges ahead us in making them available commercial systems. This article primarily covers interactions human fingers or hands with surface-haptics displays by focusing on three most popular actuation methods: vibrotactile, electrostatic, ultrasonic."
      ],
      "categories":[
        "cs.HC"
      ]
    },
    "list":{
      "title":[
        "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality\n  Generation Models",
        "Reasoning is All You Need for Video Generalization: A Counterfactual\n  Benchmark with Sub-question Evaluation",
        "All-dry pick-up and transfer method for quantum emitter arrays in\n  hexagonal boron nitride",
        "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat\n  Prediction",
        "Data-driven methods to discover stable linear models of the helicity\n  injectors on HIT-SIU",
        "Make yourself comfortable: Nudging urban heat and noise mitigation with\n  smartwatch-based Just-in-time Adaptive Interventions (JITAI)",
        "Aug3D: Augmenting large scale outdoor datasets for Generalizable Novel\n  View Synthesis",
        "A universal total anomalous dissipator",
        "Autocatalysis due to combinatorial enhancement",
        "W-class states-identification and quantification of Bell-CHSH\n  inequalities' violation",
        "Illuminating Youth: Decades of Mid-Infrared Variability and Color\n  Evolution of Young Stellar Objects",
        "Effective Two-Stage Double Auction for Dynamic Resource Trading in Edge\n  Networks via Overbooking",
        "Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear\n  Algebra",
        "SoK: Understanding Vulnerabilities in the Large Language Model Supply\n  Chain",
        "When Unsupervised Domain Adaptation meets One-class Anomaly Detection:\n  Addressing the Two-fold Unsupervised Curse by Leveraging Anomaly Scarcity",
        "Quadrotor Neural Dead Reckoning in Periodic Trajectories",
        "Gensors: Authoring Personalized Visual Sensors with Multimodal\n  Foundation Models and Reasoning",
        "Low-overhead Magic State Circuits with Transversal CNOTs",
        "DocVideoQA: Towards Comprehensive Understanding of Document-Centric\n  Videos through Question Answering",
        "On quasisymmetric mappings between ultrametric spaces",
        "Template Matching in Images using Segmented Normalized Cross-Correlation",
        "The Quest for Visual Understanding: A Journey Through the Evolution of\n  Visual Question Answering",
        "How manipulable are prediction markets?",
        "Quantitative analysis of vectorial torques in thin 3d Co ferromagnet\n  using orbital-spin conversion",
        "Rigidity of Higson coronas",
        "Semi-Lie arithmetic fundamental lemma for the full spherical Hecke\n  algebra",
        "The Liabilities of Robots.txt",
        "Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic\n  Pick-and-Place Setups",
        "Label Distribution Learning with Biased Annotations by Learning\n  Multi-Label Representation"
      ],
      "abstract":[
        "Current Cross-Modality Generation Models (GMs) demonstrate remarkable\ncapabilities in various generative tasks. Given the ubiquity and information\nrichness of vision modality inputs in real-world scenarios, Cross-vision,\nencompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), tasks\nhave attracted significant attention. Large Vision Language Models (LVLMs) and\nI2I GMs are employed to handle VLP and I2I tasks, respectively. Previous\nresearch indicates that printing typographic words into input images\nsignificantly induces LVLMs and I2I GMs to generate disruptive outputs\nsemantically related to those words. Additionally, visual prompts, as a more\nsophisticated form of typography, are also revealed to pose security risks to\nvarious applications of VLP tasks when injected into images. In this paper, we\ncomprehensively investigate the performance impact induced by Typographic\nVisual Prompt Injection (TVPI) in various LVLMs and I2I GMs. To better observe\nperformance modifications and characteristics of this threat, we also introduce\nthe TVPI Dataset. Through extensive explorations, we deepen the understanding\nof the underlying causes of the TVPI threat in various GMs and offer valuable\ninsights into its potential origins.",
        "Counterfactual reasoning is crucial for robust video understanding but\nremains underexplored in existing multimodal benchmarks. In this paper, we\nintroduce \\textbf{COVER} (\\textbf{\\underline{CO}}unterfactual\n\\textbf{\\underline{V}}id\\textbf{\\underline{E}}o\n\\textbf{\\underline{R}}easoning), a multidimensional multimodal benchmark that\nsystematically evaluates MLLMs across the abstract-concrete and\nperception-cognition dimensions. Beyond prior multimodal benchmarks, COVER\ndecomposes complex queries into structured sub-questions, enabling fine-grained\nreasoning analysis. Experiments on commercial and open-source models reveal a\nstrong correlation between sub-question accuracy and counterfactual reasoning\nperformance, highlighting the role of structured inference in video\nunderstanding. Furthermore, our results suggest a key insight: enhancing the\nreasoning capability of models is essential for improving the robustness of\nvideo understanding. COVER establishes a new standard for assessing MLLMs'\nlogical reasoning abilities in dynamic environments.",
        "Single photon emitters in hexagonal boron nitride are based on fluorescent\npoint-like defects. These defects typically have exceptional photophysical\nproperties and therefore been the focus of extensive research due to their\npotential to advance photonic quantum technologies. However, achieving scalable\nintegration of these emitters to arbitrary platforms with high yield while\nretaining their characteristics remains a significant challenge, particularly\nwhen the target substrate is not compatible with the fabrication method. In\nthis work, we introduce an all-dry transfer method aimed at addressing these\nchallenges with improved effectiveness compared to existing techniques. This\npolymer stamp-assisted transfer method maintains high output and preserves the\nfundamental characteristics of the emitters while eliminating wet chemical\nprocesses. A comprehensive post-transfer characterization verified not only the\nmaintenance of the defining characteristic of a single photon emitter, the\nsecond-order correlation function $g^{(2)}(0)$, but also showed improvement by\nabout 46%. In contrast, the lifetime, emission spectrum, and the photostability\nshowed only negligible change, demonstrating that the characteristics of the\nemitters were retained during the transfer process. This transfer technique has\nsuccess rate of 81.8%, determined by the proportion of single photon emitters\nthat retain their optical and preserve physical structure post-transfer. This\nhigh success rate shows the potential to scale the integration of single photon\nemitters across diverse platforms. We expect that this process contributes to\nthe applications of boron nitride defects in quantum technologies.",
        "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks.",
        "Accurate and efficient circuit models are necessary to control the power\nelectronic circuits found on plasma physics experiments. Tuning and controlling\nthe behavior of these circuits is inextricably linked to plasma performance.\nLinear models are greatly preferred for control applications due to their\nwell-established performance guarantees, but they typically fail to capture\nnonlinear dynamics and changes in experimental parameters. Data-driven system\nidentification can help mitigate these shortcomings by learning interpretable\nand accurate reduced-order models of a complex system, in this case the\ninjector circuits of the Helicity Injected Torus - Steady Inductive Upgrade\n(HIT-SIU) experiment. Specifically, the Bagging Optimized Dynamic Mode\nDecomposition (BOP-DMD), is leveraged to learn stable, reduced order models of\nthe interaction between the spheromak plasma formed in the confinement volume,\nand the injector circuits of the device. BOP-DMD is trained and evaluated on an\nanalytic model of the vacuum dynamics of the injector circuits of HIT-SIU, as\nwell as an analytic linear reduced-order model for the injector dynamics when a\nplasma is present. BOP-DMD is then fit on experimental data, both on shots with\nand without a plasma in the confinement volume. In doing so, we demonstrate the\ncapability of data-driven methods to produce stable, linear models for control\nand uncertainty quantification in plasma experiments.",
        "Humans can play a more active role in improving their comfort in the built\nenvironment if given the right information at the right place and time. This\npaper outlines the use of Just-in-Time Adaptive Interventions (JITAI)\nimplemented in the context of the built environment to provide information that\nhelps humans minimize the impact of heat and noise on their daily lives. This\nframework builds upon the open-source Cozie iOS smartwatch platform. It\nincludes data collection through micro-surveys and intervention messages\ntriggered by environmental, contextual, and personal history conditions. An\neight-month deployment of the method was completed in Singapore with 103\nparticipants who submitted over 12,000 micro-surveys and delivered over 3,600\nJITAI intervention messages. A weekly survey conducted during two deployment\nphases revealed an overall increase in perceived usefulness ranging from 8-19%\nover the first three weeks of data collection. For noise-related interventions,\nparticipants showed an overall increase in location changes ranging from 4-11%\nand a 2-17% increase in earphone use to mitigate noise distractions. For\nthermal comfort-related interventions, participants demonstrated a 3-13%\nincrease in adjustments to their location or thermostat to feel more\ncomfortable. The analysis found evidence that personality traits (such as\nconscientiousness), gender, and environmental preferences could be factors in\ndetermining the perceived helpfulness of JITAIs and influencing behavior\nchange. These findings underscore the importance of tailoring intervention\nstrategies to individual traits and environmental conditions, setting the stage\nfor future research to refine the delivery, timing, and content of intervention\nmessages.",
        "Recent photorealistic Novel View Synthesis (NVS) advances have increasingly\ngained attention. However, these approaches remain constrained to small indoor\nscenes. While optimization-based NVS models have attempted to address this,\ngeneralizable feed-forward methods, offering significant advantages, remain\nunderexplored. In this work, we train PixelNeRF, a feed-forward NVS model, on\nthe large-scale UrbanScene3D dataset. We propose four training strategies to\ncluster and train on this dataset, highlighting that performance is hindered by\nlimited view overlap. To address this, we introduce Aug3D, an augmentation\ntechnique that leverages reconstructed scenes using traditional\nStructure-from-Motion (SfM). Aug3D generates well-conditioned novel views\nthrough grid and semantic sampling to enhance feed-forward NVS model learning.\nOur experiments reveal that reducing the number of views per cluster from 20 to\n10 improves PSNR by 10%, but the performance remains suboptimal. Aug3D further\naddresses this by combining the newly generated novel views with the original\ndataset, demonstrating its effectiveness in improving the model's ability to\npredict novel views.",
        "For all $\\alpha\\in(0,1)$, we construct an explicit divergence-free vector\nfield $V\\in L^\\infty_tC^\\alpha_x \\cap C^{\\frac{\\alpha}{1-\\alpha}}_t L^\\infty_x$\nso that the solutions to the drift-diffusion equations\n$$\\partial_t\\theta^\\kappa-\\kappa\\Delta\\theta^\\kappa+V\\cdot\\nabla\\theta^\\kappa=0$$\nexhibit asymptotic total dissipation for all mean-zero initial data:\n$\\lim_{\\kappa\\rightarrow 0}\\|\\theta^\\kappa(1,\\cdot)\\|_{L^2}=0$. Additionally,\nwe give explicit rates in $\\kappa$ and uniform dependence on initial data.",
        "We demonstrate that autocatalytic reactions, where a product catalyzes its\nown formation, can be significantly accelerated when the product molecules are\nindistinguishable from each other. This ``combinatorial enhancement,\" analogous\nto the driving force of osmotic pressure, arises from the increased\nmultiplicity of microscopic configurations. We quantify this effect with a\nfree-energy gain, Fgain, and validate our theoretical predictions using\nmolecular dynamics simulations. We also propose an experiment to directly test\nthis phenomenon, potentially providing new insights into self-assembly,\nbiomolecular binding, and other cooperative processes.",
        "We discuss a family of W-class states describing three-qubit systems. For\nsuch systems, we analyze the relations between the entanglement measures and\nthe nonlocality parameter for a two-mode mixed state related to the two-qubit\nsubsystem. We find the conditions determining the boundary values of the\nnegativity, parameterized by concurrence, for violating the Bell-CHSH\ninequality. Additionally, we derive the value ranges of the mixedness measure,\nparameterized by concurrence and negativity for the qubit--qubit mixed state,\nguaranteeing the violation and non-violation of the Bell-CHSH inequality.",
        "The variability of Young Stellar Objects (YSOs) is a crucial tool for\nunderstanding the mechanisms driving flux changes. In this study, we present an\ninfrared variability analysis of a large sample of over 20,000 candidate YSOs,\nusing data from the ALLWISE and NEOWISE surveys, which span around a decade\nwith a 6-month cadence. We applied Lomb-Scargle Periodogram (LSP) analysis and\nlinear fitting to the light curves, classifying them into distinct categories:\n{\\it Secular} ({\\it Linear}, {\\it Curved}, and {\\it Periodic}) and {\\it\nStochastic} ({\\it Burst}, {\\it Drop}, and {\\it Irregular}). Our findings show\nthat 5,467 (26.2$\\pm$0.3\\%) of the sources exhibit variability, with most\n(19.7$\\pm$0.3\\%) showing {\\it Irregular} variations, followed by {\\it Curved}\nand {\\it Periodic} variations. In addition, 235 sources of {\\it Bursts} and 122\n{\\it Drop} sources were identified. Variability is more pronounced in Class I\nsources with a higher fraction of variables (36.3$\\pm$0.6\\%) compared to Class\nII (22.1$\\pm$0.4\\%) and Class III (22.5$\\pm$1.0\\%) sources. The color (W1 $-$\nW2) versus magnitude analysis (W2) using linear fitting shows that the trend\n``redder-when-brighter\" (RWB) is more prevalent (85.4$\\pm$0.5\\%) among YSOs. In\ncontrast, the trend ``bluer-when-brighter\" (BWB) is more common in younger\nsources compared to more evolved ones, having a BWB fraction of 29.0$\\pm$1.1\\%\nfor Class I to 4.0$\\pm$0.9\\% for Class III.",
        "To facilitate responsive and cost-effective computing resource scheduling and\nservice delivery over edge-assisted mobile networks, this paper investigates a\nnovel two-stage double auction methodology via utilizing an interesting idea of\nresource overbooking to overcome dynamic and uncertain nature from edge servers\n(sellers) and demand from mobile devices (as buyers). The proposed auction\nintegrates multiple essential factors such as social welfare maximization and\ndecision-making latency (e.g., the time for determining winning seller-buyer\npairs) reduction, by introducing a stagewise strategy: an overbooking-driven\npre-double auction (OPDAuction) for determining long-term cooperations between\nsellers and buyers before practical resource transactions as Stage I, and a\nreal-time backup double auction (RBDAuction) for handling residual resource\ndemands during actual transactions. In particular, by applying a proper\noverbooking rate, OPDAuction helps with facilitating trading contracts between\nappropriate sellers and buyers as guidance for future transactions, by allowing\nthe booked resources to exceed supply. Then, since pre-auctions may cause\nrisks, our RBDAuction adjusts to real-time market changes, further enhancing\nthe overall social welfare. More importantly, we offer an interesting view to\nshow that our proposed two-stage auction can support significant design\nproperties such as truthfulness, individual rationality, and budget balance.\nThrough extensive experiments, we demonstrate good performance in social\nwelfare, time efficiency, and computational scalability, outstripping\nconventional methods in dynamic edge computing settings.",
        "K-means is a popular clustering algorithm with significant applications in\nnumerous scientific and engineering areas. One drawback of K-means is its\ninability to identify non-linearly separable clusters, which may lead to\ninaccurate solutions in certain cases. Kernel K-means is a variant of classical\nK-means that can find non-linearly separable clusters. However, it scales\nquadratically with respect to the size of the dataset, taking several minutes\nto cluster even medium-sized datasets on traditional CPU-based machines. In\nthis paper, we present a formulation of Kernel K-means using sparse-dense\nmatrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV),\nand we show that our formulation enables the rapid implementation of a fast\nGPU-based version of Kernel K-means with little programming effort. Our\nimplementation, named Popcorn, is the first open-source GPU-based\nimplementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x\nover a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a\nGPU implementation of Kernel K-means that does not use sparse matrix\ncomputations. Our results support the effectiveness of sparse matrices as tools\nfor efficient parallel programming.",
        "Large Language Models (LLMs) transform artificial intelligence, driving\nadvancements in natural language understanding, text generation, and autonomous\nsystems. The increasing complexity of their development and deployment\nintroduces significant security challenges, particularly within the LLM supply\nchain. However, existing research primarily focuses on content safety, such as\nadversarial attacks, jailbreaking, and backdoor attacks, while overlooking\nsecurity vulnerabilities in the underlying software systems. To address this\ngap, this study systematically analyzes 529 vulnerabilities reported across 75\nprominent projects spanning 13 lifecycle stages. The findings show that\nvulnerabilities are concentrated in the application (50.3%) and model (42.7%)\nlayers, with improper resource control (45.7%) and improper neutralization\n(25.1%) identified as the leading root causes. Additionally, while 56.7% of the\nvulnerabilities have available fixes, 8% of these patches are ineffective,\nresulting in recurring vulnerabilities. This study underscores the challenges\nof securing the LLM ecosystem and provides actionable insights to guide future\nresearch and mitigation strategies.",
        "This paper introduces the first fully unsupervised domain adaptation (UDA)\nframework for unsupervised anomaly detection (UAD). The performance of UAD\ntechniques degrades significantly in the presence of a domain shift, difficult\nto avoid in a real-world setting. While UDA has contributed to solving this\nissue in binary and multi-class classification, such a strategy is ill-posed in\nUAD. This might be explained by the unsupervised nature of the two tasks,\nnamely, domain adaptation and anomaly detection. Herein, we first formulate\nthis problem that we call the two-fold unsupervised curse. Then, we propose a\npioneering solution to this curse, considered intractable so far, by assuming\nthat anomalies are rare. Specifically, we leverage clustering techniques to\nidentify a dominant cluster in the target feature space. Posed as the normal\ncluster, the latter is aligned with the source normal features. Concretely,\ngiven a one-class source set and an unlabeled target set composed mostly of\nnormal data and some anomalies, we fit the source features within a hypersphere\nwhile jointly aligning them with the features of the dominant cluster from the\ntarget set. The paper provides extensive experiments and analysis on common\nadaptation benchmarks for anomaly detection, demonstrating the relevance of\nboth the newly introduced paradigm and the proposed approach. The code will be\nmade publicly available.",
        "In real world scenarios, due to environmental or hardware constraints, the\nquadrotor is forced to navigate in pure inertial navigation mode while\noperating indoors or outdoors. To mitigate inertial drift, end-to-end neural\nnetwork approaches combined with quadrotor periodic trajectories were\nsuggested. There, the quadrotor distance is regressed and combined with\ninertial model-based heading estimation, the quadrotor position vector is\nestimated. To further enhance positioning performance, in this paper we propose\na quadrotor neural dead reckoning approach for quadrotors flying on periodic\ntrajectories. In this case, the inertial readings are fed into a simple and\nefficient network to directly estimate the quadrotor position vector. Our\napproach was evaluated on two different quadrotors, one operating indoors while\nthe other outdoors. Our approach improves the positioning accuracy of other\ndeep-learning approaches, achieving an average 27% reduction in error outdoors\nand an average 79% reduction indoors, while requiring only software\nmodifications. With the improved positioning accuracy achieved by our method,\nthe quadrotor can seamlessly perform its tasks.",
        "Multimodal large language models (MLLMs), with their expansive world\nknowledge and reasoning capabilities, present a unique opportunity for\nend-users to create personalized AI sensors capable of reasoning about complex\nsituations. A user could describe a desired sensing task in natural language\n(e.g., \"alert if my toddler is getting into mischief\"), with the MLLM analyzing\nthe camera feed and responding within seconds. In a formative study, we found\nthat users saw substantial value in defining their own sensors, yet struggled\nto articulate their unique personal requirements and debug the sensors through\nprompting alone. To address these challenges, we developed Gensors, a system\nthat empowers users to define customized sensors supported by the reasoning\ncapabilities of MLLMs. Gensors 1) assists users in eliciting requirements\nthrough both automatically-generated and manually created sensor criteria, 2)\nfacilitates debugging by allowing users to isolate and test individual criteria\nin parallel, 3) suggests additional criteria based on user-provided images, and\n4) proposes test cases to help users \"stress test\" sensors on potentially\nunforeseen scenarios. In a user study, participants reported significantly\ngreater sense of control, understanding, and ease of communication when\ndefining sensors using Gensors. Beyond addressing model limitations, Gensors\nsupported users in debugging, eliciting requirements, and expressing unique\npersonal requirements to the sensor through criteria-based reasoning; it also\nhelped uncover users' \"blind spots\" by exposing overlooked criteria and\nrevealing unanticipated failure modes. Finally, we discuss how unique\ncharacteristics of MLLMs--such as hallucinations and inconsistent\nresponses--can impact the sensor-creation process. These findings contribute to\nthe design of future intelligent sensing systems that are intuitive and\ncustomizable by everyday users.",
        "With the successful demonstration of transversal CNOTs in many recent\nexperiments, it is the right moment to examine its implications on one of the\nmost critical parts of fault-tolerant computation -- magic state preparation.\nUsing an algorithm that can recompile and simplify a circuit of consecutive\nmulti-qubit phase rotations, we manage to construct fault-tolerant circuits for\nCCZ, CS and T states with minimal T-depth and also much lower CNOT depths and\nqubit counts than before. These circuits can play crucial roles in\nfault-tolerant computation with transversal CNOTs, and we hope that the\nalgorithms and methods developed in this paper can be used to further simplify\nother protocols in similar contexts.",
        "Remote work and online courses have become important methods of knowledge\ndissemination, leading to a large number of document-based instructional\nvideos. Unlike traditional video datasets, these videos mainly feature\nrich-text images and audio that are densely packed with information closely\ntied to the visual content, requiring advanced multimodal understanding\ncapabilities. However, this domain remains underexplored due to dataset\navailability and its inherent complexity. In this paper, we introduce the\nDocVideoQA task and dataset for the first time, comprising 1454 videos across\n23 categories with a total duration of about 828 hours. The dataset is\nannotated with 154k question-answer pairs generated manually and via GPT,\nassessing models' comprehension, temporal awareness, and modality integration\ncapabilities. Initially, we establish a baseline using open-source MLLMs.\nRecognizing the challenges in modality comprehension for document-centric\nvideos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances\nunimodal feature extraction with diverse instruction-tuning data and employs\ncontrastive learning to strengthen modality integration. Through fine-tuning,\nthe LLM is equipped with audio-visual capabilities, leading to significant\nimprovements in document-centric video understanding. Extensive testing on the\nDocVideoQA dataset shows that DV-LLaMA significantly outperforms existing\nmodels. We'll release the code and dataset to facilitate future research.",
        "In 1980 P. Tukia and J. V\\\"{a}is\\\"{a}l\\\"{a} in seminal paper [P. Tukia and J.\nV\\\"ais\\\"al\\\"a, Quasisymmetric embeddings of metric spaces, Ann. Acad. Sci.\nFenn., Ser. A I, Math. 5, 97--114 (1980)] extended a concept of quasisymmetric\nmapping known from the theory of quasiconformal mappings to the case of general\nmetric spaces. They also found an estimation for the ratio of diameters of two\nsubsets which are images of two bounded subsets of a metric space under a\nquasisymmetric mapping. We improve this estimation for the case of ultrametric\nspaces. It was also shown that the image of an ultrametric space under an\n$\\eta$-quasisymmetric mapping with $\\eta(1)=1$ is again an ultrametric space.\nIn the case of finite ultrametric spaces it is proved that such mappings are\nball-preserving.",
        "In this paper, a new variant of an algorithm for normalized cross-correlation\n(NCC) is proposed in the context of template matching in images. The proposed\nalgorithm is based on the precomputation of a template image approximation,\nenabling more efficient calculation of approximate NCC with the source image\nthan using the original template for exact NCC calculation. The approximate\ntemplate is precomputed from the template image by a split-and-merge approach,\nresulting in a decomposition to axis-aligned rectangular segments, whose sizes\ndepend on per-segment pixel intensity variance. In the approximate template,\neach segment is assigned the mean grayscale value of the corresponding pixels\nfrom the original template. The proposed algorithm achieves superior\ncomputational performance with negligible NCC approximation errors compared to\nthe well-known Fast Fourier Transform (FFT)-based NCC algorithm, when applied\non less visually complex and\/or smaller template images. In other cases, the\nproposed algorithm can maintain either computational performance or NCC\napproximation error within the range of the FFT-based algorithm, but not both.",
        "Visual Question Answering (VQA) is an interdisciplinary field that bridges\nthe gap between computer vision (CV) and natural language processing(NLP),\nenabling Artificial Intelligence(AI) systems to answer questions about images.\nSince its inception in 2015, VQA has rapidly evolved, driven by advances in\ndeep learning, attention mechanisms, and transformer-based models. This survey\ntraces the journey of VQA from its early days, through major breakthroughs,\nsuch as attention mechanisms, compositional reasoning, and the rise of\nvision-language pre-training methods. We highlight key models, datasets, and\ntechniques that shaped the development of VQA systems, emphasizing the pivotal\nrole of transformer architectures and multimodal pre-training in driving recent\nprogress. Additionally, we explore specialized applications of VQA in domains\nlike healthcare and discuss ongoing challenges, such as dataset bias, model\ninterpretability, and the need for common-sense reasoning. Lastly, we discuss\nthe emerging trends in large multimodal language models and the integration of\nexternal knowledge, offering insights into the future directions of VQA. This\npaper aims to provide a comprehensive overview of the evolution of VQA,\nhighlighting both its current state and potential advancements.",
        "In this paper, we conduct a large-scale field experiment to investigate the\nmanipulability of prediction markets. The main experiment involves randomly\nshocking prices across 817 separate markets; we then collect hourly price data\nto examine whether the effects of these shocks persist over time. We find that\nprediction markets can be manipulated: the effects of our trades are visible\neven 60 days after they have occurred. However, as predicted by our model, the\neffects of the manipulations somewhat fade over time. Markets with more\ntraders, greater trading volume, and an external source of probability\nestimates are harder to manipulate.",
        "Recent findings in orbitronics pointed out large current-induced torques\noriginating, in the current understanding, from incident orbital currents.\nThese are generated by orbital Rashba-Edelstein effect (OREE) produced at the\ninterface between some light metal and oxides films e.g. by naturally oxidized\ncopper layer (Cu*). In the present work, by using second harmonic Hall\ntechniques, we determine the ratio of orbital vs spin currents exerting torques\non thin transition metals Co ferromagnet in systems using an orbit-to-spin Pt\nconverter as interlayer with Cu*. Our results quantifying damping like torques\nshow that both orbital and spin currents are enhanced in these systems.\nMoreover, the experimental determination of the decoherence length in a sample\nseries with varying Co thickness clearly demonstrates the interfacial\ngeneration of the orbital currents in Cu* by Orbital Rashba-Edelstein effects\n(REE) leading to subsequent magnetic torque in Co over a typical lengthscale of\nseveral nanometers",
        "We show that under mild set theoretic hypotheses we have rigidity for\nalgebras of continuous functions over Higson coronas, topological spaces\narising in coarse geometry. In particular, we show that under $\\mathsf{OCA}$\nand $\\mathsf {MA}_{\\aleph_1}$, if two uniformly locally finite metric spaces\n$X$ and $Y$ have homeomorphic Higson coronas $\\nu X$ and $\\nu Y$, then $X$ and\n$Y$ are coarsely equivalent, a statement which provably does not follow from\n$\\mathsf{ZFC}$ alone.",
        "As an analog to the Jacquet-Rallis fundamental lemma that appears in the\nrelative trace formula approach to the Gan-Gross-Prasad conjectures, the\narithmetic fundamental lemma was proposed by Wei Zhang and used in an approach\nto the arithmetic Gan-Gross-Prasad conjectures. The Jacquet-Rallis fundamental\nlemma was recently generalized by Spencer Leslie to a statement holding for the\nfull spherical Hecke algebra. In the same spirit, Li, Rapoport, and Zhang have\nrecently formulated a conjectural generalization of the arithmetic fundamental\nlemma to the full spherical Hecke algebra. This paper formulates another\nanalogous conjecture for the semi-Lie version of the arithmetic fundamental\nlemma proposed by Yifeng Liu. Then this paper produces explicit formulas for\nparticular cases of the weighted orbital integrals in the two conjectures\nmentioned above, and proves the first non-trivial case of the conjecture.",
        "The robots.txt file, introduced as part of the Robots Exclusion Protocol in\n1994, provides webmasters with a mechanism to communicate access permissions to\nautomated bots. While broadly adopted as a community standard, the legal\nliabilities associated with violating robots.txt remain ambiguous. The rapid\nrise of large language models, which depend on extensive datasets for training,\nhas amplified these challenges, prompting webmasters to increasingly use\nrobots.txt to restrict the activities of bots engaged in large-scale data\ncollection. This paper clarifies the liabilities associated with robots.txt\nwithin the contexts of contract, copyright, and tort law. Drawing on key cases,\nlegal principles, and scholarly discourse, it proposes a legal framework for\nweb scraping disputes. It also addresses the growing fragmentation of the\ninternet, as restrictive practices by webmasters threaten the principles of\nopenness and collaboration. Through balancing innovation with accountability,\nthis paper offers insights to ensure that robots.txt remains an equitable\nprotocol for the internet and thus contributes to digital governance in the age\nof AI.",
        "Simulating object dynamics from real-world perception shows great promise for\ndigital twins and robotic manipulation but often demands labor-intensive\nmeasurements and expertise. We present a fully automated Real2Sim pipeline that\ngenerates simulation-ready assets for real-world objects through robotic\ninteraction. Using only a robot's joint torque sensors and an external camera,\nthe pipeline identifies visual geometry, collision geometry, and physical\nproperties such as inertial parameters. Our approach introduces a general\nmethod for extracting high-quality, object-centric meshes from photometric\nreconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing\nalpha-transparent training while explicitly distinguishing foreground\nocclusions from background subtraction. We validate the full pipeline through\nextensive experiments, demonstrating its effectiveness across diverse objects.\nBy eliminating the need for manual intervention or environment modifications,\nour pipeline can be integrated directly into existing pick-and-place setups,\nenabling scalable and efficient dataset creation.",
        "Multi-label learning (MLL) has gained attention for its ability to represent\nreal-world data. Label Distribution Learning (LDL), an extension of MLL to\nlearning from label distributions, faces challenges in collecting accurate\nlabel distributions. To address the issue of biased annotations, based on the\nlow-rank assumption, existing works recover true distributions from biased\nobservations by exploring the label correlations. However, recent evidence\nshows that the label distribution tends to be full-rank, and naive apply of\nlow-rank approximation on biased observation leads to inaccurate recovery and\nperformance degradation. In this paper, we address the LDL with biased\nannotations problem from a novel perspective, where we first degenerate the\nsoft label distribution into a hard multi-hot label and then recover the true\nlabel information for each instance. This idea stems from an insight that\nassigning hard multi-hot labels is often easier than assigning a soft label\ndistribution, and it shows stronger immunity to noise disturbances, leading to\nsmaller label bias. Moreover, assuming that the multi-label space for\npredicting label distributions is low-rank offers a more reasonable approach to\ncapturing label correlations. Theoretical analysis and experiments confirm the\neffectiveness and robustness of our method on real-world datasets."
      ]
    }
  },
  {
    "id":2411.0656,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Tracing Power With Circuit Theory",
    "start_abstract":"Power tracing is the task of disaggregating power injection a generator (or load) into sum constituent components that can unambiguously be attributed to loads (generators) and losses. Applications range broad spectrum of: transmission services pricing, loss allocation in distribution networks, fixed-cost allocation, modelling bilateral transactions, financial storage rights. This paper develops an analytical approach leveraging elementary circuit laws. The method rigorous from system-theoretic vantage point, it yields unambiguous results are consistent with constitutive principles describe steady-state behaviour networks. Moreover, implemented limited computational burden, applies networks arbitrary topologies, preserves coupling between activeand reactive-power injections. Numerical experiments indicate given solved power-flow solution, disaggregations computed for test system 2383 buses, 327 generators, 2056 4.34 s on personal computer, hence establishing scalability. Furthermore, applications demonstrated case studies focused quantifying impact distributed generation extracting nodal contributions respectively.",
    "start_categories":[
      "Digital Circuits"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Carbon-Aware Optimal Power Flow"
      ],
      "abstract":[
        "To facilitate effective decarbonization of the electric power sector, this paper introduces generic Carbon-aware Optimal Power Flow (C-OPF) method for system decision-making that considers demand-side carbon accounting and emission management. Built upon classic optimal flow (OPF) model, C-OPF incorporates equations constraints, as well carbon-related objectives, to jointly optimize flow. In particular, establishes invertibility matrix proposes modeling linearization techniques address issues undetermined directions bilinear terms in model. Additionally, two novel models, together with schemes, energy storage systems are developed integrated into Numerical simulations demonstrate characteristics effectiveness method, comparison OPF solutions."
      ],
      "categories":[
        "physics.soc-ph"
      ]
    },
    "list":{
      "title":[
        "DUAL: Diversity and Uncertainty Active Learning for Text Summarization",
        "The global representation fibered ring",
        "Analysis of Niobium Electropolishing Using a Generalized Distribution of\n  Relaxation Times Method",
        "Rota-Baxter operators of nonzero weight on the split Cayley-Dickson\n  algebra",
        "Quantum Neural Networks for Cloud Cover Parameterizations in Climate\n  Models",
        "The existence of pyramidal Steiner triple systems over abelian groups",
        "Molecular Pseudorotation in Phthalocyanines as a Tool for Magnetic Field\n  Control at the Nanoscale",
        "Advanced Deep Learning Techniques for Analyzing Earnings Call\n  Transcripts: Methodologies and Applications",
        "Ill-Posedness of the Incompressible Euler--Maxwell Equations in the\n  Yudovich Class",
        "Pessimistic bilevel optimization approach for decision-focused learning",
        "Beyond Interpolation: Extrapolative Reasoning with Reinforcement\n  Learning and Graph Neural Networks",
        "Diffusion-aware Censored Gaussian Processes for Demand Modelling",
        "Drop-Upcycling: Training Sparse Mixture of Experts with Partial\n  Re-initialization",
        "Strategy Coopetition Explains the Emergence and Transience of In-Context\n  Learning",
        "A deep learning approach to inverse medium scattering: Learning\n  regularizers from a direct imaging method",
        "Quantum oscillation studies of the nodal line semimetal Ni3In2S2-xSex",
        "Proximity-Induced Nodal Metal in an Extremely Underdoped CuO$_2$ Plane\n  in Triple-Layer Cuprates",
        "Infrared spectroscopy of astrophysical ice analogs at oblique angles",
        "Hamiltonian Lattice Gauge Theories: emergent properties from Tensor\n  Network methods",
        "Distributed LLMs and Multimodal Large Language Models: A Survey on\n  Advances, Challenges, and Future Directions",
        "Advancing Singlish Understanding: Bridging the Gap with Datasets and\n  Multimodal Models",
        "Optical conductivity of the topologically-nontrivial MXenes,\n  Mo$_2$HfC$_2$O$_2$ and W$_2$HfC$_2$O$_2$: first-principles calculation and\n  effective model analysis",
        "Low rank MSO",
        "Countably compact inverse semigroups and Nyikos problem",
        "In Specs we Trust? Conformance-Analysis of Implementation to\n  Specifications in Node-RED and Associated Security Risks",
        "RNN-DAS: A New Deep Learning Approach for Detection and Real-Time\n  Monitoring of Volcano-Tectonic Events Using Distributed Acoustic Sensing",
        "Stability of multi-state configurations of fuzzy dark matter",
        "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
        "Adaptive Noise-Tolerant Network for Image Segmentation"
      ],
      "abstract":[
        "With the rise of large language models, neural text summarization has\nadvanced significantly in recent years. However, even state-of-the-art models\ncontinue to rely heavily on high-quality human-annotated data for training and\nevaluation. Active learning is frequently used as an effective way to collect\nsuch datasets, especially when annotation resources are scarce. Active learning\nmethods typically prioritize either uncertainty or diversity but have shown\nlimited effectiveness in summarization, often being outperformed by random\nsampling. We present Diversity and Uncertainty Active Learning (DUAL), a novel\nalgorithm that combines uncertainty and diversity to iteratively select and\nannotate samples that are both representative of the data distribution and\nchallenging for the current model. DUAL addresses the selection of noisy\nsamples in uncertainty-based methods and the limited exploration scope of\ndiversity-based methods. Through extensive experiments with different\nsummarization models and benchmark datasets, we demonstrate that DUAL\nconsistently matches or outperforms the best performing strategies. Using\nvisualizations and quantitative metrics, we provide valuable insights into the\neffectiveness and robustness of different active learning strategies, in an\nattempt to understand why these strategies haven't performed consistently in\ntext summarization. Finally, we show that DUAL strikes a good balance between\ndiversity and robustness.",
        "In this paper, we combine the concepts of the fibered Burnside ring and the\ncharacter ring, viewing them as fibered biset functors, into what we call the\nglobal representation fibered ring of a finite group. We compute all ring\nhomomorphisms from this ring to the complex numbers, determine its spectrum and\nits connected components, and identify the primitive idempotents of this ring\ntensor with $\\mathbb{Q}$ and its conductors.",
        "Using electrochemical impedance spectroscopy, we have devised a method of\nsensing the microscopic surface conditions on the surface of niobium as it is\nundergoing an electrochemical polishing (EP) treatment. The method uses\nelectrochemical impedance spectroscopy (EIS) to gather information on the\nsurface state of the electrode without disrupting the polishing reaction. The\nEIS data is analyzed using a so-called distribution of relaxation times (DRT)\nmethod. Using DRT, the EIS data can be deconvolved into discrete relaxation\ntime peaks without any a priori knowledge of the electrode dynamics. By\nanalyzing the relaxation time peaks, we are able to distinguish two distinct\nmodes of the EP reaction. As the polishing voltage is increased, the electrode\ntransitions from the low voltage EP mode, characterized by a single relaxation\ntime peaks, to the high voltage EP mode, characterized by two relaxation time\npeaks. We theorize that this second peak is caused by the formation of an oxide\nlayer on the electrode. We also find that this oxide induced peak transitions\nfrom to a negative relaxation time, which is indicative of a blocking electrode\nprocess. By analyzing EPed samples, we show that samples polished in the low\nvoltage mode have significantly higher surface roughness due to grain etching\nand faceting. We find that the surface roughness of the samples only improves\nwhen the oxide film peak is present and in the negative relaxation time region.\nThis shows that EIS combined with DRT analysis can be used to predict etching\non EPed Nb. This method can also be performed before or during the EP, which\ncould allow for adjustment of polishing parameters to guarantee a smooth cavity\nsurface finish.",
        "We describe Rota-Baxter operators on split octonions. It turns out that up to\nsome transformations there exists exactly one such non-splitting operator over\nany field. We also obtain a description of all decompositions of split\noctonions over a quadratically closed field of characteristic different from 2\ninto a sum of two subalgebras, which describes the splitting Rota-Baxter\noperators. It completes the classification of Rota-Baxter operators on\ncomposition algebras of any weight.",
        "Long-term climate projections require running global Earth system models on\ntimescales of hundreds of years and have relatively coarse resolution (from 40\nto 160 km in the horizontal) due to their high computational costs. Unresolved\nsubgrid-scale processes, such as clouds, are described in a semi-empirical\nmanner by so called parameterizations, which are a major source of uncertainty\nin climate projections. Machine learning models trained on short\nhigh-resolution climate simulations are promising candidates to replace\nconventional parameterizations. In this work, we explore the potential of\nquantum machine learning (QML), and in particular quantum neural networks\n(QNNs), to develop cloud cover parameterizations. QNNs differ from their\nclassical counterparts, and their potentially high expressivity turns them into\npromising tools for accurate data-driven schemes to be used in climate models.\nWe perform an extensive comparative analysis between several QNNs and classical\nneural networks (NNs), by training both ansatzes on data coming from\nhigh-resolution simulations with the ICOsahedral Non-hydrostatic weather and\nclimate model (ICON). Our results show that the overall performance of the\ninvestigated QNNs is comparable to that of classical NNs of similar size, i.e.,\nwith the same number of trainable parameters, with both ansatzes outperforming\nstandard parameterizations used in climate models. Our study includes an\nanalysis of the generalization ability of the models as well as the geometrical\nproperties of their optimization landscape. We also investigate the effects of\nfinite sampling noise, and show that the training and the predictions of the\nQNNs are stable even in this noisy setting. These results demonstrate the\napplicability of QML to learn meaningful patterns in climate data, and are thus\nrelevant for a broad range of problems within the climate modeling community.",
        "A Steiner triple system STS$(v)$ is called $f$-pyramidal if it has an\nautomorphism group fixing $f$ points and acting sharply transitively on the\nremaining ones. In this paper, we focus on the STSs that are $f$-pyramidal over\nsome abelian group. Their existence has been settled only for the smallest\nadmissible values of $f$, that is, $f=0,1,3$.\n  In this paper, we complete this result and determine, for every $f>3$, the\nspectrum of values $(f,v)$ for which there is an $f$-pyramidal STS$(v)$ over an\nabelian group. This result is obtained by constructing difference families\nrelative to a suitable partial spread.",
        "Metal phthalocyanines, a highly versatile class of aromatic, planar,\nmacrocyclic molecules with a chelated central metal ion, are topical objects of\nongoing research and particularly interesting due to their magnetic properties.\nHowever, while current focus lies almost exclusively on spin-Zeeman-related\neffects, the high symmetry of the molecule and its circular shape suggests the\nexploitation of light-induced excitation of twofold degenerate vibrational\nstates in order to generate, switch and manipulate magnetic fields at the\nnanoscale. The underlying mechanism is a molecular pseudorotation that can be\ntriggered by infrared pulses and gives rise to a quantized, small but\ncontrollable magnetic dipole moment. We investigate the optical stimulation of\nvibrationally-induced molecular magnetism and estimate changes in the magnetic\nshielding constants for confirmation by future experiments.",
        "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.",
        "It was shown recently by Ars\\'enio and the author that the two-dimensional\nincompressible Euler--Maxwell system is globally well-posed in the Yudovich\nclass, provided that the electromagnetic field enjoys appropriate conditions,\nincluding the Normal Structure. In this paper, we prove that this assumption is\nsharp, in the sense that the Euler--Maxwell system becomes ill-posed in the\nYudovich class for initial data that do not obey the Normal Structure\ncondition. The proof applies to both the whole plane and the two-dimensional\ntorus, and holds for any value of the speed of light $c\\in (0,\\infty)$. This is\nachieved by expanding the magnetic field around a horizontal background and\nshowing that the Lorentz force can be decomposed into two parts: the first is\nin the form of a singular operator acting on the vorticity, and the second, a\n\"remainder\", is of lower order when analyzed in a specific time regime.",
        "The recent interest in contextual optimization problems, where randomness is\nassociated with side information, has led to two primary strategies for\nformulation and solution. The first, estimate-then-optimize, separates the\nestimation of the problem's parameters from the optimization process. The\nsecond, decision-focused optimization, integrates the optimization problem's\nstructure directly into the prediction procedure. In this work, we propose a\npessimistic bilevel approach for solving general decision-focused formulations\nof combinatorial optimization problems. Our method solves an\n$\\varepsilon$-approximation of the pessimistic bilevel problem using a\nspecialized cut generation algorithm. We benchmark its performance on the 0-1\nknapsack problem against estimate-then-optimize and decision-focused methods,\nincluding the popular SPO+ approach. Computational experiments highlight the\nproposed method's advantages, particularly in reducing out-of-sample regret.",
        "Despite incredible progress, many neural architectures fail to properly\ngeneralize beyond their training distribution. As such, learning to reason in a\ncorrect and generalizable way is one of the current fundamental challenges in\nmachine learning. In this respect, logic puzzles provide a great testbed, as we\ncan fully understand and control the learning environment. Thus, they allow to\nevaluate performance on previously unseen, larger and more difficult puzzles\nthat follow the same underlying rules. Since traditional approaches often\nstruggle to represent such scalable logical structures, we propose to model\nthese puzzles using a graph-based approach. Then, we investigate the key\nfactors enabling the proposed models to learn generalizable solutions in a\nreinforcement learning setting. Our study focuses on the impact of the\ninductive bias of the architecture, different reward systems and the role of\nrecurrent modeling in enabling sequential reasoning. Through extensive\nexperiments, we demonstrate how these elements contribute to successful\nextrapolation on increasingly complex puzzles.These insights and frameworks\noffer a systematic way to design learning-based systems capable of\ngeneralizable reasoning beyond interpolation.",
        "Inferring the true demand for a product or a service from aggregate data is\noften challenging due to the limited available supply, thus resulting in\nobservations that are censored and correspond to the realized demand, thereby\nnot accounting for the unsatisfied demand. Censored regression models are able\nto account for the effect of censoring due to the limited supply, but they\ndon't consider the effect of substitutions, which may cause the demand for\nsimilar alternative products or services to increase. This paper proposes\nDiffusion-aware Censored Demand Models, which combine a Tobit likelihood with a\ngraph diffusion process in order to model the latent process of transfer of\nunsatisfied demand between similar products or services. We instantiate this\nnew class of models under the framework of GPs and, based on both simulated and\nreal-world data for modeling sales, bike-sharing demand, and EV charging\ndemand, demonstrate its ability to better recover the true demand and produce\nmore accurate out-of-sample predictions.",
        "The Mixture of Experts (MoE) architecture reduces the training and inference\ncost significantly compared to a dense model of equivalent capacity. Upcycling\nis an approach that initializes and trains an MoE model using a pre-trained\ndense model. While upcycling leads to initial performance gains, the training\nprogresses slower than when trained from scratch, leading to suboptimal\nperformance in the long term. We propose Drop-Upcycling - a method that\neffectively addresses this problem. Drop-Upcycling combines two seemingly\ncontradictory approaches: utilizing the knowledge of pre-trained dense models\nwhile statistically re-initializing some parts of the weights. This approach\nstrategically promotes expert specialization, significantly enhancing the MoE\nmodel's efficiency in knowledge acquisition. Extensive large-scale experiments\ndemonstrate that Drop-Upcycling significantly outperforms previous MoE\nconstruction methods in the long term, specifically when training on hundreds\nof billions of tokens or more. As a result, our MoE model with 5.9B active\nparameters achieves comparable performance to a 13B dense model in the same\nmodel family, while requiring approximately 1\/4 of the training FLOPs. All\nexperimental resources, including source code, training data, model checkpoints\nand logs, are publicly available to promote reproducibility and future research\non MoE.",
        "In-context learning (ICL) is a powerful ability that emerges in transformer\nmodels, enabling them to learn from context without weight updates. Recent work\nhas established emergent ICL as a transient phenomenon that can sometimes\ndisappear after long training times. In this work, we sought a mechanistic\nunderstanding of these transient dynamics. Firstly, we find that, after the\ndisappearance of ICL, the asymptotic strategy is a remarkable hybrid between\nin-weights and in-context learning, which we term \"context-constrained\nin-weights learning\" (CIWL). CIWL is in competition with ICL, and eventually\nreplaces it as the dominant strategy of the model (thus leading to ICL\ntransience). However, we also find that the two competing strategies actually\nshare sub-circuits, which gives rise to cooperative dynamics as well. For\nexample, in our setup, ICL is unable to emerge quickly on its own, and can only\nbe enabled through the simultaneous slow development of asymptotic CIWL. CIWL\nthus both cooperates and competes with ICL, a phenomenon we term \"strategy\ncoopetition.\" We propose a minimal mathematical model that reproduces these key\ndynamics and interactions. Informed by this model, we were able to identify a\nsetup where ICL is truly emergent and persistent.",
        "This paper aims to solve numerically the two-dimensional inverse medium\nscattering problem with far-field data. This is a challenging task due to the\nsevere ill-posedness and strong nonlinearity of the inverse problem. As already\nknown, it is necessary but also difficult numerically to employ an appropriate\nregularization strategy which effectively incorporates certain a priori\ninformation of the unknown scatterer to overcome the severe ill-posedness of\nthe inverse problem. In this paper, we propose to use a deep learning approach\nto learn the a priori information of the support of the unknown scatterer from\na direct imaging method. Based on the learned a priori information, we propose\ntwo inversion algorithms for solving the inverse problem. In the first one, the\nlearned a priori information is incorporated into the projected Landweber\nmethod. In the second one, the learned a priori information is used to design\nthe regularization functional for the regularized variational formulation of\nthe inverse problem which is then solved with a traditional iteration\nalgorithm. Extensive numerical experiments show that our inversion algorithms\nprovide good reconstruction results even for the high contrast case and have a\nsatisfactory generalization ability.",
        "Ternary shandite compounds with the general formula T3M2X2 (T = Ni, Co, Rh or\nPd; M = Sn, In or Pb and X = S or Se) have emerged as a large pool of\ntopological semimetals. This family of compounds hosts different topological\nphases for various combinations of T, M and X. This paper reports the\nobservation of quantum oscillations under the high magnetic fields in\nNi3In2S2-xSex single crystals. Angular dependence of oscillation frequency\nsuggests an evolution of the Fermi surface from three-dimensional to\ntwo-dimensional on Se substitution for S in Ni3In2S2. The effective mass\nobtained for each composition by fitting the oscillation amplitude with the\nLifshitz-Kosevich formula, shows no significant change, suggesting that the\ntopological phase might be relatively robust against enhanced SOC upon Se\ndoping in Ni3In2S2.",
        "ARPES studies have established that the high-$T_c$ cuprates with single and\ndouble CuO$_2$ layers evolve from the Mott insulator to the pseudogap state\nwith a Fermi arc, on which the superconducting (SC) gap opens. In four- to\nsix-layer cuprates, on the other hand, small hole Fermi pockets are formed in\nthe innermost CuO$_2$ planes, indicating antiferromagnetism. Here, we performed\nARPES studies on the triple-layer Bi$_2$Sr$_2$Ca$_2$Cu$_3$O$_{10+\\delta}$ over\na wide doping range, and found that, although the doping level of the inner\nCuO$_2$ plane was extremely low in underdoped samples, the $d$-wave SC gap was\nenhanced to the unprecedentedly large value of $\\Delta_0\\sim$100 meV at the\nantinode and persisted well above $T_{{c}}$ without the appearance of a Fermi\narc, indicating a robust ``nodal metal''. We attribute the nodal metallic\nbehavior to the unique local environment of the inner clean CuO$_2$ plane in\nthe triple-layer cuprates, sandwiched by nearly optimally-doped two outer\nCuO$_2$ planes and hence subject to strong proximity effect from both sides. In\nthe nodal metal, quasiparticle peaks showed electron-hole symmetry, suggesting\n$d$-wave pairing fluctuations. Thus the proximity effect on the innermost\nCuO${_2}$ plane is the strongest in the triple-layer cuprates, which explains\nwhy the $T_c$ reaches the maximum at the layer number of three in every\nmulti-layer cuprate family.",
        "In astrochemical exploration, infrared (IR) spectroscopy is vital for\nunderstanding the composition and structure of ice in various space\nenvironments. This article explores the impact of incident angles on IR\nspectroscopy, focusing on molecular components present in interstellar and\ncircumstellar ice mantles such as CO, CO$_2$, H$_2$O, CH$_3$OH, NH$_3$, CH$_4$,\nH$_2$S. The experiment involves changing the angle at which the infrared beam\nhits the surface used for ice deposition. It is important to measure the\ndensity of the ice layer accurately, especially for experiments that involve\nusing different angles in infrared spectroscopy. Furthermore, the experimental\nmethodology allowed us to derive the {\\it effective} refraction index values in\nthe infrared range for each ice component. Existing corrections typically\nconsider geometric configurations but overlook the refractive index of the ice\n($n$), a factor dependent on ice composition. The study reveals that the\nincident angle and the refractive index, determine the pathlength of the IR\nbeam across the ice sample. This insight challenges conventional corrections,\nimpacting the integrated absorption values of the IR bands and column\ndensities. In addition, for certain ice components, variations in the incidence\nangle affect the longitudinal (LO) and transverse (TO) optical modes of the\nice, leading to observable changes in the IR band profiles that provide\ninformation on the amorphous or crystalline structure of the ice. The practical\nimplications of this work apply to experimental setups where normal IR\nmeasurements are unfeasible. Researchers using, for example, the standard\n45$^{\\circ}$ angle for IR spectroscopy, will benefit from a more accurate\nestimation of ice column density.",
        "This thesis develops advanced Tensor Network (TN) methods to address\nHamiltonian Lattice Gauge Theories (LGTs), overcoming limitations in real-time\ndynamics and finite-density regimes. A novel dressed-site formalism is\nintroduced, enabling efficient truncation of gauge fields while preserving\ngauge invariance for both Abelian and non-Abelian theories. This formalism is\nsuccessfully applied to SU(2) Yang-Mills LGTs in two dimensions, providing the\nfirst TN simulations of this system and revealing critical aspects of its phase\ndiagram and non-equilibrium behavior, such as a Quantum Many-Body (QMB)\nscarring dynamics. A generalization of the dressed-site formalism is proposed\nthrough a new fermion-to-qubit mapping for general lattice fermion theories,\nrevealing powerful for classical and quantum simulations. Numerical\ninnovations, including the use of optimal space-filling curves such as the\nHilbert curve to preserve locality in high-dimensional simulations, further\nenhance the efficiency of these methods. Together with high-performance\ncomputing techniques, these advances open current and future development\npathways toward optimized, efficient, and faster simulations on scales\ncomparable to Monte Carlo state-of-the-art.",
        "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs.",
        "Singlish, a Creole language rooted in English, is a key focus in linguistic\nresearch within multilingual and multicultural contexts. However, its spoken\nform remains underexplored, limiting insights into its linguistic structure and\napplications. To address this gap, we standardize and annotate the largest\nspoken Singlish corpus, introducing the Multitask National Speech Corpus\n(MNSC). These datasets support diverse tasks, including Automatic Speech\nRecognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue\nSummarization (SDS), and Paralinguistic Question Answering (PQA). We release\nstandardized splits and a human-verified test set to facilitate further\nresearch. Additionally, we propose SingAudioLLM, a multi-task multimodal model\nleveraging multimodal large language models to handle these tasks concurrently.\nExperiments reveal our models adaptability to Singlish context, achieving\nstate-of-the-art performance and outperforming prior models by 10-30% in\ncomparison with other AudioLLMs and cascaded solutions.",
        "The optical conductivity and the relevant electronic excitation processes are\ninvestigated in topologically-nontrivial MXenes, Mo$_2$HfC$_2$O$_2$ and\nW$_2$HfC$_2$O$_2$, utilizing first-principles calculation and effective model\nanalysis. The numerical calculation based on the first-principles band\nstructure reveals the presence of several characteristic features in the\nspectrum of optical conductivity as a function of photon energy. The drastic\ndependence on the photon polarization angle is also presented in terms of\napparent features. In this paper, an effective model is also generated\nreferring to the crystal symmetries and applied to reveal the microscopic\norigin of the characteristics. Then, it is shown that some features are\nstrongly related to parity inversion between the conduction and valence bands,\nthe key signature in electronic structures of topologically nontrivial\ninsulators.",
        "We introduce a new logic for describing properties of graphs, which we call\nlow rank MSO. This is the fragment of monadic second-order logic in which set\nquantification is restricted to vertex sets of bounded cutrank. We prove the\nfollowing statements about the expressive power of low rank MSO.\n  - Over any class of graphs that is weakly sparse, low rank MSO has the same\nexpressive power as separator logic. This equivalence does not hold over all\ngraphs.\n  - Over any class of graphs that has bounded VC dimension, low rank MSO has\nthe same expressive power as flip-connectivity logic. This equivalence does not\nhold over all graphs.\n  - Over all graphs, low rank MSO has the same expressive power as\nflip-reachability logic.\n  Here, separator logic is an extension of first-order logic by basic\npredicates for checking connectivity, which was proposed by Boja\\'nczyk [ArXiv\n2107.13953] and by Schirrmacher, Siebertz, and Vigny [ACM ToCL 2023].\nFlip-connectivity logic and flip-reachability logic are analogues of separator\nlogic suited for non-sparse graphs, which we propose in this work. In\nparticular, the last statement above implies that every property of undirected\ngraphs expressible in low rank MSO can be decided in polynomial time.",
        "A regular separable first-countable countably compact space is called a {\\em\nNyikos} space. In this paper, we give a partial solution to an old problem of\nNyikos by showing that each locally compact Nyikos inverse topological\nsemigroup is compact. Also, we show that a topological semigroup $S$ that\ncontains a dense inverse subsemigroup is a topological inverse semigroup,\nprovided (i) $S$ is compact, or (ii) $S$ is countably compact and sequential.\nThe latter result solves a problem of Banakh and Pastukhova and provides the\nautomatic continuity of inversion in certain compact-like inverse semigroups.",
        "Low-code development frameworks for IoT platforms offer a simple\ndrag-and-drop mechanism to create applications for the billions of existing IoT\ndevices without the need for extensive programming knowledge. The security of\nsuch software is crucial given the close integration of IoT devices in many\nhighly sensitive areas such as healthcare or home automation. Node-RED is such\na framework, where applications are built from nodes that are contributed by\nopen-source developers. Its reliance on unvetted open-source contributions and\nlack of security checks raises the concern that the applications could be\nvulnerable to attacks, thereby imposing a security risk to end users. The\nlow-code approach suggests, that many users could lack the technical knowledge\nto mitigate, understand, or even realize such security concerns. This paper\nfocuses on \"hidden\" information flows in Node-RED nodes, meaning flows that are\nnot captured by the specifications. They could (unknowingly or with malicious\nintent) cause leaks of sensitive information to unauthorized entities. We\nreport the results of a conformance analysis of all nodes in the Node-RED\nframework, for which we compared the numbers of specified inputs and outputs of\neach node against the number of sources and sinks detected with CodeQL. The\nresults show, that 55% of all nodes exhibit more possible flows than are\nspecified. A risk assessment of a subset of the nodes showed, that 28% of them\nare associated with a high severity and 36% with a medium severity rating.",
        "In this article, we present a novel Deep Learning model based on Recurrent\nNeural Networks (RNNs) with Long Short-Term Memory (LSTM) cells, designed as a\nreal-time Volcano-seismic Signal Recognition (VSR) system for Distributed\nAcoustic Sensing (DAS) measurements. The model was trained on an extensive\ndatabase of Volcano-Tectonic (VT) events derived from the co-eruptive\nseismicity of the 2021 La Palma eruption, recorded by a High-fidelity submarine\nDistributed Acoustic Sensing array (HDAS) near the eruption site. The features\nused for supervised model training, based on signal energy average in frequency\nbands, effectively enable the model to leverage spatial contextual information\nand the temporal evolution of volcano-seismic signals provided by the DAS\ntechnique. The proposed model not only detects the presence of VT events but\nalso analyzes their temporal evolution, selecting and classifying their\ncomplete waveforms with an accuracy of approximately 97% for correctly detected\nand classified VT events. Furthermore, the model has demonstrated robust\nperformance in generalizing to other time intervals and volcanoes, enabling\ncontinuous real-time monitoring of seismicity. Such results highlight the\npotential of using RNN-based approaches with LSTM cells for application to\nother active volcanoes, enabling fast, automatic analysis with low\ncomputational requirements and the need of minimal retraining, for the creation\nof labeled seismic catalogs directly from DAS measurements. This represents a\nsignificant advancement in the use of DAS technology as a viable tool to study\nactive volcanoes and their seismic activity.",
        "We study the stability properties of multi-state configurations of the\nSchr\\\"odinger-Poisson system without self-interaction, with monopolar and first\ndipolar components $(1,0,0)$+$(2,1,0)$. We show these configurations studied\nare stable using numerical simulations, and using criteria of stationarity,\nunitarity and time dependence consistency. The study covers a range of states\nwith monopolar to dipolar mass ratio between 47 and 0.17. The astrophysical\nimplication of this result is that this type of configurations is at least\nstable and can be considered physically sound in multi-state ultralight bosonic\ndark matter.",
        "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
        "Unlike image classification and annotation, for which deep network models\nhave achieved dominating superior performances compared to traditional computer\nvision algorithms, deep learning for automatic image segmentation still faces\ncritical challenges. One of such hurdles is to obtain ground-truth\nsegmentations as the training labels for deep network training. Especially when\nwe study biomedical images, such as histopathological images (histo-images), it\nis unrealistic to ask for manual segmentation labels as the ground truth for\ntraining due to the fine image resolution as well as the large image size and\ncomplexity. In this paper, instead of relying on clean segmentation labels, we\nstudy whether and how integrating imperfect or noisy segmentation results from\noff-the-shelf segmentation algorithms may help achieve better segmentation\nresults through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend\nthe noisy label deep learning to image segmentation with two novel aspects: (1)\nmultiple noisy labels can be integrated into one deep learning model; (2) noisy\nsegmentation modeling, including probabilistic parameters, is adaptive,\ndepending on the given testing image appearance. Implementation of the new ANTN\nmodel on both the synthetic data and real-world histo-images demonstrates its\neffectiveness and superiority over off-the-shelf and other existing\ndeep-learning-based image segmentation algorithms."
      ]
    }
  },
  {
    "id":2411.0656,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Carbon-Aware Optimal Power Flow",
    "start_abstract":"To facilitate effective decarbonization of the electric power sector, this paper introduces generic Carbon-aware Optimal Power Flow (C-OPF) method for system decision-making that considers demand-side carbon accounting and emission management. Built upon classic optimal flow (OPF) model, C-OPF incorporates equations constraints, as well carbon-related objectives, to jointly optimize flow. In particular, establishes invertibility matrix proposes modeling linearization techniques address issues undetermined directions bilinear terms in model. Additionally, two novel models, together with schemes, energy storage systems are developed integrated into Numerical simulations demonstrate characteristics effectiveness method, comparison OPF solutions.",
    "start_categories":[
      "physics.soc-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Tracing Power With Circuit Theory"
      ],
      "abstract":[
        "Power tracing is the task of disaggregating power injection a generator (or load) into sum constituent components that can unambiguously be attributed to loads (generators) and losses. Applications range broad spectrum of: transmission services pricing, loss allocation in distribution networks, fixed-cost allocation, modelling bilateral transactions, financial storage rights. This paper develops an analytical approach leveraging elementary circuit laws. The method rigorous from system-theoretic vantage point, it yields unambiguous results are consistent with constitutive principles describe steady-state behaviour networks. Moreover, implemented limited computational burden, applies networks arbitrary topologies, preserves coupling between activeand reactive-power injections. Numerical experiments indicate given solved power-flow solution, disaggregations computed for test system 2383 buses, 327 generators, 2056 4.34 s on personal computer, hence establishing scalability. Furthermore, applications demonstrated case studies focused quantifying impact distributed generation extracting nodal contributions respectively."
      ],
      "categories":[
        "Digital Circuits"
      ]
    },
    "list":{
      "title":[
        "On the Jucys-Murphy method and fusion procedure for the Sergeev\n  superalgebra",
        "Designing Telepresence Robots to Support Place Attachment",
        "Large Neighborhood Search and Bitmask Dynamic Programming for Wireless\n  Mobile Charging Electric Vehicle Routing Problems in Medical Transportation",
        "Maritime Mission Planning for Unmanned Surface Vessel using Large\n  Language Model",
        "Nonlinear multidomain model for nerve bundles with random structure",
        "Predicting Cognitive Decline: A Multimodal AI Approach to Dementia\n  Screening from Speech",
        "Visual Attention Exploration in Vision-Based Mamba Models",
        "DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe\n  Medication Recommendation",
        "Ego vs. Exo and Active vs. Passive: Investigating the Effects of\n  Viewpoint and Navigation on Spatial Immersion and Understanding in Immersive\n  Storytelling",
        "Assessing Autonomous Inspection Regimes: Active Versus Passive Satellite\n  Inspection",
        "Survey of image processing settings used for mammography systems in the\n  United Kingdom: how variable is it?",
        "R-equivalence classes of $\\mathrm{Rot} \\mathbb{E}^{2}$-colorings of\n  torus knots",
        "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More",
        "Dual Arm Steering of Deformable Linear Objects in 2-D and 3-D\n  Environments Using Euler's Elastica Solutions",
        "ClinText-SP and RigoBERTa Clinical: a new set of open resources for\n  Spanish Clinical NLP",
        "Unified Few-shot Crack Segmentation and its Precise 3D Automatic\n  Measurement in Concrete Structures",
        "Diffusion Adversarial Post-Training for One-Step Video Generation",
        "Exploring Interpretability for Visual Prompt Tuning with Hierarchical\n  Concepts",
        "Diagnostic tools for exploring differences in distributional properties\n  between two samples: nonparametric approach",
        "Quantum Fourier transform computational accuracy analysis",
        "Adaptive Meta-learning-based Adversarial Training for Robust Automatic\n  Modulation Classification",
        "MOFA: Discovering Materials for Carbon Capture with a GenAI- and\n  Simulation-Based Workflow",
        "Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as\n  an Adaptive Feature Model: Generalization and Adaptivity",
        "On the emergence and properties of weird quasiperiodic attractors",
        "Deep Learning within Tabular Data: Foundations, Challenges, Advances and\n  Future Directions",
        "Deep Distance Map Regression Network with Shape-aware Loss for\n  Imbalanced Medical Image Segmentation",
        "Model Tampering Attacks Enable More Rigorous Evaluations of LLM\n  Capabilities",
        "Causal Inference for Qualitative Outcomes",
        "Estimating unknown dynamics and cost as a bilinear system with\n  Koopman-based Inverse Optimal Control"
      ],
      "abstract":[
        "We use the Jucys-Murphy elements to construct a complete set of primitive\nidempotents for the Sergeev superalgebra ${\\mathcal S}_n$. We produce\nseminormal forms for the simple modules over ${\\mathcal S}_n$ and over the spin\nsymmetric group algebra with explicit constructions of basis vectors. We show\nthat the idempotents can also be obtained from a new version of the fusion\nprocedure.",
        "People feel attached to places that are meaningful to them, which\npsychological research calls \"place attachment.\" Place attachment is associated\nwith self-identity, self-continuity, and psychological well-being. Even small\ncues, including videos, images, sounds, and scents, can facilitate feelings of\nconnection and belonging to a place. Telepresence robots that allow people to\nsee, hear, and interact with a remote place have the potential to establish and\nmaintain a connection with places and support place attachment. In this paper,\nwe explore the design space of robotic telepresence to promote place\nattachment, including how users might be guided in a remote place and whether\nthey experience the environment individually or with others. We prototyped a\ntelepresence robot that allows one or more remote users to visit a place and be\nguided by a local human guide or a conversational agent. Participants were 38\nuniversity alumni who visited their alma mater via the telepresence robot. Our\nfindings uncovered four distinct user personas in the remote experience and\nhighlighted the need for social participation to enhance place attachment. We\ngenerated design implications for future telepresence robot design to support\npeople's connections with places of personal significance.",
        "The transition to electric vehicles (EVs) is critical to achieving\nsustainable transportation, but challenges such as limited driving range and\ninsufficient charging infrastructure have hindered the widespread adoption of\nEVs, especially in time-sensitive logistics such as medical transportation.\nThis paper presents a new model to break through this barrier by combining\nwireless mobile charging technology with optimization. We propose the Wireless\nMobile Charging Electric Vehicle Routing Problem (WMC-EVRP), which enables\nMedical Transportation Electric Vehicles (MTEVs) to be charged while traveling\nvia Mobile Charging Carts (MCTs). This eliminates the time wastage of stopping\nfor charging and ensures uninterrupted operation of MTEVs for such\ntime-sensitive transportation problems. However, in this problem, the decisions\nof these two types of heterogeneous vehicles are coupled with each other, which\ngreatly increases the difficulty of vehicle routing optimizations. To address\nthis complex problem, we develop a mathematical model and a tailored\nmeta-heuristic algorithm that combines Bit Mask Dynamic Programming (BDP) and\nLarge Neighborhood Search (LNS). The BDP approach efficiently optimizes\ncharging strategies, while the LNS framework utilizes custom operators to\noptimize the MTEV routes under capacity and synchronization constraints. Our\napproach outperforms traditional solvers in providing solutions for medium and\nlarge instances. Using actual hospital locations in Singapore as data, we\nvalidated the practical applicability of the model through extensive\nexperiments and provided important insights into minimizing costs and ensuring\nthe timely delivery of healthcare services.",
        "Unmanned Surface Vessels (USVs) are essential for various maritime\noperations. USV mission planning approach offers autonomous solutions for\nmonitoring, surveillance, and logistics. Existing approaches, which are based\non static methods, struggle to adapt to dynamic environments, leading to\nsuboptimal performance, higher costs, and increased risk of failure. This paper\nintroduces a novel mission planning framework that uses Large Language Models\n(LLMs), such as GPT-4, to address these challenges. LLMs are proficient at\nunderstanding natural language commands, executing symbolic reasoning, and\nflexibly adjusting to changing situations. Our approach integrates LLMs into\nmaritime mission planning to bridge the gap between high-level human\ninstructions and executable plans, allowing real-time adaptation to\nenvironmental changes and unforeseen obstacles. In addition, feedback from\nlow-level controllers is utilized to refine symbolic mission plans, ensuring\nrobustness and adaptability. This framework improves the robustness and\neffectiveness of USV operations by integrating the power of symbolic planning\nwith the reasoning abilities of LLMs. In addition, it simplifies the mission\nspecification, allowing operators to focus on high-level objectives without\nrequiring complex programming. The simulation results validate the proposed\napproach, demonstrating its ability to optimize mission execution while\nseamlessly adapting to dynamic maritime conditions.",
        "We present a derivation of a multidomain model for the electric potential in\nbundles of randomly distributed axons with different radii. The FitzHugh-Nagumo\ndynamics is assumed on the axons' membrane, and the conductivity depends\nnonlinearly on the electric field. Under ergodicity conditions, we study the\nasymptotic behavior of the potential in the bundle when the number of the axons\nin the bundle is sufficiently large and derive a macroscopic multidomain model\ndescribing the electrical activity of the bundle. Due to the randomness of\ngeometry, the effective intracellular potential is not deterministic but is\nshown to be a stationary function with realizations that are constant on axons'\ncross sections. The technique combines the stochastic two-scale convergence and\nthe method of monotone operators.",
        "Recent progress has been made in detecting early stage dementia entirely\nthrough recordings of patient speech. Multimodal speech analysis methods were\napplied to the PROCESS challenge, which requires participants to use audio\nrecordings of clinical interviews to predict patients as healthy control, mild\ncognitive impairment (MCI), or dementia and regress the patient's Mini-Mental\nState Exam (MMSE) scores. The approach implemented in this work combines\nacoustic features (eGeMAPS and Prosody) with embeddings from Whisper and\nRoBERTa models, achieving competitive results in both regression (RMSE: 2.7666)\nand classification (Macro-F1 score: 0.5774) tasks. Additionally, a novel\ntwo-tiered classification setup is utilized to better differentiate between MCI\nand dementia. Our approach achieved strong results on the test set, ranking\nseventh on regression and eleventh on classification out of thirty-seven teams,\nexceeding the baseline results.",
        "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
        "Medication Recommendation (MR) is a promising research topic which booms\ndiverse applications in the healthcare and clinical domains. However, existing\nmethods mainly rely on sequential modeling and static graphs for representation\nlearning, which ignore the dynamic correlations in diverse medical events of a\npatient's temporal visits, leading to insufficient global structural\nexploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is\nanother issue determining the utility of the MR systems. To address the\nchallenges mentioned above, this paper proposes a novel MR method with the\nintegration of dynamic networks and multi-view drug representations (DNMDR).\nSpecifically, weighted snapshot sequences for dynamic heterogeneous networks\nare constructed based on discrete visits in temporal EHRs, and all the dynamic\nnetworks are jointly trained to gain both structural correlations in diverse\nmedical events and temporal dependency in historical health conditions, for\nachieving comprehensive patient representations with both semantic features and\nstructural relationships. Moreover, combining the drug co-occurrences and\nadverse drug-drug interactions (DDIs) in internal view of drug molecule\nstructure and interactive view of drug pairs, the safe drug representations are\navailable to obtain high-quality medication combination recommendation.\nFinally, extensive experiments on real world datasets are conducted for\nperformance evaluation, and the experimental results demonstrate that the\nproposed DNMDR method outperforms the state-of-the-art baseline models with a\nlarge margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.",
        "Visual storytelling combines visuals and narratives to communicate important\ninsights. While web-based visual storytelling is well-established, leveraging\nthe next generation of digital technologies for visual storytelling,\nspecifically immersive technologies, remains underexplored. We investigated the\nimpact of the story viewpoint (from the audience's perspective) and navigation\n(when progressing through the story) on spatial immersion and understanding.\nFirst, we collected web-based 3D stories and elicited design considerations\nfrom three VR developers. We then adapted four selected web-based stories to an\nimmersive format. Finally, we conducted a user study (N=24) to examine\negocentric and exocentric viewpoints, active and passive navigation, and the\ncombinations they form. Our results indicated significantly higher preferences\nfor egocentric+active (higher agency and engagement) and exocentric+passive\n(higher focus on content). We also found a marginal significance of viewpoints\non story understanding and a strong significance of navigation on spatial\nimmersion.",
        "This paper addresses the problem of satellite inspection, where one or more\nsatellites (inspectors) are tasked with imaging or inspecting a resident space\nobject (RSO) due to potential malfunctions or anomalies. Inspection strategies\nare often reduced to a discretized action space with predefined waypoints,\nfacilitating tractability in both classical optimization and machine learning\nbased approaches. However, this discretization can lead to suboptimal guidance\nin certain scenarios. This study presents a comparative simulation to explore\nthe tradeoffs of passive versus active strategies in multi-agent missions. Key\nfactors considered include RSO dynamic mode, state uncertainty, unmodeled\nentrance criteria, and inspector motion types. The evaluation is conducted with\na focus on fuel utilization and surface coverage. Building on a Monte-Carlo\nbased evaluator of passive strategies and a reinforcement learning framework\nfor training active inspection policies, this study investigates conditions\nunder which passive strategies, such as Natural Motion Circumnavigation (NMC),\nmay perform comparably to active strategies like Reinforcement Learning based\nwaypoint transfers.",
        "The aim was to undertake a national survey of the setup of mammography\nimaging systems in the UK, we were particularly interested in image processing\nand software version. We created a program that can extract selected tags from\nthe DICOM header. 28 medical physics departments used the program on processed\nimages of the TORMAM phantom acquired since 2023 and this produced data for 497\nsystems. We received data for 7 different models of mammography systems. We\nfound that currently in use each model had between 2 and 7 different versions\nof software for the acquisition workstation. Each of the systems had multiple\nversions of image processing settings, a preliminary investigation with TORMAM\ndemonstrated large differences in the appearance of the image for the same\nX-ray model. The Fujifilm, GE and Siemens systems showed differences in the\nsetup of the dose levels. In addition to these settings there were differences\nin the paddles used and grid type. Our snapshot of system set up showed that\nthere is a potential for the images to appear differently according to the\nsettings seen in the headers. These differences may affect the outcomes of AI\nand also human readers. Thus the introduction of AI must take these differences\ninto consideration and the inevitably changes of settings in the future. There\nare responsibilities on AI suppliers, physics, mammographic equipment\nmanufacturers, and breast-screening units to manage the use of AI and ensure\nthe outcomes of breast screening are not adversely affected by the set-up of\nequipment.",
        "We introduce a new equivalence relation, named R-equivalence relation, on the\nset of colorings of an oriented knot diagram by a quandle. We determine the\nR-equivalence classes of colorings of a diagram of a torus knot by a quandle,\ncalled $\\mathrm{Rot} \\mathbb{E}^{2}$, under a certain condition.",
        "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.",
        "This paper describes a method for steering deformable linear objects using\ntwo robot hands in environments populated by sparsely spaced obstacles. The\napproach involves manipulating an elastic inextensible rod by varying the\ngripping endpoint positions and tangents. Closed form solutions that describe\nthe flexible linear object shape in planar environments, Euler's elastica, are\ndescribed. The paper uses these solutions to formulate criteria for non\nself-intersection, stability and obstacle avoidance. These criteria are\nformulated as constraints in the flexible object six-dimensional configuration\nspace that represents the robot gripping endpoint positions and tangents. In\nparticular, this paper introduces a novel criterion that ensures the flexible\nobject stability during steering. All safety criteria are integrated into a\nscheme for steering flexible linear objects in planar environments, which is\nlifted into a steering scheme in three-dimensional environments populated by\nsparsely spaced obstacles. Experiments with a dual-arm robot demonstrate the\nmethod.",
        "We present a novel contribution to Spanish clinical natural language\nprocessing by introducing the largest publicly available clinical corpus,\nClinText-SP, along with a state-of-the-art clinical encoder language model,\nRigoBERTa Clinical. Our corpus was meticulously curated from diverse open\nsources, including clinical cases from medical journals and annotated corpora\nfrom shared tasks, providing a rich and diverse dataset that was previously\ndifficult to access. RigoBERTa Clinical, developed through domain-adaptive\npretraining on this comprehensive dataset, significantly outperforms existing\nmodels on multiple clinical NLP benchmarks. By publicly releasing both the\ndataset and the model, we aim to empower the research community with robust\nresources that can drive further advancements in clinical NLP and ultimately\ncontribute to improved healthcare applications.",
        "Visual-Spatial Systems has become increasingly essential in concrete crack\ninspection. However, existing methods often lacks adaptability to diverse\nscenarios, exhibits limited robustness in image-based approaches, and struggles\nwith curved or complex geometries. To address these limitations, an innovative\nframework for two-dimensional (2D) crack detection, three-dimensional (3D)\nreconstruction, and 3D automatic crack measurement was proposed by integrating\ncomputer vision technologies and multi-modal Simultaneous localization and\nmapping (SLAM) in this study. Firstly, building on a base DeepLabv3+\nsegmentation model, and incorporating specific refinements utilizing foundation\nmodel Segment Anything Model (SAM), we developed a crack segmentation method\nwith strong generalization across unfamiliar scenarios, enabling the generation\nof precise 2D crack masks. To enhance the accuracy and robustness of 3D\nreconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized\ntogether with image data and segmentation masks. By leveraging both image- and\nLiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that\nproduces dense, colorized point clouds, effectively capturing crack semantics\nat a 3D real-world scale. Furthermore, the crack geometric attributions were\nmeasured automatically and directly within 3D dense point cloud space,\nsurpassing the limitations of conventional 2D image-based measurements. This\nadvancement makes the method suitable for structural components with curved and\ncomplex 3D geometries. Experimental results across various concrete structures\nhighlight the significant improvements and unique advantages of the proposed\nmethod, demonstrating its effectiveness, accuracy, and robustness in real-world\napplications.",
        "The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods.",
        "Visual prompt tuning offers significant advantages for adapting pre-trained\nvisual foundation models to specific tasks. However, current research provides\nlimited insight into the interpretability of this approach, which is essential\nfor enhancing AI reliability and enabling AI-driven knowledge discovery. In\nthis paper, rather than learning abstract prompt embeddings, we propose the\nfirst framework, named Interpretable Visual Prompt Tuning (IVPT), to explore\ninterpretability for visual prompts, by introducing hierarchical concept\nprototypes. Specifically, visual prompts are linked to human-understandable\nsemantic concepts, represented as a set of category-agnostic prototypes, each\ncorresponding to a specific region of the image. Then, IVPT aggregates features\nfrom these regions to generate interpretable prompts, which are structured\nhierarchically to explain visual prompts at different granularities.\nComprehensive qualitative and quantitative evaluations on fine-grained\nclassification benchmarks show its superior interpretability and performance\nover conventional visual prompt tuning methods and existing interpretable\nmethods.",
        "This paper reconsiders the problem of testing the equality of two unspecified\ncontinuous distributions. The framework, which we propose, allows for readable\nand insightful data visualisation and helps to understand and quantify how two\ngroups of data differ. We consider a useful weighted rank empirical process on\n(0,1) and utilise a grid-based approach, based on diadic partitions of (0,1),\nto discretize the continuous process and construct local simultaneous\nacceptance regions. These regions help to identify statistically significant\ndeviations from the null model. In addition, the form of the process and its\ndicretization lead to a highly interpretable visualisation of distributional\ndifferences. We also introduce a new two-sample test, explicitly related to the\nvisualisation. Numerical studies show that the new test procedure performs very\nwell. We illustrate the use and diagnostic capabilities of our approach by an\napplication to a known set of neuroscience data.",
        "In this work, we present a rigorous accuracy analysis of the quantum Fourier\ntransform (QFT), that identifies three natural sources of accuracy degeneracy:\n(i) discretization accuracy inherited from classical sampling theory, (ii)\naccuracy degeneracy due to limited resolution in eigenvalue (phase) estimation,\nand (iii) accuracy degeneracy resulting from finite quantum resources. We\nformalize these accuracy degradation sources by proving two theorems that\nrelate the minimal amplitude and eigenvalue resolution to the number of qubits.\nIn addition, we describe a gate-level implementation of the QFT and present\nsimulation results on small-scale quantum systems that illustrate our\ntheoretical findings. Our results clarify the interplay between classical\nsignal discretization limits and quantum hardware limitations, and they provide\nguidelines for the resource requirements needed to achieve a desired precision.",
        "DL-based automatic modulation classification (AMC) models are highly\nsusceptible to adversarial attacks, where even minimal input perturbations can\ncause severe misclassifications. While adversarially training an AMC model\nbased on an adversarial attack significantly increases its robustness against\nthat attack, the AMC model will still be defenseless against other adversarial\nattacks. The theoretically infinite possibilities for adversarial perturbations\nmean that an AMC model will inevitably encounter new unseen adversarial attacks\nif it is ever to be deployed to a real-world communication system. Moreover,\nthe computational limitations and challenges of obtaining new data in real-time\nwill not allow a full training process for the AMC model to adapt to the new\nattack when it is online. To this end, we propose a meta-learning-based\nadversarial training framework for AMC models that substantially enhances\nrobustness against unseen adversarial attacks and enables fast adaptation to\nthese attacks using just a few new training samples, if any are available. Our\nresults demonstrate that this training framework provides superior robustness\nand accuracy with much less online training time than conventional adversarial\ntraining of AMC models, making it highly efficient for real-world deployment.",
        "We present MOFA, an open-source generative AI (GenAI) plus simulation\nworkflow for high-throughput generation of metal-organic frameworks (MOFs) on\nlarge-scale high-performance computing (HPC) systems. MOFA addresses key\nchallenges in integrating GPU-accelerated computing for GPU-intensive GenAI\ntasks, including distributed training and inference, alongside CPU- and\nGPU-optimized tasks for screening and filtering AI-generated MOFs using\nmolecular dynamics, density functional theory, and Monte Carlo simulations.\nThese heterogeneous tasks are unified within an online learning framework that\noptimizes the utilization of available CPU and GPU resources across HPC\nsystems. Performance metrics from a 450-node (14,400 AMD Zen 3 CPUs + 1800\nNVIDIA A100 GPUs) supercomputer run demonstrate that MOFA achieves\nhigh-throughput generation of novel MOF structures, with CO$_2$ adsorption\ncapacities ranking among the top 10 in the hypothetical MOF (hMOF) dataset.\nFurthermore, the production of high-quality MOFs exhibits a linear relationship\nwith the number of nodes utilized. The modular architecture of MOFA will\nfacilitate its integration into other scientific applications that dynamically\ncombine GenAI with large-scale simulations.",
        "This paper introduces a diagonal adaptive kernel model that dynamically\nlearns kernel eigenvalues and output coefficients simultaneously during\ntraining. Unlike fixed-kernel methods tied to the neural tangent kernel theory,\nthe diagonal adaptive kernel model adapts to the structure of the truth\nfunction, significantly improving generalization over fixed-kernel methods,\nespecially when the initial kernel is misaligned with the target. Moreover, we\nshow that the adaptivity comes from learning the right eigenvalues during\ntraining, showing a feature learning behavior. By extending to deeper\nparameterization, we further show how extra depth enhances adaptability and\ngeneralization. This study combines the insights from feature learning and\nimplicit regularization and provides new perspective into the adaptivity and\ngeneralization potential of neural networks beyond the kernel regime.",
        "We recently described a specific type of attractors of two-dimensional\ndiscontinuous piecewise linear maps, characterized by two discontinuity lines\ndividing the phase plane into three partitions, related to economic\napplications. To our knowledge, this type of attractor, which we call a weird\nquasiperiodic attractor, has not yet been studied in detail. They have a rather\ncomplex geometric structure and other interesting properties that are worth\nunderstanding better. To this end, we consider a simpler map that can also\npossess weird quasiperiodic attractors, namely, a 2D discontinuous piecewise\nlinear map $F$ with a single discontinuity line dividing the phase plane into\ntwo partitions, where two different homogeneous linear maps are defined. Map\n$F$ depends on four parameters -- the traces and determinants of the two\nJacobian matrices. In the parameter space of map $F$, we obtain specific\nregions associated with the existence of weird quasiperiodic attractors;\ndescribe some characteristic properties of these attractors; and explain one of\nthe possible mechanisms of their appearance.",
        "Tabular data remains one of the most prevalent data types across a wide range\nof real-world applications, yet effective representation learning for this\ndomain poses unique challenges due to its irregular patterns, heterogeneous\nfeature distributions, and complex inter-column dependencies. This survey\nprovides a comprehensive review of state-of-the-art techniques in tabular data\nrepresentation learning, structured around three foundational design elements:\ntraining data, neural architectures, and learning objectives. Unlike prior\nsurveys that focus primarily on either architecture design or learning\nstrategies, we adopt a holistic perspective that emphasizes the universality\nand robustness of representation learning methods across diverse downstream\ntasks. We examine recent advances in data augmentation and generation,\nspecialized neural network architectures tailored to tabular data, and\ninnovative learning objectives that enhance representation quality.\nAdditionally, we highlight the growing influence of self-supervised learning\nand the adaptation of transformer-based foundation models for tabular data. Our\nreview is based on a systematic literature search using rigorous inclusion\ncriteria, encompassing 127 papers published since 2020 in top-tier conferences\nand journals. Through detailed analysis and comparison, we identify emerging\ntrends, critical gaps, and promising directions for future research, aiming to\nguide the development of more generalizable and effective tabular data\nrepresentation methods.",
        "Small object segmentation, like tumor segmentation, is a difficult and\ncritical task in the field of medical image analysis. Although deep learning\nbased methods have achieved promising performance, they are restricted to the\nuse of binary segmentation mask. Inspired by the rigorous mapping between\nbinary segmentation mask and distance map, we adopt distance map as a novel\nground truth and employ a network to fulfill the computation of distance map.\nSpecially, we propose a new segmentation framework that incorporates the\nexisting binary segmentation network and a light weight regression network\n(dubbed as LR-Net). Thus, the LR-Net can convert the distance map computation\ninto a regression task and leverage the rich information of distance maps.\nAdditionally, we derive a shape-aware loss by employing distance maps as\npenalty map to infer the complete shape of an object. We evaluated our approach\non MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge dataset and a clinical\ndataset. Experimental results show that our approach outperforms the\nclassification-based methods as well as other existing state-of-the-arts.",
        "Evaluations of large language model (LLM) risks and capabilities are\nincreasingly being incorporated into AI risk management and governance\nframeworks. Currently, most risk evaluations are conducted by designing inputs\nthat elicit harmful behaviors from the system. However, a fundamental\nlimitation of this approach is that the harmfulness of the behaviors identified\nduring any particular evaluation can only lower bound the model's\nworst-possible-case behavior. As a complementary method for eliciting harmful\nbehaviors, we propose evaluating LLMs with model tampering attacks which allow\nfor modifications to latent activations or weights. We pit state-of-the-art\ntechniques for removing harmful LLM capabilities against a suite of 5\ninput-space and 6 model tampering attacks. In addition to benchmarking these\nmethods against each other, we show that (1) model resilience to capability\nelicitation attacks lies on a low-dimensional robustness subspace; (2) the\nattack success rate of model tampering attacks can empirically predict and\noffer conservative estimates for the success of held-out input-space attacks;\nand (3) state-of-the-art unlearning methods can easily be undone within 16\nsteps of fine-tuning. Together these results highlight the difficulty of\nremoving harmful LLM capabilities and show that model tampering attacks enable\nsubstantially more rigorous evaluations than input-space attacks alone. We\nrelease models at https:\/\/huggingface.co\/LLM-GAT",
        "Causal inference methods such as instrumental variables, regression\ndiscontinuity, and difference-in-differences are widely used to estimate\ntreatment effects. However, their application to qualitative outcomes poses\nfundamental challenges, as standard causal estimands are ill-defined in this\ncontext. This paper highlights these issues and introduces an alternative\nframework that focuses on well-defined and interpretable estimands that\nquantify how treatment affects the probability distribution over outcome\ncategories. We establish that standard identification assumptions are\nsufficient for identification and propose simple, intuitive estimation\nstrategies that remain fully compatible with conventional econometric methods.\nTo facilitate implementation, we provide an open-source R package,\n$\\texttt{causalQual}$, which is publicly available on GitHub.",
        "In this work, we address the challenge of approximating unknown system\ndynamics and costs by representing them as a bilinear system using\nKoopman-based Inverse Optimal Control (IOC). Using optimal trajectories, we\nconstruct a bilinear control system in transformed state variables through a\nmodified Extended Dynamic Mode Decomposition with control (EDMDc) that\nmaintains exact dynamical equivalence with the original nonlinear system. We\nderive Pontryagin's Maximum Principle (PMP) optimality conditions for this\nsystem, which closely resemble those of the inverse Linear Quadratic Regulator\n(LQR) problem due to the consistent control input and state independence from\nthe control. This similarity allows us to apply modified inverse LQR theory,\noffering a more tractable and robust alternative to nonlinear Inverse Optimal\nControl methods, especially when dealing with unknown dynamics. Our approach\nalso benefits from the extensive analytical properties of bilinear control\nsystems, providing a solid foundation for further analysis and application. We\ndemonstrate the effectiveness of the proposed method through theoretical\nanalysis, simulation studies and a robotic experiment, highlighting its\npotential for broader applications in the approximation and design of control\nsystems."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association",
    "start_abstract":"We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.",
    "start_categories":[
      "cs.DC"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "A Global Geometric Framework for Nonlinear Dimensionality Reduction"
      ],
      "abstract":[
        "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure."
      ],
      "categories":[
        "math.ST"
      ]
    },
    "list":{
      "title":[
        "Efficient inference of rankings from multi-body comparisons",
        "GWSkyNet-Multi II: an updated deep learning model for rapid\n  classification of gravitational-wave events",
        "On the components of random geometric graphs in the dense limit",
        "Infinitely many saturated travelling waves for epidemic models with\n  distributed-contacts",
        "Precision Higgs Constraints in U(1) Extensions of the Standard Model\n  with a Light Z'-Boson",
        "Chance-Constrained Covariance Steering for Discrete-Time Markov Jump\n  Linear Systems",
        "Noncommutative Novikov bialgebras and differential antisymmetric\n  infinitesimal bialgebras with weight",
        "The second-order intrinsic Wiedemann-Franz law",
        "A model calculation of the CKM matrix",
        "A Unified Blockwise Measurement Design for Learning Quantum Channels and\n  Lindbladians via Low-Rank Matrix Sensing",
        "Generalized Recurrence Criteria for Classes of Open Quantum Walks",
        "Survival Concept-Based Learning Models",
        "Is a phonon excitation of a superfluid Bose gas a Goldstone boson?",
        "Multiple Horn problems for planar networks and invertible matrices",
        "Torsion in Magnitude homology theories",
        "Primordial black holes as cosmic expansion accelerators",
        "On de Bruijn Array Codes Part II: Linear Codes",
        "Event-Based Limit Order Book Simulation under a Neural Hawkes Process:\n  Application in Market-Making",
        "Propagation of extreme events in multiplex neuronal networks",
        "Diagnosing the solar atmosphere through the Mg I b$_2$ 5173 \\AA\\ line.\n  II. Morphological classification of the intensity and circular polarization\n  profiles",
        "Partial separability of the Schroedinger equation combined with a\n  Jastrow factor",
        "Transfer Learning Analysis of Variational Quantum Circuits",
        "Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow\n  in Shallow Linear Networks",
        "On the approaching geodesics property",
        "Designing Illumination Patterns for Single-Pixel Imaging Using Lattice\n  Models",
        "Relativistic Gas Accretion onto Supermassive Black Hole Binaries from\n  Inspiral through Merger",
        "A Guide to Molecular Properties from the Bethe-Salpeter Equation",
        "From non-equilibrium Green functions to Lattice Wigner: A toy model for\n  quantum nanofluidics simulations",
        "Randomized block-Krylov subspace methods for low-rank approximation of\n  matrix functions"
      ],
      "abstract":[
        "Many of the existing approaches to assess and predict the performance of\nplayers, teams or products in competitive contests rely on the assumption that\ncomparisons occur between pairs of such entities. There are, however, several\nreal contests where more than two entities are part of each comparison, e.g.,\nsports tournaments,multiplayer board and card games, and preference surveys.\nThe Plackett-Luce (PL) model provides a principled approach to infer the\nranking of entities involved in such contests characterized by multi-body\ncomparisons. Unfortunately, traditional algorithms used to compute PL rankings\nsuffer from slow convergence limiting the application of the PL model to\nrelatively small-scale systems. We present here an alternative implementation\nthat allows for significant speed-ups and validate its efficiency in both\nsynthetic and real-world sets of data. Further, we perform systematic\ncross-validation tests concerning the ability of the PL model to predict\nunobserved comparisons. We find that a PL model trained on a set composed of\nmulti-body comparisons is more predictive than a PL model trained on a set of\nprojected pairwise comparisons derived from the very same training set,\nemphasizing the need of properly accounting for the true multi-body nature of\nreal-world systems whenever such an information is available.",
        "Multi-messenger observations of gravitational waves and electromagnetic\nemission from compact object mergers offer unique insights into the structure\nof neutron stars, the formation of heavy elements, and the expansion rate of\nthe Universe. With the LIGO-Virgo-KAGRA (LVK) gravitational-wave detectors\ncurrently in their fourth observing run (O4), it is an exciting time for\ndetecting these mergers. However, assessing whether to follow up a candidate\ngravitational-wave event given limited telescope time and resources is\nchallenging; the candidate can be a false alert due to detector glitches, or\nmay not have any detectable electromagnetic counterpart even if it is real.\nGWSkyNet-Multi is a deep learning model developed to facilitate follow-up\ndecisions by providing real-time classification of candidate events, using\nlocalization information released in LVK rapid public alerts. Here we introduce\nGWSkyNet-Multi II, an updated model targeted towards providing more robust and\ninformative predictions during O4 and beyond. Specifically, the model now\nprovides normalized probability scores and associated uncertainties for each of\nthe four corresponding source categories released by the LVK: glitch, binary\nblack hole, neutron star-black hole, and binary neutron star. Informed by\nexplainability studies of the original model, the updated model architecture is\nalso significantly simplified, including replacing input images with intuitive\nsummary values, making it more interpretable. For significant O4 event alerts\nissued between May 2023 and December 2024, GWSkyNet-Multi II produces a\nprediction that is consistent with the updated LVK classification for 93% of\nevents. The updated model can be used by the community to help make\ntime-critical follow-up decisions.",
        "Consider the geometric graph on $n$ independent uniform random points in a\nconnected compact region $A$ of ${\\bf R}^d, d \\geq 2$ with $C^2$ boundary, or\nin the unit square, with distance parameter $r_n$. Let $K_n$ be the number of\ncomponents of this graph, and $R_n$ the number of vertices not in the giant\ncomponent. Let $S_n$ be the number of isolated vertices. We show that if $r_n$\nis chosen so that $nr_n^d$ tends to infinity but slowly enough that ${\\bf\nE}[S_n]$ also tends to infinity, then $K_n$, $R_n$ and $S_n$ are all asymptotic\nto $\\mu_n$ in probability as $n \\to \\infty$ where (with $|A|$, $\\theta_d$ and\n$|\\partial A|$ denoting the volume of $A$, of the unit $d$-ball, and the\nperimeter of $A$ respectively) $\\mu_n := ne^{-\\pi n r_n^d\/|A|}$ if $d=2$ and\n$\\mu_n := ne^{-\\theta_d n r_n^d\/|A|} + \\theta_{d-1}^{-1} |\\partial A| r_n^{1-d}\ne^{- \\theta_d n r_n^d\/(2|A|)}$ if $d\\geq 3$. We also give variance asymptotics\nand central limit theorems for $K_n$ and $R_n$ in this limiting regime when $d\n\\geq 3$, and for Poisson input with $d \\geq 2$. We extend these results\n(substituting ${\\bf E}[S_n]$ for $\\mu_n$) to a class of non-uniform\ndistributions on $A$.",
        "We consider an epidemic model with distributed-contacts. When the contact\nkernel concentrates, one formally reaches a very degenerate Fisher-KPP equation\nwith a diffusion term that is not in divergence form. We make an exhaustive\nstudy of its travelling waves. For every admissible speed, there exists not\nonly a non-saturated (smooth) wave but also infinitely many saturated (sharp)\nones. Furthermore their tails may differ from what is usually expected. These\nresults are thus in sharp contrast with their counterparts on related models.",
        "Anomaly free $U(1)$ extensions of the standard model (SM) predict a new\nneutral gauge boson $Z'$. When the $Z'$ obtains its mass from the spontaneous\nbreaking of the new $U(1)$ symmetry by a new complex scalar field, the model\nalso predicts a second real scalar $s$ and the searches for the new scalar and\nthe new gauge boson become intertwined. We present the computation of\nproduction cross sections and decay widths of such a scalar $s$ in models with\na light $Z'$ boson, when the decay $h\\to Z' Z'$ may have a sizeable branching\nratio. We show how Higgs signal strength measurement in this channel can\nprovide stricter exclusion bounds on the parameters of the model than those\nobtained from the total signal strength for Higgs boson production.",
        "In this paper, we propose a novel convex optimization framework to solve the\noptimal covariance steering problem for discrete-time Markov Jump Linear\nSystems (MJLS) with chance constraints. We derive the analytical expressions\nfor the mean and covariance trajectories of time-varying discrete-time MJLS and\nshow that they cannot be separated even without chance constraints, unlike the\nsingle-mode dynamics case. To solve the covariance steering problem, we propose\na two-step convex optimization framework, which optimizes the mean and\ncovariance subproblems sequentially. Further, we use Gaussian approximations to\nincorporate chance constraints and propose an iterative optimization framework\nto solve the chance-constrained covariance steering problem. Both problems are\noriginally nonconvex, and we derive convex relaxations which are proved to be\nlossless at optimality using the Karush-Kuhn-Tucker (KKT) conditions. Numerical\nsimulations demonstrate the proposed method by achieving target covariances\nwhile respecting chance constraints under Gaussian noise and Markovian jump\ndynamics.",
        "This paper first develops a bialgebra theory for a noncommutative Novikov\nalgebra, called a noncommutative Novikov bialgebra, which is further\ncharacterized by matched pairs and Manin triples of noncommutative Novikov\nalgebras. The classical Yang-Baxter type equation, $\\mathcal{O}$-operators, and\nnoncommutative pre-Novikov algebras are introduced to study noncommutative\nNovikov bialgebra. As an application, noncommutative pre-Novikov algebras are\nobtained from differential dendriform algebras. Next, to generalize Gelfand's\nclassical construction of a Novikov algebra from a commutative differential\nalgebra to the bialgebra context in the noncommutative case, we establish\nantisymmetric infinitesimal (ASI) bialgebras for (noncommutative) differential\nalgebras, and obtain the condition under which a differential ASI bialgebra\ninduces a noncommutative Novikov bialgebra.",
        "In recent years, the nonlinear anomalous thermal Hall effect has attracted\nsubstantial attention. In this paper, we carry out a theoretical exploration of\nthe intrinsic anomalous thermal Hall and Nernst effect that is induced by the\nthermal Berry connection polarizability. This effect is independent of the\nrelaxation time and can be present in antiferromagnets possessing PT symmetry.\nAdditionally, we put forward a second-order intrinsic Wiedemann-Franz law,\nwhich represents the ratio of the second-order intrinsic thermal conductivity\ncoefficient to the second-order intrinsic electrical conductivity coefficient .\nWhen analyzed within a four-band PT symmetric Dirac model, we observe that the\nsecond-order intrinsic thermal conductivity coefficient is linearly\nproportional to the second-order intrinsic electrical conductivity coefficient\n, and the second-order intrinsic Wiedemann-Franz law is characterized by the\nchemical potential $\\mu$ in the low-temperature regime. These findings provide\nsignificant implications for experimental verification.",
        "We propose a strategy to compute the CKM matrix based on the conjecture,\nrecently put forward in the literature, according to which elementary particle\nmasses are not generated like in the standard Higgs scenario, but emerge from a\nnon-perturbative mechanism triggered by the presence in the fundamental\nLagrangian of ``irrelevant'' chiral breaking operators of the Wilson type of\ndimension $d\\geq 6$ scaled by $d-4$ powers of the UV cutoff. Non-perturbatively\ngenerated quark masses have the form $m_q\\sim C_q(\\alpha) \\Lambda_{RGI}$ where\n$\\Lambda_{RGI}$ is the RGI scale of the theory and $C_q(\\alpha)$ is a function\nof the gauge couplings. For the (elementary) fermion $q$ the $C_q(\\alpha)$\nleading behaviour is $C_q(\\alpha)={\\mbox{O}}(\\alpha^{1+(d_q-4)\/2})$. The\ndependence of the gauge coupling power behaviour from the dimension $d_q$ of\nthe Wilson-like operators associated with the fermion $q$ can be exploited to\nconstruct hierarchically organized up and down ''proto-mass matrices'' for\n''proto-flavours'', the diagonalization of which yields flavoured quarks with\ndefinite masses and a first principle construction of the CKM matrix.",
        "Quantum superoperator learning is a pivotal task in quantum information\nscience, enabling accurate reconstruction of unknown quantum operations from\nmeasurement data. We propose a robust approach based on the matrix sensing\ntechniques for quantum superoperator learning that extends beyond the positive\nsemidefinite case, encompassing both quantum channels and Lindbladians. We\nfirst introduce a randomized measurement design using a near-optimal number of\nmeasurements. By leveraging the restricted isometry property (RIP), we provide\ntheoretical guarantees for the identifiability and recovery of low-rank\nsuperoperators in the presence of noise. Additionally, we propose a blockwise\nmeasurement design that restricts the tomography to the sub-blocks,\nsignificantly enhancing performance while maintaining a comparable scale of\nmeasurements. We also provide a performance guarantee for this setup. Our\napproach employs alternating least squares (ALS) with acceleration for\noptimization in matrix sensing. Numerical experiments validate the efficiency\nand scalability of the proposed methods.",
        "In this paper, we study the recurrence of open quantum walks (OQWs) induced\nby finite-dimensional coins $(L,B,R)$. The focus is on homogeneous OQWs with a\nset of vertices $\\mathbb{Z}$, the set of integers. We present three distinct\nrecurrence criteria, each adapted to different types of coins. The first\ncriterion was developed for a class of Lazy OQWs in any finite dimension, where\nthe presented criterion is associated with an auxiliary map and its only\ninvariant state, resulting in the first recurrence criterion for Lazy OQWs. The\nsecond one is restricted to Lazy OQWs of dimension 2, where we provide a\ncomplete characterization of the recurrence for this lower dimension. Finally,\nwe present a general criterion for finite-dimensional coins in the non-lazy\ncase $(B=0)$, which generalizes many of the previously known results. This new\ncriterion holds for irreducible and reducible OQWs through a decomposition of\nthe Hilbert space where our quantum states act.",
        "Concept-based learning enhances prediction accuracy and interpretability by\nleveraging high-level, human-understandable concepts. However, existing CBL\nframeworks do not address survival analysis tasks, which involve predicting\nevent times in the presence of censored data -- a common scenario in fields\nlike medicine and reliability analysis. To bridge this gap, we propose two\nnovel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM\n(Survival Regularized Concept-based Model), which integrate concept-based\nlearning with survival analysis to handle censored event time data. The models\nemploy the Cox proportional hazards model and the Beran estimator. SurvCBM is\nbased on the architecture of the well-known concept bottleneck model, offering\ninterpretable predictions through concept-based explanations. SurvRCM uses\nconcepts as regularization to enhance accuracy. Both models are trained\nend-to-end and provide interpretable predictions in terms of concepts. Two\ninterpretability approaches are proposed: one leveraging the linear\nrelationship in the Cox model and another using an instance-based explanation\nframework with the Beran estimator. Numerical experiments demonstrate that\nSurvCBM outperforms SurvRCM and traditional survival models, underscoring the\nimportance and advantages of incorporating concept information. The code for\nthe proposed algorithms is publicly available.",
        "It is generally accepted that phonons in a superfluid Bose gas are Goldstone\nbosons. This is justified by spontaneous symmetry breaking (SSB), which is\nusually defined as follows: the Hamiltonian of the system is invariant under\nthe $U(1)$ transformation $\\hat{\\Psi}(\\mathbf{r},t)\\rightarrow e^{i\\alpha}%\n\\hat{\\Psi}(\\mathbf{r},t)$, whereas the order parameter $\\Psi(\\mathbf{r},t)$ is\nnot. However, the strict definition of SSB is different: the Hamiltonian and\nthe boundary conditions are invariant under a symmetry transformation, while\nthe \\emph{ground state} is not. Based on the latter criterion, we study a\nfinite system of spinless, weakly interacting bosons using three approaches:\nthe standard Bogoliubov method, the particle-number-conserving Bogoliubov\nmethod, and the approach based on the exact ground-state wave function. Our\nresults show that the answer to the question in the title is \\textquotedblleft\nno\\textquotedblright. Thus, phonons in a real-world (finite) superfluid Bose\ngas are similar to sound in a classical gas: they are not Goldstone bosons, but\nquantised collective vibrational modes arising from the interaction between\natoms. In the case of an infinite Bose gas, however, the picture becomes\nparadoxical: the ground state can be regarded as either infinitely degenerate\nor non-degenerate, making the phonon both similar to a Goldstone boson and\ndifferent from it.",
        "The multiplicative multiple Horn problem is asking to determine possible\nsingular values of the combinations $AB, BC$ and $ABC$ for a triple of\ninvertible matrices $A,B,C$ with given singular values. There are similar\nproblems for eigenvalues of sums of Hermitian matrices (the additive problem),\nand for maximal weights of multi-paths in concatenations of planar networks\n(the tropical problem).\n  For the planar network multiple Horn problem, we establish necessary\nconditions, and we conjecture that for large enough networks they are also\nsufficient. These conditions are given by the trace equalities and rhombus\ninequalities (familiar from the hive description of the classical Horn\nproblem), and by the new set of tetrahedron equalities. Furthermore, if one\nimposes Gelfand-Zeitlin conditions on weights of planar networks, tetrahedron\nequalities turn into the octahedron recurrence from the theory of crystals. We\ngive a geometric interpretation of our results in terms of positive varieties\nwith potential. In this approach, rhombus inequalities follow from the\ninequality $\\Phi^t \\leqslant 0$ for the tropicalized potential, and tetrahedron\nequalities are obtained as tropicalization of certain Pl\\\"ucker relations.\n  For the multiplicative problem, we introduce a scaling parameter $s$, and we\nshow that for $s$ large enough (corresponding to exponentially large\/small\nsingular values) the Duistermaat-Heckman measure associated to the\nmultiplicative problem concentrates in a small neighborhood of the octahedron\nrecurrence locus.",
        "In this article, we analyze the structure and relationships between magnitude\nhomology and Eulerian magnitude homology of finite graphs. Building on the work\nof Kaneta and Yoshinaga, Sazdanovic and Summers, and Asao and Izumihara, we\nprovide two proofs of the existence of torsion in Eulerian magnitude homology,\noffer insights into the types and orders of torsion, and present explicit\ncomputations for various classes of graphs.",
        "We propose a novel and natural mechanism for cosmic acceleration driven by\nprimordial black holes (PBHs) exhibiting repulsive behavior. Using a new\n``Swiss Cheese'' cosmological approach, we demonstrate that this cosmic\nacceleration mechanism is a general phenomenon by examining three regular black\nhole spacetimes - namely the Hayward, Bardeen and Dymnikova spacetimes - as\nwell as the singular de Sitter-Schwarzschild spacetime. Interestingly, by\nmatching these black hole spacetimes with an isotropic and homogeneous\nexpanding Universe, we obtain a phase of cosmic acceleration that ends at an\nenergy scale characteristic to the black hole parameters or due to black hole\nevaporation. This cosmic acceleration mechanism can be relevant either to an\ninflationary phase with a graceful exit and reheating or to an early dark\nenergy type of contribution pertinent to the Hubble tension. Remarkably, we\nfind that ultra-light PBHs with masses $m<5\\times 10^8\\mathrm{g}$ dominating\nthe energy content of the Univese before Big Bang Nucleosynthesis, can drive a\nsuccessful inflationary expansion era without the use of an inflaton field.\nAdditionally, PBHs with masses $m \\sim 10^{12}\\mathrm{g}$ and abundances $0.107\n< \\Omega^\\mathrm{eq}_\\mathrm{PBH}< 0.5$, slightly before matter-radiation\nequality, can produce a substantial amount of early dark energy, helping to\nalleviate the $H_0$ tension.",
        "An M-sequence generated by a primitive polynomial has many interesting and\ndesirable properties. A pseudo-random array is the two-dimensional\ngeneralization of an M-sequence. Similarly to primitive polynomials, there are\nirreducible and reducible polynomials whose all nonzero sequences have the same\nlength. In this paper, a two-dimensional generalization for such sequences is\ngiven. This generalization is for a pseudo-random array code which is a set of\n$r_1 \\times r_2$ arrays in which each $n_1 \\times n_2$ nonzero matrix is\ncontained exactly once as a window in one of the arrays. Moreover, these arrays\nhave the shift-and-add property, i.e., the bitwise addition of two arrays (or a\nnontrivial shift of such arrays) is another array (or a shift of another array)\nfrom the code. All the known arrays can be formed by folding sequences\ngenerated from an irreducible polynomial or a reducible polynomial whose\nfactors have the same degree and the same exponent. Two proof techniques are\nused to prove the parameters of the constructed arrays. The first one is based\non another method for constructing some of these arrays. The second one is a\ngeneralization of a known proof technique. This generalization enables to\npresent pseudo-random arrays with parameters not known before and also a\nvariety of pseudo-random array codes which cannot be generated by the first\nmethod. The two techniques also suggest two different hierarchies between\npseudo-random array codes. Finally, a method to verify whether a folding of\nsequences, generated by these polynomials, yields a pseudo-random array or a\npseudo-random array code, will be presented.",
        "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "The Mg I b$_2$ line at 5173 \\r{A} is primarily magnetically sensitive to\nheights between the mid photosphere and the low chromosphere, a region that has\nnot been sufficiently explored in the solar atmosphere but is crucial for\nunderstanding the magnetic coupling between the two layers. New generation\nsolar observatories are now performing polarimetric observations of this\nspectral line, enabling simultaneous measurements with multiple spectral lines.\nThis allows for detailed studies of the magnetism around the temperature\nminimum region at high spatial, temporal, and spectral resolutions. We present\na morphological classification of the Stokes $I$ and $V$ profiles of the Mg I\nb$_2$ line using the Euclidean distance method on high spatial resolution\nobservations from the Swedish 1-m Solar Telescope. The physical properties of\nthe resulting classes were analyzed using classical inference methods.\nAdditionally, we present a two-line full-Stokes inversion of the representative\nprofiles in which the Mg I b$_2$ line is treated fully under non-local\nthermodynamic equilibrium (NLTE) conditions, while the Fe I 6173 \\r{A} line is\nsimultaneously inverted under LTE assumptions to provide photospheric\nconstraints. This approach offers insights into the temperature stratification\nand other physical gradients involved in the formation of the different profile\nmorphologies. We found nine classes of Stokes $V$ profiles and 16 classes of\nStokes $I$ profiles in our Mg I b$_2$ dataset. These classes can be further\ngrouped into families based on shared characteristics, physical properties, and\nlocation. Our classification provides important information on the different\nenvironments and processes occurring in the solar atmosphere around the\ntemperature minimum region. It is also relevant for improving the performance\nof NLTE inversions.",
        "Describing the Coulomb interactions between electrons in atomic or molecular\nsystems is an important step to help us obtain accurate results for the\ndifferent observables in the system. One convenient approach is to separate the\ndynamic electronic correlation, i.e., Coulomb electron-electron repulsion, from\nthe motion of the electrons in the nuclei electric field. The wave function is\nwritten as the product of two terms, one accounting for the electron-electron\ninteractions, which is symmetric under identical particle exchange; the other\nis antisymmetric and represents the dynamics and exchange of electrons within\nthe nuclear electric field. In this work, we present a novel computational\nscheme based on this idea that leads to an expression of the energy as the sum\nof two terms. To illustrate the method, we look into few-body Coulombic\nsystems, H2, H3+ and Li(1s2,2s), and discuss the possible extension to larger\nsystems. A simple correlation factor, based on the Jastrow exponential term, is\nemployed to represent the dynamics of the electron pairs leading to simple\nanalytical forms and accurate results. We also present and illustrate a\ndifferent approach with the Li atom based on the partial separability applied\nto a portion of the atom.",
        "This work analyzes transfer learning of the Variational Quantum Circuit\n(VQC). Our framework begins with a pretrained VQC configured in one domain and\ncalculates the transition of 1-parameter unitary subgroups required for a new\ndomain. A formalism is established to investigate the adaptability and\ncapability of a VQC under the analysis of loss bounds. Our theory observes\nknowledge transfer in VQCs and provides a heuristic interpretation for the\nmechanism. An analytical fine-tuning method is derived to attain the optimal\ntransition for adaptations of similar domains.",
        "We study the gradient descent (GD) dynamics of a depth-2 linear neural\nnetwork with a single input and output. We show that GD converges at an\nexplicit linear rate to a global minimum of the training loss, even with a\nlarge stepsize -- about $2\/\\textrm{sharpness}$. It still converges for even\nlarger stepsizes, but may do so very slowly. We also characterize the solution\nto which GD converges, which has lower norm and sharpness than the gradient\nflow solution. Our analysis reveals a trade off between the speed of\nconvergence and the magnitude of implicit regularization. This sheds light on\nthe benefits of training at the ``Edge of Stability'', which induces additional\nregularization by delaying convergence and may have implications for training\nmore complex models.",
        "We survey some recent results and open questions on the approaching geodesics\nproperty and its application to the study of the Gromov and horofunction\ncompactifications of a proper geodesic Gromov metric space. We obtain results\non the dynamics of isometries and we exhibit an example of a Gromov hyperbolic\ndomain of $\\mathbb{C}$ which does not satisfy the approaching geodesic\nproperty.",
        "Single-pixel imaging leverages a single-pixel detector and structured\nillumination patterns to reconstruct images, offering a cost-effective solution\nfor imaging across a wide range of wavelengths, such as x-ray and terahertz.\nHowever, the technique faces challenges in efficiency due to the need for\nnumerous patterns to achieve high-quality image reconstruction. In this study,\nwe explore the use of spin lattice models from statistical mechanics to design\nillumination patterns for single-pixel imaging. By employing models like Ising,\nPotts, XY, and Heisenberg, we generate structured patterns that are adaptable\nfor binary, grayscale, and color imaging. This work creates a direct connection\nbetween lattice models and imaging applications, providing a systematic\napproach to pattern generation that can enhance single-pixel imaging\nefficiency.",
        "Accreting supermassive black hole binaries are powerful multimessenger\nsources emitting both gravitational and electromagnetic (EM) radiation.\nUnderstanding the accretion dynamics of these systems and predicting their\ndistinctive EM signals is crucial to informing and guiding upcoming efforts\naimed at detecting gravitational waves produced by these binaries. To this end,\naccurate numerical modeling is required to describe both the spacetime and the\nmagnetized gas around the black holes. In this paper, we present two key\nadvances in this field of research.\n  First, we have developed a novel 3D general relativistic magnetohydrodynamics\n(GRMHD) framework that combines multiple numerical codes to simulate the\ninspiral and merger of supermassive black hole binaries starting from realistic\ninitial data and running all the way through merger. Throughout the evolution,\nwe adopt a simple but functional prescription to account for gas cooling\nthrough the emission of photons.\n  Next, we have applied our new computational method to follow the time\nevolution of a circular, equal-mass, non-spinning black hole binary for\n${\\sim\\!200}$ orbits starting from a separation of ${20\\,r_g}$ and reaching the\npost-merger evolutionary stage of the system. We have identified how and when\nthe minidisks dissolve as the binary compresses. We also show that even when\nthe binary ``decouples'' from its surrounding disk, its luminosity decreases by\nonly a factor of a few and abruptly increases by ${\\sim\\!50\\%}$ at the time of\nmerger, accompanied by an equally abrupt change in spectrum. Finally, the\nmagnetic flux brought to the spin-parameter ${\\sim\\!0.68}$ merger remnant is\nable to drive a relativistic, Poynting-flux-dominated jet.",
        "The Bethe-Salpeter equation (BSE) combined with the Green's function GW\nmethod has successfully transformed into a robust computational tool to\ndescribe light-matter interactions and excitation spectra for molecules,\nsolids, and materials from first principles. Thanks to its ability to\naccurately describe charge-transfer and Rydberg excitations, the GW-BSE already\nforms an established and cost-efficient alternative to time-dependent density\nfunctional theory. This raises the question whether the GW-BSE approach can\nbecome a more general framework for molecular properties beyond excitation\nenergies. In this mini-review, we recapitulate recent endeavors along this\npoint in terms of both theoretical and practical developments for quantum\nchemistry, physical chemistry, and related fields. In doing so, we provide\nguidelines for current applications to chemical challenges in collaboration\nwith experimentalists as well as to future developments to extended the GW-BSE\ntoolkit.",
        "Recent experiments of fluid transport in nano-channels have shown evidence of\na dramatic reduction of friction due to the coupling between\ncharge-fluctuations in polar fluids and electronic excitations in graphene\nsolids, a phenomenon dubbed \"negative quantum friction\". In this paper, we\npresent a semi-classical mesoscale Boltzmann-Wigner lattice kinetic model of\nquantum-nanoscale transport and perform a numerical study of the effects of the\nquantum interactions on the evolution of a one-dimensional nano-fluid subject\nto a periodic external potential. It is shown that the effects of quantum\nfluctuations become visible once the quantum length scale (Fermi wavelength) of\nthe quasiparticles becomes comparable to the wavelength of the external\npotential. Under such conditions, quantum fluctuations are mostly felt on the\nodd kinetic moments, while the even ones remain nearly unaffected because they\nare \"protected\" by thermal fluctuations. It is hoped that the present\nBoltzmann-Wigner lattice model and extensions thereof may offer a useful tool\nfor the computer simulation of quantum-nanofluidic transport phenomena.",
        "The randomized SVD is a method to compute an inexpensive, yet accurate,\nlow-rank approximation of a matrix. The algorithm assumes access to the matrix\nthrough matrix-vector products (matvecs). Therefore, when we would like to\napply the randomized SVD to a matrix function, $f(A)$, one needs to approximate\nmatvecs with $f(A)$ using some other algorithm, which is typically treated as a\nblack-box. Chen and Hallman (SIMAX 2023) argued that, in the common setting\nwhere matvecs with $f(A)$ are approximated using Krylov subspace methods\n(KSMs), more efficient low-rank approximation is possible if we open this\nblack-box. They present an alternative approach that significantly outperforms\nthe naive combination of KSMs with the randomized SVD, although the method\nlacked theoretical justification. In this work, we take a closer look at the\nmethod, and provide strong and intuitive error bounds that justify its\nexcellent performance for low-rank approximation of matrix functions."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"A Global Geometric Framework for Nonlinear Dimensionality Reduction",
    "start_abstract":"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure.",
    "start_categories":[
      "math.ST"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association"
      ],
      "abstract":[
        "We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure."
      ],
      "categories":[
        "cs.DC"
      ]
    },
    "list":{
      "title":[
        "The effect of accretion on scalar superradiant instability",
        "Scaling of the elastic proton-proton cross-section",
        "Feedback-enhanced squeezing or cooling of fluctuations in a parametric\n  resonator",
        "Ising superconductivity in noncentrosymmetric bulk NbSe2",
        "Investigating and Improving Counter-Stereotypical Action Relation in\n  Text-to-Image Diffusion Models",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "Selection Function of Clusters in Dark Energy Survey Year 3 Data from\n  Cross-Matching with South Pole Telescope Detections",
        "The Complexity of Local Stoquastic Hamiltonians on 2D Lattices",
        "Neural cyberattacks applied to the vision under realistic visual stimuli",
        "Proof-Producing Translation of Functional Programs into a Time \\& Space\n  Reasonable Model",
        "Neural Radiance Fields for the Real World: A Survey",
        "Israel-Hamas war through Telegram, Reddit and Twitter",
        "Roles of the $N(1535)$ and $a_0(980)$ in the process $\\Lambda_c^+ \\to\n  \\pi^+\\eta n$",
        "Resurgence of the Tilted Cusp Anomalous Dimension",
        "New Frontiers in Fighting Misinformation",
        "Hybrid sub- and superradiant states in emitter arrays with quantized\n  motion",
        "Probing new hadronic forces with heavy exotic atoms",
        "Modeling Feature Maps for Quantum Machine Learning",
        "Efficient Deployment of Large Language Models on Resource-constrained\n  Devices",
        "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor\n  Segmentation in PET-CT Images",
        "Two-stage Incomplete Utterance Rewriting on Editing Operation",
        "Elucidating the high compliance mechanism by which the urinary bladder\n  fills under low pressures",
        "Enhancing LLM Character-Level Manipulation via Divide and Conquer",
        "Growing black-hole hair in nonminimally coupled biscalar gravity",
        "Federated Learning in NTNs: Design, Architecture and Challenges",
        "Statistically validated projection of bipartite signed networks",
        "A Peanut-hull-PLA based 3D printing filament with antimicrobial effect",
        "Operator Spreading in Random Unitary Circuits with Unitary-invariant\n  Gate Distributions",
        "Data efficiency and long-term prediction capabilities for neural\n  operator surrogate models of edge plasma simulations"
      ],
      "abstract":[
        "Superradiance can lead to the formation of a black hole (BH) condensate\nsystem. We thoroughly investigate the accretion effect on the evolution of this\nsystem, and the gravitational wave signals it emits in the presence of multiple\nsuperradiance modes. Assuming the multiplication of the BH mass and scalar mass\nas a small number, we obtain the analytical approximations of all important\nquantities, which can be directly applied to phenomenological studies. In\naddition, we confirm that accretion could significantly enhance the\ngravitational wave (GW) emission and reduce its duration, and show that the GW\nbeat signature is similarly modified.",
        "We discuss scaling properties of the elastic $pp$ cross-section both at the\nISR and the LHC. We observe that the ratio of bump to dip positions of the\ndifferential cross-section $d\\sigma_{\\rm el}\/dt$ is constant over a wide energy\nrange. We next study the consequences of this property, including geometric\nscaling at the ISR and new scaling laws at the LHC.",
        "Here we analyse ways to achieve deep subthreshold parametric squeezing of\nfluctuations beyond the $-6$~dB limit of single degree-of-freedom parametric\nresonators. One way of accomplishing this is via a lock-in amplifier feedback\nloop. Initially, we calculate the phase-dependent parametric amplification with\nfeedback of an added ac signal. In one approach, we use the averaging method to\nobtain the amplification gain, while in the second approach, we obtain the ac\nresponse of the parametric amplifier with feedback using the harmonic balance\nmethod. In this latter approach, the feedback is proportional to an integral\nterm that emulates the cosine quadrature output of a lock-in amplifier\nmultiplied by a sine at the same tone of the lock-in. We find that the gain\nobtained via these two methods are the same whenever the integration time span\nof the integral is a multiple of the tone period. When this is not the case, we\ncan obtain considerable deamplification. Finally, we analyse the response of\nthe parametric resonator with feedback, described by this integro-differential\nmodel, to an added white noise in the frequency domain. Using this model we\nwere able to calculate, in addition to squeezing, the noise spectral density in\nthis resonator with feedback. Very strong squeezing or cooling can be obtained.",
        "Ising superconductivity allows in-plane upper critical magnetic fields to\nvastly surpass Pauli limit by locking the antiparallel electron spins of Cooper\npairs in the out-of-plane direction. It was first explicitly demonstrated in\nfully two-dimensional monolayers of transition metal dichalcogenides with large\nspin-orbit coupling and broken inversion symmetry. Since then, several studies\nhave shown that it can be present in layered bulk materials, too. In our\nprevious study, we have clarified the underlying microscopic mechanism of Ising\nsuperconductivity in bulk, based on a reduced electronic coupling between\nsuperconducting layers due to intercalation by insulating layers and restricted\ninversion symmetry. But earlier studies suggest that in some transition metal\ndichalcogenide polytypes Pauli paramagnetic limit is violated even without\nintercalation. Here, using heat capacity measurements we unambiguously\ndemonstrate, that the pristine noncentrosymmetric bulk 4Ha-NbSe2 polytype\nsignificantly violates the Pauli limit. The band structure parameters obtained\nfrom ab initio calculations using the experimentally determined crystal\nstructure are used in the theoretical model which provides the microscopic\nmechanism of the Ising protection based solely on broken inversion symmetry.",
        "Text-to-image diffusion models consistently fail at generating\ncounter-stereotypical action relationships (e.g., \"mouse chasing cat\"),\ndefaulting to frequent stereotypes even when explicitly prompted otherwise.\nThrough systematic investigation, we discover this limitation stems from\ndistributional biases rather than inherent model constraints. Our key insight\nreveals that while models fail on rare compositions when their inversions are\ncommon, they can successfully generate similar intermediate compositions (e.g.,\n\"mouse chasing boy\"). To test this hypothesis, we develop a Role-Bridging\nDecomposition framework that leverages these intermediates to gradually teach\nrare relationships without architectural modifications. We introduce\nActionBench, a comprehensive benchmark specifically designed to evaluate\naction-based relationship generation across stereotypical and\ncounter-stereotypical configurations. Our experiments validate that\nintermediate compositions indeed facilitate counter-stereotypical generation,\nwith both automatic metrics and human evaluations showing significant\nimprovements over existing approaches. This work not only identifies\nfundamental biases in current text-to-image systems but demonstrates a\npromising direction for addressing them through compositional reasoning.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Galaxy clusters selected based on overdensities of galaxies in photometric\nsurveys provide the largest cluster samples. Yet modeling the selection\nfunction of such samples is complicated by non-cluster members projected along\nthe line of sight (projection effects) and the potential detection of\nunvirialized objects (contamination). We empirically constrain the magnitude of\nthese effects by cross-matching galaxy clusters selected in the Dark Energy\nsurvey data with the \\rdmpr$\\,$ algorithm with significant detections in three\nSouth Pole Telescope surveys (SZ, pol-ECS, pol-500d). For matched clusters, we\naugment the \\rdmpr$\\,$catalog by the SPT detection significance. For unmatched\nobjects we use the SPT detection threshold as an upper limit on the SZe\nsignature. Using a Bayesian population model applied to the collected\nmulti-wavelength data, we explore various physically motivated models to\ndescribe the relationship between observed richness and halo mass. Our analysis\nreveals the limitations of a simple lognormal scatter model in describing the\ndata. We rule out significant contamination by unvirialized objects at the\nhigh-richness end of the sample. While dedicated simulations offer a\nwell-fitting calibration of projection effects, our findings suggest the\npresence of redshift-dependent trends that these simulations may not have\ncaptured. Our findings highlight that modeling the selection function of\noptically detected clusters remains a complicated challenge, requiring a\ncombination of simulation and data-driven approaches.",
        "We show the 2-Local Stoquastic Hamiltonian problem on a 2D square lattice is\nStoqMA-complete. We achieve this by extending the spatially sparse circuit\nconstruction of Oliveira and Terhal, as well as the perturbative gadgets of\nBravyi, DiVincenzo, Oliveira, and Terhal. Our main contributions demonstrate\nStoqMA circuits can be made spatially sparse and that geometrical,\nstoquastic-preserving, perturbative gadgets can be constructed.",
        "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine\nand designed to interact with the brain to record or stimulate neurons. Despite\ntheir benefits, the literature has demonstrated that invasive BCIs focused on\nneurostimulation present vulnerabilities allowing attackers to gain control. In\nthis context, neural cyberattacks emerged as threats able to disrupt\nspontaneous neural activity by performing neural overstimulation or inhibition.\nPrevious work validated these attacks in small-scale simulations with a reduced\nnumber of neurons, lacking real-world complexity. Thus, this work tackles this\nlimitation by analyzing the impact of two existing neural attacks, Neuronal\nFlooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of\nthe primary visual cortex of mice consisting of approximately 230,000 neurons,\ntested on three realistic visual stimuli: flash effect, movie, and drifting\ngratings. Each attack was evaluated over three relevant events per stimulus,\nalso testing the impact of attacking 25% and 50% of the neurons. The results,\nbased on the number of spikes and shift percentages metrics, showed that the\nattacks caused the greatest impact on the movie, while dark and fixed events\nare the most robust. Although both attacks can significantly affect neural\nactivity, JAM was generally more damaging, producing longer temporal delays,\nand had a larger prevalence. Finally, JAM did not require to alter many neurons\nto significantly affect neural activity, while the impact in FLO increased with\nthe number of neurons attacked.",
        "We present a semi-automated framework to construct and reason about programs\nin a deeply-embedded while-language. The while-language we consider is a simple\ncomputation model that can simulate (and be simulated by) Turing machines with\na linear time and constant space blow-up. Our framework derives while-programs\nfrom functional programs written in a subset of Isabelle\/HOL, namely\ntail-recursive functions with first-order arguments and algebraic datatypes. As\nfar as we are aware, it is the first framework targeting a computation model\nthat is reasonable in time and space from a complexity-theoretic perspective.",
        "Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since\nrelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,\nadvancing different fields and applications such as scene understanding, 3D\ncontent generation, and robotics. Despite significant research progress, a\nthorough review of recent innovations, applications, and challenges is lacking.\nThis survey compiles key theoretical advancements and alternative\nrepresentations and investigates emerging challenges. It further explores\napplications on reconstruction, highlights NeRFs' impact on computer vision and\nrobotics, and reviews essential datasets and toolkits. By identifying gaps in\nthe literature, this survey discusses open challenges and offers directions for\nfuture research.",
        "The Israeli-Palestinian conflict started on 7 October 2023, have resulted\nthus far to over 48,000 people killed including more than 17,000 children with\na majority from Gaza, more than 30,000 people injured, over 10,000 missing, and\nover 1 million people displaced, fleeing conflict zones. The infrastructure\ndamage includes the 87\\% of housing units, 80\\% of public buildings and 60\\% of\ncropland 17 out of 36 hospitals, 68\\% of road networks and 87\\% of school\nbuildings damaged. This conflict has as well launched an online discussion\nacross various social media platforms. Telegram was no exception due to its\nencrypted communication and highly involved audience. The current study will\ncover an analysis of the related discussion in relation to different\nparticipants of the conflict and sentiment represented in those discussion. To\nthis end, we prepared a dataset of 125K messages shared on channels in Telegram\nspanning from 23 October 2025 until today. Additionally, we apply the same\nanalysis in two publicly available datasets from Twitter containing 2001 tweets\nand from Reddit containing 2M opinions. We apply a volume analysis across the\nthree datasets, entity extraction and then proceed to BERT topic analysis in\norder to extract common themes or topics. Next, we apply sentiment analysis to\nanalyze the emotional tone of the discussions. Our findings hint at polarized\nnarratives as the hallmark of how political factions and outsiders mold public\nopinion. We also analyze the sentiment-topic prevalence relationship, detailing\nthe trends that may show manipulation and attempts of propaganda by the\ninvolved parties. This will give a better understanding of the online discourse\non the Israel-Palestine conflict and contribute to the knowledge on the\ndynamics of social media communication during geopolitical crises.",
        "We have investigated the process $\\Lambda_c^+ \\to \\pi^+\\eta n$ by taking into\naccount the contributions from the nucleon resonance $N(1535)$ and the scalar\nmeson $a_0(980)$, which could be dynamically generated by the interaction of\nthe $S$-wave pseudosalar meson-octet baryon and the $S$-wave pseudosalar\nmeson-pseudosalar meson, respectively. Our results show that, in $\\eta n$\ninvariant mass distribution, there is a significant near-threshold enhancement\nstructure, which could be associated with $N(1535)$. On the other hand, one can\nfind a clear cusp structure of $a_0(980)$ in $\\pi^+\\eta$ invariant mass\ndistribution. We further estimate the ratio $R$ = $\\mathcal{B}(\\Lambda_c^+ \\to\na_0(980)^+ n)\/\\mathcal{B}(\\Lambda_c^+ \\to \\pi^+\\eta n)\\approx 0.313$. Our\nresults can be tested by BESIII, Belle~II, and the proposed Super Tau-Charm\nFacility experiments in the future.",
        "We use resurgent extrapolation and continuation methods to extract detailed\nanalytic information about the tilted cusp anomalous dimension solely from its\nweak coupling and strong coupling expansions. This enables accurate and smooth\ninterpolation between the weak and strong coupling limits, and identifies the\nrelevant singularities governing the finite radius of convergence of the weak\ncoupling expansion and the asymptotic nature of the strong coupling expansion.\nThe input data is purely perturbative, generated from the BES equations, and\nthese resurgent methods extract accurate non-perturbative information which\nmatches the underlying physical structure.",
        "Despite extensive research and development of tools and technologies for\nmisinformation tracking and detection, we often find ourselves largely on the\nlosing side of the battle against misinformation. In an era where\nmisinformation poses a substantial threat to public discourse, trust in\ninformation sources, and societal and political stability, it is imperative\nthat we regularly revisit and reorient our work strategies. While we have made\nsignificant strides in understanding how and why misinformation spreads, we\nmust now broaden our focus and explore how technology can help realise new\napproaches to address this complex challenge more efficiently.",
        "Ensembles of dipolar emitters which couple collectively to the radiation\nfield display sub- and superradiance. These terms refer to a reduction or an\nenhancement of photon emission rates due to the interference of emission\nchannels. Arrays of trapped neutral atoms constitute a promising platform for\nharnessing this phenomenon in technological applications, e.g. for excitation\nstorage, single-photon switches and mirrors. However, vibrational motion of the\natoms within their traps leads to position fluctuations that entangle the\nmotion and the internal atomic degrees of freedom, which is expected to affect\nthe collective photon emission. We develop here a theory for collective\natom-light coupling in the presence of this quantized motion within the\nLamb-Dicke limit. We show the existence of sub- and superradiant states, which\nare hybrids of electronic and vibrational excitations and explore their\nproperties for analytically and numerically efficiently solvable cases.",
        "We explore the potential of precision spectroscopy of heavy exotic atoms\nwhere electrons are substituted by negative hadrons to detect new force\ncarriers with hadronic couplings. The selected transitions are unaffected by\nnuclear contact terms, thus enabling highly accurate calculations using\nbound-state QED, provided that the nuclear polarization is under control.\nAlternatively, we demonstrate that the dipole polarizability, a fundamental\nproperty of nuclei, can be extracted from the spectroscopy of exotic atoms in a\nnovel way by combining two transitions while maintaining high sensitivity to\nnew physics. Based on existing data, we extracted world-leading bounds on\nmediator masses ranging from $0.1\\,$MeV to $10\\,$MeV for two benchmark models\nand show that forthcoming experiments could enhance the sensitivity to new\nphysics by two orders of magnitude.",
        "Quantum Machine Learning (QML) offers significant potential for complex tasks\nlike genome sequence classification, but quantum noise on Noisy\nIntermediate-Scale Quantum (NISQ) devices poses practical challenges. This\nstudy systematically evaluates how various quantum noise models including\ndephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and\nphase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature\nmapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results\nindicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are\nmore sensitive, particularly to depolarizing and amplitude-damping noise. The\nPauliFeatureMap is especially vulnerable, highlighting difficulties in\nmaintaining accurate classification under noisy conditions. These findings\nunderscore the critical importance of feature map selection and noise\nmitigation strategies in optimizing QML for genomic classification, with\npromising implications for personalized medicine.",
        "Deploying Large Language Models (LLMs) on resource-constrained (or weak)\ndevices presents significant challenges due to limited resources and\nheterogeneous data distribution. To address the data concern, it is necessary\nto fine-tune LLMs using on-device private data for various downstream tasks.\nWhile Federated Learning (FL) offers a promising privacy-preserving solution,\nexisting fine-tuning methods retain the original LLM size, leaving issues of\nhigh inference latency and excessive memory demands unresolved. Hence, we\ndesign FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning\n(PEFT) with structured pruning for efficient deployment of LLMs on\nresource-constrained devices. Specifically, FedSpine introduces an iterative\nprocess to prune and tune the parameters of LLMs. To mitigate the impact of\ndevice heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed\nto adaptively determine different pruning ratios and LoRA ranks for\nheterogeneous devices without any prior knowledge of their computing and\ncommunication capabilities. As a result, FedSpine maintains higher inference\naccuracy while improving fine-tuning efficiency. Experimental results conducted\non a physical platform with 80 devices demonstrate that FedSpine can speed up\nfine-tuning by 1.4$\\times$-6.9$\\times$ and improve final accuracy by 0.4%-4.5%\nunder the same sparsity level compared to other baselines.",
        "Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps:\/\/github.com\/mj129\/CIPA.",
        "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.",
        "The high compliance of the urinary bladder during filling is essential for\nits proper function, enabling it to accommodate significant volumetric\nincreases with minimal rise in transmural pressure. This study aimed to\nelucidate the physical mechanisms underlying this phenomenon by analyzing the\nex vivo filling process in rat from a fully voided state to complete\ndistension, without preconditioning, using three complementary imaging\nmodalities. High-resolution micro-CT at 10.8 {\\mu}m resolution was used to\ngenerate detailed 3D reconstructions of the bladder lumen, revealing a 62 fold\nincrease in bladder volume during filling. Pressure-volume studies of whole\nbladder delineated three mechanical filling regimes: an initial high-compliance\nphase, a transitional phase, and a final high-pressure phase. While prior\nstudies conjectured small mucosal rugae (450 {\\mu}m) are responsible for the\nhigh compliance phase, multiphoton microscopy (MPM) of the dome of the voided\nbladder revealed large folds an order of magnitude larger than these rugae.\nBladder imaging during the inflation process demonstrated flattening of these\nlarge scale folds is responsible for volume increases in the initial high\ncompliance phase. The 3D reconstructions of the bladder lumen in the filled and\nvoided state revealed a high voiding efficiency of 97.13%. The MPM imaging\nresults suggest the large scale folds in the dome enable this high voiding\nfraction by driving urine toward the bladder outlet. These insights are vital\nfor computational models of bladder biomechanics and understanding changes to\nbladder function due to pathological conditions such as bladder outlet\nobstruction and age-related dysfunction.",
        "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.",
        "Black holes offer a unique laboratory for fundamental physics and are crucial\nfor probing theories beyond Einstein's theory of General Relativity. In this\npaper, we consider 4D effective field theories with scalar fields. We focus on\naxi-dilaton gravity, a quadratic gravity theory with two kinetically coupled\nscalar fields, an axion and a dilaton. To evolve these fields around black\nholes, we introduce Canuda-AxiDil, the first open-source, parameterized\nnumerical relativity code for quadratic and bi-scalar gravity. Using this code,\nwe perform single black hole simulations to show the dynamical formation of\naxion and dilaton hairs. Through these simulations, we measure the impact of\nblack-hole spin and curvature coupling strength on the axion and dilaton, and\nshow that a kinetic coupling between the fields increases the observed\ndeviations from General Relativity. Furthermore, we simulate the axion and\ndilaton fields around a binary black hole coalescence demonstrating the growth\nof axion hair during the inspiral and the production of radiative modes for\nboth fields.",
        "Non-terrestrial networks (NTNs) are emerging as a core component of future 6G\ncommunication systems, providing global connectivity and supporting\ndata-intensive applications. In this paper, we propose a distributed\nhierarchical federated learning (HFL) framework within the NTN architecture,\nleveraging a high altitude platform station (HAPS) constellation as\nintermediate distributed FL servers. Our framework integrates both low-Earth\norbit (LEO) satellites and ground clients in the FL training process while\nutilizing geostationary orbit (GEO) and medium-Earth orbit (MEO) satellites as\nrelays to exchange FL global models across other HAPS constellations worldwide,\nenabling seamless, global-scale learning. The proposed framework offers several\nkey benefits: (i) enhanced privacy through the decentralization of the FL\nmechanism by leveraging the HAPS constellation, (ii) improved model accuracy\nand reduced training loss while balancing latency, (iii) increased scalability\nof FL systems through ubiquitous connectivity by utilizing MEO and GEO\nsatellites, and (iv) the ability to use FL data, such as resource utilization\nmetrics, to further optimize the NTN architecture from a network management\nperspective. A numerical study demonstrates the proposed framework's\neffectiveness, with improved model accuracy, reduced training loss, and\nefficient latency management. The article also includes a brief review of FL in\nNTNs and highlights key challenges and future research directions.",
        "Bipartite networks provide a major insight into the organisation of many\nreal-world systems, describing the mechanisms that drive interactions between\ndistinct groups of nodes. Of particular interest are two-mode networks whose\nedges admit a sign: examples are provided by human interactions with entities\nsuch as products, where agents either cast either a positive or negative vote\nor abstain from voting at all. One of the most relevant issues encountered when\nmodelling a bipartite network is devising a way to obtain a monopartite\nprojection onto the layer of interest that preserves the information encoded\ninto the original structure as much as possible. In the present contribution,\nwe propose an unsupervised algorithm to obtain statistically validated\nprojections of bipartite signed networks, according to which any two nodes\nsharing a statistically significant number of concordant (discordant)\nrelationships are connected by a positive (negative) edge. More precisely, we\npropose two variants of it, according to the way ambivalent patterns and\nmissing ties are treated. Since assessing the statistical significance of any\ntwo nodes similarity requires a proper benchmark, here we consider four\ndifferent Exponential Random Graphs, defined by global as well as local\nconstraints, either leaving the topology free or keeping the topology fixed.\nOur algorithm outputs a matrix of link-specific $p-$values, from which a\nvalidated projection can be obtained upon running a multiple-hypothesis testing\nprocedure. After testing our method on synthetic configurations output by a\nfully controllable generative model, we apply it to several real-world\nconfigurations: in all cases, non-trivial mesoscopic structures, induced by\nrelationships that cannot be traced back to the constraints defining the\nemployed benchmarks, are detected.",
        "Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an\nabundant biomass source with a long shelf life. In this study, we incorporate\npeanut hull powder into PLA polymer, imparting recyclability, biodegradability,\nand biocompatibility, along with the antimicrobial properties of AHL particles.\nIn particular, we treat AHL particles as a reinforcement for PLA polymer to\nproduce 3D printing filament compatible with the fused filament fabrication\n(FFF) 3D printing method. We provide a step-by-step method for preparing AHL\nparticles, incorporating them into PLA, and ultimately forming high-quality\nfilaments. We assess the quality of the filaments in terms of extruded\ndimensions, mechanical strength, and elastic modulus, along with physical\nproperties such as porosity and melt flow index. We evaluate the printability\nand wettability of the filaments as well. Notably, and unlike other\nbiomass-based reinforcements in PLA, AHL preserves the filament's strength and\nenhances its elastic modulus. 3D-printed components fabricated using our\nPLA-AHL filaments successfully retain their antimicrobial properties and\nexhibit increased overall hardness. However, this comes at the expense of\nforming more microvoids and a rougher surface, making the material more prone\nto fracture and leading to a slight reduction in fracture toughness with\nincreasing AHL mass fraction.",
        "Random unitary circuits have become a model system to investigate information\nscrambling in quantum systems. In the literature, mostly random circuits with\nHaar-distributed gate operations have been considered. In this work, we\ninvestigate operator spreading in random unitary circuits in which the\nelementary gate operations are drawn from general unitary-invariant ensembles,\nwhich include the well-studied Haar-distributed random unitary circuits as a\nspecial case. Similar to the Haar-distributed case, the long-time behavior of\noperator spreading with the more general unitary-invariant gate distribution is\ngoverned by drift-diffusion equations characterized by the butterfly velocity\n$v_{\\rm B}$ and a diffusion constant $\\mathcal{D}$. Differences with the\nHaar-random case are (i) that it takes a finite time $\\tau_{\\rm b}$ until\nensemble-averaged Pauli-string weights take a ``binary'' form, in which they\ndepend only on whether Pauli operators inside the support of the Pauli strong\nare equal to the identity matrix, and (ii) that the operator spreading is\ncharacterized by a finite ``domain-wall width'' $n_{\\rm DW}$ separating regions\nwith a random-matrix-like Pauli-string distribution. To illustrate these\nfindings, we perform explicit calculations for random unitary circuits\ndistributed according to the Poisson kernel, which interpolates between the\ntrivial and Haar-distributed circuits.",
        "Modelling of plasma dynamics is fundamental to ensure appropriate diverter\nand core performance, and is desirable for both interpreting the current\ngeneration of experiments and informing the next generation devices like ITER\n\\cite{Loarte2007Chapter4P,Eich2013ScalingOT}. Yet the computational expense of\nmany plasma simulations makes them unsuitable for real-time applications or\niterative design workflows. Neural operator surrogate models of JOREK\n\\cite{Hoelzl_2021} and STORM \\cite{Walkden2016-ys} are evaluated, investigating\ntheir capability to replicate plasma dynamics accurately whilst reducing\ncomputational cost. It is found that the accuracy of the surrogate models will\ndegrade for long term predictions, and that physics considerations are\nimportant in assessing the performance of the surrogates. Surrogates trained on\none dataset can be effectively fine tuned with only a few simulations from a\ntarget domain. This is particularly effective where the source domain is a low\nfidelity physics model and the target domain is a high fidelity model, with an\norder of magnitude improvement in performance for a small dataset and a short\nrollout."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "start_abstract":"When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Symbolic Transfer Entropy"
      ],
      "abstract":[
        "We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "A domain decomposition strategy for natural imposition of mixed boundary\n  conditions in port-Hamiltonian systems",
        "Non-reciprocal interactions drive emergent chiral crystallites",
        "Reduced Basis Model for Compressible Flow",
        "Double Momentum and Error Feedback for Clipping with Fast Rates and\n  Differential Privacy",
        "Application of resolved low-J multi-CO line modeling with RADEX to\n  constrain the molecular gas properties in the starburst M82",
        "A regional implementation of a mixed finite-element, semi-implicit\n  dynamical core",
        "1\/f and Random Telegraph Noise of Single-Layer Graphene Devices with\n  Interdigitated Electrodes",
        "Multivariate Distribution-Free Nonparametric Testing: Generalizing\n  Wilcoxon's Tests via Optimal Transport",
        "Spectrum management and the EVN",
        "Multipole generalization of the Witten effect in Mie-resonant photonics",
        "Congruence properties of prime sums and Bernoulli polynomials",
        "Determination of unscaled blood input for human dynamic FDG brain PET",
        "Harnessing Hybrid Frequency-Entangled Qudits through Quantum\n  Interference",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Quantum Maslov classes",
        "Triangulations of the `magic manifold' and families of census knots",
        "Positive Polytopes with Few Facets in the Grassmannian",
        "Speeding up Lindblad dynamics via time-rescaling engineering",
        "Tomographic identification of all molecular orbitals in a wide binding\n  energy range",
        "Chemical abundance ratios for the bulge of M31",
        "Simulation of current-driven magnetisation switching in nanopillars with\n  Perpendicular Shape Anisotropy",
        "Non-perturbative corrections in the semi-classical limit of\n  double-scaled SYK",
        "Proof-theoretic dilator and intermediate pointclasses",
        "On the Minimax Regret of Sequential Probability Assignment via\n  Square-Root Entropy",
        "Einstein-Maxwell-Dilaton Wormholes that meet the Energy Conditions",
        "Preparing for the 2061 return of Halley's comet -- A rendezvous mission\n  with an innovative imaging system",
        "Certified Inductive Synthesis for Online Mixed-Integer Optimization",
        "A general form of Newton-Maclaurin type inequalities",
        "Relating elliptic curve point-counting and solutions of quadratic forms\n  with congruence conditions"
      ],
      "abstract":[
        "In this contribution, a finite element scheme to impose mixed boundary\nconditions without introducing Lagrange multipliers is presented for wave\npropagation phenomena described as port-Hamiltonian systems. The strategy\nrelies on finite element exterior calculus and a domain decomposition to\ninterconnect two systems with different causalities. The spatial domain is\nsplit into two parts by introducing an arbitrary interface. Each subdomain is\ndiscretized with a mixed finite element formulation that introduces a uniform\nboundary condition in a natural way as the input. In each subdomain the spaces\nare selected from a finite element subcomplex to obtain a stable\ndiscretization. The two systems are then interconnected together by making use\nof a feedback interconnection. This is achieved by discretizing the boundary\ninputs using appropriate spaces that couple the two formulations. The final\nsystems includes all boundary conditions explicitly and does not contain any\nLagrange multiplier. Each subdomain is integrated using an implicit midpoint\nscheme in an uncoupled way from the other by means of a leapfrog scheme. The\nproposed strategy is tested on three different examples: the Euler-Bernoulli\nbeam, the wave equation and the Maxwell equations. Numerical tests assess the\nconservation properties of the scheme and the effectiveness of the methodology.",
        "We study a new type of 2D active material that exhibits macroscopic phases\nwith two emergent broken symmetries: self-propelled achiral particles that form\ndense hexatic clusters, which spontaneously rotate. We experimentally realise\nactive colloids that self-organise into both polar and hexatic crystallites,\nexhibiting exotic emergent phenomena. This is accompanied by a field theory of\ncoupled order parameters formulated on symmetry principles, including\nnon-reciprocity, to capture the non-equilibrium dynamics. We find that the\npresence of two interacting broken symmetry fields leads to the emergence of\nnovel chiral phases built from (2D) achiral active colloids (here Quincke\nrollers). These phases are characterised by the presence of both clockwise and\ncounterclockwise rotating clusters. We thus show that spontaneous rotation can\nemerge in non-equilibrium systems, even when the building blocks are achiral,\ndue to non-reciprocally coupled broken symmetries. This interplay leads to\nself-organized stirring through counter-rotating vortices in confined colloidal\nsystems, with cluster size controlled by external electric fields.",
        "Numerical simulations are a valuable research and layout tool for fluid flow\nproblems, yet repeated evaluations of parametrized problems, necessary to solve\noptimization problems, can be very costly. One option to speed up this process\nis to replace the costly CFD model with a cheaper one. These surrogate models\ncan be either data-driven or they can also rely on reduced basis (RB) methods\nto speed up the calculations. In contrast to data-driven surrogate models, the\nlatter are not based on regression techniques but are still aimed at explicitly\nsolving the conservation equations. Their speed-up comes from a strong\nreduction of the solution space, which results in much smaller algebraic\nsystems that need to be solved. Within this work, an RB model, suited for\nslightly compressible flow, is presented and tested on different flow\nconfigurations. The model is stabilized using a Petrov-Galerkin method with\ntrial and test function spaces of different dimensionality to generate stable\nresults for a wide range of Reynolds numbers. The presented model applies to\ngeometrically and physically parametrized flow problems. Finally, a data-driven\napproach was used to extend it to turbulent flows.",
        "Strong Differential Privacy (DP) and Optimization guarantees are two\ndesirable properties for a method in Federated Learning (FL). However, existing\nalgorithms do not achieve both properties at once: they either have optimal DP\nguarantees but rely on restrictive assumptions such as bounded\ngradients\/bounded data heterogeneity, or they ensure strong optimization\nperformance but lack DP guarantees. To address this gap in the literature, we\npropose and analyze a new method called Clip21-SGD2M based on a novel\ncombination of clipping, heavy-ball momentum, and Error Feedback. In\nparticular, for non-convex smooth distributed problems with clients having\narbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal\nconvergence rate and also near optimal (local-)DP neighborhood. Our numerical\nexperiments on non-convex logistic regression and training of neural networks\nhighlight the superiority of Clip21-SGD2M over baselines in terms of the\noptimization performance for a given DP-budget.",
        "The distribution and physical conditions of molecular gas are closely linked\nto star formation and the subsequent evolution of galaxies. Emission from\ncarbon monoxide (CO) and its isotopologues traces the bulk of molecular gas and\nprovides constraints on the physical conditions through their line ratios.\nHowever, comprehensive understanding on how the particular choice of line\nmodeling approach impacts derived molecular properties remain incomplete. Here,\nwe study the nearby starburst galaxy M82, known for its intense star formation\nand molecular emission, using the large set of available multi-CO line\nobservations. We present high-resolution (${\\sim}85$ pc) emission of seven CO\nisotopologue lines, including $^{12}$CO, $^{13}$CO, and C$^{18}$O from the $J =\n1-0$, $2-1$ and $3-2$ transitions. Using \\texttt{RADEX} for radiative transfer\nmodeling, we analyze M82\\textsc{\\char39}s molecular properties with (i) a\none-zone model and (ii) a variable density model, comparing observed and\nsimulated emissions via a minimum $\\chi^2$ analysis. We find that inferred gas\nconditions -- kinetic temperature and density -- are consistent across models,\nwith minimal statistical differences. However, due to their low critical\ndensities (${<}10^{4}$ cm$^{-3}$), low-$J$ CO isotopologue lines do not\neffectively probe higher density gas prevalent in starburst environments like\nthat of M82. Our results further imply that this limitation extends to\nhigh-redshift ($z{\\gtrapprox}1$) galaxies with similar conditions, where\nlow-$J$ CO lines are inadequate for density constraints. Future studies of\nextreme star-forming regions like M82 will require higher-$J$ CO lines or\nalternative molecular tracers with higher critical densities.",
        "This paper explores how to adapt a new dynamical core to enable its use in\none-way nested regional weather and climate models, where lateral boundary\nconditions (LBCs) are provided by a lower-resolution driving model. The\ndynamical core has recently been developed by the Met Office and uses an\niterated-semi-implicit time discretisation and mixed finite-element spatial\ndiscretisation.\n  The essential part of the adaptation is the addition of the LBCs to the\nright-hand-side of the linear system which solves for pressure and momentum\nsimultaneously. The impacts on the associated Helmholtz preconditioner and\nmultigrid techniques are also described.\n  The regional version of the dynamical core is validated through big-brother\nexperiments based on idealised dynamical core tests. These experiments\ndemonstrate that the subdomain results are consistent with those from the full\ndomain, confirming the correct application of LBCs. Inconsistencies arise in\ncases where the LBCs are not perfect, but it is shown that the application of\nblending can be used to overcome these problems.",
        "Single-layer Graphene (SLG) is a promising material for sensing applications.\nHigh performance graphene sensors can be achieved when Interdigitated\nElectrodes (IDE) are used. In this research work, we fabricated SLG\nmicro-ribbon (GMR) devices with IDE having different geometric parameters. 1\/f\nnoise behavior was observed in all of the examined devices, and in some cases\nrandom telegraph noise (RTN) signals suggesting that carrier\ntrapping\/de-trapping is taking place. Our experimental results indicate that\nthe geometrical characteristics can have a crucial impact on device\nperformance, due to the direct area dependence of the noise level.",
        "This paper reviews recent advancements in the application of optimal\ntransport (OT) to multivariate distribution-free nonparametric testing.\nInspired by classical rank-based methods, such as Wilcoxon's rank-sum and\nsigned-rank tests, we explore how OT-based ranks and signs generalize these\nconcepts to multivariate settings, while preserving key properties, including\ndistribution-freeness, robustness, and efficiency. Using the framework of\nasymptotic relative efficiency (ARE), we compare the power of the proposed\n(generalized Wilcoxon) tests against the Hotelling's $T^2$ test. The ARE lower\nbounds reveal the Hodges-Lehmann and Chernoff-Savage phenomena in the context\nof multivariate location testing, underscoring the high power and efficiency of\nthe proposed methods. We also demonstrate how OT-based ranks and signs can be\nseamlessly integrated with more modern techniques, such as kernel methods, to\ndevelop universally consistent, distribution-free tests. Additionally, we\npresent novel results on the construction of consistent and distribution-free\nkernel-based tests for multivariate symmetry, leveraging OT-based ranks and\nsigns.",
        "In recent years, the utilisation of the radio spectrum has dramatically\nincreased. Digital telecommunication applications, be it terrestrial cell-phone\nnetworks or new-space low-earth orbit satellite constellations, have not only\nacquired unprecedented amounts of spectrum but also use their frequencies\neverywhere on Earth. The consequences for radio astronomy and other scientific\nradio services are severe. A single cell-phone tower within hundreds of\nkilometers around a radio telescope can blind us and there is no place on Earth\nto escape the ubiquitous transmissions of satellite megaconstellations.\n  Since 1988, the Committee on Radio Astronomy Frequencies (CRAF) is advocating\nfor astronomers' rights to use the spectrum. CRAF does this by participation in\nthe national and international regulatory frameworks. Hundreds if not thousands\nof documents need to be processed every year. CRAF not only contributes to\nregulatory texts, but even more importantly, performs spectrum compatibility\ncalculations. In this contribution, CRAF's latest activities are summarized\nwith a focus on matters relevant to EVN operations.",
        "We present a generalization of the Witten effect on the case of oscillating\nmultipole sources exciting nonreciprocal sphere with effective axion response.\nWe find that the fields outside of the sphere are presented as a superposition\nof electric and magnetic multipoles. In addition to appearance of\ncross-polarized component in the radiation, Mie resonances of the system\nhybridize with each other, exhibiting characteristic double peaks in Mie\nspectra observed especially clearly for higher-order multipole resonances. This\ncharacteristic feature may provide a sensitive probe of axion-type\nnonreciprocal responses in Mie-resonant photonics.",
        "In this article, we derive a congruence property of particular sum rules\ninvolving prime numbers. The resulting expression involves Bernoulli numbers\nand polynomials, for which we obtain, as a consequence, a general congruence\nrelation as well.",
        "Objectives: Many existing techniques for the non-invasive quantification of\nthe blood input function in dynamic FDG-PET imaging require strong historical\ninformation or user input. The technique proposed in this work utilizes the\nassumption that a dynamic PET scan can be modeled by the Patlak plot to\ndetermine an unscaled blood input function. Materials and Methods: The time\nactivity curve (TAC) for each voxel in a dynamic image can be considered as an\nn-dimensional vector. In this context, a TAC follows the Patlak plot if and\nonly if the TAC is a linear combination of the blood input function and the\nintegral of the blood input function. Given a set of TACs which follow the\nPatlak plot, we can thus use PCA to determine a basis which spans the same\nvector space as the blood input function and the integral of the blood input\nfunction. We then seek to find two TACs in this vector space which best satisfy\nthat the estimated anti-derivative of one of the TACs is close to the other\nTAC; such TACs are candidates for the blood input function and the integral of\nthe blood input function. We were able to construct a low (2) dimensional\noptimization problem to find such TACs. Results: We applied our results to\nobtain predicted blood input functions and Ki maps for twelve normal subjects.\nScaling the predicted blood input function to best match the ground truth, we\nachieved an average SSE of $0.042 \\pm 0.032$ and an average DTW distance of\n$0.141 \\pm 0.053$. Matching the means of the predicted and ground truth Ki\nmaps, we achieved an average MAPE of $2.539 \\pm 0.928$ and an average SSIM of\n$0.991 \\pm 0.005$. Conclusion: While not often viewed as such, the assumption\nthat some dynamic data follows a kinetic model gives strong prior information.\nIn the case of the Patlak plot, we can use this assumption to estimate an\nunscaled blood input function and unscaled Ki map.",
        "High-dimensional (HD) quantum entanglement expands the Hilbert space,\noffering a robust framework for quantum information processing with enhanced\ncapacity and error resilience. In this work, we present a novel HD\nfrequency-domain entangled state, the hybrid frequency-entangled qudit (HFEQ),\ngenerated via Hong-Ou-Mandel (HOM) interference, exhibiting both\ndiscrete-variable (DV) and continuous-variable (CV) characteristics. By tuning\nHOM interference, we generate and control HFEQs with dimensions $D=5,7,9,11$,\nconfirming their DV nature. Franson interferometry confirms the global\nfrequency correlations with visibility exceeding 98% and verifies the CV\nentanglement within individual frequency modes with visibility greater than\n95%. Our findings provide deeper insight into the physical nature of\nfrequency-entangled qudits generated by quantum interference and introduce a\nnovel resource for HD time-frequency quantum information processing.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "We give a construction of ``quantum Maslov characteristic classes'',\ngeneralizing to higher dimensional cycles the Hu-Lalonde-Seidel morphism. We\nalso state a conjecture extending this to an $A _{\\infty}$ functor from the\nexact path category of the space of monotone Lagrangian branes to the Fukaya\ncategory. Quantum Maslov classes are used here for the study of Hofer geometry\nof Lagrangian equators in $S ^{2}$, giving a rigidity phenomenon for the Hofer\nmetric 2-systole, which stands in contrast to the flexibility phenomenon of the\nclosely related Hofer metric girth studied by Rauch ~\\cite{cite_Itamar}, in the\nsame context of Lagrangian equators of $S ^{2}$. More applications appear in\n~\\cite{cite_SavelyevGlobalFukayacategoryII}.",
        "We describe five ideal triangulations of the 3-cusped hyperbolic `magic\nmanifold' that are each compatible with well-established techniques for\ntriangulating Dehn fillings. Using these techniques, we construct\nlow-complexity triangulations for all partial fillings of the magic manifold,\nand in particular, recover minimal triangulations for 229 of the hyperbolic\ncensus knots. Along the way, these census knots are sorted into 42 families\nrelated by twisting that can be extended indefinitely, with each member of each\ninfinite family inheriting an upper bound on its triangulation complexity.\nThese triangulations are conjectured to be minimal for all 42 families.",
        "In this article we study adjoint hypersurfaces of geometric objects obtained\nby intersecting simple polytopes with few facets in $\\mathbb{P}^5$ with the\nGrassmannian $\\mathrm{Gr}(2,4)$. These generalize the positive Grassmannian,\nwhich is the intersection of $\\mathrm{Gr}(2,4)$ with the simplex. We show that\nif the resulting object has five facets, it is a positive geometry and the\nadjoint hypersurface is unique. For the case of six facets we show that the\nadjoint hypersurface is not necessarily unique and give an upper bound on the\ndimension of the family of adjoints. We illustrate our results with a range of\nexamples. In particular, we show that even if the adjoint is not unique, a\npositive hexahedron can still be a positive geometry.",
        "We introduce a universal method for accelerating Lindblad dynamics that\npreserves the original trajectory. The technique provides exact fast processes\nanalytically, which are Markovian with time-independent Lindblad operators, by\ntime-rescaling a reference dynamics. In particular, the engineered control\nprotocols are based only on local interactions, and no additional control\nfields are required compared to the reference protocol. We demonstrate the\nscheme with two examples: a driven two-level system in an amplitude damping\nchannel and the dissipative transverse field Ising model. Our approach can help\nadvance techniques for quantum control and computation towards more complex\nnoisy systems.",
        "In the past decade, photoemission orbital tomography (POT) has evolved into a\npowerful tool to investigate the electronic structure of organic molecules\nadsorbed on surfaces. Here we show that POT allows for the comprehensive\nexperimental identification of all molecular orbitals in a substantial binding\nenergy range, in the present case more than 10 eV. Making use of the angular\ndistribution of photoelectrons as a function of binding energy, we exemplify\nthis by extracting orbital-resolved partial densities of states (pDOS) for 15\n$\\pi$ and 23 $\\sigma$ orbitals from the experimental photoemission intensities\nof the prototypical organic molecule bisanthene (C$_{28}$H$_{14}$) on a Cu(110)\nsurface. In their entirety, these experimentally measured orbital-resolved pDOS\nfor an essentially complete set of orbitals serve as a stringent benchmark for\nelectronic structure methods, which we illustrate by performing density\nfunctional theory (DFT) calculations employing four frequently-used\nexchange-correlation functionals. By computing the respective\nmolecular-orbital-projected densities of states of the bisanthene\/Cu(110)\ninterface, a one-to-one comparison with experimental data for an unprecedented\nnumber of 38 orbital energies becomes possible. The quantitative analysis of\nour data reveals that the range-separated hybrid functional HSE performs best\nfor the investigated organic\/metal interface. At a more fundamental level, the\nremarkable agreement between the experimental and the Kohn-Sham orbital\nenergies over a binding energy range larger than 10\\,eV suggests that --\nperhaps unexpectedly -- Kohn-Sham orbitals approximate Dyson orbitals, which\nwould rigorously account for the electron extraction process in photoemission\nspectroscopy but are notoriously difficult to compute, in a much better way\nthan previously thought.",
        "We present abundance ratio estimates of individual elements, namely C, N, Na,\nand the so-called alpha elements, Mg, O, Si, Ca, and Ti, for the bulge of M31.\nThe analysis is based on long-slit, high-quality spectroscopy of the bulge,\ntaken with the OSIRIS spectrograph at the Gran Telescopio CANARIAS (GTC).\nAbundance ratios, [X\/Fe]s, are inferred by comparing radially binned spectra of\nM31 with different state-of-the-art stellar population models, averaging out\nresults from various methods, namely full-spectral, full-index, and\nline-strength fitting, respectively. For the bulk of the bulge, we find that O,\nN, and Na are significantly enhanced compared to Fe, with abundances of about\n0.3dex, followed by C, Mg, and Si, with [X\/Fe] about 0.2dex, and lastly, Ti and\nCa, mostly tracking Fe ([X\/Fe]<0.1dex), within the error bars. Performing the\nsame analysis on SDSS stacked spectra of early-type galaxies with different\nvelocity dispersion, we find that the abundance pattern of the M31 bulge is\nvery similar to that of most massive galaxies, supporting a scenario where most\nof the bulge formed in a fast and intense episode of star-formation.",
        "The Perpendicular Shape Anisotropy Spin Transfer Torque Magnetic Random\nAccess Memory (PSA-STT-MRAM) is a recent concept proposed to maintain the\nthermal stability of standard MRAM at small diameters, considering thick\nvertical pillars as the free layer. In order to explore the specific physics of\nPSA-STT-MRAMs expected in relation with their three-dimensional nature, we have\nperformed simulations combining a micromagnetic model coupled self-consistently\nwith spin-dependent transport equations. The 3D shape induces flower states at\nthe upper and lower surfaces. Besides, the field-like component of STT is found\nto be larger than in standard MRAMs, suggesting that it needs to be considered.\nThe combination of both effects leads to the excitation of high-order 3D\nferromagnetic resonance modes, playing a key role in magnetisation reversal.\nThese results highlight features of 3D nanomagnetic systems, largely\ndisregarded so far, which need to be considered to optimise PSA-STT-MRAM to be\na competitive solution for technological implementation.",
        "We study the disk partition function of double-scaled SYK model (DSSYK) in\nthe small $\\lambda$ limit, where $\\lambda=-\\log q$ is the coupling of DSSYK. We\nfind that the partition function receives non-perturbative corrections in\n$\\lambda$, which can be resummed by the cubic power of the Dedekind eta\nfunction in a certain low temperature limit. We also discuss a possible bulk\ninterpretation of our findings.",
        "There are two major generalizations of the standard ordinal analysis: One is\nGirard's $\\Pi^1_2$-proof theory in which dilators are assigned to theories\ninstead of ordinals. The other is Pohlers' generalized ordinal analysis with\nSpector classes, where ordinals greater than $\\omega_1^{\\mathsf{CK}}$ are\nassigned to theories. In this paper, we show that these two are systematically\nentangled, and $\\Sigma^1_2$-proof theoretic analysis has a critical role in\nconnecting these two.",
        "We study the problem of sequential probability assignment under logarithmic\nloss, both with and without side information. Our objective is to analyze the\nminimax regret -- a notion extensively studied in the literature -- in terms of\ngeometric quantities, such as covering numbers and scale-sensitive dimensions.\nWe show that the minimax regret for the case of no side information\n(equivalently, the Shtarkov sum) can be upper bounded in terms of sequential\nsquare-root entropy, a notion closely related to Hellinger distance. For the\nproblem of sequential probability assignment with side information, we develop\nboth upper and lower bounds based on the aforementioned entropy. The lower\nbound matches the upper bound, up to log factors, for classes in the Donsker\nregime (according to our definition of entropy).",
        "One of the latest predictions of Einstein's theory is the existence of\nWormholes (WH). In this work, we present exact solutions of the\nEinstein-Maxwell-Dilaton equations representing traversable Wormholes. These\nsolutions satisfy the energy conditions and have a ring singularity satisfying\nthe cosmic censorship of WHs, i.e. we show that, as in previous solutions,\ngeodesics cannot touch the singularity. We find that the most optimal input\nregions for the first class of solutions traversing these wormholes are near\nthe poles and near the equatorial plane for the second class. We also find that\nthe solution associated with the first class is physically feasible, while for\nthe second class it presents the problem of not being asymptotically flat when\nconsidering a dilatonic-type scalar field. Finally, we give examples of\nrealistic astrophysical objects that could fulfill these conditions.",
        "The return of Comet 1P\/Halley will promote a wide interest for ground and\nspace observations of a celestial body of outstanding scientific and cultural\ninterest. In addition to remote observations, space will open the possibility\nof in situ science similarly to the passage of 1986. In this paper, we first\ndiscuss the scientific motivations for a rendezvous mission, capable to\novercome the limitations of the flyby missions that took place at that time. In\nthe second part, we describe an example of a rendezvous trajectory that can be\ncarried out with existing power and propulsion technologies. The transfer is\nmade possible by the gravitational assistance of a giant planet. The resulting\nmission will be capable to reach the comet beyond the distance of Saturn, when\nthe sublimation of super-volatile species will be ongoing, and well before the\nonset of the sublimation of water (4 AU). After rendezvous, the spacecraft will\naccompany the comet for several years before, around and after perihelion (July\n2061). Our concept mission does not foresee the implementation of solar panels.\nIn this way, operations can occur even inside the dense dust coma at short\ndistance from the nucleus. In the third part of the paper, an innovative\nimaging system is proposed, with a very large field of view (100{\\deg}) capable\nto record on the same frame details on the surface and the surrounding space,\nin order to follow for several degrees the trajectories of chunks and clouds\nejected by pits or fractures, crucial to the understanding of the cometary\nactivity. A concerted effort is needed in the current decade to plan and\napprove a rendezvous mission to 1P. Indeed, the scenario here described\nrequires launching before 2040, less than 15 years from now. Later launches\nimply a severe loss of scientific knowledge, because the spacecraft will not be\nable to reach the comet before the onset of water sublimation.",
        "In fields such as autonomous and safety-critical systems, online optimization\nplays a crucial role in control and decision-making processes, often requiring\nthe integration of continuous and discrete variables. These tasks are\nfrequently modeled as mixed-integer programming (MIP) problems, where feedback\ndata are incorporated as parameters. However, solving MIPs within strict time\nconstraints is challenging due to their $\\mathcal{NP}$-complete nature. A\npromising solution to this challenge involves leveraging the largely invariant\nstructure of these problems to perform most computations offline, thus enabling\nefficient online solving even on platforms with limited hardware capabilities.\nIn this paper we present a novel implementation of this strategy that uses\ncounterexample-guided inductive synthesis to split the MIP solution process\ninto two stages. In the offline phase, we construct a mapping that provides\nfeasible assignments for binary variables based on parameter values within a\nspecified range. In the online phase, we solve the remaining continuous part of\nthe problem by fixing the binary variables to the values predicted by this\nmapping. Our numerical evaluation demonstrates the efficiency and solution\nquality of this approach compared to standard mixed-integer solvers,\nhighlighting its potential for real-time applications in resource-constrained\nenvironments.",
        "In this paper, we extend the classical Newton-Maclaurin inequalities to\nfunctions $S_{k;s}(x)=E_k(x)+\\dsum_{i=1}^s \\al_i E_{k-i}(x)$, which are formed\nby linear combinations of multiple basic symmetric mean. We proved that when\nthe coefficients $\\al_1,\\al_2,\\cdots,\\al_s$ satisfy the condition that the\npolynomial $$t^s+\\al_1 t^{s-1}+\\al_2 t^{s-2}+\\cdots+\\al_s $$ has only real\nroots, the Newton-Maclaurin type inequalities hold for $S_{k;s}(x)$.",
        "In this paper, we analyze the theta series associated to the quadratic form\n$Q(\\vec{x}) = x_1^2+x_2^2+x_3^2+x_4^2$ with congruence conditions on $x_i$\nmodulo $2,3,4$ and $6$. By employing special operators on modular,\nnon-holomorphic Eisenstein series of weight 2, we construct a basis for\nEisenstein space for levels $2^k, k\\leq 7$, $3^{\\ell}, \\ell\\leq 3$ and $p$, for\nodd prime $p$. By analyzing cusp form part of theta series corresponding to\n$Q(\\vec{x})$, we prove a linear relation between the number of integer\nsolutions to the equation $Q(\\vec{x}) = p$ under the congruence condition $x_i\n\\equiv 1 \\pmod{3}$ and the number of $\\mathbb{F}_p$-rational points on the\nelliptic curve $y^2=x^3+1$ for primes $p \\equiv 1 \\pmod{6}$."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Symbolic Transfer Entropy",
    "start_abstract":"We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer"
      ],
      "abstract":[
        "When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Collective Reasoning Among LLMs A Framework for Answer Validation\n  Without Ground Truth",
        "Predictive Target-to-User Association in Complex Scenarios via\n  Hybrid-Field ISAC Signaling",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Structure and Context of Retweet Coordination in the 2022 U.S. Midterm\n  Elections",
        "AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN\n  Model",
        "Ro-To-Go! Robust Reactive Control with Signal Temporal Logic",
        "Inverse Intersections for Boolean Satisfiability Problems",
        "Electromagnetic Side-Channel Analysis of PRESENT Lightweight Cipher",
        "Local well-posedness for nonlinear Schr\\\"odinger equations on compact\n  product manifolds",
        "Spectrum of L\\'evy-Ornstein-Uhlenbeck semigroups on $\\mathbb{R}^d$",
        "Spike-and-Slab Posterior Sampling in High Dimensions",
        "Tight Bounds for some Classical Problems Parameterized by Cutwidth",
        "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review",
        "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models",
        "Complexity of approximate conflict-free, linearly-ordered, and\n  nonmonochromatic hypergraph colourings",
        "Test-Time Compute: from System-1 Thinking to System-2 Thinking",
        "AdaptiveCoPilot: Design and Testing of a NeuroAdaptive LLM Cockpit\n  Guidance System in both Novice and Expert Pilots",
        "Dagger Behind Smile: Fool LLMs with a Happy Ending Story",
        "Knudsen boundary layer equations with incoming boundary condition: full\n  range of cutoff collision kernels and Mach numbers of the far field",
        "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation",
        "Drift: Decoding-time Personalized Alignments with Implicit User\n  Preferences",
        "Open-Source Tool for Evaluating Human-Generated vs. AI-Generated Medical\n  Notes Using the PDQI-9 Framework",
        "Scalable Decision-Making in Stochastic Environments through Learned\n  Temporal Abstraction",
        "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
        "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark\n  Challenging to Frontier LLMs",
        "Improving Adaptive Density Control for 3D Gaussian Splatting",
        "A Multi-Agent Framework for Automated Vulnerability Detection and Repair\n  in Solidity and Move Smart Contracts",
        "Deep Learning-Driven Malware Classification with API Call Sequence\n  Analysis and Concept Drift Handling",
        "InfoFusion Controller: Informed TRRT Star with Mutual Information based\n  on Fusion of Pure Pursuit and MPC for Enhanced Path Planning"
      ],
      "abstract":[
        "We present a collaborative framework where multiple large language models,\nnamely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash, work together to generate and respond to complex PhD-level\nprobability questions in the absence of definitive ground truth. This study\nexplores how inter-model consensus enhances response reliability and serves as\na proxy for assessing the quality of generated questions. To quantify agreement\nand consistency, we employ statistical methods including chi-square tests,\nFleiss' Kappa, and confidence interval analysis, measuring both response\nprecision and question clarity. Our findings highlight that Claude and Gemini\ngenerate well-structured and less ambiguous questions, leading to higher\ninter-model agreement. This is reflected in their narrower confidence intervals\nand stronger alignment with answering models. Conversely, LLaMA demonstrates\nincreased variability and lower reliability in question formulation, as\nindicated by broader confidence intervals and reduced consensus rates. These\nresults suggest that multi-model collaboration not only enhances the\nreliability of responses but also provides a valuable framework for assessing\nand improving question quality in the absence of explicit ground truth. This\nresearch offers meaningful insights into optimizing AI-driven reasoning through\ncollaborative large-language model interactions.",
        "This paper presents a novel and robust target-to-user (T2U) association\nframework to support reliable vehicle-to-infrastructure (V2I) networks that\npotentially operate within the hybrid field (near-field and far-field). To\naddress the challenges posed by complex vehicle maneuvers and user association\nambiguity, an interacting multiple-model filtering scheme is developed, which\ncombines coordinated turn and constant velocity models for predictive\nbeamforming. Building upon this foundation, a lightweight association scheme\nleverages user-specific integrated sensing and communication (ISAC) signaling\nwhile employing probabilistic data association to manage clutter measurements\nin dense traffic. Numerical results validate that the proposed framework\nsignificantly outperforms conventional methods in terms of both tracking\naccuracy and association reliability.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "The ability to detect coordinated activity in communication networks is an\nongoing challenge. Prior approaches emphasize considering any activity\nexceeding a specific threshold of similarity to be coordinated. However,\nidentifying such a threshold is often arbitrary and can be difficult to\ndistinguish from grassroots organized behavior. In this paper, we investigate a\nset of Twitter retweeting data collected around the 2022 US midterm elections,\nusing a latent sharing-space model, in which we identify the main components of\nan association network, thresholded with a k-nearest neighbor criterion. This\napproach identifies a distribution of association values with different roles\nin the network at different ranges, where the shape of the distribution\nsuggests a natural place to threshold for coordinated user candidates. We find\ncoordination candidates belonging to two broad categories, one involving music\nawards and promotion of Korean pop or Taylor Swift, the other being users\nengaged in political mobilization. In addition, the latent space suggests\ncommon motivations for different coordinated groups otherwise fragmented by\nusing an appropriately high threshold criterion for coordination.",
        "The Graphical User Interface (GUI) plays a critical role in the interaction\nbetween users and mobile applications (apps), aiming at facilitating the\noperation process. However, due to the variety of functions and\nnon-standardized design, GUIs might have many accessibility issues, like the\nsize of components being too small or their intervals being narrow. These\nissues would hinder the operation of low vision users, preventing them from\nobtaining information accurately and conveniently. Although several\ntechnologies and methods have been proposed to address these issues, they are\ntypically confined to issue identification, leaving the resolution in the hands\nof developers. Moreover, it can be challenging to ensure that the color, size,\nand interval of the fixed GUIs are appropriately compared to the original ones.\nIn this work, we propose a novel approach named AccessFixer, which utilizes the\nRelational-Graph Convolutional Neural Network (R-GCN) to simultaneously fix\nthree kinds of accessibility issues, including small sizes, narrow intervals,\nand low color contrast in GUIs. With AccessFixer, the fixed GUIs would have a\nconsistent color palette, uniform intervals, and adequate size changes achieved\nthrough coordinated adjustments to the attributes of related components. Our\nexperiments demonstrate the effectiveness and usefulness of AccessFixer in\nfixing GUI accessibility issues. After fixing 30 real-world apps, our approach\nsolves an average of 81.2% of their accessibility issues. Also, we apply\nAccessFixer to 10 open-source apps by submitting the fixed results with pull\nrequests (PRs) on GitHub. The results demonstrate that developers approve of\nour submitted fixed GUIs, with 8 PRs being merged or under fixing. A user study\nexamines that low vision users host a positive attitude toward the GUIs fixed\nby our method.",
        "Signal Temporal Logic (STL) robustness is a common objective for optimal\nrobot control, but its dependence on history limits the robot's decision-making\ncapabilities when used in Model Predictive Control (MPC) approaches. In this\nwork, we introduce Signal Temporal Logic robustness-to-go (Ro-To-Go), a new\nquantitative semantics for the logic that isolates the contributions of suffix\ntrajectories. We prove its relationship to formula progression for Metric\nTemporal Logic, and show that the robustness-to-go depends only on the suffix\ntrajectory and progressed formula. We implement robustness-to-go as the\nobjective in an MPC algorithm and use formula progression to efficiently\nevaluate it online. We test the algorithm in simulation and compare it to MPC\nusing traditional STL robustness. Our experiments show that using\nrobustness-to-go results in a higher success rate.",
        "Boolean Satisfiability (SAT) problems are expressed as mathematical formulas.\nThis paper presents an alternative matrix representation for any type of these\nSAT problems. It shows how to use this matrix representation to get the full\nset of valid assignments. It proves that this is the full set of answers for\nthe given problem, and it shows that this is exponential in size, relative to\nthe matrix. It then presents an algorithm that utilizes the inverses of the\nclauses in this matrix for faster searching through this set of answers. It\nshows that this algorithm is both correct and polynomial.",
        "Side-channel vulnerabilities pose an increasing threat to cryptographically\nprotected devices. Consequently, it is crucial to observe information leakages\nthrough physical parameters such as power consumption and electromagnetic (EM)\nradiation to reduce susceptibility during interactions with cryptographic\nfunctions. EM side-channel attacks are becoming more prevalent. PRESENT is a\npromising lightweight cryptographic algorithm expected to be incorporated into\nInternet-of-Things (IoT) devices in the future. This research investigates the\nEM side-channel robustness of PRESENT using a correlation attack model. This\nwork extends our previous Correlation EM Analysis (CEMA) of PRESENT with\nimproved results. The attack targets the Substitution box (S-box) and can\nretrieve 8 bytes of the 10-byte encryption key with a minimum of 256 EM\nwaveforms. This paper presents the process of EM attack modelling, encompassing\nboth simple and correlation attacks, followed by a critical analysis.",
        "We prove new local well-posedness results for nonlinear Schr\\\"odinger\nequations posed on a general product of spheres and tori, by the standard\napproach of multi-linear Strichartz estimates. To prove these estimates, we\nestablish and utilize multi-linear bounds for the joint spectral projector\nassociated to the Laplace--Beltrami operators on the individual sphere factors\nof the product manifold. To treat the particular case of the cubic NLS on a\nproduct of two spheres at critical regularity, we prove a sharp\n$L^\\infty_xL^p_t$ estimate of the solution to the linear Schr\\\"odinger equation\non the two-torus.",
        "We investigate the spectral properties of Markov semigroups associated with\nOrnstein-Uhlenbeck (OU) processes driven by L\\'evy processes. These semigroups\nare generated by non-local, non-self-adjoint operators. In the special case\nwhere the driving L\\'evy process is Brownian motion, one recovers the classical\ndiffusion OU semigroup, whose spectral properties have been extensively studied\nover past few decades. Our main results establish that, under suitable\nconditions on the L\\'evy process, the spectrum of the L\\'evy-OU semigroup in\nthe $L^p$-space weighted with the invariant distribution coincides with that of\nthe diffusion OU semigroup. Furthermore, when the drift matrix $B$ is\ndiagonalizable with real eigenvalues, we derive explicit formulas for\neigenfunctions and co-eigenfunctions--an observation that, to the best of our\nknowledge, has not appeared in the literature. We also show that the\nmultiplicities of the eigenvalues remain independent of the choice of the\nL\\'evy process. A key ingredient in our approach is intertwining relationship:\nwe prove that every L\\'evy-OU semigroup is intertwined with a diffusion OU\nsemigroup. Additionally, we examine the compactness properties of these\nsemigroups and provide examples of non-compact L\\'evy-OU semigroups.",
        "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
        "Cutwidth is a widely studied parameter that quantifies how well a graph can\nbe decomposed along small edge-cuts. It complements pathwidth, which captures\ndecomposition by small vertex separators, and it is well-known that cutwidth\nupper-bounds pathwidth. The SETH-tight parameterized complexity of problems on\ngraphs of bounded pathwidth (and treewidth) has been actively studied over the\npast decade while for cutwidth the complexity of many classical problems\nremained open.\n  For Hamiltonian Cycle, it is known that a $(2+\\sqrt{2})^{\\operatorname{pw}}\nn^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\\ JACM\n2022]. Van Geffen et al.~[J.\\ Graph Algorithms Appl.\\ 2020] and Bojikian et\nal.~[STACS 2023] asked which running time is optimal for this problem\nparameterized by cutwidth. We answer this question with\n$(1+\\sqrt{2})^{\\operatorname{ctw}} n^{O(1)}$ by providing matching upper and\nlower bounds. Second, as our main technical contribution, we close the gap left\nby van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by\nimproving both upper and lower bound and getting a tight bound of\n$\\sqrt[3]{3}^{\\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits\nthe only known tight non-integral basis apart from Hamiltonian Cycle. We show\nthat cuts inducing a disjoint union of paths of length three (unions of\nso-called $Z$-cuts) lie at the core of the complexity of the problem -- usually\nlower-bound constructions use simpler cuts inducing either a matching or a\ndisjoint union of bicliques. Finally, we determine the optimal running times\nfor Max Cut ($2^{\\operatorname{ctw}} n^{O(1)}$) and Induced Matching\n($3^{\\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the\nexisting algorithms -- the latter result also answers an open question for\ntreewidth by Chaudhary and Zehavi~[WG 2023].",
        "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.",
        "Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods.",
        "Using the algebraic approach to promise constraint satisfaction problems, we\nestablish complexity classifications of three natural variants of hypergraph\ncolourings: standard nonmonochromatic colourings, conflict-free colourings, and\nlinearly-ordered colourings.\n  Firstly, we show that finding an $\\ell$-colouring of a $k$-colourable\n$r$-uniform hypergraph is NP-hard for all constant $2\\leq k\\leq \\ell$ and\n$r\\geq 3$. This provides a shorter proof of a celebrated result by Dinur et al.\n[FOCS'02\/Combinatorica'05].\n  Secondly, we show that finding an $\\ell$-conflict-free colouring of an\n$r$-uniform hypergraph that admits a $k$-conflict-free colouring is NP-hard for\nall constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, except for $r=4$ and $k=2$ (and\nany $\\ell$); this case is solvable in polynomial time. The case of $r=3$ is the\nstandard nonmonochromatic colouring, and the case of $r=2$ is the notoriously\ndifficult open problem of approximate graph colouring.\n  Thirdly, we show that finding an $\\ell$-linearly-ordered colouring of an\n$r$-uniform hypergraph that admits a $k$-linearly-ordered colouring is NP-hard\nfor all constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, thus improving on the results\nof Nakajima and \\v{Z}ivn\\'y~[ICALP'22\/ACM TocT'23].",
        "The remarkable performance of the o1 model in complex reasoning demonstrates\nthat test-time compute scaling can further unlock the model's potential,\nenabling powerful System-2 thinking. However, there is still a lack of\ncomprehensive surveys for test-time compute scaling. We trace the concept of\ntest-time compute back to System-1 models. In System-1 models, test-time\ncompute addresses distribution shifts and improves robustness and\ngeneralization through parameter updating, input modification, representation\nediting, and output calibration. In System-2 models, it enhances the model's\nreasoning ability to solve complex problems through repeated sampling,\nself-correction, and tree search. We organize this survey according to the\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\ncompute in the transition from System-1 models to weak System-2 models, and\nthen to strong System-2 models. We also point out a few possible future\ndirections.",
        "Pilots operating modern cockpits often face high cognitive demands due to\ncomplex interfaces and multitasking requirements, which can lead to overload\nand decreased performance. This study introduces AdaptiveCoPilot, a\nneuroadaptive guidance system that adapts visual, auditory, and textual cues in\nreal time based on the pilot's cognitive workload, measured via functional\nNear-Infrared Spectroscopy (fNIRS). A formative study with expert pilots (N=3)\nidentified adaptive rules for modality switching and information load\nadjustments during preflight tasks. These insights informed the design of\nAdaptiveCoPilot, which integrates cognitive state assessments, behavioral data,\nand adaptive strategies within a context-aware Large Language Model (LLM). The\nsystem was evaluated in a virtual reality (VR) simulated cockpit with licensed\npilots (N=8), comparing its performance against baseline and random feedback\nconditions. The results indicate that the pilots using AdaptiveCoPilot\nexhibited higher rates of optimal cognitive load states on the facets of\nworking memory and perception, along with reduced task completion times. Based\non the formative study, experimental findings, qualitative interviews, we\npropose a set of strategies for future development of neuroadaptive pilot\nguidance systems and highlight the potential of neuroadaptive systems to\nenhance pilot performance and safety in aviation environments.",
        "The wide adoption of Large Language Models (LLMs) has attracted significant\nattention from $\\textit{jailbreak}$ attacks, where adversarial prompts crafted\nthrough optimization or manual design exploit LLMs to generate malicious\ncontents. However, optimization-based attacks have limited efficiency and\ntransferability, while existing manual designs are either easily detectable or\ndemand intricate interactions with LLMs. In this paper, we first point out a\nnovel perspective for jailbreak attacks: LLMs are more responsive to\n$\\textit{positive}$ prompts. Based on this, we deploy Happy Ending Attack (HEA)\nto wrap up a malicious request in a scenario template involving a positive\nprompt formed mainly via a $\\textit{happy ending}$, it thus fools LLMs into\njailbreaking either immediately or at a follow-up malicious request.This has\nmade HEA both efficient and effective, as it requires only up to two turns to\nfully jailbreak LLMs. Extensive experiments show that our HEA can successfully\njailbreak on state-of-the-art LLMs, including GPT-4o, Llama3-70b, Gemini-pro,\nand achieves 88.79\\% attack success rate on average. We also provide\nquantitative explanations for the success of HEA.",
        "This paper establishes tahe existence and uniqueness of the nonlinear Knudsen\nlayer equation with incoming boundary conditions. It is well-known that the\nsolvability conditions of the problem vary with the Mach number of the far\nMaxwellian $\\mathcal{M}^\\infty$. We consider full ranges of cutoff collision\nkernels (i.e., $- 3 < \\gamma \\leq 1$) and all the Mach numbers of the far field\nin the $L^\\infty_{x,v}$ framework. Additionally, the solution exhibits\nexponential decay $\\exp \\{- c x^\\frac{2}{3 - \\gamma} - c |v|^2 \\}$ for some $c\n> 0$. To address the general angular cutoff collision kernel, we introduce a\n$(x,v)$-mixed weight $\\sigma$. The proof is essentially bsed on adding an\nartificial damping term.",
        "We introduce a novel non-cooperative game to analyse opinion formation and\nresistance, incorporating principles from social psychology such as\nconfirmation bias, resource constraints, and influence penalties. Our\nsimulation features Large Language Model (LLM) agents competing to influence a\npopulation, with penalties imposed for generating messages that propagate or\ncounter misinformation. This framework integrates resource optimisation into\nthe agents' decision-making process. Our findings demonstrate that while higher\nconfirmation bias strengthens opinion alignment within groups, it also\nexacerbates overall polarisation. Conversely, lower confirmation bias leads to\nfragmented opinions and limited shifts in individual beliefs. Investing heavily\nin a high-resource debunking strategy can initially align the population with\nthe debunking agent, but risks rapid resource depletion and diminished\nlong-term influence.",
        "Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable.",
        "Background: The increasing use of artificial intelligence (AI) in healthcare\ndocumentation necessitates robust methods for evaluating the quality of\nAI-generated medical notes compared to those written by humans. This paper\nintroduces an open-source tool, the Human Notes Evaluator, designed to assess\nclinical note quality and differentiate between human and AI authorship.\nMethods: The Human Notes Evaluator is a Flask-based web application implemented\non Hugging Face Spaces. It employs the Physician Documentation Quality\nInstrument (PDQI-9), a validated 9-item rubric, to evaluate notes across\ndimensions such as accuracy, thoroughness, clarity, and more. The tool allows\nusers to upload clinical notes in CSV format and systematically score each note\nagainst the PDQI-9 criteria, as well as assess the perceived origin (human, AI,\nor undetermined). Results: The Human Notes Evaluator provides a user-friendly\ninterface for standardized note assessment. It outputs comprehensive results,\nincluding individual PDQI-9 scores for each criterion, origin assessments, and\noverall quality metrics. Exportable data facilitates comparative analyses\nbetween human and AI-generated notes, identification of quality trends, and\nareas for documentation improvement. The tool is available online at\nhttps:\/\/huggingface.co\/spaces\/iyadsultan\/human_evaluator . Discussion: This\nopen-source tool offers a valuable resource for researchers, healthcare\nprofessionals, and AI developers to rigorously evaluate and compare the quality\nof medical notes. By leveraging the PDQI-9 framework, it provides a structured\nand reliable approach to assess clinical documentation, contributing to the\nresponsible integration of AI in healthcare. The tool's availability on Hugging\nFace promotes accessibility and collaborative development in the field of\nAI-driven medical documentation.",
        "Sequential decision-making in high-dimensional continuous action spaces,\nparticularly in stochastic environments, faces significant computational\nchallenges. We explore this challenge in the traditional offline RL setting,\nwhere an agent must learn how to make decisions based on data collected through\na stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),\nwhich addresses this challenge by learning a set of temporally extended\nmacro-actions through a state-conditional Vector Quantized Variational\nAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs\na (separate) learned prior model that acts as a latent transition model and\nallows efficient sampling of plausible actions. During planning, our approach\naccounts for stochasticity in both the environment and the behavior policy by\nusing Monte Carlo tree search (MCTS). In offline RL settings, including\nstochastic continuous control tasks, L-MAP efficiently searches over discrete\nlatent actions to yield high expected returns. Empirical results demonstrate\nthat L-MAP maintains low decision latency despite increased action\ndimensionality. Notably, across tasks ranging from continuous control with\ninherently stochastic dynamics to high-dimensional robotic hand manipulation,\nL-MAP significantly outperforms existing model-based methods and performs\non-par with strong model-free actor-critic baselines, highlighting the\neffectiveness of the proposed approach in planning in complex and stochastic\nenvironments with high-dimensional action spaces.",
        "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https:\/\/github.com\/Ivan-Tang-3D\/ENEL",
        "We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy.",
        "3D Gaussian Splatting (3DGS) has become one of the most influential works in\nthe past year. Due to its efficient and high-quality novel view synthesis\ncapabilities, it has been widely adopted in many research fields and\napplications. Nevertheless, 3DGS still faces challenges to properly manage the\nnumber of Gaussian primitives that are used during scene reconstruction.\nFollowing the adaptive density control (ADC) mechanism of 3D Gaussian\nSplatting, new Gaussians in under-reconstructed regions are created, while\nGaussians that do not contribute to the rendering quality are pruned. We\nobserve that those criteria for densifying and pruning Gaussians can sometimes\nlead to worse rendering by introducing artifacts. We especially observe\nunder-reconstructed background or overfitted foreground regions. To encounter\nboth problems, we propose three new improvements to the adaptive density\ncontrol mechanism. Those include a correction for the scene extent calculation\nthat does not only rely on camera positions, an exponentially ascending\ngradient threshold to improve training convergence, and significance-aware\npruning strategy to avoid background artifacts. With these adaptions, we show\nthat the rendering quality improves while using the same number of Gaussians\nprimitives. Furthermore, with our improvements, the training converges\nconsiderably faster, allowing for more than twice as fast training times while\nyielding better quality than 3DGS. Finally, our contributions are easily\ncompatible with most existing derivative works of 3DGS making them relevant for\nfuture works.",
        "The rapid growth of the blockchain ecosystem and the increasing value locked\nin smart contracts necessitate robust security measures. While languages like\nSolidity and Move aim to improve smart contract security, vulnerabilities\npersist. This paper presents Smartify, a novel multi-agent framework leveraging\nLarge Language Models (LLMs) to automatically detect and repair vulnerabilities\nin Solidity and Move smart contracts. Unlike traditional methods that rely\nsolely on vast pre-training datasets, Smartify employs a team of specialized\nagents working on different specially fine-tuned LLMs to analyze code based on\nunderlying programming concepts and language-specific security principles. We\nevaluated Smartify on a dataset for Solidity and a curated dataset for Move,\ndemonstrating its effectiveness in fixing a wide range of vulnerabilities. Our\nresults show that Smartify (Gemma2+codegemma) achieves state-of-the-art\nperformance, surpassing existing LLMs and enhancing general-purpose models'\ncapabilities, such as Llama 3.1. Notably, Smartify can incorporate\nlanguage-specific knowledge, such as the nuances of Move, without requiring\nmassive language-specific pre-training datasets. This work offers a detailed\nanalysis of various LLMs' performance on smart contract repair, highlighting\nthe strengths of our multi-agent approach and providing a blueprint for\ndeveloping more secure and reliable decentralized applications in the growing\nblockchain landscape. We also provide a detailed recipe for extending this to\nother similar use cases.",
        "Malware classification in dynamic environments presents a significant\nchallenge due to concept drift, where the statistical properties of malware\ndata evolve over time, complicating detection efforts. To address this issue,\nwe propose a deep learning framework enhanced with a genetic algorithm to\nimprove malware classification accuracy and adaptability. Our approach\nincorporates mutation operations and fitness score evaluations within genetic\nalgorithms to continuously refine the deep learning model, ensuring robustness\nagainst evolving malware threats. Experimental results demonstrate that this\nhybrid method significantly enhances classification performance and\nadaptability, outperforming traditional static models. Our proposed approach\noffers a promising solution for real-time malware classification in\never-changing cybersecurity landscapes.",
        "In this paper, we propose the InfoFusion Controller, an advanced path\nplanning algorithm that integrates both global and local planning strategies to\nenhance autonomous driving in complex urban environments. The global planner\nutilizes the informed Theta-Rapidly-exploring Random Tree Star (Informed-TRRT*)\nalgorithm to generate an optimal reference path, while the local planner\ncombines Model Predictive Control (MPC) and Pure Pursuit algorithms. Mutual\nInformation (MI) is employed to fuse the outputs of the MPC and Pure Pursuit\ncontrollers, effectively balancing their strengths and compensating for their\nweaknesses. The proposed method addresses the challenges of navigating in\ndynamic environments with unpredictable obstacles by reducing uncertainty in\nlocal path planning and improving dynamic obstacle avoidance capabilities.\nExperimental results demonstrate that the InfoFusion Controller outperforms\ntraditional methods in terms of safety, stability, and efficiency across\nvarious scenarios, including complex maps generated using SLAM techniques.\n  The code for the InfoFusion Controller is available at https:\n\/\/github.com\/DrawingProcess\/InfoFusionController."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging",
    "start_abstract":"Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Magnetic resonance fingerprinting"
      ],
      "abstract":[
        "Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "Integrating UX Design in Astronomical Software Development: A Case Study",
        "Bibliometric Analysis of Scientific Production on the COVID-19 Effect in\n  Information Sciences",
        "Spatiotemporal Deep Learning Network for Photon-Level Block Compressed\n  Sensing Imaging",
        "Hybrid Channel- and Coding-Based Challenge-Response Physical-Layer\n  Authentication",
        "Thermoelectric properties of magic angle twisted bilayer\n  graphene-superconductor hetero-junction: effect of valley polarization and\n  trigonal warping",
        "Low-Eddington ratio, changing-look active galactic nuclei: the case of\n  NGC 4614",
        "Impacto del Enfoque Matematicas en Tres Actos en la Educacion Matematica",
        "Molecular Weight-Dependent Evaporation Dynamics and Morphology of PEG\n  Sessile Drops on Hydrophobic Substrates",
        "Quantized crystalline-electromagnetic responses in insulators",
        "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Evolution of SMBHs in light of PTA measurements: implications for growth\n  by mergers and accretion",
        "Optimizing Portfolios with Pakistan-Exposed ETFs: Risk and Performance\n  Insight",
        "Quantum locally recoverable code with intersecting recovery sets",
        "How many unseen species are in multiple areas?",
        "Thermal investigation of bistability in high index doped silica\n  integrated ring resonators",
        "Tensor Cross Interpolation of Purities in Quantum Many-Body Systems",
        "Disks around young free-floating planetary-mass objects: Ultradeep\n  Spitzer imaging of IC348",
        "The quantum relative entropy of the Schwarzschild black-hole and the\n  area law",
        "Global hyperbolicity and manifold topology from the Lorentzian distance",
        "Compact Accelerator-Based Production of Carrier-free $^{177}$Lu From 18\n  MeV $D^+$ on [$^{176}$Yb]Yb$_2$O$_3$",
        "Individual Confidential Computing of Polynomials over Non-Uniform\n  Information",
        "The singlet scalar state in a chiral ensemble in $SU(2)$ with two\n  fundamental flavours",
        "Towards Accurate Mixed Quantum Classical Simulations of Vibrational\n  Polaritonic Chemistry",
        "Convergence Guarantees for Unmixing PSFs over a Manifold with Non-Convex\n  Optimization",
        "The Forestry of Adversarial Totient Iterations",
        "The r-Dynamic Chromatic Number is Bounded in the Strong 2-Coloring\n  Number",
        "Ancestral lineages and sampling in populations with density-dependent\n  interactions",
        "JADES: Average Nitrogen Enhancement in High-Redshift Broad-Line Active\n  Galactic Nuclei"
      ],
      "abstract":[
        "In 2023, ASTRON took the step of incorporating a dedicated User Experience\n(UX) designer into its software development process. This decision aimed to\nenhance the accessibility and usability of services providing access to the\ndata holdings from the telescopes we are developing.\n  The field of astronomical software development has historically under\nemphasized UX design. ASTRON's initiative not only improves our own tools, but\ncan also be used to demonstrate to the broader community the value of\nintegrating UX expertise into development teams.\n  We discuss how we integrate the UX designer at the start of our software\ndevelopment lifecycle. We end with providing some considerations on how other\nprojects could make use of UX knowledge in their development process.",
        "This paper analyzes the scientific production on the COVID-19 effect in the\narea of Information Sciences from a bibliometric perspective. The objectives\nfocused on: 1) determining the most productive authors, countries, institutions\nand journals; 2) identifying the sources that constitute the core of scientific\nproduction; 3) examining the manuscripts with the greatest impact; and 4)\nvisualizing the thematic and conceptual structure of the scientific domain\nanalyzed. Bibliometric indicators and factor analysis techniques were used for\ndata analysis. A total of 1,175 publications indexed in the Web of Science\n(WoS) core collection from 2020 to 2022 were retrieved. The results showed that\nthe most relevant countries were the United States, United Kingdom, China and\nSpain. The core of the scientific production was formed by the publications:\nJournal of the American Medical Informatics Association, Information\nProfessional, Scientometrics and Journal of Health Communication. The papers\nwith the greatest impact were concentrated in those dedicated to the analysis\nof the role of telemedicine in medical care. The conceptual structure showed\nthe main research fronts, such as the role of telehealth, academic libraries\nand digital literacy in the fight against the pandemic, the role of social\nnetworks in the health crisis, as well as the problem of misinformation and\nfake news",
        "In this paper, we propose a spatiotemporal deep learning network for\nphoton-level Block Compressed Sensing Imaging, aimed to address challenges such\nas signal loss, artifacts, and noise interference in large-pixel dynamic\nimaging and tracking at the photon level. This approach combines information in\nthe time and frequency domains with a U-Net-LSTM deep learning model,\nsignificantly improving the restoration quality of dynamic target images at\nhigh frame rates. The experimental results demonstrate that dynamic target\nimaging with an average photon number of less than 10 per pixel can be achieved\nusing 16-channel parallel detection, where the pixel size is 256*256 and the\nframe rate is 200 fps.. Compared to conventional Block Compressed Sensing\nImaging, this method increases the peak signal-to-noise ratio to 38.66 dB and\nimproves the structural similarity index to 0.96. In the presence of a dynamic\nscattering medium and a static complex background, we successfully achieved\nimaging and tracking of two targets undergoing complex motion. Even in\nscenarios where the targets overlap or obstruct each other, we can still\nreconstruct clear images of each individual target separately.. This method\nprovides an effective solution for large-pixel dynamic target recognition,\ntracking, and real-time imaging in complex environments, offering promising\napplications in remote sensing, military reconnaissance, and beyond.",
        "This letter proposes a new physical layer authentication mechanism operating\nat the physical layer of a communication system where the receiver has partial\ncontrol of the channel conditions (e.g., using an intelligent reflecting\nsurface). We aim to exploit both instantaneous channel state information (CSI)\nand a secret shared key for authentication. This is achieved by both\ntransmitting an identifying key by wiretap coding (to conceal the key from the\nattacker) and checking that the instantaneous CSI corresponds to the channel\nconfiguration randomly selected by the receiver. We investigate the trade-off\nbetween the pilot signals used for CSI estimation and the coding rate (or key\nlength) to improve the overall security of the authentication procedure.",
        "We theoretically investigate the thermoelectric properties (electronic\ncontribution) of a normal-superconductor (NS) hybrid junction, where the normal\nregion consists of magic-angle twisted bilayer graphene (MATBG). The\nsuperconducting region is characterized by a common $s$-wave superconductor\nclosely proximitized to the MATBG. We compute various thermoelectric\ncoefficients, including thermal conductance, thermopower, and the figure of\nmerit ($zT$), using the scattering matrix formalism. These results are further\nsupported by calculations based on a lattice-regularized version of the\neffective Hamiltonian. Additionally, we explore the impact of trigonal warping\nand valley polarization on the thermoelectric coefficients. Notably, we find a\nsignificant variation in $zT$ as a function of these parameters, reaching\nvalues as high as 2.5. Interestingly, we observe a violation of the\nWiedemann-Franz law near the charge neutrality point with the superconducting\ncorrelation, indicating that MATBG electrons behave as slow Dirac fermions in\nthis regime. This observation is further confirmed by the damped oscillatory\nbehavior of the thermal conductance as a function of the barrier strength when\nan insulating barrier is modelled at the interface of the NS junction. Beyond\ntheoretical insights, our findings suggest new possibilities for thermoelectric\napplications using MATBG based NS junctions.",
        "Active galactic nuclei (AGN) are known to be variable sources across the\nentire electromagnetic spectrum, in particular at optical\/ultraviolet and X-ray\nenergies. Over the past decades, a growing number of AGN have displayed type\ntransitions: from type 1 to type 2 or viceversa within a few years or even\nseveral months. These galaxies have been commonly referred to as changing-look\nAGN (CLAGN). Here we report on a new CLAGN, NGC 4614, which transitioned from a\ntype 1.9 to a type 2 state. NGC 4614 is a nearly face-on barred galaxy at\nredshift $z = 0.016$, classified as a low-luminosity AGN. Its central black\nhole has a mass of about $1.6\\times 10^7 M_\\odot$ and an Eddington ratio around\n1 percent. We recently acquired optical spectra of NGC 4614 at the Telescopio\nNazionale Galileo and the data clearly suggest that the broad H$\\alpha$\ncomponent has strongly dimmed, if not disappeared. A very recent Swift\nobservation confirmed our current optical data, with the AGN weakened by almost\na factor of 10 with respect to previous X-ray observations. Indeed, NGC 4614\nhad been also observed by Swift\/XRT 6 times in 2011, when the source was\nclearly detected in all observations. By fitting the stack of the 2011 Swift\nobservations we obtain a photon index of $\\Gamma=1.3\\pm0.3$ and an equivalent\nhydrogen column density of $N_{\\rm H}$=$1.2\\pm0.3$ $\\times$10$^{22}$ cm$^{-2}$,\nindicating that NGC 4614 can be moderately absorbed in the X-rays. Although a\nsignificant change in the foreground gas absorption that may have obscured the\nbroad line region cannot be entirely ruled out, the most likely explanation for\nour optical and X-ray data is that NGC 4614 is experiencing a change in the\naccretion state that reduces the radiative efficiency of the X-ray corona.",
        "The \"Mathematics in Three Acts\" approach, proposed by Dan Meyer, aims to\ntransform the teaching of mathematics through a model that encourages active\nstudent participation, fostering creativity, problem-solving, and\nmetacognition. This study explores the implementation of this approach in a\nmathematics contest for secondary school students, evaluating its impact on\nvarious key competencies. Aspects such as mathematical creativity,\nproblem-solving skills, metacognitive abilities, and students' perceptions of\nmathematics are examined. The results show that the approach contributes to the\ndevelopment of creative skills, improves understanding and problem-solving\nabilities, and increases student motivation and confidence. However, areas for\nimprovement are also identified, particularly in the justification of\nprocedures and cognitive flexibility. This study highlights the effectiveness\nof the \"Mathematics in Three Acts\" approach as an innovative methodology that\nfosters more meaningful, reflective, and autonomous learning, suggesting its\npotential to transform mathematics teaching in diverse educational contexts.",
        "The evaporation dynamics of sessile drops are crucial for material deposition\nin applications like inkjet printing and pharmaceutical development. However,\nthe evaporation behavior of high molecular weight polymer solutions and their\nimpact on deposit morphology and flow fields are not well understood. This\nstudy investigates the evaporation dynamics and deposit morphology of\npolyethylene glycol (PEG) solution drops on hydrophobic substrates, with\nmolecular weights ranging from 200 to 1000k g\/mol, covering five orders of\nmagnitude. The results show that vapor diffusion dominates the evaporation\nprocess across all PEG molecular weights. Using image analysis and\nmicro-particle image velocimetry ($\\mu$-PIV), we reveal that molecular weight\naffects contact line dynamics and internal flow, leading to diverse deposit\nmorphologies, including spherical caps, pillars, pool-shaped disks, and flat\ndisks. Transient divergence and P\\'eclet number calculations further confirm\nthe role of hydrodynamics in deposit formation. These findings provide insights\ninto the hydrodynamic and thermodynamic factors governing evaporation in\npolymeric sessile drops, with implications for material fabrication and the\ndevelopment of inkjet printing and coating techniques.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "We study the growth of supermassive black holes accounting for both accretion\nand mergers. The former is informed by observations of the quasar luminosity\nfunction (QLF) and the latter by the gravitational wave-background (GWB)\nrecently detected by PTAs, while estimates of the present-day black hole mass\nfunction provide a boundary condition. The GWB is dominated by the most massive\nblack holes ($\\gtrsim10^{9}M_{\\odot}$). We show that their evolution can be\nsimplified into a two-step process: mergers dominate at $z\\leq1$, while\naccretion peaks at $1.4\\leq z\\leq2$. The large amplitude of the observed GWB\nsuggests a significant number of mergers. We show that this generically implies\na higher average Eddington ratio for quasars relative to a scenario in which\nmergers are negligible. In the absence of mergers, matching local estimates of\nBH abundance to the QLF implies a radiative efficiency $\\epsilon_r=0.12$ and\nEddington ratio $\\lambda=0.2$. With mergers, a progenitor of mass $M_i$ is\nboosted to a final total mass $M_f$ and there is a direct relation between the\nmass gained in mergers and the average Eddington ratio of the quasar\npopulation, given by $M_f\/M_i\\sim\\lambda\/0.2$. There is thus a tension between\nthe observed GWB, quasar properties, and the BH mass function: estimates of the\nmass function consistent with Eddington ratios inferred in quasars and\n$\\epsilon_r\\sim0.1$ underpredict the GWB; multiple\/equal mass mergers can boost\nthe GWB, but lead to a high Eddington ratio. If the local mass function is on\nthe high end of current estimates, the GWB is more readily explained, but\nrequires low efficiencies $\\epsilon_r\\sim10^{-2}$ not expected in standard\nluminous accretion models. The significant merger rate implied by the GWB also\nstrongly suggests that the most massive BHs in the local universe have\nsignificant spin due to the orbital angular momentum from mergers, perhaps\n$a\\sim0.5$.",
        "This study examines the investment landscape of Pakistan as an emerging and\nfrontier market, focusing on implications for international investors,\nparticularly those in the United States, through exchange-traded funds (ETFs)\nwith exposure to Pakistan. The analysis encompasses 30 ETFs with varying\ndegrees of exposure to Pakistan, covering the period from January 1, 2016, to\nFebruary 2024. This research highlights the potential benefits and risks\nassociated with investing in these ETFs, emphasizing the importance of thorough\nrisk assessments and portfolio performance comparisons. By providing\ndescriptive statistics and performance metrics based on historical\noptimization, this paper aims to equip investors with the necessary insights to\nmake informed decisions when optimizing their portfolios with Pakistan-exposed\nETFs. The second part of the paper introduces and assesses dynamic optimization\nmethodologies. This section is designed to explore the adaptability and\nperformance metrics of dynamic optimization techniques in comparison with\nconventional historical optimization methods. By integrating dynamic\noptimization into the investigation, this research aims to offer insights into\nthe efficacy of these contrasting methodologies in the context of\nPakistan-exposed ETFs. The findings underscore the significance of Pakistan's\nmarket dynamics within the broader context of emerging markets, offering a\npathway for diversification and potential growth in investment strategies.",
        "We introduce the concept of quantum locally recoverable codes (qLRCs) with\nintersecting recovery sets. We derive a singleton-like bound for these codes by\nleveraging the additional information provided by the intersecting recovery\nsets. Furthermore, we provide a construction for qLRCs with intersecting\nrecovery sets by introducing a variation of the hypergraph product. Finally, we\napply our qLRC methods to obtain improved results for classical LRCs. These\nresults may provide new insights into the locality of quantum error correction\ncode.",
        "In ecology, the description of species composition and biodiversity calls for\nstatistical methods that involve estimating features of interest in unobserved\nsamples based on an observed one. In the last decade, the Bayesian\nnonparametrics literature has thoroughly investigated the case where data arise\nfrom a homogeneous population. In this work, we propose a novel framework to\naddress heterogeneous populations, specifically dealing with scenarios where\ndata arise from two areas. This setting significantly increases the\nmathematical complexity of the problem and, as a consequence, it received\nlimited attention in the literature. While early approaches leverage on\ncomputational methods, we provide a distributional theory for the in-sample\nanalysis of any observed sample and we enable out-of-sample prediction for the\nnumber of unseen distinct and shared species in additional samples of arbitrary\nsizes. The latter also extends the frequentist estimators which solely deal\nwith the one-step ahead prediction. Furthermore, our results can be applied to\naddress the sample size determination in sampling problems aimed at detecting\nshared species. Our results are illustrated in a real-world dataset concerning\na population of ants in the city of Trieste.",
        "The utilization and engineering of thermo-optic effects have found broad\napplications in integrated photonic devices, facilitating efficient light\nmanipulation to achieve various functionalities. Here, we perform both an\nexperimental characterization and theoretical analysis of these effects in\nintegrated micro-ring resonators in high index doped silica (HIDS), which has\nhad many applications in integrated photonics and nonlinear optics. By fitting\nthe experimental results with theory, we obtain fundamental parameters that\ncharacterize their thermo-optic performance, including the thermo-optic\ncoefficient, the efficiency for the optically induced thermo-optic process, and\nthe thermal conductivity. The characteristics of these parameters are compared\nto those of other materials commonly used for integrated photonic platforms,\nsuch as silicon, silicon nitride, and silica. These results offer a\ncomprehensive insight into the thermo-optic properties of HIDS based devices.\nUnderstanding these properties is essential for efficiently controlling and\nengineering them in many practical applications.",
        "A defining feature of quantum many-body systems is the exponential scaling of\nthe Hilbert space with the number of degrees of freedom. This exponential\ncomplexity na\\\"ively renders a complete state characterization, for instance\nvia the complete set of bipartite Renyi entropies for all disjoint regions, a\nchallenging task. Recently, a compact way of storing subregions' purities by\nencoding them as amplitudes of a fictitious quantum wave function, known as\nentanglement feature, was proposed. Notably, the entanglement feature can be a\nsimple object even for highly entangled quantum states. However the complexity\nand practical usage of the entanglement feature for general quantum states has\nnot been explored. In this work, we demonstrate that the entanglement feature\ncan be efficiently learned using only a polynomial amount of samples in the\nnumber of degrees of freedom through the so-called tensor cross interpolation\n(TCI) algorithm, assuming it is expressible as a finite bond dimension MPS. We\nbenchmark this learning process on Haar and random MPS states, confirming\nanalytic expectations. Applying the TCI algorithm to quantum eigenstates of\nvarious one dimensional quantum systems, we identify cases where eigenstates\nhave entanglement feature learnable with TCI. We conclude with possible\napplications of the learned entanglement feature, such as quantifying the\ndistance between different entanglement patterns and finding the optimal\none-dimensional ordering of physical indices in a given state, highlighting the\npotential utility of the proposed purity interpolation method.",
        "Protoplanetary disks have been found around free-floating objects with masses\ncomparable to those of giant planets. The frequency and properties of these\ndisks around planetary-mass objects are still debated. Here we present\nultradeep mid-infrared images for the young cluster IC348, obtained through\nstacking of time series images from Spitzer. We measure fluxes at 3.6 and 4.5\nmicrons for known free-floating planetary-mass objects (FFPMOs, spectral type\nM9 or later) in this cluster. By comparing the observed infrared spectral\nenergy distributions with photospheric templates, we identify six\nplanetary-mass objects with disks, plus three which may or may not have a disk.\nThis corresponds to a disk fraction of 46% (34-59%). The disk fraction among\nplanetary-mass objects is comparable to more massive brown dwarfs. We show the\ndisk fraction among free-floating planetary-mass objects as a function of age,\ndemonstrating that these objects retain disks for several million years,\nsimilar to low-mass stars and brown dwarfs.",
        "The area law obeyed by the thermodynamic entropy of black holes is one of the\nfundamental results relating gravity to statistical mechanics. In this work we\nprovide a derivation of the area law for the quantum relative entropy of the\nSchwarzschild black-hole for arbitrary Schwarzschild radius. The quantum\nrelative entropy between the metric of the manifold and the metric induced by\nthe geometry and the matter field has been proposed in G. Bianconi \"Gravity\nfrom entropy\", Phys. Rev. D (2025) as the action for entropic quantum gravity\nleading to modified Einstein equations. The quantum relative entropy\ngeneralizes Araki entropy and treats the metrics between zero-forms, one-forms,\nand two-forms as quantum operators. Although the Schwarzschild metric is not an\nexact solution of the modified Einstein equations of the entropic quantum\ngravity, it is an approximate solution valid in the low coupling, small\ncurvature limit. Here we show that the quantum relative entropy associated to\nthe Schwarzschild metric obeys the area law for large Schwarzschild radius. We\nprovide a full statistical mechanics interpretation of the results.",
        "In this work, we seek characterizations of global hyperbolicity in smooth\nLorentzian manifolds that do not rely on the manifold topology and that are\ninspired by metric geometry. In particular, strong causality is not assumed, so\npart of the problem is precisely that of recovering the manifold topology so as\nto make sense of it also in rough frameworks. After verifying that known\nstandard characterizations do not meet this requirement, we propose two\npossible formulations. The first is based solely on chronological diamonds and\nis interesting due to its analogies with the Hopf-Rinow theorem. The second\nuses only properties of the Lorentzian distance function and it is suitable for\nextension to abstract `Lorentzian metric' frameworks. It turns out to be\nequivalent to the definition of `Lorentzian metric space' proposed in our\nprevious joint work with S. Suhr, up to slightly strengthening weak\n$d$-distinction to `future or past $d$-distinction'. The role of a new property\nwhich we term `$d$-reflectivity' is also discussed. We then investigate\ncontinuity properties of the Lorentzian distance and the property of\n$d$-reflectivity in non-smooth frameworks. Finally, we establish a result of\nbroader interest: the exponential map of a smooth spray is $C^{1,1}$ (smooth\noutside the zero section). Additionally, we derive a Lorentz-Finsler version of\nthe Busemann-Mayer formula and demonstrate that, in strongly causal smooth\nFinsler spacetimes, the Finsler fundamental function can be reconstructed from\nthe distance. As a consequence, distance-preserving bijections are shown to be\nLorentz-Finsler isometries in the conventional smooth sense.",
        "We use experimental and simulated excitation functions to estimate the yield\nof deuteron activations on a [$^{176}$Yb]Yb$_2$O$_3$ target enriched to 99%.\nSubsequent calculations are used to determine the production of\nradiotherapeutic $^{177}$Lu according to a 10 mA, 18 MeV $D^+$ compact linear\naccelerator. The design comprises a single radio-frequency quadrupole\naccelerator (RFQ) and seven drift tube linacs (DTLs) that achieve a beam\nefficiency of 99.5% over a length of $12\\,\\text{m}$. Our results show that a\n5-day irradiation can yield more than $1$ mg of $^{177}$Lu, exceeding $4.4$\nTBq. After a 2 day processing period, it is estimated that the sample will have\na radiopurity greater than 99.8% (carrier-free). Given recent EMA and FDA\napprovals of $^{177}$Lu-DOTATATE and $^{177}$Lu-PSMA-617, our results confirm\nthe viability of accelerator-based $^{177}$Lu production and provide a\npromising clinical alternative to reactor-based methods.",
        "In this paper, we address the problem of secure distributed computation in\nscenarios where user data is not uniformly distributed, extending existing\nframeworks that assume uniformity, an assumption that is challenging to enforce\nin data for computation. Motivated by the pervasive reliance on single service\nproviders for data storage and computation, we propose a privacy-preserving\nscheme that achieves information-theoretic security guarantees for computing\npolynomials over non-uniform data distributions. Our framework builds upon the\nconcept of perfect subset privacy and employs linear hashing techniques to\ntransform non-uniform data into approximately uniform distributions, enabling\nrobust and secure computation. We derive leakage bounds and demonstrate that\ninformation leakage of any subset of user data to untrusted service providers,\ni.e., not only to colluding workers but also (and more importantly) to the\nadmin, remains negligible under the proposed scheme.",
        "Composite Higgs models are a class of Beyond the Standard Model (BSM) models\nproposed to address the hierarchy and naturalness problems associated with the\nStandard Model (SM) Higgs. A new QCD-like strongly interacting sector based on\n$SU(2)$ with two fundamental flavours can be used to build a composite Higgs\nmodel which is not yet ruled out by experiment. The role of the singlet scalar\nresonance will affect Higgs phenomenology at the LHC. In this project our goal\nis to understand the properties of the singlet scalar state in the new strongly\ninteracting sector in isolation as a first step to understanding the role of\nthis state in composite Higgs models. We present here the first lattice results\nfor the mass of the $\\sigma$ in $SU(2)$ with two fundamental flavours using\nexponential clover Wilson fermions.",
        "Interest in vibrational polaritonic chemistry, where ground-state chemical\nkinetics are modified via confined optical modes in a cavity, has surged in\nrecent years. Although models have been developed to understand cavity-modified\nreactions, fully quantum mechanical simulations remain out of reach for the\ncollective regime that involves many molecules, a critical aspect of the\nphenomenon. Mixed quantum-classical (MQC) simulations offer a scalable\nalternative, but their accuracy requires testing and potential improvements\neven in the single-molecule limit. In this work, we take this step by first\nintroducing the mapping approach to surface hopping (MASH) to address the\nlimitations of traditional MQC methods. Second, we incorporate a quantum\ntreatment of the cavity mode, moving beyond the classical approximations often\nemployed in previous studies. Results for a single-molecule model of\nvibrational polaritonic chemistry show that combining MASH with a quantum\ncavity mode yields the most accurate rates. However, this scheme may produce\ndifferent long-time population dynamics at zero coupling depending on whether\nthe cavity mode is quantized; a problem known as size-inconsistency in MASH. We\naddress this problem proposing the $\\epsilon$-MASH approach, which forbids\nhopping between states with negligible nonadiabatic couplings (NACs). Combining\nMASH with a quantum cavity mode thus provides a promising approach for scalable\nand accurate MQC simulations in the collective regime.",
        "The problem of recovering the parameters of a mixture of spike signals\nconvolved with different PSFs is considered. Herein, the spike support is\nassumed to be known, while the PSFs lie on a manifold. A non-linear least\nsquares estimator of the mixture parameters is formulated. In the absence of\nnoise, a lower bound on the radius of the strong basin of attraction i.e., the\nregion of convergence, is derived. Key to the analysis is the introduction of\ncoherence and interference functions, which capture the conditioning of the PSF\nmanifold in terms of the minimal separation of the support. Numerical\nexperiments validate the theoretical findings. Finally, the practicality and\nefficacy of the non-linear least squares approach are showcased on spectral\ndata from laser-induced breakdown spectroscopy.",
        "We give a closed-form expression for\n$\\varphi(1+\\varphi(2+\\varphi(3+...+\\varphi(n)$, where $\\varphi$ is Euler's\ntotient function. More generally, for an integer sequence $A=\\{a_j\\}$ we study\nthe value of\n$A^\\varphi(n)=\\varphi(a_1+\\varphi(a_2+\\varphi(a_3+...+\\varphi(a_n)$ when $A$ is\nthe perfect squares or the perfect cubes. We show $A^\\varphi(n)$ is bounded for\nall sequences considered. We also present the Arboreal Algorithm which can\nsometimes determine a closed form of $A^\\varphi(n)$ using tree-like structures.",
        "A proper vertex-coloring of a graph is $r$-dynamic if the neighbors of each\nvertex $v$ receive at least $\\min(r, \\mathrm{deg}(v))$ different colors. In\nthis note, we prove that if $G$ has a strong $2$-coloring number at most $k$,\nthen $G$ admits an $r$-dynamic coloring with no more than $(k-1)r+1$ colors. As\na consequence, for every class of graphs of bounded expansion, the $r$-dynamic\nchromatic number is bounded by a linear function in $r$. We give a concrete\nupper bound for graphs of bounded row-treewidth, which includes for example all\nplanar graphs.",
        "We study a density-dependent Markov jump process describing a population\nwhere each individual is characterized by a type, and reproduces at rates\ndepending both on its type and on the population type distribution. First,\nusing an appropriate change in probability, we exhibit a time-inhomogeneous\nMarkov process, the auxiliary process, which allows to capture the behavior of\na sampled lineage in the population process. This is achieved through a\nmany-to-one formula, which relates the average of a function over ancestral\nlineages sampled in the population processes to its average over the auxiliary\nprocess, yielding a direct interpretation of the underlying survivorship bias.\nIn addition, this construction allows for more general sampling procedures than\nwhat was previously obtained in the literature, such as sampling restricted to\nsubpopulations. Second, we consider the large population regime, when the\npopulation size grows to infinity. Under classical assumptions, the population\ntype distribution can then be approached by a diffusion approximation, which\ncaptures the fluctuations of the population process around its deterministic\nlarge population limit. We establish a many-to-one formula allowing to sample\nin the diffusion approximation, and quantify the associated approximation\nerror.",
        "The unexpectedly high nitrogen-to-oxygen (N\/O) ratios observed in\nhigh-redshift (z) galaxies have challenged our understanding of early star\nformation. Notably, many of these nitrogen-rich galaxies show signatures of\nactive galactic nuclei (AGNs), suggesting a possible connection between black\nhole formation and nitrogen enrichment. To explore this connection, we analyse\nstacked spectra of z=4-7 broad-line and narrow-line AGNs using deep NIRSpec\ndata from the JADES survey. We identify a significant Niii] quintuplet and a\nhigh electron density ($\\sim10^{4}\\,\\mathrm{cm^{-3}}$) only in the broad-line\nAGN stack, indicating nitrogen-rich ($\\log(\\mathrm{N\/C})\\simeq0.5$,\n$\\log(\\mathrm{N\/O})>-0.6$) and dense gas similar to the high-z nitrogen-rich\ngalaxies. Our findings suggest that dense nuclear star formation may trap\nnitrogen-rich gas in proto-globular clusters, in line with the high N\/O\nobserved in local globular clusters; associated runaway stellar collisions\ncould produce intermediate-mass black hole seeds, as predicted by some models\nand simulations, whose accretion results into AGN signatures. These findings\nsupport scenarios connecting the early black hole seeding and growth to merging\nprocesses within and between proto-globular clusters in primeval galaxies."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Magnetic resonance fingerprinting",
    "start_abstract":"Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging"
      ],
      "abstract":[
        "Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Eigenvalue conditions implying edge-disjoint spanning trees and a forest\n  with constraints",
        "The Journey Matters: Average Parameter Count over Pre-training Unifies\n  Sparse and Dense Scaling Laws",
        "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "Personalized Interpolation: An Efficient Method to Tame Flexible\n  Optimization Window Estimation",
        "Logarithmic Width Suffices for Robust Memorization",
        "On the stress transit function",
        "Time Series Language Model for Descriptive Caption Generation",
        "Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent\n  Collaborative Field Coverage",
        "TULIP: Towards Unified Language-Image Pretraining",
        "Towards Generalizable Trajectory Prediction Using Dual-Level\n  Representation Learning And Adaptive Prompting",
        "A note on improved bounds for hypergraph rainbow matching problems",
        "The Ball-Proximal (=\"Broximal\") Point Method: a New Algorithm,\n  Convergence Theory, and Applications",
        "Further Results for the Capacity Statistic Distribution on Compositions\n  of 1's and 2's",
        "3D Point Cloud Generation via Autoregressive Up-sampling",
        "Rerailing Automata",
        "Global Group Fairness in Federated Learning via Function Tracking",
        "Revisiting gender bias research in bibliometrics: Standardizing\n  methodological variability using Scholarly Data Analysis (SoDA) Cards",
        "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
        "Uncovering Utility Functions from Observed Outcomes",
        "Media and responsible AI governance: a game-theoretic and LLM analysis",
        "Anderson localization for the multi-frequency quasi-periodic CMV\n  matrices and quantum walks",
        "Supervaluations, truth, and intuitionistic logic",
        "Cookie cutters: Bisections with fixed shapes",
        "A Stochastic Linear-Quadratic Leader-Follower Differential Game with\n  Elephant Memory",
        "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion",
        "Strong Low Degree Hardness for Stable Local Optima in Spin Glasses",
        "DuCos: Duality Constrained Depth Super-Resolution via Foundation Model",
        "Improving Similar Case Retrieval Ranking Performance By Revisiting\n  RankSVM",
        "From Analog to Digital -- Successful Implementation of IoT Solutions in\n  the Petrochemical Industry"
      ],
      "abstract":[
        "Let $G$ be a nontrivial graph with minimum degree $\\delta$ and $k$ an integer\nwith $k\\ge 2$. In the literature, there are eigenvalue conditions that imply\n$G$ contains $k$ edge-disjoint spanning trees. We give eigenvalue conditions\nthat imply $G$ contains $k$ edge-disjoint spanning trees and another forest $F$\nwith $|E(F)|>\\frac{\\delta-1}{\\delta}(|V(G)|-1)$, and if $F$ is not a spanning\ntree, then $F$ has a component with at least $\\delta$ edges.",
        "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference.",
        "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
        "In the realm of online advertising, optimizing conversions is crucial for\ndelivering relevant products to users and enhancing business outcomes.\nPredicting conversion events is challenging due to variable delays between user\ninteractions, such as impressions or clicks, and the actual conversions. These\ndelays differ significantly across various advertisers and products,\nnecessitating distinct optimization time windows for targeted conversions. To\naddress this, we introduce a novel approach named the \\textit{Personalized\nInterpolation} method, which innovatively builds upon existing fixed conversion\nwindow models to estimate flexible conversion windows. This method allows for\nthe accurate estimation of conversions across a variety of delay ranges, thus\nmeeting the diverse needs of advertisers without increasing system complexity.\nTo validate the efficacy of our proposed method, we conducted comprehensive\nexperiments using ads conversion model. Our experiments demonstrate that this\nmethod not only achieves high prediction accuracy but also does so more\nefficiently than other existing solutions. This validation underscores the\npotential of our Personalized Interpolation method to significantly enhance\nconversion optimization in real-world online advertising systems, promising\nimproved targeting and effectiveness in advertising strategies.",
        "The memorization capacity of neural networks with a given architecture has\nbeen thoroughly studied in many works. Specifically, it is well-known that\nmemorizing $N$ samples can be done using a network of constant width,\nindependent of $N$. However, the required constructions are often quite\ndelicate. In this paper, we consider the natural question of how well\nfeedforward ReLU neural networks can memorize robustly, namely while being able\nto withstand adversarial perturbations of a given radius. We establish both\nupper and lower bounds on the possible radius for general $l_p$ norms, implying\n(among other things) that width logarithmic in the number of input samples is\nnecessary and sufficient to achieve robust memorization (with robustness radius\nindependent of $N$).",
        "The stress interval $S(u,v)$ between $u,v\\in V(G)$ is the set of all vertices\nin a graph $G$ that lie on every shortest $u,v$-path. A set $U \\subseteq V(G)$\nis stress convex if $S(u,v) \\subseteq U$ for any $u,v\\in U$. A vertex $v \\in\nV(G)$ is s-extreme if $V(G)-v$ is a stress convex set in $G$. The stress number\n$sn(G)$ of $G$ is the minimum cardinality of a set $U$ where $\\bigcup_{u,v \\in\nU}S(u,v)=V(G)$. The stress hull number $sh(G)$ of $G$ is the minimum\ncardinality of a set whose stress convex hull is $V(G)$. In this paper, we\npresent many basic properties of stress intervals. We characterize s-extreme\nvertices of a graph $G$ and construct graphs $G$ with arbitrarily large\ndifference between the number of s-extreme vertices, $sh(G)$ and $sn(G)$. Then\nwe study these three invariants for some special graph families, such as graph\nproducts, split graphs, and block graphs. We show that in any split graph $G$,\n$sh(G)=sn(G)=|Ext_s(G)|$, where $Ext_s(G)$ is the set of s-extreme vertices of\n$G$. Finally, we show that for $k \\in \\mathbb{N}$, deciding whether $sn(G) \\leq\nk$ is NP-complete problem, even when restricted to bipartite graphs.",
        "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
        "Multi-agent reinforcement learning is a challenging and active field of\nresearch due to the inherent nonstationary property and coupling between\nagents. A popular approach to modeling the multi-agent interactions underlying\nthe multi-agent RL problem is the Markov Game. There is a special type of\nMarkov Game, termed Markov Potential Game, which allows us to reduce the Markov\nGame to a single-objective optimal control problem where the objective function\nis a potential function. In this work, we prove that a multi-agent\ncollaborative field coverage problem, which is found in many engineering\napplications, can be formulated as a Markov Potential Game, and we can learn a\nparameterized closed-loop Nash Equilibrium by solving an equivalent\nsingle-objective optimal control problem. As a result, our algorithm is 10x\nfaster during training compared to a game-theoretic baseline and converges\nfaster during policy execution.",
        "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image\/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code\/checkpoints are available at\nhttps:\/\/tulip-berkeley.github.io",
        "Existing vehicle trajectory prediction models struggle with generalizability,\nprediction uncertainties, and handling complex interactions. It is often due to\nlimitations like complex architectures customized for a specific dataset and\ninefficient multimodal handling. We propose Perceiver with Register queries\n(PerReg+), a novel trajectory prediction framework that introduces: (1)\nDual-Level Representation Learning via Self-Distillation (SD) and Masked\nReconstruction (MR), capturing global context and fine-grained details.\nAdditionally, our approach of reconstructing segmentlevel trajectories and lane\nsegments from masked inputs with query drop, enables effective use of\ncontextual information and improves generalization; (2) Enhanced Multimodality\nusing register-based queries and pretraining, eliminating the need for\nclustering and suppression; and (3) Adaptive Prompt Tuning during fine-tuning,\nfreezing the main architecture and optimizing a small number of prompts for\nefficient adaptation. PerReg+ sets a new state-of-the-art performance on\nnuScenes [1], Argoverse 2 [2], and Waymo Open Motion Dataset (WOMD) [3].\nRemarkable, our pretrained model reduces the error by 6.8% on smaller datasets,\nand multi-dataset training enhances generalization. In cross-domain tests,\nPerReg+ reduces B-FDE by 11.8% compared to its non-pretrained variant.",
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "Non-smooth and non-convex global optimization poses significant challenges\nacross various applications, where standard gradient-based methods often\nstruggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or\nBall Point Method (BPM) for short - a novel algorithmic framework inspired by\nthe classical Proximal Point Method (PPM) (Rockafellar, 1976), which, as we\nshow, sheds new light on several foundational optimization paradigms and\nphenomena, including non-convex and non-smooth optimization, acceleration,\nsmoothing, adaptive stepsize selection, and trust-region methods. At the core\nof BPM lies the ball-proximal (\"broximal\") operator, which arises from the\nclassical proximal operator by replacing the quadratic distance penalty by a\nball constraint. Surprisingly, and in sharp contrast with the sublinear rate of\nPPM in the nonsmooth convex regime, we prove that BPM converges linearly and in\na finite number of steps in the same regime. Furthermore, by introducing the\nconcept of ball-convexity, we prove that BPM retains the same global\nconvergence guarantees under weaker assumptions, making it a powerful tool for\na broader class of potentially non-convex optimization problems. Just like PPM\nplays the role of a conceptual method inspiring the development of practically\nefficient algorithms and algorithmic elements, e.g., gradient descent, adaptive\nstep sizes, acceleration (Ahn & Sra, 2020), and \"W\" in AdamW (Zhuang et al.,\n2022), we believe that BPM should be understood in the same manner: as a\nblueprint and inspiration for further development.",
        "In this paper, we study additional aspects of the capacity distribution on\nthe set $\\mathcal{B}_n$ of compositions of $n$ consisting of $1$'s and $2$'s.\nAmong our results are further recurrences for this distribution as well as\nformulas for the total capacity and sign balance on $\\mathcal{B}_n$. We provide\nalgebraic and combinatorial proofs of our results. We also give combinatorial\nexplanations of some prior results where such a proof was requested. Finally,\nthe joint distribution of the capacity statistic with two further parameters on\n$\\mathcal{B}_n$ is briefly considered.",
        "We introduce a pioneering autoregressive generative model for 3D point cloud\ngeneration. Inspired by visual autoregressive modeling (VAR), we conceptualize\npoint cloud generation as an autoregressive up-sampling process. This leads to\nour novel model, PointARU, which progressively refines 3D point clouds from\ncoarse to fine scales. PointARU follows a two-stage training paradigm: first,\nit learns multi-scale discrete representations of point clouds, and then it\ntrains an autoregressive transformer for next-scale prediction. To address the\ninherent unordered and irregular structure of point clouds, we incorporate\nspecialized point-based up-sampling network modules in both stages and\nintegrate 3D absolute positional encoding based on the decoded point cloud at\neach scale during the second stage. Our model surpasses state-of-the-art (SoTA)\ndiffusion-based approaches in both generation quality and parameter efficiency\nacross diverse experimental settings, marking a new milestone for\nautoregressive methods in 3D point cloud generation. Furthermore, PointARU\ndemonstrates exceptional performance in completing partial 3D shapes and\nup-sampling sparse point clouds, outperforming existing generative models in\nthese tasks.",
        "In this paper, we introduce rerailing automata for $\\omega$-regular\nlanguages. They generalize both deterministic parity (DPW) and minimized\nhistory-deterministic co-B\\\"uchi automata (with transition based acceptance,\nHdTbcBW) while combining their favorable properties. In particular, rerailing\nautomata can represent arbitrary $\\omega$-regular languages while allowing for\npolynomial-time minimization, just as HdTbcBW do. Since DPW are a special case\nof rerailing automata, a minimized rerailing automaton is never larger than the\nsmallest deterministic parity automaton for the same language. We also show\nthat rerailing automata can be used as a replacement for deterministic parity\nautomata for the realizability check of open systems.\n  The price to be paid to obtain the useful properties of rerailing automata is\nthat the acceptance condition in such automata refers to the dominating colors\nalong all runs for a given word, where just as in parity automata, the\ndominating color along a run is the lowest one occurring infinitely often along\nit. A rerailing automaton accepts those words for which the greatest of the\ndominating colors along the runs is even. Additionally, rerailing automata\nguarantee that every prefix of a run for a word can be extended to eventually\nreach a point from which all runs for the word extending the prefix have the\nsame dominating color, and it is even if and only if the word is in the\nlanguage of the automaton. We show that these properties together allow\ncharacterizing the role of each state in such an automaton in a way that\nrelates it to state combinations in a sequence of co-B\\\"uchi automata for the\nrepresented language. This characterization forms the basis of the\npolynomial-time minimization approach in this paper.",
        "We investigate group fairness regularizers in federated learning, aiming to\ntrain a globally fair model in a distributed setting. Ensuring global fairness\nin distributed training presents unique challenges, as fairness regularizers\ntypically involve probability metrics between distributions across all clients\nand are not naturally separable by client. To address this, we introduce a\nfunction-tracking scheme for the global fairness regularizer based on a Maximum\nMean Discrepancy (MMD), which incurs a small communication overhead. This\nscheme seamlessly integrates into most federated learning algorithms while\npreserving rigorous convergence guarantees, as demonstrated in the context of\nFedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD\nregularization enables straightforward analysis through a change of kernel,\nleveraging an intuitive interpretation of kernel convolution. Numerical\nexperiments confirm our theoretical insights.",
        "Gender biases in scholarly metrics remain a persistent concern, despite\nnumerous bibliometric studies exploring their presence and absence across\nproductivity, impact, acknowledgment, and self-citations. However,\nmethodological inconsistencies, particularly in author name disambiguation and\ngender identification, limit the reliability and comparability of these\nstudies, potentially perpetuating misperceptions and hindering effective\ninterventions. A review of 70 relevant publications over the past 12 years\nreveals a wide range of approaches, from name-based and manual searches to more\nalgorithmic and gold-standard methods, with no clear consensus on best\npractices. This variability, compounded by challenges such as accurately\ndisambiguating Asian names and managing unassigned gender labels, underscores\nthe urgent need for standardized and robust methodologies. To address this\ncritical gap, we propose the development and implementation of ``Scholarly Data\nAnalysis (SoDA) Cards.\" These cards will provide a structured framework for\ndocumenting and reporting key methodological choices in scholarly data\nanalysis, including author name disambiguation and gender identification\nprocedures. By promoting transparency and reproducibility, SoDA Cards will\nfacilitate more accurate comparisons and aggregations of research findings,\nultimately supporting evidence-informed policymaking and enabling the\nlongitudinal tracking of analytical approaches in the study of gender and other\nsocial biases in academia.",
        "User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. We evaluate UPCORE across three\nstandard unlearning methods consistently achieving a superior balance between\nthe competing objectives of deletion efficacy and model preservation. To better\nevaluate this trade-off, we introduce a new metric, measuring the\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\nimproves both standard metrics and AUC, benefitting from positive transfer\nbetween the coreset and pruned points while reducing negative transfer from the\nforget set to points outside of it.",
        "Determining consumer preferences and utility is a foundational challenge in\neconomics. They are central in determining consumer behaviour through the\nutility-maximising consumer decision-making process. However, preferences and\nutilities are not observable and may not even be known to the individual making\nthe choice; only the outcome is observed in the form of demand. Without the\nability to observe the decision-making mechanism, demand estimation becomes a\nchallenging task and current methods fall short due to lack of scalability or\nability to identify causal effects. Estimating these effects is critical when\nconsidering changes in policy, such as pricing, the impact of taxes and\nsubsidies, and the effect of a tariff. To address the shortcomings of existing\nmethods, we combine revealed preference theory and inverse reinforcement\nlearning to present a novel algorithm, Preference Extraction and Reward\nLearning (PEARL) which, to the best of our knowledge, is the only algorithm\nthat can uncover a representation of the utility function that best\nrationalises observed consumer choice data given a specified functional form.\nWe introduce a flexible utility function, the Input-Concave Neural Network\nwhich captures complex relationships across goods, including cross-price\nelasticities. Results show PEARL outperforms the benchmark on both noise-free\nand noisy synthetic data.",
        "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
        "In this paper, we establish Anderson localization for the CMV matrices with\nmulti-frequency analytic quasi-periodic Verblunsky coefficients in the regime\nof the positive Lyapunov exponent. As an application, we further derive the\nAnderson localization for the multi-frequency analytic quasi-periodic quantum\nwalks. We extend the results of Wang and Damanik (J. Funct. Anal. 276 (2019))\nfor one-frequency quasi-periodic CMV matrices to multi-frequency case.",
        "The supervaluationist approach to fixed-point semantics is, arguably, the\nmost celebrated and studied competitor to the Strong Kleene approach within\nKripkean truth. In this paper, we show how to obtain a supervaluationist\nfixed-point theory of truth for intuitionistic logic. In particular, we show\nhow to do supervaluations over Kripke structures for intuitionistic logic, and\nwe obtain the corresponding theories of truth, both semantic and axiomatic.\nFurthermore, we show how the theory of truth changes when the Kripke structures\nover which the supervaluations are defined change. Finally, we advance a\nsupervaluationist theory of truth that, unlike the classical case or any of the\nother intuitionistic, supervaluationist theories, is compositional in nature.",
        "In a mass partition problem, we are interested in finding equitable\npartitions of smooth measures in $\\mathbb{R}^d$. In this manuscript, we study\nthe problem of finding simultaneous bisections of measures using scaled copies\nof a prescribed set $K$. We distinguish the problem when we are allowed to use\nscaled and translated copies of $K$ and the problem when we are allowed to use\nscaled isometric copies of $K$. These problems have only previously been\nstudied if $K$ is a half-space or a Euclidean ball. We obtain positive results\nfor simultaneous bisection of any $d+1$ masses for star-shaped compact sets $K$\nwith non-empty interior, where the conditions on the problem depend on the\nsmoothness of the boundary of $K$. Additional proofs are included for\nparticular instances of $K$, such as hypercubes and cylinders, answering\npositively a conjecture of Sober\\'on and Takahashi. The proof methods are\ntopological and involve new Borsuk--Ulam-type theorems.",
        "This paper is concerned with a stochastic linear-quadratic leader-follower\ndifferential game with elephant memory. The model is general in that the state\nequation for both the leader and the follower includes the elephant memory of\nthe state and the control, which are part of the diffusion term. Under certain\nassumptions, the state feedback representation of the open-loop Stackelberg\nstrategy is derived by introducing two Riccati equations and a special\nmatrix-valued equation. Finally, theoretical results are illustrated by means\nof an example concerning a dynamic advertising problem with elephant memory.",
        "We introduce InfiFusion, an efficient training pipeline designed to integrate\nmultiple domain-specialized Large Language Models (LLMs) into a single pivot\nmodel, effectively harnessing the strengths of each source model. Traditional\nfusion methods either merge model parameters directly or rely on knowledge\ndistillation with rigid assumptions, limiting their flexibility and efficiency.\nInfiFusion overcomes these limitations by enhancing Universal Logit\nDistillation (ULD) with Top-K selection and Logits Standardization. We propose\ntwo fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source\nmodel knowledge is distilled individually into the pivot model followed by\nmerging and Unified Fusion (InfiFusion$_u$), where knowledge from all source\nmodels is distilled simultaneously into the pivot model. InfiFusion outperforms\nthe state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11\nwidely applied benchmarks covering reasoning, coding, mathematics, and\ninstruction-following tasks. Notably, InfiFusion achieves this superior\nperformance while significantly reduces computational costs, completing full\ntraining with only 160 H800 GPU hours compared to the millions typically\nrequired for traditional LLM training.",
        "It is a folklore belief in the theory of spin glasses and disordered systems\nthat out-of-equilibrium dynamics fail to find stable local optima exhibiting\ne.g. local strict convexity on physical time-scales. In the context of the\nSherrington--Kirkpatrick spin glass, Behrens-Arpino-Kivva-Zdeborov\\'a and\nMinzer-Sah-Sawhney have recently conjectured that this obstruction may be\ninherent to all efficient algorithms, despite the existence of exponentially\nmany such optima throughout the landscape. We prove this search problem\nexhibits strong low degree hardness for polynomial algorithms of degree $D\\leq\no(N)$: any such algorithm has probability $o(1)$ to output a stable local\noptimum. To the best of our knowledge, this is the first result to prove that\neven constant-degree polynomials have probability $o(1)$ to solve a random\nsearch problem without planted structure. To prove this, we develop a\ngeneral-purpose enhancement of the ensemble overlap gap property, and as a\nbyproduct improve previous results on spin glass optimization, maximum\nindependent set, random $k$-SAT, and the Ising perceptron to strong low degree\nhardness. Finally for spherical spin glasses with no external field, we prove\nthat Langevin dynamics does not find stable local optima within dimension-free\ntime.",
        "We introduce DuCos, a novel depth super-resolution framework grounded in\nLagrangian duality theory, offering a flexible integration of multiple\nconstraints and reconstruction objectives to enhance accuracy and robustness.\nOur DuCos is the first to significantly improve generalization across diverse\nscenarios with foundation models as prompts. The prompt design consists of two\nkey components: Correlative Fusion (CF) and Gradient Regulation (GR). CF\nfacilitates precise geometric alignment and effective fusion between prompt and\ndepth features, while GR refines depth predictions by enforcing consistency\nwith sharp-edged depth maps derived from foundation models. Crucially, these\nprompts are seamlessly embedded into the Lagrangian constraint term, forming a\nsynergistic and principled framework. Extensive experiments demonstrate that\nDuCos outperforms existing state-of-the-art methods, achieving superior\naccuracy, robustness, and generalization. The source codes and pre-trained\nmodels will be publicly available.",
        "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https:\/\/github.com\/liuyuqi123study\/RankSVM_for_SLR",
        "This document describes the development and implementation of a technological\nsolution based on IoT devices to modernize a machine known as the Cyclone. This\nequipment is used by a contractor collaborating with petrochemical companies in\nthe state of Texas, performing specialized work in mechanics, engineering,\ncatalytic material replacement, and rescue operations in refinery complexes.\nThe Cyclone machine, with outdated relay logic technology, poses challenges in\nterms of operational efficiency, critical condition monitoring, and safety. The\nproject was carried out with the collaboration of specialists in equipment\nhandling, focusing on demonstrating the feasibility of integrating advanced\nIndustry 4.0 technologies into legacy industrial equipment. The methodology\nincluded the incorporation of IoT sensors for real-time monitoring, an\nautomated control system, and the digitization of key processes. Preliminary\nresults indicate improvements in the precision of operational control and the\nability for remote supervision, highlighting the potential for modernization in\ncritical industrial applications. This work not only validates the use of IoT\ndevices in obsolete equipment but also sets a precedent for the transition\ntowards more sustainable and efficient technologies in the petrochemical\nsector."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Additive manufacturing and sustainability: an exploratory study of the advantages and challenges",
    "start_abstract":"The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Machine learning in additive manufacturing: State-of-the-art and perspectives"
      ],
      "abstract":[
        "Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "RF Desense significance and its impact on the EVM at Signal Near the\n  Noise Floor",
        "Concentration phenomena for a mixed local\/nonlocal Schr\\\"{o}dinger\n  equation with Dirichlet datum",
        "Toward a Flexible Framework for Linear Representation Hypothesis Using\n  Maximum Likelihood Estimation",
        "Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based\n  Perspective",
        "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
        "Real Time Control of Tandem-Wing Experimental Platform Using Concerto\n  Reinforcement Learning",
        "Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and\n  Deployment",
        "Transformer-based Wireless Symbol Detection Over Fading Channels",
        "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
        "Increasing Batch Size Improves Convergence of Stochastic Gradient\n  Descent with Momentum",
        "Disharmony: Forensics using Reverse Lighting Harmonization",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "Robust Data Watermarking in Language Models by Injecting Fictitious\n  Knowledge",
        "Large language models streamline automated systematic review: A\n  preliminary study",
        "Semi-Quantum Conference Key Agreement with GHZ-type states",
        "Relative knot probabilities in confined lattice polygons",
        "Improving Similar Case Retrieval Ranking Performance By Revisiting\n  RankSVM",
        "The slicing conjecture via small ball estimates",
        "Supercritical phase transition on the Toeplitz algebra of $\\mathbb\n  N^\\times \\ltimes \\mathbb Z$",
        "Homotopy types of complexes of hyperplanes in quasi-median graphs and\n  applications to right-angled Artin groups",
        "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
        "Improved Sublinear Algorithms for Classical and Quantum Graph Coloring",
        "InDeed: Interpretable image deep decomposition with guaranteed\n  generalizability",
        "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
        "Forecasting Drought Using Machine Learning in California",
        "User Agency and System Automation in Interactive Intelligent Systems",
        "Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber\n  Attack Taxonomies",
        "Towards Data-Driven Multi-Stage OPF"
      ],
      "abstract":[
        "Hardware impairments and system non-linearities impacting communication\nsignal is one of key aspect for having harmonics and RF desense which overall\ncausing the lower quality and integrity of the modulated signal, resulting in\nI\/Q imbalance, further bit error and spectral efficiency degradation. This\npresentation outlines the RF Desense results, EVM Measurement and it impact at\nthe almost noise floor with step size by 1 dB in QPSK at LTE Bands, Note for\nmmW 3GPP 38.521-2 clause 6.4.2.1 indicates single polarization.",
        "We consider the mixed local\/nonlocal semilinear equation\n  \\begin{equation*}\n  -\\epsilon^{2}\\Delta u +\\epsilon^{2s}(-\\Delta)^s u +u=u^p\\qquad \\text{in }\n\\Omega\n  \\end{equation*} with zero Dirichlet datum, where $\\epsilon>0$ is a small\nparameter, $s\\in(0,1)$, $p\\in(1,\\frac{n+2}{n-2})$ and $\\Omega$ is a smooth,\nbounded domain. We construct a family of solutions that concentrate, as\n$\\epsilon\\rightarrow 0$, at an interior point of $\\Omega$ having uniform\ndistance to $\\partial\\Omega$ (this point can also be characterized as a local\nminimum of a nonlocal functional).\n  In spite of the presence of the Laplace operator, the leading order of the\nrelevant reduced energy functional in the Lyapunov-Schmidt procedure is\npolynomial rather than exponential in the distance to the boundary, in light of\nthe nonlocal effect at infinity. A delicate analysis is required to establish\nsome uniform estimates with respect to $\\epsilon$, due to the difficulty caused\nby the different scales coming from the mixed operator.",
        "Linear representation hypothesis posits that high-level concepts are encoded\nas linear directions in the representation spaces of LLMs. Park et al. (2024)\nformalize this notion by unifying multiple interpretations of linear\nrepresentation, such as 1-dimensional subspace representation and\ninterventions, using a causal inner product. However, their framework relies on\nsingle-token counterfactual pairs and cannot handle ambiguous contrasting\npairs, limiting its applicability to complex or context-dependent concepts. We\nintroduce a new notion of binary concepts as unit vectors in a canonical\nrepresentation space, and utilize LLMs' (neural) activation differences along\nwith maximum likelihood estimation (MLE) to compute concept directions (i.e.,\nsteering vectors). Our method, Sum of Activation-base Normalized Difference\n(SAND), formalizes the use of activation differences modeled as samples from a\nvon Mises-Fisher (vMF) distribution, providing a principled approach to derive\nconcept directions. We extend the applicability of Park et al. (2024) by\neliminating the dependency on unembedding representations and single-token\npairs. Through experiments with LLaMA models across diverse concepts and\nbenchmarks, we demonstrate that our lightweight approach offers greater\nflexibility, superior performance in activation engineering tasks like\nmonitoring and manipulation.",
        "Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as\nthe number of layers increases, node embeddings become increasingly similar,\nand model performance drops sharply. Traditionally, oversmoothing has been\nquantified using metrics that measure the similarity of neighbouring node\nfeatures, such as the Dirichlet energy. While these metrics are related to\noversmoothing, we argue they have critical limitations and fail to reliably\ncapture oversmoothing in realistic scenarios. For instance, they provide\nmeaningful insights only for very deep networks and under somewhat strict\nconditions on the norm of network weights and feature representations. As an\nalternative, we propose measuring oversmoothing by examining the numerical or\neffective rank of the feature representations. We provide theoretical support\nfor this approach, demonstrating that the numerical rank of feature\nrepresentations converges to one for a broad family of nonlinear activation\nfunctions under the assumption of nonnegative trained weights. To the best of\nour knowledge, this is the first result that proves the occurrence of\noversmoothing without assumptions on the boundedness of the weight matrices.\nAlong with the theoretical findings, we provide extensive numerical evaluation\nacross diverse graph architectures. Our results show that rank-based metrics\nconsistently capture oversmoothing, whereas energy-based metrics often fail.\nNotably, we reveal that a significant drop in the rank aligns closely with\nperformance degradation, even in scenarios where energy metrics remain\nunchanged.",
        "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
        "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
        "Adaptive teaming, the ability to collaborate with unseen teammates without\nprior coordination, remains an underexplored challenge in multi-robot\ncollaboration. This paper focuses on adaptive teaming in multi-drone\ncooperative pursuit, a critical task with real-world applications such as\nborder surveillance, search-and-rescue, and counter-terrorism. We first define\nand formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone\n\\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a\ncomprehensive framework that integrates simulation, algorithm training and\nreal-world deployment. AT-MDP framework provides a flexible experiment\nconfigurator and interface for simulation, a distributed training framework\nwith an extensive algorithm zoo (including two newly proposed baseline methods)\nand an unseen drone zoo for evaluating adaptive teaming, as well as a\nreal-world deployment system that utilizes edge computing and Crazyflie drones.\nTo the best of our knowledge, AT-MDP framework is the first adaptive framework\nfor continuous-action decision-making in complex real-world drone tasks,\nenabling multiple drones to coordinate effectively with unseen teammates.\nExtensive experiments in four multi-drone pursuit environments of increasing\ndifficulty confirm the effectiveness of AT-MDP framework, while real-world\ndeployments further validate its feasibility in physical systems. Videos and\ncode are available at https:\/\/sites.google.com\/view\/at-mdp.",
        "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
        "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
        "Stochastic gradient descent with momentum (SGDM), which is defined by adding\na momentum term to SGD, has been well studied in both theory and practice.\nTheoretically investigated results showed that the settings of the learning\nrate and momentum weight affect the convergence of SGDM. Meanwhile, practical\nresults showed that the setting of batch size strongly depends on the\nperformance of SGDM. In this paper, we focus on mini-batch SGDM with constant\nlearning rate and constant momentum weight, which is frequently used to train\ndeep neural networks in practice. The contribution of this paper is showing\ntheoretically that using a constant batch size does not always minimize the\nexpectation of the full gradient norm of the empirical loss in training a deep\nneural network, whereas using an increasing batch size definitely minimizes it,\nthat is, increasing batch size improves convergence of mini-batch SGDM. We also\nprovide numerical results supporting our analyses, indicating specifically that\nmini-batch SGDM with an increasing batch size converges to stationary points\nfaster than with a constant batch size. Python implementations of the\noptimizers used in the numerical experiments are available at\nhttps:\/\/anonymous.4open.science\/r\/momentum-increasing-batch-size-888C\/.",
        "Content generation and manipulation approaches based on deep learning methods\nhave seen significant advancements, leading to an increased need for techniques\nto detect whether an image has been generated or edited. Another area of\nresearch focuses on the insertion and harmonization of objects within images.\nIn this study, we explore the potential of using harmonization data in\nconjunction with a segmentation model to enhance the detection of edited image\nregions. These edits can be either manually crafted or generated using deep\nlearning methods. Our findings demonstrate that this approach can effectively\nidentify such edits. Existing forensic models often overlook the detection of\nharmonized objects in relation to the background, but our proposed Disharmony\nNetwork addresses this gap. By utilizing an aggregated dataset of harmonization\ntechniques, our model outperforms existing forensic networks in identifying\nharmonized objects integrated into their backgrounds, and shows potential for\ndetecting various forms of edits, including virtual try-on tasks.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.",
        "Large Language Models (LLMs) have shown promise in natural language\nprocessing tasks, with the potential to automate systematic reviews. This study\nevaluates the performance of three state-of-the-art LLMs in conducting\nsystematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across\nfour systematic review tasks: study design formulation, search strategy\ndevelopment, literature screening, and data extraction. Sourced from a\npreviously published systematic review, we provided reference standard\nincluding standard PICO (Population, Intervention, Comparison, Outcome) design,\nstandard eligibility criteria, and data from 20 reference literature. Three\ninvestigators evaluated the quality of study design and eligibility criteria\nusing 5-point Liker Scale in terms of accuracy, integrity, relevance,\nconsistency and overall performance. For other tasks, the output is defined as\naccurate if it is the same as the reference standard. Search strategy\nperformance was evaluated through accuracy and retrieval efficacy. Screening\naccuracy was assessed for both abstracts screening and full texts screening.\nData extraction accuracy was evaluated across 1,120 data points comprising\n3,360 individual fields. Claude-3 demonstrated superior overall performance in\nPICO design. In search strategy formulation, GPT-4 and Claude-3 achieved\ncomparable accuracy, outperforming Mistral. For abstract screening, GPT-4\nachieved the highest accuracy, followed by Mistral and Claude-3. In data\nextraction, GPT-4 significantly outperformed other models. LLMs demonstrate\npotential for automating systematic review tasks, with GPT-4 showing superior\nperformance in search strategy formulation, literature screening and data\nextraction. These capabilities make them promising assistive tools for\nresearchers and warrant further development and validation in this field.",
        "We propose a semi-quantum conference key agreement (SQCKA) protocol that\nleverages on GHZ states. We provide a comprehensive security analysis for our\nprotocol that does not rely on a trusted mediator party. We present\ninformation-theoretic security proof, addressing collective attacks within the\nasymptotic limit of infinitely many rounds. This assumption is practical, as\nparticipants can monitor and abort the protocol if deviations from expected\nnoise patterns occur. This advancement enhances the feasibility of SQCKA\nprotocols for real-world applications, ensuring strong security without complex\nnetwork topologies or third-party trust.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https:\/\/github.com\/liuyuqi123study\/RankSVM_for_SLR",
        "Bourgain's slicing conjecture was recently resolved by Joseph Lehec and Bo'az\nKlartag. We present an alternative proof by establishing small ball probability\nestimates for isotropic log-concave measures. Our approach relies on the\nstochastic localization process and Guan's bound, techniques also used by\nKlartag and Lehec. The link between small ball probabilities and the slicing\nconjecture was first observed by Dafnis and Paouris and is established through\nMilman's theory of M-ellipsoids.",
        "We study the high-temperature equilibrium for the C*-algebra $\\mathcal\nT(\\mathbb N^\\times \\ltimes \\mathbb Z)$ recently considered by an Huef, Laca and\nRaeburn. We show that the simplex of KMS$_\\beta$ states at each inverse\ntemperature $\\beta$ in the critical interval $(0,1]$ is a Bauer simplex whose\nspace of extreme points is homeomorphic to $\\mathbb N \\sqcup\\{\\infty\\}$. This\nis in contrast to the uniqueness of equilibrium at high temperature observed in\npreviously considered systems arising from number theory. We also establish a\nconnection between the phase transitions on quotients of our system and the\nBost-Connes phase transition.",
        "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
        "Graph learning plays a vital role in mining and analyzing complex\nrelationships involved in graph data, which is widely used in many real-world\napplications like transaction networks and communication networks. Foundation\nmodels in CV and NLP have shown powerful cross-domain capabilities that are\nalso significant in graph domains. However, existing graph learning approaches\nstruggle with cross-domain tasks. Inspired by successes in CV and NLP,\ncross-domain graph learning has once again become a focal point of attention to\nrealizing true graph foundation models. In this survey, we present a\ncomprehensive review and analysis of existing works on cross-domain graph\nlearning. Concretely, we first propose a new taxonomy, categorizing existing\napproaches based on the learned cross-domain information: structure, feature,\nand structure-feature mixture. Next, we systematically survey representative\nmethods in these categories. Finally, we discuss the remaining limitations of\nexisting studies and highlight promising avenues for future research. Relevant\npapers are summarized and will be consistently updated at:\nhttps:\/\/github.com\/cshhzhao\/Awesome-Cross-Domain-Graph-Learning.",
        "We present three sublinear randomized algorithms for vertex-coloring of\ngraphs with maximum degree $\\Delta$. The first is a simple algorithm that\nextends the idea of Morris and Song to color graphs with maximum degree\n$\\Delta$ using $\\Delta+1$ colors. Combined with the greedy algorithm, it\nachieves an expected runtime of $O(n^{3\/2}\\sqrt{\\log n})$ in the query model,\nimproving on Assadi, Chen, and Khanna's algorithm by a $\\sqrt{\\log n}$ factor\nin expectation. When we allow quantum queries to the graph, we can accelerate\nthe first algorithm using Grover's famous algorithm, resulting in a runtime of\n$\\tilde{O}(n^{4\/3})$ quantum queries. Finally, we introduce a quantum algorithm\nfor $(1+\\epsilon)\\Delta$-coloring, achieving\n$O(\\epsilon^{-1}n^{5\/4}\\log^{3\/2}n)$ quantum queries, offering a polynomial\nimprovement over the previous best bound by Morris and Song.",
        "Image decomposition aims to analyze an image into elementary components,\nwhich is essential for numerous downstream tasks and also by nature provides\ncertain interpretability to the analysis. Deep learning can be powerful for\nsuch tasks, but surprisingly their combination with a focus on interpretability\nand generalizability is rarely explored. In this work, we introduce a novel\nframework for interpretable deep image decomposition, combining hierarchical\nBayesian modeling and deep learning to create an architecture-modularized and\nmodel-generalizable deep neural network (DNN). The proposed framework includes\nthree steps: (1) hierarchical Bayesian modeling of image decomposition, (2)\ntransforming the inference problem into optimization tasks, and (3) deep\ninference via a modularized Bayesian DNN. We further establish a theoretical\nconnection between the loss function and the generalization error bound, which\ninspires a new test-time adaptation approach for out-of-distribution scenarios.\nWe instantiated the application using two downstream tasks, \\textit{i.e.},\nimage denoising, and unsupervised anomaly detection, and the results\ndemonstrated improved generalizability as well as interpretability of our\nmethods. The source code will be released upon the acceptance of this paper.",
        "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https:\/\/scalingintelligence.stanford.edu\/pubs\/codemonkeys.",
        "Drought is a frequent and costly natural disaster in California, with major\nnegative impacts on agricultural production and water resource availability,\nparticularly groundwater. This study investigated the performance of applying\ndifferent machine learning approaches to predicting the U.S. Drought Monitor\nclassification in California. Four approaches were used: a convolutional neural\nnetwork (CNN), random forest, XGBoost, and long short term memory (LSTM)\nrecurrent neural network, and compared to a baseline persistence model. We\nevaluated the models' performance in predicting severe drought (USDM drought\ncategory D2 or higher) using a macro F1 binary classification metric. The LSTM\nmodel emerged as the top performer, followed by XGBoost, CNN, and random\nforest. Further evaluation of our results at the county level suggested that\nthe LSTM model would perform best in counties with more consistent drought\npatterns and where severe drought was more common, and the LSTM model would\nperform worse where drought scores increased rapidly. Utilizing 30 weeks of\nhistorical data, the LSTM model successfully forecasted drought scores for a\n12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less\nthan half a drought category on a scale of 0 to 5. Additionally, the LSTM\nachieved a macro F1 score of 0.9, indicating high accuracy in binary\nclassification for severe drought conditions. Evaluation of different window\nand future horizon sizes in weeks suggested that at least 24 weeks of data\nwould result in the best performance, with best performance for shorter horizon\nsizes, particularly less than eight weeks.",
        "Balancing user agency and system automation is essential for effective\nhuman-AI interactions. Fully automated systems can deliver efficiency but risk\nundermining usability and user autonomy, while purely manual tools are often\ninefficient and fail to enhance user capabilities. This dissertation addresses\nthe question: \"How can we balance user agency and system automation for\ninteractions with intelligent systems?\"\n  We present four main contributions. First, we develop a spherical\nelectromagnet that provides adjustable forces on an untethered tool, allowing\nhaptic feedback while preserving user agency. Second, we create an integrated\nsensing and actuation system that tracks a passive magnetic tool in 3D and\ndelivers haptic feedback without external tracking. Third, we propose an\noptimal control method for electromagnetic haptic guidance that balances user\ninput with system control, enabling users to adjust trajectories and speed.\nFinally, we introduce a model-free reinforcement learning approach for adaptive\ninterfaces that learns interface adaptations without heuristics or real user\ndata. Our simulations and user studies show that shared control significantly\noutperforms naive strategies. By incorporating explicit or implicit models of\nhuman behavior into control strategies, intelligent systems can better account\nfor user agency. We demonstrate that the trade-off between agency and\nautomation is both an algorithmic challenge and an engineering concern, shaped\nby the design of physical devices and user interfaces. We advocate an\nintegrated, end-to-end approach-combining algorithmic, engineering, and design\nperspectives-to enable more intuitive and effective interactions with\nintelligent systems.",
        "This paper introduces a novel mathematical framework for analyzing cyber\nthreat campaigns through fractal geometry. By conceptualizing hierarchical\ntaxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics,\ntechniques, and sub-techniques forming concentric layers, we establish a\nrigorous method for campaign comparison using Hutchinson's Theorem and\nHausdorff distance metrics. Evaluation results confirm that our fractal\nrepresentation preserves hierarchical integrity while providing a\ndimensionality-based complexity assessment that correlates with campaign\ncomplexity. The proposed methodology bridges taxonomy-driven cyber threat\nanalysis and computational geometry, providing analysts with both mathematical\nrigor and interpretable visualizations for addressing the growing complexity of\nadversarial operations across multiple threat domains.",
        "The operation of large-scale power systems is usually scheduled ahead via\nnumerical optimization. However, this requires models of grid topology, line\nparameters, and bus specifications. Classic approaches first identify the\nnetwork topology, i.e., the graph of interconnections and the associated\nimpedances. The power generation schedules are then computed by solving a\nmulti-stage optimal power flow (OPF) problem built around the model. In this\npaper, we explore the prospect of data-driven approaches to multi-stage optimal\npower flow. Specifically, we leverage recent findings from systems and control\nto bypass the identification step and to construct the optimization problem\ndirectly from data. We illustrate the performance of our method on a 118-bus\nsystem and compare it with the classical identification-based approach."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Machine learning in additive manufacturing: State-of-the-art and perspectives",
    "start_abstract":"Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Additive manufacturing and sustainability: an exploratory study of the advantages and challenges"
      ],
      "abstract":[
        "The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Modified FOX Optimizer for Solving optimization problems",
        "On the minimum cut-sets of the power graph of a finite cyclic group, II",
        "Inferring collective synchrony observing spiking of one or several\n  neurons",
        "Bridging statistical mechanics and thermodynamics away from equilibrium:\n  a data-driven approach for learning internal variables and their dynamics",
        "Scalar probability density function mixing models need not comply with\n  the linearity and independence hypothesis",
        "The entropy profiles of a definable set over finite fields",
        "Optimal Functional $2^{s-1}$-Batch Codes: Exploring New Sufficient\n  Conditions",
        "On the ascent of almost and quasi-atomicity to monoid semidomains",
        "Refined curve counting with descendants and quantum mirrors",
        "Dual Control for Interactive Autonomous Merging with Model Predictive\n  Diffusion",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "The influence of missing data mechanisms and simple missing data\n  handling techniques on fairness",
        "From Mutation to Degradation: Predicting Nonsense-Mediated Decay with\n  NMDEP",
        "Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet\n  EXplorer (UVEX)",
        "Zubarev response approach to polarization phenomena in local equilibrium",
        "On the perfect $k$-divisibility of graphs",
        "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
        "Magnetic Trampoline Resonators from (La,Sr)MnO3 Single-Crystal Thin\n  Films",
        "Detection of Physiological Data Tampering Attacks with Quantum Machine\n  Learning",
        "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration",
        "On conductor submonoids of factorial monoids",
        "Vertex Partitioning and $p$-Energy of Graphs",
        "Nuclear magnetic resonance spectroscopy in pulsed magnetic fields",
        "Exploring a tentative link between FRBs and pulsars with broad energy\n  distributions and applications for nearby FRB survey strategies",
        "Mechanics and Design of Metastructured Auxetic Patches with Bio-inspired\n  Materials",
        "A second-order accurate, positivity-preserving numerical scheme for the\n  Poisson-Nernst-Planck-Navier-Stokes system",
        "Spacetime Structure of Regular Accelerating Black Hole Pair in General\n  Relativity",
        "A Few Observations on Sample-Conditional Coverage in Conformal\n  Prediction",
        "Spatiotemporal clustering of GHGs emissions in Europe: exploring the\n  role of spatial component"
      ],
      "abstract":[
        "The FOX optimizer, inspired by red fox hunting behavior, is a powerful\nalgorithm for solving real-world and engineering problems. However, despite\nbalancing exploration and exploitation, it can prematurely converge to local\noptima, as agent positions are updated solely based on the current best-known\nposition, causing all agents to converge on one location. This study proposes\nthe modified FOX optimizer (mFOX) to enhance exploration and balance\nexploration and exploitation in three steps. First, the Oppositional-Based\nLearning (OBL) strategy is used to improve the initial population. Second,\ncontrol parameters are refined to achieve a better balance between exploration\nand exploitation. Third, a new update equation is introduced, allowing agents\nto adjust their positions relative to one another rather than relying solely on\nthe best-known position. This approach improves exploration efficiency without\nadding complexity. The mFOX algorithm's performance is evaluated against 12\nwell-known algorithms on 23 classical benchmark functions, 10 CEC2019\nfunctions, and 12 CEC2022 functions. It outperforms competitors in 74% of the\nclassical benchmarks, 60% of the CEC2019 benchmarks, and 58% of the CEC2022\nbenchmarks. Additionally, mFOX effectively addresses four engineering problems.\nThese results demonstrate mFOX's strong competitiveness in solving complex\noptimization tasks, including unimodal, constrained, and high-dimensional\nproblems.",
        "The power graph $\\mathcal{P}(G)$ of a finite group $G$ is the simple graph\nwith vertex set $G$ and two distinct vertices are adjacent if one of them is a\npower of the other. Let $n=p_1^{n_1}p_2^{n_2}\\cdots p_r^{n_r},$ where\n$p_1,p_2,\\ldots,p_r$ are primes with $p_1<p_2<\\cdots <p_r$ and $n_1,n_2,\\ldots,\nn_r$ are positive integers. For the cyclic group $C_n$ of order $n$, the\nminimum cut-sets of $\\mathcal{P}(C_n)$ are characterized in \\cite{cps} for\n$r\\leq 3$. Recently, in \\cite{MPS}, certain cut-sets of $\\mathcal{P}(C_n)$ are\nidentified such that any minimum cut-set of $\\mathcal{P}(C_n)$ must be one of\nthem. In this paper, for $r\\geq 4$, we explicitly determine the minimum\ncut-sets, in particular, the vertex connectivity of $\\mathcal{P}(C_n)$ when:\n(i) $n_r\\geq 2$, (ii) $r=4$ and $n_r=1$, and (iii) $r=5$, $n_r=1$, $p_1\\geq 3$.",
        "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
        "Thermodynamics with internal variables is a common approach in continuum\nmechanics to model inelastic (i.e., non-equilibrium) material behavior. While\nthis approach is computationally and theoretically attractive, it currently\nlacks a well-established statistical mechanics foundation. As a result,\ninternal variables are typically chosen phenomenologically and lack a direct\nlink to the underlying physics which hinders the predictability of the theory.\nTo address these challenges, we propose a machine learning approach that is\nconsistent with the principles of statistical mechanics and thermodynamics. The\nproposed approach leverages the following techniques (i) the information\nbottleneck (IB) method to ensure that the learned internal variables are\nfunctions of the microstates and are capable of capturing the salient feature\nof the microscopic distribution; (ii) conditional normalizing flows to\nrepresent arbitrary probability distributions of the microscopic states as\nfunctions of the state variables; and (iii) Variational Onsager Neural Networks\n(VONNs) to guarantee thermodynamic consistency and Markovianity of the learned\nevolution equations. The resulting framework, called IB-VONNs, is tested on two\nproblems of colloidal systems, governed at the microscale by overdamped\nLangevin dynamics. The first one is a prototypical model for a colloidal\nparticle in an optical trap, which can be solved analytically, and thus ideal\nto verify the framework. The second problem is a one-dimensional\nphase-transforming system, whose macroscopic description still lacks a\nstatistical mechanics foundation under general conditions. The results in both\ncases indicate that the proposed machine learning strategy can indeed bridge\nstatistical mechanics and thermodynamics with internal variables away from\nequilibrium.",
        "In a mixture of scalar fields undergoing diffusive processes governed by\nFick's law, the concentration at each point evolves linearly in the\nconcentrations at all points and independently from the other concentrations,\nwhen one considers a finite differences integration of their evolution\nequations. However, these properties must not necessarily be enforced in\nprobability density function models, since they are relaxed when conditional\nexpected values are taken.",
        "A definable set $X$ in the first-order language of rings defines a family of\nrandom vectors: for each finite field $\\mathbb{F}_q$, let the distribution be\nsupported and uniform on the $\\mathbb{F}_q$-rational points of $X$. We employ\nresults from the model theory of finite fields to show that their entropy\nprofiles settle into one of finitely many stable asymptotic behaviors as $q$\ngrows. The attainable asymptotic entropy profiles and their dominant terms as\nfunctions of $q$ are computable. This generalizes a construction of Mat\\'u\\v{s}\nwhich gives an information-theoretic interpretation to algebraic matroids.",
        "A functional $k$-batch code of dimension $s$ consists of $n$ servers storing\nlinear combinations of $s$ linearly independent information bits. These codes\nare designed to recover any multiset of $k$ requests, each being a linear\ncombination of the information bits, by $k$ disjoint subsets of servers. A\nrecent conjecture suggests that for any set of $k = 2^{s-1}$ requests, the\noptimal solution requires $2^s-1$ servers. This paper shows that the problem of\nfunctional $k$-batch codes is equivalent to several other problems. Using these\nequivalences, we derive sufficient conditions that improve understanding of the\nproblem and enhance the ability to find the optimal solution.",
        "A commutative monoid is atomic if every non-invertible element factors into\nirreducibles (also called atoms), while an integral (semi)domain is atomic if\nits multiplicative monoid is atomic. Notions weaker than atomicity have been\nintroduced and studied during the past decade, including almost atomicity and\nquasi-atomicity, which were coined and first investigated by Boynton and\nCoykendall in their study of graphs of divisibility of integral domains. The\nascent of atomicity to polynomial extensions was settled by Roitman back in\n1993 while the ascent of atomicity to monoid domains was settled by Coykendall\nand the second author in 2019 (in both cases the answer was negative). The main\npurpose of this paper is to study the ascent of almost atomicity and\nquasi-atomicity to polynomial extensions and monoid domains. Under certain\nreasonable conditions, we establish the ascent of both properties to polynomial\nextensions (over semidomains). Then we construct an explicit example\nillustrating that, with no extra conditions, quasi-atomicity does not ascend to\npolynomial extensions. Finally, we show that, in general, neither almost\natomicity nor quasi-atomicity ascend to monoid domains, improving upon a\nconstruction first provided by Coykendall and the second author for the\nnon-ascent of atomicity.",
        "We establish a formula for structure constants of the quantum mirror to a log\nCalabi Yau surface $(Y,D)$ in terms of descendent logarithmic Gromov--Witten\ninvariants of $(Y,D)$. Our result generalises the weak Frobenius structure\nconjecture for surfaces to the $q$-refined setting, and is proved by relating\nthese invariants to counts of quantum broken lines in the associated quantum\nscattering diagram.",
        "Interactive decision-making is essential in applications such as autonomous\ndriving, where the agent must infer the behavior of nearby human drivers while\nplanning in real-time. Traditional predict-then-act frameworks are often\ninsufficient or inefficient because accurate inference of human behavior\nrequires a continuous interaction rather than isolated prediction. To address\nthis, we propose an active learning framework in which we rigorously derive\npredicted belief distributions. Additionally, we introduce a novel model-based\ndiffusion solver tailored for online receding horizon control problems,\ndemonstrated through a complex, non-convex highway merging scenario. Our\napproach extends previous high-fidelity dual control simulations to hardware\nexperiments, which may be viewed at https:\/\/youtu.be\/Q_JdZuopGL4, and verifies\nbehavior inference in human-driven traffic scenarios, moving beyond idealized\nmodels. The results show improvements in adaptive planning under uncertainty,\nadvancing the field of interactive decision-making for real-world applications.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "Fairness of machine learning algorithms is receiving increasing attention, as\nsuch algorithms permeate the day-to-day aspects of our lives. One way in which\nbias can manifest in a dataset is through missing values. If data are missing,\nthese data are often assumed to be missing completely randomly; in reality the\npropensity of data being missing is often tied to the demographic\ncharacteristics of individuals. There is limited research into how missing\nvalues and the handling thereof can impact the fairness of an algorithm. Most\nresearchers either apply listwise deletion or tend to use the simpler methods\nof imputation (e.g. mean or mode) compared to the more advanced ones (e.g.\nmultiple imputation); we therefore study the impact of the simpler methods on\nthe fairness of algorithms. The starting point of the study is the mechanism of\nmissingness, leading into how the missing data are processed and finally how\nthis impacts fairness. Three popular datasets in the field of fairness are\namputed in a simulation study. The results show that under certain scenarios\nthe impact on fairness can be pronounced when the missingness mechanism is\nmissing at random. Furthermore, elementary missing data handling techniques\nlike listwise deletion and mode imputation can lead to higher fairness compared\nto more complex imputation methods like k-nearest neighbour imputation, albeit\noften at the cost of lower accuracy.",
        "Nonsense-mediated mRNA decay (NMD) is a critical post-transcriptional\nsurveillance mechanism that degrades transcripts with premature termination\ncodons, safeguarding transcriptome integrity and shaping disease phenotypes.\nHowever, accurately predicting NMD efficiency remains challenging, as existing\nmodels often rely on simplistic rule-based heuristics or limited feature sets,\nconstraining their accuracy and generalizability. Using paired DNA and RNA data\nfrom The Cancer Genome Atlas, we benchmark embedding-only models and\ndemonstrate that they underperform compared to a simple rule-based approach. To\naddress this, we develop NMDEP (NMD Efficiency Predictor), an integrative\nframework that combines optimized rule-based methods, sequence embeddings, and\ncurated biological features, achieving state-of-the-art predictive performance.\nThrough explainable AI, we identify key NMD determinants, reaffirming\nestablished factors such as variant position while uncovering novel\ncontributors like ribosome loading. Applied to over 2.9 million simulated\nstop-gain variants, NMDEP facilitates large-scale mRNA degradation assessments,\nadvancing variant interpretation and disease research.",
        "The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope\nselected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.\nUVEX will undertake deep, cadenced surveys of the entire sky to probe low mass\ngalaxies and explore the ultraviolet (UV) time-domain sky, and it will carry\nthe first rapidly deployable UV spectroscopic capability for a broad range of\nscience applications. One of UVEX's prime objectives is to follow up\ngravitational wave (GW) binary neutron star mergers as targets of opportunity\n(ToOs), rapidly scanning across their localization regions to search for their\nkilonova (KN) counterparts. Early-time multiband ultraviolet light curves of\nKNe are key to explaining the interplay between jet and ejecta in binary\nneutron star mergers. Owing to high Galactic extinction in the ultraviolet and\nthe variation of GW distance estimates over the sky, the sensitivity to\nkilonovae can vary significantly across the GW localization and even across the\nfootprint of a single image given UVEX's large field of view. Good ToO\nobserving strategies to trade off between area and depth are neither simple nor\nobvious. We present an optimal strategy for GW follow-up with UVEX in which\nexposure time is adjusted dynamically for each field individually to maximize\nthe overall probability of detection. We model the scheduling problem using the\nexpressive and powerful mathematical framework of mixed integer linear\nprogramming (MILP), and employ a state-of-the-art MILP solver to automatically\ngenerate observing plan timelines that achieve high probabilities of kilonova\ndetection. We have implemented this strategy in an open-source astronomical\nscheduling software package called the Multi-Mission Multi-Messenger\nObservation Planning Toolkit (M4OPT), on GitHub at\nhttps:\/\/github.com\/m4opt\/m4opt.",
        "Using the expansion of Zubarev's density operator, we develop a linear\nresponse approach to study various spin physics in a locally equilibrated\nmedium, particularly focusing on various polarization phenomena in heavy-ion\ncollisions. Specifically, we connect familiar correlation functions and\ndiagrammatic methods to the Zubarev formalism, enabling the use of established\ntechniques like the Matsubara\/imaginary time formalism to facilitate\ncalculations. For a spin-1\/2 particle, we re-derive its vector polarization\nusing this Zubarev response approach, which exactly reproduces with our\nprevious results based on Luttinger's method. For a spin-1 particle, we\ncalculate the vector polarization and find the expected contributions from\nvorticity, temperature gradients, and shear, which are identical to those for\nspin-1\/2 particles except for a factor of 4\/3 as expected. For the tensor\npolarization and spin alignment of a spin-1 boson, we explicitly prove that the\nnon-dissipative contribution is zero at leading order in gradients, and briefly\nreiterate our previous findings for the dissipative contribution with further\ndiscussions on several concerns. Additionally, we discuss several relevant\nsubtleties and questions, including an alternative derivation for Zubarev\nresponse approach, the covariance issues of different spin density matrix\ndefinitions, a further explanation of slow and fast modes, the mode selection\nscheme, etc. We also discuss skeleton expansions, higher-order contributions,\nand non-perturbative methods, particularly their potential connection to\nlattice field theory. In summary, this work discusses the foundations and\nsubtleties of Zubarev response approach, with specific examples from spin\nphysics in heavy-ion collisions.",
        "A graph $G$ is perfectly divisible if, for every induced subgraph $H$ of $G$,\neither $V(H)$ is a stable set or admits a partition into two sets $X_1$ and\n$X_2$ such that $\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is a perfect graph.\nIn this article, we propose the following generalisation of perfectly divisible\ngraphs. A graph $G$ is perfectly $1$-divisible if $G$ is perfect and perfectly\n$k$-divisible if, for every induced subgraph $H$ of $G$, either $V(H)$ is a\nstable set or admits a partition into two sets $X_1$ and $X_2$ such that\n$\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is perfectly $(k-1)$-divisible, $k\n\\in \\mathbb{N}_{> 1}$. Our main result establishes that every perfectly\n$k$-divisible graph $G$ satisfies $\\chi(G) \\leq \\binom{\\omega(G)+k-1}{k}$ which\ngeneralises the known bound for perfectly divisible graphs.",
        "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V\/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
        "Micro-electro-mechanical resonators employing a magnetic element have been\nproposed for magnetic field sensing applications, but the integration of\nmagnetic materials with standard semiconductor compounds is challenging and\nrequires complex fabrication protocols. We present a different approach relying\non (La0.7,Sr0.3)MnO3 (LSMO), an oxide compound that works both as structural\nelement for the resonator and functional magnetic layer. Suspended trampolines\nare realized in a single step process from LSMO thin films and show quality\nfactor up to 60k and fQ products reaching 10$^{10}$ Hz. Their magnetic\nproperties are probed by a SQUID magnetometer and magnetic force microscopy,\nshowing saturation magnetization of 240 kA\/m at room temperature and in-plane\nmagnetic domains with coercivity of 2.5 mT. Being entirely made from a magnetic\nmaterial, these resonators exhibit a larger magnetic interaction volume\ncompared to other solutions, making them ideal candidates as building blocks\nfor high-sensitivity magnetic field sensors.",
        "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases.",
        "Compressive sensing is a signal processing technique that enables the\nreconstruction of sparse signals from a limited number of measurements,\nleveraging the signal's inherent sparsity to facilitate efficient recovery.\nRecent works on the Quantum Compressive Sensing (QCS) architecture, a quantum\ndata-driven approach to compressive sensing where the state of the tensor\nnetwork is represented by a quantum state over a set of entangled qubits, have\nshown promise in advancing quantum data-driven methods for compressive sensing.\nHowever, the QCS framework has remained largely untested on quantum computing\nresources or in the presence of quantum noise. In this work, we present a\npractical implementation of QCS on Amazon Braket, utilizing the Quantum\nImaginary Time Evolution (QITE) projection technique to assess the framework's\ncapabilities under quantum noise. We outline the necessary modifications to the\nQCS framework for deployment on Amazon Braket, followed by results under four\ntypes of quantum noise. Finally, we discuss potential long-term directions\naimed at unlocking the full potential of quantum compressive sensing for\napplications such as signal recovery and image processing.",
        "We give affirmative answers to Conjecture 4.16 in [1] and to Conjecture 2.3\nin [3].",
        "For a Hermitian matrix $A$ of order $n$ with eigenvalues $\\lambda_1(A)\\ge\n\\cdots\\ge \\lambda_n(A)$, define \\[ \\mathcal{E}_p^+(A)=\\sum_{\\lambda_i > 0}\n\\lambda_i^p(A), \\quad \\mathcal{E}_p^-(A)=\\sum_{\\lambda_i<0} |\\lambda_i(A)|^p,\\]\nto be the positive and the negative $p$-energy of $A$, respectively. In this\nnote, first we show that if $A=[A_{ij}]_{i,j=1}^k$, where $A_{ii}$ are square\nmatrices, then \\[ \\mathcal{E}_p^+(A)\\geq \\sum_{i=1}^{k}\n\\mathcal{E}_p^+(A_{ii}), \\quad \\mathcal{E}_p^-(A)\\geq \\sum_{i=1}^{k}\n\\mathcal{E}_p^-(A_{ii}),\\] for any real number $p\\geq 1$. We then apply the\nprevious inequality to establish lower bounds for $p$-energy of the adjacency\nmatrix of graphs.",
        "This article provides an introduction to nuclear magnetic resonance\nspectroscopy in pulsed magnetic fields (PFNMR), focusing on its capabilities,\napplications, and future developments in research involving high magnetic\nfields. It highlights the significance of PFNMR in enhancing the understanding\nof solid-state materials, with particular emphasis on those exhibiting complex\ninteractions and strong electronic correlations. Several technical aspects are\ndiscussed, including the challenges associated with high-frequency NMR\nexperiments. The power of PFNMR is showcased through several examples,\nincluding studies on the topical materials LiCuVO$_4$, SrCu$_2$(BO$_3$)$_2$,\nand CeIn$_3$, offering insights into their magnetic and electronic properties\nat high magnetic fields. The article also discusses possible future directions\nfor the technique, including improvements in PFNMR instrumentation and the\nexploration of materials under extreme conditions. This exposition underscores\nthe role of PFNMR in advancing the frontiers of materials-science research.",
        "Fast radio bursts (FRBs) are energetic, short-duration radio pulses of\nunclear origins. In this study, we investigate the FRBs and pulsars with broad\nenergy distributions by fitting their high energy tails with a power-law model.\nTwo cosmological repeating FRBs (FRB 20201124A and FRB 20220912A), one nearby\nFRB (FRB 20200120E), and two pulsars (RRATs J1846-0257 and J1854+0306), exhibit\npower-law indices of $\\alpha \\gtrsim -1$, suggesting that their bright pulses\ncontribute significantly to the total radio pulse energy. The brightest bursts\nfrom these sources fit well with a simple power-law model ($\\alpha = -0.26 \\pm\n0.05$), indicating a tentative link between certain high-luminosity FRBs and\nlow-luminosity radio bursts. We also discuss detailed survey strategies for\nFAST, MeerKAT and Parkes cryoPAF in the search for FRBs in nearby globular\nclusters (GCs) using different power-law indices, recommending targets for\nobservation. We suggest that combining observations with FAST ($\\sim$ 3 hours)\nand Parkes cryoPAF (10-20 hours) are practicable for discovering new FRBs in\nthe nearby GCs.",
        "Metastructured auxetic patches, characterized by negative Poisson's ratios,\noffer unique mechanical properties that closely resemble the behavior of human\ntissues and organs. As a result, these patches have gained significant\nattention for their potential applications in organ repair and tissue\nregeneration. This study focuses on neural networks-based computational\nmodeling of auxetic patches with a sinusoidal metastructure fabricated from\nsilk fibroin, a bio-inspired material known for its biocompatibility and\nstrength. The primary objective of this research is to introduce a novel,\ndata-driven framework for patch design. To achieve this, we conducted\nexperimental fabrication and mechanical testing to determine material\nproperties and validate the corresponding finite element models. Finite element\nsimulations were then employed to generate the necessary data, while greedy\nsampling, an active learning technique, was utilized to reduce the\ncomputational cost associated with data labeling. Two neural networks were\ntrained to accurately predict Poisson's ratios and stresses for strains up to\n15\\%, respectively. Both models achieved $R^2$ scores exceeding 0.995, which\nindicates highly reliable predictions. Building on this, we developed a neural\nnetwork-based design model capable of tailoring patch designs to achieve\nspecific mechanical properties. This model demonstrated superior performance\nwhen compared to traditional optimization methods, such as genetic algorithms,\nby providing more efficient and precise design solutions. The proposed\nframework represents a significant advancement in the design of bio-inspired\nmetastructures for medical applications, paving the way for future innovations\nin tissue engineering and regenerative medicine.",
        "In this paper, we propose and analyze a second order accurate (in both time\nand space) numerical scheme for the Poisson-Nernst-Planck-Navier-Stokes system,\nwhich describes the ion electro-diffusion in fluids. In particular, the\nPoisson-Nernst-Planck equation is reformulated as a non-constant mobility\ngradient flow in the Energetic Variational Approach. The marker and cell finite\ndifference method is chosen as the spatial discretization, which facilitates\nthe analysis for the fluid part. In the temporal discretization, the mobility\nfunction is computed by a second order extrapolation formula for the sake of\nunique solvability analysis, while a modified Crank-Nicolson approximation is\napplied to the singular logarithmic nonlinear term. Nonlinear artificial\nregularization terms are added in the chemical potential part, so that the\npositivity-preserving property could be theoretically proved. Meanwhile, a\nsecond order accurate, semi-implicit approximation is applied to the convective\nterm in the PNP evolutionary equation, and the fluid momentum equation is\nsimilarly computed. In addition, an optimal rate convergence analysis is\nprovided, based on the higher order asymptotic expansion for the numerical\nsolution, the rough and refined error estimate techniques. The following\ncombined theoretical properties have been established for the second order\naccurate numerical method: (i) second order accuracy, (ii) unique solvability\nand positivity, (iii) total energy stability, and (iv) optimal rate\nconvergence. A few numerical results are displayed to validate the theoretical\nanalysis.",
        "We revisit the one-parameter generalization of the C-metric derived by Ernst,\nwhich solves the vacuum Einstein equations. Resolving conflicting claims in the\nliterature, we determine the correct value of the parameter that ensures the\nregularity of the metric on the axis. This \"regularized C-metric\" describes a\npair of accelerating black holes without the line source present in the\noriginal C-metric. Additionally, this generalization changes the Petrov type\nfrom D to I. We use the Gauss-Bonnet theorem to analyze the nodal\nsingularities, the line source, and their relation to the horizon topology.\nBoth the black hole and acceleration horizons are found to be embeddable in\n$\\mathrm{E}^3$. We examine various geometric and asymptotic properties in\ndetail using several coordinate systems and construct the corresponding 2D and\n3D conformal diagrams. This process is more involved than for the original\nC-metric due to the presence of the exponential factors. These exponential\nfactors also introduce curvature singularities at infinity, which obstructs\nasymptotic flatness. Contrary to Bonnor's expectation, we demonstrate why\nBondi's algorithm for obtaining the standard Bondi form fails for the C-metric,\ndespite its asymptotic flatness. We also show that Ernst's solution-generating\nprescription in boost-rotation symmetric coordinates is a symmetry of the wave\nequation.",
        "We revisit the problem of constructing predictive confidence sets for which\nwe wish to obtain some type of conditional validity. We provide new arguments\nshowing how ``split conformal'' methods achieve near desired coverage levels\nwith high probability, a guarantee conditional on the validation data rather\nthan marginal over it. In addition, we directly consider (approximate)\nconditional coverage, where, e.g., conditional on a covariate $X$ belonging to\nsome group of interest, we would like a guarantee that a predictive set covers\nthe true outcome $Y$. We show that the natural method of performing quantile\nregression on a held-out (validation) dataset yields minimax optimal guarantees\nof coverage here. Complementing these positive results, we also provide\nexperimental evidence that interesting work remains to be done to develop\ncomputationally efficient but valid predictive inference methods.",
        "In this study, we propose a novel application of spatiotemporal clustering in\nthe environmental sciences, with a particular focus on regionalised time series\nof greenhouse gases (GHGs) emissions from a range of economic sectors.\nUtilising a hierarchical spatiotemporal clustering methodology, we analyse\nyearly time series of emissions by gases and sectors from 1990 to 2022 for\nEuropean regions at the NUTS-2 level. While the clustering algorithm inherently\nincorporates spatial information based on geographical distance, the extent to\nwhich space contributes to the definition of groups still requires further\nexploration. To address this gap in the literature, we propose a novel\nindicator, namely the Joint Inertia, which quantifies the contribution of\nspatial distances when integrated with other features. Through a simulation\nexperiment, we explore the relationship between the Joint Inertia and the\nrelevance of geography in exploiting the groups structure under several\nconfigurations of spatial and features patterns, providing insights into the\nbehaviour and potential of the proposed indicator. The empirical findings\ndemonstrate the relevance of the spatial component in identifying emission\npatterns and dynamics, and the results reveal significant heterogeneity across\nclusters in trends and dynamics by gases and sectors. This reflects the\nheterogeneous economic and industrial characteristics of European regions. The\nstudy highlights the importance of the spatial and temporal dimensions in\nunderstanding GHGs emissions, offering baseline insights for future\nspatiotemporal modelling and supporting more targeted and regionally informed\nenvironmental policies."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
    "start_abstract":"This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals"
      ],
      "abstract":[
        "Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli)."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Anomalous Dynamics of a Liquid Corner Film",
        "HST Observations within the Sphere of Influence of the Powerful\n  Supermassive Black Hole in PKS0745-191",
        "Global bifurcations of nodal solutions for coupled elliptic equations",
        "A new convection scheme for GCMs of temperate sub-Neptunes",
        "Singularity of compound stationary measures",
        "The Stability and Accuracy of The Adams-Bashforth-type Integrator",
        "Characterizing Continuous Gravitational Waves from Supermassive Black\n  Hole Binaries in Realistic Pulsar Timing Array Data",
        "Electromagnetic System Conceptual Design for a Negative Triangularity\n  Tokamak",
        "Theory of quantum-geometric charge and spin Josephson diode effects in\n  strongly spin-polarized hybrid structures with noncoplanar spin textures",
        "QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility",
        "Quantum dot-based device for high-performance magnetic microscopy and\n  spin filtering in the Kondo regime",
        "Indirect reciprocity as a dynamics for weak balance",
        "Simple games with minimum",
        "Fully viable DHOST bounce with extra scalar",
        "Thermodynamic properties and Joule-Thomson expansion of AdS black hole\n  with Gaussian distribution in non-commutative geometry",
        "Goldstone bosons at non-zero temperature",
        "Dirac-type condition for Hamilton-generated graphs",
        "Prescribed-Time Newton Extremum Seeking using Delays and Time-Periodic\n  Gains",
        "Metrizability and Dynamics of Weil Bundles",
        "A formula of Perrin-Riou and characteristic power series of signed\n  Selmer groups",
        "A physical model approach to order lot sizing",
        "Intermediate band analysis in Green's functions calculations of\n  quasiparticle interference",
        "Forecasting Constraints on SIGW with Future Pulsar Timing Array\n  Observations",
        "Evolutions of in-medium baryon-baryon scattering cross sections and\n  stiffness of dense nuclear matter from Bayesian analyses of FOPI proton flow\n  excitation functions",
        "On the Curvature and Topology of Compact Stationary Spacetimes",
        "Identification of Genetic Factors Associated with Corpus Callosum\n  Morphology: Conditional Strong Independence Screening for Non-Euclidean\n  Responses",
        "Gaussian Universality of Products Over Split Reductive Groups and the\n  Satake Isomorphism",
        "Characterizing Gaussian quantum processes with Gaussian resources",
        "Behavior of Ising spins and ecological oscillators on dynamically\n  rewired small world networks"
      ],
      "abstract":[
        "Measuring the rheology of liquids typically requires precise control over\nshear rates and stresses. However, we demonstrate that the features of a\npower-law fluid can be predicted by simply observing the capillary spreading\ndynamics of viscous droplets within a wedge-shaped geometry. By considering the\ninfluence of capillary and viscous forces within this geometry, we show that\nthe spreading dynamics can be described by a nonlinear diffusion equation.\nAnalytical predictions indicate subdiffusive behavior, establishing a direct\nrelationship between the diffusion exponent and the rheological exponent, which\nis also corroborated by experimental results. Since this relationship is\nindependent of flow details, it provides robust predictions for the rheological\nproperties of power-law fluids.",
        "We present Space Telescope Imaging Spectrograph observations from the Hubble\nSpace Telescope of the supermassive black hole (SMBH) at the center of\nPKS0745-191, a brightest cluster galaxy (BCG) undergoing powerful radio-mode\nAGN feedback ($P_{\\rm cav}\\sim5\\times10^{45}$ erg s$^{-1}$). These\nhigh-resolution data offer the first spatially resolved map of gas dynamics\nwithin a SMBHs sphere of influence under such powerful feedback. Our results\nreveal the presence of highly chaotic, non-rotational ionized gas flows on\nsub-kpc scales, in contrast to the more coherent flows observed on larger\nscales. While radio-mode feedback effectively thermalizes hot gas in galaxy\nclusters on kiloparsec scales, within the core, the hot gas flow may decouple,\nleading to a reduction in angular momentum and supplying ionized gas through\ncooling, which could enhance accretion onto the SMBH. This process could, in\nturn, lead to a self-regulating feedback loop. Compared to other BCGs with\nweaker radio-mode feedback, where rotation is more stable, intense feedback may\nlead to more chaotic flows, indicating a stronger coupling between jet activity\nand gas dynamics. Additionally, we observe a sharp increase in velocity\ndispersion near the nucleus, consistent with a very massive $M_{\\rm\nBH}\\sim1.5\\times10^{10} M_\\odot$ SMBH. The density profile of the ionized gas\nis also notably flat, paralleling the profiles observed in X-ray gas around\ngalaxies where the Bondi radius is resolved. These results provide valuable\ninsights into the complex mechanisms driving galaxy evolution, highlighting the\nintricate relationship between SMBH fueling and AGN feedback within the host\ngalaxy.",
        "We investigate the global bifurcation structure of the radial nodal solutions\nto the coupled elliptic equations \\begin{equation}\n  \\left\\{\n  \\begin{array}{lr}\n  -{\\Delta}u+u=u^3+\\beta uv^2\\mbox{ in }B_1 ,\\nonumber\n  -{\\Delta}v+v=v^3+\\beta u^2v\\mbox{ in }B_1 ,\\nonumber\n  u,v\\in H_{0,r}^1(B_1).\\nonumber\n  \\end{array}\n  \\right. \\end{equation} Here $B_1$ is a unit ball in $\\mathbb{R}^3$ and\n$\\beta\\in\\mathbb{R}$ the coupling constant is used as bifurcation parameter.\nFor each $k$, the unique pair of nodal solutions $\\pm w_k$ with exactly $k-1$\nzeroes to the scalar field equation $-\\Delta w + w=w^3$ generate exactly four\nsynchronized solution curves and exactly four semi-trivial solution curves to\nthe above system. We obtain a fairly complete global bifurcation structure of\nall bifurcating branches emanating from these eight solution curves of the\nsystem, and show that for different $k$ these bifurcation structures are\ndisjoint. We obtain exact and distinct nodal information for each of the\nbifurcating branches, thus providing a fairly complete characterization of\nnodal solutions of the system in terms of the coupling.",
        "Atmospheric characterisation of temperate sub-Neptunes is the new frontier of\nexoplanetary science with recent JWST observations of possible Hycean world\nK2-18b. Accurate modelling of atmospheric processes is essential to\ninterpreting high-precision spectroscopic data given the wide range of possible\nconditions in the sub-Neptune regime, including on potentially habitable\nplanets. Notably, convection is an important process which can operate in\ndifferent modes across sub-Neptune conditions. Convection can act very\ndifferently in atmospheres with a high condensible mass fraction (non-dilute\natmospheres) or with a lighter background gas, e.g. water convection in a\nH$_2$-rich atmosphere, and can be much weaker or even shut down entirely in the\nlatter case. We present a new mass-flux scheme which can capture these\nvariations and simulate convection over a wide range of parameter space for use\nin 3D general circulation models (GCMs). We validate our scheme for two\nrepresentative cases, a terrestrial-like atmosphere and a mini-Neptune\natmosphere. In the terrestrial case, considering TRAPPIST-1e with an Earth-like\natmosphere, the model performs near-identically to Earth-tuned models in an\nEarth-like convection case. In the mini-Neptune case, considering the bulk\nproperties of K2-18b and assuming a deep H$_2$-rich atmosphere, we demonstrate\nthe capability of the scheme to reproduce non-condensing convection. We find\nconvection occurring at pressures greater than 0.3 bar and the dynamical\nstructure shows high-latitude prograde jets. Our convection scheme will aid in\nthe 3D climate modelling of a wide range of exoplanet atmospheres, and enable\nfurther exploration of temperate sub-Neptune atmospheres.",
        "We show that the product or convex combination of two Markov operators with\nequivalent stationary measures need not have a stationary measure from the same\nmeasure class. More specifically, we exhibit examples of a hitherto undescribed\nphenomenon: maximal entropy random walks for which the resulting compound\nrandom walks no longer have maximal entropy. The underlying group in these\nexamples is $PSL(2,\\mathbb Z)\\cong{{\\mathbb Z}_2}*{{\\mathbb Z}_3}$, and the\nassociated harmonic measures belong to the canonical Minkowski and Denjoy\nmeasure classes on the boundary. These examples also demonstrate that a number\nof other natural families of random walks are not closed under convolutions or\nconvex combinations of step distributions.",
        "This paper presents stability and accuracy analysis of a high-order explicit\ntime stepping scheme introduced by \\cite[Section 2.2]{Buvoli2019}, which\nexhibits superior stability compared to classical Adams-Bashforth. A conjecture\nthat is supported by several numerical phenomena in \\cite[Figure\n2.5]{Buvoli2018}, the method appears to remain stable when the accuracy\napproaches infinity, although it is not yet proven. It is regrettable that this\nhypothesis has been refuted from a fundamental perspective in harmonic\nanalysis. Notwithstanding the aforementioned, this method displays considerably\nenhanced stability in comparison to conventional explicit schemes. Furthermore,\nwe present a criterion for ascertaining the maximum permissible accuracy for a\ngiven specific parabolic stability radius. Conversely, the original method will\nlose one order associated with the expected accuracy, which can be recovered\nwith a slight modification. Consequently, a unified analysis strategy for the\n\\( L^2 \\)-stability will be presented for extensional PDEs under the CFL\ncondition. Finally, a selection of representative numerical examples will be\nshown in order to substantiate the theoretical analysis.",
        "Pulsar timing arrays recently found evidence for a gravitational wave\nbackground (GWB), likely the stochastic overlap of GWs from many supermassive\nblack hole binaries. Anticipating a continuous gravitational wave (CW)\ndetection from a single binary soon to follow, we examine how well current\nBayesian methods can detect CWs and characterize their binary properties by\nmodeling the response of the NANOGrav 15-year pulsar timing array to simulated\nbinary populations. We run Markov Chain Monte Carlo searches for CWs in these\ndatasets and compare them to quicker detection statistics including the optimal\nsignal-to-noise ratio, matched filter detection statistic, and reduced\nlog-likelihood ratio between the signal and noise models calculated at the\ninjected parameters. The latter is the best proxy for Bayesian detection\nfractions, corresponding to a 50% detection fraction (by Bayes factors >10\nfavoring a CW detection over noise-only model) at a signal-to-noise ratio of\n4.6. Source confusion between the GWB and a CW, or between multiple CWs, can\ncause false detections and unexpected dismissals. 53% of realistic binary\npopulations consistent with the recently observed GWB have successful CW\ndetections. 82% of these CWs are in the 4th or 5th frequency bin of the 16.03\nyr dataset (6.9 nHz and 10.8 nHz), with 95 percentile regions spanning\n4nHz-12nHz frequencies, $7-20\\times10^9 M_\\odot$ chirp masses, 60Mpc-8Gpc\nluminosity distances, and 18-13,000 sq. deg 68% confidence localization areas.\nThese successful detections often poorly recover the chirp mass, with only 29%\nidentifying the chirp mass accurately to within 1 dex with a 68% posterior\nwidth also narrower than 1 dex.",
        "Negative triangularity (NT) tokamak configurations have several key benefits\nincluding sufficient core confinement, improved power handling, and reduced\nedge pressure gradients that allow for edge-localized mode (ELM) free\noperation. We present the design of a compact NT device for testing\nsophisticated simulation and control software, with the aim of demonstrating NT\ncontrollability and informing power plant operation. The TokaMaker code is used\nto develop the basic electromagnetic system of the $R_0$ = 1 m, $a$ = 0.27 m,\n$B_t$ = 3 T, $I_p$ = 0.75 MA tokamak. The proposed design utilizes eight\npoloidal field coils with maximum currents of 1 MA to achieve a wide range of\nplasma geometries with $-0.7 < \\delta < -0.3$ and $1.5 < \\kappa < 1.9$.\nScenarios with strong negative triangularity and high elongation are\nparticularly susceptible to vertical instability, necessitating the inclusion\nof high-field side and\/or low-field side passive stabilizing plates which\ntogether reduce vertical instability growth rates by $\\approx$75%. Upper limits\nfor the forces on poloidal and toroidal field coils are predicted and\nmechanical loads on passive structures during current quench events are\nassessed. The 3 T on-axis toroidal field is achieved with 16 demountable copper\ntoroidal field coils, allowing for easy maintenance of the vacuum vessel and\npoloidal field coils. This pre-conceptual design study demonstrates that the\nkey capabilities required of a dedicated NT tokamak experiment can be realized\nwith existing copper magnet technologies.",
        "We present a systematic study of the spin-resolved Josephson diode effect\n(JDE) in strongly spin-polarized ferromagnets (sFM) coupled to singlet\nsuperconductors (SC) via ferromagnetic insulating interfaces (FI). All metallic\nparts are described in the framework of the quasiclassical Usadel Green's\nfunction theory applicable to diffusive systems. The interfaces are\ncharacterized by an S-matrix obtained for a model potential with exchange\nvectors pointing in an arbitrary direction with respect to the magnetization in\nthe sFM. Our theory predicts a large charge Josephson diode effect with an\nefficiency exceeding $33\\%$ and a perfect spin diode effect with $100\\%$\nefficiency. To achieve these the following conditions are necessary: (i) a\nnoncoplanar profile of the three magnetization vectors in the system and (ii)\ndifferent densities of states of spin-$\\uparrow$ and spin-$\\downarrow$ bands in\nthe sFM achieved by a strong spin polarization. The former gives rise to the\nquantum-geometric phase, $\\Delta\\varphi$, that enters the theory in a very\nsimilar manner as the superconducting phase difference across the junction,\n$\\Delta\\chi$. We perform a harmonic analysis of the Josephson current in both\nvariables and find symmetries between Fourier coefficients allowing an\ninterpretation in terms of transfer processes of multiple equal-spin Cooper\npairs across the two ferromagnetic spin bands. We point out the importance of\ncrossed pair transmission processes. Finally, we study a spin-switching effect\nof an equal-spin supercurrent by reversing the magnetic flux in a SQUID device\nincorporating the mentioned junction and propose a way for measuring it.",
        "Machine learning methods are well established in the classification of\nquasars (QSOs). However, the advent of light curve observations adds a great\namount of complexity to the problem. Our goal is to use the Zwicky Transient\nFacility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light\ncurves with a transformer artificial neural network and combine the Pan-STARRS\n(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF\ng-band data with at least 100 observational epochs per light curve, we obtain\n97% F1 score for QSOs. We find that with 3 day median cadence, a survey time\nspan of at least 900 days is required to achieve 90% QSO F1 score. However, one\ncan obtain the same score with a survey time span of 1800 days and the median\ncadence prolonged to 12 days. We find that ZTF classification is superior to\nthe PS static bands, and on par with WISE and Gaia measurements. Additionally,\nwe find that the light curves provide the most important features for QSO\nclassification in the ZTF dataset. We robustly classify objects fainter than\nthe $5\\sigma$ SNR limit at $g=20.8$ by requiring $g < \\mathrm{n_{obs}} \/ 80 +\n20.375$. For this sample, we run inference with added WISE observations, and\nfind 4,849,574 objects classified as QSOs. For 33% of QZO objects, with\navailable WISE data, we publish redshifts with estimated error $\\Delta z\/(1 +\nz) = 0.14$.",
        "We propose a nanoscale device consisting of a double quantum dot with a full\nexchange and pair hopping interaction. In this design, the current can only\nflow through the upper dot, but is sensitive to the spin state of the lower\ndot. The system is immersed in a highly inhomogeneous magnetic field, and only\nthe bottom dot feels a substantial magnetic field, while the top dot\nexperiences only a residual one.\n  We show that our device exhibits very interesting magnetic field-dependent\ntransport properties at low temperatures. The Kondo effect partially survives\nthe presence of the magnetic field and allows to obtain conductances that\ndiffer by several orders of magnitude for the two spin types across the top\ndot.\n  Interestingly, as a function of the magnetic field, our two-dot device\nchanges from a spin singlet state to a spin triplet state, in which the\namplitudes of the spin-dependent conductances are reversed.\n  Our device is able to discriminate between positive and negative magnetic\nfields with a high sensitivity and is therefore particularly interesting for\nimaging the surface of anti-ferromagnetic (AF) insulating materials with\nalternated surface magnetic field, as well as for spin filtering applications.",
        "A social network is often divided into many factions. People are friends\nwithin each faction, while they are enemies of the other factions, and even my\nenemy's enemy is not necessarily my friend. This configuration can be described\nin terms of a weak form of structural balance. Although weak balance explains a\nnumber of real social networks, which dynamical rule achieves it has remained\nunexplored. In this work, we show that the answer can be found in the field of\nindirect reciprocity, which assumes that people assess each other's behavior\nand choose how to behave to others based on the assessment according to a\nsocial norm. We begin by showing that weak structural balance is equivalent to\nstationarity when the rule is given by a norm called `judging'. By analyzing\nits cluster dynamics of merging, division, and migration induced by assessment\nerror in complete graphs, we obtain the cluster size distribution in a steady\nstate, which shows the coexistence of a giant cluster and smaller ones. We\ncompare this shape with the distributions of seats among the parties in the\nparliaments of Germany, the United Kingdom, and Spain. This study suggests that\nindirect reciprocity can provide insight into the interplay between a norm that\nindividuals abide by and the macroscopic group structure in society.",
        "Every simple game is a monotone Boolean function. For the other direction we\njust have to exclude the two constant functions. The enumeration of monotone\nBoolean functions with distinguishable variables is also known as the\nDedekind's problem. The corresponding number for nine variables was determined\njust recently by two disjoint research groups. Considering permutations of the\nvariables as symmetries we can also speak about non-equivalent monotone Boolean\nfunctions (or simple games). Here we consider simple games with minimum, i.e.,\nsimple games with a unique minimal winning vector. A closed formula for the\nnumber of such games is found as well as its dimension in terms of the number\nof players and equivalence classes of players.",
        "In this paper we construct a class of Degenerate Higher-Order Scalar-Tensor\n(DHOST) theories with an extra scalar field, which admits viable solutions of\nbouncing universe satisfying the following requirements: (i) absence of\nBelinski-Khalatnikov-Lifshitz (BKL) instability, ghost and gradient\ninstability, (ii) absence of superluminality, (iii) generation of nearly\nscale-invariant curvature perturbations and very small tensor-to-scalar ratio,\nand (iv) conventional asymptotics in the distant past and future, where gravity\nsector is described by General Relativity and the DHOST scalar has a canonical\nform of Lagrangian. We also expect our models to have sufficiently small\nnon-Gaussianities of primordial curvature perturbations to be compatible with\nobservations. As such, this work exemplifies for the first time the fully\nviable two-field DHOST bouncing cosmology, which is free of instability and\nsuperluminality problems as well as compatible with observations.",
        "The thermodynamics and Joule-Thomson expansion of anti-de Sitter black hole\n(AdS BH) with Gaussian distribution in non-commutative geometry is\nsystematically studied. The metric of Gaussian-distributed BH is obtained,\nshowing a dS geometry at the core of BH. The research indicates that the BH\ncharacterized by a Gaussian distribution exhibit thermodynamic properties that\nare remarkably similar to those of BH with a Lorentzian distribution in\nnon-commutative geometry. This similarity is specifically manifested in the\nsmall BH-large BH phase transition, the corrected first law of thermodynamics,\nthe criticality, the heat capacity, the zeroth-order phase transition and the\nJoule-Thomson process. Notably, the critical ratio of Gaussian-distributed BH\n(0.46531) is significantly larger than those observed in Van der Waals fluids\n(0.375), and indeed, it is also substantially exceed those of\nLorentzian-distributed BH (0.36671). Moreover, compared to the case of\nLorentzian source, the zeroth-order phase transition effect in\nGaussian-distributed BH is exceedingly subtle (accompanied by a relative\nincrease in the Gibbs free energy on the order of $10^{-3}\\!\\sim\\!\\!10^{-2}$)\nand is difficult to detect distinctly.",
        "Spontaneous symmetry breaking in quantum field theories at non-zero\ntemperature still holds fundamental open questions, in particular what happens\nto vacuum Goldstone bosons when the temperature is increased. By investigating\na complex scalar field theory on the lattice we demonstrate that Goldstone\nbosons at non-zero temperature behave like screened massless particle-like\nexcitations, so-called thermoparticles, which continue to exist even in the\nsymmetry-restored phase of the theory. We provide non-perturbative evidence for\nthe functional form of the Goldstone mode's dissipative behaviour, which is\ndistinct from standard perturbative expectations, and determine its\ncorresponding spectral properties.",
        "The cycle space $\\mathcal{C}(G)$ of a graph $G$ is defined as the linear\nspace spanned by all cycles in $G$. For an integer $k\\ge 3$, let $\\mathcal{C}_k\n(G)$ denote the subspace of $\\mathcal{C}(G)$ generated by the cycles of length\nexactly $k$. A graph $G$ on $n$ vertices is called Hamilton-generated if\n$\\mathcal{C}_n (G) = \\mathcal{C}(G)$, meaning every cycle in $G$ is a symmetric\ndifference of some Hamilton cycles of $G$. %A necessary condition for this\nproperty is that $n$ must be odd. Heinig (European J. Combin., 2014) showed\nthat for any $\\sigma >0$ and sufficiently large odd $n$, every $n$-vertex graph\nwith minimum degree $(1+ \\sigma)n\/2$ is Hamilton-generated. He further posed\nthe question that whether the minimum degree requirement could be lowered to\nthe Dirac threshold $n\/2$. Recent progress by Christoph, Nenadov, and\nPetrova~(arXiv:2402.01447) reduced the minimum degree condition to $n\/2 + C$\nfor some large constant $C$. In this paper, we resolve Heinig's problem\ncompletely by proving that for sufficiently large odd $n$, every\nHamilton-connected graph $G$ on $n$ vertices with minimum degree at least\n$(n-1)\/2$ is Hamilton-generated. Moreover, this result is tight for the minimum\ndegree and the Hamilton-connected condition. The proof relies on the\nparity-switcher technique introduced by Christoph, et al in their recent work,\nas well as a classification lemma that strengthens a previous result by\nKrivelevich, Lee, and Sudakov~(Trans. Amer. Math. Soc., 2014).",
        "We study prescribed-time extremum seeking (ES) for scalar maps in the\npresence of time delay. The problem has been solved by Yilmaz and Krstic using\nchirpy probing and time-varying singular gains. To alleviate the gain\nsingularity, we present an alternative approach, employing delays with bounded\ntime-periodic gains, for achieving prescribed-time convergence to the extremum.\nOur results are not extensions or refinements but a new methodological\ndirection, even in the absence of the delay on the map. The main result we\npresent compensates the map's delay and uses perturbation-based and the Newton\n(rather than gradient) approaches. The simultaneous presence of perturbation\nperiod, and two delays -- a map delay and a seeking feedback delay -- whose\nvalues are different (feedback delay must be longer than map delay), makes for\nan intricate situation in the design and analysis.\n  ES can settle arbitrarily soon after four times the map delay. In the absence\nof a map delay, the settling time is arbitrarily short, with feedback delay\nchosen as one quarter of the prescribed settling time, i.e., the search settles\nafter four times any positive feedback delay. In addition to removing the gain\nsingularity of the Yilmaz-Krstic singular-gain prescribed-time ES, we go beyond\nthat method's limitation to operating only up to the terminal time. With the\nhelp of averaging theorems in infinite dimension, we conduct a prescribed-time\nconvergence analysis on a suitable perturbation-averaged \\textit{target} ES\nsystem, which contains the time-periodic gains of the map and feedback delays.\nSince the notion of ``dead-beat'' Lyapunov stabilization by time-periodic\ndelayed feedback originates from Hale and Verduyn-Lunel (analysis, 1993) and\nKarafyllis (feedback design, 2006), we refer to our approach to prescribed-time\nES as the ``Karafyllis, Hale, Verduyn-Lunel\" (KHV) PT-ES approach.",
        "This paper bridges synthetic and classical differential geometry by\ninvestigating the metrizability and dynamics of Weil bundles. For a smooth,\ncompact manifold \\(M\\) and a Weil algebra \\(\\mathbf{A}\\), we prove that the\nmanifold \\(M^\\mathbf{A}\\) of \\(\\mathbf{A}\\)-points admits a canonical,\ncomplete, weighted metric \\(\\mathfrak{d}_w\\) that encodes both base-manifold\ngeometry and infinitesimal deformations. Key results include: (1) Metrization:\n\\(\\mathfrak{d}_w\\) induces a complete metric topology on \\(M^\\mathbf{A}\\). (2)\nPath Lifting: Curves lift from \\(M\\) to \\(M^\\mathbf{A}\\) while preserving\ntopological invariants. (3) Dynamics: Fixed-point theorems for diffeomorphisms\non \\(M^\\mathbf{A}\\) connected to stability analysis. (4) Topological\nEquivalence: \\(H^*(M^\\mathbf{A}) \\cong H^*(M)\\) and \\(\\pi_\\ast(M^\\mathbf{A})\n\\cong \\pi_\\ast(M)\\).",
        "We prove a conjecture of Kundu--Ray, following from the $p$-adic\nBirch--Swinnerton-Dyer conjecture for supersingular primes by\nBernardi--Perrin-Riou and Kato's Main Conjecture, predicting an expression for\nthe leading term (up to a $p$-adic unit) of a characteristic power series of\nKobayashi's signed Selmer groups attached to elliptic curves $E\/\\mathbb{Q}$\nwith supersingular reduction at a prime $p>2$ with $a_p=0$. The proof is\ndeduced from a similar formula due to Perrin-Riou for a generator of her module\nof arithmetic $p$-adic $L$-functions with values in the Dieudonn\\'{e} module of\n$E$.",
        "The growing need for companies to reduce costs and maximize profits has led\nto an increased focus on logistics activities. Among these, inventory\nmanagement plays a crucial role in minimizing organizational expenses by\noptimizing the storage and transportation of materials. In this context, this\nstudy introduces an optimization model for the lot-sizing problem based on a\nphysical system approach. By establishing that the material supply problem is\nisomorphic to a one-dimensional mechanical system of point particles connected\nby elastic elements, we leverage this analogy to derive cost optimization\nconditions naturally and obtain an exact solution. This approach determines lot\nsizes that minimize the combined ordering and inventory holding costs in a\nsignificantly shorter time, eliminating the need for heuristic methods. The\noptimal lot sizes are defined in terms of the parameter $ \\gamma = 2C_O \/ C_H\n$, which represents the relationship between the ordering cost per order ($ C_O\n$) and the holding cost per period for the material required in one period ($\nC_H $). This parameter fully dictates the system's behavior: when $ \\gamma \\leq\n1 $, the optimal strategy is to place one order per period, whereas for $\n\\gamma > 1 $, the number of orders $ N $ is reduced relative to the planning\nhorizon $ M $, meaning $ N < M $. By formulating the total cost function in\nterms of the intensive variable $ N\/M $, we consolidate the entire optimization\nproblem into a single function of $ \\gamma $. This eliminates the need for\ncomplex algorithms, enabling faster and more precise purchasing decisions. The\nproposed model was validated through a real-world case study and benchmarked\nagainst classical algorithms, demonstrating superior cost optimization and\nreduced execution time. These findings underscore the potential of this\napproach for improving material lot-sizing strategies.",
        "The measurement of quasiparticle scattering patterns on material surfaces\nusing scanning tunneling microscopy (STM) is now an established technique for\naccessing the momentum-resolved electronic band structure of solids. However,\nsince these quasiparticle interference (QPI) patterns reflect spatial\nvariations related to differences in the band momenta rather than the momenta\nthemselves, their interpretation often relies on comparisons with simple\ngeometrical models such as the joint density of states (JDOS) or with the\nconvolution of Green's functions. In this paper, we highlight non-intuitive\ndifferences between Green's function and JDOS results. To understand the origin\nof these discrepancies, we analyze the convolution of Green's functions using\nthe Feynman parametrization technique and introduce a framework that we call\nthe intermediate band analysis. This approach allows us to derive simple\nselection rules for interband QPI, based on electron group velocities.\nConnecting the intermediate band analysis with the experiment, we consider\nexperimental Bogoliubov QPI patterns measured for FeSe1-xSx, which were\nrecently used to demonstrate a highly anisotropic superconducting gap,\nindicating superconductivity mediated by nematic fluctuations [1]. The\ncalculated Green's functions convolutions reproduce the particle-hole asymmetry\nin the intensity of QPI patterns across the Fermi level observed in\nexperiments. Finally, we demonstrate the utility of intermediate band analysis\nin tracing the origin of this asymmetry to a coherence factor effect of the\nsuperconducting state.",
        "Pulsar Timing Arrays are playing a crucial role in the ongoing gravitational\nwave astronomy revolution. The current evidence for a stochastic gravitational\nwave background (SGWB) at nHz frequencies offers an opportunity to discover\ncosmological signals and threatens the observability of other subdominant GWs.\nWe explore prospects to constrain second-order scalar-induced GWs (SIGWs)\nassociated with enhanced curvature perturbations in the primordial universe,\nforecasting realistic future PTA datasets. We assess how the currently observed\nsignal could eventually limit future capabilities to search for GW relics of\nprimordial phenomena and associated phenomenological consequences such as\nprimordial black hole (PBH) formation. Given the sensitivity of PBH abundance\nto spectral parameters, measuring it remains a challenge for realistic signals.\nHowever, future observation could still rule out nearly subsolar mass PBHs\nformed through standard formation scenarios in some cases. Future progress in\nconstraining PBH models is expected to stem from theoretical advancements in\nPBH computations, which should help resolve the tension between different\ncomputational methods. The analysis is based on and extends the Python code\n$\\texttt{fastPTA}$.",
        "Within a Bayesian statistical framework using a Gaussian Process (GP)\nemulator for an isospin-dependent Boltzmann-Uehling-Uhlenbeck (IBUU) transport\nmodel simulator of heavy-ion reactions, we infer from the proton directed and\nelliptical flow in mid-central Au+Au reactions at beam energies from 150 to\n1200 MeV\/nucleon taken by the FOPI Collaboration the posterior probability\ndistribution functions (PDFs) of the in-medium baryon-baryon scattering cross\nsection modification factor $X$ (with respect to their free-space values) and\nthe stiffness parameter $K$ of dense nuclear matter. We find that the most\nprobable value of $X$ evolves from around 0.7 to 1.0 as the beam energy\n$E_{beam}\/A$ increases. On the other hand, the posterior PDF($K$) may have dual\npeaks having roughly the same height or extended shoulders at high $K$ values.\nMore quantitatively, the posterior PDF($K$) changes from having a major peak\naround 220 MeV characterizing a soft EOS in the reaction at $E_{beam}\/A$=150\nMeV to one that peaks around 320 MeV indicating a stiff EOS in the reactions at\n$E_{beam}\/A$ higher than about 600 MeV. The transition from soft to stiff\nhappens in mid-central Au+Au reactions at beam energies around 250 MeV\/nucleon\nin which $K=220$ MeV and $K=320$ MeV are approximately equally probable.\nAltogether, the FOPI proton flow excitation function data indicate a gradual\nhardening of hot and dense nuclear matter as its density and temperature\nincrease in reactions with higher beam energies.",
        "Using the result of Petersen $\\&$ Wink '21, we find obstructions to the\ncurvature and topology of compact Lorentzian manifolds admitting a unit-length\ntimelike Killing vector field.",
        "The corpus callosum, the largest white matter structure in the brain, plays a\ncritical role in interhemispheric communication. Variations in its morphology\nare associated with various neurological and psychological conditions, making\nit a key focus in neurogenetics. Age is known to influence the structure and\nmorphology of the corpus callosum significantly, complicating the\nidentification of specific genetic factors that contribute to its shape and\nsize. We propose a conditional strong independence screening method to address\nthese challenges for ultrahigh-dimensional predictors and non-Euclidean\nresponses. Our approach incorporates prior knowledge, such as age. It\nintroduces a novel concept of conditional metric dependence, quantifying\nnon-linear conditional dependencies among random objects in metric spaces\nwithout relying on predefined models. We apply this framework to identify\ngenetic factors associated with the morphology of the corpus callosum.\nSimulation results demonstrate the efficacy of this method across various\nnon-Euclidean data types, highlighting its potential to drive genetic discovery\nin neuroscience.",
        "We establish that the singular numbers (arising from Cartan decomposition)\nand corners (emerging from Iwasawa decomposition) in split reductive groups\nover non-archimedean fields are fundamentally determined by Hall-Littlewood\npolynomials. Through applications of the Satake isomorphism, we extend Van\nPeski's results (arXiv:2011.09356, Theorem 1.3) to encompass arbitrary root\nsystems. Leveraging this theoretical foundation, we further develop Shen's work\n(arXiv:2411.01104, Theorem 1.1) to demonstrate that both singular numbers and\ncorners of such products exhibit minimal separation. This characterization\nenables the derivation of asymptotic properties for singular numbers in matrix\nproducts, particularly establishing the strong law of large numbers and central\nlimit theorem for these quantities. Our results provide a unified framework\nconnecting algebraic decomposition structures with probabilistic limit theorems\nin non-archimedean settings.",
        "Characterizing quantum processes is indispensable for the implementation of\nany task in quantum information processing. In this paper, we develop an\nefficient method to fully characterize arbitrary Gaussian processes in\ncontinuous-variable quantum systems. This is done by directly obtaining all\nelements of the symplectic matrix that describes the process. Only Gaussian\nresources such as coherent probes and quadrature measurements are needed for\nthis task. The method is efficient, involving only $O(N^2)$ steps to\ncharacterize an $N$-mode system. Further, the method is resilient to uniform\nloss. We simulate this procedure using the Python package Strawberry Fields. We\nobserve that heterodyne measurements outperform homodyne measurements for\nreconstructing Gaussian processes.",
        "Many ecological populations are known to display a cyclic behavior with\nperiod 2. Previous work has shown that when a metapopulation (group of coupled\npopulations) with such dynamics is allowed to interact via nearest neighbor\ndispersal in two dimensions, it undergoes a phase transition from disordered\n(spatially asynchronous) to ordered (spatially synchronous) that falls under\nthe 2-D Ising universality class. While nearest neighbor dispersal may\nsatisfactorily describe how most individuals migrate between habitats, we\nshould expect a small fraction of individuals to venture on a journey to\nfurther locations. We model this behavior by considering ecological oscillators\non dynamically rewired small-world networks, in which at each time step a\nfraction $p$ of the nearest neighbor interactions is replaced by a new\ninteraction with a random node on the network. We measure how this connectivity\nchange affects the critical point for synchronizing ecological oscillators. Our\nresults indicate that increasing the amount of long-range interaction\n(increasing $p$) favors the ordered regime, but the presence of memory in\necological oscillators leads to quantitative differences in how much long-range\ndispersal is needed to order the network, relative to an analogous network of\nIsing spins. We also show that, even for very small values of $p$, the phase\ntransition falls into the mean-field universality class, and argue that\necosystems where dispersal can occasionally happen across the system's length\nscale will display a phase transition in the mean-field universality class."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
    "start_abstract":"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli).",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation"
      ],
      "abstract":[
        "This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Bridging Classical and Modern Approaches to Thales' Theorem",
        "S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time\n  Series Forecasting",
        "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery",
        "Family-wise Error Rate Control with E-values",
        "GRNFormer: A Biologically-Guided Framework for Integrating Gene\n  Regulatory Networks into RNA Foundation Models",
        "Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and\n  Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS",
        "PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments",
        "The H\\\"{o}lder regularity of div-curl system with anisotropic\n  coefficients",
        "A Semi-Orthogonal Decomposition Theorem for Weighted Blowups",
        "Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs",
        "Randomized block-Krylov subspace methods for low-rank approximation of\n  matrix functions",
        "Algorithmic Data Minimization for Machine Learning over\n  Internet-of-Things Data Streams",
        "Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference",
        "Medial Axis in Pseudo-Euclidean Spaces",
        "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
        "QLIO: Quantized LiDAR-Inertial Odometry",
        "From an odd arity signature to a Holant dichotomy",
        "What are Models Thinking about? Understanding Large Language Model\n  Hallucinations \"Psychology\" through Model Inner State Analysis",
        "Bootstrapped Reward Shaping",
        "Factor Modelling for Biclustering Large-dimensional Matrix-valued Time\n  Series",
        "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
        "Vulnerability Detection: From Formal Verification to Large Language\n  Models and Hybrid Approaches: A Comprehensive Overview",
        "Local geometry of high-dimensional mixture models: Effective spectral\n  theory and dynamical transitions",
        "Targetless Intrinsics and Extrinsic Calibration of Multiple LiDARs and\n  Cameras with IMU using Continuous-Time Estimation",
        "Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space",
        "Optimal Transport-based Conformal Prediction",
        "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous\n  Sensors via Language Grounding",
        "Inference Computation Scaling for Feature Augmentation in Recommendation\n  Systems",
        "Non-invertible symmetry breaking in a frustration-free spin chain"
      ],
      "abstract":[
        "In this paper, we reconstruct Euclid's theory of similar triangles, as\ndeveloped in Book VI of the \\textit{Elements}, along with its 20th-century\ncounterparts, formulated within the systems of Hilbert, Birkhoff, Borsuk and\nSzmielew, Millman and Parker, as well as Hartshorne. In the final sections, we\npresent recent developments concerning non-Archimedean fields and mechanized\nproofs. Thales' theorem (VI.2) serves as the reference point in our\ncomparisons. It forms the basis of Euclid's system and follows from VI.1 the\nonly proposition within the theory of similar triangles that explicitly applies\nthe definition of proportion. Instead of the ancient proportion, modern systems\nadopt the arithmetic of line segments or real numbers. Accordingly, they adopt\nother propositions from Euclid's Book VI, such as VI.4, VI.6, or VI.9, as a\nbasis. In {\\S}\\,10, we present a system that, while meeting modern criteria of\nrigor, reconstructs Euclid's theory and mimics its deductive structure,\nbeginning with VI.1. This system extends to automated proofs of Euclid's\npropositions from Book VI. Systems relying on real numbers provide the\nfoundation for trigonometry as applied in modern mathematics. In {\\S}\\,9, we\nprove Thales' theorem in geometry over the hyperreal numbers. Just as Hilbert\nmanaged to prove Thales' theorem without referencing the Archimedean axiom, so\ndo we by applying the arithmetic of the non-Archimedean field of hyperreal\nnumbers.",
        "Time series forecasting has recently achieved significant progress with\nmulti-scale models to address the heterogeneity between long and short range\npatterns. Despite their state-of-the-art performance, we identify two potential\nareas for improvement. First, the variates of the multivariate time series are\nprocessed independently. Moreover, the multi-scale (long and short range)\nrepresentations are learned separately by two independent models without\ncommunication. In light of these concerns, we propose State Space Transformer\nwith cross-attention (S2TX). S2TX employs a cross-attention mechanism to\nintegrate a Mamba model for extracting long-range cross-variate context and a\nTransformer model with local window attention to capture short-range\nrepresentations. By cross-attending to the global context, the Transformer\nmodel further facilitates variate-level interactions as well as local\/global\ncommunications. Comprehensive experiments on seven classic long-short range\ntime-series forecasting benchmark datasets demonstrate that S2TX can achieve\nhighly robust SOTA results while maintaining a low memory footprint.",
        "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.",
        "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors.",
        "Foundation models for single-cell RNA sequencing (scRNA-seq) have shown\npromising capabilities in capturing gene expression patterns. However, current\napproaches face critical limitations: they ignore biological prior knowledge\nencoded in gene regulatory relationships and fail to leverage multi-omics\nsignals that could provide complementary regulatory insights. In this paper, we\npropose GRNFormer, a new framework that systematically integrates multi-scale\nGene Regulatory Networks (GRNs) inferred from multi-omics data into RNA\nfoundation model training. Our framework introduces two key innovations. First,\nwe introduce a pipeline for constructing hierarchical GRNs that capture\nregulatory relationships at both cell-type-specific and cell-specific\nresolutions. Second, we design a structure-aware integration framework that\naddresses the information asymmetry in GRNs through two technical advances: (1)\nA graph topological adapter using multi-head cross-attention to weight\nregulatory relationships dynamically, and (2) a novel edge perturbation\nstrategy that perturb GRNs with biologically-informed co-expression links to\naugment graph neural network training. Comprehensive experiments have been\nconducted on three representative downstream tasks across multiple model\narchitectures to demonstrate the effectiveness of GRNFormer. It achieves\nconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\\%$\nincrease in drug response prediction correlation, $9.6\\%$ improvement in\nsingle-cell drug classification AUC, and $1.1\\%$ average gain in gene\nperturbation prediction accuracy.",
        "This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary\nsearch (BS) locking, designed to cover a broad frequency range from 533 MHz to\n4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a\nlinear to a logarithmic function, completing in B+1 cycles, where B represents\nthe digital-to-analog (DAC) resolution controlling the voltage-controlled delay\nline (VCDL). At the start of the BS process, large step sizes can cause\nsignificant bias overshoots, potentially leading to clock failure conditions\n(i.e., clocks fail to propagate through the VCDL). To address this issue, a\ntoggle detector is introduced to monitor clock activity and adjust the binary\nsearch controller. Upon detecting a stalled clock, the controller reverts the\nDAC code to the previous working code and resumes the BS with a reduced step\nsize. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a\nlocking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at\n4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with\na static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter\nof 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit\n(FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.",
        "Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.",
        "This research examines the regularity of weak solutions to the Div-Curl\nsystem with low regularity anisotropic coefficients. The H\\\"older regularity of\nthe Div-Curl system with one anisotropic coefficient was an unresolved problem\nraised by Yin in 2016. We have addressed the open problem, and the findings\nextend to the scenario involving two anisotropic coefficients. We establish the\nH\\\"{o}lder regularity of the solution when the coefficients is H\\\"{o}lder\ncontinuous. Moreover, the degree of H\\\"{o}lder regularity of the solution can\nbe improved if the coefficient has a greater degree of H\\\"{o}lder regularity.",
        "We establish a semi-orthogonal decomposition for the weighted blowup of an\nalgebraic stack along a Koszul-regular weighted centre, generalising the\nclassic result of Orlov. Our approach is based on the work of Bergh-Schn\\\"urer.",
        "Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment\/emotion detection of a given input as a sentence\/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.",
        "The randomized SVD is a method to compute an inexpensive, yet accurate,\nlow-rank approximation of a matrix. The algorithm assumes access to the matrix\nthrough matrix-vector products (matvecs). Therefore, when we would like to\napply the randomized SVD to a matrix function, $f(A)$, one needs to approximate\nmatvecs with $f(A)$ using some other algorithm, which is typically treated as a\nblack-box. Chen and Hallman (SIMAX 2023) argued that, in the common setting\nwhere matvecs with $f(A)$ are approximated using Krylov subspace methods\n(KSMs), more efficient low-rank approximation is possible if we open this\nblack-box. They present an alternative approach that significantly outperforms\nthe naive combination of KSMs with the randomized SVD, although the method\nlacked theoretical justification. In this work, we take a closer look at the\nmethod, and provide strong and intuitive error bounds that justify its\nexcellent performance for low-rank approximation of matrix functions.",
        "Machine learning can analyze vast amounts of data generated by IoT devices to\nidentify patterns, make predictions, and enable real-time decision-making. By\nprocessing sensor data, machine learning models can optimize processes, improve\nefficiency, and enhance personalized user experiences in smart systems.\nHowever, IoT systems are often deployed in sensitive environments such as\nhouseholds and offices, where they may inadvertently expose identifiable\ninformation, including location, habits, and personal identifiers. This raises\nsignificant privacy concerns, necessitating the application of data\nminimization -- a foundational principle in emerging data regulations, which\nmandates that service providers only collect data that is directly relevant and\nnecessary for a specified purpose. Despite its importance, data minimization\nlacks a precise technical definition in the context of sensor data, where\ncollections of weak signals make it challenging to apply a binary \"relevant and\nnecessary\" rule. This paper provides a technical interpretation of data\nminimization in the context of sensor streams, explores practical methods for\nimplementation, and addresses the challenges involved. Through our approach, we\ndemonstrate that our framework can reduce user identifiability by up to 16.7%\nwhile maintaining accuracy loss below 1%, offering a viable path toward\nprivacy-preserving IoT data processing.",
        "Multi-armed bandits (MABs) are frequently used for online sequential\ndecision-making in applications ranging from recommending personalized content\nto assigning treatments to patients. A recurring challenge in the applicability\nof the classic MAB framework to real-world settings is ignoring\n\\textit{interference}, where a unit's outcome depends on treatment assigned to\nothers. This leads to an exponentially growing action space, rendering standard\napproaches computationally impractical. We study the MAB problem under network\ninterference, where each unit's reward depends on its own treatment and those\nof its neighbors in a given interference graph. We propose a novel algorithm\nthat uses the local structure of the interference graph to minimize regret. We\nderive a graph-dependent upper bound on cumulative regret showing that it\nimproves over prior work. Additionally, we provide the first lower bounds for\nbandits with arbitrary network interference, where each bound involves a\ndistinct structural property of the interference graph. These bounds\ndemonstrate that when the graph is either dense or sparse, our algorithm is\nnearly optimal, with upper and lower bounds that match up to logarithmic\nfactors. We complement our theoretical results with numerical experiments,\nwhich show that our approach outperforms baseline methods.",
        "We investigate the notion of the medial axis for pseudo-Euclidean spaces. For\nmost of the article, we follow the path of Birbrair and Denkowski's article\n\"Medial Axis and Singularities\", checking its feasibility in the new context.",
        "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE",
        "LiDAR-Inertial Odometry (LIO) is widely used for autonomous navigation, but\nits deployment on Size, Weight, and Power (SWaP)-constrained platforms remains\nchallenging due to the computational cost of processing dense point clouds.\nConventional LIO frameworks rely on a single onboard processor, leading to\ncomputational bottlenecks and high memory demands, making real-time execution\ndifficult on embedded systems. To address this, we propose QLIO, a\nmulti-processor distributed quantized LIO framework that reduces computational\nload and bandwidth consumption while maintaining localization accuracy. QLIO\nintroduces a quantized state estimation pipeline, where a co-processor\npre-processes LiDAR measurements, compressing point-to-plane residuals before\ntransmitting only essential features to the host processor. Additionally, an\nrQ-vector-based adaptive resampling strategy intelligently selects and\ncompresses key observations, further reducing computational redundancy.\nReal-world evaluations demonstrate that QLIO achieves a 14.1% reduction in\nper-observation residual data while preserving localization accuracy.\nFurthermore, we release an open-source implementation to facilitate further\nresearch and real-world deployment. These results establish QLIO as an\nefficient and scalable solution for real-time autonomous systems operating\nunder computational and bandwidth constraints.",
        "\\textsf{Holant} is an essential framework in the field of counting\ncomplexity. For over fifteen years, researchers have been clarifying the\ncomplexity classification for complex-valued \\textsf{Holant} on the Boolean\ndomain, a challenge that remains unresolved. In this article, we prove a\ncomplexity dichotomy for complex-valued \\textsf{Holant} on Boolean domain when\na non-trivial signature of odd arity exists. This dichotomy is based on the\ndichotomy for \\textsf{\\#EO}, and consequently is an $\\text{FP}^\\text{NP}$ vs.\n\\#P dichotomy as well, stating that each problem is either in\n$\\text{FP}^\\text{NP}$ or \\#P-hard.\n  Furthermore, we establish a generalized version of the decomposition lemma\nfor complex-valued \\textsf{Holant} on Boolean domain. It asserts that each\nsignature can be derived from its tensor product with other signatures, or\nconversely, the problem itself is in $\\text{FP}^\\text{NP}$. We believe that\nthis result is a powerful method for building reductions in complex-valued\n\\textsf{Holant}, as it is also employed as a pivotal technique in the proof of\nthe aforementioned dichotomy in this article.",
        "Large language model (LLM) systems suffer from the models' unstable ability\nto generate valid and factual content, resulting in hallucination generation.\nCurrent hallucination detection methods heavily rely on out-of-model\ninformation sources, such as RAG to assist the detection, thus bringing heavy\nadditional latency. Recently, internal states of LLMs' inference have been\nwidely used in numerous research works, such as prompt injection detection,\netc. Considering the interpretability of LLM internal states and the fact that\nthey do not require external information sources, we introduce such states into\nLLM hallucination detection. In this paper, we systematically analyze different\ninternal states' revealing features during inference forward and\ncomprehensively evaluate their ability in hallucination detection.\nSpecifically, we cut the forward process of a large language model into three\nstages: understanding, query, generation, and extracting the internal state\nfrom these stages. By analyzing these states, we provide a deep understanding\nof why the hallucinated content is generated and what happened in the internal\nstate of the models. Then, we introduce these internal states into\nhallucination detection and conduct comprehensive experiments to discuss the\nadvantages and limitations.",
        "In reinforcement learning, especially in sparse-reward domains, many\nenvironment steps are required to observe reward information. In order to\nincrease the frequency of such observations, \"potential-based reward shaping\"\n(PBRS) has been proposed as a method of providing a more dense reward signal\nwhile leaving the optimal policy invariant. However, the required \"potential\nfunction\" must be carefully designed with task-dependent knowledge to not deter\ntraining performance. In this work, we propose a \"bootstrapped\" method of\nreward shaping, termed BSRS, in which the agent's current estimate of the\nstate-value function acts as the potential function for PBRS. We provide\nconvergence proofs for the tabular setting, give insights into training\ndynamics for deep RL, and show that the proposed method improves training speed\nin the Atari suite.",
        "A novel unsupervised learning method is proposed in this paper for\nbiclustering large-dimensional matrix-valued time series based on an entirely\nnew latent two-way factor structure. Each block cluster is characterized by its\nown row and column cluster-specific factors in addition to some common matrix\nfactors which impact on all the matrix time series. We first estimate the\nglobal loading spaces by projecting the observation matrices onto the row or\ncolumn loading space corresponding to common factors. The loading spaces for\ncluster-specific factors are then further recovered by projecting the\nobservation matrices onto the orthogonal complement space of the estimated\nglobal loading spaces. To identify the latent row\/column clusters\nsimultaneously for matrix-valued time series, we provide a $K$-means algorithm\nbased on the estimated row\/column factor loadings of the cluster-specific weak\nfactors. Theoretically, we derive faster convergence rates for global loading\nmatrices than those of the state-of-the-art methods available in the literature\nunder mild conditions. We also propose an one-pass eigenvalue-ratio method to\nestimate the numbers of global and cluster-specific factors. The consistency\nwith explicit convergence rates is also established for the estimators of the\nlocal loading matrices, the factor numbers and the latent cluster memberships.\nNumerical experiments with both simulated data as well as a real data example\nare also reported to illustrate the usefulness of our proposed method.",
        "Despite recent advances in artificial intelligence (AI), it poses challenges\nto ensure personalized decision-making in tasks that are not considered in\ntraining datasets. To address this issue, we propose ValuePilot, a two-phase\nvalue-driven decision-making framework comprising a dataset generation toolkit\nDGT and a decision-making module DMM trained on the generated data. DGT is\ncapable of generating scenarios based on value dimensions and closely mirroring\nreal-world tasks, with automated filtering techniques and human curation to\nensure the validity of the dataset. In the generated dataset, DMM learns to\nrecognize the inherent values of scenarios, computes action feasibility and\nnavigates the trade-offs between multiple value dimensions to make personalized\ndecisions. Extensive experiments demonstrate that, given human value\npreferences, our DMM most closely aligns with human decisions, outperforming\nClaude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is\na preliminary exploration of value-driven decision-making. We hope it will\nstimulate interest in value-driven decision-making and personalized\ndecision-making within the community.",
        "Software testing and verification are critical for ensuring the reliability\nand security of modern software systems. Traditionally, formal verification\ntechniques, such as model checking and theorem proving, have provided rigorous\nframeworks for detecting bugs and vulnerabilities. However, these methods often\nface scalability challenges when applied to complex, real-world programs.\nRecently, the advent of Large Language Models (LLMs) has introduced a new\nparadigm for software analysis, leveraging their ability to understand insecure\ncoding practices. Although LLMs demonstrate promising capabilities in tasks\nsuch as bug prediction and invariant generation, they lack the formal\nguarantees of classical methods. This paper presents a comprehensive study of\nstate-of-the-art software testing and verification, focusing on three key\napproaches: classical formal methods, LLM-based analysis, and emerging hybrid\ntechniques, which combine their strengths. We explore each approach's\nstrengths, limitations, and practical applications, highlighting the potential\nof hybrid systems to address the weaknesses of standalone methods. We analyze\nwhether integrating formal rigor with LLM-driven insights can enhance the\neffectiveness and scalability of software verification, exploring their\nviability as a pathway toward more robust and adaptive testing frameworks.",
        "We study the local geometry of empirical risks in high dimensions via the\nspectral theory of their Hessian and information matrices. We focus on settings\nwhere the data, $(Y_\\ell)_{\\ell =1}^n\\in \\mathbb R^d$, are i.i.d. draws of a\n$k$-component Gaussian mixture model, and the loss depends on the projection of\nthe data into a fixed number of vectors, namely $\\mathbf{x}^\\top Y$, where\n$\\mathbf{x}\\in \\mathbb{R}^{d\\times C}$ are the parameters, and $C$ need not\nequal $k$. This setting captures a broad class of problems such as\nclassification by one and two-layer networks and regression on multi-index\nmodels. We prove exact formulas for the limits of the empirical spectral\ndistribution and outlier eigenvalues and eigenvectors of such matrices in the\nproportional asymptotics limit, where the number of samples and dimension\n$n,d\\to\\infty$ and $n\/d=\\phi \\in (0,\\infty)$. These limits depend on the\nparameters $\\mathbf{x}$ only through the summary statistic of the $(C+k)\\times\n(C+k)$ Gram matrix of the parameters and class means, $\\mathbf{G} =\n(\\mathbf{x},\\mathbf{\\mu})^\\top(\\mathbf{x},\\mathbf{\\mu})$. It is known that\nunder general conditions, when $\\mathbf{x}$ is trained by stochastic gradient\ndescent, the evolution of these same summary statistics along training\nconverges to the solution of an autonomous system of ODEs, called the effective\ndynamics. This enables us to connect the spectral theory to the training\ndynamics. We demonstrate our general results by analyzing the effective\nspectrum along the effective dynamics in the case of multi-class logistic\nregression. In this setting, the empirical Hessian and information matrices\nhave substantially different spectra, each with their own static and even\ndynamical spectral transitions.",
        "Accurate spatiotemporal calibration is a prerequisite for multisensor fusion.\nHowever, sensors are typically asynchronous, and there is no overlap between\nthe fields of view of cameras and LiDARs, posing challenges for intrinsic and\nextrinsic parameter calibration. To address this, we propose a calibration\npipeline based on continuous-time and bundle adjustment (BA) capable of\nsimultaneous intrinsic and extrinsic calibration (6 DOF transformation and time\noffset). We do not require overlapping fields of view or any calibration board.\nFirstly, we establish data associations between cameras using Structure from\nMotion (SFM) and perform self-calibration of camera intrinsics. Then, we\nestablish data associations between LiDARs through adaptive voxel map\nconstruction, optimizing for extrinsic calibration within the map. Finally, by\nmatching features between the intensity projection of LiDAR maps and camera\nimages, we conduct joint optimization for intrinsic and extrinsic parameters.\nThis pipeline functions in texture-rich structured environments, allowing\nsimultaneous calibration of any number of cameras and LiDARs without the need\nfor intricate sensor synchronization triggers. Experimental results demonstrate\nour method's ability to fulfill co-visibility and motion constraints between\nsensors without accumulating errors.",
        "Spatial reasoning is a fundamental capability of embodied agents and has\ngarnered widespread attention in the field of multimodal large language models\n(MLLMs). In this work, we propose a novel benchmark, Open3DVQA, to\ncomprehensively evaluate the spatial reasoning capacities of current\nstate-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consists\nof 9k VQA samples, collected using an efficient semi-automated tool in a\nhigh-fidelity urban simulator. We evaluate several SOTA MLLMs across various\naspects of spatial reasoning, such as relative and absolute spatial\nrelationships, situational reasoning, and object-centric spatial attributes.\nOur results reveal that: 1) MLLMs perform better at answering questions\nregarding relative spatial relationships than absolute spatial relationships,\n2) MLLMs demonstrate similar spatial reasoning abilities for both egocentric\nand allocentric perspectives, and 3) Fine-tuning large models significantly\nimproves their performance across different spatial reasoning tasks. We believe\nthat our open-source data collection tools and in-depth analyses will inspire\nfurther research on MLLM spatial reasoning capabilities. The benchmark is\navailable at https:\/\/github.com\/WeichenZh\/Open3DVQA.",
        "Conformal Prediction (CP) is a principled framework for quantifying\nuncertainty in blackbox learning models, by constructing prediction sets with\nfinite-sample coverage guarantees. Traditional approaches rely on scalar\nnonconformity scores, which fail to fully exploit the geometric structure of\nmultivariate outputs, such as in multi-output regression or multiclass\nclassification. Recent methods addressing this limitation impose predefined\nconvex shapes for the prediction sets, potentially misaligning with the\nintrinsic data geometry. We introduce a novel CP procedure handling\nmultivariate score functions through the lens of optimal transport.\nSpecifically, we leverage Monge-Kantorovich vector ranks and quantiles to\nconstruct prediction region with flexible, potentially non-convex shapes,\nbetter suited to the complex uncertainty patterns encountered in multivariate\nlearning tasks. We prove that our approach ensures finite-sample,\ndistribution-free coverage properties, similar to typical CP methods. We then\nadapt our method for multi-output regression and multiclass classification, and\nalso propose simple adjustments to generate adaptive prediction regions with\nasymptotic conditional coverage guarantees. Finally, we evaluate our method on\npractical regression and classification problems, illustrating its advantages\nin terms of (conditional) coverage and efficiency.",
        "Interacting with the world is a multi-sensory experience: achieving effective\ngeneral-purpose interaction requires making use of all available modalities --\nincluding vision, touch, and audio -- to fill in gaps from partial observation.\nFor example, when vision is occluded reaching into a bag, a robot should rely\non its senses of touch and sound. However, state-of-the-art generalist robot\npolicies are typically trained on large datasets to predict robot actions\nsolely from visual and proprioceptive observations. In this work, we propose\nFuSe, a novel approach that enables finetuning visuomotor generalist policies\non heterogeneous sensor modalities for which large datasets are not readily\navailable by leveraging natural language as a common cross-modal grounding. We\ncombine a multimodal contrastive loss with a sensory-grounded language\ngeneration loss to encode high-level semantics. In the context of robot\nmanipulation, we show that FuSe enables performing challenging tasks that\nrequire reasoning jointly over modalities such as vision, touch, and sound in a\nzero-shot setting, such as multimodal prompting, compositional cross-modal\nprompting, and descriptions of objects it interacts with. We show that the same\nrecipe is applicable to widely different generalist policies, including both\ndiffusion-based generalist policies and large vision-language-action (VLA)\nmodels. Extensive experiments in the real world show that FuSeis able to\nincrease success rates by over 20% compared to all considered baselines.",
        "Large language models have become a powerful method for feature augmentation\nin recommendation systems. However, existing approaches relying on quick\ninference often suffer from incomplete feature coverage and insufficient\nspecificity in feature descriptions, limiting their ability to capture\nfine-grained user preferences and undermining overall performance. Motivated by\nthe recent success of inference scaling in math and coding tasks, we explore\nwhether scaling inference can address these limitations and enhance feature\nquality.\n  Our experiments show that scaling inference leads to significant improvements\nin recommendation performance, with a 12% increase in NDCG@10. The gains can be\nattributed to two key factors: feature quantity and specificity. In particular,\nmodels using extended Chain-of-Thought (CoT) reasoning generate a greater\nnumber of detailed and precise features, offering deeper insights into user\npreferences and overcoming the limitations of quick inference. We further\ninvestigate the factors influencing feature quantity, revealing that model\nchoice and search strategy play critical roles in generating a richer and more\ndiverse feature set. This is the first work to apply inference scaling to\nfeature augmentation in recommendation systems, bridging advances in reasoning\ntasks to enhance personalized recommendation.",
        "A nearest-neighbor, frustration-free spin $\\frac{1}{2}$ chain can be\nconstructed {\\it via} projectors of various ranks \\'{a} la Bravyi-Gosset. We\nshow that in the rank 1 case this system is gapped and has two ground states\nresembling ferromagnetic states. These states spontaneously break the\nnon-invertible symmetry connecting them. The latter is proved using the\nmachinery of algebraic quantum theory. The non-invertible symmetries of this\nsystem do not come from a duality."
      ]
    }
  },
  {
    "id":2412.2062,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Artificial intelligence in elderly healthcare: A scoping review",
    "start_abstract":"The ageing population has led to a surge in the adoption of artificial intelligence (AI) technologies in elderly healthcare worldwide. However, in the advancement of AI technologies, there is currently a lack of clarity about the types and roles of AI technologies in elderly healthcare. This scoping review aimed to provide a comprehensive overview of AI technologies in elderly healthcare by exploring the types of AI technologies employed, and identifying their roles in elderly healthcare based on existing studies. A total of 10 databases were searched for this review, from January 1 2000 to July 31 2022. Based on the inclusion criteria, 105 studies were included. The AI devices utilized in elderly healthcare were summarised as robots, exoskeleton devices, intelligent homes, AI-enabled health smart applications and wearables, voice-activated devices, and virtual reality. Five roles of AI technologies were identified: rehabilitation therapists, emotional supporters, social facilitators, supervisors, and cognitive promoters. Results showed that the impact of AI technologies on elderly healthcare is promising and that AI technologies are capable of satisfying the unmet care needs of older adults and demonstrating great potential in its further development in this area. More well-designed randomised controlled trials are needed in the future to validate the roles of AI technologies in elderly healthcare.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Digital health platforms for the elderly? Key adoption and usage barriers and ways to address them"
      ],
      "abstract":[
        "Digital healthcare platforms (DHPs) represent a relatively new phenomenon that could provide valuable complement to physical primary care \u2013 for example, by reducing costs, improving access healthcare, and allowing patient monitoring. However, such are mainly used today the younger generations, which creates \"digital divide\" between elderly. This article aims identify: i) perceived key barriers inhibit adoption usage of DHPs elderly, ii) what DHP providers can do facilitate increased The draws on qualitative interviews with elderly complementary process data from major Swedish DHP. We find perceives two initial DHPs: negative attitudes technology anxiety one barrier affecting both lack trust. analysis also identifies multiple development suggestions improvement better accommodate needs including application tailored education activities. an integrated framework outlining ways address them. In so doing, we contribute literature mHealth in healthcare."
      ],
      "categories":[
        "Healthcare"
      ]
    },
    "list":{
      "title":[
        "Complete heteroclinic networks derived from graphs consisting of two\n  cycles",
        "Continual Release Moment Estimation with Differential Privacy",
        "Exploratory study on the masses of odd-$Z$ nuclei and $r$-process\n  simulation based on the deformed relativistic Hartree-Bogoliubov theory in\n  continuum",
        "A Zero-Knowledge Proof for the Syndrome Decoding Problem in the Lee\n  Metric",
        "Estimating Network Models using Neural Networks",
        "Perturbations of a minimal surface with triple junctions in\n  $\\mathbb{R}^2 \\times \\mathbb{S}^1$",
        "GLIMPSE: An ultra-faint $\\simeq$ 10$^{5}$ $M_{\\odot}$ Pop III Galaxy\n  Candidate and First Constraints on the Pop III UV Luminosity Function at\n  $z\\simeq6-7$",
        "Spin separation and filtering assisted by topological corner states in\n  the Kekul\\'{e} lattice",
        "Wavelet-based density sketching with functional hierarchical tensor",
        "Half a Million M Dwarf Stars Characterized Using Domain-Adapted Spectral\n  Analysis",
        "Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD",
        "Tangent Currents, King's Residue Formula and Intersection Theory",
        "On the flow characteristics in the shock formation region due to the\n  diaphragm opening process in a shock tube",
        "Connections between the minimal neighborhood and the activity value of\n  cellular automata",
        "Qoala: an Application Execution Environment for Quantum Internet Nodes",
        "The probabilistic combinatorial attacks on atmospheric\n  continuous-variable quantum secret sharing",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "The Faber-Krahn inequality for partial sums of eigenvalues of Toeplitz\n  operators",
        "Inferring the density and membership of stellar streams with flexible\n  models: The GD-1 stream in Gaia Data Release 3",
        "Miniaturized optical system for a chip based cold atom inertial sensor",
        "A sturdy spin-momentum locking in a chiral organic superconductor",
        "Exploring experimental limit of deep quantum signal processing using a\n  trapped-ion simulator",
        "H0 tension in Tsallis and Renyi statistics",
        "Dual-Source SPIR over a noiseless MAC without Data Replication or Shared\n  Randomness",
        "Real-time Bus Travel Time Prediction and Reliability Quantification: A\n  Hybrid Markov Model",
        "Bulk-edge correspondence at the spin-to-integer quantum Hall effect\n  crossover in topological superconductors",
        "Aggregation of dipolar molecules in SiO$_2$ hybrid organic inorganic\n  films: use of silver nanoparticles as inhibitors of molecular aggregation",
        "Applications of Random Matrix Theory in Machine Learning and Brain\n  Mapping",
        "Crystal field splittings and magnetic ground state of the square-lattice\n  antiferromagnets YbBi2ClO4 and YbBi2IO4 with Jeff = 1\/2"
      ],
      "abstract":[
        "We address the question how a given connection structure (directed graph) can\nbe realised as a heteroclinic network that is complete in the sense that it\ncontains all unstable manifolds of its equilibria. For a directed graph\nconsisting of two cycles we provide a constructive method to achieve this: (i)\nenlarge the graph by adding some edges and (ii) apply the simplex method to\nobtain a network in phase space. Depending on the length of the cycles we\nderive the minimal number of required new edges. In the resulting network each\nadded edge leads to a positive transverse eigenvalue at the respective\nequilibrium. We discuss the total number of such positive eigenvalues in an\nindividual cycle and some implications for the stability of this cycle.",
        "We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10.",
        "\\textbf{Background:} Nuclear masses of exotic nuclei are important for both\nnuclear physics and astrophysics. The deformed relativistic Hartree-Bogoliubov\ntheory in continuum (DRHBc) is capable of providing proper descriptions for\nexotic nuclei by simultaneously including deformation and continuum effects.\nThe mass table of even-$Z$ nuclei with $8\\le Z\\le 120$ has been established\nbased on the DRHBc theory [ADNDT 158, 101661 (2024)]. \\textbf{Purpose:} This\nwork aims to systematically estimate the masses of odd-$Z$ nuclei based on the\navailable DRHBc results of even-$Z$ nuclei, thereby providing a pseudo DRHBc\nmass table for all nuclei with $8\\le Z\\le 120$. This mass table can then be\nemployed in the $r$-process studies to investigate the influence of deformation\non $r$-process. \\textbf{Method:} The mass of an odd nucleus is expressed as a\nfunction of the masses and odd-even mass differences of its neighboring even\nnuclei, with the odd-even mass difference approximated by the average pairing\ngap. The $r$-process simulations are carried out using the site-independent\nclassical $r$-process model based on the waiting-point approximation.\n\\textbf{Results and Conclusions:} The approximation of the odd-even mass\ndifference with the average pairing gap is validated to be effective, by\nreproducing the masses of even-$Z$ odd-$N$ nuclei calculated by DRHBc.\nCombining the DRHBc masses of even-$Z$ nuclei and the estimated masses of\nodd-$Z$, a pseudo DRHBc mass table is established, with the root-mean-square\n(rms) deviation from available mass data $\\sigma=1.50$ MeV. This pseudo DRHBc\nmass table is applied to the $r$-process simulation, and the impact of nuclear\ndeformation effects is analyzed. The deformation effects can influence the\n$r$-process path and thus affect the $r$-process abundance. In particular, the\nnuclear shape transitions can even lead to the discontinuity of the $r$-process\npath.",
        "The syndrome decoding problem is one of the NP-complete problems lying at the\nfoundation of code-based cryptography. The variant thereof where the distance\nbetween vectors is measured with respect to the Lee metric, rather than the\nmore commonly used Hamming metric, has been analyzed recently in several works\ndue to its potential relevance for building more efficient code-based\ncryptosystems. The purpose of this article is to describe a zero-knowledge\nproof for this variant of the problem.",
        "Exponential random graph models (ERGMs) are very flexible for modeling\nnetwork formation but pose difficult estimation challenges due to their\nintractable normalizing constant. Existing methods, such as MCMC-MLE, rely on\nsequential simulation at every optimization step. We propose a neural network\napproach that trains on a single, large set of parameter-simulation pairs to\nlearn the mapping from parameters to average network statistics. Once trained,\nthis map can be inverted, yielding a fast and parallelizable estimation method.\nThe procedure also accommodates extra network statistics to mitigate model\nmisspecification. Some simple illustrative examples show that the method\nperforms well in practice.",
        "We construct stationary perturbations of a specific minimal surface with a\ncircle of triple junctions in $\\mathbb{R}^2 \\times \\mathbb{S}^1$, that satisfy\ngiven boundary data.",
        "Detecting the first generation of stars, Population III (PopIII), has been a\nlong-standing goal in astrophysics, yet they remain elusive even in the JWST\nera. Here we present a novel NIRCam-based selection method for PopIII galaxies,\nand carefully validate it through completeness and contamination simulations.\nWe systematically search ~500 arcmin$^{2}$ across JWST legacy fields for PopIII\ncandidates, including GLIMPSE which, assisted by gravitational lensing, has\nproduced JWST's deepest NIRCam imaging thus far. We discover one promising\nPopIII galaxy candidate (GLIMPSE-16043) at $z=6.50^{+0.03}_{-0.24}$, a\nmoderately lensed galaxy (mu=2.9) with an intrinsic UV magnitude of\n$M_{UV}$=-15.89. It exhibits key PopIII features: strong H$\\alpha$ emission\n(rest-frame EW $2810\\pm550$\\AA); a Balmer jump; no dust (UV slope\n$\\beta=-2.34\\pm0.36$); and undetectable metal lines (e.g., [OIII];\n[OIII]\/H$\\beta$<0.44) implying a gas-phase metallicity of Zgas\/Zsun<0.5%. These\nproperties indicate the presence of a nascent, metal-deficient young stellar\npopulation (<5Myr) with a stellar mass of $\\simeq10^{5}M_{\\odot}$.\nIntriguingly, this source deviates significantly from the extrapolated\nUV-metallicity relation derived from recent JWST observations at $z=4-10$,\nconsistent with UV enhancement by a top-heavy PopIII initial mass function or\nthe presence of an extremely metal-poor AGN. We also derive the first\nobservational constraints on the PopIII UV luminosity function at z~6-7. The\nvolume density of GLIMPSE-16043 ($\\approx10^{-4}$ cMpc$^{-3}$) is in excellent\nagreement with theoretical predictions, independently reinforcing its\nplausibility. This study demonstrates the power of our novel NIRCam method to\nfinally reveal distant galaxies even more pristine than the Milky Way's most\nmetal-poor satellites, thereby promising to bring us closer to the first\ngeneration of stars than we have ever been before.",
        "Higher-order topological corner states have been realized in two-dimensional\nKekul\\'{e} lattice, which can be further coupled with spin polarization through\nthe implementation of local magnetization. In this work, we numerically\ninvestigate the spin-dependent transport properties assisted by topological\ncorner states in the Kekul\\'{e} lattice. By applying local magnetization and\nelectric potential, the topological corner states are spin polarized with\nopposite spins localized at different corners, thereby demonstrating a\nspin-corner state locking mechanism. Transport characteristics, including\ntransmission, local density of states, and local current density, are\ncalculated for a two-terminal setup consisting of a diamond-shaped Kekul\\'{e}\nlattice connected to two leads. When opposite local magnetization is applied to\nthe corners, spin-up and spin-down electrons are perfectly separated, forming\ntwo spin-polarized conducting channels and leading to spin spatial separation.\nIn the presence of identical local magnetization on both corners and an\nelectric potential at one corner, the spin-polarized corner states can\nfacilitate selective filtering of different spins and generate spin-polarized\ncurrents by tuning the energy. Furthermore, spin-resolved transmission diagrams\nas functions of both the Fermi energy and electric potential are presented,\nillustrating the global distribution of spin filtering through topological\ncorner states.",
        "We introduce the functional hierarchical tensor under a wavelet basis (FHT-W)\nansatz for high-dimensional density estimation in lattice models. Recently, the\nfunctional tensor network has emerged as a suitable candidate for density\nestimation due to its ability to calculate the normalization constant exactly,\na defining feature not enjoyed by neural network alternatives such as\nenergy-based models or diffusion models. While current functional tensor\nnetwork models show good performance for lattice models with weak or moderate\ncouplings, we show that they face significant model capacity constraints when\napplied to lattice models with strong coupling. To address this issue, this\nwork proposes to perform density estimation on the lattice model under a\nwavelet transformation. Motivated by the literature on scale separation, we\nperform iterative wavelet coarsening to separate the lattice model into\ndifferent scales. Based on this multiscale structure, we design a new\nfunctional hierarchical tensor ansatz using a hierarchical tree topology,\nwhereby information on the finer scale is further away from the root node of\nthe tree. Our experiments show that the numerical rank of typical lattice\nmodels is significantly lower under appropriate wavelet transformation.\nFurthermore, we show that our proposed model allows one to model challenging\nGaussian field models and Ginzburg-Landau models.",
        "We present fundamental atmospheric parameters (Teff and log g) and\nmetallicities ([M\/H]) for 507,513 M dwarf stars using low-resolution spectra\n(R~1800) from LAMOST DR10. By employing Cycle-StarNet, an innovative domain\nadaptation approach, we successfully bridge the gap between theoretical PHOENIX\nsynthetic spectra and observed LAMOST spectra, enabling parameter measurements\neven for lower signal-to-noise data (S\/N>5). The fitting residual analysis\nshows a reduction from 2.0 times to 1.68 times the flux uncertainty. Comparing\nwith available literature values, we find systematic offsets and precisions of\n12$\\pm$70 K in Teff, -0.04$\\pm$0.17 dex in log g, and -0.06$\\pm$0.20 dex in\n[M\/H]. The precision improves for higher quality spectra (S\/N>50) to 47 K, 0.12\ndex, and 0.14 dex respectively. The metallicity consistency between wide\nbinaries shows a scatter of 0.24 dex, improving to 0.15 dex at S\/N>50. We\nprovide a comprehensive catalog including stellar parameters, spectral\nclassifications, activity indicators, and binary\/variability flags,\nestablishing a resource for studies of the most numerous stellar population.\nThe complete catalog is available at https:\/\/doi.org\/10.5281\/zenodo.14030249.",
        "Correlations between dislocations in crystals reduce the elastic energy via\nscreening of the strain by the surrounding dislocations. We study the\ncorrelations of threading dislocations in GaN epitaxial films with dislocation\ndensities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in\nreciprocal space and by high-resolution electron backscatter diffraction (EBSD)\nin real space, where the strain is derived from a cross-correlation analysis of\nthe Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps\nare compared with Monte Carlo simulations within one and the same model for the\ndislocation distributions. The screening of the dislocation strains is provided\nby creating pairs of dislocations with opposite Burgers vectors, with the mean\ndistance between dislocations in a pair equal to the screening distance. The\npairs overlap and cannot be distinguished as separate dipoles. The\nEBSD-measured autocorrelation functions of the strain and rotation components\nfollow the expected logarithmic law for distances smaller than the screening\ndistances and become zero for larger distances, which is confirmed by the Monte\nCarlo simulations. Screening distances of 2 \\textmu m and 0.3 \\textmu m are\nobtained for the samples with low and high dislocation densities, respectively.\nThe dislocation strain is thus screened by only 4 neighboring dislocations.\nHigh-resolution EBSD allows for a more precise determination of the screening\ndistances than from fits of the XRD curves. In addition, an anisotropic\nresolution of the EBSD measurements is observed and quantified.",
        "In this work, we study the intersection of positive closed currents on\ndomains. We use the theory of tangent currents in connection with King's\nresidue formula. We find a sufficient condition for the local existence of\ntangent currents, and express the shadow of tangent currents and the\n$h$-dimension of tangent currents in terms of the complex Monge-Amp\\`ere type\ncurrent. Further, a reasonable integrability condition for the existence of the\nunique tangent current with minimal $h$-dimension is introduced. We apply it to\nthe study of the intersection of positive closed currents, find a sufficient\ncondition for the intersection of positive closed currents on domains and\ndescribe the intersection in terms of the complex Monge-Amp\\`ere type current.\nAt the same time, we obtain regularizations of positive closed currents that\nwork well with the suggested intersection of positive closed currents. In\nparticular, the standard regularization of currents by convolution actually\nproduces the convergence towards the intersection of positive closed currents.\nIn this sense, our approach generalizes King's work on currents defined by\nanalytic varieties, which is obtained from Federer's slicing theory. Some\nclassical examples are computed. Our work is applicable to general complex\nmanifolds not necessarily compact or K\\\"ahler.",
        "The shock formation process in shock tubes has been extensively studied;\nhowever, significant gaps remain in understanding the effects of the diaphragm\nrupture process on the resulting flow non-uniformities. Existing models\npredicting the shock attenuation and propagation dynamics overlook critical\ndiaphragm mechanics and their impact on shock behavior. Addressing this gap is\nvital for improving predictive capabilities and optimizing shock tube designs\nfor applications in combustion kinetics, aerodynamics, and high-speed\ndiagnostics. This study investigates the shock wave formation and propagation\nthrough combined experimental and numerical approaches over a range of\ndriver-to-driven pressure ratios (Driver pressure: 9.4 - 24.2 bar of helium;\nDriven pressure: 100 Torr of argon). High-speed imaging captures the diaphragm\nopening dynamics, while pressure and shock velocity measurements along the\nentire driven section of the shock tube provide key validation data for CFD.\nTwo-dimensional numerical simulations incorporate experimentally measured\ndiaphragm opening profiles, offering detailed insights into flow features and\nthermodynamic gradients behind the moving shock front. Key parameters,\nincluding deceleration and acceleration phases within the shock formation\nregion, shock formation distances, and times, have been quantified. A novel\ntheoretical framework is introduced to correlate these parameters, enabling\naccurate predictions of shock Mach number evolution under varying conditions.\nThis unified methodology bridges theoretical and experimental gaps, providing a\nrobust foundation for advancing shock tube research and design.",
        "For a group $G$ and a finite set $A$, a cellular automaton is a\ntransformation of the configuration space $A^G$ defined via a finite\nneighborhood and a local map. Although neighborhoods are not unique, every CA\nadmits a unique minimal neighborhood, which consists on all the essential cells\nin $G$ that affect the behavior of the local map. An active transition of a\ncellular automaton is a pattern that produces a change on the current state of\na cell when the local map is applied. In this paper, we study the links between\nthe minimal neighborhood and the number of active transitions, known as the\nactivity value, of cellular automata. Our main results state that the activity\nvalue usually imposes several restrictions on the size of the minimal\nneighborhood of local maps.",
        "Recently, a first-of-its-kind operating system for programmable quantum\nnetwork nodes was developed, called QNodeOS. Here, we present an extension of\nQNodeOS called Qoala, which introduces (1) a unified program format for hybrid\ninteractive classical-quantum programs, providing a well-defined target for\ncompilers, and (2) a runtime representation of a program that allows joint\nscheduling of the hybrid classical-quantum program, multitasking, and\nasynchronous program execution. Based on concrete design considerations, we put\nforward the architecture of Qoala, including the program structure and\nexecution mechanism. We implement Qoala in the form of a modular and extendible\nsimulator that is validated against real-world quantum network hardware\n(available online). However, Qoala is not meant to be purely a simulator, and\nimplementation is planned on real hardware. We evaluate Qoala's effectiveness\nand performance sensitivity to latencies and network schedules using an\nextensive simulation study. Qoala provides a framework that opens the door for\nfuture computer science research into quantum network applications, including\nscheduling algorithms and compilation strategies that can now readily be\nexplored using the framework and tools provided.",
        "The combination of quantum secret sharing (QSS) and continuous-variable\nquantum key distribution (CV-QKD) has demonstrated clear advantages and has\nundergone significant development in recent years. However, research on the\npractical security of CV-QSS remains limited, particularly in the context of\nfree-space channels, which exhibit considerable flexibility. In this paper, we\nstudy the practical security of free-space CV-QSS, innovatively propose an\nattack strategy that probabilistically combines two-point distribution attack\n(TDA) and uniform distribution attack (UDA). We also establish channel\nparameter models, especially a channel noise model based on local local\noscillators (LLO), to further evaluate the key rate. In principle, the analysis\ncan be extended to any number of probabilistic combinations of channel\nmanipulation attacks. The numerical results demonstrate that the probabilistic\ncombination attacks reduce the real key rate of CV-QSS under moderate intensity\nturbulence, but still enable secure QSS at a distance of 8 km on a scale of\nhundreds. However, it should be noted that the probabilistic combination\nattacks will make the deviation between the estimated key rate and the real key\nrate, i.e., the key rate is overestimated, which may pose a security risk.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "We prove that, among all radial subsets $\\Omega\\subset \\mathbb{C}$ of\nprescribed measure, the ball is the only maximizer of the sum of the first $K$\neigenvalues ($K\\geq 1$) of the corresponding Toeplitz operator $T_\\Omega$ on\nthe Fock space $\\mathcal{F}^2(\\mathbb{C})$. As a byproduct, we prove that balls\nmaximize any Schatten $p$-norm of $T_\\Omega$ for $p>1$ (and minimize the\ncorresponding quasinorm for $p<1$), and that the second eigenvalue is maximized\nby a particular annulus. Moreover, we extend some of these results to general\nradial symbols in $L^p(\\mathbb{C})$, with $p > 1$, characterizing those that\nmaximize the sum of the first $K$ eigenvalues.",
        "As bound stellar systems orbit within a galaxy, stars may be tidally stripped\nto form streams of stars that nearly follow the orbit of their progenitor\nsystem. Stellar streams provide one of the most promising avenues for\nconstraining the global mass distribution of the Milky Way and the nature of\ndark matter (DM). The stream stars' kinematic \"track\" enables inferring\nlarge-scale properties of the DM distribution, while density variations and\nanomalies provide information about local DM clumps (e.g., from DM subhalos).\nUsing precise astrometric data from the Gaia Mission, which enables clean\nselections of Milky Way stream stars, we now know of a few streams with\nperturbations and density anomalies. A full accounting of the density tracks\nand substructures within all >100 Milky Way stellar streams will therefore\nenable powerful new constraints on DM. However, methods for discovering and\ncharacterizing membership of streams are heterogeneous and often highly\ncustomized to individual streams. Here we present a new, flexible framework for\nmodeling stellar stream density and membership. Our framework allows us to\ninclude off-track or non-Gaussian components to the stream density, meaning we\ncan capture anomalous features (such as the GD-1 steam's spur). We test our\nmodel on GD-1, where we characterize previously-known features and provide the\nlargest catalog of probable member stars to date (1689 stars). Our framework\n(built on JAX and numpyro) provides a path toward uniform analysis of all Milky\nWay streams, enabling tight constraints on the Galactic mass distribution and\nits dark matter.",
        "We miniaturized the complex optical system responsible for the cooling,\npumping and imaging of an on-chip based cold atom inertial sensor. This optical\nbench uses bonded miniature optics and includes all the necessary optical\nfunctions. The bench has a volume of 35x25x5~cm$^3$. We developed a laser\nfrequency lock adapted to the optical bench using saturated absorption in a\nrubidium cell. The entire laser source based on frequency doubling of\n1.56~$\\mu$m fiber lasers, including the control system and the saturated\nabsorption module, fits in a $5U$-rack. Using the miniaturized bench, we\nrealized two and three dimensional magneto optical traps for Rubidium 87 atoms.",
        "Among noncentrosymmetric structures, chirality has recently been recognized\nas a novel source of asymmetrical charge\/spin transports as exemplified by\nelectrical magnetochiral anisotropy (EMChA) and chirality-induced spin\nselectivity. Although similar bulk-charge rectification and Rashba-Edelstein\neffect in polar systems are quantitively reproducible by theory based on the\nelectronic band structures, the relevance of band parameters in chiral effects\nremains elusive. Here, by working with a chiral organic superconductor, we\nexperimentally demonstrate a gigantic EMChA and large superconducting diode\neffect, both of which are difficult to be explained solely by its band\nparameters. A two-critical-current signature and an enhanced critical field\nsuggested triplet-mixed Cooper pairs with anomalously enhanced spin-orbit\ncoupling above atomic limit. Our results clearly highlight a unique\nspin-momentum locking with large stiffness beyond the expectation, suggesting\nan unknown driving force for spin polarization inherent to chirality.",
        "Quantum signal processing (QSP), which enables systematic polynomial\ntransformations on quantum data through sequences of qubit rotations, has\nemerged as a fundamental building block for quantum algorithms and data\nre-uploading quantum neural networks. While recent experiments have\ndemonstrated the feasibility of shallow QSP circuits, the inherent limitations\nin scaling QSP to achieve complex transformations on quantum hardware remain an\nopen and critical question. Here we report the first experimental realization\nof deep QSP circuits in a trapped-ion quantum simulator. By manipulating the\nqubit encoded in a trapped $^{43}\\textrm{Ca}^{+}$ ion, we demonstrate\nhigh-precision simulation of some prominent functions used in quantum\nalgorithms and machine learning, with circuit depths ranging from 15 to 360\nlayers and implementation time significantly longer than coherence time of the\nqubit. Our results reveal a crucial trade-off between the precision of function\nsimulation and the concomitant accumulation of hardware noise, highlighting the\nimportance of striking a balance between circuit depth and accuracy in\npractical QSP implementation. This work addresses a key gap in understanding\nthe scalability and limitations of QSP-based algorithms on quantum hardware,\nproviding valuable insights for developing quantum algorithms as well as\npractically realizing quantum singular value transformation and data\nre-uploading quantum machine learning models.",
        "Motivated by recent attempts to study the implications of Tsallis and Renyi\nstatistics in gravitational, cosmological, and astrophysical systems, the\npossible relationships between H0 tension and generalized statistics are\nexplored. It is obtained that, in the light of H0 tension, the energytime\nuncertainty relations in Tsallis and Renyi statistics constrain the values of\nTsallis and Renyi parameters. Hence, the way to find the footprints of\nnon-extensivity in the universe is paved.",
        "Information-theoretically secure Symmetric Private Information Retrieval\n(SPIR) is known to be infeasible over noiseless channels with a single server.\nKnown solutions to overcome this infeasibility involve additional resources\nsuch as database replication, shared randomness, or noisy channels. In this\npaper, we propose an alternative approach for achieving SPIR with\ninformation-theoretic security guarantees, without relying on shared\nrandomness, noisy channels, or data replication. Specifically, we demonstrate\nthat it is sufficient to use a noiseless binary adder multiple-access channel,\nwhere inputs are controlled by two non-colluding servers and the output is\nobserved by the client, alongside a public noiseless communication channel\nbetween the client and the servers. Furthermore, in this setting, we\ncharacterize the optimal file rates, i.e., the file lengths normalized by the\nnumber of channel uses, that can be transferred.",
        "Accurate and reliable bus travel time prediction in real-time is essential\nfor improving the operational efficiency of public transportation systems.\nHowever, this remains a challenging task due to the limitations of existing\nmodels and data sources. This study proposed a hybrid Markovian framework for\nreal-time bus travel time prediction, incorporating uncertainty quantification.\nFirstly, the bus link travel time distributions were modeled by integrating\nvarious influential factors while explicitly accounting for heteroscedasticity.\nParticularly, the parameters of the distributions were estimated using Maximum\nLikelihood Estimation, and the Fisher Information Matrix was then employed to\ncalculate the 95\\% uncertainty bounds for the estimated parameters, ensuring a\nrobust and reliable quantification of prediction uncertainty of bus link travel\ntimes. Secondly, a Markovian framework with transition probabilities based on\npreviously predicted bus link travel times was developed to predict travel\ntimes and their uncertainties from a current location to any future stop along\nthe route. The framework was evaluated using the General Transit Feed\nSpecification (GTFS) Static and Realtime data collected in 2023 from\nGainesville, Florida. The results showed that the proposed model consistently\nachieved better prediction performance compared to the selected baseline\napproaches (including historical mean, statistical and AI-based models) while\nproviding narrower uncertainty bounds. The model also demonstrated high\ninterpretability, as the estimated coefficients provided insights into how\ndifferent factors influencing bus travel times across links with varying\ncharacteristics. These findings suggest that the model could serve as a\nvaluable tool for transit system performance evaluation and real-time trip\nplanning.",
        "The spin and integer quantum Hall effects are two cousins of topological\nphase transitions in two-dimensional electronic systems. Their close\nrelationship makes it possible to transform spin to integer quantum Hall effect\nin two-dimensional topological superconductors by continuous increase in a\nsymmetry breaking Zeeman magnetic field. We study peculiarities of bulk-edge\ncorrespondence and a fate of massless edge and bulk topological (instantons)\nexcitations at such the crossover.",
        "The technological implementation of hybrid organic-inorganic materials in\nsecond order nonlinear optical photonic devices depends strongly on the ability\nof the host matrixes to contain high loads of dipolar molecules without\naggregation. Some organic molecules are often used to diminish the attracting\ninteractions between dipolar molecules in such kind of materials, but their\nefficiency as inhibitors of molecular aggregation is limited by their\npolarizability. In this work, we report the use of silver nanoparticles as\ninhibitors of molecular aggregation in hybrid organic-inorganic films doped\nwith dipolar molecules. The large polarizability of the silver nanoparticles\nmakes them ideal moieties for the inhibition of the electrostatic interactions\nbetween dipolar nonlinear optical molecules. The average size of the silver\nnanoparticles in this work was 70.5 nm in diameter, they were synthesized using\nsilver nitrate (AgNO$_3$) as precursor and\naminoethylaminopropyltrimethoxysilane as reducing agent. These nanoparticles\nwere immersed in SiO$_2$ hybrid organic-inorganic sol-gel films doped with\ndipolar chromophores to study their effect as inhibitors of dipolar\nchromophores aggregation. The presence of the silver nanoparticles in the solid\nfilms was confirmed by transmission electronic microscopy and UV-Visible\nspectroscopy. UV-Visible spectroscopy was also used to monitor the dipolar\nchromophores aggregation in the SiO$_2$ films. We found that, at room\ntemperature, silver nanoparticles are good inhibiting chromophores aggregation\nin comparison with the performance of organic inhibitors.",
        "Brain mapping analyzes the wavelengths of brain signals and outputs them in a\nmap, which is then analyzed by a radiologist. Introducing Machine Learning (ML)\ninto the brain mapping process reduces the variable of human error in reading\nsuch maps and increases efficiency. A key area of interest is determining the\ncorrelation between the functional areas of the brain on a voxel (3-dimensional\npixel) wise basis. This leads to determining how a brain is functioning and can\nbe used to detect diseases, disabilities, and sicknesses. As such, random noise\npresents a challenge in consistently determining the actual signals from the\nscan. This paper discusses how an algorithm created by Random Matrix Theory\n(RMT) can be used as a tool for ML, as it detects the correlation of the\nfunctional areas of the brain. Random matrices are simulated to represent the\nvoxel signal intensity strength for each time interval where a stimulus is\npresented in an fMRI scan. Using the Marchenko-Pastur law for Wishart Matrices,\na result of RMT, it was found that no matter what type of noise was added to\nthe random matrices, the observed eigenvalue distribution of the Wishart\nMatrices would converge to the theoretical distribution. This means that RMT is\nrobust and has a high test-re-test reliability. These results further indicate\nthat a strong correlation exists between the eigenvalues, and hence the\nfunctional regions of the brain. Any eigenvalue that differs significantly from\nthose predicted from RMT may indicate the discovery of a new discrete brain\nnetwork.",
        "We report on the crystal field level splitting and magnetic ground state of\nthe Jeff = 1\/2 square lattice antiferromagnets YbBi2ClO4 and YbBi2IO4 using\npowder inelastic neutron scattering (INS) and neutron diffraction measurements.\nBoth compounds exhibit a well-isolated $\\Gamma_{7}$ doublet ground state under\na tetragonal crystal field environment, confirming a robust Jeff = 1\/2 picture\nwith slight XY-type anisotropic character in the g-tensor. Notably, the ground\nstate wave functions closely resemble the $\\Gamma_{7}$ doublet expected in the\nperfect cubic limit, consistent with the nearly cubic ligand configuration of\neight O2- ions surrounding Yb3+. Below TN =0.21 K, YbBi2IO4 exhibits a stripe\nlong-range magnetic order characterized by an ordering wave vector qm = (1\/2,\n0, 0) or its symmetry-equivalent (0, 1\/2, 0), with magnetic moments aligned\nalong qm. The ordered moment is approximately 79 % of the classical prediction,\nsignificantly larger than expected from the isotropic J1-J2 model, suggesting\nthe possible involvement of exchange anisotropy in explaining this observation.\nWe show that symmetry-allowed XXZ and bond-dependent anisotropic exchange terms\nin a square lattice can play a critical role in stabilizing the stripe order\nand suppressing the moment reduction as observed. These findings establish\nYbBi2ClO4 and YbBi2IO4 as unique platforms for exploring rich Jeff = 1\/2\nmagnetism from two less investigated perspectives: (i) on a square lattice and\n(ii) within a (nearly) cubic ligand environment."
      ]
    }
  },
  {
    "id":2412.2062,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Digital health platforms for the elderly? Key adoption and usage barriers and ways to address them",
    "start_abstract":"Digital healthcare platforms (DHPs) represent a relatively new phenomenon that could provide valuable complement to physical primary care \u2013 for example, by reducing costs, improving access healthcare, and allowing patient monitoring. However, such are mainly used today the younger generations, which creates \"digital divide\" between elderly. This article aims identify: i) perceived key barriers inhibit adoption usage of DHPs elderly, ii) what DHP providers can do facilitate increased The draws on qualitative interviews with elderly complementary process data from major Swedish DHP. We find perceives two initial DHPs: negative attitudes technology anxiety one barrier affecting both lack trust. analysis also identifies multiple development suggestions improvement better accommodate needs including application tailored education activities. an integrated framework outlining ways address them. In so doing, we contribute literature mHealth in healthcare.",
    "start_categories":[
      "Healthcare"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Artificial intelligence in elderly healthcare: A scoping review"
      ],
      "abstract":[
        "The ageing population has led to a surge in the adoption of artificial intelligence (AI) technologies in elderly healthcare worldwide. However, in the advancement of AI technologies, there is currently a lack of clarity about the types and roles of AI technologies in elderly healthcare. This scoping review aimed to provide a comprehensive overview of AI technologies in elderly healthcare by exploring the types of AI technologies employed, and identifying their roles in elderly healthcare based on existing studies. A total of 10 databases were searched for this review, from January 1 2000 to July 31 2022. Based on the inclusion criteria, 105 studies were included. The AI devices utilized in elderly healthcare were summarised as robots, exoskeleton devices, intelligent homes, AI-enabled health smart applications and wearables, voice-activated devices, and virtual reality. Five roles of AI technologies were identified: rehabilitation therapists, emotional supporters, social facilitators, supervisors, and cognitive promoters. Results showed that the impact of AI technologies on elderly healthcare is promising and that AI technologies are capable of satisfying the unmet care needs of older adults and demonstrating great potential in its further development in this area. More well-designed randomised controlled trials are needed in the future to validate the roles of AI technologies in elderly healthcare."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Beamforming with Oversampled Time-Modulated Arrays",
        "Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired\n  Training",
        "The Impact of $^{12}$C($\\alpha, \\gamma$)$^{16}$O Reaction on the\n  Presupernova Evolution and Supernova Explodability of Massive Stars",
        "No Evidence of Asymmetrically Enhanced Star Formation in Infalling\n  Galaxies in UNIONS",
        "Hermite numbers and new families of polynomials",
        "Advancing Differentiable Economics: A Neural Network Framework for\n  Revenue-Maximizing Combinatorial Auction Mechanisms",
        "Efficient and inefficient hydrodynamic escape of exo-satellite\n  atmospheres driven by irradiation from their young giant planets",
        "Anisotropic Hybridization Dynamics in the Quasi-One-Dimensional Kondo\n  Lattice CeCo$_2$Ga$_8$ Revealed by Ultrafast Optical Spectroscopy",
        "Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal\n  Control",
        "Three topological phases of the elliptic Ginibre ensembles with a point\n  charge",
        "Bit-depth color recovery via off-the-shelf super-resolution models",
        "Gradient-descent methods for fast quantum state tomography",
        "An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D\n  Brain Images",
        "Evaluating open-source Large Language Models for automated fact-checking",
        "Spatial Transcriptomics Analysis of Spatially Dense Gene Expression\n  Prediction",
        "MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use",
        "Cross-Model Validation of Coronagraphic Exposure Time Calculators for\n  the Habitable Worlds Observatory: A Report from the Exoplanet Science Yield\n  sub-Working Group",
        "The \"double\" square-root law: Evidence for the mechanical origin of\n  market impact using Tokyo Stock Exchange data",
        "Optical stabilization for laser communication satellite systems through\n  proportional-integral-derivative (PID) control and reinforcement learning\n  approach",
        "Quantized crystalline-electromagnetic responses in insulators",
        "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
        "A primer on optimal transport for causal inference with observational\n  data",
        "Group Decision-Making System with Sentiment Analysis of Discussion Chat\n  and Fuzzy Consensus Modeling",
        "Evaluating Visual Explanations of Attention Maps for Transformer-based\n  Medical Imaging",
        "The four-gluon vertex from lattice QCD",
        "Prompt-Aware Controllable Shadow Removal",
        "Quasiparticle Fermi surfaces of niobium and niobium-titanium alloys at\n  high pressure",
        "Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor\n  Graph",
        "Intrinsic momentum transport driven by almost-rational surfaces in\n  tokamak plasmas"
      ],
      "abstract":[
        "The time-modulated array (TMA) is a simple array architecture in which each\nantenna is connected via a multi-throw switch. The switch acts as a modulator\nswitching state faster than the symbol rate. The phase shifting and beamforming\nis achieved by a cyclic shift of the periodical modulating signal across\nantennas. In this paper, the TMA mode of operation is proposed to improve the\nresolution of a conventional phase shifter. The TMAs are analyzed under\nconstrained switching frequency being a small multiple of the symbol rate. The\npresented generic signal model gives insight into the magnitude, phase and\nspacing of the harmonic components generated by the quantized modulating\nsequence. It is shown that the effective phase-shifting resolution can be\nimproved multiplicatively by the oversampling factor ($O$) at the cost of\nintroducing harmonics. Finally, the array tapering with an oversampled\nmodulating signal is proposed. The oversampling provides $O+1$ uniformly\ndistributed tapering amplitudes.",
        "Unpaired training has been verified as one of the most effective paradigms\nfor real scene dehazing by learning from unpaired real-world hazy and clear\nimages. Although numerous studies have been proposed, current methods\ndemonstrate limited generalization for various real scenes due to limited\nfeature representation and insufficient use of real-world prior. Inspired by\nthe strong generative capabilities of diffusion models in producing both hazy\nand clear images, we exploit diffusion prior for real-world image dehazing, and\npropose an unpaired framework named Diff-Dehazer. Specifically, we leverage\ndiffusion prior as bijective mapping learners within the CycleGAN, a classic\nunpaired learning framework. Considering that physical priors contain pivotal\nstatistics information of real-world data, we further excavate real-world\nknowledge by integrating physical priors into our framework. Furthermore, we\nintroduce a new perspective for adequately leveraging the representation\nability of diffusion models by removing degradation in image and text\nmodalities, so as to improve the dehazing effect. Extensive experiments on\nmultiple real-world datasets demonstrate the superior performance of our\nmethod. Our code https:\/\/github.com\/ywxjm\/Diff-Dehazer.",
        "Among the uncertainties of stellar evolution theory, we investigate how the\n$^{12}$C($\\alpha, \\gamma$)$^{16}$O reaction rate affects the evolution of\nmassive stars for the initial masses of $M ({\\rm ZAMS})=$ 13 - 40 M$_\\odot$ and\nthe solar metallicity. We show that the {\\sl explodability} of these stars,\ni.e., which of a neutron star (NS) or a black hole (BH) is formed, is sensitive\nto the strength of convective shell burning of C and O, and thus the mass\nfractions of C ($X$(C)) and O in the shell. For the small $^{12}$C($\\alpha,\n\\gamma$)$^{16}$O reaction rate that yields larger $X$(C), $X$(C) is further\nenhanced by mixing of C from the overlying layer and then C shell burning is\nstrengthened. The extra heating by C shell burning tends to prevent the\ncontraction of outer layers and decrease the {\\sl compactness parameter} at\n$M_r$ = 2.5 M$_\\odot$. This effect leads to the formation of smaller mass cores\nof Si and Fe and steeper density and pressure gradients at the O burning shell\nin the presupernova models. If the pressure gradient there is steeper, the\nmodel is more likely to explode to form a NS rather than a BH. We describe the\npressure gradient against $M_r$ with $V\/U$ and the density drop with $1\/U$,\nwhere $U$ and $V$ are non-dimensional variables to describe the stellar\nstructure. We estimate the critical values of $V\/U$ and $1\/U$ at the O-burning\nshell above which the model is more likely to explode. We conclude that the\nsmaller $^{12}$C($\\alpha, \\gamma$)$^{16}$O reaction rate makes the mass range\nof $M ({\\rm ZAMS})$ that forms a NS larger.",
        "Ram pressure stripping is a well-known environmental quenching mechanism that\nremoves gas from galaxies infalling into groups and clusters. In some extreme\nexamples of ram pressure stripping, galaxies with extended gas tails show\nevidence of enhanced star formation prior to quenching. In this work we use a\nsample of 5277 local satellite galaxies in which a stripped tail of gas has not\nnecessarily been observed, to quantify the strength of ram pressure-enhanced\nstar formation and compare these results to a control sample of 8360 field\ngalaxies. We use u-band imaging from the Ultraviolet-Near Infrared Northern\nSurvey (UNIONS) as a star formation tracer and several metrics to quantify star\nformation asymmetry. We compare these results to environmental properties of\nthe galaxy, such as their time since infall and host halo mass, to constrain\nthe degree of ram pressure enhanced star formation as a function of\nenvironment. We find no significant differences between the satellite and the\nfield samples. We further restrict our sample to galaxies which we most expect\nto be experiencing significant ram pressure but find no strong evidence of\nthese galaxies having systematically enhanced star formation. Finally, we\ninvestigate the properties of the most asymmetric galaxies in our sample and\nagain find no strong evidence of ram pressure-induced star formation\nenhancement. We conclude that any star formation enhancement must be small for\ninfalling galaxies, suggesting that this effect is either uncommon or\nshort-lived.",
        "The operational calculus associated with Hermite numbers has been shown to be\nan effective tool for simplifying the study of special functions. Within this\ncontext, Hermite polynomials have been viewed as Newton binomials, with the\nconsequent possibility of establishing previously unknown properties. In this\narticle, this method is extended to study the lacunary Hermite polynomials and\nobtain novel results concerning their generating functions, recurrence\nrelations, differential equations and certain integral transforms. Furthermore,\nwe extend the idea to combinatorial interpretation of these polynomials,\nbroadening their applicability in mathematical analysis and discrete\nstructures.",
        "Differentiable economics, which uses neural networks as function\napproximators and gradient-based optimization in automated mechanism design\n(AMD), marked a significant breakthrough with the introduction of RegretNet\n\\citep{regretnet_paper}. It combines the flexibility of deep learning with a\nregret-based approach to relax incentive compatibility, allowing for\napproximations of revenue-maximizing auctions. However, applying these\ntechniques to combinatorial auctions (CAs) - where bidders value bundles rather\nthan individual items, capturing item interdependencies - remains a challenge,\nprimarily due to the lack of methodologies that can effectively deal with\ncombinatorial constraints. To tackle this, we propose two architectures: CANet,\na fully connected neural network, and CAFormer, a transformer-based model\ndesigned to learn optimal randomized mechanisms. Unlike existing methods in\ntraditional AMD, our approach is more scalable and free of assumptions about\nthe structures of allowable bundles or bidder valuations. We demonstrate that\nour models match current methods in non-combinatorial settings and set new\nbenchmarks for CAs. Specifically, our models consistently outperform benchmark\nmechanisms derived from heuristic approaches and provide empirical solutions\nwhere analytical results are unavailable. This work bridges the gap in applying\ndifferentiable economics to combinatorial auctions, offering a scalable and\nflexible framework for designing revenue-maximizing mechanisms.",
        "The bolometric radiation from a central body is potentially a powerful driver\nof atmospheric escape from planets or satellites. When heated above their\nequilibrium temperatures those satellites, due to their low surface gravity,\nare be prone to significant atmospheric erosion. Such high temperatures can be\nreached through a known mechanism: a large ratio of the irradiation to\nre-radiation opacities of the atmospheric species. We investigate this\nmechanism for irradiating black-bodies of sub-stellar temperatures and find\nthat specific molecules exist, such as $\\rm NH_3$ and $\\rm CH_4$, which develop\ntemperature inversions under the irradiation of young post-formation giant\nplanets. These non-isothermal temperature profiles lead to escape rates that\ncan significantly exceed isothermal Parker-model escape rates evaluated at the\nsatellite's equilibrium temperature. Our results indicate that exo-satellites\ncan lose most of their atmospheric mass through this mechanism if the cooling\nof the exo-satellite's interior is not too rapid. In all scenarios, we find a\nhierarchical ordering of escape rates of atmospheric species due to thermal\ndecoupling in the upper atmosphere. This thermal decoupling leads to a natural\ndepletion of $\\rm CH_4$ and retention of $\\rm NH_3$ in our models. We find that\ngiant planets with masses above 2$m_{\\rm Jup}$, for cold starts and above\n1$m_{\\rm Jup}$ in hot start scenarios are able to remove the majority of a\nTitan analogue's atmosphere. Hence, finding and characterizing exomoon\natmospheres in hypothetical future surveys can constrain the post-formation\ncooling behaviour of giant planets.",
        "We investigate the ultrafast dynamics of the quasi-one-dimensional Kondo\nlattice CeCo$_2$Ga$_8$ using optical pump-probe spectroscopy. Time-resolved\npump-probe reflectivity measurements reveal a strong anisotropy in the\nphotoinduced response, which is a direct consequence of the material's unique\nelectronic structure. The temperature dependence of the relaxation dynamics\nprovides evidence for the formation of two distinct hybridization gaps that\nappear at different temperatures in the heavy fermion state. A direct gap of\n2$\\Delta_{dir}$ $\\approx$ 50 meV that persists up to $T^\\dag$ $\\approx$ 90 K,\nwell above the coherence temperature $T^*$ $\\approx$ 20 K. We attribute this\nhigher-temperature gap to the hybridization fluctuations. An indirect gap of\n2$\\Delta_{ind}$ $\\approx$ 10 meV opens closer to $T^*$, signifying the\ndevelopment of long-range coherence in the heavy fermion state. Furthermore, we\nfind that the hybridization gap can be suppressed with increasing pump fluence,\nindicating a delicate interplay between photoexcitation and the coherent heavy\nfermion state. Our results provide insights into the interplay of Kondo physics\nand low dimensionality in CeCo$_2$Ga$_8$, and establish ultrafast optical\nspectroscopy as a sensitive probe of anisotropic hybridization in heavy fermion\nmaterials.",
        "Control policies that can achieve high task performance and satisfy safety\nconstraints are desirable for any system, including multi-agent systems (MAS).\nOne promising technique for ensuring the safety of MAS is distributed control\nbarrier functions (CBF). However, it is difficult to design distributed\nCBF-based policies for MAS that can tackle unknown discrete-time dynamics,\npartial observability, changing neighborhoods, and input constraints,\nespecially when a distributed high-performance nominal policy that can achieve\nthe task is unavailable. To tackle these challenges, we propose DGPPO, a new\nframework that simultaneously learns both a discrete graph CBF which handles\nneighborhood changes and input constraints, and a distributed high-performance\nsafe policy for MAS with unknown discrete-time dynamics. We empirically\nvalidate our claims on a suite of multi-agent tasks spanning three different\nsimulation engines. The results suggest that, compared with existing methods,\nour DGPPO framework obtains policies that achieve high task performance\n(matching baselines that ignore the safety constraints), and high safety rates\n(matching the most conservative baselines), with a constant set of\nhyperparameters across all environments.",
        "We consider the complex and symplectic elliptic Ginibre matrices of size\n$(c+1)N \\times (c+1)N$, conditioned to have a deterministic eigenvalue at $ p\n\\in \\mathbb{R} $ with multiplicity $ c N $. We show that their limiting\nspectrum is either simply connected, doubly connected, or composed of two\ndisjoint simply connected components. Moreover, denoting by $\\tau \\in [0,1]$\nthe non-Hermiticity parameter, we explicitly characterise the regions in the\nparameter space $ (p, c, \\tau) $ where each topological type emerges. For cases\nwhere the droplet is either simply or doubly connected, we provide an explicit\ndescription of the limiting spectrum and the corresponding electrostatic\nenergies. As an application, we derive the asymptotic behaviour of the moments\nof the characteristic polynomial for elliptic Ginibre matrices in the\nexponentially varying regime.",
        "Advancements in imaging technology have enabled hardware to support 10 to 16\nbits per channel, facilitating precise manipulation in applications like image\nediting and video processing. While deep neural networks promise to recover\nhigh bit-depth representations, existing methods often rely on scale-invariant\nimage information, limiting performance in certain scenarios. In this paper, we\nintroduce a novel approach that integrates a super-resolution architecture to\nextract detailed a priori information from images. By leveraging interpolated\ndata generated during the super-resolution process, our method achieves\npixel-level recovery of fine-grained color details. Additionally, we\ndemonstrate that spatial features learned through the super-resolution process\nsignificantly contribute to the recovery of detailed color depth information.\nExperiments on benchmark datasets demonstrate that our approach outperforms\nstate-of-the-art methods, highlighting the potential of super-resolution for\nhigh-fidelity color restoration.",
        "Quantum state tomography (QST) is a widely employed technique for\ncharacterizing the state of a quantum system. However, it is plagued by two\nfundamental challenges: computational and experimental complexity grows\nexponentially with the number of qubits, rendering experimental implementation\nand data post-processing arduous even for moderately sized systems. Here, we\nintroduce gradient-descent (GD) algorithms for the post-processing step of QST\nin discrete- and continuous-variable systems. To ensure physically valid state\nreconstruction at each iteration step of the algorithm, we use various\ndensity-matrix parameterizations: Cholesky decomposition, Stiefel manifold, and\nprojective normalization. These parameterizations have the added benefit of\nenabling a rank-controlled ansatz, which simplifies reconstruction when there\nis prior information about the system. We benchmark the performance of our\nGD-QST techniques against state-of-the-art methods, including constrained\nconvex optimization, conditional generative adversarial networks, and iterative\nmaximum likelihood estimation. Our comparison focuses on time complexity,\niteration counts, data requirements, state rank, and robustness against noise.\nWe find that rank-controlled ansatzes in our stochastic mini-batch GD-QST\nalgorithms effectively handle noisy and incomplete data sets, yielding\nsignificantly higher reconstruction fidelity than other methods. Simulations\nachieving full-rank seven-qubit QST in under three minutes on a standard\nlaptop, with 18 GB of RAM and no dedicated GPU, highlight that GD-QST is\ncomputationally more efficient and outperforms other techniques in most\nscenarios, offering a promising avenue for characterizing noisy\nintermediate-scale quantum devices. Our Python code for GD-QST algorithms is\npublicly available at https:\/\/github.com\/mstorresh\/GD-QST.",
        "We developed a generative patch based 3D image registration model that can\nregister very high resolution images obtained from a biochemical process name\ntissue clearing. Tissue clearing process removes lipids and fats from the\ntissue and make the tissue transparent. When cleared tissues are imaged with\nLight-sheet fluorescent microscopy, the resulting images give a clear window to\nthe cellular activities and dynamics inside the tissue.Thus the images obtained\nare very rich with cellular information and hence their resolution is extremely\nhigh (eg .2560x2160x676). Analyzing images with such high resolution is a\ndifficult task for any image analysis pipeline.Image registration is a common\nstep in image analysis pipeline when comparison between images are required.\nTraditional image registration methods fail to register images with such\nextant. In this paper we addressed this very high resolution image registration\nissue by proposing a patch-based generative network named InvGAN. Our proposed\nnetwork can register very high resolution tissue cleared images. The tissue\ncleared dataset used in this paper are obtained from a tissue clearing protocol\nnamed CUBIC. We compared our method both with traditional and deep-learning\nbased registration methods.Two different versions of CUBIC dataset are used,\nrepresenting two different resolutions 25% and 100% respectively. Experiments\non two different resolutions clearly show the impact of resolution on the\nregistration quality. At 25% resolution, our method achieves comparable\nregistration accuracy with very short time (7 minutes approximately). At 100%\nresolution, most of the traditional registration methods fail except Elastix\nregistration tool.Elastix takes 28 hours to register where proposed InvGAN\ntakes only 10 minutes.",
        "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers.",
        "Spatial transcriptomics (ST) measures gene expression at fine-grained spatial\nresolution, offering insights into tissue molecular landscapes. Previous\nmethods for spatial gene expression prediction usually crop spots of interest\nfrom pathology tissue slide images, and learn a model that maps each spot to a\nsingle gene expression profile. However, it fundamentally loses spatial\nresolution of gene expression: 1) each spot often contains multiple cells with\ndistinct gene expression; 2) spots are cropped at fixed resolutions, limiting\nthe ability to predict gene expression at varying spatial scales. To address\nthese limitations, this paper presents PixNet, a dense prediction network\ncapable of predicting spatially resolved gene expression across spots of\nvarying sizes and scales directly from pathology images. Different from\nprevious methods that map individual spots to gene expression values, we\ngenerate a dense continuous gene expression map from the pathology image, and\naggregate values within spots of interest to predict the gene expression. Our\nPixNet outperforms state-of-the-art methods on 3 common ST datasets, while\nshowing superior performance in predicting gene expression across multiple\nspatial scales. The source code will be publicly available.",
        "When a human requests an LLM to complete a coding task using functionality\nfrom a large code repository, how do we provide context from the repo to the\nLLM? One approach is to add the entire repo to the LLM's context window.\nHowever, most tasks involve only fraction of symbols from a repo, longer\ncontexts are detrimental to the LLM's reasoning abilities, and context windows\nare not unlimited. Alternatively, we could emulate the human ability to\nnavigate a large repo, pick out the right functionality, and form a plan to\nsolve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan\nSearch), an approach to search for plans that decompose a user request into\nnatural language steps grounded in the codebase. MutaGReP performs neural tree\nsearch in plan space, exploring by mutating plans and using a symbol retriever\nfor grounding. On the challenging LongCodeArena benchmark, our plans use less\nthan 5% of the 128K context window for GPT-4o but rival the coding performance\nof GPT-4o with a context window filled with the repo. Plans produced by\nMutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o\nwith full repo context and enable progress on the hardest LongCodeArena tasks.\nProject page: zaidkhan.me\/MutaGReP",
        "Estimating the exoplanet scientific productivity of the Habitable Worlds\nObservatory requires estimating science exposure times. From exoplanet yields\nto spectral retrievals, exposure times are at the heart of our understanding of\nthe capabilities of this future mission. As such, ensuring accuracy and\nconsistency between different exposure time calculators (ETCs) is critical. We\nsummarize the efforts of the Exoplanet Science Yield sub-Working Group's ETC\nCalibration Task Group, which conducted a calibration study from March 4 to\nJune 30 of 2024. We compare three commonly-used coronagraphic exposure time\ncalculators. We find that the ETCs use a broad variety of differing methods,\nassumptions, and inputs that produce variation in the final exposure times at\nthe ~60% level. The causes for the disagreement have largely been identified,\nflagged for further development efforts, and in some cases retired since the\nconclusion of this effort. We expect that addressing the flagged efforts will\nbring the ETCs to within better than ~30% agreement.",
        "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.",
        "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https:\/\/github.com\/ccccai239\/PixelRIST.",
        "The theory of optimal transportation has developed into a powerful and\nelegant framework for comparing probability distributions, with wide-ranging\napplications in all areas of science. The fundamental idea of analyzing\nprobabilities by comparing their underlying state space naturally aligns with\nthe core idea of causal inference, where understanding and quantifying\ncounterfactual states is paramount. Despite this intuitive connection, explicit\nresearch at the intersection of optimal transport and causal inference is only\nbeginning to develop. Yet, many foundational models in causal inference have\nimplicitly relied on optimal transport principles for decades, without\nrecognizing the underlying connection. Therefore, the goal of this review is to\noffer an introduction to the surprisingly deep existing connections between\noptimal transport and the identification of causal effects with observational\ndata -- where optimal transport is not just a set of potential tools, but\nactually builds the foundation of model assumptions. As a result, this review\nis intended to unify the language and notation between different areas of\nstatistics, mathematics, and econometrics, by pointing out these existing\nconnections, and to explore novel problems and directions for future work in\nboth areas derived from this realization.",
        "Group Decision-Making (GDM) plays a crucial role in various real-life\nscenarios where individuals express their opinions in natural language rather\nthan structured numerical values. Traditional GDM approaches often overlook the\nsubjectivity and ambiguity present in human discussions, making it challenging\nto achieve a fair and consensus-driven decision. This paper proposes a fuzzy\nconsensus-based group decision-making system that integrates sentiment and\nemotion analysis to extract preference values from textual inputs. The proposed\nframework combines explicit voting preferences with sentiment scores derived\nfrom chat discussions, which are then processed using a Fuzzy Inference System\n(FIS) to compute a total preference score for each alternative and determine\nthe top-ranked option. To ensure fairness in group decision-making, we\nintroduce a fuzzy logic-based consensus measurement model that evaluates\nparticipants' agreement and confidence levels to assess overall feedback. To\nillustrate the effectiveness of our approach, we apply the methodology to a\nrestaurant selection scenario, where a group of individuals must decide on a\ndining option based on brief chat discussions. The results demonstrate that the\nfuzzy consensus mechanism successfully aggregates individual preferences and\nensures a balanced outcome that accurately reflects group sentiment.",
        "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
        "The four-gluon one-particle irreducible Green function contributes to various\nquantities with phenomenological relevance. An example where the four-gluon\nplays a role is the determination of the gluon propagator, a basic building\nblock for QCD, using continuum methods. This four leg Green function is poorly\nknown and we are only starting to grasp its non-perturbative structure. Here,\nwe report on the computation of the one-particle irreducible four-gluon Green\nfunction, in the Landau gauge, with lattice simulations. Besides stating the\nproblems associated with the computation, several form factors that\ncharacterise this Green function are measured.",
        "Shadow removal aims to restore the image content in shadowed regions. While\ndeep learning-based methods have shown promising results, they still face key\nchallenges: 1) uncontrolled removal of all shadows, or 2) controllable removal\nbut heavily relies on precise shadow region masks. To address these issues, we\nintroduce a novel paradigm: prompt-aware controllable shadow removal. Unlike\nexisting approaches, our paradigm allows for targeted shadow removal from\nspecific subjects based on user prompts (e.g., dots, lines, or subject masks).\nThis approach eliminates the need for shadow annotations and offers flexible,\nuser-controlled shadow removal. Specifically, we propose an end-to-end\nlearnable model, the Prompt-Aware Controllable Shadow Removal Network\n(PACSRNet). PACSRNet consists of two key modules: a prompt-aware module that\ngenerates shadow masks for the specified subject based on the user prompt, and\na shadow removal module that uses the shadow prior from the first module to\nrestore the content in the shadowed regions. Additionally, we enhance the\nshadow removal module by incorporating feature information from the\nprompt-aware module through a linear operation, providing prompt-guided support\nfor shadow removal. Recognizing that existing shadow removal datasets lack\ndiverse user prompts, we contribute a new dataset specifically designed for\nprompt-based controllable shadow removal. Extensive experimental results\ndemonstrate the effectiveness and superiority of PACSRNet.",
        "The electronic structure of pure niobium and the niobium-titanium alloy\nNb$_{0.44}$Ti$_{0.56}$ in the bcc-phase at pressures up to $250$ GPa is\ninvestigated, to reveal possible factors conducing toward the robust\nsuperconductivity reported for Ti-doped niobium upon a considerable volume\nreduction. We model the structural disorder using the coherent potential\napproximation, and the electronic correlations are taken into account using\ndynamical mean-field theory. At high pressure, a significant change in the\ntopology of the Fermi surface is observed, while electronic correlations weaken\nwith increasing pressure. Thus, the normal state of Nb$_{0.44}$Ti$_{0.56}$ is\nfound to be a Fermi liquid with a well-defined Fermi surface, and well-defined\nquasiparticles near it. The systematic study of the impact of disorder upon the\nFermi surface at such ultra high pressures allows notable insights into the\nnature of the electronic states near the Fermi level, i.e., within the energy\nscale relevant for superconducting pairing. Furthermore, our results clearly\nindicate the necessity of further experimental Fermi surface explorations.",
        "Event prediction tasks often handle spatio-temporal data distributed in a\nlarge spatial area. Different regions in the area exhibit different\ncharacteristics while having latent correlations. This spatial heterogeneity\nand correlations greatly affect the spatio-temporal distributions of event\noccurrences, which has not been addressed by state-of-the-art models. Learning\nspatial dependencies of events in a continuous space is challenging due to its\nfine granularity and a lack of prior knowledge. In this work, we propose a\nnovel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event\nprediction. It adopts an encoder-decoder architecture that jointly models the\nstate dynamics of spatially localized regions using neural Ordinary\nDifferential Equations (ODEs). The state evolution is built on the foundation\nof a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial\ndependencies. By adaptively localizing the anchor nodes in the space and\njointly constructing the correlation edges between them, the SAAG enhances the\nmodel's ability of learning complex spatial event patterns. The proposed GSTPP\nmodel greatly improves the accuracy of fine-grained event prediction. Extensive\nexperimental results show that our method greatly improves the prediction\naccuracy over existing spatio-temporal event prediction approaches.",
        "We demonstrate that a symmetry of the local gyrokinetic model is broken when\nthe safety factor q is almost (but not exactly) a rational number and magnetic\nshear is $\\hat{s} \\approx 0$. Tokamaks with such a q profile will spontaneously\nrotate due to turbulent momentum transport. Nonlinear gyrokinetic simulations\nindicate this mechanism is significantly stronger than all other drives of\nintrinsic rotation. It also generates intrinsic electric current that pulls q\ntowards rational values, potentially aiding non-inductive current drive. This\nis likely important in the triggering of internal transport barriers."
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window",
    "start_abstract":"SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Fast fit-free analysis of fluorescence lifetime imaging via deep learning"
      ],
      "abstract":[
        "Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Testing the limits of ITkPixV2: the ATLAS inner tracker pixel detector\n  readout chip",
        "Analysis of pitchfork bifurcations and symmetry breaking in the elliptic\n  restricted three-body problem",
        "Range-Only Localization System for Small-Scale Flapping-Wing Robots",
        "Point-LN: A Lightweight Framework for Efficient Point Cloud\n  Classification Using Non-Parametric Positional Encoding",
        "A Novel Interpretation of the Radon Transform's Ray- and Pixel-Driven\n  Discretizations under Balanced Resolutions",
        "ADAPT: An Autonomous Forklift for Construction Site Operation",
        "Perimeter length of the convex hull of Brownian motion in the hyperbolic\n  plane",
        "Post-disaster building indoor damage and survivor detection using\n  autonomous path planning and deep learning with unmanned aerial vehicles",
        "On families of strongly divisible modules of rank 2",
        "On-demand storage and retrieval of single photons from a semiconductor\n  quantum dot in a room-temperature atomic vapor memory",
        "High Energy Jet Emission from GRS 1758-258 & 1E 1740.7-2942 with\n  INTEGRAL?",
        "Emotional Multifaceted Feedback on AI Tool Use in EFL Learning\n  Initiation: Chain-Mediated Effects of Motivation and Metacognitive Strategies\n  in an Optimized TAM Model",
        "Derivation of the Planck Units Based in a Membranes Model",
        "Nonparametric Smoothing of Directional and Axial Data",
        "Carbonic anhydrase II simulated with a universal neural network\n  potential",
        "MADS: Multi-Attribute Document Supervision for Zero-Shot Image\n  Classification",
        "Enhanced Tuberculosis Bacilli Detection using Attention-Residual U-Net\n  and Ensemble Classification",
        "Iterative Counterfactual Data Augmentation",
        "Investigating the Effect of Relaxation Time on Richtmyer-Meshkov\n  Instability under Reshock Impact: A Two-Component Discrete Boltzmann Method\n  Study",
        "Planar tropical caustics: trivalency and convexity",
        "Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X\n  Collaboration",
        "Axial current as the origin of quantum intrinsic orbital angular\n  momentum",
        "Expected Return Symmetries",
        "Counterexamples for T\\\"urkelli's Modification on Malle's Conjecture",
        "Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation",
        "Global well-posedness of Vlasov-Poisson-Boltzmann equations with neutral\n  initial data and small relative entropy",
        "Improving LLM-as-a-Judge Inference with the Judgment Distribution",
        "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation",
        "Ion-Trap Chip Architecture Optimized for Implementation of Quantum\n  Error-Correcting Code"
      ],
      "abstract":[
        "The ITkPixV2 chip is the final production readout chip for the ATLAS Phase 2\nInner Tracker (ITk) upgrade at the upcoming High-Luminosity LHC (HL-LHC). Due\nto the extraordinarily high peak luminosity at the HL-LHC of $5 \\times 10^{34}$\ncm$^{-1}$s$^{-1}$, ITkPixV2 must meet significant increases in nearly all\ndesign requirements, including a 10x increase in trigger rate, a 7.5x increase\nin hit rate, a 3x increase in radiation tolerance, and a 12.5x decrease in\npixel current draw per unit area, all while maintaining a similar power per\nunit area as present pixel detectors. Here we present the first measurements of\nthe ITkPixV2 chip operated at the limits of the full chip design requirements,\nincluding in particular a measurement of the activity-induced current of the\nchip as a function of increasing hit rate.",
        "A unified framework is proposed to quantitatively characterize pitchfork\nbifurcations and associated symmetry breaking in the elliptic restricted\nthree-body problem (ERTBP). It is known that planar\/vertical Lyapunov orbits\nand Lissajous orbits near the collinear libration points undergo pitchfork\nbifurcations with varying orbital energy. These bifurcations induce symmetry\nbreaking, generating bifurcated families including halo\/quasi-halo orbits,\naxial\/quasi-axial orbits, and their corresponding invariant manifolds.\nTraditional semi-analytical methods for constructing halo orbits, based on\nresonant bifurcation mechanisms, have obstacles in fully exploiting the\nintrinsic symmetry breaking characteristics in pitchfork bifurcations. In this\npaper, we propose a unified trigonometric series-based framework to analyze\nthese bifurcated families from the perspective of coupling-induced bifurcation\nmechanisms. By introducing a coupling coefficient and various bifurcation\nequations into the ERTBP, different symmetry breaking is achieved when the\ncoupling coefficient is non-zero. This unified semi-analytical framework\ncaptures bifurcations of both periodic\/quasi-periodic and transit\/non-transit\norbits. Furthermore, it reveals that pitchfork bifurcation solutions in the\nERTBP fundamentally depend solely on the orbital eccentricity and three\namplitude parameters of the system's degrees of freedom, governing both the\nelliptic direction and the hyperbolic one.",
        "The design of localization systems for small-scale flapping-wing aerial\nrobots faces relevant challenges caused by the limited payload and onboard\ncomputational resources. This paper presents an ultra-wideband localization\nsystem particularly designed for small-scale flapping-wing robots. The solution\nrelies on custom 5 grams ultra-wideband sensors and provides robust, very\nefficient (in terms of both computation and energy consumption), and accurate\n(mean error of 0.28 meters) 3D position estimation. We validate our system\nusing a Flapper Nimble+ flapping-wing robot.",
        "We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.",
        "Tomographic investigations are a central tool in medical applications,\nallowing doctors to image the interior of patients. The corresponding\nmeasurement process is commonly modeled by the Radon transform. In practice,\nthe solution of the tomographic problem requires discretization of the Radon\ntransform and its adjoint (called the backprojection). There are various\ndiscretization schemes; often structured around three discretization\nparameters: spatial-, detector-, and angular resolutions. The most widespread\napproach uses the ray-driven Radon transform and the pixel-driven\nbackprojection in a balanced resolution setting, i.e., the spatial resolution\nroughly equals the detector resolution. The use of these particular\ndiscretization approaches is based on anecdotal reports of their approximation\nperformance, but there is little rigorous analysis of these methods'\napproximation errors. This paper presents a novel interpretation of ray-driven\nand pixel-driven methods as convolutional discretizations, illustrating that\nfrom an abstract perspective these methods are similar. Moreover, we announce\nstatements concerning the convergence of the ray-driven Radon transform and the\npixel-driven backprojection under balanced resolutions. Our considerations are\nsupported by numerical experiments highlighting aspects of the discussed\nmethods.",
        "Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of the Autonomous Dynamic All-terrain Pallet\nTransporter (ADAPT), a fully autonomous off-road forklift designed for\nconstruction environments. Unlike structured warehouse settings, construction\nsites pose significant challenges, including dynamic obstacles, unstructured\nterrain, and varying weather conditions. To address these challenges, our\nsystem integrates AI-driven perception techniques with traditional approaches\nfor decision making, planning, and control, enabling reliable operation in\ncomplex environments. We validate the system through extensive real-world\ntesting, comparing its long-term performance against an experienced human\noperator across various weather conditions. We also provide a comprehensive\nanalysis of challenges and key lessons learned, contributing to the advancement\nof autonomous heavy machinery. Our findings demonstrate that autonomous outdoor\nforklifts can operate near human-level performance, offering a viable path\ntoward safer and more efficient construction logistics.",
        "We relate the expected hyperbolic length of the perimeter of the convex hull\nof the trajectory of Brownian motion in the hyperbolic plane to an expectation\nof a certain exponential functional of a one-dimensional real-valued Brownian\nmotion, and hence derive small- and large-time asymptotics for the expected\nhyperbolic perimeter. In contrast to the case of Euclidean Brownian motion with\nnon-zero drift, the large-time asymptotics are a factor of two greater than the\nlower bound implied by the fact that the convex hull includes the hyperbolic\nline segment from the origin to the endpoint of the hyperbolic Brownian motion.\nWe also obtain an exact expression for the expected perimeter length after an\nindependent exponential random time.",
        "Rapid response to natural disasters such as earthquakes is a crucial element\nin ensuring the safety of civil infrastructures and minimizing casualties.\nTraditional manual inspection is labour-intensive, time-consuming, and can be\ndangerous for inspectors and rescue workers. This paper proposed an autonomous\ninspection approach for structural damage inspection and survivor detection in\nthe post-disaster building indoor scenario, which incorporates an autonomous\nnavigation method, deep learning-based damage and survivor detection method,\nand a customized low-cost micro aerial vehicle (MAV) with onboard sensors.\nExperimental studies in a pseudo-post-disaster office building have shown the\nproposed methodology can achieve high accuracy in structural damage inspection\nand survivor detection. Overall, the proposed inspection approach shows great\npotential to improve the efficiency of existing manual post-disaster building\ninspection.",
        "Let $p$ be an odd prime, and $\\mathbf{Q}_{p^f}$ the unramified extension of\n$\\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of\nconstructing strongly divisible modules for $2$-dimensional semi-stable\nnon-crystalline representations of\n$\\mathrm{Gal}(\\overline{\\mathbf{Q}}_p\/\\mathbf{Q}_{p^f})$ with Hodge--Tate\nweights in the Fontaine--Laffaille range to solving systems of linear equations\nand inequalities. We also determine the Breuil modules corresponding to the\nmod-$p$ reduction of the strongly divisible modules. We expect our method to\nproduce at least one Galois-stable lattice in each such representation for\ngeneral $f$. Moreover, when the mod-$p$ reduction is an extension of distinct\ncharacters, we further expect our method to provide the two non-homothetic\nlattices. As applications, we show that our approach recovers previously known\nresults for $f=1$ and determine the mod-$p$ reduction of the semi-stable\nrepresentations with some small Hodge--Tate weights when $f=2$.",
        "Interfacing light from solid-state single-photon sources with scalable and\nrobust room-temperature quantum memories has been a long-standing challenge in\nphotonic quantum information technologies due to inherent noise processes and\ntime-scale mismatches between the operating conditions of solid-state and\natomic systems. Here, we demonstrate on-demand storage and retrieval of single\nphotons from a semiconductor quantum dot device in a room-temperature atomic\nvapor memory. A deterministically fabricated InGaAs quantum dot light source\nemits single photons at the wavelength of the cesium D1 line at 895\\,nm which\nexhibit an inhomogeneously broadened linewidth of 5.1(7)\\,GHz and are\nsubsequently stored in a low-noise ladder-type cesium vapor memory. We show\ncontrol over the interaction between the single photons and the atomic vapor,\nallowing for variable retrieval times of up to 19.8(3)\\,ns at an internal\nefficiency of $\\eta_\\mathrm{int}=0.6(1)\\%$. Our results significantly expand\nthe application space of both room-temperature vapor memories and semiconductor\nquantum dots in future quantum network architectures.",
        "GRS 1758-258 and 1E 1740.7-2942 are two long-known persistent black hole\nbinaries in the Galactic Center region. Using INTEGRAL's extensive monitoring\nof the Galactic Center and Bulge, we studied their temporal and spectral\nevolutions in the 30-610 keV energy range from March 2003 through April 2022\nwith the IBIS\/ISGRI gamma-ray telescope. Our analyses found that the sources\ntypically had Comptonized spectra, though not always with the same parameters.\nThe spectral states with more than 8 Ms of observation time show deviations\nfrom a Comptonized spectrum above ~200 keV or a \"hard tail\" that extends up to\nat least 600 keV. The origin of this component remains debated with the most\npopular scenarios being synchrotron emission from the jet or Comptonization in\na hybrid thermal\/non-thermal plasma. Anyway, the GRS 1758-258 and 1E\n1740.7-2942 spectra are acceptably described by CompTT+po (jet) and Eqpair\n(hybrid Comptonization) scenarios. To differentiate between the two scenarios,\nwe calculated the Spearman correlation coefficient comparing 30-50 keV count\nrates with those in higher energy bands (50-100, 100-300, and 300-600 keV). The\ncount rates below 300 keV are strongly correlated, indicating those photons\narise from the same physical process. Above 300 keV the count rates are either\nanti-correlated or not correlated with the 30-50 keV count rates for GRS\n1758-258, which suggests that the photons originate from a different physical\nprocess. For 1E 1740.7-2942, the level of correlation is unclear due to scatter\nin the data points. However, the 300-600 keV count rates are consistent with a\nconstant value. This disfavors the hybrid Comptonization scenario for both\nsources.",
        "This study specifically investigates the initiation phase of EFL learners'\nengagement with AI tools, focusing on how technology acceptance constructs\nperceived usefulness (PU), perceived ease of use (PEOU), and perceived\nself-efficacy (PSE) influence learning resilience. Drawing on an optimized\nTechnology Acceptance Model (TAM) and integrating constructs from positive\npsychology, the study examines the chain-mediated effects of learning\nmotivation (LM) and metacognitive strategies (MS) on resilience outcomes,\noperationalized through optimism (OP), psychological resilience (PR), and\ngrowth mindset (GM). A survey of first-year English majors (N = 730) was\nconducted, and structural equation modeling was employed to analyze the data.\nThe findings indicate that favorable perceptions of AI tools are significantly\nassociated with enhanced LM and MS, which in turn positively impact resilience\nmeasures. These results suggest that the interplay between technology\nacceptance and internal regulatory processes is vital in shaping EFL learners'\nearly experiences with AI-assisted learning. Practical implications for\neducators and researchers are discussed, with an emphasis on promoting\nuser-friendly and effective AI environments to support the development of\nadaptive learning behaviors.",
        "In this study, the Planck units (mass, time and length) have only been\nderived, explained and attributed a physical meaning when they were deduced\nbased on the concept of interacting membranes (membranes instead of strings of\nstring theory). For this purpose, a set of five assumptions were proposed: (a)\nthe existence of the interacting membranes; (b) the curvatures of the membranes\noscillate according to the classical wave equation; (c) the spatial period of\nthe wave that arise when the membranes oscillate is given by $\\lambda =\n{\\xi}{\\pi}\/k$; (d) the membranes oscillate with wavelength given by de Broglie\nrelation and (e) $x=ct$ holds. The parameter $\\xi$ determines the period of\noscillation of the given membranes. In deriving the Planck units in this work,\n$\\xi$ must take the value 2 and determines a period 2$\\pi$, closely to minimum\nvalue 1 or to fundamental period $\\pi$, respectively. In this context, Planck\nunits must be fundamental. Moreover, the parameter $\\xi$ was reported as a\nunification parameter between the formulas for the Coulomb$^{\\prime}$s law and\nNewton$^{\\prime}$s law of universal gravitation linking the forces of\nmicroworld and macroworld. Depending on the value $\\xi$ takes, one force or\nanother will be had. It is also shown that the potential $V = hc\/{\\xi}{\\pi}x$\ndeduced from the above assumptions and which contributes to deduce the Planck\nunits, can be derived from Yukawa$^{\\prime}$s equation. Hence, the present work\nwould be contributing to theoretical physics, since at the Planck scale\npredictions of some theories like Standard Model, quantum field theory and\ngeneral relativity are not expected to be valid.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "The carbonic anhydrase II enzyme (CA II) is one of the most significant\nenzymes in nature, reversibly converting CO$_2$ to bicarbonate at a remarkable\nrate. The precise mechanism it uses to achieve this rapid turnover remains\nunclear due to our inability to directly observe or simulate the full process\ndynamically. Here, we use a recently developed universal neural network\npotential (Orb) to simulate the active site of CA II. We reproduce several\nknown features of the reaction mechanism, including the proton relay that\nconducts protons out of the active site to the His64 residue. Additionally, we\nobserve a new reaction pathway where CO$_2$ reacts with a water molecule in the\nactive site, which donates a proton to the zinc-bound hydroxide. This differs\nfrom the established mechanism where CO$_2$ directly reacts with hydroxide.\nExisting experimental data and independent quantum chemistry calculations are\nused to support the plausibility of this new mechanism. This demonstrates the\npotential of Orb to efficiently generate novel insights into important\nmolecular scale processes that can potentially be harnessed to improve CO$_2$\ncapture technologies and drug design.",
        "Zero-shot learning (ZSL) aims to train a model on seen classes and recognize\nunseen classes by knowledge transfer through shared auxiliary information.\nRecent studies reveal that documents from encyclopedias provide helpful\nauxiliary information. However, existing methods align noisy documents,\nentangled in visual and non-visual descriptions, with image regions, yet solely\ndepend on implicit learning. These models fail to filter non-visual noise\nreliably and incorrectly align non-visual words to image regions, which is\nharmful to knowledge transfer. In this work, we propose a novel multi-attribute\ndocument supervision framework to remove noises at both document collection and\nmodel learning stages. With the help of large language models, we introduce a\nnovel prompt algorithm that automatically removes non-visual descriptions and\nenriches less-described documents in multiple attribute views. Our proposed\nmodel, MADS, extracts multi-view transferable knowledge with information\ndecoupling and semantic interactions for semantic alignment at local and global\nlevels. Besides, we introduce a model-agnostic focus loss to explicitly enhance\nattention to visually discriminative information during training, also\nimproving existing methods without additional parameters. With comparable\ncomputation costs, MADS consistently outperforms the SOTA by 7.2% and 8.2% on\naverage in three benchmarks for document-based ZSL and GZSL settings,\nrespectively. Moreover, we qualitatively offer interpretable predictions from\nmultiple attribute views.",
        "Tuberculosis (TB), caused by Mycobacterium tuberculosis, remains a critical\nglobal health issue, necessitating timely diagnosis and treatment. Current\nmethods for detecting tuberculosis bacilli from bright field microscopic sputum\nsmear images suffer from low automation, inadequate segmentation performance,\nand limited classification accuracy. This paper proposes an efficient hybrid\napproach that combines deep learning for segmentation and an ensemble model for\nclassification. An enhanced U-Net model incorporating attention blocks and\nresidual connections is introduced to precisely segment microscopic sputum\nsmear images, facilitating the extraction of Regions of Interest (ROIs). These\nROIs are subsequently classified using an ensemble classifier comprising\nSupport Vector Machine (SVM), Random Forest, and Extreme Gradient Boost\n(XGBoost), resulting in an accurate identification of bacilli within the\nimages. Experiments conducted on a newly created dataset, along with public\ndatasets, demonstrate that the proposed model achieves superior segmentation\nperformance, higher classification accuracy, and enhanced automation compared\nto existing methods.",
        "Counterfactual data augmentation (CDA) is a method for controlling\ninformation or biases in training datasets by generating a complementary\ndataset with typically opposing biases. Prior work often either relies on\nhand-crafted rules or algorithmic CDA methods which can leave unwanted\ninformation in the augmented dataset. In this work, we show iterative CDA\n(ICDA) with initial, high-noise interventions can converge to a state with\nsignificantly lower noise. Our ICDA procedure produces a dataset where one\ntarget signal in the training dataset maintains high mutual information with a\ncorresponding label and the information of spurious signals are reduced. We\nshow training on the augmented datasets produces rationales on documents that\nbetter align with human annotation. Our experiments include six human produced\ndatasets and two large-language model generated datasets.",
        "The Richtmyer-Meshkov (RM) instability plays an important role in various\nnatural and engineering fields, such as inertial confinement fusion. In this\nwork, the effect of relaxation time on the RM instability under reshock impact\nis investigated by using a two-component discrete Boltzmann method. The\nhydrodynamic and thermodynamic characteristics of the fluid system are\ncomprehensively analyzed from the perspectives of the density gradient,\nvorticity, kinetic energy, mixing degree, mixing width, and non-equilibrium\nintensity. Simulation results indicate that for larger relaxation time, the\ndiffusion and dissipation are enhanced, the physical gradients decrease, and\nthe growth of the interface is suppressed. Furthermore, the non-equilibrium\nmanifestations show complex patterns, driven by the competitive physical\nmechanisms of the diffusion, dissipation, shock wave, rarefaction wave,\ntransverse wave, and fluid instabilities. These findings provide valuable\ninsights into the fundamental mechanism of compressible fluid flows.",
        "Tropical caustic of a convex domain on the plane is a canonically associated\ntropical analytic curve inside the domain. In this note we give a graphical\nproof for the classification of its intermediate vertices, implying in\nparticular that they are always trivalent. Apart from that we explain how\nvarious known examples of tropical caustics are constructed and discuss the\npossibility of relaxing the convexity condition for the domain.",
        "Vehicle-to-everything (V2X) collaborative perception has emerged as a\npromising solution to address the limitations of single-vehicle perception\nsystems. However, existing V2X datasets are limited in scope, diversity, and\nquality. To address these gaps, we present Mixed Signals, a comprehensive V2X\ndataset featuring 45.1k point clouds and 240.6k bounding boxes collected from\nthree connected autonomous vehicles (CAVs) equipped with two different types of\nLiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides\nprecisely aligned point clouds and bounding box annotations across 10 classes,\nensuring reliable data for perception training. We provide detailed statistical\nanalysis on the quality of our dataset and extensively benchmark existing V2X\nmethods on it. Mixed Signals V2X Dataset is one of the highest quality,\nlarge-scale datasets publicly available for V2X perception research. Details on\nthe website https:\/\/mixedsignalsdataset.cs.cornell.edu\/.",
        "We show that it is impossible to experimentally observe the quantum intrinsic\norbital angular momentum (IOAM) effect without its axial current. Broadly\nspeaking, we argue that the spiral or interference characteristics of the axial\ncurrent density determine the occurrence of nonlinear or tunneling effects in\nany spacetimedependent quantum systems. Our findings offer a comprehensive\ntheoretical framework that addresses the limitations of Keldysh theory and\nprovides new insights into the angular momentum properties of quantum systems,\nparticularly in tunneling-dominated regimes. Using Wigner function methods,\nfermionic generalized two-level model, and Berry phase simulations, we predict\nthat IOAM effect can persist even in pure quantum tunneling processes. These\nresults open the door for experimental verification of IOAM effects in future\nhigh-intensity QED experiments, such as those using X-ray free electron lasers.",
        "Symmetry is an important inductive bias that can improve model robustness and\ngeneralization across many deep learning domains. In multi-agent settings, a\npriori known symmetries have been shown to address a fundamental coordination\nfailure mode known as mutually incompatible symmetry breaking; e.g. in a game\nwhere two independent agents can choose to move \"left'' or \"right'', and where\na reward of +1 or -1 is received when the agents choose the same action or\ndifferent actions, respectively. However, the efficient and automatic discovery\nof environment symmetries, in particular for decentralized partially observable\nMarkov decision processes, remains an open problem. Furthermore, environmental\nsymmetry breaking constitutes only one type of coordination failure, which\nmotivates the search for a more accessible and broader symmetry class. In this\npaper, we introduce such a broader group of previously unexplored symmetries,\nwhich we call expected return symmetries, which contains environment symmetries\nas a subgroup. We show that agents trained to be compatible under the group of\nexpected return symmetries achieve better zero-shot coordination results than\nthose using environment symmetries. As an additional benefit, our method makes\nminimal a priori assumptions about the structure of their environment and does\nnot require access to ground truth symmetries.",
        "We give counterexamples for the modification on Malle's Conjecture given by\nT\\\"urkelli. T\\\"urkelli's modification on Malle's conjecture is inspired by an\nanalogue of Malle's conjecture over a function field. As a result, our\ncounterexamples demonstrate that the $b$ constant can differ between function\nfields and number fields. We also show that Kl\\\"uners' counterexamples give\ncounterexamples for a natural extension of Malle's conjecture to counting\nnumber fields by product of ramified primes. We then propose a refined version\nof Malle's conjecture which implies a new conjectural value for the constant\n$b$ for number fields.",
        "Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub.",
        "The dynamics of dilute plasma particles such as electrons and ions can be\nmodeled by the fundamental two species Vlasov-Poisson-Boltzmann equations,\nwhich describes mutual interactions of plasma particles through collisions in\nthe self-induced electric field. In this paper, we are concerned with global\nwell-posedness of mild solutions of the equations. We establish the global\nexistence and uniqueness of mild solutions to the two species\nVlasov-Poisson-Boltzmann equations in the torus for a class of initial data\nwith bounded time-velocity weighted $L^{\\infty}$ norm under nearly neutral\ncondition and some smallness condition on $L^1_xL^\\infty_v$ norm as well as\ndefect mass, energy and entropy so that the initial data allow large amplitude\noscillations. Due to the nonlinear effect of electric field, we consider the\nproblem in $W^{1, \\infty}_{x,v}$ with large amplitude data, new difficulty\narises when establishing globally uniform $W^{1, \\infty}_{x,v}$ bound, which\nhas been overcome based on nearly neutral condition, time-velocity weight\nfunction and a logarithmic estimate. Moreover, the large time behavior of\nsolutions in $W^{1, \\infty}_{x,v}$ norm with exponential decay rates of\nconvergence is also obtained.",
        "Using language models to scalably approximate human preferences on text\nquality (LLM-as-a-judge) has become a standard practice applicable to many\ntasks. A judgment is often extracted from the judge's textual output alone,\ntypically with greedy decoding. However, LLM judges naturally provide\ndistributions over judgment tokens, inviting a breadth of inference methods for\nextracting fine-grained preferences. We find that taking the mean of the\njudgment distribution consistently outperforms taking the mode (i.e. greedy\ndecoding) in all evaluation settings (i.e. pointwise, pairwise, and listwise).\nWe further explore novel methods of deriving preferences from judgment\ndistributions, and find that methods incorporating risk aversion often improve\nperformance. Lastly, we analyze LLM-as-a-judge paired with chain-of-thought\n(CoT) prompting, showing that CoT can collapse the spread of the judgment\ndistribution, often harming performance. Our findings suggest leveraging\ndistributional output can improve LLM-as-a-judge, as opposed to using the text\ninterface alone.",
        "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https:\/\/github.com\/opendatalab\/ProverGen",
        "We propose a new ion-trap architecture optimized for the efficient execution\nof both transversal and non-transversal gate operations in a two-dimensional\ncolor code. By differentiating the regions for transversal gates from those for\nnon-transversal gates and syndrome extraction, which require distinct qubit\nconnectivities, our chip layout minimizes ion shuttling and simplifies physical\nimplementations. We also provide a dedicated transpiler and scheduler for this\narchitecture, wherein the scheduler coordinates the sequence of operations and\ninserts the necessary swap and shuttling operations. Finally, we developed an\nerror analyzer to evaluate the chip's performance across a variety of quantum\nalgorithms. Simulation results confirm that our architecture can significantly\nincrease success rates and reduce gate error probabilities, particularly\nlowering the effective two-qubit gate error probability to about 10^{-8} when a\nquantum error-correcting code of 31 physical qubits is employed. Our findings\nclearly show that the improvement in success rates clearly outweighs the\nruntime overhead, demonstrating that strategic hardware-scheduler co-design can\nadvance quantum systems towards reliable, large-scale computing, potentially\nsurpassing classical capabilities."
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Fast fit-free analysis of fluorescence lifetime imaging via deep learning",
    "start_abstract":"Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window"
      ],
      "abstract":[
        "SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Short Paths in the Planar Graph Product Structure Theorem",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Monotonicity and convergence of two-relaxation-times lattice Boltzmann\n  schemes for a non-linear conservation law",
        "Coherent dynamics of flavor mode entangled neutrinos",
        "On the Use of WGANs for Super Resolution in Dark-Matter Simulations",
        "Relationship between the $\\gamma-$ray variability and the pc-scale jet\n  in the blazar 3C 454.3",
        "Gas Perturbations in Hot Smooth Atmospheres X-ray Surface Brightness\n  Fluctuations in Smooth Galaxy Cluster Atmospheres",
        "Causal Inference on Outcomes Learned from Text",
        "Propagation of extreme events in multiplex neuronal networks",
        "The Energy Cascade Rate in Supersonic Magnetohydrodynamic Turbulence",
        "Self-organized institutions in evolutionary dynamical-systems game",
        "Identifying Flare Locations Through Exoplanet Transit Occultations",
        "Effect of 3d Transition Metal Doping (Mn, Fe, Co, Ni) on the Electronic\n  and Magnetic Properties of Pd Alloys at Low Impurity Concentrations: An Ab\n  initio Study",
        "Resonant current from singlet-triplet state mixing in coupled quantum\n  dots",
        "Exchange Rate Sensitivity in Free Zone Trade: An Empirical Study of the\n  Istanbul Ataturk Airport Free Zone",
        "Phase evolution of strong-field ionization",
        "Fault tolerance for metric dimension and its variants",
        "On the viability of higher order theories",
        "PhysicsSolver: Transformer-Enhanced Physics-Informed Neural Networks for\n  Forward and Forecasting Problems in Partial Differential Equations",
        "Enhancement of Large Eddy Simulation for the prediction of an intake\n  flow rig using sequential Data Assimilation",
        "A network-driven framework for enhancing gene-disease association\n  studies in coronary artery disease",
        "The Jacobian of a regular orthogonal matroid and torsor structures on\n  spanning quasi-trees of ribbon graphs",
        "Prediction and observation of a stellar occultation by Haumea's\n  satellite Namaka",
        "Hausdorffness of certain nilpotent cohomology spaces",
        "The r-Dynamic Chromatic Number is Bounded in the Strong 2-Coloring\n  Number",
        "Non-orbital particle trapping in binary black holes through dynamic\n  stability",
        "Numerical security analysis for quantum key distribution with partial\n  state characterization",
        "Embedding 1D BDI topological models into continuous elastic plates",
        "On Exponents of Thickness in Geometry Rigidity Inequality for Shells"
      ],
      "abstract":[
        "The Planar Graph Product Structure Theorem of Dujmovi\\'c et al. [J. ACM '20]\nsays that every planar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_3$\nfor some planar graph $H$ with treewidth at most 3 and some path $P$. This\nresult has been the key to solving several old open problems. Several people\nhave asked whether the Planar Graph Product Structure Theorem can be proved\nwith good upper bounds on the length of $P$. No $o(n)$ upper bound was\npreviously known for $n$-vertex planar graphs. We answer this question in the\naffirmative, by proving that for any $\\epsilon\\in (0,1)$ every $n$-vertex\nplanar graph is contained in $H\\boxtimes P\\boxtimes K_{O(1\/\\epsilon)}$, for\nsome planar graph $H$ with treewidth 3 and for some path $P$ of length\n$O(\\frac{1}{\\epsilon}n^{(1+\\epsilon)\/2})$. This bound is almost tight since\nthere is a lower bound of $\\Omega(n^{1\/2})$ for certain $n$-vertex planar\ngraphs. In fact, we prove a stronger result with $P$ of length\n$O(\\frac{1}{\\epsilon}\\,\\textrm{tw}(G)\\,n^{\\epsilon})$, which is tight up to the\n$O(\\frac{1}{\\epsilon}\\,n^{\\epsilon})$ factor for every $n$-vertex planar graph\n$G$. Finally, taking $\\epsilon=\\frac{1}{\\log n}$, we show that every $n$-vertex\nplanar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ for some\nplanar graph $H$ with treewidth at most 3 and some path $P$ of length\n$O(\\textrm{tw}(G)\\,\\log n)$. This result is particularly attractive since the\ntreewidth of the product $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ is within a\n$O(\\log^2n)$ factor of the treewidth of $G$.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "We address the convergence analysis of lattice Boltzmann methods for scalar\nnon-linear conservation laws, focusing on two-relaxation-times (TRT) schemes.\nUnlike Finite Difference\/Finite Volume methods, lattice Boltzmann schemes offer\nexceptional computational efficiency and parallelization capabilities. However,\ntheir monotonicity and $L^{\\infty}$-stability remain underexplored. Extending\nexisting results on simpler BGK schemes, we derive conditions ensuring that TRT\nschemes are monotone and stable by leveraging their unique relaxation\nstructure. Our analysis culminates in proving convergence of the numerical\nsolution to the weak entropy solution of the conservation law. Compared to BGK\nschemes, TRT schemes achieve reduced numerical diffusion while retaining\nprovable convergence. Numerical experiments validate and illustrate the\ntheoretical findings.",
        "As the lynchpin of all quantum correlations, quantum coherence is fundamental\nfor distinguishing quantum systems from classical ones and is essential for\nrealizing quantum advantages in areas such as computation, communication, and\nmetrology. In this study, we investigate the relationship between quantum\ncoherence and neutrino oscillations by mapping the neutrino state as a\nmulti-mode quantum system into qubit and qutrit frameworks. Our analysis\nextends beyond the commonly used $l_1$-norm and relative entropy of coherence\nto include all relevant measures of coherence such as robustness of coherence,\ncoherence concurrence, trace-norm distance measure of coherence, coherence of\nformation, Schatten-$p$-norm-based functionals, geometric coherence and\nlogarithmic coherence rank, each offering unique insights into the quantum\ncorrelations in these systems. Notably, while the $l_1$-norm and relative\nentropy-based measures apply to general quantum states, the other measures are\nparticularly relevant for entangled systems, highlighting the critical role of\nentanglement in neutrino oscillations. We present a detailed methodology for\ncalculating coherence measures in both two-flavor and three-flavor mixing\nscenarios, contributing to a deeper understanding of how quantum coherence\nmanifests and evolves in mode-entangled neutrino systems. Our findings\nemphasize the potential of these systems as robust candidates for quantum\ninformation tasks, facilitated by the weak interaction nature of neutrinos.",
        "Super-resolution techniques have the potential to reduce the computational\ncost of cosmological and astrophysical simulations. This can be achieved by\nenabling traditional simulation methods to run at lower resolution and then\nefficiently computing high-resolution data corresponding to the simulated\nlow-resolution data. In this work, we investigate the application of a\nWasserstein Generative Adversarial Network (WGAN) to increase the particle\nresolution of dark-matter-only simulations, reproducing and building on prior\nresults. Our WGAN models successfully generate high-resolution data with\nsummary statistics, including the power spectrum and halo mass function, that\nclosely match those of true high-resolution simulations. We also identify a\nlimitation of the WGAN model in the form of smeared features in the generated\nhigh-resolution snapshots, particularly in the shapes of dark-matter halos.",
        "3C 454.3 is a flat spectrum radio quasar (FSRQ) known for its high\nvariability across the electromagnetic spectrum, showing structural and flux\nvariability in its pc-scale jet, and correlated variability among frequency\nbands. This study aims to identify the structure, dynamics, and radiative\nprocesses common to the innermost regions of the blazar 3C 454.3. We\ninvestigate whether any jet component can be associated with $\\gamma-$ray\nemission and variability. We analyze the relationship between the variable\n$\\gamma-$ray emission and pc-scale jet properties in 3C 454.3 by combining\n$\\gamma-$ray data spanning twelve years with contemporaneous VLBA multi-epoch\nimages at 15 and 43 GHz. Spearman rank correlation tests are conducted to\ndetermine if the flux variability of any jet component is associated with\n$\\gamma-$ray variability. Core emission at 43 and 15 GHz strongly correlates\nwith $\\gamma-$ray emission. The 43 GHz core (Q0) contributes around 37$\\%$ of\nthe observed $\\gamma-$ray variability, while the 15 GHz core (K0) accounts for\n30$\\%$. A quasi-stationary component at 43 GHz, at a projected distance of 4.6\npc, correlates with the $\\gamma-$ray flux, accounting for 20$\\%$ of its\nemission between 2016 and 2021. We found a mobile component (Q3 between 2010.18\nand 2011.16) at 43 GHz with a projected distance between 0.8 and 2.3 pc and\napparent velocity of $\\beta_{app} = 9.9 \\pm 1.1$ c, accounting for\napproximately 28% of the $\\gamma-$ray emission. The observed simultaneous\nvariability in emission regions beyond the central parsec strongly suggests\nsynchrotron self-Compton (SSC) as the primary mechanism for $\\gamma-$ray\nproduction in these regions. Our findings demonstrate the existence of multiple\n$\\gamma-$ray emission regions within the blazar jet but also suggest that some\nof these regions are non-stationary over time.",
        "We measure surface brightness fluctuations in Chandra X-ray images of the\ncores of the galaxy clusters Abell 2029, Abell 2151, Abell 2107, RBS0533, and\nRBS0540. Their relatively structureless X-ray atmospheres exhibit the\nthermodynamic properties of cool cores including short central cooling times\nand low entropy. However, unlike typical cool-core clusters, molecular gas,\nstar formation, and bubbles associated with radio jets are faint or absent near\ntheir central galaxies. Four clusters show typical gas density fluctuation\namplitudes of $\\sim$ 10 per cent on the scales probed, apart from RBS0540,\nwhich exhibits lower amplitudes, suggesting that its gas is mildly disturbed.\nUnder the assumption that gas density fluctuations are indicative of random gas\nvelocities, we estimate scale-dependent velocity amplitudes of gas motions\nacross all studied clusters, which range from 100 km\/s to 200 km\/s in Abell\n2029, Abell 2151, and Abell 2107. These velocity estimates are comparable to\nthe atmospheric velocity dispersion in the Perseus cluster measured by the\nHitomi X-ray Observatory. The turbulent heating rates implied by our\nmeasurements are of the same order as the radiative cooling rates. Our results\nsuggest that atmospheric sloshing and perhaps turbulent motion may aid radio\njets in stabilizing atmospheric cooling.",
        "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "Three-dimensional direct numerical simulations (DNS) are implemented to\ninvestigate the energy cascade rate in compressible isothermal\nmagnetohydrodynamic (MHD) turbulence. Utilizing an exact law derived from the\nK\\'arm\\'an-Howarth equation, we examine the contributions of flux and non-flux\nterms to the cascade rate across a broad range of sonic and Alfv\\'enic Mach\nnumbers, from subsonic to supersonic regimes and varying mean magnetic fields.\nCascade rates are computed using on-grid 3-D decomposition and two plasma\nincrement approaches: signed and absolute values. Anisotropy induced by strong\nmagnetic fields is analyzed through angular-dependent scaling of the cascade\nterms. Moreover, the increment calculation method significantly influences the\nrelative contributions of flux and non-flux terms, with absolute methods\ntending to overestimate the latter. These findings extend current studies of\ncompressible turbulence and offer critical insights into energy transfer\nmechanisms relevant to many astrophysical phenomena.",
        "Social institutions are systems of shared norms and rules that regulate\npeople's behaviors, often emerging without external enforcement. They provide\ncriteria to distinguish cooperation from defection and establish rules to\nsustain cooperation, shaped through long-term trial and error. While principles\nfor successful institutions have been proposed, the mechanisms underlying their\nemergence remain poorly understood. Here, we introduce the evolutionary\ndynamical-systems game, a framework that couples game actions with\nenvironmental dynamics and explores the evolution of cognitive frameworks for\ndecision-making. We analyze a minimal model of common-pool resource management,\nwhere resources grow naturally and are harvested. Players use decision-making\nfunctions to determine whether to harvest at each step, based on environmental\nand peer monitoring. As these functions evolve, players detect selfish\nharvesting and punish it by degrading the environment through harvesting. This\nprocess leads to the self-organization of norms that classify harvesting\nactions as cooperative, defective, or punitive. The emergent norms for\n``cooperativeness'' and rules of punishment serve as institutions. The\nenvironmental and players' states converge to distinct modes characterized by\nlimit-cycles, representing temporal regularities in socio-ecological systems.\nThese modes remain stable despite slight variations in decision-making,\nillustrating the stability of institutions. The evolutionary robustness of\ndecision-making functions serves as a measure of the evolutionary favorability\nof institutions, highlighting the role of plasticity in responding to diverse\nopponents. This work introduces foundational concepts in evolutionary\ndynamical-systems games and elucidates the mechanisms underlying the\nself-organization of institutions by modeling the interplay between ecological\ndynamics and human decision-making.",
        "M dwarfs are the most common stars in the galaxy, with long lifespans, a high\noccurrence rate of rocky planets, and close-in habitable zones. However, high\nstellar activity in the form of frequent flaring and any associated coronal\nmass ejections may drive atmospheric escape with the bombardment of radiation\nand high-energy particles, drastically impacting the habitability of these\nsystems. The stellar latitude where flares and coronal mass ejections occur\ndetermines the space weather that exoplanets are subject to, with high-energy\nparticle events associated with equatorial flares producing significant\natmospheric erosion. However, the flaring latitudes for M dwarfs remain largely\nunconstrained. To aid in the effort to locate these flaring regions we explore\nthe applicability of flare occultations using optical photometry to identify\nthe latitudes of flares. As a planet transits in front of an ongoing flare the\ntiming and geometry of the transit can be used to constrain the latitude and\nlongitude of the flare. We predict the probability of detecting an occultation\nfor known transiting planets and eclipsing binaries. From this, we estimate\n3-22 detectable occultations exist within the TESS primary mission photometry,\nwith the majority occurring in eclipsing binary observations. To demonstrate\nthis technique, we analyze a candidate flare occultation event for the\neclipsing binary CM Draconis.",
        "The nature of low-impurity ferromagnetism remains a challenging problem in\nthe solid-state community. Despite initial experiments dating back to the\nmid-20th century, a comprehensive theoretical explanation and reliable ab\ninitio evaluations have remained elusive. The present research aims to bridge\nthis gap by refining first-principle calculations by elucidating the magnetic\nand electronic behavior of Pd1-xMx alloys (where M = Mn, Fe, Co, Ni). Our study\nincludes calculations of magnetic properties throughout the range of impurity\nconcentrations, from 1 to 100 atomic percent (at.%), where we estimate critical\nconcentrations and perform a comparative analysis for the listed alloys.\nFurthermore, electronic structure was analyzed, including the calculations of\natomic, spin, and orbital-resolved states density, and exploration of the\nspatial formation of magnetic clusters containing ferromagnetic impurities\nacross all concentration ranges.",
        "Electrically driven spin resonances in double quantum dots can lift the spin\nblockade and give rise to a resonant current. This current can probe the\nproperties of coupled two-spin states for different quantum dot configurations.\nUsing a Floquet-Markov quantum transport model we compute the resonant current\nfor different driving amplitudes and ac field frequencies in spin-orbit coupled\nquantum dots. We show that the resonant current has a very rich interference\npattern which can give valuable insight into the singlet-triplet state mixing.",
        "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
        "We investigate the time-dependent evolution of the dipole phase shift induced\nby strong-field ionization (SFI) using attosecond transient absorption\nspectroscopy (ATAS) for time-delays where the pump-probe pulses overlap. We\nstudy measured and calculated time-dependent ATA spectra of the ionic 4d-5p\ntransition in xenon, and present the time-dependent line shape parameters in\nthe complex plane. We attribute the complex, attosecond-scale dynamics to the\ncontribution of three distinct processes: accumulation of ionization, transient\npopulation, and reversible population of excited states arising from\npolarization of the ground state.",
        "Hernando et al. (2008) introduced the fault-tolerant metric dimension\n$\\text{ftdim}(G)$, which is the size of the smallest resolving set $S$ of a\ngraph $G$ such that $S-\\left\\{s\\right\\}$ is also a resolving set of $G$ for\nevery $s \\in S$. They found an upper bound $\\text{ftdim}(G) \\le \\dim(G) (1+2\n\\cdot 5^{\\dim(G)-1})$, where $\\dim(G)$ denotes the standard metric dimension of\n$G$. It was unknown whether there exists a family of graphs where\n$\\text{ftdim}(G)$ grows exponentially in terms of $\\dim(G)$, until recently\nwhen Knor et al. (2024) found a family with $\\text{ftdim}(G) =\n\\dim(G)+2^{\\dim(G)-1}$ for any possible value of $\\dim(G)$. We improve the\nupper bound on fault-tolerant metric dimension by showing that $\\text{ftdim}(G)\n\\le \\dim(G)(1+3^{\\dim(G)-1})$ for every connected graph $G$. Moreover, we find\nan infinite family of connected graphs $J_k$ such that $\\dim(J_k) = k$ and\n$\\text{ftdim}(J_k) \\ge 3^{k-1}-k-1$ for each positive integer $k$. Together,\nour results show that \\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ }\n\\dim(G) = k} \\frac{\\log_3(\\text{ftdim}(G))}{k} \\right) = 1.\\] In addition, we\nconsider the fault-tolerant edge metric dimension $\\text{ftedim}(G)$ and bound\nit with respect to the edge metric dimension $\\text{edim}(G)$, showing that\n\\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ } \\text{edim}(G) = k}\n\\frac{\\log_2(\\text{ftedim}(G))}{k} \\right) = 1.\\] We also obtain sharp extremal\nbounds on fault-tolerance for adjacency dimension and $k$-truncated metric\ndimension. Furthermore, we obtain sharp bounds for some other extremal problems\nabout metric dimension and its variants. In particular, we prove an equivalence\nbetween an extremal problem about edge metric dimension and an open problem of\nErd\\H{o}s and Kleitman (1974) in extremal set theory.",
        "In physics, all dynamical equations that describe fundamental interactions\nare second order ordinary differential equations in the time derivatives. In\nthe literature, this property is traced back to a result obtained by\nOstrogradski in the mid 19th century, which is the technical basis of a 'no-go'\ntheorem for higher order theories. In this work, we review the connection of\nsymmetry properties with the order of dynamical equations, before reconsidering\nOstrogradski's result. Then, we show how Ostrogradski's conclusion is reached\nby applying to higher order theories concepts and method that have been\nspecifically developed for second order theories. We discuss a potential lack\nof consistency in this approach, to support the claim that Ostrogradski's\nresult applies to a class of higher order theories that is nowhere\nrepresentative of generic ones: we support this claim by giving an example of a\nhigher-order Lagrangian that is asymptotically stable, but that would be\nunstable under Ostrogradski's criterion. We also conclude that, when\nconsidering higher order theories as fundamental, we may need to reconsider and\nextend the conceptual framework on which our standard treatment of second order\ntheories is based.",
        "Time-dependent partial differential equations are a significant class of\nequations that describe the evolution of various physical phenomena over time.\nOne of the open problems in scientific computing is predicting the behaviour of\nthe solution outside the given temporal region. Most traditional numerical\nmethods are applied to a given time-space region and can only accurately\napproximate the solution of the given region. To address this problem, many\ndeep learning-based methods, basically data-driven and data-free approaches,\nhave been developed to solve these problems. However, most data-driven methods\nrequire a large amount of data, which consumes significant computational\nresources and fails to utilize all the necessary information embedded\nunderlying the partial differential equations (PDEs). Moreover, data-free\napproaches such as Physics-Informed Neural Networks (PINNs) may not be that\nideal in practice, as traditional PINNs, which primarily rely on multilayer\nperceptrons (MLPs) and convolutional neural networks (CNNs), tend to overlook\nthe crucial temporal dependencies inherent in real-world physical systems. We\npropose a method denoted as \\textbf{PhysicsSolver} that merges the strengths of\ntwo approaches: data-free methods that can learn the intrinsic properties of\nphysical systems without using data, and data-driven methods, which are\neffective at making predictions. Extensive numerical experiments have\ndemonstrated the efficiency and robustness of our proposed method. We provide\nthe code at\n\\href{https:\/\/github.com\/PhysicsSolver\/PhysicsSolver}{https:\/\/github.com\/PhysicsSolver}.",
        "A Data Assimilation (DA) strategy based on an ensemble Kalman filter (EnKF)\nis used to enhance the predictive capabilities of scale resolving numerical\ntools for the analysis of flows exhibiting cyclic behaviour. More precisely, an\nensemble of numerical runs using Large Eddy Simulation (LES) for the\ncompressible steady-state flow rig is augmented via the integration of\nhigh-fidelity data. This observation is in the form of instantaneous velocity\nmeasurements, which are sampled at localized sensors in the physical domain.\nTwo objectives are targeted. The first one is the calibration of an unsteady\ninlet condition suitable to capture the cyclic flow investigated. The second\none is the analysis of the synchronization of velocity field predicted by the\nLES with the available observation. In order to reduce the computational costs\nrequired for this analysis, a hyper-localization procedure (HLEnKF) is proposed\nand it is integrated in the library CONES, tailored to perform fast online DA.\nThe proposed strategy performs a satisfactory calibration of the inlet\nconditions, and its robustness is assessed using two different prior\ndistributions for the free parameters optimized in this task. DA state\nestimation is efficient in obtaining accurate local synchronization of the\ninferred velocity fields with the observed data. The modal analysis of the\nkinetic energy of the flow field provides additional information on the quality\nof the reconstruction of the velocity field, which shows improvements. Thus,\nthe HLEnKF shows promising features for the calibration and the synchronization\nof scale-resolved turbulent flows, opening perspectives of applications for\ncomplex phenomena using advanced tools such as digital twins.",
        "Over the last decade, genome-wide association studies (GWAS) have\nsuccessfully identified numerous genetic variants associated with complex\ndiseases. These associations have the potential to reveal the molecular\nmechanisms underlying complex diseases and lead to the identification of novel\ndrug targets. Despite these advancements, the biological pathways and\nmechanisms linking genetic variants to complex diseases are still not fully\nunderstood. Most trait-associated variants reside in non-coding regions and are\npresumed to influence phenotypes through regulatory effects on gene expression.\nYet, it is often unclear which genes they regulate and in which cell types this\nregulation occurs. Transcriptome-wide association studies (TWAS) aim to bridge\nthis gap by detecting trait-associated tissue gene expression regulated by GWAS\nvariants. However, traditional TWAS approaches frequently overlook the critical\ncontributions of trans-regulatory effects and fail to integrate comprehensive\nregulatory networks. Here, we present a novel framework that leverages\ntissue-specific gene regulatory networks (GRNs) to integrate cis- and\ntrans-genetic regulatory effects into the TWAS framework for complex diseases.\nWe validate our approach using coronary artery disease (CAD), utilizing data\nfrom the STARNET project, which provides multi-tissue gene expression and\ngenetic data from around 600 living patients with cardiovascular disease.\nPreliminary results demonstrate the potential of our GRN-driven framework to\nuncover more genes and pathways that may underlie CAD. This framework extends\ntraditional TWAS methodologies by utilizing tissue-specific regulatory insights\nand advancing the understanding of complex disease genetic architecture.",
        "Previous work of Chan--Church--Grochow and Baker--Wang shows that the set of\nspanning trees in a plane graph $G$ is naturally a torsor for the Jacobian\ngroup of $G$. Informally, this means that the set of spanning trees of $G$\nnaturally forms a group, except that there is no distinguished identity\nelement. We generalize this fact to graphs embedded on orientable surfaces of\narbitrary genus, which can be identified with ribbon graphs. In this\ngeneralization, the set of spanning trees of $G$ is replaced by the set of\nspanning quasi-trees of the ribbon graph, and the Jacobian group of $G$ is\nreplaced by the Jacobian group of the associated regular orthogonal matroid $M$\n(along with an associated regular representation of $M$). Our proof shows, more\ngenerally, that the family of \"BBY torsors\" constructed by Backman--Baker--Yuen\nand later generalized by Ding admit natural generalizations to (regular\nrepresentations of) regular orthogonal matroids. In addition to shedding light\non the role of planarity in the earlier work mentioned above, our results\nrepresent one of the first substantial applications of orthogonal matroids\n(also called \"even delta-matroids\" or \"Lagrangian orthogonal matroids\") to a\nnatural combinatorial problem about graphs.",
        "Stellar occultations are an ideal way to characterize the physical and\norbital properties of trans-Neptunian binary systems. In this research note, we\ndetail the prediction and observation of a stellar occultation observed with\nNASA's IRTF on March 16$^{\\mathrm{th}}$, 2025 (UT), with drop-outs from both\nthe dwarf planet Haumea and its smaller satellite Namaka. This occultation\nplaces a lower limit of 83 $\\pm$ 2 km on Namaka's diameter. We also discuss the\npossibility that this detection could help to constrain the orbit of Namaka,\nmeasure Haumea's gravitational harmonics, and provide a path to measuring the\ninternal structure of Haumea.",
        "Let $(\\pi,V)$ be a smooth representation of a compact Lie group $G$ on a\nquasi-complete locally convex complex topological vector space. We show that\nthe Lie algebra cohomology space $\\mathrm{H} ^\\bullet(\\mathfrak{u}, V)$ and the\nLie algebra homology space $\\mathrm{H}_\\bullet(\\mathfrak{u}, V)$ are both\nHausdorff, where $\\mathfrak{u}$ is the nilpotent radical of a parabolic\nsubalgebra of the complexified Lie algebra $\\mathfrak{g}$ of $G$.",
        "A proper vertex-coloring of a graph is $r$-dynamic if the neighbors of each\nvertex $v$ receive at least $\\min(r, \\mathrm{deg}(v))$ different colors. In\nthis note, we prove that if $G$ has a strong $2$-coloring number at most $k$,\nthen $G$ admits an $r$-dynamic coloring with no more than $(k-1)r+1$ colors. As\na consequence, for every class of graphs of bounded expansion, the $r$-dynamic\nchromatic number is bounded by a linear function in $r$. We give a concrete\nupper bound for graphs of bounded row-treewidth, which includes for example all\nplanar graphs.",
        "We present an interdisciplinary comparison between binary black hole systems\nand Radio Frequency (RF) Paul Traps, modeling the gravitational binary system\nas a rotating saddle near its center. This analogy connects these seemingly\nunrelated systems through the concept of dynamic stability. The rotating saddle\npotential is analytically tractable, allowing us to prove the existence of\nbounded charged particle trajectories under certain conditions. By focusing on\nstellar-mass black holes with a weak electric charge-a feature consistent with\nspecific astrophysical conditions that leaves the spacetime metric largely\nunaffected but can influence nearby particle interactions-we can neglect\ncomplicating factors such as magnetic fields from large accretion disks of\nheavier black holes or stellar winds. Our simulation results demonstrate that\ncharged particles can exhibit stable, non-orbital trajectories near the center\nof a binary system with charged stellar-mass black holes, providing unique\nthree-dimensional trapping primarily through gravity. This system is\ndistinctive in the literature for its non-orbital trapping mechanism. While\ntheoretically intriguing, this trapping relies on specific conditions,\nincluding nearly identical black hole masses. These types of non-orbital\ntrapping mechanisms could potentially allow for longer-lived plasma\nconfigurations, enhancing our ability to detect electromagnetic signatures from\nthese systems. The significance of this work lies in the novel comparison\nbetween a laboratory-scale quantum system and a larger astrophysical one,\nopening new avenues for exploring parallels between microscopic and cosmic\nphenomena across fourteen orders of magnitude in distance.",
        "Numerical security proofs offer a versatile approach for evaluating the\nsecret-key generation rate of quantum key distribution (QKD) protocols.\nHowever, existing methods typically require perfect source characterization,\nwhich is unrealistic in practice due to the presence of inevitable encoding\nimperfections and side channels. In this paper, we introduce a novel security\nproof technique based on semidefinite programming that can evaluate the\nsecret-key rate for both prepare-and-measure and measurement-device-independent\nQKD protocols when only partial information about the emitted states is\navailable, significantly improving the applicability and practical relevance\ncompared to existing numerical techniques. We demonstrate that our method can\noutperform current analytical approaches addressing partial state\ncharacterization in terms of achievable secret-key rates, particularly for\nprotocols with non-qubit encoding spaces. This represents a significant step\ntowards bridging the gap between theoretical security proofs and practical QKD\nimplementations.",
        "One-dimensional mechanical topological metamaterials belonging to the BDI\nsymmetry class (that is, preserving time-reversal, chiral, and particle-hole\nsymmetries) have been realized in discrete systems by exploiting arrangements\nof either masses and springs or acoustic resonators. This study presents an\napproach to embed one-dimensional BDI class metamaterials into fully continuous\nelastic two-dimensional waveguides. The design leverages the concept of\nevanescently coupled waveguides and defect resonances in order to reproduce the\nequivalent dynamics of prototypical BDI systems, such as the\nSu-Schrieffer-Heeger (SSH) model. Starting with a continuous plate waveguide\nwith a periodic distribution of pillars, resonant waveguides and local defects\nare created by either eliminating or by properly adjusting the height of\nselected pillars. The approach is validated by designing fully continuous\nelastic analogs of the SSH model and the dual SSH model. Numerical simulations\nconfirm the emergence of topological edge modes at the interface of\ntopologically distinct systems. In addition, edge modes in the elastic analog\nof the dual SSH model are shown to be Majorana-like modes.",
        "We study exponents of thickness in Frieseck-James-M\\\"uller's inequalities for\nshells. We derive the following results: (a) the exponent of thickness\n$\\mu(S)\\leq15\/8$ if the middle surface $S$ is parabolic; (b) the exponent of\nthickness $\\mu(S)\\leq11\/6$ if the middle surface $S$ is a minimal surface with\nnegative curvature; (c) the exponent of thickness $\\mu(S)\\leq11\/6$ if the\nmiddle surface $S$ is a ruled surface with negative curvature. The exponents of\nthickness in Frieseck-James-M\\\"uller's inequalities for thin shells represent\nthe relationship between rigidity and thickness $h$ of a shell when the large\ndeformations take place, i. e., the rigidity of the shell related to the\nthickness $h$ is $$Ch^{\\mu(S)}.$$ Thus the above results of $\\mu(S)<2$ show\nthat those shells are strictly more rigid than plates since $\\mu(S)=2$ for\nplates. Moreover, we present another result which shows that when $\\mu(S)<2,$\nany $W^{2,2}$ isometry of the middle surface is rigid."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Pathology image analysis using segmentation deep learning algorithms",
    "start_abstract":"With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Comparative gene expression profiles of intestinal transporters in mice, rats and humans"
      ],
      "abstract":[
        "We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Thermodynamic properties of fcc lead: A scalar and fully relativistic\n  first principle study",
        "QUOKKA-based understanding of outflows (QED) -- III. Outflow loading and\n  phase structure as a function of galactic environment",
        "Viscoelastic tensor and hydrodynamics of altermagnets",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Euclid Quick Data Release (Q1). Combined Euclid and Spitzer galaxy\n  density catalogues at $z>$ 1.3 and detection of significant Euclid passive\n  galaxy overdensities in Spitzer overdense regions",
        "Vacuum axisymmetric gravitational collapse revisited: preliminary\n  investigation",
        "Asymptotic integrability and its consequences",
        "Bidirectional controlled quantum state preparation in high-dimensional\n  quantum system",
        "Multiple orthogonal polynomial ensembles of derivative type",
        "Bayesian Hierarchical Emulators for Multi-Level Models: BayHEm",
        "Scaffold-Assisted Window Junctions for Superconducting Qubit Fabrication",
        "A benchmark analysis of saliency-based explainable deep learning methods\n  for the morphological classification of radio galaxies",
        "Simulations of Magnetic Monopole Collisions",
        "First-principles study of dielectric properties of ferroelectric\n  perovskite oxides with on-site and inter-site Hubbard interactions",
        "When Less is More: Evolutionary Dynamics of Deception in a\n  Sender-Receiver Game",
        "Continuous-Time Analysis of Federated Averaging",
        "PIETOOLS 2024: User Manual",
        "Loss Functions for Inventory Control",
        "Effect of thickness on the maximum potential drop of current collectors",
        "Optical absorption and luminescence of $\\alpha$-LiV$_2$O$_5$ from the\n  Bethe Salpeter Equation",
        "Around the topological classification problem of polynomial maps: A\n  survey",
        "Ghost Kohnert posets",
        "Modularity theorems for abelian surfaces",
        "Subgroups of Bestvina-Brady groups",
        "Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial\n  CDW Ground State",
        "Torsion models for tensor-triangulated categories",
        "On properties of eigenvalue regions for monotone stochastic matrices",
        "Infinitely many solutions for elliptic system with Hamiltonian type",
        "Solar oblateness & asphericities temporal variations: outstanding some\n  unsolved issues"
      ],
      "abstract":[
        "This study investigates the thermodynamic properties of face-centered cubic\nlead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation\n(QHA), examining the influence of spin-orbit coupling (SOC) and the\nexchange-correlation functionals. Two types of ultrasoft pseudopotential\n(US-PP) are considered: one that excludes (scalar relativistic PP) and one that\nincludes the SOC effects (fully relativistic PP). Further, for each PP, we test\nthe performance of three popular exchange-correlation functionals:\nPerdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.\nPhys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)\n(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density\napproximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate\nthe Helmholtz free energy, incorporating lattice vibrations (phonons) and\nelectronic excitation contributions. The estimated equation of state (at 4 K\nand 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\\\"uneisen\nparameters ({\\gamma}q{\\eta}) (at 100 K), volume thermal expansion coefficient\n(\\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic\naverage Gr\\\"uneisen parameter ({\\gamma}) are compared with the available\nexperimental and theoretical studies. Moreover, the 0 K pressure-dependent\nelastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye\ntemperature, and longitudinal and transverse sound velocities for\npolycrystalline lead are presented. The contributions of electronic excitations\nin all the thermodynamic properties are found to be negligible. With increasing\npressure, the role of spin-orbit effects decreases but does not vanish. Our\nfindings demonstrate that SOC leads to results distinct from the SR approach,\nbut agreement with the experiment is not consistently improved by including\nSOC.",
        "We present results from a suite of 3D high-resolution hydrodynamic\nsimulations of supernova-driven outflows from galactic disc regions with a\nrange of gas surface density, metallicity, and supernova scale height. We use\nthis suite to quantify how outflow properties -- particularly the loading\nfactors for mass, metallicity, and energy -- vary with these parameters. We\nfind that the winds fall into three broad categories: steady and hot,\nmultiphase and moderately bursty, and cool and highly bursty. The first of\nthese is characterised by efficient metal and energy loading but weak mass\nloading, the second by moderate loading of mass, metals, and energy, and the\nthird by negligible metal and energy loading but substantial mass loading. The\nmost important factor in determining the kind of wind a galaxy will produce is\nthe ratio of supernova to gas gas scale heights, with the latter set by a\ncombination of supernova rate, metallicity-dependent cooling rate, and the\ngravitational potential. These often combine in counterintuitive ways -- for\nexample increased cooling causes cold clouds to sink into the galactic midplane\nmore rapidly, lowering the volume-filling factor of dense gas and making the\nenvironment more favourable for strong winds. Our findings suggest that the\nnature of galactic winds is likely highly sensitive to phenomena such as\nrunaway stars occuring at a large height and dense gas and are poorly captured\nin most simulations, and that metal loading factors for type Ia supernovae may\nbe substantially larger than those for type II, with important implications for\ngalactic chemical evolution.",
        "We calculate the viscoelasticity tensor for altermagnets and formulate the\ncorresponding hydrodynamic equations. The anisotropy of altermagnetic Fermi\nsurfaces allows for additional terms in the viscoelasticity tensor and is\nmanifested in transport properties including electron and spin flows in a\nchannel and nonlocal responses. In the channel geometry, the altermagnetic spin\nsplitting leads to nontrivial spin density and spin current. Like the electric\ncurrent, the spin current acquires a Poiseuille profile for no-slip boundary\nconditions. In nonlocal responses, the altermagnetic anisotropy affects current\nstreamlines and electric potential distributions in the viscous regime. Our\nresults provide signatures of the hydrodynamic transport regime in altermagnets\npotentially facilitating its experimental studies and discovery.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Euclid will detect tens of thousands of clusters and protoclusters at\n$z$>1.3. With a total coverage of 63.1deg$^2$, the Euclid Quick Data Release 1\n(Q1) is large enough to detect tens of clusters and hundreds of protoclusters\nat these early epochs. The Q1 photometric redshift catalogue enables us to\ndetect clusters out to $z$ < 1.5; however, infrared imaging from Spitzer\nextends this limit to higher redshifts by using high local projected densities\nof Spitzer-selected galaxies as signposts for cluster and protocluster\ncandidates. We use Spitzer imaging of the Euclid Deep Fields (EDFs) to derive\ndensities for a sample of Spitzer-selected galaxies at redshifts $z$ > 1.3,\nbuilding Spitzer IRAC1 and IRAC2 photometric catalogues that are 95% complete\nat a magnitude limit of IRAC2=22.2, 22.6, and 22.8 for the EDF-S, EDF-F, and\nEDF-N, respectively. We apply two complementary methods to calculate galaxy\ndensities: (1) aperture and surface density; and (2) the Nth-nearest-neighbour\nmethod. When considering a sample selected at a magnitude limit of IRAC2 <\n22.2, at which all three EDFs are 95% complete, our surface density\ndistributions are consistent among the three EDFs and with the SpUDS blank\nfield survey. We also considered a deeper sample (IRAC2 < 22.8), finding that\n2% and 3% of the surface densities in the North and Fornax fields are 3$\\sigma$\nhigher than the average field distribution and similar to densities found in\nthe CARLA cluster survey. Our surface densities are also consistent with\npredictions from the GAEA semi-analytical model. Using combined Euclid and\nground-based i-band photometry we show that our highest Spitzer-selected galaxy\noverdense regions, found at $z$~1.5, also host high densities of passive\ngalaxies. This means that we measure densities consistent with those found in\nclusters and protoclusters at $z$>1.3.",
        "Validating the results of [A.M. Abrahams and C.R. Evans, Phys. Rev. Lett. 70,\n2980] poses a numerical challenge and has been inspiring a lot of research. We\njoin these efforts and present our first steps to achieve this goal: we discuss\na formulation of Einstein equations for a vacuum axisymmetric spacetime with\nvanishing twist in spherical-polar coordinates, its linearised approximation,\nand identify some problems in achieving numerically stable evolution at the\nthreshold of a black hole formation.",
        "We give a brief review of the concept of asymptotic integrability, which\nmeans that the Hamilton equations for the propagation of short-wavelength\npackets along a smooth, large-scale background wave have an integral\nindependent of the initial conditions. The existence of such an integral leads\nto a number of important consequences, which include, besides the direct\napplication to the packets propagation problems, Hamiltonian theory of narrow\nsolitons motion and generalized Bohr-Sommerfeld rule for parameters of solitons\nproduced from an intensive initial pulse. We show that in the case of systems\nwith two wave variables and exact fulfillment of the asymptotic integrability\ncondition, the `quantization' of mechanical systems, associated with the\nadditional integrals, yields the Lax pairs for a number of typical completely\nintegrable equations, and this sheds new light on the origin of the complete\nintegrability in nonlinear wave physics.",
        "High-dimensional quantum system exhibits unique advantages over the qubit\nsystem in some quantum information processing tasks. We present a program for\nimplementing deterministic bidirectional controlled remote quantum state\npreparation (BCRSP) in arbitrary $N$-dimensional (quNit) system. By introducing\ntwo generalized Greenberger-Horne-Zeilinger (GHZ) states as quantum channels,\ntwo communication parties can simultaneously prepare a single-particle\nhigh-dimensional state at each other's site under the control of Charlie.\nCompared with the previous counterparts, the significant advantage of our\nscheme is that the high-dimensional CNOT operations are not required. Moreover,\nthe performance our scheme are evaluated. The evaluation of the performance\nshows that if the quNit is encoded in the spatial mode of single photons, our\nscheme can be accomplished solely using only linear optical elements.",
        "We characterize the biorthogonal ensembles that are both a multiple\northogonal polynomial ensemble and a polynomial ensemble of derivative type\n(also called a P\\'olya ensemble). We focus on the two notions of derivative\ntype that typically appear in connection with the squared singular values of\nproducts of invertible random matrices and the eigenvalues of sums of Hermitian\nrandom matrices. Essential in the characterization is the use of the Mellin and\nLaplace transform: we show that the derivative type structure, which is a\npriori analytic in nature, becomes algebraic after applying the appropriate\ntransform. Afterwards, we explain how these notions of derivative type can be\nused to provide a partial solution to an open problem related to orthogonality\nof the finite finite free multiplicative and additive convolution from finite\nfree probability. In particular, we obtain families of multiple orthogonal\npolynomials that (de)compose naturally using these convolutions.",
        "Decision making often uses complex computer codes run at the exa-scale (10e18\nflops). Such computer codes or models are often run in a hierarchy of different\nlevels of fidelity ranging from the basic to the very sophisticated. The top\nlevels in this hierarchy are expensive to run, limiting the number of possible\nruns. To make use of runs over all levels, and crucially improve emulation at\nthe top level, we use multi-level Gaussian process emulators (GPs). We will\npresent a new method of building GP emulators from hierarchies of models. In\norder to share information across the different levels, l=1,...,L, we define\nthe form of the prior of the l+1th level to be the posterior of the lth level,\nhence building a Bayesian hierarchical structure for the top Lth level. This\nenables us to not only learn about the GP hyperparameters as we move up the\nmulti-level hierarchy, but also allows us to limit the total number of\nparameters in the full model, whilst maintaining accuracy.",
        "The superconducting qubit is one of the promising directions in realizing\nfault-tolerant quantum computing (FTQC), which requires many high-quality\nqubits. To achieve this, it is desirable to leverage modern semiconductor\nindustry technology to ensure quality, uniformity, and reproducibility.\nHowever, conventional Josephson junction fabrication relies mainly on\nresist-assistant double-angle evaporation, posing integration challenges. Here,\nwe demonstrate a lift-off-free qubit fabrication that integrates seamlessly\nwith existing industrial technologies. This method employs a silicon oxide\n(SiO$_2$) scaffold to define an etched window with a well-controlled size to\nform a Josephson junction. The SiO$_2$, which has a large dielectric loss, is\netched away in the final step using vapor HF leaving little residue. This\nWindow junction (WJ) process mitigates the degradation of qubit quality during\nfabrication and allows clean removal of the scaffold. The WJ process is\nvalidated by inspection and Josephson junction measurement. The scaffold\nremoval process is verified by measuring the quality factor of the resonators.\nFurthermore, compared to scaffolds fabricated by plasma-enhanced chemical vapor\ndeposition (PECVD), qubits made by WJ through physical vapor deposition (PVD)\nachieve relaxation time up to $57\\,\\mu\\text{s}$. Our results pave the way for a\nlift-off-free qubit fabrication process, designed to be compatible with modern\nfoundry tools and capable of minimizing damage to the substrate and material\nsurfaces.",
        "This work proposes a saliency-based attribution framework to evaluate and\ncompare 10 state-of-the-art explainability methods for deep learning models in\nastronomy, focusing on the classification of radio galaxy images. While\nprevious work has primarily emphasized classification accuracy, we prioritize\nmodel interpretability. Qualitative assessments reveal that Score-CAM,\nGrad-CAM, and Grad-CAM++ consistently produce meaningful attribution maps,\nhighlighting the brightest regions of FRI and FRII galaxies in alignment with\nknown astrophysical features. In contrast, other methods often emphasize\nirrelevant or noisy areas, reducing their effectiveness.",
        "In this paper, we investigate the scattering of BPS magnetic monopoles\nthrough numerical simulations. We present an ansatz for various multi-monopole\nconfigurations suitable for analyzing monopole scattering processes. Our study\nincludes planar scattering scenarios involving two, three, and four monopoles,\nas well as non-planar processes where three and four monopoles form\nintermediate tetrahedral and cubic states, respectively. Our observations align\nwith the theoretical predictions of the moduli space approximation.\nFurthermore, we extend our analysis to relativistic velocities and explore\nparameters beyond the BPS limit.",
        "We study the atomic and electronic structures of ferroelectric perovskite\noxides, BaTiO$_3$, LiNbO$_3$, and PbTiO$_3$ using ab initio extended Hubbard\nfunctionals in which the on-site and inter-site Hubbard interactions are\ndetermined self-consistently, adapted from the pseudohybrid density functional\nproposed by Agapito-Curtarolo-Buongiorno Nardelli. Band structures,\nferroelectric distortions, polarization, Born effective charges, and switching\nbarriers are calculated with extended Hubbard functionals, that are compared\nwith those using local density approximation (LDA), generalized gradient\napproximation (GGA), and Hybrid (HSE06) functionals. The properties of all\nthree compounds calculated by extended Hubbard functionals are in good\nagreement with experimental data. We find a substantial increase in band gaps\ndue to the inter-site Coulomb interactions, which show better agreement with\n$GW$ results compared to those from LDA and GGA functionals. The crucial role\nof the inter-site Coulomb interactions in restoring the suppressed polar\ninstability, which is computed when only the on-site Hubbard interactions are\nconsidered, is also highlighted. Overall, we find that the properties\ncalculated using our extended Hubbard functionals exhibit trends similar to\nthose obtained with the HSE06 functional, while reducing computational costs by\nover an order of magnitude. Thus, we propose that the current method is\nwell-suited for high-throughput calculations for perovskite oxides, offering\nsignificantly improved accuracy in computing band gap and other related\nphysical properties such as the shift current photovoltaic effect and band\nalignments in ferroelectric heterostructures.",
        "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail.",
        "Federated averaging (FedAvg) is a popular algorithm for horizontal federated\nlearning (FL), where samples are gathered across different clients and are not\nshared with each other or a central server. Extensive convergence analysis of\nFedAvg exists for the discrete iteration setting, guaranteeing convergence for\na range of loss functions and varying levels of data heterogeneity. We extend\nthis analysis to the continuous-time setting where the global weights evolve\naccording to a multivariate stochastic differential equation (SDE), which is\nthe first time FedAvg has been studied from the continuous-time perspective. We\nuse techniques from stochastic processes to establish convergence guarantees\nunder different loss functions, some of which are more general than existing\nwork in the discrete setting. We also provide conditions for which FedAvg\nupdates to the server weights can be approximated as normal random variables.\nFinally, we use the continuous-time formulation to reveal generalization\nproperties of FedAvg.",
        "The PIETOOLS 2024 User Manual describes all the features of version 2024 of\nthe MATLAB toolbox PIETOOLS for the analysis and control of Partial Integral\nEquations (PIEs). The manual is aimed to guide, with examples, first-time users\nto four fundamental features of PIETOOLS: converting coupled ODE-PDEs, DDEs,\nDDFs, etc., to PIE representation; analysis of stability and input-output\nproperties of PIEs; design of optimal observers and controllers for PIEs;\nsimulation of open- and closed-loop PIE systems. The use of PIETOOLS is not\nlimited to the features described above. However, the manual focuses on these\nfeatures to provide a holistic understanding of the workflow of PIETOOLS, which\nwill serve as a foundation to develop more complicated programs, for example,\nthe design of boundary feedback controllers, robust observers, robust\ncontrollers, etc..",
        "In this paper, we provide analytic expressions for the first-order loss\nfunction, the complementary loss function and the second-order loss function\nfor several probability distributions. These loss functions are important\nfunctions in inventory optimization and other quantitative fields. For several\nreasons, which will become apparent throughout this paper, the implementation\nof these loss functions prefers the use of an analytic expression, only using\nstandard probability functions. However, complete and consistent references of\nanalytic expressions for these loss functions are lacking in literature. This\npaper aims to close this gap and can serve as a reference for researchers,\nsoftware engineers and practitioners that are concerned with the optimization\nof a quantitative system. This should lead directly to easily using different\nprobability distributions in quantitive models which is at the core of\noptimization. Also, this paper serves as a broad introduction to loss functions\nand their use in inventory control.",
        "The basic principle for achieving high-power capability on an electrochemical\nenergy storage cell is minimizing the overall resistance. The resistance due to\ncurrent collecting systems has not received sufficient attention in the past,\npresumably because it was not considered of significance for low-power\nbatteries and supercapacitors. However, the necessity of high-power cells has\nreduced other sources of the inner resistance, and the current collector\npotential drop has become more important. Moreover, the miniaturization of\nenergy storage devices could increase the ohmic loses in current collectors. In\nthis work, we have developed an electrical model to assess the effect of the\ncurrent collector thickness on the maximum potential drop. We have found that\nthe thickness of current collectors is a critical parameter that can increase\nthe maximum potential drop drastically. Indeed, the maximum potential drop of\ncurrent collectors remains almost constant for thicknesses greater than 500 lm,\nbut below this value, there is an inverse relationship between the maximum\npotential drop and the thickness. We have also analyzed the effect of the\nmaterial and tab position in the maximum potential drop.",
        "$\\alpha$-Li$_x$V$_2$O$_5$ is obtained by intercalating Li between the layers\nof V$_2$O$_5$. The partial filling of the split-off conduction band by electron\ndonation from Li leads to significant changes in optical properties. Here we\nstudy the electronic band structure of $\\alpha$-LiV$_2$O$_5$ using\nquasiparticle self-consistent (QS) $GW$ calculations and the optical dielectric\nfunction by means of the Bethe Salpeter equation. We find a very strong optical\nabsorption band related to transitions between the filled V-$d_{xy}$ like\nstates to the empty ones with strong polarization along the $a$-direction. We\nrelate this to recent experimental observations of cathodoluminescence (CL) in\nwhich a supression of the CL was observed upon addition of Li.",
        "The study of the topology of polynomial maps originates from classical\nquestions in affine geometry, such as the Jacobian Conjecture, as well as from\nworks of Whitney, Thom, and Mather in the 1950-70s on diffeomorphism types of\nsmooth maps. During that period, Thom came up with a famous construction of a\none-dimensional family of real polynomial maps having fixed degree nine and\ninfinitely many topological types. In his convention, a topological type of a\nmap is preserved precisely when it is composed with a homeomorphism on the\nsource space, and one on the target space.\n  Thom also conjectured that for each pair $(n,d)$, any family of $n$--variate,\ndegree--$d$ (complex, or real) polynomial functions has at most finitely-many\ntopological types. Soon after, a collection of results by several\nmathematicians throughout the 1970s and 1980s settled this conjecture, and\nsolved its subsequent generalization to polynomial maps.\n  In this survey, we outline the historical context and highlight a range of\nsignificant works from the 1950s to the present day that lead to the current\nstate of the art in the study of polynomial maps' topology. The focal point of\nthis survey is to shed some light on the ensuing classification problem of\ntopological types of polynomial maps. The presentation is achieved by making a\ngentle introduction to several other prominent questions in affine geometry,\nall of which are recounted through the lens of the above classification\nproblem.",
        "Recently, Pan and Yu showed that Lascoux polynomials can be defined in terms\nof certain collections of diagrams consisting of unit cells arranged in the\nfirst quadrant. Starting from certain initial diagrams, one forms a finite set\nof diagrams by applying two types of moves: Kohnert and ghost moves. Both moves\ncause at most one cell to move to a lower row with ghost moves leaving a new\n\"ghost cell\" in its place. Each diagram formed in this way defines a monomial\nin the associated Lascoux polynomial. Restricting attention to diagrams formed\nby applying sequences of only Kohnert moves in the definition of Lascoux\npolynomials, one obtains the family of key polynomials. Recent articles have\nconsidered a poset structure on the collections of diagrams formed when one\nuses only Kohnert moves. In general, these posets are not \"well-behaved,\" not\nusually having desirable poset properties. Here, as an intermediate step to\nstudying the analogous posets associated with Lascoux polynomials, we consider\nthe posets formed by restricting attention to those diagrams formed by using\nonly ghost moves. Unlike in the case of Kohnert posets, we show that such\n\"ghost Kohnert posets\" are always ranked join semi-lattices. In addition, we\nestablish a necessary condition for when ghost Kohnert posets are bounded and,\nconsequently, lattices.",
        "We prove the modularity of a positive proportion of abelian surfaces over\n$\\mathbf{Q}$. More precisely, we prove the modularity of abelian surfaces which\nare ordinary at $3$ and are $3$-distinguished, subject to some assumptions on\nthe $3$-torsion representation (a \"big image\" hypothesis, and a technical\nhypothesis on the action of a decomposition group at $2$). We employ a 2-3\nswitch and a new classicality theorem (in the style of Lue Pan) for ordinary\n$p$-adic Siegel modular forms.",
        "In \"Subgroups of Graph Groups\", 1987, J. Alg., Droms proved that all the\nsubgroups of a right-angled Artin group (RAAG) defined by a finite simplicial\ngraph $\\Gamma$ are themselves RAAGs if, and only if, $\\Gamma$ has no induced\nsquare graph nor line-graph of length $3$. The present work provides a similar\nresult for specific normal subgroups of RAAGs, called Bestvina-Brady groups: We\ncharacterize those graphs in which every subgroup of such a group is itself a\nRAAG. In turn, we confirm several Galois theoretic conjectures for the pro-$p$\ncompletions of these groups.",
        "We evaluate the Hall coefficient characterising magnetotransport in an\nintercalated graphite CaC$_6$ with the Fermi surface reconstructed by an\nuniaxial charge density wave from closed pockets to open sheets. As the typical\norder parameter, corresponding to the pseudo-gap in electronic spectrum and\nconsequently to spacing between electron trajectories in reciprocal space, is\nof the order of $10^2$K, magnetic breakdown in strong experimentally achievable\nfields of the order of 10T is inevitable. The classical expressions for the\ncomponents of the magnetoconductivity tensor are strongly modified by magnetic\nfield-assisted over-gap tunneling causing quantum interference. Due to magnetic\nbreakdown, all magnetoconductivity components undergo strong quantum\noscillations reflected in the Hall coefficient. In their nature, these are\ndifferent than standard Shubnikov de Haas oscillations which would not appear\nin a system with an open Fermi surface.",
        "Given a rigidly-compactly generated tensor-triangulated category whose Balmer\nspectrum is finite dimensional and Noetherian, we construct a torsion model for\nit, which is equivalent to the original tensor-triangulated category. The\ntorsion model is determined in an adelic fashion by objects with singleton\nsupports. This categorifies the Cousin complex from algebra, and the process of\nreconstructing a spectrum from its monochromatic layers in chromatic stable\nhomotopy theory. This model is inspired by work of the second author in\nrational equivariant stable homotopy theory, and extends previous work of the\nauthors from the one-dimensional setting.",
        "Monotone stochastic matrices are stochastic matrices in which each row\nstochastically dominates the previous one. While the eigenvalue regions for\nstochastic matrices have been fully described by F.I. Karpelevich in 1951, this\nstudy focuses on the analysis of monotone matrices. This paper examines their\nspectral properties and establishes a reduction theorem stating that, for all n\nfrom 3 on, the eigenvalue region for the nxn monotone matrices is included in\nthose for the (n-1)x(n-1) stochastic matrices. Moreover, the eigenvalue region,\nalong with the corresponding realising matrices, is determined for monotone\nmatrices up till order 3.",
        "In this paper, we use Legendre-Fenchel transform and a space decomposition to\ncarry out Fountain theorem and dual Fountain theorem for the following elliptic\nsystem of Hamiltonian type: \\[ \\begin{cases} \\begin{aligned} -\\Delta u&=H_v(u,\nv) \\,\\quad&&\\text{in}~\\Omega,\\\\ -\\Delta v&=H_u(u, v)\n\\,\\quad&&\\text{in}~\\Omega,\\\\ u,\\,v&=0~~&&\\text{on} ~ \\partial\\Omega,\\\\\n\\end{aligned} \\end{cases} \\] where $N\\ge 1$, $\\Omega \\subset \\mathbb{R}^N$ is a\nbounded domain and $H\\in C^1( \\mathbb{R}^2)$ is strictly convex, even and\nsubcritical. We mainly present two results: (i) When $H$ is superlinear, the\nsystem has infinitely many solutions, whose energies tend to infinity. (ii)\nWhen $H$ is sublinear, the system has infinitely many solutions, whose energies\nare negative and tend to 0. As a byproduct, the Lane-Emden system under\nsubcritical growth has infinitely many solutions.",
        "Solar oblateness has been the subject of several studies dating back to the\nnineteenth century. Despite diffculties, both theoretical and observational,\ntangible results have been achieved. However, variability of the solar\noblateness with time is still poorly known. How the solar shape evolves with\nthe solar cycle has been a challenging problem. Analysis of the helioseismic\ndata, which are the most accurate measure of the solar structure up to now,\nleads to the determination of asphericity coeffcients which have been found to\nchange with time. We show here that by inverting even coeffcients of f-mode\noscillation frequency splitting to obtain the oblateness magnitude and its\ntemporal dependence can be inferred. It is found that the oblateness variations\nlag the solar activity cycles by about 3 years. A major change occurred between\nsolar cycles 23 and 24 is that the oblateness was greater in cycle 24 despite\nthe lower solar activity level. Such results may help to better understand the\nnear-subsurface layers as they strongly impacts the internal dynamics of the\nSun and may induce instabilities driving the transport of angular momentum."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Comparative gene expression profiles of intestinal transporters in mice, rats and humans",
    "start_abstract":"We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Pathology image analysis using segmentation deep learning algorithms"
      ],
      "abstract":[
        "With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and\n  Bias in Icelandic Blog Comments",
        "Global Hall-magnetohydrodynamic simulations of transition disks",
        "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly",
        "MIGHTEE: exploring the relationship between spectral index, redshift and\n  radio luminosity",
        "Storing quantum coherence in a quantum dot nuclear spin ensemble for\n  over 100 milliseconds",
        "Intrinsic charm and $D^+D^-$ asymmetry produced in proton-proton\n  collisions",
        "A note on partial polynomial functions, in memory of Marek Jarnicki",
        "Gaussian Universality of Products Over Split Reductive Groups and the\n  Satake Isomorphism",
        "Random Dynamical Systems on the circle without a finite orbit",
        "First photometric investigation of V517 Cam combined with ground-based\n  and TESS data",
        "Enhancing the charging performance of an atomic quantum battery",
        "BEARCUBS: A benchmark for computer-using web agents",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Practical Spoofing Attacks on Galileo Open Service Navigation Message\n  Authentication",
        "Intelligent Gradient Boosting Algorithms for Estimating Strength of\n  Modified Subgrade Soil",
        "GIFT: Generated Indoor video frames for Texture-less point tracking",
        "Detecting Convolutional Codes: A Markovian Approach with LRT and DNN",
        "Interface reconstruction of adhering droplets for distortion correction\n  using glare points and deep learning",
        "Fine-tunings in nucleosynthesis and the emergence of life: Status and\n  perspectives",
        "STEMS: Spatial-Temporal Mapping Tool For Spiking Neural Networks",
        "Separate This, and All of these Things Around It: Music Source\n  Separation via Hyperellipsoidal Queries",
        "Robust Trajectory Generation and Control for Quadrotor Motion Planning\n  with Field-of-View Control Barrier Certification",
        "Iterative Feature Space Optimization through Incremental Adaptive\n  Evaluation",
        "Saint-Venant Estimates and Liouville-Type Theorems for the Stationary\n  MHD Equation in $\\mathbb{R}^3$",
        "Deep RC: A Scalable Data Engineering and Deep Learning Pipeline",
        "Hierarchies from deterministic non-locality in theory space Anderson\n  localisation",
        "To BEE or not to BEE: Estimating more than Entropy with Biased Entropy\n  Estimators",
        "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction",
        "Fits of $\\alpha_s$ from event-shapes in the three-jet region: extension\n  to all energies"
      ],
      "abstract":[
        "This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.",
        "Context. Transition disks (TDs) are a type of protoplanetary disk\ncharacterized by a central dust and gas cavity. The processes behind how these\ncavities are formed and maintained, along with their observed high accretion\nrates of $10^{-8} -10^{-7} \\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, continue to be\nsubjects of active research. Aims. This work aims to investigate how the\ninclusion of the Hall effect (HE) alongside Ohmic resistivity (OR) and\nambipolar diffusion (AD) affects the structure of the TD. Of key interest is\nthe dynamical evolution of the cavity and whether it can indeed produce\ntransonic accretion, as predicted by theoretical models in order to account for\nthe observed high accretion rates despite the inner disk's low density.\nMethods. We present our results of 2D axisymmetric global radiation\nmagnetohydrodynamic (MHD) simulations of TDs for which all three non-ideal MHD\neffects are accounted. We used the NIRVANA-III fluid code and initialized our\nmodel with a disk cavity reaching up to $R=8~\\mathrm{au}$ with a density\ncontrast of $10^5$. We performed three runs, one with only OR and AD, and one\nfor each of the two configurations that arise when additionally including the\nHE, that is, with the field aligned (anti-aligned) with respect to the rotation\naxis. Results. For all three runs, our models maintain an intact inner cavity\nand an outer standard disk. MHD winds are launched both from the cavity and\nfrom the disk. Notably, when the HE is included, ring-like structures develop\nwithin the cavity. We moreover obtain accretion rates of $3 - 8 \\times 10^{-8}\n\\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, comparable to typical values seen in full\ndisks. Importantly, we clearly observe transonic accretion ($v_{\\mathrm{acc}}\n\\gtrsim c_{s}$) in the cavity. Additionally, outward magnetic flux transport\noccurs in all three runs.",
        "Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.",
        "It has been known for many years that there is an apparent trend for the\nspectral index ({\\alpha}) of radio sources to steepen with redshift z, which\nhas led to attempts to select high-redshift objects by searching for radio\nsources with steep spectra. In this study we use data from the MeerKAT, LOFAR,\nGMRT, and uGMRT telescopes, particularly using the MIGHTEE and superMIGHTEE\nsurveys, to select compact sources over a wide range of redshifts and\nluminosities. We investigate the relationship between spectral index,\nluminosity and redshift and compare our results to those of previous studies.\nAlthough there is a correlation between {\\alpha} and z in our sample for some\ncombinations of frequency where good data are available, there is a clear\noffset between the {\\alpha}-z relations in our sample and those derived\npreviously from samples of more luminous objects; in other words, the\n{\\alpha}-z relation is different for low and high luminosity sources. The\nrelationships between {\\alpha} and luminosity are also weak in our sample but\nin general the most luminous sources are steeper-spectrum and this trend is\nextended by samples from previous studies. In detail, we argue that both a\n{\\alpha}-luminosity relation and an {\\alpha}-z relation can be found in the\ndata, but it is the former that drives the apparent {\\alpha}-z relation\nobserved in earlier work, which only appears because of the strong\nredshift-luminosity relation in bright, flux density-limited samples.\nSteep-spectrum selection should be applied with caution in searching for high-z\nsources in future deep surveys.",
        "States with long coherence are a crucial requirement for qubits and quantum\nmemories. Nuclear spins in epitaxial quantum dots are a great candidate,\noffering excellent isolation from external environments and on-demand coupling\nto optical flying qubits. However, coherence times are limited to $\\lesssim1$\nms by the dipole-dipole interactions between the nuclei and their quadrupolar\ncoupling to inhomogeneous crystal strain. Here, we combine strain engineering\nof the nuclear spin ensemble and tailored dynamical decoupling sequences to\nachieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible\ntransfer of quantum information into nuclear spin ensembles has been\ndemonstrated in quantum dots. Our results provide a path to develop this\nconcept into a functioning solid-state quantum memory suitable for quantum\nrepeaters in optical quantum communication networks.",
        "We investigate the contribution of the charm-anticharrm ($c{\\bar c}$)\nasymmetry of the proton eigenstate obtained from QCD lattice gauge to the\nasymmetry of $D^+D^-$ and $D^0{\\bar D}^0$ mesons produced in $pp$ collisions at\nlarge Feynman variables $x$. It is shown that an important tool for the\nestablishing the intrinsic charm (IC) content of the proton is the charm\nhadron-antihadron asymmetry formed in $pp$ collisions. Predictions for the\nasymmetry as function of $x$ for different IC probabilities are presented. We\nshow that the interference of the intrinsic $|uud c{\\bar c}>$ Fock state with\nthe standard contribution from the PQCD evolution leads to a large $D^+D^-$\nasymmetry at large Feynman $x$.",
        "We present an extension theorem for a separately holomorphic function which\nis polynomial\/rational in some variables.",
        "We establish that the singular numbers (arising from Cartan decomposition)\nand corners (emerging from Iwasawa decomposition) in split reductive groups\nover non-archimedean fields are fundamentally determined by Hall-Littlewood\npolynomials. Through applications of the Satake isomorphism, we extend Van\nPeski's results (arXiv:2011.09356, Theorem 1.3) to encompass arbitrary root\nsystems. Leveraging this theoretical foundation, we further develop Shen's work\n(arXiv:2411.01104, Theorem 1.1) to demonstrate that both singular numbers and\ncorners of such products exhibit minimal separation. This characterization\nenables the derivation of asymptotic properties for singular numbers in matrix\nproducts, particularly establishing the strong law of large numbers and central\nlimit theorem for these quantities. Our results provide a unified framework\nconnecting algebraic decomposition structures with probabilistic limit theorems\nin non-archimedean settings.",
        "In this paper, we study Random Dynamical Systems (RDSs) of homeomorphisms on\nthe circle without a finite orbit. We characterize the topological dynamics of\nthe associated semigroup by identifying the existence of invariant sets which\nare finite unions of intervals. We describe the accumulation points of the\naverage orbit of the transfer operator. For each ergodic stationary measure, we\ndemonstrate interesting properties of its weight function on the circle.\nRelationships between the minimal sets of an RDS and its inverse RDS are also\nestablished.",
        "The observations of eclipsing binary systems are of great importance in\nastrophysics, as they allow direct measurements of fundamental stellar\nparameters. By analysing high-quality space-based observations with\nground-based photometric data, it becomes possible to detect these fundamental\nparameters with greater precision using multicolour photometry. Here, we report\nthe first photometric analysis results of the V517 Cam eclipsing binary system\nby combining the Transiting Exoplanet Survey Satellite (TESS) light curve and\nnew CCD observations in BVRI filters, obtained with a 60 cm robotic telescope\n(T60) at the T\\\"UB\\.ITAK National Observatory. By means of photometric\nanalyses, the masses and radii of the primary and secondary stars were\ncarefully determined to be $M_{1}= 1.47\\pm 0.06\\,M_\\odot$, $M_{2}=\n0.79\\pm0.05\\,M_\\odot$, and $R_{1}=1.43\\pm 0.03\\,R_\\odot$, $R_{2}= 0.75\\pm\n0.04\\,R_\\odot$, respectively. Furthermore, the distance to V517 Cam was\ncalculated to be $284\\pm20$ pc. The overall age of the system is estimated to\nbe around $63\\pm15$ Myr. At this age, the primary component stands near the\nonset of its main-sequence evolution, near the ZAMS, whereas the secondary\ncomponent remains in the pre-main-sequence evolutionary phase. To better\nunderstand the evolutionary status and nature of V517 Cam, the mass ratio and\ntemperature values, obtained with relatively low sensitivity by photometric\nmeasurements, need to be confirmed by spectral analysis.",
        "We study a quantum battery (QB) model composed of two atoms, where the\ncharger and battery elements are coupled to a multimode vacuum field that\nserves as a mediator for energy transfer. Different figures of merit such as\nergotropy, charging time, and charging efficiency are analyzed, putting\nemphasis on the role of various control parameters on the charging performance.\nIt is found that there is a range of angle between the transition dipole\nmoments and interatomic axis in which the QB can be charged. The optimal\ncharging performance is achieved if the atomic dipole moments are perpendicular\nor parallel to the interatomic axis. The charging performance also improves\nwith the decrease of the interatomic distance. Besides, the charged ergotropy\ncan be enhanced by increasing the initial ergotropy of the charger and it is\nbeneficial to charge the QB starting from a passive state.",
        "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing search inefficiencies and domain knowledge gaps as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n24.3% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and\/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations.",
        "The performance of pavement under loading depends on the strength of the\nsubgrade. However, experimental estimation of properties of pavement strengths\nsuch as California bearing ratio (CBR), unconfined compressive strength (UCS)\nand resistance value (R) are often tedious, time-consuming and costly, thereby\ninspiring a growing interest in machine learning based tools which are simple,\ncheap and fast alternatives. Thus, the potential application of two boosting\ntechniques; categorical boosting (CatBoost) and extreme gradient boosting\n(XGBoost) and support vector regression (SVR), is similarly explored in this\nstudy for estimation of properties of subgrade soil modified with hydrated lime\nactivated rice husk ash (HARSH). Using 121 experimental data samples of varying\nproportions of HARSH, plastic limit, liquid limit, plasticity index, clay\nactivity, optimum moisture content, and maximum dry density as input for CBR,\nUCS and R estimation, four evaluation metrics namely coefficient of\ndetermination (R2), root mean squared error (RMSE), mean absolute error (MAE)\nand mean absolute percentage error (MAPE) are used to evaluate the models'\nperformance. The results indicate that XGBoost outperformed CatBoost and SVR in\nestimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in\nestimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in\nestimating the CBR and R with R2 of 0.9997 respectively. On the other hand,\nCatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature\nsensitivity analysis shows that the three machine learning techniques are\nunanimous that increasing HARSH proportion lead to values of the estimated\nproperties respectively. A comparison with previous results also shows\nsuperiority of XGBoost in estimating subgrade properties.",
        "Point tracking is becoming a powerful solver for motion estimation and video\nediting. Compared to classical feature matching, point tracking methods have\nthe key advantage of robustly tracking points under complex camera motion\ntrajectories and over extended periods. However, despite certain improvements\nin methodologies, current point tracking methods still struggle to track any\nposition in video frames, especially in areas that are texture-less or weakly\ntextured. In this work, we first introduce metrics for evaluating the texture\nintensity of a 3D object. Using these metrics, we classify the 3D models in\nShapeNet into three levels of texture intensity and create GIFT, a challenging\nsynthetic benchmark comprising 1800 indoor video sequences with rich\nannotations. Unlike existing datasets that assign ground truth points\narbitrarily, GIFT precisely anchors ground truth on classified target objects,\nensuring that each video corresponds to a specific texture intensity level.\nFurthermore, we comprehensively evaluate current methods on GIFT to assess\ntheir performance across different texture intensity levels and analyze the\nimpact of texture on point tracking.",
        "Identifying the unknown convolutional code corresponding to the given\nintercepted data is an important problem in military surveillance and in\nwireless communication. While a variety of code identification algorithms are\navailable in the literature, the key contribution of our work lies in the novel\nsolution and the corresponding analysis. In this paper, we focus on the\nsituation when the given data corresponds to either of the two potential\nconvolutional codes and the goal is to detect the correct code. We first\nprovide a new interpretation of the convolutional code as a Markov chain, which\nis more suitable for analyzing the code detection problem. Our problem then\ngets reduced to identifying between the two Markov chains. We provide the\nclosed-form expressions for the corresponding state transition matrices and\nestimate the error exponent for the underlying likelihood ratio test (LRT). We\nalso provide a computationally efficient BCJR-based method for computing the\nlikelihoods required for the LRT. We observe that BCJR-based likelihoods suffer\nfrom numerical issues for a longer data sequence, and hence, in this case, we\ndesign neural networks that have been found to achieve the optimal performance\nof the LRT.",
        "The flow within adhering droplets subjected to external shear flows has a\nsignificant influence on the stability and eventual detachment of the droplets\nfrom the surface. Most commonly, the velocity field inside adhering droplets is\nmeasured by means of particle image velocimetry (PIV), which requires a\ncorrection step for distortion caused by refraction of light at the gas-liquid\ninterface. Current methods for distortion correction based on ray tracing are\nlimited to low external flow velocities. However, the ray-tracing method can be\nextended to arbitrarily deformed droplet shapes if the instantaneous\nthree-dimensional droplet interface is availble. In the present work, a\npreviously introduced method for the image-based reconstruction of gas-liquid\ninterfaces by means of deep learning is adapted to determine the instantaneous\ninterface of adhering droplets in external shear flows. In this regard, a\npurposefully developed optical measurement technique based on the shadowgraphy\nmethod is employed that encodes additional three-dimensional (3D) information\nof the interface in the images via glare points from lateral light sources. On\nthe basis of the images recorded in the experiments, the volumetric shape of\nthe droplet is reconstructed by a neural network that was trained on the\nspatio-temporal dynamics of the gas-liquid interface from a synthetic dataset\nobtained by numerical simulation. The results for experiments with adhering\ndroplets at different velocities of external flow demonstrate that the\ncombination of the learned droplet geometry with the depth encoding through the\nglare points facilitates a robust and flexible reconstruction. The proposed\nmethod reconstructs the instantaneous three-dimensional interface of adhering\ndroplets at both high resolution and spatial accuracy and thereby enables the\ndistortion correction of PIV measurements at high external flow velocities.",
        "We discuss the fine-tunings of nuclear reactions in the Big Bang and in stars\nand draw some conclusions on the emergence of the light elements and the\nlife-relevant elements carbon and oxygen. We also stress how to improve these\ncalculations in the future. This requires a concerted effort of different\ncommunities, especially in nuclear reaction theory, lattice QCD for few-nucleon\nsystems, stellar evolution calculations, particle physics and philosophy.",
        "Spiking Neural Networks (SNNs) are promising bio-inspired third-generation\nneural networks. Recent research has trained deep SNN models with accuracy on\npar with Artificial Neural Networks (ANNs). Although the event-driven and\nsparse nature of SNNs show potential for more energy efficient computation than\nANNs, SNN neurons have internal states which evolve over time. Keeping track of\nSNN states can significantly increase data movement and storage requirements,\npotentially losing its advantages with respect to ANNs. This paper investigates\nthe energy effects of having neuron states, and how it is influenced by the\nchosen mapping to realistic hardware architectures with advanced memory\nhierarchies. Therefore, we develop STEMS, a mapping design space exploration\ntool for SNNs. STEMS models SNN's stateful behavior and explores intra-layer\nand inter-layer mapping optimizations to minimize data movement, considering\nboth spatial and temporal SNN dimensions. Using STEMS, we show up to 12x\nreduction in off-chip data movement and 5x reduction in energy (on top of\nintra-layer optimizations), on two event-based vision SNN benchmarks. Finally,\nneuron states may not be needed for all SNN layers. By optimizing neuron states\nfor one of our benchmarks, we show 20x reduction in neuron states and 1.4x\nbetter performance without accuracy loss.",
        "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.",
        "Many approaches to multi-robot coordination are susceptible to failure due to\ncommunication loss and uncertainty in estimation. We present a real-time\ncommunication-free distributed algorithm for navigating robots to their desired\ngoals certified by control barrier functions, that model and control the\nonboard sensing behavior to keep neighbors in the limited field of view for\nposition estimation. The approach is robust to temporary tracking loss and\ndirectly synthesizes control in real time to stabilize visual contact through\ncontrol Lyapunov-barrier functions. The main contributions of this paper are a\ncontinuous-time robust trajectory generation and control method certified by\ncontrol barrier functions for distributed multi-robot systems and a discrete\noptimization procedure, namely, MPC-CBF, to approximate the certified\ncontroller. In addition, we propose a linear surrogate of high-order control\nbarrier function constraints and use sequential quadratic programming to solve\nMPC-CBF efficiently. We demonstrate results in simulation with 10 robots and\nphysical experiments with 2 custom-built UAVs. To the best of our knowledge,\nthis work is the first of its kind to generate a robust continuous-time\ntrajectory and controller concurrently, certified by control barrier functions\nutilizing piecewise splines.",
        "Iterative feature space optimization involves systematically evaluating and\nadjusting the feature space to improve downstream task performance. However,\nexisting works suffer from three key limitations:1) overlooking differences\namong data samples leads to evaluation bias; 2) tailoring feature spaces to\nspecific machine learning models results in overfitting and poor\ngeneralization; 3) requiring the evaluator to be retrained from scratch during\neach optimization iteration significantly reduces the overall efficiency of the\noptimization process. To bridge these gaps, we propose a gEneralized Adaptive\nfeature Space Evaluator (EASE) to efficiently produce optimal and generalized\nfeature spaces. This framework consists of two key components: Feature-Sample\nSubspace Generator and Contextual Attention Evaluator. The first component aims\nto decouple the information distribution within the feature space to mitigate\nevaluation bias. To achieve this, we first identify features most relevant to\nprediction tasks and samples most challenging for evaluation based on feedback\nfrom the subsequent evaluator. This decoupling strategy makes the evaluator\nconsistently target the most challenging aspects of the feature space. The\nsecond component intends to incrementally capture evolving patterns of the\nfeature space for efficient evaluation. We propose a weighted-sharing\nmulti-head attention mechanism to encode key characteristics of the feature\nspace into an embedding vector for evaluation. Moreover, the evaluator is\nupdated incrementally, retaining prior evaluation knowledge while incorporating\nnew insights, as consecutive feature spaces during the optimization process\nshare partial information. Extensive experiments on fourteen real-world\ndatasets demonstrate the effectiveness of the proposed framework. Our code and\ndata are publicly available.",
        "In this paper, we investigate a Liouville-type theorem for the MHD equations\nusing Saint-Venant type estimates. We show that \\( (u, B) \\) is a trivial\nsolution if the growth of the \\( L^s \\) mean oscillation of the potential\nfunctions for both the velocity and magnetic fields are controlled. Our growth\nassumption is weaker than those previously known for similar results. The main\nidea is to refine the Saint-Venant type estimates using the Froullani integral.",
        "Significant obstacles exist in scientific domains including genetics, climate\nmodeling, and astronomy due to the management, preprocess, and training on\ncomplicated data for deep learning. Even while several large-scale solutions\noffer distributed execution environments, open-source alternatives that\nintegrate scalable runtime tools, deep learning and data frameworks on\nhigh-performance computing platforms remain crucial for accessibility and\nflexibility. In this paper, we introduce Deep Radical-Cylon(RC), a\nheterogeneous runtime system that combines data engineering, deep learning\nframeworks, and workflow engines across several HPC environments, including\ncloud and supercomputing infrastructures. Deep RC supports heterogeneous\nsystems with accelerators, allows the usage of communication libraries like\nMPI, GLOO and NCCL across multi-node setups, and facilitates parallel and\ndistributed deep learning pipelines by utilizing Radical Pilot as a task\nexecution framework. By attaining an end-to-end pipeline including\npreprocessing, model training, and postprocessing with 11 neural forecasting\nmodels (PyTorch) and hydrology models (TensorFlow) under identical resource\nconditions, the system reduces 3.28 and 75.9 seconds, respectively. The design\nof Deep RC guarantees the smooth integration of scalable data frameworks, such\nas Cylon, with deep learning processes, exhibiting strong performance on cloud\nplatforms and scientific HPC systems. By offering a flexible, high-performance\nsolution for resource-intensive applications, this method closes the gap\nbetween data preprocessing, model training, and postprocessing.",
        "The nearest-neighbour or local mass terms in theory space among quantum\nfields, with their generic disordered values, are known to lead to the\nlocalisation of mass eigenstates, analogous to Anderson localisation in a\none-dimensional lattice. This mechanism can be used to create an exponential\nhierarchy in the coupling between two fields by placing them at opposite ends\nof the lattice chain. Extending this mechanism, we show that when copies of\nsuch fields are appropriately attached to the lattice chain, it leads to the\nemergence of multiple massless modes. These vanishing masses are a direct\nconsequence of the locality of interactions in theory space. The latter may\nbreak down in an ordered and deterministic manner through quantum effects if\nadditional interactions exist among the chain fields. Such non-locality can\ninduce small masses for the otherwise massless modes without necessarily\ndelocalising the mass eigenstates. We provide examples of interactions that\npreserve or even enhance localisation. Applications to flavour hierarchies,\nneutrino mass, and the $\\mu$-problem in supersymmetric theories are discussed.",
        "Entropy estimation plays a significant role in biology, economics, physics,\ncommunication engineering and other disciplines. It is increasingly used in\nsoftware engineering, e.g. in software confidentiality, software testing,\npredictive analysis, machine learning, and software improvement. However\naccurate estimation is demonstrably expensive in many contexts, including\nsoftware. Statisticians have consequently developed biased estimators that aim\nto accurately estimate entropy on the basis of a sample. In this paper we apply\n18 widely employed entropy estimators to Shannon measures useful to the\nsoftware engineer: entropy, mutual information and conditional mutual\ninformation. Moreover, we investigate how the estimators are affected by two\nmain influential factors: sample size and domain size. Our experiments range\nover a large set of randomly generated joint probability distributions and\nvarying sample sizes, rather than choosing just one or two well known\nprobability distributions as in previous investigations.\n  Our most important result is identifying that the Chao-Shen and\nChao-Wang-Jost estimators stand out for consistently converging more quickly to\nthe ground truth, regardless of domain size and regardless of the measure used.\nThey also tend to outperform the others in terms of accuracy as sample sizes\nincrease. This discovery enables a significant reduction in data collection\neffort without compromising performance.",
        "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements.",
        "This work is an extension of a previous publication [1] where we fitted the\nstrong coupling $\\alpha_s$ together with the non-perturbative parameter\n$\\alpha_0$ from event-shape and jet-shape distributions using power corrections\ncomputed in the three-jet region. In ref. [1] only ALEPH data at the $Z$-pole\nwere used in the fit. Here, instead, we include a large data sample from\nvarious $e^+e^-$ experiments at energies ranging from 22 to 207 GeV and\nrevisited the treatment of theoretical uncertainties. We find that the\ninclusion of different energies, while not changing the central fit result\nconsiderably, helps to disentangle the dependence of perturbative and\nnon-perturbative corrections. Our best fit result is $\\alpha_s(M_Z) = 0.1181\n(+0.0002 -0.0005) (+0.0018 -0.0021)$, where the first error includes\nexperimental uncertianties and the second one includes uncertainties associated\nwith scale variation, mass effects, fit limits, non-perturbative schemes and\nnon-perturbative uncertainties."
      ]
    }
  },
  {
    "id":2412.02083,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Learning internal representations by back-propagating errors",
    "start_abstract":"We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Quantum algorithms for supervised and unsupervised machine learning"
      ],
      "abstract":[
        "Machine-learning tasks frequently involve problems of manipulating and classifying large numbers vectors in high-dimensional spaces. Classical algorithms for solving such typically take time polynomial the number dimension space. Quantum computers are good at tensor product This paper provides supervised unsupervised quantum machine learning cluster assignment finding. can logarithmic both their dimension, an exponential speed-up over classical algorithms."
      ],
      "categories":[
        "Quantum Physics"
      ]
    },
    "list":{
      "title":[
        "Financial Adviser Misconduct and Labor Market Penalties: Uncovering\n  Racial Disparities in the Absence of Gender Gaps",
        "Geometric origin of self-intersection points in non-Hermitian energy\n  spectra",
        "$L^{p}-L^{q}$ existence for the open compressible MHD system",
        "A New Approach for Fourier Extension Based on Weighted Generalized\n  Inverse",
        "Analog Quantum Teleportation",
        "An estimate for $\\beta$-Hermite ensembles via the zeros of Hermite\n  polynomials",
        "Asymptotic coefficients of Weil-Petersson volumes in the large genus",
        "A search for sterile neutrinos in interacting dark energy models using\n  DESI baryon acoustic oscillations and DES supernovae data",
        "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA). III. NGC 288\n  as the first Splashed globular cluster",
        "Decoding Financial Health in Kenyas' Medical Insurance Sector: A\n  Data-Driven Cluster Analysis",
        "Path degeneracy and applications",
        "The latest monthly highs suggest that the 1.5{\\deg}C Paris Agreement\n  threshold will probably be exceeded before 2028",
        "Strain-Induced Optical and Molecular Transformations in PET Films for\n  Organic Electronic Applications",
        "Continuous Variable Quantum MacWilliams Identities",
        "Caught in the Act of Quenching: A Population of Post-Starburst\n  Ultra-Diffuse Galaxies",
        "Coboundaries of 3-IETs",
        "Relative Entropy Methods for Calculating Committors",
        "Pervasiveness of $\\mathcal{L}^r(E,F)$ in $\\mathcal{L}^r(E,F^{\\delta})$",
        "Electric Power Enhancement using Spin-Polarized Fuel in Fusion Power\n  Plants",
        "Fault tolerance for metric dimension and its variants",
        "Enhanced and Efficient Extraction of Uranyl Ions from Aqueous Waste\n  through Graphene\/CNT-PAMAM Nanocomposites",
        "On Pancyclicity in a Mixed Model for Domination Reconfiguration",
        "On counting numerical semigroups by maximum primitive and Wilf's\n  conjecture",
        "2024 'Key Reflections' on the 1824 Sadi Carnot's 'Reflexions' and 200\n  Year Legacy",
        "On conservative, stable boundary and coupling conditions for diffusion\n  equations I -- The conservation property for explicit schemes",
        "Subtree Distances, Tight Spans and Diversities",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "Object Detection with Deep Learning for Rare Event Search in the GADGET\n  II TPC",
        "Orbits of photon in Bardeen-boson stars and their frozen states"
      ],
      "abstract":[
        "Using a comprehensive matched employer-employee dataset for U.S. financial\nadvisers from 2008 to 2018, we revisit established evidence on labor market\npenalties following financial misconduct. Prior studies report that female\nadvisers are 20% more likely to exit their firms following misconduct and that\nsimilar disparities exist for non-white advisers. However, by disaggregating\nmisconduct into distinct disclosure events - differentiating those that nearly\nalways trigger job terminations from those that do not - we show that the\napparent gender gap vanishes, while significant racial disparities persist.\nSpecifically, non-white advisers face approximately 24% higher job separation\nrates than their white counterparts. Robustness checks confirm these findings\nacross alternative specifications, suggesting that race-based differential\ntreatment in the labor market is a distinct phenomenon warranting further\ninvestigation.",
        "Unlike Hermitian systems, non-Hermitian energy spectra under periodic\nboundary conditions can form closed loops in the complex energy plane, a\nphenomenon known as point gap topology. In this paper, we investigate the\nself-intersection points of such non-Hermitian energy spectra and reveal their\ngeometric origins. We rigorously demonstrate that these self-intersection\npoints result from the intersection of the auxiliary generalized Brillouin zone\nand the Brillouin zone in one-band systems, as confirmed by an extended\nHatano-Nelson model. This finding is further generalized to multi-band systems,\nillustrated through a non-Hermitian Su-Schrieffer-Heeger model. Moreover, we\naddress multiple self-intersection points and derive the geometric conditions\nfor general n-fold self-intersection points. Our results enhance the\nfundamental understanding of generic non-Hermitian quantum systems and provide\ntheoretical support for further experimental investigations of energy\nself-intersection points.",
        "We study the local existence of solutions to the magnetohydrodynamics (MHD)\nsystem describing the motion of a compressible, viscous, electrically and heat\nconducting fluid in the $L^p-L^q$ class with inhomogeneous boundary conditions.\nThe open system is allowed to receive incoming matter from the outside through\n(part of) the boundary which we refer to as an inflow boundary. This setup\nbrings about a difficulty in estimating the regularity of the density $\\varrho$\nwhich we remedy by assuming appropriate hypotheses on the velocity field,\ndomain boundary and on the boundary and initial data of $\\varrho$. The main\nresult ensures the local well-posedness of the full MHD system which is shown\nthrough a linearization combined with a Banach fixed-point theorem.",
        "This paper examines the Fourier extension from a new perspective of solving\nthe compact operator equation with perturbed data. By converting the\napproximation target from the best approximate solution to the weighted best\napproximate solution, the oscillation in the extended region has been overcome.\nThe error estimation of the solution is theoretically established. Furthermore,\nwe point out the difficulties faced by the original weighted operator in\ncalculation due to the limitation of machine precision and propose an effective\ncorrection operator. The relevant parameters involved in the method are further\ntested, and finally the effectiveness of the method is verified through\nnumerical experiments.",
        "Digital teleportation protocols make use of entanglement, local measurements\nand a classical communication channel to transfer quantum states between remote\nparties. We consider analog teleportation protocols, where classical\ncommunication is replaced by transmission through a noisy quantum channel. We\nshow that analog teleportation protocols outperform digital protocols if and\nonly if Alice and Bob are linked by a channel that does not reduce entanglement\nwhen applied to a part of the resource state. We first derive general\nanalytical results in the broader context of Gaussian-channel simulation. Then,\nwe apply it to the quantum teleportation of a uniformly distributed codebook of\ncoherent states, showing that an analog protocol is optimal for a wide range of\ncommunication channel transmissivities. Our result contributes to mitigating\nnoise in the intermediate case when the communication channel is far from being\nideal but is not too lossy, as is the case of cryogenic links in microwave\nsuperconducting circuits.",
        "Let $X$ be an $N$-dimensional random vector which describes the ordered\neigenvalues of a $\\beta$-Hermite ensemble, and let $z$ the vector containing\nthe ordered zeros of the Hermite poynomial $H_N$. We present an explicit\nestimate for $P(\\|X-z\\|_2\\ge\\epsilon)$ for small $\\epsilon>0$ and large\nparameters $\\beta$. The proof is based on a central limit theorem for these\nensembles for $\\beta\\to\\infty$ with explicit eigenvalues of the covariance\nmatrices of the limit. The estimate is similar to previous estimates of Dette\nand Imhof (2009).",
        "Mirzakhani-Zograf proved the large genus asymptotic expansions of\nWeil-Petersson volumes and showed that the asymptotic coefficients are\npolynomials in $\\mathbb Q[\\pi^{-2},\\pi^2]$. They also conjectured that these\nare actually polynomials in $\\mathbb Q[\\pi^{-2}]$. In this paper, we prove\nMirzakhani-Zograf's conjecture.",
        "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless\/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
        "The globular clusters (GCs) system of the Milky Way (MW) comprises a mixture\nof both in situ and accreted clusters. Tracing the origin of GCs provides\ninvaluable insights into the formation history of the MW. However, reconciling\ndiverse strands of evidence is often challenging: a notable example is NGC 288,\nwhere despite significant efforts in the literature, the available\nchrono-chemodynamical data have yet to provide a definitive conclusion\nregarding its origin. On one side, all post-Gaia dynamical studies indicate an\naccreted origin for NGC 288 from in the Gaia-Sausage-Enceladus (GSE) dwarf\ngalaxy. On the other, NGC 288 has been found to be 2.5 Gyr older than other GSE\nGCs at the same metallicity, this suggesting a different, possibly in situ\norigin. In this work, we address the unresolved question on the origin of NGC\n288 by analyzing its chrono-chemical properties in an unprecedentedly\nhomogeneous framework. First, we compare the location of NGC 288 in the\nage-metallicity plane with that of other two in situ GCs at similar\nmetallicity, namely NGC 6218 and NGC 6362. The age estimates obtained within\nthe homogeneous framework of the CARMA collaboration show that the three\nclusters are coeval, this reinforcing the contrast with the dynamical\ninterpretation. Then, we compare the abundances with the sample of in situ and\naccreted clusters at similar metallicity presented in Ceccarelli et al. 2024,\nfinding again consistency with the chemistry of in situ systems. To reconcile\nthese results with its orbital properties, we propose a scenario where NGC 288\nformed in the proto-disc of the MW, and then was dynamically heated by the\ninteraction with the GSE merger, a fate similar to that of proto-disc stars\nexperiencing the so-called Splash event. NGC 288 therefore demonstrates the\nimportance of a homogeneous chrono-chemodynamical information in the\ninterpretation of the origin of MW GCs.",
        "This study examines insurance companies' financial performance and reporting\ntrends within the medical sector using advanced clustering techniques to\nidentify distinct patterns. Four clusters were identified by analyzing\nfinancial ratios and time series data, each representing unique financial\nperformance and reporting consistency combinations. Dynamic Time Warping (DTW)\nand KMeans clustering were employed to capture temporal variations and uncover\nkey insights into company behaviors. The findings reveal that resilient\nperformers consistently report and have financial stability, making them\nreliable options for policyholders. In contrast, clusters of underperforming\ncompanies and those with reporting gaps highlight operational challenges and\nissues related to data consistency. These insights emphasize the importance of\ntransparency and timely reporting to ensure the sector's resilience. This study\ncontributes to the literature by integrating time series analysis into\nfinancial clustering, offering practical recommendations for improving data\ngovernance and financial stability in the insurance sector. Future research\ncould further investigate non-financial indicators and explore alternative\nclustering methods to provide a deeper understanding of performance dynamics.",
        "In this work, we relate girth and path-degeneracy in classes with\nsub-exponential expansion, with explicit bounds for classes with polynomial\nexpansion and proper minor-closed classes that are tight up to a constant\nfactor (and tight up to second order terms if a classical conjecture on\nexistence of $g$-cages is verified). As an application, we derive bounds on the\ngeneralized acyclic indices, on the generalized arboricities, and on the weak\ncoloring numbers of high-girth graphs in such classes. Along the way, we prove\na conjecture proposed in [T.~Bartnicki et al., Generalized arboricity of graphs\nwith large girth, Discrete Mathematics 342 (2019), no.~5, 1343--1350.], which\nasserts that, for every integer $k$, there is an integer $g(p,k)$ such that\nevery $K_k$ minor-free graph with girth at least $g(p,k)$ has $p$-arboricity at\nmost $p+1$.",
        "An attempt is made to estimate and forecast the trend of the global annual\nand monthly mean temperatures. The results of a conventional statistical\nanalysis suggest that in the absence of unforeseeable events such as a sudden\nacceleration in the rate of warming, the 1.5{\\deg}C Paris Agreement threshold\ncould be exceeded between 2027 and 2031. However, carrying out a proper\nseasonal adjustment and examining the autocorrelation structure carefully, we\nfind in a subsequent purely statistical simulation study that even the\npessimistic scenario of a breach in late 2027 is inconsistent with the recent\nmonthly highs, which means that it will probably happen much sooner.",
        "Poly(ethylene terephthalate) (PET) films are widely used in flexible\nelectronics and optoelectronics, where their mechanical durability and optical\nperformance under strain are essential for device reliability. This study\ninvestigates the impact of applied mechanical strain on the optical and\nmolecular properties of PET at room temperature,using UV-Vis absorption and\nRaman spectroscopy. The work explores how varying strain levels, from 0%\n(unstretched) to 30%, affect the transparency, vibrational modes, and molecular\nreorganization within PET films. UV-Vis absorbance measurements reveal that\nstrain induces significant changes in the light transmission properties of PET,\nparticularly in the visible range, and increases absorption in the UVA and\nvisible region by up to 100%. Raman spectra indicate that strain levels higher\nthan 5% lead to irreversible shifts of vibrational lines, accompanied by an\nincrease of their full width at half maximum (FWHM), suggesting molecular\nreorientation and crystallinity changes. The phonon mode coupled with C-O\nstretching [O-CH2] shows the strongest response to applied mechanical stress.\nThis study provides a comprehensive understanding of strain-induced optical and\nstructural alterations in PET, with implications for improving the mechanical\nand optical performance of PET-based devices in strainsensitive applications,\nsuch as organic solar cells (OSCs), organic light-emitting diodes (OLEDs), and\nflexible sensors.",
        "We derive bounds on general quantum error correcting codes against the\ndisplacement noise channel. The bounds limit the distances attainable by codes\nand also apply in an approximate setting. Our main result is a quantum analogue\nof the classical Cohn-Elkies bound on sphere packing densities attainable in\nEuclidean space. We further derive a quantum version of Levenshtein's sphere\npacking bound and argue that Gottesman--Kitaev--Preskill (GKP) codes based on\nthe $E_8$ and Leech lattices achieve optimal distances. The main technical tool\nis a continuous variable version of the quantum MacWilliams identities, which\nwe introduce. The identities relate a pair of weight distributions which can be\nobtained for any two trace-class operators. General properties of these weight\ndistributions are discussed, along with several examples.",
        "We report the discovery of post-starburst ultra-diffuse galaxies (UDGs),\nidentified through spectroscopic analysis with KCWI at the Keck II Telescope.\nOur analysis is based on a sample of 44 candidate UDGs selected from the\nSystematically Measuring Ultra-Diffuse Galaxies (SMUDGes) program. Our measured\nspectroscopic redshifts reveal $\\sim 80\\%$ of the entire KCWI sample exhibit\nlarge physical sizes ($R_{e} \\gtrsim 1~{\\rm kpc}$) and low surface brightnesses\n($24 \\lesssim \\mu_{0,g} \\lesssim 25$ mag arcsec$^{-2}$) which categorize them\nas UDGs. We find $20\\%$ of the confirmed UDG population contain post-starburst\n(or K+A) features, characterized by minimal to no emission in H$\\beta$\nindicative of quenched star formation and a predominant presence of spectral\nA-type stars. Studying the local environments of the post-starburst UDGs, we\nfind that half are isolated systems, including two systems that reside\n$2-3~R_{\\rm vir}$ away from potential nearby massive hosts ($M_{\\star}\n>10^{10}~\\mathrm{M}_{\\odot}$). Without the influence of external environmental\nmechanisms, these post-starburst UDGs may represent systems experiencing star\nformation feedback such that a recent burst may lead to (at least temporary)\nquenching. Overall, our results highlight the potentially diverse quenching\npathways of UDGs in the local Universe.",
        "In this note, we investigate the coboundaries of interval exchange\ntransformations of 3 intervals (3-IETs). More precisely, we show that a\ndifferentiable function with absolutely continuous derivative with bounded\nvariation, whose integral and integral of its derivative is 0, is a coboundary\nfor typical 3-IET if and only if the values at the endpoints of the domain are\nzero. We also show the existence of rare counterexamples for both cases of\npossible values at the endpoints of the interval. We obtain our result by\nstudying the properties of associated skew products.",
        "Motivated by challenges arising in molecular simulation, we analyze and\ndevelop methods of computing reactive trajectories and committor functions for\nsystems described by the overdamped Langevin dynamics. Our main technical\nadvance is a new loss function that measures the accuracy of approximations to\nthe committor function related to a given chemical reaction or other rare\ntransition event. Our loss admits a simple interpretation in terms of the\ndistribution of reactive trajectories, and it can be computed in practice to\ncompare the accuracies of different approximations of the committor. We also\nderive a method of calculating committors by direct minimization of the loss\nvia stochastic gradient descent.",
        "Let $E, F$ be Archimedean Riesz spaces, and let $F^{\\delta}$ denote an order\ncompletion of $F$. In this note, we provide necessary conditions under which\nthe space of regular operators $\\mathcal{L}^r(E, F)$ is pervasive in\n$\\mathcal{L}^r(E, F^{\\delta})$. Pervasiveness of $\\mathcal{L}^r(E, F)$ in\n$\\mathcal{L}^r(E, F^{\\delta})$ implies that the Riesz completion of $\n\\mathcal{L}^r(E, F)$ can be realized as a Riesz subspace of $ \\mathcal{L}^r(E,\nF^{\\delta}$. It also ensures that the regular part of the space of order\ncontinuous operators $\\mathcal{L}^{oc}(E, F)$ forms a band of $\\mathcal{L}^r(E,\nF)$. Furthermore, the positive part $T^+$ of any operator $T \\in\n\\mathcal{L}^r(E, F)$, provided it exists, is given by the Riesz-Kantorovich\nformula. The results apply in particular to cases where $E = \\ell_0^{\\infty}$,\n$E = c$, or $F$ is atomic, and they provide solutions to some problems posed in\n[3] and [16].",
        "Using a range of fusion power plant (FPP) concepts, we demonstrate that\nspin-polarized fuel (SPF) can significantly enhance net electric power output,\noften by many multiples. Notably, the electric power gain from SPF generally\nexceeds the corresponding increase in thermal fusion power. Plants close to\nengineering breakeven stand to benefit most, where even modest boosts in fusion\npower produce disproportionately larger gains in net electricity. As a\nrepresentative example, a 25% increase in fusion power via SPF could allow an\nITER-like device (with an added turbine to recover thermal fusion power) to\nachieve engineering breakeven. These findings strongly motivate the development\nof spin-polarized fuel for FPPs.",
        "Hernando et al. (2008) introduced the fault-tolerant metric dimension\n$\\text{ftdim}(G)$, which is the size of the smallest resolving set $S$ of a\ngraph $G$ such that $S-\\left\\{s\\right\\}$ is also a resolving set of $G$ for\nevery $s \\in S$. They found an upper bound $\\text{ftdim}(G) \\le \\dim(G) (1+2\n\\cdot 5^{\\dim(G)-1})$, where $\\dim(G)$ denotes the standard metric dimension of\n$G$. It was unknown whether there exists a family of graphs where\n$\\text{ftdim}(G)$ grows exponentially in terms of $\\dim(G)$, until recently\nwhen Knor et al. (2024) found a family with $\\text{ftdim}(G) =\n\\dim(G)+2^{\\dim(G)-1}$ for any possible value of $\\dim(G)$. We improve the\nupper bound on fault-tolerant metric dimension by showing that $\\text{ftdim}(G)\n\\le \\dim(G)(1+3^{\\dim(G)-1})$ for every connected graph $G$. Moreover, we find\nan infinite family of connected graphs $J_k$ such that $\\dim(J_k) = k$ and\n$\\text{ftdim}(J_k) \\ge 3^{k-1}-k-1$ for each positive integer $k$. Together,\nour results show that \\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ }\n\\dim(G) = k} \\frac{\\log_3(\\text{ftdim}(G))}{k} \\right) = 1.\\] In addition, we\nconsider the fault-tolerant edge metric dimension $\\text{ftedim}(G)$ and bound\nit with respect to the edge metric dimension $\\text{edim}(G)$, showing that\n\\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ } \\text{edim}(G) = k}\n\\frac{\\log_2(\\text{ftedim}(G))}{k} \\right) = 1.\\] We also obtain sharp extremal\nbounds on fault-tolerance for adjacency dimension and $k$-truncated metric\ndimension. Furthermore, we obtain sharp bounds for some other extremal problems\nabout metric dimension and its variants. In particular, we prove an equivalence\nbetween an extremal problem about edge metric dimension and an open problem of\nErd\\H{o}s and Kleitman (1974) in extremal set theory.",
        "The increasing threat of uranium contamination to environmental and human\nhealth due to its radiotoxicity demands the development of novel and efficient\nadsorbents for remediation. In this study, we investigated the potential of\npoly(amidoamine) (PAMAM) dendrimers of generations 1 to 4 (G1 - G4)\nfunctionalized with graphene and carbon nanotubes (CNTs) as adsorbents for\nuranyl ion removal from aqueous solutions. By combining atomistic molecular\ndynamics (MD) simulations with experimental validation, we examined the\ninfluence of pH, uranyl ion concentration, and dendrimer generation on\nadsorption behavior. Our study revealed that uranyl ion adsorption is greater\nwhen PAMAM is grafted onto graphene\/CNT than pristine PAMAM. However,\nPAMAM-grafted CNTs exhibit superior adsorption capacity at specific uranyl\nconcentrations due to their curvature and abundant accessible binding sites.\nHigher-generation PAMAM dendrimers grafted onto graphene\/CNTs exhibit greater\nadsorption capacity due to the increased availability of binding sites, which\nis consistent with experimental observations. The adsorption capability of\nuranyl ions in all four generations of the PAMAM dendrimer increased as the\nconcentration of uranyl ions increased. Adsorption capacity increases with\nincreasing uranyl ion concentration, and adsorption occurs on both PAMAM and\ngraphene\/CNT surfaces, with saturation observed at higher concentrations. This\nstudy provides insights into the adsorption mechanisms and highlights the\npotential of PAMAM-based nanocomposites for efficient uranyl ion extraction and\nenvironmental remediation.",
        "A new model for domination reconfiguration is introduced which combines the\nproperties of the preexisting token addition\/removal (TAR) and token sliding\n(TS) models. The vertices of the TARS-graph correspond to the dominating sets\nof $G$, where two vertices are adjacent if and only if they are adjacent via\neither the TAR reconfiguration rule or the TS reconfiguration rule. While the\ndomination reconfiguration graph obtained by using only the TAR rule (sometimes\ncalled the dominating graph) will never have a Hamilton cycle, we show that for\nsome classes of graphs $G$, by adding a relatively small number of token\nsliding edges, the resulting graph is not only hamiltonian, but is in fact\npancyclic. In particular, if the underlying graphs are trees, complete graphs,\nor complete multipartite graphs, their TARS-graphs will be pancyclic. We also\nprovide pancyclicity results for TARS-graphs of graph unions and joins, and\nconclude by posing the question: Are all TARS-graphs pancyclic?",
        "We introduce a new way of counting numerical semigroups, namely by their\nmaximum primitive, and show its relation with the counting of numerical\nsemigroups by their Frobenius number. For any positive integer $n$, let $A_{n}$\ndenote the number of numerical semigroups whose maximum primitive is $n$, and\nlet $N_{n}$ denote the number of numerical semigroups whose Frobenius number is\n$n$. We show that the sequences $(A_{n})$ and $(N_{n})$ are M\\\"obius transforms\nof one another. We also establish that almost all numerical semigroups with\nlarge enough maximum primitive satisfy Wilf's conjecture. A crucial step in the\nproof is a result of independent interest: a numerical semigroup $S$ with\nmultiplicity $\\mathrm{m}$ such that $|S\\cap (\\mathrm{m},2 \\mathrm{m})|\\geq\n\\sqrt{3\\mathrm{m}}$ satisfies Wilf's conjecture.",
        "This author is not a philosopher nor historian of science, but an engineering\nthermodynamicist. In that regard and in addition to various philosophical \"why\n& how\" treatises and existing historical analyses, the physical and logical\n\"what it is\" reflections, as sequential Key Points, where a key Sadi Carnot's\nreasoning infers the next one, along with novel contributions and original\ngeneralizations, are presented. We need to keep in mind that in Sadi Carnot's\ntime (early 1800s) the steam engines were inefficient (below 5%, so the heat in\nand out were comparable within experimental uncertainty, as if caloric were\nconserved), the conservation of caloric flourished (might be a fortunate\nmisconception leading to the critical analogy with the waterwheel), and many\ncritical thermal-concepts, including the conservation of energy (The First Law)\nwere not even established. Since Clausius and Kelvin earned to be \"Fathers of\nthermodynamics,\" then Sadi Carnot was 'the ingenious' \"Forefather of\nthermodynamics-to-become\".",
        "This paper introduces improved numerical techniques for addressing numerical\nboundary and interface coupling conditions in the context of diffusion\nequations in cellular biophysics or heat conduction problems in fluid-structure\ninteractions. Our primary focus is on two critical numerical aspects related to\ncoupling conditions: the preservation of the conservation property and ensuring\nstability. Notably, a key oversight in some existing literature on coupling\nmethods is the neglect of upholding the conservation property within the\noverall scheme. This oversight forms the central theme of the initial part of\nour research. As a first step, we limited ourselves to explicit schemes on\nuniform grids. Implicit schemes and the consideration of varying mesh sizes at\nthe interface will be reserved for a subsequent paper \\cite{CMW3}. Another\npaper \\cite{CMW2} will address the issue of stability.\n  We examine these schemes from the perspective of finite differences,\nincluding finite elements, following the application of a nodal quadrature\nrule. Additionally, we explore a finite volume-based scheme involving cells and\nflux considerations. Our analysis reveals that discrete boundary and flux\ncoupling conditions uphold the conservation property in distinct ways in\nnodal-based and cell-based schemes. The coupling conditions under investigation\nencompass well-known approaches such as Dirichlet-Neumann coupling, heat flux\ncoupling, and specific channel and pumping flux conditions drawn from the field\nof biophysics. The theoretical findings pertaining to the conservation property\nare corroborated through computations across a range of test cases.",
        "Metric embeddings are central to metric theory and its applications. Here we\nconsider embeddings of a different sort: maps from a set to subsets of a metric\nspace so that distances between points are approximated by minimal distances\nbetween subsets. Our main result is a characterization of when a set of\ndistances $d(x,y)$ between elements in a set $X$ have a subtree representation,\na real tree $T$ and a collection $\\{S_x\\}_{x \\in X}$ of subtrees of~$T$ such\nthat $d(x,y)$ equals the length of the shortest path in~$T$ from a point in\n$S_x$ to a point in $S_y$ for all $x,y \\in X$. The characterization was first\nestablished for {\\em finite} $X$ by Hirai (2006) using a tight span\nconstruction defined for distance spaces, metric spaces without the triangle\ninequality. To extend Hirai's result beyond finite $X$ we establish fundamental\nresults of tight span theory for general distance spaces, including the\nsurprising observation that the tight span of a distance space is hyperconvex.\nWe apply the results to obtain the first characterization of when a diversity\n-- a generalization of a metric space which assigns values to all finite\nsubsets of $X$, not just to pairs -- has a tight span which is tree-like.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "In the pursuit of identifying rare two-particle events within the GADGET II\nTime Projection Chamber (TPC), this paper presents a comprehensive approach for\nleveraging Convolutional Neural Networks (CNNs) and various data processing\nmethods. To address the inherent complexities of 3D TPC track reconstructions,\nthe data is expressed in 2D projections and 1D quantities. This approach\ncapitalizes on the diverse data modalities of the TPC, allowing for the\nefficient representation of the distinct features of the 3D events, with no\nloss in topology uniqueness. Additionally, it leverages the computational\nefficiency of 2D CNNs and benefits from the extensive availability of\npre-trained models. Given the scarcity of real training data for the rare\nevents of interest, simulated events are used to train the models to detect\nreal events. To account for potential distribution shifts when predominantly\ndepending on simulations, significant perturbations are embedded within the\nsimulations. This produces a broad parameter space that works to account for\npotential physics parameter and detector response variations and uncertainties.\nThese parameter-varied simulations are used to train sensitive 2D CNN object\ndetectors. When combined with 1D histogram peak detection algorithms, this\nmulti-modal detection framework is highly adept at identifying rare,\ntwo-particle events in data taken during experiment 21072 at the Facility for\nRare Isotope Beams (FRIB), demonstrating a 100% recall for events of interest.\nWe present the methods and outcomes of our investigation and discuss the\npotential future applications of these techniques.",
        "In a recent study [1], the Bardeen-boson star (BBS) model involving a scalar\nfield minimally coupled to Einstein gravity and a Bardeen's nonlinear\nelectromagnetic field was investigated. It was found that when the magnetic\ncharge $q$ of the electromagnetic field exceeds a certain critical value $q_c$,\na frozen Bardeen-boson star (FBBS) can be obtained with the frequency\napproaching zero. In this paper, we study the null orbits in the background of\nthe general BBS and FBBS. We find that similar to the boson star (BS), all BBSs\ndo not have the event horizon and possess complete null geodesics, allowing\nphotons to move throughout the entire spacetime of BBS. Among these BBSs, the\nFBBSs whose spacetime is very similar to that of black holes are particularly\nspecial. The null orbits around the FBBSs exhibit sharp deflections near the\ncritical horizon while becoming nearly straight inside the critical horizon.\nFurthermore, the photon in the background of FBBSs moves for a very long time\ninside the critical horizon from the perspective of an infinity viewer."
      ]
    }
  },
  {
    "id":2412.02083,
    "research_type":"basic",
    "start_id":"b8",
    "start_title":"Quantum algorithms for supervised and unsupervised machine learning",
    "start_abstract":"Machine-learning tasks frequently involve problems of manipulating and classifying large numbers vectors in high-dimensional spaces. Classical algorithms for solving such typically take time polynomial the number dimension space. Quantum computers are good at tensor product This paper provides supervised unsupervised quantum machine learning cluster assignment finding. can logarithmic both their dimension, an exponential speed-up over classical algorithms.",
    "start_categories":[
      "Quantum Physics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Learning internal representations by back-propagating errors"
      ],
      "abstract":[
        "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CBVLM: Training-free Explainable Concept-based Large Vision Language\n  Models for Medical Image Classification",
        "Regular black holes in Lovelock with degenerated ground state",
        "MITracker: Multi-View Integration for Visual Object Tracking",
        "The fibration sequences of the lattice path operad",
        "Optimal Transport-based Conformal Prediction",
        "Photometric Calibration & Spectral Validation of the Solar Ultraviolet\n  Imaging Telescope onboard Aditya-L1",
        "Solitonic vortices and black holes with vortex hair in AdS$_3$",
        "Quantum neural compressive sensing for ghost imaging",
        "Dimension-free estimates for discrete maximal functions and lattice\n  points in high-dimensional spheres and balls with small radii",
        "Fineness and smoothness of a KSBA moduli of marked cubic surfaces",
        "Wearable Haptics for a Marionette-inspired Teleoperation of Highly\n  Redundant Robotic Systems",
        "Improved Training Technique for Latent Consistency Models",
        "EXACT-CT: EXplainable Analysis for Crohn's and Tuberculosis using CT",
        "Dynamics of Quantum Correlations within the double Caldeira-Leggett\n  formalism",
        "PIMutation: Exploring the Potential of PIM Architecture for Quantum\n  Circuit Simulation",
        "Computational Assessment of Hemodynamics in Asymmetric-type Lesion of\n  Idealized Coronary Stenoses",
        "Digit quantum simulation of a fermion field in an expanding universe",
        "A new reducibility results for minihypers in finite projective\n  geometries",
        "AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End\n  Speech-to-Text Translation",
        "Unveiling Symmetry Instability induced by Topological Phase Transitions",
        "Latent-space adversarial training with post-aware calibration for\n  defending large language models against jailbreak attacks",
        "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via\n  Event-Driven Text-Code Cyclic Training",
        "An Improved Lower Bound on the Image of the 2-adic Character Map for the\n  Heisenberg Algebra via Modular Linear Differential Equations",
        "Pure Shape Dynamics: Relational General Relativity",
        "SPLD polynomial optimization and bounded degree SOS hierarchies",
        "Benchmarking global optimization techniques for unmanned aerial vehicle\n  path planning",
        "STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and\n  Causal Policy Optimization",
        "scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological\n  Profiling",
        "To Patch or Not to Patch: Motivations, Challenges, and Implications for\n  Cybersecurity"
      ],
      "abstract":[
        "The main challenges limiting the adoption of deep learning-based solutions in\nmedical workflows are the availability of annotated data and the lack of\ninterpretability of such systems. Concept Bottleneck Models (CBMs) tackle the\nlatter by constraining the final disease prediction on a set of predefined and\nhuman-interpretable concepts. However, the increased interpretability achieved\nthrough these concept-based explanations implies a higher annotation burden.\nMoreover, if a new concept needs to be added, the whole system needs to be\nretrained. Inspired by the remarkable performance shown by Large\nVision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet\neffective, methodology, CBVLM, which tackles both of the aforementioned\nchallenges. First, for each concept, we prompt the LVLM to answer if the\nconcept is present in the input image. Then, we ask the LVLM to classify the\nimage based on the previous concept predictions. Moreover, in both stages, we\nincorporate a retrieval module responsible for selecting the best examples for\nin-context learning. By grounding the final diagnosis on the predicted\nconcepts, we ensure explainability, and by leveraging the few-shot capabilities\nof LVLMs, we drastically lower the annotation cost. We validate our approach\nwith extensive experiments across four medical datasets and twelve LVLMs (both\ngeneric and medical) and show that CBVLM consistently outperforms CBMs and\ntask-specific supervised methods without requiring any training and using just\na few annotated examples. More information on our project page:\nhttps:\/\/cristianopatricio.github.io\/CBVLM\/.",
        "A new regular black hole solution for Lovelock gravity with an\n\\textit{n}-fold degenerate ground state AdS is provided. An alternative\ndefinition of the Kretschmann scalar for this theory is proposed, which is\nassociated with the gravitational tension of the Schwarzschild vacuum AdS of\nthis theory, which diverges at the radial origin. The proposed energy density\nencodes the latter's information in such a way that it takes a finite value at\nthe origin, thereby suppressing the existence of a central singularity. There\nis a value of the extremal radius, $r_{ext}$, which can be of the order of the\nPlanck length, such that for a value $r_* > r_{ext}$, just slightly greater\nthan it (also of the order of Planck), the solutions of the vacuum AdS black\nhole (BH) and our regular solution become indistinguishable. Thus, for\nrealistic values of the horizon radius, both cases mentioned are numerically\nindistinguishable. However, at short length scales such that $r < r_*$, both\nbehaviors differ. Therefore, the proposed matter sources lead to the formation\nof a de Sitter core rather than a central singularity. The thermodynamics of\nboth cases become indistinguishable for $r \\geq r_*$, \\text{i.e.}, for\nrealistic scales. However, at shorter scales $r < r_*$, quantum effects would\narise, meaning that instead of the temperature evolving to infinity as in the\nvacuum AdS BH, the matter sources proposed in this work cause the cooling of\nthe BH, through a phase transition, such that a BH remnant is reached at $T =\n0$, which represents what remains of the BH once the evaporation process halts.",
        "Multi-view object tracking (MVOT) offers promising solutions to challenges\nsuch as occlusion and target loss, which are common in traditional single-view\ntracking. However, progress has been limited by the lack of comprehensive\nmulti-view datasets and effective cross-view integration methods. To overcome\nthese limitations, we compiled a Multi-View object Tracking (MVTrack) dataset\nof 234K high-quality annotated frames featuring 27 distinct objects across\nvarious scenes. In conjunction with this dataset, we introduce a novel MVOT\nmethod, Multi-View Integration Tracker (MITracker), to efficiently integrate\nmulti-view object features and provide stable tracking outcomes. MITracker can\ntrack any object in video frames of arbitrary length from arbitrary viewpoints.\nThe key advancements of our method over traditional single-view approaches come\nfrom two aspects: (1) MITracker transforms 2D image features into a 3D feature\nvolume and compresses it into a bird's eye view (BEV) plane, facilitating\ninter-view information fusion; (2) we propose an attention mechanism that\nleverages geometric information from fused 3D feature volume to refine the\ntracking results at each view. MITracker outperforms existing methods on the\nMVTrack and GMTD datasets, achieving state-of-the-art performance. The code and\nthe new dataset will be available at\nhttps:\/\/mii-laboratory.github.io\/MITracker\/.",
        "We introduce a notion of an operad of complexity $m$, for $m \\geq 1$. Operads\nof complexity $1$ are monoids in the category of $\\mathbb{N}$-indexed\ncollections, with monoidal product given by the Day convolution, and operads of\ncomplexity $2$ are non-symmetric operads. In general, we prove that the operad\nfor operads of complexity $m$ is a suboperad of the $m$-th stage filtration of\nthe lattice path operad introduced by Batanin and Berger. Finally, we exhibit\nfibration sequences involving this new notion, extending the results of Turchin\nand Dwyer-Hess.",
        "Conformal Prediction (CP) is a principled framework for quantifying\nuncertainty in blackbox learning models, by constructing prediction sets with\nfinite-sample coverage guarantees. Traditional approaches rely on scalar\nnonconformity scores, which fail to fully exploit the geometric structure of\nmultivariate outputs, such as in multi-output regression or multiclass\nclassification. Recent methods addressing this limitation impose predefined\nconvex shapes for the prediction sets, potentially misaligning with the\nintrinsic data geometry. We introduce a novel CP procedure handling\nmultivariate score functions through the lens of optimal transport.\nSpecifically, we leverage Monge-Kantorovich vector ranks and quantiles to\nconstruct prediction region with flexible, potentially non-convex shapes,\nbetter suited to the complex uncertainty patterns encountered in multivariate\nlearning tasks. We prove that our approach ensures finite-sample,\ndistribution-free coverage properties, similar to typical CP methods. We then\nadapt our method for multi-output regression and multiclass classification, and\nalso propose simple adjustments to generate adaptive prediction regions with\nasymptotic conditional coverage guarantees. Finally, we evaluate our method on\npractical regression and classification problems, illustrating its advantages\nin terms of (conditional) coverage and efficiency.",
        "The Solar Ultraviolet Imaging Telescope (SUIT) is one of the seven payloads\non board Aditya-L1 mission of the Indian Space Research Organization (ISRO).\nSUIT provides full and partial disk images of the Sun in the 200-400 nm\nwavelength range. This would help us probe the solar atmosphere at different\nheights and understand the mass and energy transfer process between its layers.\nFor the first time, SUIT will also help us measure spatially resolved solar\nspectral irradiance at this wavelength band, which is significant for studying\nthe sun-climate relationships. To perform these studies, it is necessary to\nphotometrically calibrate the payload and validate the spectral coverage of the\nvarious bandpasses. We perform the photometric calibration and spectral\nvalidation of 8 bandpasses using light of known intensity and spectral\ncoverage. For photometric calibration, the telescope throughput is modeled\nusing sun-as-a-star spectrum from SOLSTICE and SOLSPEC. The modeled throughput\nis compared with in-lab measurements taken with light of known intensity. The\nratio of measured photoelectrons gathered with the modeled prediction agree\nwithin 20%. For spectral validation, readings are taken across the transmission\nspectrum of each filter, keeping adjacent readings independent of each other.\nThe relative intensity measured at each wavelength is seen to trace the modeled\ntelescope bandpass for that filter. These tests could not be performed for\nfilters with bandpasses operating below 250 nm (NB01, BB01 and BB02), primarily\ndue to heavy atmospheric attenuation in these wavelengths leading to decreased\nSNR of the data. The experimentally measured results agree closely with the\nmodeled values, validating SUIT's optical performance and presenting the\nreliability of the developed throughput model.",
        "We study soliton and black hole solutions with scalar hair in AdS$_3$ in a\ntheory with a Maxwell field and a charged scalar field with double trace\nboundary conditions, which can trigger the dual boundary theory to become a\nsuperconductor. We investigate the phase diagram as a function of the\ntemperature $T$ and of the double trace coupling $\\kappa$ and we find a rich\npattern of phase transitions, which can be of the Hawking-Page kind or can be\ndue to the condensation of the order parameter. We also find a transition\nbetween vortex solutions and the zero temperature limit of the black hole for a\ncritical value of the double trace coupling $\\kappa$. The Little-Park\nperiodicity is realized for the dual of the black hole solution with hair as a\nshift in the winding number and in the gauge field.",
        "Demonstrating the utility of quantum algorithms is a long-standing challenge,\nwhere quantum machine learning becomes one of the most promising candidate that\ncan be resorted to. In this study, we investigate a quantum neural compressive\nsensing algorithm for ghost imaging to showcase its utility. The algorithm\nutilizes the variational quantum circuits to reparameterize the inverse problem\nof ghost imaging and uses the inductive bias of the physical forward model to\nperform optimization. To validate the algorithm's effectiveness, we conduct\noptical ghost imaging experiments, capturing signals from objects at different\nphysical sampling rates and detection signal-to-noise ratios. The experimental\nresults show that our proposed algorithm surpasses conventional methods in both\nvisual appearance and quantitative metrics, achieving state-of-the-art\nperformance. Importantly, we observe that the quantum neural network, guided by\nprior knowledge of physics, effectively overcomes the challenge of barren\nplateau in the optimization process. The proposed algorithm demonstrates\nrobustness against various quantum noise levels, making it suitable for\nnear-term quantum devices. Our study leverages physical inductive bias guided\nvariational quantum algorithm, underscoring the potential of quantum\ncomputation in tackling a broad range of optimization and inverse problems.",
        "We prove that the discrete Hardy-Littlewood maximal function associated with\nEuclidean spheres with small radii has dimension-free estimates on\n$\\ell^p(\\mathbb{Z}^d)$ for $p\\in[2,\\infty).$ This implies an analogous result\nfor the Euclidean balls, thus making progress on a question of E.M. Stein from\nthe mid 1990s. Our work provides the first dimension-free estimates for full\ndiscrete maximal functions related to spheres and balls without relying on\ncomparisons with their continuous counterparts. An important part of our\nargument is a uniform (dimension-free) count of lattice points in\nhigh-dimensional spheres and balls with small radii. We also established a\ndimension-free estimate for a multi-parameter maximal function of a\ncombinatorial nature, which is a new phenomenon and may be useful for studying\nsimilar problems in the future.",
        "By work of Gallardo-Kerr-Schaffler, it is known that Naruki's\ncompactification of the moduli space of marked cubic surfaces is isomorphic to\nthe normalization of the Koll\\'ar, Shepherd-Barron, and Alexeev\ncompactification parametrizing pairs\n$\\left(S,\\left(\\frac{1}{9}+\\epsilon\\right)D\\right)$, with $D$ the sum of the\n$27$ marked lines on $S$, and their stable degenerations. In the current paper,\nwe show that the normalization assumption is not necessary as we prove that\nthis KSBA compactification is smooth. Additionally, we show it is a fine moduli\nspace. This is done by studying the automorphisms and the\n$\\mathbb{Q}$-Gorenstein obstructions of the stable pairs parametrized by it.",
        "The teleoperation of complex, kinematically redundant robots with\nloco-manipulation capabilities represents a challenge for human operators, who\nhave to learn how to operate the many degrees of freedom of the robot to\naccomplish a desired task. In this context, developing an easy-to-learn and\neasy-to-use human-robot interface is paramount. Recent works introduced a novel\nteleoperation concept, which relies on a virtual physical interaction interface\nbetween the human operator and the remote robot equivalent to a \"Marionette\"\ncontrol, but whose feedback was limited to only visual feedback on the human\nside. In this paper, we propose extending the \"Marionette\" interface by adding\na wearable haptic interface to cope with the limitations given by the previous\nworks. Leveraging the additional haptic feedback modality, the human operator\ngains full sensorimotor control over the robot, and the awareness about the\nrobot's response and interactions with the environment is greatly improved. We\nevaluated the proposed interface and the related teleoperation framework with\nnaive users, assessing the teleoperation performance and the user experience\nwith and without haptic feedback. The conducted experiments consisted in a\nloco-manipulation mission with the CENTAURO robot, a hybrid leg-wheel quadruped\nwith a humanoid dual-arm upper body.",
        "Consistency models are a new family of generative models capable of producing\nhigh-quality samples in either a single step or multiple steps. Recently,\nconsistency models have demonstrated impressive performance, achieving results\non par with diffusion models in the pixel space. However, the success of\nscaling consistency training to large-scale datasets, particularly for\ntext-to-image and video generation tasks, is determined by performance in the\nlatent space. In this work, we analyze the statistical differences between\npixel and latent spaces, discovering that latent data often contains highly\nimpulsive outliers, which significantly degrade the performance of iCT in the\nlatent space. To address this, we replace Pseudo-Huber losses with Cauchy\nlosses, effectively mitigating the impact of outliers. Additionally, we\nintroduce a diffusion loss at early timesteps and employ optimal transport (OT)\ncoupling to further enhance performance. Lastly, we introduce the adaptive\nscaling-$c$ scheduler to manage the robust training process and adopt\nNon-scaling LayerNorm in the architecture to better capture the statistics of\nthe features and reduce outlier impact. With these strategies, we successfully\ntrain latent consistency models capable of high-quality sampling with one or\ntwo steps, significantly narrowing the performance gap between latent\nconsistency and diffusion models. The implementation is released here:\nhttps:\/\/github.com\/quandao10\/sLCT\/",
        "Crohn's disease and intestinal tuberculosis share many overlapping features\nsuch as clinical, radiological, endoscopic, and histological features -\nparticularly granulomas, making it challenging to clinically differentiate\nthem. Our research leverages 3D CTE scans, computer vision, and machine\nlearning to improve this differentiation to avoid harmful treatment\nmismanagement such as unnecessary anti-tuberculosis therapy for Crohn's disease\nor exacerbation of tuberculosis with immunosuppressants. Our study proposes a\nnovel method to identify radiologist - identified biomarkers such as VF to SF\nratio, necrosis, calcifications, comb sign and pulmonary TB to enhance\naccuracy. We demonstrate the effectiveness by using different ML techniques on\nthe features extracted from these biomarkers, computing SHAP on XGBoost for\nunderstanding feature importance towards predictions, and comparing against\nSOTA methods such as pretrained ResNet and CTFoundation.",
        "This study investigates the effects of decoherence and squeezing on the\ndynamics of various kinds of quantum features--local quantum coherence, local\nentropy, EPR correlations, and entanglement--in the high-temperature limit of\nthe double Caldeira-Leggett model, focusing on initially squeezed states. We\ncompare two scenarios: (1) particles interacting with distinct environments and\n(2) particles coupled to a common environment. Our analysis reveals that common\nenvironments better preserve local coherence over time, whereas distinct\nenvironments accelerate decoherence. Temperature enhances decoherence and\nsuppresses coherence revivals, while squeezing affects transient dynamics but\nnot long-term coherence saturation. Local entropy increases with temperature\nand squeezing, though their underlying physical mechanisms differ. EPR\ncorrelations degrade due to environmental interactions, with squeezing\ninitially enhancing them but failing to prevent their eventual loss.\nEntanglement exhibits distinct behaviors: in separate environments, it\nundergoes sudden death, whereas in common environments, it experiences a dark\nperiod whose duration shortens with stronger squeezing. These findings provide\na comprehensive understanding of how decoherence and squeezing influence\nquantum correlations in open quantum systems.",
        "Quantum circuit simulations are essential for the verification of quantum\nalgorithms on behalf of real quantum devices. However, the memory requirements\nfor such simulations grow exponentially with the number of qubits involved in\nquantum programs. Moreover, a substantial number of computations in quantum\ncircuit simulations cause low locality data accesses, as they require extensive\ncomputations across the entire table of the full state vector. These\ncharacteristics lead to significant latency and energy overheads during data\ntransfers between the CPU and main memory. Processing-in-Memory (PIM), which\nintegrates computational logic near DRAM banks, could present a promising\nsolution to address these challenges. In this paper, we introduce PIMutation\n(PIM framework for qUanTum circuit simulATION) for achieving fast and\nenergy-efficient quantum circuit simulation. PIMutation is the first attempt to\nleverage UPMEM, a publicly available PIM-integrated DIMM, to implement quantum\ncircuit simulations. PIMutation incorporates three optimization strategies to\novercome the overhead of quantum circuit simulation using the real PIM system:\n(i) gate merging, (ii) row swapping, and (iii) vector partitioning. Our\nevaluations show that PIMutation achieves an average speedup of 2.99x and\n16.51x with a reduction of energy of 25.23% and 75.29% over the QuEST simulator\non CPU in 16- and 32-qubit benchmarks, respectively.",
        "Coronary artery stenosis, characterized by the narrowing of the lumen,\nsignificantly affects blood flow and contributes to the progression of\ncardiovascular diseases. This study investigates the hemodynamics of coronary\nartery models with varying stenosis configurations, all maintaining an 80%\nlumen reduction, to determine how differences in morphology influence flow\nbehavior and mechanical stresses. We employed computational fluid dynamics to\nanalyze five idealized geometries with (10% & 70%), (20% & 60%), (30% & 50%),\n(40% & 40%), and (0% & 80%) stenosis configurations. Through physiological\npulsatile flow conditions, we evaluated key hemodynamic pattern including\nvelocity profiles, wall shear stress, and pressure distribution. Our results\nreveal that despite the same degree of lumen reduction, each stenosis\nconfiguration produced distinct flow patterns and hemodynamic profiles.\nAsymmetric configurations, such as 10% & 70% and 20% & 60%, exhibited\npronounced flow disruptions and higher wall shear stress at the stenosis\nthroats, while symmetric configurations, such as 40% & 40%, demonstrated more\nuniform flow and reduced vortex. Our findings challenge the practice of\ngeneralizing results across stenosis configurations without accounting for\nmorphological variations, which is prevalent in many CFD studies using\nidealized models. This study emphasizes the importance of considering\nstenosis-specific morphology in CFD analyses and clinical interpretations to\nenhance the accuracy of diagnostic tools, improve personalized treatment\nplanning, and guide the design of medical devices such as stents.",
        "Quantum simulation is a rapidly evolving tool with great potential for\nresearch at the frontiers of physics, and is particularly suited to be used in\ncomputationally intensive lattice simulations, such as problems with\nnon-equilibrium. In this work, a basic scenario, namely free fermions in an\nexpanding universe, is considered and quantum simulations are used to perform\nthe evolution and study the phenomena involved. Using digital quantum\nsimulations with the Jordan-Wigner transformation and Trotter expansion, the\nevolutions of fermion number density, correlation functions, polarization, and\nchiral condensation are analyzed. A spread out phenomenon can be observed in\nthe simulation, which is a consequence of momentum redshift. This work also\ndemonstrates the simplicity and convenience of using quantum simulations when\nstudying time-evolution problems.",
        "In this paper we prove a new reducibility result for mini-hypers in\nprojective geometries over finite fields. It is further used to characterize\nthe minihypers with parameters (70, 22) in PG(4, 3). The latter can be used to\nattack the existence problem for some hypothetical ternary Griesmer codes of\ndimension 6.",
        "In end-to-end speech translation, acoustic representations learned by the\nencoder are usually fixed and static, from the perspective of the decoder,\nwhich is not desirable for dealing with the cross-modal and cross-lingual\nchallenge in speech translation. In this paper, we show the benefits of varying\nacoustic states according to decoder hidden states and propose an adaptive\nspeech-to-text translation model that is able to dynamically adapt acoustic\nstates in the decoder. We concatenate the acoustic state and target word\nembedding sequence and feed the concatenated sequence into subsequent blocks in\nthe decoder. In order to model the deep interaction between acoustic states and\ntarget hidden states, a speech-text mixed attention sublayer is introduced to\nreplace the conventional cross-attention network. Experiment results on two\nwidely-used datasets show that the proposed method significantly outperforms\nstate-of-the-art neural speech translation models.",
        "The symmetry-topology interplay dictates how to define order parameters and\nclassify material ordered phases. However, current understanding of this\ninterplay has been predominately approached from a one-sided perspective, with\ntopological states being classified within the constraints imposed by specific\nfixed symmetries. Here we complete this full circle by demonstrating\nspontaneous symmetry breaking that results from a periodic alteration of\ntopological phases induced by light in a centrosymmetric Dirac material\nZrTe$_5$. The distinguishing feature is the observation of robust correlation\nand striking anomalies in the fluence and temperature dependence of key\ntransport parameters.First, both shift current $J_{\\text{s}}$ and displacement\ncurrent $J_{\\text{d}}$, arising from interband transition and infrared phonon\ndriving, respectively, along with charge carrier pumping, exhibit similar\nbehaviors. Second, they all peak at similar low pump fluence, followed by a\nsubsequent reduction as the fluence further increases. This behavior cannot be\nexplained by conventional energetically allowed, direct excitations. Third, all\nthe three observables exhibit anomalies when they approach the topological\nphase transition temperature. These results highlight the unique low-energy\npumping behaviors in ZrTe$_5$, characterized by reversible fluence dependence\nand a 'hinge-like' interaction that connects various electronic and lattice\nobservables, including phonons, charge carriers, and currents. Our findings,\nsupported by model analysis, provide key insights into the fragility of\ncrystalline (inversion) and time-reversal symmetries during the dynamics of\ntopological phase transitions. This fragility drives spontaneous symmetry\nbreaking, evidenced by the synchronized emergence of off-resonant infrared\nphonons and broken-symmetry photocurrents.",
        "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks.",
        "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.",
        "We describe families of MLDEs whose solutions are modular forms of level one\nthat converge, $2$-adically, to a Hauptmodul on $\\Gamma_0(2)$ by using a\ntheorem of Serre. Then, we apply this to show that the image of the character\nmap on the $2$-adic Heisenberg VOA $S_{1}$ contains the space of $2$-adic\noverconvergent modular forms $M_{2}^{\\dagger}(1\/2)$ of weight zero.",
        "We present a Pure Shape Dynamics (PSD) formulation of General Relativity\n(GR), which implements full relationalism by eliminating absolute scale and\nexternal time references from the fundamental description of gravity. Starting\nfrom the Arnowitt-Deser-Misner (ADM) formulation, we derive a decoupled\ndynamical system that governs the evolution of the spatial conformal geometry\nand relational matter degrees of freedom, while eliminating the total volume\nand York time as independent dynamical variables. This results in an autonomous\nsubsystem describing an unparametrized trajectory in the conformal superspace\nof metric and matter configurations, with its evolution encoded in an equation\nof state that characterises the intrinsic geometric properties of the curve in\nshape space. We show that this equation of state is structurally analogous to\nthe corresponding PSD description of the Newtonian $N$-body problem,\nreinforcing the fundamental similarity between gravity and relational particle\ndynamics. Our framework is applied to the homogeneous Bianchi IX cosmological\nmodel, demonstrating that the Janus point evolution through the Big Bang, as\npreviously found in a symmetry-reduced setting, is a generic feature of the\nfull inhomogeneous PSD description. This work establishes PSD as a fully scale-\nand reparametrization-invariant formulation of classical gravity and lays the\nfoundation for addressing key open questions that are discussed at the end of\nthe paper.",
        "In this paper, a new class of structured polynomials, which we dub the {\\it\nseparable plus lower degree {\\rm (SPLD in short)} polynomials}, is introduced.\nThe formal definition of an SPLD polynomial, which extends the concept of the\nSPQ polynomial (Ahmadi et al. in Math Oper Res 48:1316--1343, 2023), is\ndefined. A type of bounded degree SOS hierarchy (BSOS-SPLD) is proposed to\nefficiently solve the optimization problems with SPLD polynomials, and several\nnumerical examples are performed much better than the bounded degree SOS\nhierarchy (Lasserre et al. in EURO J Comput Optim 5:87--117, 2017). An exact\nSOS relaxation for a class of convex SPLD polynomial optimization problems is\nproposed. Finally, an application of SPLD polynomials to polynomial regression\nproblems in statistics is presented.",
        "The Unmanned Aerial Vehicle (UAV) path planning problem is a complex\noptimization problem in the field of robotics. In this paper, we investigate\nthe possible utilization of this problem in benchmarking global optimization\nmethods. We devise a problem instance generator and pick 56 representative\ninstances, which we compare to established benchmarking suits through\nExploratory Landscape Analysis to show their uniqueness. For the computational\ncomparison, we select twelve well-performing global optimization techniques\nfrom both subfields of stochastic algorithms (evolutionary computation methods)\nand deterministic algorithms (Dividing RECTangles, or DIRECT-type methods). The\nexperiments were conducted in settings with varying dimensionality and\ncomputational budgets. The results were analyzed through several criteria\n(number of best-found solutions, mean relative error, Friedman ranks) and\nutilized established statistical tests. The best-ranking methods for the UAV\nproblems were almost universally the top-performing evolutionary techniques\nfrom recent competitions on numerical optimization at the Institute of\nElectrical and Electronics Engineers Congress on Evolutionary Computation.\nLastly, we discussed the variable dimension characteristics of the studied UAV\nproblems that remain still largely under-investigated.",
        "This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic\nmedal distributions by integrating the spatio-temporal relationships among\ncountries and the long-term dependencies of national performance. The\nSpatial-Temporal Graph Convolution Network (STGCN) captures geographic and\ninteractive factors-such as coaching exchange and socio-economic links-while\nthe Long Short-Term Memory (LSTM) module models historical trends in medal\ncounts, economic data, and demographics. To address zero-inflated outputs\n(i.e., the disparity between countries that consistently yield wins and those\nnever having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is\nincorporated to separate random zeros from structural zeros, providing a\nclearer view of potential breakthrough performances. Validation includes\nhistorical backtracking, policy shock simulations, and causal inference checks,\nconfirming the robustness of the proposed method. Results shed light on the\ninfluence of coaching mobility, event specialization, and strategic investment\non medal forecasts, offering a data-driven foundation for optimizing sports\npolicies and resource allocation in diverse Olympic contexts.",
        "The rise of single-cell sequencing technologies has revolutionized the\nexploration of drug resistance, revealing the crucial role of cellular\nheterogeneity in advancing precision medicine. By building computational models\nfrom existing single-cell drug response data, we can rapidly annotate cellular\nresponses to drugs in subsequent trials. To this end, we developed scGSDR, a\nmodel that integrates two computational pipelines grounded in the knowledge of\ncellular states and gene signaling pathways, both essential for understanding\nbiological gene semantics. scGSDR enhances predictive performance by\nincorporating gene semantics and employs an interpretability module to identify\nkey pathways contributing to drug resistance phenotypes. Our extensive\nvalidation, which included 16 experiments covering 11 drugs, demonstrates\nscGSDR's superior predictive accuracy, when trained with either bulk-seq or\nscRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's\napplication has extended from single-drug predictions to scenarios involving\ndrug combinations. Leveraging pathways of known drug target genes, we found\nthat scGSDR's cell-pathway attention scores are biologically interpretable,\nwhich helped us identify other potential drug-related genes. Literature review\nof top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,\nand PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for\nPaclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating\ngene semantics, enhances predictive modeling of cellular responses to diverse\ndrugs, proving invaluable for scenarios involving both single drug and\ncombination therapies and effectively identifying key resistance-related\npathways, thus advancing precision medicine and targeted therapy development.",
        "As technology has become more embedded into our society, the security of\nmodern-day systems is paramount. One topic which is constantly under discussion\nis that of patching, or more specifically, the installation of updates that\nremediate security vulnerabilities in software or hardware systems. This\ncontinued deliberation is motivated by complexities involved with patching; in\nparticular, the various incentives and disincentives for organizations and\ntheir cybersecurity teams when deciding whether to patch. In this paper, we\ntake a fresh look at the question of patching and critically explore why\norganizations and IT\/security teams choose to patch or decide against it\n(either explicitly or due to inaction). We tackle this question by aggregating\nand synthesizing prominent research and industry literature on the incentives\nand disincentives for patching, specifically considering the human aspects in\nthe context of these motives. Through this research, this study identifies key\nmotivators such as organizational needs, the IT\/security team's relationship\nwith vendors, and legal and regulatory requirements placed on the business and\nits staff. There are also numerous significant reasons discovered for why the\ndecision is taken not to patch, including limited resources (e.g.,\nperson-power), challenges with manual patch management tasks, human error, bad\npatches, unreliable patch management tools, and the perception that related\nvulnerabilities would not be exploited. These disincentives, in combination\nwith the motivators above, highlight the difficult balance that organizations\nand their security teams need to maintain on a daily basis. Finally, we\nconclude by discussing implications of these findings and important future\nconsiderations."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"3-D ultrasound imaging: a review",
    "start_abstract":"The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames",
        "An edge crack and a crack close to the vertex of a wedge",
        "An Arbitrary Time Interval Generator Base on Vernier Clocks with 0.67 ps\n  Adjustable Steps Implemented in FPGA",
        "Beyond the Median Voter Theorem: A New Framework for Ideological\n  Positioning",
        "Analyzing the Impact of AC False Data Injection Attacks on Power System\n  Operation",
        "Measurement of energy reduction by inertial Alfv\\'en waves propagating\n  through parallel gradients in the Alfv\\'en speed",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Gas excitation in galaxies and active galactic nuclei with He\n  II{\\lambda}4686 and X-ray emission",
        "A Practical Introduction to Kernel Discrepancies: MMD, HSIC & KSD",
        "New developments in 3D-trench electrode sensors",
        "Self-propulsion and self-rotation of an inertial chiral active\n  Ornstein-Uhlenbeck particle",
        "Deriving pulsar pair-production multiplicities from pulsar wind nebulae\n  using H.E.S.S. and LHAASO observations",
        "Double metasurfaces and Optimal transport",
        "Edge spectrum for truncated $\\mathbb{Z}_2$-insulators",
        "On the emergence and properties of weird quasiperiodic attractors"
      ],
      "abstract":[
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics.",
        "Two model problems of an elastic wedge with an internal and edge crack are\nanalyzed. The problem of an internal crack reduces to an order-4 vector\nRiemann-Hilbert problem whose matrix kernel entries are meromorphic functions\nand have exponential factors. When the internal crack is located along one of\nthe wedge sides, an efficient method of solution is proposed. It requires a\nfactorization of the order-2 matrix coefficient associated with the\ncorresponding problem of an edge crack and the solution of an infinite system\nof linear algebraic system with an exponential rate of convergence of an\napproximate solution to the exact one. The order-2 Khrapkov's factorization is\nmodified by splitting the matrix kernel into a scalar dominant function and a\n``regular\" matrix whose factorization is more convenient for numerical\npurposes. Expressions for the stress intensity coefficients and the potential\nenergy released when the crack advances are derived. Asymptotic relations for\nthe stress intensity coefficients and the potential energy when one of the\ncrack tips is close the wedge vertex are obtained.",
        "In TDC testing or timing system implementation tasks, it is often desirable\nto generate signal pulses with fine adjustable time intervals. In delay\ncell-based schemes, the time adjustment steps are limited by the propagation\ndelays of the cells, which are typically 15 to 20 picoseconds per step and are\nsensitive to temperature and operating voltage. In this document, a purely\ndigital scheme based on two vernier clocks with small frequency difference\ngenerated using cascaded PLL is reported. The scheme is tested in two families\nof low-cost FPGA and 0.67 and 0.97 picoseconds adjustable steps of the time\nintervals are achieved.",
        "This paper revisits the limitations of the Median Voter Theorem and\nintroduces a novel framework to analyze the optimal economic ideological\npositions of political parties. By incorporating Nash equilibrium, we examine\nthe mechanisms and elasticity of ideal deviation costs, voter distribution, and\npolicy feasibility. Our findings show that an increase in a party's ideal\ndeviation cost shifts its optimal ideological position closer to its ideal\npoint. Additionally, if a voter distribution can be expressed as a positive\nlinear combination of two other distributions, its equilibrium point must lie\nwithin the interval defined by the equilibrium points of the latter two. We\nalso find that decreasing feasibility costs incentivize governments, regardless\nof political orientation, to increase fiscal expenditures (e.g., welfare) and\nreduce fiscal revenues (e.g., taxes). This dynamic highlights the fiscal\npressures commonly faced by democratic nations under globalization. Moreover,\nwe demonstrate that even with uncertain voter distributions, parties can\nidentify optimal ideological positions to maximize their utility. Lastly, we\nexplain why the proposed framework cannot be applied to community ideologies\ndue to their fundamentally different nature. This study provides new\ntheoretical insights into political strategies and establishes a foundation for\nfuture empirical research.",
        "False Data Injection (FDI) attacks are a significant threat to modern power\nsystems. Although numerous research studies have focused on FDI attacks on\npower systems, these studies have primarily concentrated on designing or\ndetecting DC FDI attacks, with less attention given to the impact analysis of\nAC FDI attacks. AC FDI attacks are potentially more harmful as they can easily\nbypass bad data detection (BDD) algorithms. In this paper, we present a unified\napproach to investigate the impact of AC FDI attacks on power transmission\nlines using the PowerWorld simulator. We also investigate the impact of\ndifferent FDI attack designs, including those optimally designed to evade BDD\nalgorithms and compare them accordingly. Our findings demonstrate that in\ndesigning optimal AC FDI attacks, a trade-off between the residuals of state\nvariables and the corresponding impacts of the proposed attack should be\nconsidered. This is because optimal attacks result in fewer changes in the\nattacked variable states and their estimated residuals compared to arbitrary AC\nFDI attacks. Moreover, the impacts of optimal AC FDI attacks can be less severe\nthan those of arbitrary attacks. We implement and analyze the proposed approach\non the IEEE 39-bus test system using PowerWorld simulator.",
        "We have studied the propagation of inertial Alfv\\'en waves through parallel\ngradients in the Alfv\\'en speed using the Large Plasma Device at the University\nof California, Los Angeles. The reflection and transmission of Alfv\\'en waves\nthrough inhomogeneities in the background plasma is important for understanding\nwave propagation, turbulence, and heating in space, laboratory, and\nastrophysical plasmas. Here we \\rev{present inertial Alfv\\'en waves, under\nconditions relevant to solar flares and the solar corona. We find} that the\ntransmission of the inertial Alfv\\'en waves is reduced as the sharpness of the\ngradient is increased. Any reflected waves were below the detection limit of\nour experiment and reflection cannot account for all of the energy not\ntransmitted through the gradient. Our findings indicate that, for both kinetic\nand inertial Alfv\\'en waves, the controlling parameter for the transmission of\nthe waves through an Alfv\\'en speed gradient is the ratio of the Alfv\\'en\nwavelength along the gradient divided by the scale length of the gradient.\nFurthermore, our results suggest that an as-yet-unidentified damping process\noccurs in the gradient.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "The origin of He II emission in galaxies remains a debated topic, requiring\nionizing photons with energies exceeding 54 eV. While massive stars, such as\nWolf-Rayet stars, have been considered potential sources, their UV flux often\nfails to fully explain the observed He II emission. Recent studies suggest that\nX-ray binaries (XRBs) might contribute significantly to this ionization. We\nexplore the relationship between X-ray and $\\rm He~II \\lambda4686$ emission in\na statistically significant sample of galaxies, investigating whether X-ray\nsources, including active galactic nuclei (AGNs) and XRBs, serve as the primary\nmechanism for He II ionization across different galactic environments. We\ncross-matched a sample of known well-detected He II galaxies with the Chandra\nSource Catalog, yielding 165 galaxies with X-ray and $\\rm He~II \\lambda4686$\ndetections. The sources were classified into star-forming galaxies (SFGs) and\nAGNs based on the BPT diagram and a classification scheme defined for He II\ngalaxies. We find a strong, linear correlation between X-ray and He II\nluminosity across AGNs and SFGs spanning over seven orders of magnitude. AGNs\ngenerally exhibit higher He II\/H$\\beta$ flux ratios, stronger extinction, and\nharder X-ray spectra. The O32 ratio of SFGs is tightly correlated with the\nH$\\beta$ equivalent width ($\\rm EW_{H\\beta}$) but not with the He II\/H$\\beta$\nratio, suggesting a different excitation mechanism. We derive an O32--$\\rm\nEW_{H\\beta}$ line above which only AGNs of our sample reside. The tight\ncorrelation between X-ray and He II luminosity supports X-rays as the primary\ndriver of He II excitation. While AGNs have one common ionization source, the\ncentral black hole, in SFGs low-energy species are mainly excited by UV\nemission related to star-forming activity, however, high-energy species like He\nII require the presence of XRBs.",
        "This article provides a practical introduction to kernel discrepancies,\nfocusing on the Maximum Mean Discrepancy (MMD), the Hilbert-Schmidt\nIndependence Criterion (HSIC), and the Kernel Stein Discrepancy (KSD). Various\nestimators for these discrepancies are presented, including the commonly-used\nV-statistics and U-statistics, as well as several forms of the more\ncomputationally-efficient incomplete U-statistics. The importance of the choice\nof kernel bandwidth is stressed, showing how it affects the behaviour of the\ndiscrepancy estimation. Adaptive estimators are introduced, which combine\nmultiple estimators with various kernels, addressing the problem of kernel\nselection.",
        "Future high-luminosity hadron collider experiments feature unprecedented\nlevels of event pile-up and extreme radiation environments, calling for sensors\ncapable of 4D tracking, even after significant radiation damage. To this\npurpose, 3D sensors represent a viable solution, since they provide excellent\nradiation tolerance and very good temporal resolution. In particular, owing to\nthe uniform electric field and weighting field distributions, 3D-trench\nelectrode sensors from the INFN TIMESPOT project have shown a temporal\nresolution of $\\sim$10 ps after irradiation fluences up to 1$\\times$10$^{17}$\n1-Mev n$_{eq}$\/cm$^2$. In spite of the excellent performance of these sensors,\n3D-trench pixel technology is not yet fully established and the fabrication\nyield is not yet adequate for the production of large size pixel sensors. To\nimprove the potential of the 3D-trench concept for large-area sensors, a new\nbatch of sensors was designed at the University of Trento and fabricated at\nFBK, as part of the AIDA Innova project. Besides introducing some process\nimprovements, this batch includes two different sensor variants: the standard\none with continuous ohmic trenches, and a modified one with dashed ohmic\ntrenches. On-wafer electrical test results show that most of the sensors have\nlow leakage current and high breakdown voltage. Moreover, the fabrication yield\nfor the new design variant is higher than that of the standard design.",
        "We investigate the transport feature of an inertial chiral active\nOrnstein-Uhlenbeck particle moving on a two-dimensional surface. Using both\nanalytical approach and numerical simulations, we have exactly explored the\ntransient and steady-state behavior of the particle by analyzing the simulated\nparticle trajectories, probability distribution functions for position and\nvelocity, mean square displacement, mean square velocity, and effective kinetic\ntemperature of the medium. From the mean square displacement calculations, we\nobserve that, unlike an inertial active Brownian particle, a chiral active\nparticle manifests an initial ballistic, intermediate sub-diffusive to\nnon-diffusive, and the conventional long-time diffusive behavior. The\nintermediate sub-diffusive to non-diffusive behavior is prominent for the\nself-propulsion of an overdamped particle. It can be understood by\nchirality-induced transient confinement, which persists for short time\nintervals and diffuses away in the time asymptotic limit or at the steady\nstate. This behavior is further complemented by the exact calculation of mean\nsquare velocity or effective kinetic temperature of the medium, which is a\ndecreasing function of the magnitude of chirality. Moreover, the steady-state\nMSD and MSV are found to have a dependence both on chirality and activity time\nscale and hence can be controlled by tuning the persistent duration of activity\nor strength of the chirality of the particle.",
        "Pulsar wind nebulae (PWNe) dominate the galactic gamma-ray sky at very high\nenergies and they are major contributors to the leptonic cosmic ray flux.\nHowever, the question of whether or not pulsars also accelerate ions to\ncomparable energies has not yet been experimentally confirmed. We aim to\nconstrain the birth period and pair-production multiplicity for a set of\npulsars. In doing so, we aim to constrain the proportion of ions in the pulsar\nmagnetosphere and, hence, the proportion of ions that could enter the pulsar\nwind. We estimated possible ranges of the value of the average pair production\nmultiplicity for a sample of 26 pulsars in the Australia Telescope National\nFacility (ATNF) catalogue, which have also been observed by the High Energy\nStereoscopic System (H.E.S.S.) telescopes. We then derived lower limits for the\npulsar birth periods and average pair production multiplicities for a subset of\nthese sources where the extent of the pulsar wind nebula and surrounding\nsupernova shell have been measured in the radio. We also derived curves for the\naverage pair production multiplicities as a function of birth period for\nsources recently observed by the Large High Altitude Air Shower Observatory\n(LHAASO). We show that there is a potential for hadrons entering the pulsar\nwind for most of the H.E.S.S. and LHAASO sources we consider here, which is\ndependent upon the efficiency of luminosity conversion into particles. We also\npresent estimates of the pulsar birth period for six of these sources, all\nfalling into the range of $\\sim$10-50 ms.",
        "This paper constructs metalenses that separate homogeneous media with\ndifferent refractive indices, refracting one domain into another while\nconserving a prescribed energy distribution. Using optimal transport theory, we\ndesign singlet and doublet metalenses for energy-conserving by refraction and\nemploy multi-marginal optimal transport to create a refracting-reflecting\nmetalens that preserves given energy distributions.",
        "Fermionic time-reversal-invariant insulators in two dimensions -- class AII\nin the Kitaev table -- come in two different topological phases. These are\ncharacterized by a $\\mathbb{Z}_2$-index: the Fu-Kane-Mele index. We prove that\nif two such insulators with different indices occupy regions containing\narbitrarily large balls, then the spectrum of the resulting operator fills the\nbulk spectral gap. Our argument follows a proof by contradiction developed in\nan earlier work by two of the authors for quantum Hall systems. It boils down\nto showing that the $\\mathbb{Z}_2$-index can be computed only from bulk\ninformation in sufficiently large balls. This is achieved via a result of\nindependent interest: a local trace formula for the $\\mathbb{Z}_2$-index.",
        "We recently described a specific type of attractors of two-dimensional\ndiscontinuous piecewise linear maps, characterized by two discontinuity lines\ndividing the phase plane into three partitions, related to economic\napplications. To our knowledge, this type of attractor, which we call a weird\nquasiperiodic attractor, has not yet been studied in detail. They have a rather\ncomplex geometric structure and other interesting properties that are worth\nunderstanding better. To this end, we consider a simpler map that can also\npossess weird quasiperiodic attractors, namely, a 2D discontinuous piecewise\nlinear map $F$ with a single discontinuity line dividing the phase plane into\ntwo partitions, where two different homogeneous linear maps are defined. Map\n$F$ depends on four parameters -- the traces and determinants of the two\nJacobian matrices. In the parameter space of map $F$, we obtain specific\nregions associated with the existence of weird quasiperiodic attractors;\ndescribe some characteristic properties of these attractors; and explain one of\nthe possible mechanisms of their appearance."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Generative AI for Medical Imaging: extending the MONAI Framework",
    "start_abstract":"Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames",
        "An edge crack and a crack close to the vertex of a wedge",
        "An Arbitrary Time Interval Generator Base on Vernier Clocks with 0.67 ps\n  Adjustable Steps Implemented in FPGA",
        "Beyond the Median Voter Theorem: A New Framework for Ideological\n  Positioning",
        "Analyzing the Impact of AC False Data Injection Attacks on Power System\n  Operation",
        "Measurement of energy reduction by inertial Alfv\\'en waves propagating\n  through parallel gradients in the Alfv\\'en speed",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Gas excitation in galaxies and active galactic nuclei with He\n  II{\\lambda}4686 and X-ray emission",
        "A Practical Introduction to Kernel Discrepancies: MMD, HSIC & KSD",
        "New developments in 3D-trench electrode sensors",
        "Self-propulsion and self-rotation of an inertial chiral active\n  Ornstein-Uhlenbeck particle",
        "Deriving pulsar pair-production multiplicities from pulsar wind nebulae\n  using H.E.S.S. and LHAASO observations",
        "Double metasurfaces and Optimal transport",
        "Edge spectrum for truncated $\\mathbb{Z}_2$-insulators",
        "On the emergence and properties of weird quasiperiodic attractors"
      ],
      "abstract":[
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics.",
        "Two model problems of an elastic wedge with an internal and edge crack are\nanalyzed. The problem of an internal crack reduces to an order-4 vector\nRiemann-Hilbert problem whose matrix kernel entries are meromorphic functions\nand have exponential factors. When the internal crack is located along one of\nthe wedge sides, an efficient method of solution is proposed. It requires a\nfactorization of the order-2 matrix coefficient associated with the\ncorresponding problem of an edge crack and the solution of an infinite system\nof linear algebraic system with an exponential rate of convergence of an\napproximate solution to the exact one. The order-2 Khrapkov's factorization is\nmodified by splitting the matrix kernel into a scalar dominant function and a\n``regular\" matrix whose factorization is more convenient for numerical\npurposes. Expressions for the stress intensity coefficients and the potential\nenergy released when the crack advances are derived. Asymptotic relations for\nthe stress intensity coefficients and the potential energy when one of the\ncrack tips is close the wedge vertex are obtained.",
        "In TDC testing or timing system implementation tasks, it is often desirable\nto generate signal pulses with fine adjustable time intervals. In delay\ncell-based schemes, the time adjustment steps are limited by the propagation\ndelays of the cells, which are typically 15 to 20 picoseconds per step and are\nsensitive to temperature and operating voltage. In this document, a purely\ndigital scheme based on two vernier clocks with small frequency difference\ngenerated using cascaded PLL is reported. The scheme is tested in two families\nof low-cost FPGA and 0.67 and 0.97 picoseconds adjustable steps of the time\nintervals are achieved.",
        "This paper revisits the limitations of the Median Voter Theorem and\nintroduces a novel framework to analyze the optimal economic ideological\npositions of political parties. By incorporating Nash equilibrium, we examine\nthe mechanisms and elasticity of ideal deviation costs, voter distribution, and\npolicy feasibility. Our findings show that an increase in a party's ideal\ndeviation cost shifts its optimal ideological position closer to its ideal\npoint. Additionally, if a voter distribution can be expressed as a positive\nlinear combination of two other distributions, its equilibrium point must lie\nwithin the interval defined by the equilibrium points of the latter two. We\nalso find that decreasing feasibility costs incentivize governments, regardless\nof political orientation, to increase fiscal expenditures (e.g., welfare) and\nreduce fiscal revenues (e.g., taxes). This dynamic highlights the fiscal\npressures commonly faced by democratic nations under globalization. Moreover,\nwe demonstrate that even with uncertain voter distributions, parties can\nidentify optimal ideological positions to maximize their utility. Lastly, we\nexplain why the proposed framework cannot be applied to community ideologies\ndue to their fundamentally different nature. This study provides new\ntheoretical insights into political strategies and establishes a foundation for\nfuture empirical research.",
        "False Data Injection (FDI) attacks are a significant threat to modern power\nsystems. Although numerous research studies have focused on FDI attacks on\npower systems, these studies have primarily concentrated on designing or\ndetecting DC FDI attacks, with less attention given to the impact analysis of\nAC FDI attacks. AC FDI attacks are potentially more harmful as they can easily\nbypass bad data detection (BDD) algorithms. In this paper, we present a unified\napproach to investigate the impact of AC FDI attacks on power transmission\nlines using the PowerWorld simulator. We also investigate the impact of\ndifferent FDI attack designs, including those optimally designed to evade BDD\nalgorithms and compare them accordingly. Our findings demonstrate that in\ndesigning optimal AC FDI attacks, a trade-off between the residuals of state\nvariables and the corresponding impacts of the proposed attack should be\nconsidered. This is because optimal attacks result in fewer changes in the\nattacked variable states and their estimated residuals compared to arbitrary AC\nFDI attacks. Moreover, the impacts of optimal AC FDI attacks can be less severe\nthan those of arbitrary attacks. We implement and analyze the proposed approach\non the IEEE 39-bus test system using PowerWorld simulator.",
        "We have studied the propagation of inertial Alfv\\'en waves through parallel\ngradients in the Alfv\\'en speed using the Large Plasma Device at the University\nof California, Los Angeles. The reflection and transmission of Alfv\\'en waves\nthrough inhomogeneities in the background plasma is important for understanding\nwave propagation, turbulence, and heating in space, laboratory, and\nastrophysical plasmas. Here we \\rev{present inertial Alfv\\'en waves, under\nconditions relevant to solar flares and the solar corona. We find} that the\ntransmission of the inertial Alfv\\'en waves is reduced as the sharpness of the\ngradient is increased. Any reflected waves were below the detection limit of\nour experiment and reflection cannot account for all of the energy not\ntransmitted through the gradient. Our findings indicate that, for both kinetic\nand inertial Alfv\\'en waves, the controlling parameter for the transmission of\nthe waves through an Alfv\\'en speed gradient is the ratio of the Alfv\\'en\nwavelength along the gradient divided by the scale length of the gradient.\nFurthermore, our results suggest that an as-yet-unidentified damping process\noccurs in the gradient.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "The origin of He II emission in galaxies remains a debated topic, requiring\nionizing photons with energies exceeding 54 eV. While massive stars, such as\nWolf-Rayet stars, have been considered potential sources, their UV flux often\nfails to fully explain the observed He II emission. Recent studies suggest that\nX-ray binaries (XRBs) might contribute significantly to this ionization. We\nexplore the relationship between X-ray and $\\rm He~II \\lambda4686$ emission in\na statistically significant sample of galaxies, investigating whether X-ray\nsources, including active galactic nuclei (AGNs) and XRBs, serve as the primary\nmechanism for He II ionization across different galactic environments. We\ncross-matched a sample of known well-detected He II galaxies with the Chandra\nSource Catalog, yielding 165 galaxies with X-ray and $\\rm He~II \\lambda4686$\ndetections. The sources were classified into star-forming galaxies (SFGs) and\nAGNs based on the BPT diagram and a classification scheme defined for He II\ngalaxies. We find a strong, linear correlation between X-ray and He II\nluminosity across AGNs and SFGs spanning over seven orders of magnitude. AGNs\ngenerally exhibit higher He II\/H$\\beta$ flux ratios, stronger extinction, and\nharder X-ray spectra. The O32 ratio of SFGs is tightly correlated with the\nH$\\beta$ equivalent width ($\\rm EW_{H\\beta}$) but not with the He II\/H$\\beta$\nratio, suggesting a different excitation mechanism. We derive an O32--$\\rm\nEW_{H\\beta}$ line above which only AGNs of our sample reside. The tight\ncorrelation between X-ray and He II luminosity supports X-rays as the primary\ndriver of He II excitation. While AGNs have one common ionization source, the\ncentral black hole, in SFGs low-energy species are mainly excited by UV\nemission related to star-forming activity, however, high-energy species like He\nII require the presence of XRBs.",
        "This article provides a practical introduction to kernel discrepancies,\nfocusing on the Maximum Mean Discrepancy (MMD), the Hilbert-Schmidt\nIndependence Criterion (HSIC), and the Kernel Stein Discrepancy (KSD). Various\nestimators for these discrepancies are presented, including the commonly-used\nV-statistics and U-statistics, as well as several forms of the more\ncomputationally-efficient incomplete U-statistics. The importance of the choice\nof kernel bandwidth is stressed, showing how it affects the behaviour of the\ndiscrepancy estimation. Adaptive estimators are introduced, which combine\nmultiple estimators with various kernels, addressing the problem of kernel\nselection.",
        "Future high-luminosity hadron collider experiments feature unprecedented\nlevels of event pile-up and extreme radiation environments, calling for sensors\ncapable of 4D tracking, even after significant radiation damage. To this\npurpose, 3D sensors represent a viable solution, since they provide excellent\nradiation tolerance and very good temporal resolution. In particular, owing to\nthe uniform electric field and weighting field distributions, 3D-trench\nelectrode sensors from the INFN TIMESPOT project have shown a temporal\nresolution of $\\sim$10 ps after irradiation fluences up to 1$\\times$10$^{17}$\n1-Mev n$_{eq}$\/cm$^2$. In spite of the excellent performance of these sensors,\n3D-trench pixel technology is not yet fully established and the fabrication\nyield is not yet adequate for the production of large size pixel sensors. To\nimprove the potential of the 3D-trench concept for large-area sensors, a new\nbatch of sensors was designed at the University of Trento and fabricated at\nFBK, as part of the AIDA Innova project. Besides introducing some process\nimprovements, this batch includes two different sensor variants: the standard\none with continuous ohmic trenches, and a modified one with dashed ohmic\ntrenches. On-wafer electrical test results show that most of the sensors have\nlow leakage current and high breakdown voltage. Moreover, the fabrication yield\nfor the new design variant is higher than that of the standard design.",
        "We investigate the transport feature of an inertial chiral active\nOrnstein-Uhlenbeck particle moving on a two-dimensional surface. Using both\nanalytical approach and numerical simulations, we have exactly explored the\ntransient and steady-state behavior of the particle by analyzing the simulated\nparticle trajectories, probability distribution functions for position and\nvelocity, mean square displacement, mean square velocity, and effective kinetic\ntemperature of the medium. From the mean square displacement calculations, we\nobserve that, unlike an inertial active Brownian particle, a chiral active\nparticle manifests an initial ballistic, intermediate sub-diffusive to\nnon-diffusive, and the conventional long-time diffusive behavior. The\nintermediate sub-diffusive to non-diffusive behavior is prominent for the\nself-propulsion of an overdamped particle. It can be understood by\nchirality-induced transient confinement, which persists for short time\nintervals and diffuses away in the time asymptotic limit or at the steady\nstate. This behavior is further complemented by the exact calculation of mean\nsquare velocity or effective kinetic temperature of the medium, which is a\ndecreasing function of the magnitude of chirality. Moreover, the steady-state\nMSD and MSV are found to have a dependence both on chirality and activity time\nscale and hence can be controlled by tuning the persistent duration of activity\nor strength of the chirality of the particle.",
        "Pulsar wind nebulae (PWNe) dominate the galactic gamma-ray sky at very high\nenergies and they are major contributors to the leptonic cosmic ray flux.\nHowever, the question of whether or not pulsars also accelerate ions to\ncomparable energies has not yet been experimentally confirmed. We aim to\nconstrain the birth period and pair-production multiplicity for a set of\npulsars. In doing so, we aim to constrain the proportion of ions in the pulsar\nmagnetosphere and, hence, the proportion of ions that could enter the pulsar\nwind. We estimated possible ranges of the value of the average pair production\nmultiplicity for a sample of 26 pulsars in the Australia Telescope National\nFacility (ATNF) catalogue, which have also been observed by the High Energy\nStereoscopic System (H.E.S.S.) telescopes. We then derived lower limits for the\npulsar birth periods and average pair production multiplicities for a subset of\nthese sources where the extent of the pulsar wind nebula and surrounding\nsupernova shell have been measured in the radio. We also derived curves for the\naverage pair production multiplicities as a function of birth period for\nsources recently observed by the Large High Altitude Air Shower Observatory\n(LHAASO). We show that there is a potential for hadrons entering the pulsar\nwind for most of the H.E.S.S. and LHAASO sources we consider here, which is\ndependent upon the efficiency of luminosity conversion into particles. We also\npresent estimates of the pulsar birth period for six of these sources, all\nfalling into the range of $\\sim$10-50 ms.",
        "This paper constructs metalenses that separate homogeneous media with\ndifferent refractive indices, refracting one domain into another while\nconserving a prescribed energy distribution. Using optimal transport theory, we\ndesign singlet and doublet metalenses for energy-conserving by refraction and\nemploy multi-marginal optimal transport to create a refracting-reflecting\nmetalens that preserves given energy distributions.",
        "Fermionic time-reversal-invariant insulators in two dimensions -- class AII\nin the Kitaev table -- come in two different topological phases. These are\ncharacterized by a $\\mathbb{Z}_2$-index: the Fu-Kane-Mele index. We prove that\nif two such insulators with different indices occupy regions containing\narbitrarily large balls, then the spectrum of the resulting operator fills the\nbulk spectral gap. Our argument follows a proof by contradiction developed in\nan earlier work by two of the authors for quantum Hall systems. It boils down\nto showing that the $\\mathbb{Z}_2$-index can be computed only from bulk\ninformation in sufficiently large balls. This is achieved via a result of\nindependent interest: a local trace formula for the $\\mathbb{Z}_2$-index.",
        "We recently described a specific type of attractors of two-dimensional\ndiscontinuous piecewise linear maps, characterized by two discontinuity lines\ndividing the phase plane into three partitions, related to economic\napplications. To our knowledge, this type of attractor, which we call a weird\nquasiperiodic attractor, has not yet been studied in detail. They have a rather\ncomplex geometric structure and other interesting properties that are worth\nunderstanding better. To this end, we consider a simpler map that can also\npossess weird quasiperiodic attractors, namely, a 2D discontinuous piecewise\nlinear map $F$ with a single discontinuity line dividing the phase plane into\ntwo partitions, where two different homogeneous linear maps are defined. Map\n$F$ depends on four parameters -- the traces and determinants of the two\nJacobian matrices. In the parameter space of map $F$, we obtain specific\nregions associated with the existence of weird quasiperiodic attractors;\ndescribe some characteristic properties of these attractors; and explain one of\nthe possible mechanisms of their appearance."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain",
    "start_abstract":"One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b21"
      ],
      "title":[
        "3-D ultrasound imaging: a review",
        "Generative AI for Medical Imaging: extending the MONAI Framework"
      ],
      "abstract":[
        "The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
        "Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features."
      ],
      "categories":[
        "cs.CV",
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "General mean-field stochastic linear quadratic control problem driven by\n  L\\'evy processes with random coefficients",
        "Slowly decaying strain solitons in nonlinear viscoelastic waveguides",
        "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding",
        "Bounded Synthesis of Synchronized Distributed Models from Lightweight\n  Specifications",
        "EEG-CLIP : Learning EEG representations from natural language\n  descriptions",
        "Please, do tell",
        "Hot-carrier thermal breakdown and S-type current-voltage characteristics\n  in perforated graphene structures",
        "Goal-oriented Transmission Scheduling: Structure-guided DRL with a\n  Unified Dual On-policy and Off-policy Approach",
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
        "High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA\n  Fine-Tuning with Multimodal LLaMA 3.2",
        "Modeling of Rumor Propagation in Large Populations with Network via\n  Graphon Games",
        "On the Chermak-Delgado lattice of a finite group",
        "OfficeMate: Pilot Evaluation of an Office Assistant Robot",
        "MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for\n  Automating CFD Simulation and Post-Processing",
        "New Plasma Sheath Potential Solutions in Cylindrical and Spherical\n  Coordinates",
        "Large language models in finance : what is financial sentiment?",
        "Conditional Diffusion Model with OOD Mitigation as High-Dimensional\n  Offline Resource Allocation Planner in Clustered Ad Hoc Networks",
        "Effect of Pt bottom electrode texture selection on the tetragonality and\n  physical properties of Ba0.8Sr0.2TiO3 thin films produced by pulsed laser\n  deposition",
        "Measuring Diversity in Synthetic Datasets",
        "Noise Reversal by Entropy Quantum Computing",
        "The Geostrategy of Youth Player Recruitment in Portuguese Clubs",
        "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction",
        "Data-augmented Learning of Geodesic Distances in Irregular Domains\n  through Soner Boundary Conditions",
        "To investigate event-by-event fluctuations of mean transverse momentum\n  in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with PYTHIA8 and HERWIG7\n  models",
        "Monte Carlo model of distilled remote entanglement between\n  superconducting qubits across optical channels",
        "An Ensemble Information Filter: Retrieving Markov-information from the\n  SPDE discretisation",
        "Warnings based on risk matrices: a coherent framework with consistent\n  evaluation"
      ],
      "abstract":[
        "This paper studies a stochastic mean-field linear-quadratic optimal control\nproblem with random coefficients. The state equation is a general linear\nstochastic differential equation with mean-field terms $\\EE X(t)$ and $\\EE\nu(t)$ of the state and the control processes and is driven by a Brownian motion\nand a Poisson random measure. By the coupled system of Riccati equations, an\nexplicit expressions for the optimal state feedback control is obtained. As a\nby-product, the non-homogeneous stochastic linear-quadratic control problem\nwith random coefficients and L\\'evy driving noises is also studied.",
        "This paper is devoted to the modeling of longitudinal strain waves in a rod\ncomposed of a nonlinear viscoelastic material characterized by\nfrequency-dependent second- and third-order elastic constants. We demonstrate\nthat long waves in such a material can be effectively described by a damped\nBoussinesq-type equation for the longitudinal strain, incorporating dissipation\nthrough retarded operators. Using the existing theory of solitary wave\nsolutions in nearly integrable systems, we derive a slowly-decaying strain\nsoliton solution to this equation. The derived soliton characteristics are\nshown to be in a good agreement with results from full 3D simulations. We\ndemonstrate the importance of taking into account the frequency dependence of\nthird-order elastic constants for the description of strain solitons.",
        "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.",
        "We present an approach to automatically synthesize synchronized models from\nlightweight formal specifications. Our approach takes as input a specification\nof a distributed system along with a global linear time constraint, which must\nbe fulfilled by the interaction of the system's components. It produces\nexecutable models for the component specifications (in the style of Promela\nlanguage) whose concurrent execution satisfies the global constraint. The\ncomponent specifications consist of a collection of actions described by means\nof pre and post conditions together with first-order relational formulas\nprescribing their behavior. We use the Alloy Analyzer to encode the component\nspecifications and enumerate their potential implementations up to some bound,\nwhose concurrent composition is model checked against the global property. Even\nthough this approach is sound and complete up to the selected bound, it is\nimpractical as the number of candidate implementations grows exponentially. To\naddress this, we propose an algorithm that uses batches of counterexamples to\nprune the solution space, it has two main phases: exploration, the algorithm\ncollects a batch of counterexamples, and exploitation, where this knowledge is\nused to speed up the search. The approach is sound, while its completeness\ndepends on the batches used. We present a prototype tool, describe some\nexperiments, and compare it with related approaches.",
        "Deep networks for electroencephalogram (EEG) decoding are currently often\ntrained to only solve a specific task like pathology or gender decoding. A more\ngeneral approach leveraging the medical reports of clinical EEG recordings is\nto learn mappings between medical reports and EEG recordings. This approach was\npioneered in the computer vision domain matching images and their text captions\nand subsequently allowed to do successful zero-shot decoding using textual\nclass prompts. In this work, we follow this approach and develop a contrastive\nlearning framework EEG-CLIP that aligns EEG time series and their corresponding\nclinical text descriptions in a shared embedding space. We investigate its\npotential for versatile EEG decoding, assessing performance on a range of\nfew-shot and zero-shot settings. Overall, results show that EEG-CLIP manages to\nnontrivially align text and EEG representations. Our work presents a promising\napproach to learn general EEG representations, which could enable easier\nanalyses of diverse decoding questions through zero shot decoding or training\ntask-specific models from fewer training examples. The code for reproducing our\nresults is available at https:\/\/github.com\/tidiane-camaret\/EEGClip.",
        "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
        "We investigate the carrier transport characteristics of perforated graphene\nlayer (PGL) composed of arrays of interdigital coplanar graphene microribbons\n(GMRs) connected by graphene nanoribbon (GNR) bridges. We analyze their\noperation at room-temperature. Under an applied bias voltage, two-dimensional\nelectron and hole systems (2DES and 2DHS) form in adjacent GMRs. The terminal\ncurrent in these PGL structures is primarily governed by thermionic transport\nacross the GNR bridges. As electrons and holes traverse the GNRs, they induce\nheating in the 2DES and 2DHS, creating a positive feedback loop between carrier\nheating and thermionic emission. This phenomenon, characterized as hot-carrier\nthermal breakdown, can give rise to S-shaped inter-GMR current-voltage\ncharacteristics. These unique transport properties make PGLs promising\ncandidates for fast, voltage-controlled room-temperature switches and\nelectromagnetic radiation detectors.",
        "Goal-oriented communications prioritize application-driven objectives over\ndata accuracy, enabling intelligent next-generation wireless systems. Efficient\nscheduling in multi-device, multi-channel systems poses significant challenges\ndue to high-dimensional state and action spaces. We address these challenges by\nderiving key structural properties of the optimal solution to the goal-oriented\nscheduling problem, incorporating Age of Information (AoI) and channel states.\nSpecifically, we establish the monotonicity of the optimal state value function\n(a measure of long-term system performance) w.r.t. channel states and prove its\nasymptotic convexity w.r.t. AoI states. Additionally, we derive the\nmonotonicity of the optimal policy w.r.t. channel states, advancing the\ntheoretical framework for optimal scheduling. Leveraging these insights, we\npropose the structure-guided unified dual on-off policy DRL (SUDO-DRL), a\nhybrid algorithm that combines the stability of on-policy training with the\nsample efficiency of off-policy methods. Through a novel structural property\nevaluation framework, SUDO-DRL enables effective and scalable training,\naddressing the complexities of large-scale systems. Numerical results show\nSUDO-DRL improves system performance by up to 45% and reduces convergence time\nby 40% compared to state-of-the-art methods. It also effectively handles\nscheduling in much larger systems, where off-policy DRL fails and on-policy\nbenchmarks exhibit significant performance loss, demonstrating its scalability\nand efficacy in goal-oriented communications.",
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Craw4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Craw4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps:\/\/github.com\/cxcscmu\/Craw4LLM.",
        "Electrocardiogram (ECG) interpretation is a cornerstone of cardiac\ndiagnostics. This paper explores a practical approach to enhance ECG image\ninterpretation using the multimodal LLaMA 3.2 model. We used a\nparameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA),\nspecifically designed to boost the model's ability to understand ECG images and\nachieve better outcomes across a wide range of cardiac conditions. Our method\nis tailored for ECG analysis and leverages ECGInstruct, a large-scale\ninstruction dataset with 1 Million samples. This dataset is a rich collection\nof synthesized ECG images, generated from raw ECG data from trusted open-source\nrepositories like MIMIC-IV ECG and PTB-XL. Each ECG image in ECGInstruct comes\nwith expert-written questions and detailed answers, covering diverse ECG\ninterpretation scenarios, including complex cardiac conditions like Myocardial\nInfarction and Conduction Disturbances. Our fine-tuning approach efficiently\nadapts the LLaMA 3.2 model (built upon LLaMA 3) by integrating low-rank\nadaptation techniques, focusing on efficiency by updating only a small set of\nparameters, specifically ignoring the `lm_head` and `embed_tokens` layers. This\npaper details the model setup, our efficient fine-tuning method, and\nimplementation specifics. We provide a thorough evaluation through extensive\nexperiments, demonstrating the effectiveness of our method across various ECG\ninterpretation tasks. The results convincingly show that our\nparameter-efficient LoRA fine-tuning achieves excellent performance in ECG\nimage interpretation, significantly outperforming baseline models and reaching\naccuracy comparable to or exceeding traditional CNN-based methods in\nidentifying a wide range of cardiac abnormalities, including over 70 conditions\nfrom the PTB-XL dataset.",
        "In this paper, we propose a graphon game model to understand how rumor (such\nas fake news) propagates in large populations that are interacting on a network\nand how different policies affect the spread. We extend the SKIR model that is\nused to model rumor propagation and implement individual controls and weighted\ninteractions with other agents to have controlled dynamics. The agents aim to\nminimize their own expected costs non-cooperatively. We give the finite player\ngame model and the limiting graphon game model to approximate the Nash\nequilibrium in the population. We give the graphon game Nash equilibrium as a\nsolution to a continuum of ordinary differential equations (ODEs) and give\nexistence results. Finally, we give a numerical approach and analyze examples\nwhere we use piecewise constant graphon.",
        "By imposing conditions upon the index of a self-centralizing subgroup of a\ngroup, and upon the index of the center of the group, we are able to classify\nthe Chermak-Delgado lattice of the group. This is our main result. We use this\nresult to classify the Chermak-Delgado lattices of dicyclic groups and of\nmetabelian $p$-groups of maximal class.",
        "Office Assistant Robots (OARs) offer a promising solution to proactively\nprovide in-situ support to enhance employee well-being and productivity in\noffice spaces. We introduce OfficeMate, a social OAR designed to assist with\npractical tasks, foster social interaction, and promote health and well-being.\nThrough a pilot evaluation with seven participants in an office environment, we\nfound that users see potential in OARs for reducing stress and promoting\nhealthy habits and value the robot's ability to provide companionship and\nphysical activity reminders in the office space. However, concerns regarding\nprivacy, communication, and the robot's interaction timing were also raised.\nThe feedback highlights the need to carefully consider the robot's appearance\nand behaviour to ensure it enhances user experience and aligns with office\nsocial norms. We believe these insights will better inform the development of\nadaptive, intelligent OAR systems for future office space integration.",
        "Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and\nbiology to model fluid flow, heat transfer, and chemical reactions. While Large\nLanguage Models (LLMs) have transformed various domains, their application in\nCFD remains limited, particularly for complex tasks like post-processing. To\nbridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of\nThought (COT) decomposition and iterative verification to enhance accessibility\nfor non-expert users through natural language inputs. Tested on a new benchmark\ncovering simulation (fluid flow, heat transfer, combustion) and post-processing\n(extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score\nof 6.3\/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0\n(2.1\/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case.\nAn ablation study confirmed that COT-driven decomposition and iterative\nrefinement substantially improved task performance. Furthermore, scaling laws\nshowed that increasing COT steps enhanced accuracy while raising token usage,\naligning with LLM post-training scaling trends. These results highlight the\ntransformative potential of LLMs in automating CFD workflows for industrial and\nresearch applications. Code is available at\nhttps:\/\/github.com\/Terry-cyx\/MetaOpenFOAM",
        "Leading edges of hypersonic vehicles can reach temperatures greater than 2000\n{\\deg}C, and radii of curvature smaller than 1 cm, at which thermionic emission\n(also known as electron transpiration) can play a significant role in cooling\nthe leading edge alongside other heat transfer modes such as convection and\nradiation. Existing theoretical analyses of thermionic cooling with\nspace-charge effects at a leading edge are limited to one-dimensional (1D),\nanalytical and numerical models that do not capture the influences of geometric\ncurvature of the leading edge or temperature gradients along the leading edge.\nThe key to understanding space-charge effects is development of the plasma\nsheath potential, and to that end we demonstrate a generalized methodology to\ncalculate the sheath potential space in 1D Cartesian, cylindrical, and\nspherical coordinate systems. We accomplish this by extending Takamura's\napproach beyond the Cartesian system, and motivate sheath formation conditions\nfor potential sheathes with and without a virtual cathode similar in nature to\nhow Bohm originally presented his criterion of minimum Mach number for a valid\n1D Cartesian sheath. By observing for what parameter inputs we satisfy the\nsheath formation conditions, we illustrate parameter spaces of minimum Mach\nnumber, potential derivative at the wall, and net current, for each coordinate\nsystem and for two different input work functions; we also show example\npotential spaces for each coordinate system. With our numerical approach,\ngeneralized to multiple coordinate systems, we enable computationally efficient\nand higher fidelity analysis of thermionic emission with space-charge effects\nfor more realistic system geometries.",
        "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.",
        "Due to network delays and scalability limitations, clustered ad hoc networks\nwidely adopt Reinforcement Learning (RL) for on-demand resource allocation.\nAlbeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions\nstruggle to tackle the huge action space, which generally explodes\nexponentially along with the number of resource allocation units, enduring low\nsampling efficiency and high interaction cost. In contrast to MFRL, Model-Based\nRL (MBRL) offers an alternative solution to boost sample efficiency and\nstabilize the training by explicitly leveraging a learned environment model.\nHowever, establishing an accurate dynamic model for complex and noisy\nenvironments necessitates a careful balance between model accuracy and\ncomputational complexity $\\&$ stability. To address these issues, we propose a\nConditional Diffusion Model Planner (CDMP) for high-dimensional offline\nresource allocation in clustered ad hoc networks. By leveraging the astonishing\ngenerative capability of Diffusion Models (DMs), our approach enables the\naccurate modeling of high-quality environmental dynamics while leveraging an\ninverse dynamics model to plan a superior policy. Beyond simply adopting DMs in\noffline RL, we further incorporate the CDMP algorithm with a theoretically\nguaranteed, uncertainty-aware penalty metric, which theoretically and\nempirically manifests itself in mitigating the Out-of-Distribution\n(OOD)-induced distribution shift issue underlying scarce training data.\nExtensive experiments also show that our model outperforms MFRL in average\nreward and Quality of Service (QoS) while demonstrating comparable performance\nto other MBRL algorithms.",
        "The effect of platinum (Pt) bottom electrode texture on the tetragonality,\ndielectric, ferroelectric, and polarization switching response of pulsed laser\ndeposited Ba0.8Sr0.2TiO3 (BST) thin films has been studied. The x-ray\ndiffraction and Raman analysis revealed the higher tetragonality of BST films\nwhen they were grown on higher (111) textured Pt layer. The properties like\ndielectric permittivity, polarization, switching time, and leakage currents\nwere found to be correlated to tetragonality and orientation of the BST films.\nThe polarization current was observed to be higher in BST films on Pt epitaxial\nlayer and it exhibits exponential dependence on the electric field. The\nvoltage-current measurements displayed Ohmic behavior of leakage current\nirrespective of Pt texture for low voltages (up to 1 V), whereas at higher\nvoltages the conduction mechanism was found to be dependent on texture\nselection of bottom Pt electrode.",
        "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https:\/\/github.com\/BlueWhaleLab\/DCScore.",
        "Signal to noise ratio is key to any measurement. Recent progress in\nsemi\/super-conductor technology have pushed the signal detection sensitivity to\nthe ultimate quantum level, but the noise issue remains largely untouched and,\nin many cases, becomes even more severe because of the high sensitivity. In\nthis paper, we explore a hardware-based approach to noise removal using entropy\nquantum computing. Distinct to any existing de-noising approach, it observes\nand reproduces the quantum statistical properties of noise in an optical system\nto emulate and thereby reverse the noise from data. We show how it can recover\n1D and 2D image data mixed with much stronger noise.",
        "Portugal's prominent role as a global exporter of football talent is\nprimarily driven by youth academies. Notably, Portugal leads the global ranking\nin terms of net transfer balance. This study aims to uncover and understand the\nrecruitment strategies of Portuguese clubs for sourcing young talent and\nevaluate the relative success of different strategies. A comprehensive dataset\nspanning recent decades of Portuguese youth and professional football provides\ngranular insights, including information such as players' birthplaces and the\ninitial grassroots clubs where they developed. The initial findings suggest a\ncorrelation between a club's prominence and the geographic reach of its youth\nscouting operations, with larger clubs able to cast their net wider. Analysis\nof the correlation between players' birthplace and high-tier football club\nlocation suggests that the performance of senior teams acts as a catalyst for\ninvestment in youth teams. Regions without professional clubs are often left\nunderserved. That said, certain clubs have made significant gains by focusing\non player recruitment outside their district, such as the Algarve region,\ndemonstrating how geographically targeted strategies can deliver substantial\nreturns on investment. This study underscores data's role in sharpening youth\nplayer recruitment operations at football clubs. Clubs have access to in-depth\nand comprehensive datasets that can be used for resource allocation,\nterritorial coverage planning, and identifying strategic partnerships with\nother clubs, potentially influencing their future success both on the field and\nfinancially. This offers opportunities for growth for individual clubs and\nholds implications for the continued strength of Portuguese football.",
        "Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning.",
        "Geodesic distances play a fundamental role in robotics, as they efficiently\nencode global geometric information of the domain. Recent methods use neural\nnetworks to approximate geodesic distances by solving the Eikonal equation\nthrough physics-informed approaches. While effective, these approaches often\nsuffer from unstable convergence during training in complex environments. We\npropose a framework to learn geodesic distances in irregular domains by using\nthe Soner boundary condition, and systematically evaluate the impact of data\nlosses on training stability and solution accuracy. Our experiments demonstrate\nthat incorporating data losses significantly improves convergence robustness,\nreducing training instabilities and sensitivity to initialization. These\nfindings suggest that hybrid data-physics approaches can effectively enhance\nthe reliability of learning-based geodesic distance solvers with sparse data.",
        "Estimations of event-by-event mean transverse momentum ($\\langle p_{\\rm T}\n\\rangle$) fluctuations are reported in terms of the integral correlator,\n$\\langle \\Delta p_{\\rm T} \\Delta p_{\\rm T}\\rangle$, and the skewness of\nevent-wise $\\langle p_{\\rm T} \\rangle$ distribution in proton$-$proton (pp)\ncollisions at $\\sqrt{s}=13$ TeV with the Monte Carlo event generators PYTHIA8\nand HERWIG7. The final-state charged particles with transverse momentum\n($p_{\\rm T}$) and pseudorapidity ($\\eta$) ranges $0.15 \\leq p_{\\rm T}\\leq 2.0$\nGeV\/$c$ and $|\\eta| \\leq 0.8$ were considered for the investigation. The\ncorrelator, $\\langle \\Delta p_{\\rm T} \\Delta p_{\\rm T}\\rangle$, is observed to\nfollow distinct decreasing trends with average charged particle multiplicity\n($\\langle N_{\\rm ch} \\rangle$) for the models. Furthermore, both models yield\npositive finite skewness in low-multiplicity events. The fluctuations are\nadditionally studied using the transverse spherocity estimator ($S_{\\rm 0}$) to\ncomprehend the relative contributions from hard scattering (jets) and other\nsoft processes to the observed fluctuations. Comparing the model predictions\nwould enhance our understanding of fluctuation dynamics in pp collisions,\nestablishing a crucial baseline for studying non-trivial fluctuations in\nheavy-ion collisions.",
        "A promising quantum computing architecture comprises modules of\nsuperconducting quantum processors linked by optical channels via quantum\ntransducers. To map transducer device performance to system-level channel\nperformance, our model uses Monte Carlo simulations that incorporate 2-to-1 and\n3-to-1 entanglement distillation protocols. We show that the Extreme Photon\nLoss distillation protocol is particularly high performing and that, even\nwithout distillation, present-day transducers are at the threshold of enabling\nBell pair distribution with fidelities of 50%. If the next generation of\ntransducers can improve by 3 orders of magnitude in both added noise and\nefficiency, and increase repetition rates by 50x, then they would allow for\nremote two-qubit gates achieving 99.7% fidelities at 100 kHz rates. These\nresults set targets for transducers to be ready for deployment into scaled\nsuperconducting quantum computers.",
        "Ensemble-based Data Assimilation faces significant challenges in\nhigh-dimensional systems due to spurious correlations and ensemble collapse.\nThese issues arise from estimating dense dependencies with limited ensemble\nsizes. This paper introduces the Ensemble Information Filter, which encodes\nMarkov properties directly into the statistical model's precision matrix,\nleveraging structure from SPDE dynamics to constrain information to propagate\nlocally. EnIF eliminates the need for ad-hoc localisation, improving\nstatistical consistency and scalability. Numerical experiments demonstrate its\nadvantages in filtering, smoothing, and parameter estimation, making EnIF a\nrobust and efficient solution for large-scale data assimilation problems.",
        "Risk matrices are widely used across a range of fields and have found\nincreasing utility in warning decision practices globally. However, their\napplication in this context presents challenges, which range from potentially\nperverse warning outcomes to a lack of objective verification (i.e.,\nevaluation) methods. This paper introduces a coherent framework for generating\nmulti-level warnings from risk matrices to address these challenges. The\nproposed framework is general, is based on probabilistic forecasts of hazard\nseverity or impact and is compatible with the Common Alerting Protocol (CAP).\nMoreover, it includes a family of consistent scoring functions for objectively\nevaluating the predictive performance of risk matrix assessments and the\nwarnings they produce. These scoring functions enable the ranking of\nforecasters or warning systems and the tracking of system improvements by\nrewarding accurate probabilistic forecasts and compliance with warning service\ndirectives. A synthetic experiment demonstrates the efficacy of these scoring\nfunctions, while the framework is illustrated through warnings for heavy\nrainfall based on operational ensemble prediction system forecasts for Tropical\nCyclone Jasper (Queensland, Australia, 2023). This work establishes a robust\nfoundation for enhancing the reliability and verifiability of risk-based\nwarning systems."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
    "start_abstract":"Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
    "start_categories":[
      "cs.CV",
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors",
        "Where are the earliest stars relics in the simulated Milky Way\n  analogues?",
        "Mean value of cubic $L$-funcitons with fixed genus",
        "Peak splitting and bias fields in ferroelectric hafnia mediated by\n  interface charge effects",
        "A new and flexible class of sharp asymptotic time-uniform confidence\n  sequences",
        "SPYGLASS. VI. Feedback-Driven Star Formation in the Circinus Complex",
        "Volumetric modulated arc therapy or step-shoot IMRT? A 4D dosimetry\n  study of motion effect in lung SBRT using a dynamic virtual patient model",
        "The effect of a band gap gradient on the radiative losses in the open\n  circuit voltage of solar cells",
        "The effect of spacetime torsion on neutrino mixing",
        "Stability of Khintchine inequalities with optimal constants between the\n  second and the $p$-th moment for $p \\ge 3$",
        "Counting lifts of irreducible Brauer characters",
        "Mitigation of Artifacts in Multistatic & Passive Radar Imaging Using\n  Microlocal Analysis",
        "High-Sensitivity Imaging and Modeling of Ultra-Weak Photon Emission in\n  Plants Under Stress",
        "Weak Lefschetz property of equigenerated complete intersections.\n  Applications",
        "DNA Sensing with Whispering Gallery Mode Microlasers"
      ],
      "abstract":[
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
        "Using 6 Milky Way analogues with two different numerical resolutions from the\nAuriga simulation, we investigate the total mass, spatial distribution and\nkinematics of the earliest stars relics in the Milky Way at $z=0$. These relics\n(second generation stars) formed over a wide redshift range, from about $z=22$\nto $z=4$, with an average formation redshift of $z \\sim 10.0$, and comprise\nabout $2\\times10^{-5}$ of the entire galactic stellar population. The disk and\nbulge components host only a small fraction of these relics, contributing less\nthan $12$ percent in total. The stellar halo, in particular the outer stellar\nhalo of which galactic radius $r>30$ kpc, hosts the largest fraction (about 46\npercent on average), with an average of one relic star for per $4,000$ to\n$10,000$ stars, making it a promising region for observational searches.\nAdditionally, around $18$ percent of the earliest stars relics are found in\nsatellite galaxies, with smaller and older satellite galaxies tending to\ncontain a higher proportion of these stars. Thus, low-mass and early-formed\nsatellite galaxies are also ideal targets for finding such relics, although\nsome satellite galaxies may lack them entirely. The spatial distribution and\nkinematics of these stars show good numerical convergence across different\nsimulation resolutions. Our results provide valuable guidance for searches of\nthe earliest stars relics and offer insights for interpreting findings from\nongoing and future stellar archaeology surveys.",
        "We investigate the mean value of the first moment of primitive cubic\n$L$-functions over $\\mathbb{F}_q(T)$ in the non-Kummer setting. Specifically,\nwe study the sum\n  \\begin{equation*}\n  \\sum_{\\substack{\\chi\\ primitive\\ cubic\\\\ genus(\\chi)=g}}L_q(\\frac{1}{2},\n\\chi),\n  \\end{equation*} where $L_q(s,\\chi)$ denotes the $L$-function associated with\nprimitive cubic character $\\chi$. Using double Dirichlet series, we derive an\nerror term of size $q^{(\\frac{7}{8}+\\varepsilon)g}$.",
        "The pristine state of hafnium based ferroelectric devices exhibits various\nunwanted properties, such as imprint and peak splitting, which diminish with\nbipolar cycling. The incorporation of a niobium oxide layer at different\npositions in metal-ferroelectric-metal and metal-ferroelectric-insulator-metal\nstacks is used to modify the pristine state of the device. X-ray photoelectron\nspectroscopy and transmission electron microscopy measurements are used to\ninvestigate the influence of niobium oxide on the zirconium hafnium oxide\nlayer. It is hypothesized that the charged vacancies generated by the\nintroduced niobium oxide in the adjacent zirconium hafnium oxide layer result\nin an electric bias field that influences the pristine polarization state of\nthe domains. A comparison of different stacks shows that peak splitting in the\npristine state is most likely related to the formation of opposing electric\nbias fields in upwards and downwards polarized domains. Furthermore, the\nincorporation of niobium oxide in the zirconium hafnium oxide\/aluminum oxide\ncapacitor stack in between the ferroelectric and insulating layer leads to a\npeak splitting free device without imprint, which could be explained by the\nincreased influence of charge trapping near the zirconium hafnium\noxide-\/niobium oxide and niobium oxide-\/aluminum oxide interfaces.",
        "Confidence sequences are anytime-valid analogues of classical confidence\nintervals that do not suffer from multiplicity issues under optional\ncontinuation of the data collection. As in classical statistics, asymptotic\nconfidence sequences are a nonparametric tool showing under which high-level\nassumptions asymptotic coverage is achieved so that they also give a certain\nrobustness guarantee against distributional deviations. In this paper, we\npropose a new flexible class of confidence sequences yielding sharp asymptotic\ntime-uniform confidence sequences under mild assumptions. Furthermore, we\nhighlight the connection to corresponding sequential testing problems and\ndetail the underlying limit theorem.",
        "Young associations provide a record that traces the star formation process,\nand the youngest populations connect progenitor gas dynamics to the resulting\nstellar populations. We therefore conduct the first comprehensive overview of\nthe Circinus Complex, an under-studied and massive ($\\sim$1500 M$_{\\odot}$)\nregion consisting of approximately 3100 recently formed stars alongside the\nCircinus Molecular Cloud (CMC). We find a clear age pattern in the contiguous\ncentral region (CirCe), where younger stars are found further from the massive\ncentral cluster, and where the velocities are consistent with uniform\nexpansion. By comparing this structure to an analogous STARFORGE simulation, we\nfind that the age structure and dynamics of the association are consistent with\nstar formation in two stages: the global collapse of the parent cloud that\nbuilds the $500 M_{\\odot}$ central cluster ASCC 79, followed by triggered star\nformation in a shell swept up after the first massive stars form. We also find\nthat filaments with a range of distances from the central cluster can naturally\nproduce multi-generational age sequences due to differences in feedback\nstrength and exposure. Outlying populations show velocities consistent with\nformation independent from the CirCe region, but with similar enough velocities\nthat they may be difficult to distinguish from one another later in their\nexpansion. We therefore provide a new alternative view of sequential star\nformation that relies on feedback from a single central cluster rather than the\nmultiple sequential generations that are traditionally invoked, while also\nproviding insight into the star formation history of older populations.",
        "Purpose: To investigate the impact of delivery techniques and planning\nparameters on interplay effect in lung SBRT.\n  Methods: A dynamic virtual patient model containing normal structures and a\ntumor with adjustable sizes, locations, and 3D breathing motion was utilized.\nSBRT plans were developed using both step-and-shoot IMRT and VMAT with\ndifferent planning parameters (energy, isocenter location, PTV margin, and PTV\ndose heterogeneity). 4D doses were calculated by simulating synchronized\ndelivery of SBRT to the virtual patient model with random initial positions of\ntumor motion. The expected dose (average) and the standard deviation of the 4D\ndoses were obtained. The relative difference between the expected GTV\nminimal\/mean (GTVMin\/GTVMean) dose and the planned ITVMin\/ITVMean dose (denoted\nby %E\/P), and between the GTVMin and the prescription dose (DRx) were computed.\n  Results: The %E\/P for GTVMean was significantly lower for IMRT than VMAT\n(0.5% +\/- 7.7% v.s. 3.5% +\/- 5.0%, p=0.04). The expected GTVMin was lower than\nDRx in 9.4% of all IMRT plans versus 3.1% in VMAT. The worst-case scenario, 4D\nGTVMin was 14.1% lower than the ITVMin. Choices of PTV margin or dose\nheterogeneity to be achieved in PTV can result in significant difference\n(p<0.05) in motion interplay depending on delivery techniques.\n  Conclusion: Motion interplay may cause the expected GTVMin to be less than\nthe planned ITV minimal dose and DRx for both IMRT and VMAT plans. The\ndifferences between the expected GTV dose and the ITV dose depended on the\ndelivery technique and planning parameters. Overall, VMAT is less prone to\nmotion interplay than IMRT.",
        "The radiative open circuit voltage loss in a solar cell occurs because the\nabsorptance spectrum near the band gap shows gradual increase rather than sharp\nstep function like transition. This broadening effect has been attributed to\nband gap fluctuations and or to Urbach tails. In this report, we use modelling\nbased on Planck s generalized law to distinguish between these two effects. Our\nresults demonstrate that Urbach tails have only a minimal effect on the\nabsorptance edge broadening and clarify that even an ideal direct semiconductor\nwith no band gap fluctuations shows broadening at the absorptance onset.\nFurthermore, state of the art inorganic thin film solar cells often incorporate\na band gap gradient across their thickness, which can further contribute to\nabsorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,\nwe perform a comprehensive analysis of voltage losses through absolute\nphotoluminescence and electroluminescence spectroscopy, combined with\nphotospectrometry and high-spatial-resolution cathodoluminescence measurements.\nWe find that the loss analysis based on the combination of radiative,\ngeneration and non-radiative losses is complete. Samples with a graded band gap\nprofile show more pronounced broadening of the absorptance onset and up to 16\nmV higher radiative losses compared to the samples with uniform band gap. There\nis indication, that band gap-graded samples also have larger lateral band gap\ninhomogeneity.",
        "In the framework of quantum field theory, we analyze the neutrino\noscillations in the presence of a torsion background. We consider the Einstein\nCartan theory and we study the cases of constant torsion and of linearly\ntime-dependent torsion. We derive new neutrino oscillation formulae which\ndepend on the spin orientation and the CP asymmetry formula. Experiment such as\nPTOLEMY which analyzes the cosmological background of neutrino, can provide\ninsights into the effect shown here.",
        "We give a strengthening of the classical Khintchine inequality between the\nsecond and the $p$-th moment for $p \\ge 3$ with optimal constant by adding a\ndeficit depending on the vector of coefficients of the Rademacher sum.",
        "Let $p$ be an odd prime, and suppose that $G$ is a $p$-solvable group and\n$\\varphi\\in {\\rm IBr}(G)$ has vertex $Q$. In 2011, Cossey, Lewis and Navarro\nproved that the number of lifts of $\\varphi$ is at most $|Q:Q'|$ whenever $Q$\nis normal in $G$. In this paper, we present an explicit description of the set\nof lifts of $\\varphi$ with a given vertex pair $(Q,\\delta)$ under a weaker\ncondition on $Q$, and thus generalize their result.",
        "In the analysis of many synthetic aperture radar (SAR) experiments the\npossibility of passive background signals being recorded simultaneously and\ncorrupting the image is often overlooked. Our work addresses this by\nconsidering the multistatic experiment where two stationary emitters are\n\"always on\" so there is \"crosstalk\" between their signals. The model for the\nradar data is given by a Fourier integral operator, and we assume that the data\ncannot be separated into contributions from individual emitters. Using\ntechniques of microlocal analysis, we show that \"crosstalk\" between emitters\nleads to artifacts in the image and we determine their locations relative to\nthe scatterers that produced the data.\n  To combat the harmful effects of crosstalk, we develop methods that allow us\nto create an image of a region of interest (ROI) that is free from such\nartifacts. The first method makes use of a carefully designed data acquisition\ngeometry to localise artifacts away from a ROI, and the second is an image\nprocessing technique that displaces artifacts away from a ROI. These methods\nare verified via numerical implementation in MATLAB. The analysis carried out\nhere is valuable in bistatic and multistatic radar experiments, where an\nunwanted, passive source is also being detected, as well as in passive imaging,\nwhere one wishes to produce a high-quality image purely from uncontrolled\nsources of illumination.",
        "Ultra-weak photon emission (UPE) is a noninvasive diagnostic tool that\neffectively reflects the function and health status of plant cells. However,\ncurrent UPE measurement techniques are limited by resolution and sensitivity,\nparticularly when monitoring different plant species and stress types. This\nstudy analyzes the delayed luminescence (DL) properties of Hydrocotyle\nvulgaris, Arabidopsis leaves, and Ginkgo leaves under both stress and control\nconditions using an independently developed UPE imaging system. The results\nshowed a significant increase in initial DL intensity and an accelerated\noxidative metabolic rate under mechanical injury and oxidative stress. DL decay\ncharacteristics were significantly correlated with the plant's physiological\nstate, with stress conditions exhibiting decay curves that closely matched\ntheoretical models. These findings confirm the established correlation between\nDL and plant stress responses. The high-resolution, low-noise imaging system\nsignificantly improves the accuracy of plant physiological state monitoring and\nprovides new insights into the potential of optical signals for non-chemical\ncommunication research and agricultural applications. This technology has great\npotential for monitoring plant growth, assessing environmental stress, and\nsupporting precision agriculture.",
        "In this paper, we prove that any Artinian complete intersection homogeneous\nideal $I$ in $K[x_0,\\cdots,x_n]$ generated by $n+1$ forms of degree $d\\ge 2$\nsatisfies the weak Lefschetz property (WLP) in degree $t< d+\\lceil \\frac{d}{n}\n\\rceil$. As a consequence, we get that the Jacobian ideal of a smooth 3-fold of\ndegree $d\\ge 7$ in ${\\mathbb P}^4$ satisfies the weak Lefschetz property in\ndegree $d$, answering a recent question of Beauville.",
        "Nucleic acid sensing is crucial for advancing diagnostics, therapeutic\nmonitoring and molecu-lar biology research, by enabling the precise\nidentification of DNA and RNA interactions. Here, we present an innovative\nsensing platform based on DNA-functionalized whispering gallery mode (WGM)\nmicrolasers. By correlating spectral shifts in laser emission to changes in\nrefractive index, we demonstrate real-time detection of DNA hybridization and\nstructural changes. The addition of gold nanoparticles to the DNA strands\nsignificantly enhances sensi-tivity, and labeling exclusively the sensing\nstrand or a hairpin strand eliminates the need for secondary labeling of the\ntarget strand. We further show that ionic strength influences DNA compactness,\nand we introduce a hairpin-based system as a dual-purpose sensor and\ncon-trolled release mechanism for potential drug delivery. This versatile\nWGM-based platform of-fers promise for sequence-specific nucleic acid sensing,\nmultiplexed detection, and in vivo ap-plications in diagnostics and cellular\nresearch."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
    "start_abstract":"Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach.",
    "start_categories":[
      "cs.CV",
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors",
        "Where are the earliest stars relics in the simulated Milky Way\n  analogues?",
        "Mean value of cubic $L$-funcitons with fixed genus",
        "Peak splitting and bias fields in ferroelectric hafnia mediated by\n  interface charge effects",
        "A new and flexible class of sharp asymptotic time-uniform confidence\n  sequences",
        "SPYGLASS. VI. Feedback-Driven Star Formation in the Circinus Complex",
        "Volumetric modulated arc therapy or step-shoot IMRT? A 4D dosimetry\n  study of motion effect in lung SBRT using a dynamic virtual patient model",
        "The effect of a band gap gradient on the radiative losses in the open\n  circuit voltage of solar cells",
        "The effect of spacetime torsion on neutrino mixing",
        "Stability of Khintchine inequalities with optimal constants between the\n  second and the $p$-th moment for $p \\ge 3$",
        "Counting lifts of irreducible Brauer characters",
        "Mitigation of Artifacts in Multistatic & Passive Radar Imaging Using\n  Microlocal Analysis",
        "High-Sensitivity Imaging and Modeling of Ultra-Weak Photon Emission in\n  Plants Under Stress",
        "Weak Lefschetz property of equigenerated complete intersections.\n  Applications",
        "DNA Sensing with Whispering Gallery Mode Microlasers"
      ],
      "abstract":[
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
        "Using 6 Milky Way analogues with two different numerical resolutions from the\nAuriga simulation, we investigate the total mass, spatial distribution and\nkinematics of the earliest stars relics in the Milky Way at $z=0$. These relics\n(second generation stars) formed over a wide redshift range, from about $z=22$\nto $z=4$, with an average formation redshift of $z \\sim 10.0$, and comprise\nabout $2\\times10^{-5}$ of the entire galactic stellar population. The disk and\nbulge components host only a small fraction of these relics, contributing less\nthan $12$ percent in total. The stellar halo, in particular the outer stellar\nhalo of which galactic radius $r>30$ kpc, hosts the largest fraction (about 46\npercent on average), with an average of one relic star for per $4,000$ to\n$10,000$ stars, making it a promising region for observational searches.\nAdditionally, around $18$ percent of the earliest stars relics are found in\nsatellite galaxies, with smaller and older satellite galaxies tending to\ncontain a higher proportion of these stars. Thus, low-mass and early-formed\nsatellite galaxies are also ideal targets for finding such relics, although\nsome satellite galaxies may lack them entirely. The spatial distribution and\nkinematics of these stars show good numerical convergence across different\nsimulation resolutions. Our results provide valuable guidance for searches of\nthe earliest stars relics and offer insights for interpreting findings from\nongoing and future stellar archaeology surveys.",
        "We investigate the mean value of the first moment of primitive cubic\n$L$-functions over $\\mathbb{F}_q(T)$ in the non-Kummer setting. Specifically,\nwe study the sum\n  \\begin{equation*}\n  \\sum_{\\substack{\\chi\\ primitive\\ cubic\\\\ genus(\\chi)=g}}L_q(\\frac{1}{2},\n\\chi),\n  \\end{equation*} where $L_q(s,\\chi)$ denotes the $L$-function associated with\nprimitive cubic character $\\chi$. Using double Dirichlet series, we derive an\nerror term of size $q^{(\\frac{7}{8}+\\varepsilon)g}$.",
        "The pristine state of hafnium based ferroelectric devices exhibits various\nunwanted properties, such as imprint and peak splitting, which diminish with\nbipolar cycling. The incorporation of a niobium oxide layer at different\npositions in metal-ferroelectric-metal and metal-ferroelectric-insulator-metal\nstacks is used to modify the pristine state of the device. X-ray photoelectron\nspectroscopy and transmission electron microscopy measurements are used to\ninvestigate the influence of niobium oxide on the zirconium hafnium oxide\nlayer. It is hypothesized that the charged vacancies generated by the\nintroduced niobium oxide in the adjacent zirconium hafnium oxide layer result\nin an electric bias field that influences the pristine polarization state of\nthe domains. A comparison of different stacks shows that peak splitting in the\npristine state is most likely related to the formation of opposing electric\nbias fields in upwards and downwards polarized domains. Furthermore, the\nincorporation of niobium oxide in the zirconium hafnium oxide\/aluminum oxide\ncapacitor stack in between the ferroelectric and insulating layer leads to a\npeak splitting free device without imprint, which could be explained by the\nincreased influence of charge trapping near the zirconium hafnium\noxide-\/niobium oxide and niobium oxide-\/aluminum oxide interfaces.",
        "Confidence sequences are anytime-valid analogues of classical confidence\nintervals that do not suffer from multiplicity issues under optional\ncontinuation of the data collection. As in classical statistics, asymptotic\nconfidence sequences are a nonparametric tool showing under which high-level\nassumptions asymptotic coverage is achieved so that they also give a certain\nrobustness guarantee against distributional deviations. In this paper, we\npropose a new flexible class of confidence sequences yielding sharp asymptotic\ntime-uniform confidence sequences under mild assumptions. Furthermore, we\nhighlight the connection to corresponding sequential testing problems and\ndetail the underlying limit theorem.",
        "Young associations provide a record that traces the star formation process,\nand the youngest populations connect progenitor gas dynamics to the resulting\nstellar populations. We therefore conduct the first comprehensive overview of\nthe Circinus Complex, an under-studied and massive ($\\sim$1500 M$_{\\odot}$)\nregion consisting of approximately 3100 recently formed stars alongside the\nCircinus Molecular Cloud (CMC). We find a clear age pattern in the contiguous\ncentral region (CirCe), where younger stars are found further from the massive\ncentral cluster, and where the velocities are consistent with uniform\nexpansion. By comparing this structure to an analogous STARFORGE simulation, we\nfind that the age structure and dynamics of the association are consistent with\nstar formation in two stages: the global collapse of the parent cloud that\nbuilds the $500 M_{\\odot}$ central cluster ASCC 79, followed by triggered star\nformation in a shell swept up after the first massive stars form. We also find\nthat filaments with a range of distances from the central cluster can naturally\nproduce multi-generational age sequences due to differences in feedback\nstrength and exposure. Outlying populations show velocities consistent with\nformation independent from the CirCe region, but with similar enough velocities\nthat they may be difficult to distinguish from one another later in their\nexpansion. We therefore provide a new alternative view of sequential star\nformation that relies on feedback from a single central cluster rather than the\nmultiple sequential generations that are traditionally invoked, while also\nproviding insight into the star formation history of older populations.",
        "Purpose: To investigate the impact of delivery techniques and planning\nparameters on interplay effect in lung SBRT.\n  Methods: A dynamic virtual patient model containing normal structures and a\ntumor with adjustable sizes, locations, and 3D breathing motion was utilized.\nSBRT plans were developed using both step-and-shoot IMRT and VMAT with\ndifferent planning parameters (energy, isocenter location, PTV margin, and PTV\ndose heterogeneity). 4D doses were calculated by simulating synchronized\ndelivery of SBRT to the virtual patient model with random initial positions of\ntumor motion. The expected dose (average) and the standard deviation of the 4D\ndoses were obtained. The relative difference between the expected GTV\nminimal\/mean (GTVMin\/GTVMean) dose and the planned ITVMin\/ITVMean dose (denoted\nby %E\/P), and between the GTVMin and the prescription dose (DRx) were computed.\n  Results: The %E\/P for GTVMean was significantly lower for IMRT than VMAT\n(0.5% +\/- 7.7% v.s. 3.5% +\/- 5.0%, p=0.04). The expected GTVMin was lower than\nDRx in 9.4% of all IMRT plans versus 3.1% in VMAT. The worst-case scenario, 4D\nGTVMin was 14.1% lower than the ITVMin. Choices of PTV margin or dose\nheterogeneity to be achieved in PTV can result in significant difference\n(p<0.05) in motion interplay depending on delivery techniques.\n  Conclusion: Motion interplay may cause the expected GTVMin to be less than\nthe planned ITV minimal dose and DRx for both IMRT and VMAT plans. The\ndifferences between the expected GTV dose and the ITV dose depended on the\ndelivery technique and planning parameters. Overall, VMAT is less prone to\nmotion interplay than IMRT.",
        "The radiative open circuit voltage loss in a solar cell occurs because the\nabsorptance spectrum near the band gap shows gradual increase rather than sharp\nstep function like transition. This broadening effect has been attributed to\nband gap fluctuations and or to Urbach tails. In this report, we use modelling\nbased on Planck s generalized law to distinguish between these two effects. Our\nresults demonstrate that Urbach tails have only a minimal effect on the\nabsorptance edge broadening and clarify that even an ideal direct semiconductor\nwith no band gap fluctuations shows broadening at the absorptance onset.\nFurthermore, state of the art inorganic thin film solar cells often incorporate\na band gap gradient across their thickness, which can further contribute to\nabsorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,\nwe perform a comprehensive analysis of voltage losses through absolute\nphotoluminescence and electroluminescence spectroscopy, combined with\nphotospectrometry and high-spatial-resolution cathodoluminescence measurements.\nWe find that the loss analysis based on the combination of radiative,\ngeneration and non-radiative losses is complete. Samples with a graded band gap\nprofile show more pronounced broadening of the absorptance onset and up to 16\nmV higher radiative losses compared to the samples with uniform band gap. There\nis indication, that band gap-graded samples also have larger lateral band gap\ninhomogeneity.",
        "In the framework of quantum field theory, we analyze the neutrino\noscillations in the presence of a torsion background. We consider the Einstein\nCartan theory and we study the cases of constant torsion and of linearly\ntime-dependent torsion. We derive new neutrino oscillation formulae which\ndepend on the spin orientation and the CP asymmetry formula. Experiment such as\nPTOLEMY which analyzes the cosmological background of neutrino, can provide\ninsights into the effect shown here.",
        "We give a strengthening of the classical Khintchine inequality between the\nsecond and the $p$-th moment for $p \\ge 3$ with optimal constant by adding a\ndeficit depending on the vector of coefficients of the Rademacher sum.",
        "Let $p$ be an odd prime, and suppose that $G$ is a $p$-solvable group and\n$\\varphi\\in {\\rm IBr}(G)$ has vertex $Q$. In 2011, Cossey, Lewis and Navarro\nproved that the number of lifts of $\\varphi$ is at most $|Q:Q'|$ whenever $Q$\nis normal in $G$. In this paper, we present an explicit description of the set\nof lifts of $\\varphi$ with a given vertex pair $(Q,\\delta)$ under a weaker\ncondition on $Q$, and thus generalize their result.",
        "In the analysis of many synthetic aperture radar (SAR) experiments the\npossibility of passive background signals being recorded simultaneously and\ncorrupting the image is often overlooked. Our work addresses this by\nconsidering the multistatic experiment where two stationary emitters are\n\"always on\" so there is \"crosstalk\" between their signals. The model for the\nradar data is given by a Fourier integral operator, and we assume that the data\ncannot be separated into contributions from individual emitters. Using\ntechniques of microlocal analysis, we show that \"crosstalk\" between emitters\nleads to artifacts in the image and we determine their locations relative to\nthe scatterers that produced the data.\n  To combat the harmful effects of crosstalk, we develop methods that allow us\nto create an image of a region of interest (ROI) that is free from such\nartifacts. The first method makes use of a carefully designed data acquisition\ngeometry to localise artifacts away from a ROI, and the second is an image\nprocessing technique that displaces artifacts away from a ROI. These methods\nare verified via numerical implementation in MATLAB. The analysis carried out\nhere is valuable in bistatic and multistatic radar experiments, where an\nunwanted, passive source is also being detected, as well as in passive imaging,\nwhere one wishes to produce a high-quality image purely from uncontrolled\nsources of illumination.",
        "Ultra-weak photon emission (UPE) is a noninvasive diagnostic tool that\neffectively reflects the function and health status of plant cells. However,\ncurrent UPE measurement techniques are limited by resolution and sensitivity,\nparticularly when monitoring different plant species and stress types. This\nstudy analyzes the delayed luminescence (DL) properties of Hydrocotyle\nvulgaris, Arabidopsis leaves, and Ginkgo leaves under both stress and control\nconditions using an independently developed UPE imaging system. The results\nshowed a significant increase in initial DL intensity and an accelerated\noxidative metabolic rate under mechanical injury and oxidative stress. DL decay\ncharacteristics were significantly correlated with the plant's physiological\nstate, with stress conditions exhibiting decay curves that closely matched\ntheoretical models. These findings confirm the established correlation between\nDL and plant stress responses. The high-resolution, low-noise imaging system\nsignificantly improves the accuracy of plant physiological state monitoring and\nprovides new insights into the potential of optical signals for non-chemical\ncommunication research and agricultural applications. This technology has great\npotential for monitoring plant growth, assessing environmental stress, and\nsupporting precision agriculture.",
        "In this paper, we prove that any Artinian complete intersection homogeneous\nideal $I$ in $K[x_0,\\cdots,x_n]$ generated by $n+1$ forms of degree $d\\ge 2$\nsatisfies the weak Lefschetz property (WLP) in degree $t< d+\\lceil \\frac{d}{n}\n\\rceil$. As a consequence, we get that the Jacobian ideal of a smooth 3-fold of\ndegree $d\\ge 7$ in ${\\mathbb P}^4$ satisfies the weak Lefschetz property in\ndegree $d$, answering a recent question of Beauville.",
        "Nucleic acid sensing is crucial for advancing diagnostics, therapeutic\nmonitoring and molecu-lar biology research, by enabling the precise\nidentification of DNA and RNA interactions. Here, we present an innovative\nsensing platform based on DNA-functionalized whispering gallery mode (WGM)\nmicrolasers. By correlating spectral shifts in laser emission to changes in\nrefractive index, we demonstrate real-time detection of DNA hybridization and\nstructural changes. The addition of gold nanoparticles to the DNA strands\nsignificantly enhances sensi-tivity, and labeling exclusively the sensing\nstrand or a hairpin strand eliminates the need for secondary labeling of the\ntarget strand. We further show that ionic strength influences DNA compactness,\nand we introduce a hairpin-based system as a dual-purpose sensor and\ncon-trolled release mechanism for potential drug delivery. This versatile\nWGM-based platform of-fers promise for sequence-specific nucleic acid sensing,\nmultiplexed detection, and in vivo ap-plications in diagnostics and cellular\nresearch."
      ]
    }
  }
]