[
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms",
    "start_abstract":"Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b1"
      ],
      "title":[
        "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
        "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
      ],
      "abstract":[
        "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
        "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
      ],
      "categories":[
        "q-bio.CB",
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Ultra-high-energy $\\gamma$-ray emission associated with the tail of a\n  bow-shock pulsar wind nebula",
        "A new algorithm for detecting X-ray shots in Cyg X-1",
        "Feedforward Cancellation of High-Frequency Phase Noise in\n  Frequency-Doubled Lasers",
        "Exact Fluctuating Hydrodynamics of the Scaled Light-Heavy Model",
        "Uniqueness of solutions to elliptic and parabolic equations on metric\n  graphs",
        "A sharper Lyapunov-Katz central limit error bound for i.i.d. summands\n  Zolotarev-close to normal",
        "Unitary Friedberg-Jacquet periods and their twists: Fundamental lemmas",
        "Quantum One-Time Memories from Stateless Hardware, Random Access Codes,\n  and Simple Nonconvex Optimization",
        "A monotonicity-based globalization of the level-set method for inclusion\n  detection",
        "Attention-Based Functional-Group Coarse-Graining: A Deep Learning\n  Framework for Molecular Prediction and Design",
        "Separation control applied to the turbulent flow around a NACA4412 wing\n  section",
        "A Cheeger-type inequality for the drift Laplacian with Wentzell-type\n  boundary condition",
        "Complete intersection algebras with binomial Macaulay dual generator",
        "Time derivative estimates for parabolic $p$-Laplace equations and\n  applications to optimal regularity",
        "A family of convolution operators, part two",
        "$\\Lambda$CDM model against redshift-binned data: A mock analysis based\n  on SNIa and Cosmic Chronometers",
        "Two characterizations of Sheffer-Dunkl sequences",
        "Search for resonance-enhanced $CP$ and angular asymmetries in the\n  $\\Lambda^+_{c}\\to p\\mu^+\\mu^-$ decay at LHCb",
        "Revealing Local Structures through Machine-Learning- Fused Multimodal\n  Spectroscopy",
        "The Effect of Foreground Galaxies on the Estimation of the Hubble\n  Constant from Type Ia Supernovae",
        "Remark to a Theorem of van Geemen",
        "Towards a Generalized SA Model: Symbolic Regression-Based Correction for\n  Separated Flows",
        "BCS-like formula for $T_c$ does not necessarily imply BCS pairing\n  mechanism",
        "Stochastic quantization of $\\lambda \\phi_2^4$- theory in 2-d Moyal space",
        "Quantum chaos at finite temperature in local spin Hamiltonians",
        "Viscosity, entanglement and acceleration",
        "Fast Jet Finding in Julia",
        "Quantitative Magnetohydrodynamic Modelling of Flux Pumping in ASDEX\n  Upgrade"
      ],
      "abstract":[
        "In this study, we present a comprehensive analysis of an unidentified\npoint-like ultra-high-energy (UHE) $\\gamma$-ray source, designated as 1LHAASO\nJ1740+0948u, situated in the vicinity of the middle-aged pulsar PSR J1740+1000.\nThe detection significance reached 17.1$\\sigma$ (9.4$\\sigma$) above 25$\\,$TeV\n(100$\\,$TeV). The source energy spectrum extended up to 300$\\,$TeV, which was\nwell fitted by a log-parabola function with $N0 = (1.93\\pm0.23) \\times 10^{-16}\n\\rm{TeV^{-1}\\,cm^{-2}\\,s^{-2}}$, $\\alpha = 2.14\\pm0.27$, and $\\beta =\n1.20\\pm0.41$ at E0 = 30$\\,$TeV. The associated pulsar, PSR J1740+1000, resides\nat a high galactic latitude and powers a bow-shock pulsar wind nebula (BSPWN)\nwith an extended X-ray tail. The best-fit position of the gamma-ray source\nappeared to be shifted by $0.2^{\\circ}$ with respect to the pulsar position. As\nthe (i) currently identified pulsar halos do not demonstrate such offsets, and\n(ii) centroid of the gamma-ray emission is approximately located at the\nextension of the X-ray tail, we speculate that the UHE $\\gamma$-ray emission\nmay originate from re-accelerated electron\/positron pairs that are advected\naway in the bow-shock tail.",
        "The short-term X-ray variability of Cyg X-1 can be interpreted as random\noccurrence of mini-flares known as the shots, whose physical nature is still\nunclear. We propose a new algorithm for shot identification in the X-ray light\ncurve, based on baseline detection and template fitting. Compared with previous\ntechniques, our algorithm allows us to detect shots with lower amplitudes and\nshorter time separations. With NICER observations, we find that, after\ncorrection for detection sensitivity, both the shot amplitude and recurrence\nrate are positively scaled with the mean count rate, while the recurrence rate\nhas a much higher dependence on the count rate. These suggest that a higher\nmass accretion rate will drive more and slightly larger shots. We also find\nthat the abrupt hardening near the shot peak found in previous studies is\nattributed to different shot profiles in different energy bands; there is no\nneed to involve a rapid physical process to suddenly harden the emitting\nspectrum.",
        "The cancellation of high-frequency laser phase noise using feedforward\ntechniques, as opposed to feedback methods, has achieved significant\nadvancements in recent years. However, directly applying existing feedforward\ntechniques to laser systems based on nonlinear conversion still faces\nsubstantial challenges. Here, we propose and demonstrate a feedforward scheme\nthat suppresses phase noise in frequency-doubled light by utilizing phase noise\ninformation of its fundamental pump. This scheme is enabled by the fact that\nthe phase jitter of the frequency-doubled light is simply twice that of the\npump, except for a first-order low-pass filtering effect introduced by the SHG\nenhancement cavity. Testing this method on a 420-nm frequency-doubled laser\nsystem, we realize a 25-dB suppression of the servo noise bump near 1 MHz on\nthe 420-nm light, and an average suppression of 30 dB for strong injected noise\nranging from 100 kHz to 20 MHz. This scheme shows promising potential for\napplications requiring blue or ultraviolet light with minimal high-frequency\nphase noise, such as precision control of atoms and molecules.",
        "We study the exact fluctuating hydrodynamics of the scaled Light-Heavy model\n(sLH), in which two species of particles (light and heavy) interact with a\nfluctuating surface. This model is similar in definition to the unscaled\nLight-Heavy model (uLH), except it uses rates scaled with the system size. The\nconsequence, it turns out, is a phase diagram that differs from that of the\nunscaled model. We derive the fluctuating hydrodynamics for this model using an\naction formalism involving the construction of path integrals for the\nprobability of different states that give the complete macroscopic picture\nstarting from the microscopic one. This is then used to obtain the two-point\nsteady-state (static) correlation functions between fluctuations in the two\ndensity fields in the homogeneous phase. We show that these theoretical results\nmatch well with microscopic simulations away from the critical line. We derive\nan exponentially decaying form for the two-point steady-state correlation\nfunction with a correlation length that diverges as the critical line is\napproached. Finally, we also compute the dynamic correlations in the\nhomogeneous phase and use them to determine the relaxation dynamics as well as\nthe dynamic exponents of the system.",
        "We investigate uniqueness of solutions to certain classes of elliptic and\nparabolic equations posed on metric graphs. In particular, we address the\nlinear Schr\\\"odinger equation with a potential, and the heat equation with a\nvariable density. We assume suitable growth conditions on the solutions, which\nare related to the behaviour at infinity of the potential or of the density.",
        "We prove a central limit error bound for convolution powers of laws with\nfinite moments of order $r \\in \\mathopen]2,3\\mathclose]$, taking a closeness of\nthe laws to normality into account. Up to a universal constant, this\ngeneralises the case of $r=3$ of the sharpening of the Berry (1941) - Esseen\n(1942) theorem obtained by Mattner (2024), namely by sharpening here the Katz\n(1963) error bound for the i.i.d. case of Lyapunov's (1901) theorem. Our proof\nuses a partial generalisation of the theorem of Senatov and Zolotarev (1986)\nused for the earlier special case. A result more general than our main one\ncould be obtained by using instead another theorem of Senatov (1980), but\nunfortunately an auxiliary inequality used in the latter's proof is wrong.",
        "We formulate a global conjecture for the automorphic period integral\nassociated to the symmetric pairs defined by unitary groups over number fields,\ngeneralizing a theorem of Waldspurger's toric period for $\\mathrm{GL}(2)$. We\nintroduce a new relative trace formula to prove our global conjecture under\nsome local hypotheses. A new feature is the presence of the relative endoscopy.\nIn this paper we prove the main local theorem: a new relative fundamental lemma\ncomparing certain orbital integrals of functions matched in terms of Hironaka\nand Satake transforms.",
        "We present a construction of one-time memories (OTMs) using\nclassical-accessible stateless hardware, building upon the work of Broadbent et\nal. and Behera et al.. Unlike the aforementioned work, our approach leverages\nquantum random access codes (QRACs) to encode two classical bits, $b_0$ and\n$b_1$, into a single qubit state $\\mathcal{E}(b_0 b_1)$ where the receiver can\nretrieve one of the bits with a certain probability of error. To prove\nsoundness, we define a nonconvex optimization problem over POVMs on\n$\\mathbb{C}^2$. This optimization gives an upper bound on the probability of\ndistinguishing bit $b_{1-\\alpha}$ given that the probability that the receiver\nrecovers bit $b_\\alpha$ is high. Assuming the optimization is sufficiently\naccurate, we then prove soundness against a polynomial number of classical\nqueries to the hardware.",
        "We focus on a geometrical inverse problem that involves recovering\ndiscontinuities in electrical conductivity based on boundary measurements. This\nproblem serves as a model to introduce a shape recovery technique that merges\nthe monotonicity method with the level-set method. The level-set method,\ncommonly used in shape optimization, often relies heavily on the accuracy of\nthe initial guess. To overcome this challenge, we utilize the monotonicity\nmethod to generate a more precise initial guess, which is then used to\ninitialize the level-set method. We provide numerical results to illustrate the\neffectiveness of this combined approach.",
        "Machine learning (ML) offers considerable promise for the design of new\nmolecules and materials. In real-world applications, the design problem is\noften domain-specific, and suffers from insufficient data, particularly labeled\ndata, for ML training. In this study, we report a data-efficient, deep-learning\nframework for molecular discovery that integrates a coarse-grained\nfunctional-group representation with a self-attention mechanism to capture\nintricate chemical interactions. Our approach exploits group-contribution\ntheory to create a graph-based intermediate representation of molecules,\nserving as a low-dimensional embedding that substantially reduces the data\ndemands typically required for training. By leveraging the self-attention\nmechanism to learn subtle chemical context, our method consistently outperforms\nconventional methods in predicting multiple thermophysical properties. In a\ncase study focused on adhesive polymer monomers, we train on a limited dataset\ncomprising just 6,000 unlabeled and 600 labeled monomers. The resulting\nchemistry prediction model achieves over 92% accuracy in forecasting properties\ndirectly from SMILES strings, exceeding the performance of current\nstate-of-the-art techniques. Furthermore, the latent molecular embedding is\ninvertible, allowing the design pipeline to incorporate a decoder that can\nautomatically generate new monomers from the learned chemical subspace. We\nillustrate this functionality by targeting high and low glass transition\ntemperatures ($T_g$), successfully identifying novel candidates whose $T_g$\nextends beyond the range observed in the training data. The ease with which our\ncoarse-grained, attention-based framework navigates both chemical diversity and\ndata scarcity offers a compelling route to accelerate and broaden the search\nfor functional materials.",
        "We carried out high-resolution large-eddy simulations (LESs) to investigate\nthe effects of several separation-control approaches on a NACA4412 wing section\nwith spanwise width of $L_z = 0.6$ at an angle of attack of $AoA=11^{\\circ}$ at\na Reynolds number of $Re_c = 200,000$ based on chord length $c$ and free-stream\nvelocity $U_{\\infty}$. Two control strategies were considered: (1) steady\nuniform blowing and\/or suction on the suction and\/or pressure sides, and (2)\nperiodic control on the suction side. A wide range of control configurations\nwere evaluated in terms of aerodynamic efficiency (i.e., lift-to-drag ratio)\nand separation delay. Uniform blowing and\/or suction effectively delayed flow\nseparation, leading to a lift increase of up to $11\\%$, but yielded only\nmarginal improvements in aerodynamic efficiency. In contrast, periodic control\nneither enhanced separation delay nor improved efficiency. A detailed analysis\nof the interaction between uniform blowing and\/or suction and turbulent\nboundary layers (TBLs) over the wing was performed, including assessments of\n(1) integral boundary-layer quantities, (2) turbulence statistics, and (3)\npower-spectral densities. Significant modifications in Reynolds stresses and\nspectral characteristics were observed. To the authors' best knowledge, this is\nthe first numerical study utilizing high-resolution LESs to provide\ncomprehensive assessments on separation control.",
        "We prove lower bounds for the first non-trivial eigenvalue of the drift\nLaplacian on manifolds with Wentzell-type boundary condition in terms of some\nCheeger-type constants for bulk-boundary interactions. Our results are in the\nspirit of Cheeger's classical inequality.",
        "In this paper, we characterize all Artinian complete intersection\n$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In\naddition, we prove that such\n  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong\nLefschetz property.",
        "We establish the boundedness of time derivatives of solutions to parabolic\n$p$-Laplace equations. Our approach relies on the Bernstein technique combined\nwith a suitable approximation method. As a consequence, we obtain an optimal\nregularity result with a connection to the well-known $C^{p'}$-conjecture in\nthe elliptic setting. Finally, we extend our method to treat global regularity\nresults for both fully nonlinear and general quasilinear degenerate parabolic\nproblems.",
        "We study a family of convolution operators. Their regarding Fourier\nmultipliers are defined in terms of distributions having singularity on the\nlight-cone in $\\mathbb{R}^{n+1}$. As a result, we give a new approach to the\nBochner-Riesz summability.",
        "Despite the broad successes of the flat $\\Lambda$CDM model and its fitness to\nthe various cosmological observations, it confronts challenges stemming from\nanomalies in the measurements of the Hubble constant ($H_0$) and the amplitude\nof matter fluctuations ($\\sigma_8$). These inconsistencies have necessitated a\nreassessment of the model parameters, with a particular focus on their\npotential dependence on redshift. This study pioneers a new investigation to\nprobe this redshift dependency by generating mock data simulated from\nobservational data of Type Ia supernovae (SNIa) and cosmic chronometers (CC),\nthereby increasing the data density in this field. By sorting the data into\nhigh-redshift and low-redshift bins, we aim to refine the cosmological\nconstraints on the parameters of the $\\Lambda$CDM model and determine whether\nthe noted dependence on redshift is due to a lack of high-redshift\nobservational data or if they signify intrinsic issues within the model itself.\nOur approach employs the Markov Chain Monte Carlo (MCMC) algorithm to minimize\nthe $\\chi^2$ function, thus tightening the cosmological constraints. Our\nfindings within the mock analysis reveal discrepancies between the values of\n$\\Omega_{m0}$ and $H_0$ derived from the mock data bins with high redshift and\nlow redshift, indicating the potential deviation of the standard $\\Lambda$ CDM\ncosmology from the high-redshift SNIa and CC data. If this deviation proposes a\nnew physics beyond the standard model, then with better quality future data\ntracking the new physics, these discrepancies will be statistically\nsignificant.",
        "Sheffer polynomials can be characterized using different Stieltjes integrals.\nThese families of polynomials have been recently extended to the Dunkl context.\nIn this way some classical operators as the derivative operator or the\ndifference operator are replaced as analogous operators in the Dunkl universe.\nIn this paper we establish two Stieltjes integrals that help us to characterize\nthe Sheffer-Dunkl polynomials.",
        "The first measurement of the $CP$ asymmetry of the decay rate ($A_{CP}$) and\nthe $CP$ average ($\\Sigma A_{\\text{FB}}$) and $CP$ asymmetry ($\\Delta\nA_{\\text{FB}}$) of the forward-backward asymmetry in the muon system of\n$\\mathit{\\Lambda}^+_c\\to p\\mu^+\\mu^-$ decays is reported. The measurement is\nperformed using a data sample of proton-proton collisions, recorded by the LHCb\nexperiment from 2016 to 2018 at a center-of-mass energy of 13$\\text{ TeV}$,\nwhich corresponds to an integrated luminosity of 5.4$\\text{ fb}^{-1}$. The\nasymmetries are measured in two regions of dimuon mass near the $\\phi$-meson\nmass peak. The dimuon-mass integrated results are \\begin{align*} A_{CP} &=\n(-1.1 \\pm 4.0 \\pm 0.5)\\%,\\\\ \\Sigma A_{\\text{FB}} &= (\\phantom{-}3.9 \\pm 4.0 \\pm\n0.6)\\%,\\\\ \\Delta A_{\\text{FB}} &= (\\phantom{-}3.1 \\pm 4.0 \\pm 0.4)\\%,\n\\end{align*} where the first uncertainty is statistical and the second\nsystematic. The results are consistent with the conservation of $CP$ symmetry\nand the Standard Model expectations.",
        "Atomistic structures of materials offer valuable insights into their\nfunctionality. Determining these structures remains a fundamental challenge in\nmaterials science, especially for systems with defects. While both experimental\nand computational methods exist, each has limitations in resolving nanoscale\nstructures. Core-level spectroscopies, such as x-ray absorption (XAS) or\nelectron energy-loss spectroscopies (EELS), have been used to determine the\nlocal bonding environment and structure of materials. Recently, machine\nlearning (ML) methods have been applied to extract structural and bonding\ninformation from XAS\/EELS, but most of these frameworks rely on a single data\nstream, which is often insufficient. In this work, we address this challenge by\nintegrating multimodal ab initio simulations, experimental data acquisition,\nand ML techniques for structure characterization. Our goal is to determine\nlocal structures and properties using EELS and XAS data from multiple elements\nand edges. To showcase our approach, we use various lithium nickel manganese\ncobalt (NMC) oxide compounds which are used for lithium ion batteries,\nincluding those with oxygen vacancies and antisite defects, as the sample\nmaterial system. We successfully inferred local element content, ranging from\nlithium to transition metals, with quantitative agreement with experimental\ndata. Beyond improving prediction accuracy, we find that ML model based on\nmultimodal spectroscopic data is able to determine whether local defects such\nas oxygen vacancy and antisites are present, a task which is impossible for\nsingle mode spectra or other experimental techniques. Furthermore, our\nframework is able to provide physical interpretability, bridging spectroscopy\nwith the local atomic and electronic structures.",
        "Type Ia supernovae are the established `standard candle' in the construction\nof the Hubble diagram out to high luminosity distances. Since the Hubble\nconstant that best fits observations of these supernovae often turns out to be\nhigh compared to fits to other data, they are currently being investigated for\npossible systematic effects, with many studies focusing on the calibration of\nthe distance ladder in the local Universe. Here we present a simulation-based\nassessment of another type of systematic effect, related to the chance that the\nline of sight to a distant supernova passes close to a foreground galaxy. We\nconsider two cases separately: First, the foreground galaxy may block the line\nof sight so that the supernova is not observed. Since foreground galaxies are\ncorrelated with overdensities that typically magnify the flux of background\nsources, this effect leads to a systematic removal of lensed supernovae from\nthe sample, biasing the high-redshift Hubble diagram towards demagnified\n(fainter) supernovae. Second, if the supernova can be observed, its proximity\nto the foreground galaxy can lead to an incorrect host assignment, especially\nif the true host has a low surface brightness. Since foreground galaxies are\ntypically found at lower redshifts, this effect introduces another systematic\nbias. The probability of line-of-sight alignments with foreground galaxies\nincreases with redshift and therefore affects distant supernovae more strongly.\nWe find that both effects are small, but the effect of host misidentification\nshould be included in the systematic error budget at current levels of\nmeasurement precision.",
        "In [ Ge], Bert van Geemen computed the dimension of the space of the fourth\npower of the theta nullwerte.\n  In [SM2], it has been observe that all linear relations between the\n$\\theta_m^4$ are consequences of the quartic Riemann relations.\n  In this note, we want to give a new proof of these result and extend them.",
        "This study focuses on the numerical simulation of high Reynolds number\nseparated flows and proposes a data-driven approach to improve the predictive\ncapability of the SA turbulence model. First, data assimilation was performed\non two typical airfoils with high angle-of-attack separated flows to obtain a\nhigh-fidelity flow field dataset. Based on this dataset, a white-box model was\ndeveloped using symbolic regression to modify the production term of the SA\nmodel. To validate the effectiveness of the modified model, multiple\nrepresentative airfoils and wings, such as the SC1095 airfoil, DU91-W2-250\nairfoil, and ONERA-M6 wing, were selected as test cases. A wide range of flow\nconditions was considered, including subsonic to transonic regimes, Reynolds\nnumbers ranging from hundreds of thousands to tens of millions, and angles of\nattack varying from small to large. The results indicate that the modified\nmodel significantly improves the prediction accuracy of separated flows while\nmaintaining the predictive capability for attached flows. It notably enhances\nthe reproduction of separated vortex structures and flow separation locations,\nreducing the mean relative error in lift prediction at stall angles by 69.2%\nand improving computational accuracy by more than three times. Furthermore,\nvalidation using a zero-pressure-gradient flat plate case confirms the modified\nmodel's ability to accurately predict the turbulent boundary layer velocity\nprofile and skin friction coefficient distribution. The findings of this study\nprovide new insights and methodologies for the numerical simulation of high\nReynolds number separated flows, contributing to more accurate modeling of\ncomplex flow phenomena in engineering applications.",
        "In the BCS theory of superconductivity, an instability towards pairing\ndevelops at arbitrary weak dimensionless coupling $\\lambda$ due to a divergence\nof logarithmic perturbative series for the pairing susceptibility (Cooper\nlogarithms) at $T_c \\sim \\omega_0 e^{-1\/\\lambda}$, where $\\omega_0$ is an\nenergy cutoff. On the contrary, in many models of superconductivity out of a\nnon-Fermi liquid, Cooper logarithm is absent and superconductivity emerges only\nwhen $\\lambda$ exceeds a certain threshold. We argue that there are situations\nwhen there is no threshold and at weak coupling the formula for $T_c$ is\nBCS-like, yet the origin of the pairing instability is fundamentally different\nfrom that in the BCS scenario. As an example, we revisit superconductivity in a\nmetal at the onset of $(\\pi,\\pi)$ spin-density-wave order. Earlier studies of\nthis problem found no threshold and a BCS-like expression for $T_c$ at weak\ncoupling. We argue that, despite this, the pairing is not caused by the Cooper\nlogarithm and in many respects is qualitatively similar to that in non-Fermi\nliquids.",
        "There is strong evidence for the conjecture that the $\\lambda \\phi^4$ QFT-\nmodel on 4-dimensional non-commutative Moyal space can be non-perturbatively\nconstructed. As preparation, in this paper we construct the 2-dimensional case\nwith the method of stochastic quantization. We show the local well-posedness\nand global well-posedness of the stochastic quantization equation, leading to a\nconstruction of the Moyal $\\lambda \\phi^4_2$ measure for any non-negative\ncoupling constant $\\lambda$.",
        "Understanding the emergence of chaos in many-body quantum systems away from\nsemi-classical limits, particularly in spatially local interacting spin\nHamiltonians, has been a long-standing problem. In these intrinsically quantum\nregimes, quantum chaos has been primarily understood through the correspondence\nbetween the eigensystem statistics of midspectrum eigenstates and the universal\nstatistics described by random matrix theory (RMT). However, this\ncorrespondence no longer holds for finite-temperature eigenstates. Here we show\nthat the statistical properties of finite-temperature eigenstates of quantum\nchaotic Hamiltonians can be accurately described by pure random states\nconstrained by a local charge, with the average charge density of the\nconstrained random state ensemble playing the same role as the average energy\ndensity of the eigenstates. By properly normalizing the energy density using a\nsingle Hamiltonian-dependent parameter that quantifies the typical energy per\ndegree of freedom, we find excellent agreement between the entanglement entropy\nstatistics of eigenstates and that of constrained random states. Interestingly,\nin small pockets of Hamiltonian parameter phase space which we previously\nidentified as `maximally chaotic' [PRX 14, 031014 (2024)], we find excellent\nagreement not only at the level of the first moment, including O(1)\ncorrections, but also at the level of statistical fluctuations. These results\nshow that notions of maximal chaos -- in terms of how much randomness\neigenstates contain -- can still be defined at finite temperature in physical\nHamiltonian models away from semi-classical and large-$N$ limits.",
        "The Minkowski vacuum in an accelerated frame behaves like a fluid that has\nnot only a finite temperature due to the Unruh effect, but also a finite shear\nviscosity. Moreover, the ratio of this viscosity to the entropy density exactly\nsatisfies the Kovtun-Son-Starinets (KSS) bound, inspired by the string theory $\n\\eta\/s=1\/4\\pi $. The origin of this viscosity is purely kinematical and is\nbelieved to be related to entanglement introduced by the Rindler horizon. We\ndirectly calculate the viscosity, entropy density, and their ratio for massless\nfields with spins 1\/2 and 1. We show that locally the ratio of viscosity to\nentropy density can be below the limiting value $ 1\/4\\pi $ at distances of the\norder of the thickness of the membrane corresponding to the stretched horizon,\nand is described by the universal function for different spins. In particular,\non the membrane surface $ \\eta\/s=1\/8\\pi $.",
        "Jet reconstruction remains a critical task in the analysis of data from HEP\ncolliders. We describe in this paper a new, highly performant, Julia package\nfor jet reconstruction, JetReconstruction.jl, which integrates into the growing\necosystem of Julia packages for HEP. With this package users can run sequential\nreconstruction algorithms for jets. In particular, for LHC events, the\nAnti-${k}_\\text{T}$, Cambridge\/Aachen and Inclusive-${k}_\\text{T}$ algorithms\ncan be used. For FCCee studies the use of alternative algorithms such as the\nGeneralised ${k}_\\text{T}$ for $e^+e^-$ and Durham are also supported.\n  The performance of the core algorithms is better than Fastjet's C++\nimplementation, for typical LHC and FCCee events, thanks to the Julia\ncompiler's exploitation of single-instruction-multiple-data (SIMD), as well as\nergonomic compact data layouts.\n  The full reconstruction history is made available, allowing inclusive and\nexclusive jets to be retrieved. The package also provides the means to\nvisualise the reconstruction. Substructure algorithms have been added that\nallow advanced analysis techniques to be employed. The package can read event\ndata from EDM4hep files and reconstruct jets from these directly, opening the\ndoor to FCCee and other future collider studies in Julia.",
        "The sawtooth-free hybrid scenario has been achieved recently in ASDEX Upgrade\n(AUG) with applied non-inductive current sources and auxiliary heating [A.\nBurckhart et al 2023 Nucl. Fusion 63 126056]. Control experiments in AUG\nsuggest that the self-regulating magnetic flux pumping mechanism, characterized\nby anomalous current redistribution, is responsible for clamping the central\nsafety factor (q_0) close to unity, thereby preventing the sawtooth onset. This\nwork presents a numerical and theoretical investigation of flux pumping in the\nAUG hybrid scenario based on the two-temperature, visco-resistive, full\nmagnetohydrodynamic (MHD) model with the JOREK code. To quantitatively model\nthe flux pumping, we choose realistic parameters, plasma configurations, and\nsource terms based on AUG experiments. During the initial saturation stage of\nthe unstable 1\/1 quasi-interchange mode (on millisecond timescales), q_0\nexhibits fast under-damped oscillation and reaches a value closer to unity,\nwhich is attributed to the self-regulation of core plasma and the fast dynamo\neffect on the order of V\/m. On the longer resistive diffusion timescale of\nseconds, the slow negative dynamo effect on the order of mV\/m induced by the\n1\/1 MHD instability plays an effective role in flux pumping, which provides\nquantitative agreement with experimental observations for the first time. The\nfinal saturated 1\/1 MHD instability exhibits features of the quasi-interchange\nmode and tearing mode, and the associated convective plasma flow velocity is a\nfew m\/s. The toroidal negative electric field from the slow dynamo dominantly\noffsets the positive current drive and continuously redistributes the current\ndensity and pressure. As a result, q_0 is maintained close to unity due to the\nlow-shear profiles of current density and pressure in the plasma core, and the\nsystem enters a sawtooth-free and quasi-stationary helical state."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Screening for lung cancer: 2023 guideline update from the American Cancer Society",
    "start_abstract":"Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies",
        "The Electric Location-Routing Problem: Improved Formulations and Effects\n  of Nonlinear Charging",
        "DiffVSR: Revealing an Effective Recipe for Taming Robust Video\n  Super-Resolution Against Complex Degradations",
        "Entanglement dynamics of many-body quantum states with evolving system\n  conditions",
        "An Accurate Computational Approach for Partial Likelihood Using\n  Poisson-Binomial Distributions",
        "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel\n  Tool Invocation",
        "Two-sided Remotely Almost Periodic Solutions of Ordinary Differential\n  Equations in Banach Spaces",
        "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "Comparison of Bending-Energy Discretization Methods for Anisotropic\n  Meshes in Morphogenetic Simulations",
        "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
        "Counting for rigidity under projective transformations in the plane",
        "Green points in the reals",
        "Fault-tolerance of [[6, 1, 3]] non-CSS code family generated using\n  measurements on graph states",
        "Design of resilient structures by randomization and bistability",
        "Getting SMARTER for Motion Planning in Autonomous Driving Systems"
      ],
      "abstract":[
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones.",
        "Electric Location-Routing models (ELRP) can contribute to the effective\nplanning of electric vehicles (EVs) fleets and charging infrastructure within\nEV logistic networks because it simultaneously combines routing and location\ndecisions to find optimal solutions to the network design. This study\nintroduces ELRP models that incorporate nonlinear charging process, multiple\ncharging station types and develop new improved formulations to the problem.\nExisting ELRP models commonly assume a linear charging process and employ a\nnode-based formulation for tracking EV energy and time consumption. In\ncontrast, we propose novel formulations offering alternative approaches for\nmodeling EV energy, time consumption, and nonlinear charging. Through extensive\ncomputational experiments, our analysis demonstrates the effectiveness of the\nnew formulations, reducing the average gap from 29.1% to 11.9%, yielding\nimproved solutions for 28 out of 74 instances compared to the node-based\nformulation. Moreover, our findings provide valuable insights into the\nstrategic implications of nonlinear charging in ELRP decision-making, offering\nnew perspectives for planning charging infrastructure in EV logistic networks.",
        "Diffusion models have demonstrated exceptional capabilities in image\nrestoration, yet their application to video super-resolution (VSR) faces\nsignificant challenges in balancing fidelity with temporal consistency. Our\nevaluation reveals a critical gap: existing approaches consistently fail on\nseverely degraded videos--precisely where diffusion models' generative\ncapabilities are most needed. We identify that existing diffusion-based VSR\nmethods struggle primarily because they face an overwhelming learning burden:\nsimultaneously modeling complex degradation distributions, content\nrepresentations, and temporal relationships with limited high-quality training\ndata. To address this fundamental challenge, we present DiffVSR, featuring a\nProgressive Learning Strategy (PLS) that systematically decomposes this\nlearning burden through staged training, enabling superior performance on\ncomplex degradations. Our framework additionally incorporates an Interweaved\nLatent Transition (ILT) technique that maintains competitive temporal\nconsistency without additional training overhead. Experiments demonstrate that\nour approach excels in scenarios where competing methods struggle, particularly\non severely degraded videos. Our work reveals that addressing the learning\nstrategy, rather than focusing solely on architectural complexity, is the\ncritical path toward robust real-world video super-resolution with diffusion\nmodels.",
        "The entanglement analysis of a pure state of a many-body quantum system\nrequires a prior information about its density matrix\/ state matrix, obtained\nin principle by solving the Hamiltonian matrix. The missing information due to\ncomplexity of the many-body interactions however renders it necessary to\nconsider an ensemble of Hamiltonians and thereby an ensemble of pure states.\nThis in turn leaves a statistical description of the entanglement measures as\nthe only option.\n  We consider physical Hamiltonians that can be modelled by a multiparametric\nGaussian ensembles, theoretically derive the state ensembles for its\neigenstates and analyze the effect of varying system conditions on its\nentanglement statistics. Our approach leads to a single parametric based common\nmathematical formulation for the evolution of the statistics of different state\nensembles. The parameter turns out to be a single functional of the system\nparameters and thereby reveals a deep web of connection underlying different\nquantum states.",
        "In a Cox model, the partial likelihood, as the product of a series of\nconditional probabilities, is used to estimate the regression coefficients. In\npractice, those conditional probabilities are approximated by risk score ratios\nbased on a continuous time model, and thus result in parameter estimates from\nonly an approximate partial likelihood. Through a revisit to the original\npartial likelihood idea, an accurate partial likelihood computing method for\nthe Cox model is proposed, which calculates the exact conditional probability\nusing the Poisson-binomial distribution. New estimating and inference\nprocedures are developed, and theoretical results are established for the\nproposed computational procedure. Although ties are common in real studies,\ncurrent theories for the Cox model mostly do not consider cases for tied data.\nIn contrast, the new approach includes the theory for grouped data, which\nallows ties, and also includes the theory for continuous data without ties,\nproviding a unified framework for computing partial likelihood for data with or\nwithout ties. Numerical results show that the proposed method outperforms\ncurrent methods in reducing bias and mean squared error, while achieving\nimproved confidence interval coverage rates, especially when there are many\nties or when the variability in risk scores is large. Comparisons between\nmethods in real applications have been made.",
        "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT\/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess\/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https:\/\/corn0205.github.io\/",
        "The aim of this paper is studying the two-sided remotely almost periodic\nsolutions of ordinary differential equations in Banach spaces of the form\n$x'=A(t)x+f(t)+F(t,x)$ with two-sided remotely almost periodic coefficients if\nthe linear equation $x'=A(t)x$ satisfies the condition of exponential\ntrichotomy and nonlinearity $F$ is \"small\".",
        "Recent advances in diffusion models have greatly improved text-driven video\ngeneration. However, training models for long video generation demands\nsignificant computational power and extensive data, leading most video\ndiffusion models to be limited to a small number of frames. Existing\ntraining-free methods that attempt to generate long videos using pre-trained\nshort video diffusion models often struggle with issues such as insufficient\nmotion dynamics and degraded video fidelity. In this paper, we present\nBrick-Diffusion, a novel, training-free approach capable of generating long\nvideos of arbitrary length. Our method introduces a brick-to-wall denoising\nstrategy, where the latent is denoised in segments, with a stride applied in\nsubsequent iterations. This process mimics the construction of a staggered\nbrick wall, where each brick represents a denoised segment, enabling\ncommunication between frames and improving overall video quality. Through\nquantitative and qualitative evaluations, we demonstrate that Brick-Diffusion\noutperforms existing baseline methods in generating high-fidelity videos.",
        "Accurately modeling bending energy in morphogenetic simulations is crucial,\nespecially when dealing with anisotropic meshes where remeshing is infeasible\ndue to the biologically meaningful entities of vertex positions (e.g., cells).\nThis study addresses the underexplored question of which bending-energy\ndiscretization methods are most accurate and suitable for such simulations.\n  The evaluation consists of two stages: First, the accuracy of each method is\ntested by comparing predicted bending energy and force against theoretical\nvalues for two benchmark cases--a wrinkled planar sheet and a smooth spherical\nsheet. Second, we simulate the formation of wrinkles in a planar sheet caused\nby anisotropic cell division, analyzing the resulting wavenumber patterns for\ntwo division orientations: uniaxial and random.\n  The results highlight that the choice of the optimal discretization method\ndepends on the application. For simulations requiring precise quantitative\npredictions, the Hamann model demonstrates superior accuracy. Conversely, for\nsimulations where qualitative trends in morphology are of primary interest, the\nJ\\\"ulicher model provides a computationally efficient alternative. These\nfindings provide guidance for selecting appropriate bending-energy\ndiscretization methods in morphogenetic simulations, ultimately leading to more\naccurate and efficient modeling of complex biological forms.",
        "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
        "Let $P$ be a set of points and $L$ a set of lines in the (extended) Euclidean\nplane, and $I \\subseteq P\\times L$, where $i =(p,l) \\in I$ means that point $p$\nand line $l$ are incident. The incidences can be interpreted as quadratic\nconstraints on the homogeneous coordinates of the points and lines. We study\nthe space of incidence preserving motions of the given incidence structure by\nlinearizing the system of quadratic equations. The Jacobian of the quadratic\nsystem, our projective rigidity matrix, leads to the notion of\nindependence\/dependence of incidences. Column dependencies correspond to\ninfinitesimal motions. Row dependencies or self-stresses allow for new\ninterpretations of classical geometric incidence theorems. We show that\nself-stresses are characterized by a 3-fold balance. As expected, infinitesimal\n(first order) projective rigidity as well as second order projective rigidity\nimply projective rigidity but not conversely. Several open problems and\npossible generalizations are indicated.",
        "We construct an expansion of a real closed field by a multiplicative subgroup\nadapting Poizat's theory of green points. Its theory is strongly dependent, and\nevery open set definable in a model of this theory is semialgebraic. We prove\nthat the real field with a dense family of logarithmic spirals, proposed by\nZilber, satisfies our theory.",
        "We construct and analyze the fault tolerance of $[[6,1,3]]$ non-CSS quantum\nerror correcting code under the anisotropic and depolarizing noise models. This\nrate-optimized code achieves fault-tolerance using a single ancilla qubit for\nsyndrome measurement under anisotropic noise conditions. This method was called\nfault-tolerance using bare ancilla by Brown \\emph{et al.} We give explicit\nconstruction of the code using measurements on non-planar graph states. We also\nargue that using our approach, we can construct a family of such fault-tolerant\ncodes. This method fills a notable gap in constructing fault-tolerant non-CSS\ncode families.",
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com\/huawei-noah\/SMARTS."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects",
    "start_abstract":"Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies",
        "The Electric Location-Routing Problem: Improved Formulations and Effects\n  of Nonlinear Charging",
        "DiffVSR: Revealing an Effective Recipe for Taming Robust Video\n  Super-Resolution Against Complex Degradations",
        "Entanglement dynamics of many-body quantum states with evolving system\n  conditions",
        "An Accurate Computational Approach for Partial Likelihood Using\n  Poisson-Binomial Distributions",
        "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel\n  Tool Invocation",
        "Two-sided Remotely Almost Periodic Solutions of Ordinary Differential\n  Equations in Banach Spaces",
        "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "Comparison of Bending-Energy Discretization Methods for Anisotropic\n  Meshes in Morphogenetic Simulations",
        "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
        "Counting for rigidity under projective transformations in the plane",
        "Green points in the reals",
        "Fault-tolerance of [[6, 1, 3]] non-CSS code family generated using\n  measurements on graph states",
        "Design of resilient structures by randomization and bistability",
        "Getting SMARTER for Motion Planning in Autonomous Driving Systems"
      ],
      "abstract":[
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones.",
        "Electric Location-Routing models (ELRP) can contribute to the effective\nplanning of electric vehicles (EVs) fleets and charging infrastructure within\nEV logistic networks because it simultaneously combines routing and location\ndecisions to find optimal solutions to the network design. This study\nintroduces ELRP models that incorporate nonlinear charging process, multiple\ncharging station types and develop new improved formulations to the problem.\nExisting ELRP models commonly assume a linear charging process and employ a\nnode-based formulation for tracking EV energy and time consumption. In\ncontrast, we propose novel formulations offering alternative approaches for\nmodeling EV energy, time consumption, and nonlinear charging. Through extensive\ncomputational experiments, our analysis demonstrates the effectiveness of the\nnew formulations, reducing the average gap from 29.1% to 11.9%, yielding\nimproved solutions for 28 out of 74 instances compared to the node-based\nformulation. Moreover, our findings provide valuable insights into the\nstrategic implications of nonlinear charging in ELRP decision-making, offering\nnew perspectives for planning charging infrastructure in EV logistic networks.",
        "Diffusion models have demonstrated exceptional capabilities in image\nrestoration, yet their application to video super-resolution (VSR) faces\nsignificant challenges in balancing fidelity with temporal consistency. Our\nevaluation reveals a critical gap: existing approaches consistently fail on\nseverely degraded videos--precisely where diffusion models' generative\ncapabilities are most needed. We identify that existing diffusion-based VSR\nmethods struggle primarily because they face an overwhelming learning burden:\nsimultaneously modeling complex degradation distributions, content\nrepresentations, and temporal relationships with limited high-quality training\ndata. To address this fundamental challenge, we present DiffVSR, featuring a\nProgressive Learning Strategy (PLS) that systematically decomposes this\nlearning burden through staged training, enabling superior performance on\ncomplex degradations. Our framework additionally incorporates an Interweaved\nLatent Transition (ILT) technique that maintains competitive temporal\nconsistency without additional training overhead. Experiments demonstrate that\nour approach excels in scenarios where competing methods struggle, particularly\non severely degraded videos. Our work reveals that addressing the learning\nstrategy, rather than focusing solely on architectural complexity, is the\ncritical path toward robust real-world video super-resolution with diffusion\nmodels.",
        "The entanglement analysis of a pure state of a many-body quantum system\nrequires a prior information about its density matrix\/ state matrix, obtained\nin principle by solving the Hamiltonian matrix. The missing information due to\ncomplexity of the many-body interactions however renders it necessary to\nconsider an ensemble of Hamiltonians and thereby an ensemble of pure states.\nThis in turn leaves a statistical description of the entanglement measures as\nthe only option.\n  We consider physical Hamiltonians that can be modelled by a multiparametric\nGaussian ensembles, theoretically derive the state ensembles for its\neigenstates and analyze the effect of varying system conditions on its\nentanglement statistics. Our approach leads to a single parametric based common\nmathematical formulation for the evolution of the statistics of different state\nensembles. The parameter turns out to be a single functional of the system\nparameters and thereby reveals a deep web of connection underlying different\nquantum states.",
        "In a Cox model, the partial likelihood, as the product of a series of\nconditional probabilities, is used to estimate the regression coefficients. In\npractice, those conditional probabilities are approximated by risk score ratios\nbased on a continuous time model, and thus result in parameter estimates from\nonly an approximate partial likelihood. Through a revisit to the original\npartial likelihood idea, an accurate partial likelihood computing method for\nthe Cox model is proposed, which calculates the exact conditional probability\nusing the Poisson-binomial distribution. New estimating and inference\nprocedures are developed, and theoretical results are established for the\nproposed computational procedure. Although ties are common in real studies,\ncurrent theories for the Cox model mostly do not consider cases for tied data.\nIn contrast, the new approach includes the theory for grouped data, which\nallows ties, and also includes the theory for continuous data without ties,\nproviding a unified framework for computing partial likelihood for data with or\nwithout ties. Numerical results show that the proposed method outperforms\ncurrent methods in reducing bias and mean squared error, while achieving\nimproved confidence interval coverage rates, especially when there are many\nties or when the variability in risk scores is large. Comparisons between\nmethods in real applications have been made.",
        "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT\/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess\/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https:\/\/corn0205.github.io\/",
        "The aim of this paper is studying the two-sided remotely almost periodic\nsolutions of ordinary differential equations in Banach spaces of the form\n$x'=A(t)x+f(t)+F(t,x)$ with two-sided remotely almost periodic coefficients if\nthe linear equation $x'=A(t)x$ satisfies the condition of exponential\ntrichotomy and nonlinearity $F$ is \"small\".",
        "Recent advances in diffusion models have greatly improved text-driven video\ngeneration. However, training models for long video generation demands\nsignificant computational power and extensive data, leading most video\ndiffusion models to be limited to a small number of frames. Existing\ntraining-free methods that attempt to generate long videos using pre-trained\nshort video diffusion models often struggle with issues such as insufficient\nmotion dynamics and degraded video fidelity. In this paper, we present\nBrick-Diffusion, a novel, training-free approach capable of generating long\nvideos of arbitrary length. Our method introduces a brick-to-wall denoising\nstrategy, where the latent is denoised in segments, with a stride applied in\nsubsequent iterations. This process mimics the construction of a staggered\nbrick wall, where each brick represents a denoised segment, enabling\ncommunication between frames and improving overall video quality. Through\nquantitative and qualitative evaluations, we demonstrate that Brick-Diffusion\noutperforms existing baseline methods in generating high-fidelity videos.",
        "Accurately modeling bending energy in morphogenetic simulations is crucial,\nespecially when dealing with anisotropic meshes where remeshing is infeasible\ndue to the biologically meaningful entities of vertex positions (e.g., cells).\nThis study addresses the underexplored question of which bending-energy\ndiscretization methods are most accurate and suitable for such simulations.\n  The evaluation consists of two stages: First, the accuracy of each method is\ntested by comparing predicted bending energy and force against theoretical\nvalues for two benchmark cases--a wrinkled planar sheet and a smooth spherical\nsheet. Second, we simulate the formation of wrinkles in a planar sheet caused\nby anisotropic cell division, analyzing the resulting wavenumber patterns for\ntwo division orientations: uniaxial and random.\n  The results highlight that the choice of the optimal discretization method\ndepends on the application. For simulations requiring precise quantitative\npredictions, the Hamann model demonstrates superior accuracy. Conversely, for\nsimulations where qualitative trends in morphology are of primary interest, the\nJ\\\"ulicher model provides a computationally efficient alternative. These\nfindings provide guidance for selecting appropriate bending-energy\ndiscretization methods in morphogenetic simulations, ultimately leading to more\naccurate and efficient modeling of complex biological forms.",
        "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
        "Let $P$ be a set of points and $L$ a set of lines in the (extended) Euclidean\nplane, and $I \\subseteq P\\times L$, where $i =(p,l) \\in I$ means that point $p$\nand line $l$ are incident. The incidences can be interpreted as quadratic\nconstraints on the homogeneous coordinates of the points and lines. We study\nthe space of incidence preserving motions of the given incidence structure by\nlinearizing the system of quadratic equations. The Jacobian of the quadratic\nsystem, our projective rigidity matrix, leads to the notion of\nindependence\/dependence of incidences. Column dependencies correspond to\ninfinitesimal motions. Row dependencies or self-stresses allow for new\ninterpretations of classical geometric incidence theorems. We show that\nself-stresses are characterized by a 3-fold balance. As expected, infinitesimal\n(first order) projective rigidity as well as second order projective rigidity\nimply projective rigidity but not conversely. Several open problems and\npossible generalizations are indicated.",
        "We construct an expansion of a real closed field by a multiplicative subgroup\nadapting Poizat's theory of green points. Its theory is strongly dependent, and\nevery open set definable in a model of this theory is semialgebraic. We prove\nthat the real field with a dense family of logarithmic spirals, proposed by\nZilber, satisfies our theory.",
        "We construct and analyze the fault tolerance of $[[6,1,3]]$ non-CSS quantum\nerror correcting code under the anisotropic and depolarizing noise models. This\nrate-optimized code achieves fault-tolerance using a single ancilla qubit for\nsyndrome measurement under anisotropic noise conditions. This method was called\nfault-tolerance using bare ancilla by Brown \\emph{et al.} We give explicit\nconstruction of the code using measurements on non-planar graph states. We also\nargue that using our approach, we can construct a family of such fault-tolerant\ncodes. This method fills a notable gap in constructing fault-tolerant non-CSS\ncode families.",
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com\/huawei-noah\/SMARTS."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Biological identifications through DNA barcodes",
    "start_abstract":"Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "BarcodeBERT: Transformers for Biodiversity Analysis"
      ],
      "abstract":[
        "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Complexity of Finite Semigroups: History and Decidability",
        "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
        "Two-component nonlinear wave solutions of the sixth-order generalised\n  Boussinesq-type equations",
        "Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC\n  Codes",
        "A central limit theorem for the giant in a stochastic block model",
        "Faster Machine Translation Ensembling with Reinforcement Learning and\n  Competitive Correction",
        "Unified Native Spaces in Kernel Methods",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
        "Pressure-induced structural and superconducting transitions in black\n  arsenic",
        "A healthier stochastic semiclassical gravity: world without\n  Schr\\\"{o}dinger cats",
        "From Target Tracking to Targeting Track -- Part II: Regularized\n  Polynomial Trajectory Optimization",
        "Open-Ended and Knowledge-Intensive Video Question Answering",
        "General Relativity and Geodesy",
        "Evaluating Interpretable Reinforcement Learning by Distilling Policies\n  into Programs",
        "An inherently parallel H2-ULV factorization for solving dense linear\n  systems on GPUs",
        "Current-induced magnetoresistance hysteresis in the kagome\n  superconductor CsV$_3$Sb$_5$",
        "Comment on \"Comment on Attosecond electron microscopy and diffraction\"",
        "The Ophiuchus DIsk Survey Employing ALMA (ODISEA): Complete Size\n  Distributions for the 100 Brightest Disks Across Multiplicity and SED Classes",
        "Comparison of Offset and Ratio Weighted Regressions in Tweedie Models\n  with Application to Mid-Term Cancellations",
        "High-Throughput Computational Screening and Interpretable Machine\n  Learning of Metal-organic Frameworks for Iodine Capture",
        "A Comprehensive Survey on Cross-Domain Recommendation: Taxonomy,\n  Progress, and Prospects",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Vibrational properties of photochromic yttrium oxyhydride and\n  oxydeuteride thin films",
        "Empirical Calibration and Metric Differential Privacy in Language Models",
        "Argument-Based Comparative Question Answering Evaluation Benchmark",
        "Bifurcation and multiplicity results for critical problems involving the\n  $p$-Grushin operator",
        "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\n  Large Language Models",
        "Teleportation scheme for the complete state of light at the example of\n  coherent states"
      ],
      "abstract":[
        "In recent papers, Margolis, Rhodes and Schilling proved that the complexity\nof a finite semigroup is computable. This solved a problem that had been open\nfor more than 50 years. The purpose of this paper is to survey the basic\nresults of Krohn-Rhodes complexity of finite semigroups and to outline the\nproof of its computability.",
        "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
        "Two different versions of cubic sixth-order generalised Boussinesq-type wave\nequations are considered in this study. A generalised perturbation reduction\nmethod is used to solve these equations, which allows the reduction of\nconsidered equations to coupled nonlinear Schrodinger equations. Two-component\nnonlinear wave solutions are obtained. The profiles and parameters of these\nsolutions for both nonlinear equations are presented and compared. These\nsolutions coincide with the vector 0 \\pi pulse of self-induced transparency,\nwhich was previously studied in several known nonlinear wave equations.",
        "Quantum low-density parity-check (QLDPC) codes present a promising route to\nlow-overhead fault-tolerant quantum computation, yet systematic strategies for\ntheir exploration remain underdeveloped. In this work, we establish a\ntopological framework for studying the bivariate-bicycle codes, a prominent\nclass of QLDPC codes tailored for real-world quantum hardware. Our framework\nenables the investigation of these codes through universal properties of\ntopological orders. Besides providing efficient characterizations for\ndemonstrations using Gr\\\"obner bases, we also introduce a novel\nalgebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko\ntheorem, allowing us to analytically determine how the topological order varies\nwith the generic choice of bivariate-bicycle codes under toric layouts. Novel\nphenomena are unveiled, including topological frustration, where ground-state\ndegeneracy on a torus deviates from the total anyon number, and quasi-fractonic\nmobility, where anyon movement violates energy conservation. We demonstrate\ntheir inherent link to symmetry-enriched topological orders and offer an\nefficient method for searching for finite-size codes. Furthermore, we extend\nthe connection between anyons and logical operators using Koszul complex\ntheory. Our work provides a rigorous theoretical basis for exploring the fault\ntolerance of QLDPC codes and deepens the interplay among topological order,\nquantum error correction, and advanced mathematical structures.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Ensembling neural machine translation (NMT) models to produce higher-quality\ntranslations than the $L$ individual models has been extensively studied.\nRecent methods typically employ a candidate selection block (CSB) and an\nencoder-decoder fusion block (FB), requiring inference across \\textit{all}\ncandidate models, leading to significant computational overhead, generally\n$\\Omega(L)$. This paper introduces \\textbf{SmartGen}, a reinforcement learning\n(RL)-based strategy that improves the CSB by selecting a small, fixed number of\ncandidates and identifying optimal groups to pass to the fusion block for each\ninput sentence. Furthermore, previously, the CSB and FB were trained\nindependently, leading to suboptimal NMT performance. Our DQN-based\n\\textbf{SmartGen} addresses this by using feedback from the FB block as a\nreward during training. We also resolve a key issue in earlier methods, where\ncandidates were passed to the FB without modification, by introducing a\nCompetitive Correction Block (CCB). Finally, we validate our approach with\nextensive experiments on English-Hindi translation tasks in both directions.",
        "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
        "We report high-pressure Raman spectra and resistance measurements of black\narsenic (b-As) up to 58 GPa, along with phonon density of states (DOS) and\nenthalpy calculations for four reported arsenic phases up to 50 GPa. It is\nfound that metastable b-As transforms into gray arsenic (g-As) phase at a\ncritical pressure of 1.51 GPa, followed by subsequent transitions to simple\ncubic arsenic (c-As) and incommensurate host-guest arsenic (hg-As) phases at\n25.9 and 44.8 GPa, respectively. Superconductivity emerges above 25 GPa in the\nc-As phase, with the superconducting transition temperature ($T$$\\rm_c$)\nremaining nearly a constant of 3 K. Upon further compression, $T$$\\rm_c$\nsteeply increases to a higher value around 4.5 K in the incommensurate hg-As\nphase above 43 GPa. We use our results to update the structural and\nsuperconducting phase diagrams under pressure for the novel semiconductor,\nblack arsenic.",
        "Semiclassical gravity couples classical gravity to the quantized matter in\nmeanfield approximation. The meanfield coupling is problematic for two reasons.\nFirst, it ignores the quantum fluctuation of matter distribution. Second, it\nviolates the linearity of the quantum dynamics. The first problem can be\nmitigated by allowing stochastic fluctuations of the geometry but the second\nproblem lies deep in quantum foundations. Restoration of quantum linearity\nrequires a conceptual approach to hybrid classical-quantum coupling. Studies of\nthe measurement problem and the quantum-classical transition point to the\nsolution. It is based on a postulated mechanism of spontaneous quantum\nmonitoring plus feedback. This approach eliminates Schr\\\"{o}dinger cat states,\ntakes quantum fluctuations into account, and restores the linearity of quantum\ndynamics. Such a captivating conceptionally `healthier' semiclassical theory\nexists in the Newtonian limit, but its relativistic covariance hits a wall.\nHere we will briefly recapitulate the concept and its realization in the\nnonrelativistic limit. We emphasize that the long-known obstacles to the\nrelativistic extension lie in quantum foundations.",
        "Target tracking entails the estimation of the evolution of the target state\nover time, namely the target trajectory. Different from the classical state\nspace model, our series of studies, including this paper, model the collection\nof the target state as a stochastic process (SP) that is further decomposed\ninto a deterministic part which represents the trend of the trajectory and a\nresidual SP representing the residual fitting error. Subsequently, the tracking\nproblem is formulated as a learning problem regarding the trajectory SP for\nwhich a key part is to estimate a trajectory FoT (T-FoT) best fitting the\nmeasurements in time series. For this purpose, we consider the polynomial T-FoT\nand address the regularized polynomial T-FoT optimization employing two\ndistinct regularization strategies seeking trade-off between the accuracy and\nsimplicity. One limits the order of the polynomial and then the best choice is\ndetermined by grid searching in a narrow, bounded range while the other adopts\n$\\ell_0$ norm regularization for which the hybrid Newton solver is employed.\nSimulation results obtained in both single and multiple maneuvering target\nscenarios demonstrate the effectiveness of our approaches.",
        "Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.",
        "Mass redistribution on Earth due to dynamic processes such as ice melting and\nsea level rise leads to a changing gravitational field, observable by geodetic\ntechniques. Monitoring this change over time allows us to learn more about our\nplanet and its dynamic evolution. In this paper, we highlight the impact of\nGeneral Relativity (GR) on geodesy: it provides corrections essential for the\ninterpretation of high-precision measurements and enables a completely novel\nmeasurement approach using chronometry, i.e., clock-based observations.\nFocusing on the latter, we review the construction of the relativistic gravity\npotential and the corresponding geoid definition as an isochronometric surface\nto elucidate the comparison to the conventional Newtonian geoid. Furthermore,\nwe comment on additional potentials due to the non-Newtonian degrees of freedom\nof the relativistic gravitational field, and assess the feasibility of\nclock-based measurements for Gravity Field Recovery (GFR) from space. Although\nclock observations in space demonstrate technical promise for GFR, achieving\nthe necessary precision for practical applications remains challenging.",
        "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
        "Hierarchical low-rank approximation of dense matrices can reduce the\ncomplexity of their factorization from O(N^3) to O(N). However, the complex\nstructure of such hierarchical matrices makes them difficult to parallelize.\nThe block size and ranks can vary between the sub-blocks, which creates load\nimbalance. The dependency between the sub-blocks during factorization results\nin serialization. Since many sub-blocks are low-rank, their small computational\nload exposes the overhead of runtime systems. The combination of these factors\nmakes it challenging to implement these methods on GPUs. In this work, we show\nthat dense matrices can be factorized with linear complexity, while extracting\nthe potential parallelism of GPUs. This is made possible through the H2-ULV\nfactorization, which removes the dependency on trailing sub-matrices.",
        "We report the observation of current-modulated magnetoresistance hysteresis\nbelow the superconducting transition temperature in the kagome superconductor\nCsV$_3$Sb$_5$. This highly tunable hysteresis behavior is confined to the\nsuperconducting state and vanishes when superconductivity is fully suppressed,\ndirectly linking magnetoresistance hysteresis to the superconducting order in\nCsV$_3$Sb$_5$. Additionally, the superconducting diode effect driven by a small\nmagnetic field is observed, indicating the enhanced electronic magnetochiral\nanisotropy by the chiral domain-wall scattering. Our findings position\nCsV$_3$Sb$_5$ as a promising platform for exploring nontrivial physical\nphenomena, including unconventional pairing mechanisms and topological\nsuperconductivity.",
        "Over the past few decades, following the first demonstration of ultrafast\nelectron microscopy, numerous research groups have focused on achieving\nattosecond temporal resolution in electron microscopy with the goal of imaging\nelectron and atomic motion. Recently, several studies have claimed to achieve\nattosecond temporal resolution in imaging(1-3). These claims are based on the\ngeneration of attosecond electron pulse trains. However, in typical\ntime-resolved measurements used to capture dynamic processes in real-time, the\ntemporal resolution is determined by the envelope of the pulse train. The\nreliance of using attosecond electron pulse trains fails to account for the\ndistinct temporal resolution advantages enabled by our attosecond optical\ngating, which are absent in the case of using a continuous-wave or long laser\npulse. These oversights highlight the limitations of this methodology (1-3) in\nstudying ultrafast phenomena of matter. It is crucial to clarify this\ndistinction to avoid confusion, misinterpretation, and potential miscitations\nwithin the community regarding attosecond temporal resolution in electron\nmicroscopy and the attosecond imaging of matter dynamics. In contrast, Hui et\nal. (4) present the first realistic demonstration of attosecond imaging\nresolution in electron microscopy, enabling the diffraction imaging of electron\nmotion dynamics in graphene. In a commentary by Peter Baum and Claus Ropers,\nthe authors conjecture that the graphene dynamics observed in our time-resolved\ndiffraction experiment (Fig. 5, Hui et al. 2024) (4) is an optical interference\nartifact or light modulation of electrons effects, similar to what was reported\npreviously (1-3), in addition to raising other technical concerns. In this\nreply, we are pleased to address these allegations and provide clarifications\nto resolve the raised technical questions.",
        "The size of a protoplanetary disk is a fundamental property, yet most remain\nunresolved, even in nearby star-forming regions (d $\\sim$ 140-200 pc). We\npresent the complete continuum size distribution for the $105$ brightest\nprotoplanetary disks (M$_{\\text{dust}}$ $\\gtrsim$ 2 M$_{\\oplus}$) in the\nOphiuchus cloud, obtained from ALMA Band 8 (410 GHz) observations at\n0.05$^{\\prime\\prime}$ (7 au) to 0.15$^{\\prime\\prime}$ (21 au) resolution. This\nsample includes 54 Class II and 51 Class I and Flat Spectrum sources, providing\na comprehensive distribution across evolutionary stages. We measure the Half\nWidth at Half Maximum (HWHM) and the radius encircling $68\\%$ of the flux\n($R_{68\\%}$) for most non-binary disks, yielding the largest flux-limited\nsample of resolved disks in any star-forming region. The distribution is\nlog-normal with a median value of $\\sim$14 au and a logarithmic standard\ndeviation $\\sigma_{\\log} = 0.46$ (factor of 2.9 in linear scale). Disks in\nclose binary systems ($<$ 200 au separation) have smaller radii, with median\nvalue of $\\sim$5 au, indicating efficient radial drift as predicted by dust\nevolution models. The size distribution for young embedded objects (SED Class I\nand Flat Spectrum, age $\\lesssim$ 1 Myr) is similar to that of Class II objects\n(age $\\sim$ a few Myr), implying that pressure bumps must be common at early\ndisk stages to prevent mm-sized particle migration at au scales.",
        "In property and casualty insurance, particularly in automobile insurance,\nrisk exposure is traditionally associated with the coverage duration. However,\nfactors such as early contract cancellations demand more precise modelling to\nensure accurate premium pricing. This study introduces and compares two\napproaches for modelling total claims (or loss costs) in insurance portfolios\nwith a high proportion of policies that have partial year exposure: the offset\nand ratio methods. We demonstrate that both approaches can be viewed as\nweighted regressions under the Tweedie distribution framework. Through an\nanalysis based on the financial balance property, we find that the ratio\napproach outperforms the offset method. This comparison is illustrated using an\nautomobile insurance portfolio, where a significant share of policyholders\nterminate their contracts before the coverage period concludes.",
        "The removal of leaked radioactive iodine isotopes in humid environments holds\nsignificant importance in nuclear waste management and nuclear accident\nmitigation. In this study, high-throughput computational screening and machine\nlearning were combined to reveal the iodine capture performance of 1816\nmetal-organic framework (MOF) materials under humid air conditions. Firstly,\nthe relationship between the structural characteristics of MOFs and their\nadsorption properties was explored, with the aim of identifying the optimal\nstructural parameters for iodine capture. Subsequently, two machine learning\nregression algorithms - Random Forest and CatBoost, were employed to predict\nthe iodine adsorption capabilities of MOFs. In addition to 6 structural\nfeatures, 25 molecular features and 8 chemical features were incorporated to\nenhance the prediction accuracy of the machine learning algorithms. Feature\nimportance was assessed to determine the relative influence of various features\non iodine adsorption performance, in which the Henry's coefficient and heat of\nadsorption to iodine were found the two most crucial chemical factors.\nFurthermore, four types of molecular fingerprints were introduced for providing\ncomprehensive and detailed structural information of MOF materials. The top 20\nmost significant MACCS molecular fingerprints were picked out, revealing that\nthe presence of six-membered ring structures and nitrogen atoms in the MOFs\nwere the key structural factors that enhanced iodine adsorption, followed by\nthe existence of oxygen atoms. This work combined high-throughput computation,\nmachine learning, and molecular fingerprints to comprehensively elucidate the\nmultifaceted factors influencing the iodine adsorption performance of MOFs,\noffering profound insightful guidelines for screening and structural design of\nadvanced MOF materials.",
        "Recommender systems (RS) have become crucial tools for information filtering\nin various real world scenarios. And cross domain recommendation (CDR) has been\nwidely explored in recent years in order to provide better recommendation\nresults in the target domain with the help of other domains. The CDR technology\nhas developed rapidly, yet there is a lack of a comprehensive survey\nsummarizing recent works. Therefore, in this paper, we will summarize the\nprogress and prospects based on the main procedure of CDR, including Cross\nDomain Relevance, Cross Domain Interaction, Cross Domain Representation\nEnhancement and Model Optimization. To help researchers better understand and\nengage in this field, we also organize the applications and resources, and\nhighlight several current important challenges and future directions of CDR.\nMore details of the survey articles are available at\nhttps:\/\/github.com\/USTCAGI\/Awesome-Cross-Domain\nRecommendation-Papers-and-Resources.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "A comprehensive study of the vibrational properties of photochromic yttrium\noxyhydride (YHO) and oxydeuteride (YDO) thin films is presented. These films\nare deposited using reactive magnetron sputtering, followed by post-oxidation.\nOur investigation employs vibrational Fourier-transform infrared (FTIR)\nspectroscopy, in conjunction with first-principles Density Functional Theory\n(DFT) calculations. The FTIR spectra of the films reveal broad vibrational\nbands, primarily attributed to the disordered structure containing small\ncrystallites (<10 nm), as confirmed by solid-state nuclear magnetic resonance\nand X-ray diffraction measurements. An isotopic shift from approximately 900 to\n745 cm-1 is observed in the hydrogen\/deuterium-related vibration band, while\nthe lower frequency bands (< 600 cm-1) remain unaffected upon replacement of\nhydrogen with deuterium. These experimental observations are consistent with\nthe DFT theoretical calculations for various stable YHO lattices reported in\nthe literature. Illumination of the films with ultraviolet light at 3.3 eV\nleads to additional absorption not only in the visible light range but also up\nto approximately 2000 cm-1 in the mid-infrared region. However, no phase\ntransformation change or formation of hydroxyl (OH) groups are observed\nfollowing illumination. Our findings provide valuable insight into the\nvibrational and photochromic properties of YH(D)O thin films.",
        "NLP models trained with differential privacy (DP) usually adopt the DP-SGD\nframework, and privacy guarantees are often reported in terms of the privacy\nbudget $\\epsilon$. However, $\\epsilon$ does not have any intrinsic meaning, and\nit is generally not possible to compare across variants of the framework. Work\nin image processing has therefore explored how to empirically calibrate noise\nacross frameworks using Membership Inference Attacks (MIAs). However, this kind\nof calibration has not been established for NLP. In this paper, we show that\nMIAs offer little help in calibrating privacy, whereas reconstruction attacks\nare more useful. As a use case, we define a novel kind of directional privacy\nbased on the von Mises-Fisher (VMF) distribution, a metric DP mechanism that\nperturbs angular distance rather than adding (isotropic) Gaussian noise, and\napply this to NLP architectures. We show that, even though formal guarantees\nare incomparable, empirical privacy calibration reveals that each mechanism has\ndifferent areas of strength with respect to utility-privacy trade-offs.",
        "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https:\/\/anonymous.4open.science\/r\/cqa-evaluation-benchmark-4561\/README.md}}.",
        "In this article we prove a bifurcation and multiplicity result for a critical\nproblem involving a degenerate nonlinear operator $\\Delta_\\gamma^p$. We extend\nto a generic $p>1$ a result which was proved only when $p=2$. When $p\\neq 2$,\nthe nonlinear operator $-\\Delta_\\gamma^p$ has no linear eigenspaces, so our\nextension is nontrivial and requires an abstract critical theorem which is not\nbased on linear subspaces. We also prove a new abstract result based on a\npseudo-index related to the $\\mathbf{Z}_2$-cohomological index that is\napplicable here. We provide a version of the Lions' Concentration-Compactness\nPrinciple for our operator.",
        "The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification.",
        "We present a scheme to teleport both the spatial and number-of-photons\ndegrees of freedom of light. This is achieved by teleportation of each pixel of\na target image. We take coherent states as the input and demonstrate the scheme\nfor the cases with ideal and realistic entanglement resources."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"BarcodeBERT: Transformers for Biodiversity Analysis",
    "start_abstract":"Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Biological identifications through DNA barcodes"
      ],
      "abstract":[
        "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "A note on improved bounds for hypergraph rainbow matching problems",
        "$\\mathcal{Z}$-stability of twisted group C*-algebras of nilpotent groups",
        "Coupled Oscillators and Dielectric Function",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Contrastive Language-Structure Pre-training Driven by Materials Science\n  Literature",
        "A confederacy of anomalies",
        "A reduced-order mean-field synchronization model for thermoacoustic\n  systems",
        "Excluding a rectangular grid",
        "Long-time asymptotics for the $N_{\\infty}$-soliton solution to the KdV\n  equation with two types of generalized reflection coefficients",
        "Maximal green sequences for $\\mathcal{Q}^N$ quivers",
        "Radio observations of the ultra-long GRB 220627A reveal a hot cocoon\n  supporting the blue supergiant progenitor scenario",
        "SN 2024ggi: another year, another striking Type II supernova",
        "Multimode fiber based high-dimensional light analyzer",
        "Efficient Over-parameterized Matrix Sensing from Noisy Measurements via\n  Alternating Preconditioned Gradient Descent",
        "Curvature Perturbations from First-Order Phase Transitions: Implications\n  to Black Holes and Gravitational Waves",
        "A Framework for Supervised and Unsupervised Segmentation and\n  Classification of Materials Microstructure Images",
        "Non-equilibrium distribution function in ultra-fast processes",
        "Electroweak Symmetry Breaking",
        "Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced\n  by High-Repetition-Rate Femtosecond Laser Filamentation",
        "Hydrodynamic Interactions in Particle Suspensions: A Perspective on\n  Stokesian Dynamics",
        "Spectral properties of a disordered insulating lattice under nonlinear\n  electric field",
        "A Near-optimal Algorithm for Learning Margin Halfspaces with Massart\n  Noise",
        "Evidence for a volcanic atmosphere on the sub-Earth L98-59b",
        "Complete intersection algebras with binomial Macaulay dual generator",
        "Vanishing coefficient results in four families of infinite q-products",
        "Quantum ensemble learning with a programmable superconducting processor",
        "New examples of geometrically special varieties: K3 surfaces, Enriques\n  surfaces, and algebraic groups",
        "The $\\alpha$-representation for the Tait coloring and for the\n  characteristic polynomial of matroid",
        "$L^p\\to L^q$ estimates for Stein's spherical maximal operators"
      ],
      "abstract":[
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "We prove that the twisted group C*-algebra of a finitely generated nilpotent\ngroup is $\\mathcal{Z}$-stable if and only if it is nowhere scattered, a\ncondition that we characterize in terms of the given group and 2-cocycle. As a\nmain application, we prove new converses to the Balian-Low Theorem for\nprojective, square-integrable representations of nilpotent Lie groups.",
        "A generalized Sellmeier model, also referred to as the Lorentz-Dirac model,\nhas been used for the description of the dielectric function of a number of\ntechnologically important materials in the literature. This model represents\nthe frequency-dependent dielectric function as a sum over Green functions of\nclassical damped harmonic oscillators, much in analogy with the functional form\nused for the dynamic polarizability of an atom, but with one important\naddition, namely, a complex-valued oscillator strength in the numerator. Here,\nwe show that this generalized functional form can be justified based on the\nresponse function of coupled damped oscillators. The encountered analogies\nsuggest an explanation for the generally observed success of the Lorentz--Dirac\nmodel in describing the dielectric function of crystals of consummate\ntechnological significance.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "Understanding structure-property relationships is an essential yet\nchallenging aspect of materials discovery and development. To facilitate this\nprocess, recent studies in materials informatics have sought latent embedding\nspaces of crystal structures to capture their similarities based on properties\nand functionalities. However, abstract feature-based embedding spaces are\nhuman-unfriendly and prevent intuitive and efficient exploration of the vast\nmaterials space. Here we introduce Contrastive Language--Structure Pre-training\n(CLaSP), a learning paradigm for constructing crossmodal embedding spaces\nbetween crystal structures and texts. CLaSP aims to achieve material embeddings\nthat 1) capture property- and functionality-related similarities between\ncrystal structures and 2) allow intuitive retrieval of materials via\nuser-provided description texts as queries. To compensate for the lack of\nsufficient datasets linking crystal structures with textual descriptions, CLaSP\nleverages a dataset of over 400,000 published crystal structures and\ncorresponding publication records, including paper titles and abstracts, for\ntraining. We demonstrate the effectiveness of CLaSP through text-based crystal\nstructure screening and embedding space visualization.",
        "A personal recollection of early years in lattice gauge theory with a bias\ntowards chiral symmetry and lattice fermions.",
        "The synchronization phenomena in thermoacoustic systems leading to\noscillatory instability can effectively be modeled using Kuramoto oscillators.\nSuch models consider the nonlinear response of flame as an ensemble of Kuramoto\nphase oscillators constrained to collectively evolve at the rhythm of acoustic\nfluctuations. However, these high-dimensional models are analytically\nintractable and computationally expensive. In this study, we reduce the\ndimensionality of such a high-dimensional model and present a low-order,\nanalytically tractable model for predicting transitions to thermoacoustic\ninstability. We reduce the dimensionality of the phase oscillator model coupled\nto the acoustic field using the Ott-Antonsen ansatz. Using the reduced-order\nequations, we estimate the transitions to thermoacoustic instability and\ncompare these transitions with the experiment. We validate the model for two\ncombustor configurations, viz., the bluff-body stabilized dump combustor and\nthe swirl-stabilized annular combustor. The low-order model accurately captures\nthe continuous and abrupt secondary transitions observed experimentally in\nthese distinct combustors.",
        "For every positive integer $k$, we define the $k$-treedepth as the largest\ngraph parameter $\\mathrm{td}_k$ satisfying (i) $\\mathrm{td}_k(\\emptyset)=0$;\n(ii) $\\mathrm{td}_k(G) \\leq 1+ \\mathrm{td}_k(G-u)$ for every graph $G$ and\nevery vertex $u \\in V(G)$; and (iii) if $G$ is a $(<k)$-clique-sum of $G_1$ and\n$G_2$, then $\\mathrm{td}_k(G) \\leq \\max\n\\{\\mathrm{td}_k(G_1),\\mathrm{td}_k(G_2)\\}$, for all graphs $G_1,G_2$. This\nparameter coincides with treedepth if $k=1$, and with treewidth plus $1$ if $k\n\\geq |V(G)|$. We prove that for every positive integer $k$, a class of graphs\n$\\mathcal{C}$ has bounded $k$-treedepth if and only if there is a positive\ninteger $\\ell$ such that for every tree $T$ on $k$ vertices, no graph in\n$\\mathcal{C}$ contains $T \\square P_\\ell$ as a minor. This implies for $k=1$\nthat a minor-closed class of graphs has bounded treedepth if and only if it\nexcludes a path, for $k=2$ that a minor-closed class of graphs has bounded\n$2$-treedepth if and only if it excludes as a minor a ladder (Huynh, Joret,\nMicek, Seweryn, and Wollan; Combinatorica, 2021), and for large values of $k$\nthat a minor-closed class of graphs has bounded treewidth if and only if it\nexcludes a grid (Grid-Minor Theorem, Robertson and Seymour; JCTB, 1986). As a\ncorollary, we obtain the following qualitative strengthening of the Grid-Minor\nTheorem in the case of bounded-height grids. For all positive integers $k,\n\\ell$, every graph that does not contain the $k \\times \\ell$ grid as a minor\nhas $(2k-1)$-treedepth at most a function of $(k, \\ell)$.",
        "We systematically investigate the long-time asymptotics for the\n$N_{\\infty}$-soliton solution to the KdV equation in the different regions with\nthe aid of the Riemann-Hilbert (RH) problems with two types of generalized\nreflection coefficients on the interval $\\left[\\eta_1, \\eta_2\\right]\\in\n\\mathbb{R}^+$: $r_0(\\lambda,\\eta_0; \\beta_0,\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\left|\\lambda-\\eta_0\\right|^{\\beta_0}\\gamma\\left(\\lambda\\right)$,\n$r_c(\\lambda,\\eta_0;\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\chi_c\\left(\\lambda,\n\\eta_0\\right)\\gamma \\left(\\lambda\\right)$, where the singularity $\\eta_0\\in\n(\\eta_1, \\eta_2)$ and $\\beta_j>-1$ ($j=0, 1, 2$), $\\gamma: \\left[\\eta_1,\n\\eta_2\\right] \\to\\mathbb{R}^+$ is continuous and positive on $\\left[\\eta_1,\n\\eta_2\\right]$, with an analytic extension to a neighborhood of this interval,\nand the step-like function $\\chi_c$ is defined as\n$\\chi_c\\left(\\lambda,\\eta_0\\right)=1$ for $\\lambda\\in\\left[\\eta_1,\n\\eta_0\\right)$ and $\\chi_c\\left(\\lambda,\\eta_0\\right)=c^2$ for\n$\\lambda\\in\\left(\\eta_0, \\eta_2\\right]$ with $c>0, \\, c\\ne1$. A critical step\nin the analysis of RH problems via the Deift-Zhou steepest descent technique is\nhow to construct local parametrices around the endpoints $\\eta_j$'s and the\nsingularity $\\eta_0$. Specifically, the modified Bessel functions of indexes\n$\\beta_j$'s are utilized for the endpoints $\\eta_j$'s, and the modified Bessel\nfunctions of index $\\left(\\beta_0\\pm 1\\right)\\left\/\\right.2$ and confluent\nhypergeometric functions are employed around the singularity $\\eta_0$ if the\nreflection coefficients are $r_0$ and $r_c$, respectively. This comprehensive\nstudy extends the understanding of generalized reflection coefficients and\nprovides valuable insights into the asymptotics of soliton gases.",
        "We introduce $\\mathcal{Q}^N$ quivers and construct maximal green sequences\nfor these quivers. We prove that any finite connected full subquiver of the\nquivers defined by Hernandez and Leclerc, arising in monoidal categorifications\nof cluster algebras, is a special case of $\\mathcal{Q}^N$ quivers. Moreover, we\nprove that the trees of oriented cycles introduced by Garver and Musiker are\nspecial cases of $\\mathcal{Q}^N$ quivers. This result resolves an open problem\nproposed by Garver and Musiker, providing a construction of maximal green\nsequences for quivers that are trees of oriented cycles. Furthermore, we prove\nthat quivers that are mutation equivalent to an orientation of a type AD Dynkin\ndiagram can also be recognized as special cases of $\\mathcal{Q}^N$ quivers.",
        "We present the discovery of the radio afterglow of the most distant\nultra-long gamma-ray burst (GRB) detected to date, GRB~220627A at redshift\n$z=3.084$. Its prompt gamma-ray light curve shows a double-pulse profile, with\nthe pulses separated by a period of quiescence lasting ${\\sim} 15\\,$min,\nleading to early speculation it could be a strongly gravitationally lensed GRB.\nHowever, our analysis of the $\\textit{Fermi}$\/GBM spectra taken during the time\nintervals of both pulses show clear differences in their spectral energy\ndistributions, disfavouring the lensing scenario. We observed the radio\nafterglow from $7$ to $456\\,$d post-burst: an initial, steep decay ($F_{\\nu}\n\\propto t^{-2}$) is followed by a shallower decline ($F_{\\nu} \\propto\nt^{-1\/2}$) after ${\\sim} 20\\,$d. Our afterglow modelling shows that these radio\nproperties can be explained by the presence of a slow, wide ejecta component in\naddition to a fast, narrow ejecta component, consistent with the picture of a\nhighly-collimated jet and its thermal cocoon decelerating into the ambient\nmedium. The properties of the cocoon point toward a progenitor with a large\nstellar radius, supporting the blue supergiant scenario proposed for ultra-long\nGRBs. We also conducted an independent test of the lensing hypothesis via Very\nLong Baseline Interferometry (VLBI) observations at ${\\sim} 12\\,$d post-burst\nby searching, for the first time, for multiple images of the candidate lensed\nGRB afterglow. Our experiment highlighted the growing need for developments in\nreal-time correlation capabilities for time-critical VLBI experiments,\nparticularly as we advance towards the SKA and ngVLA era of radio astronomy.",
        "SN 2024ggi is a Type II supernova that exploded in the nearby galaxy NGC 3621\nat a distance of approximately 7 Mpc, making it one of the closest supernovae\nof the decade. This SN shows clear signs of interaction with a dense\ncircumstellar material, and several studies have investigated the properties of\nits possible progenitor star using pre-explosion data. In this work we aim to\nconstrain the progenitor properties of SN 2024ggi by performing hydrodynamical\nmodeling of its bolometric light curve and expansion velocities. We present\nphotometric and spectroscopic observations of SN 2024ggi obtained in the\nComplejo Astron\\'omico El Leoncito, in Las Campanas Observatory, and in Las\nCumbres Observatory Global Telescope Network, spanning from 2 to 106 days after\nexplosion. We constructed its bolometric light curve and we characterize it by\ncalculating its morphological parameters. Then, we computed a grid of one\ndimensional explosion models for evolved stars with varying masses and\nestimated the properties of the progenitor star of SN 2024ggi by comparing the\nmodels to the observations. The observed bolometric luminosity and expansion\nvelocities are well-matched by a model involving the explosion of a star in the\npresence of a close circumstellar material (CSM), with a zero-age main sequence\nmass of $\\mathrm{M_{ZAMS}}$ = 15 $M_{\\odot}$, a pre-SN mass and radius of 14.1\n$M_{\\odot}$ and 517 $R_{\\odot}$, respectively, an explosion energy of\n$1.3\\times10^{51}$ erg, and a nickel mass below 0.035 $M_{\\odot}$. Our analysis\nsuggests that the progenitor suffered a mass-loss rate of $4 \\times 10^{-3}$\n$M_{\\odot}$yr$^{-1}$, confined to a distance of 3000 $R_{\\odot}$. The CSM\ndistribution is likely a two-component structure that consists of a compact\ncore and an extended tail. This analysis represents the first hydrodynamical\nmodel of SN 2024ggi with a complete coverage of the plateau phase.",
        "The wavelength and state of polarization (SOP) are fundamental properties of\nan optical field which are essential for applications in optical\ncommunications, imaging and other fields. However, it is challenging for\nexisting spectrometers and polarimeters to measure these parameters\nsimultaneously, resulting in reduced spatial and temporal efficiency. To\novercome this limitation, we propose and demonstrate a compact multimode fiber\n(MMF)-based high-dimensional light analyzer capable of simultaneously\nperforming high-precision measurements of both wavelength and SOP. Core-offset\nlaunching is introduced in the MMF to reshuffle the mode coupling. A neural\nnetwork named WP-Net has been designed dedicated to wavelength and SOP\nsynchronization measurements. Physics-informed loss function based on optical\nprior knowledge is used to optimize the learning process. These advancements\nhave enhanced the sensitivity, achieving a wavelength resolution of 0.045 pm\nand an SOP resolution of 0.0088.",
        "We consider the noisy matrix sensing problem in the over-parameterization\nsetting, where the estimated rank $r$ is larger than the true rank $r_\\star$.\nSpecifically, our main objective is to recover a matrix $ X_\\star \\in\n\\mathbb{R}^{n_1 \\times n_2} $ with rank $ r_\\star $ from noisy measurements\nusing an over-parameterized factorized form $ LR^\\top $, where $ L \\in\n\\mathbb{R}^{n_1 \\times r}, \\, R \\in \\mathbb{R}^{n_2 \\times r} $ and $\n\\min\\{n_1, n_2\\} \\ge r > r_\\star $, with the true rank $ r_\\star $ being\nunknown. Recently, preconditioning methods have been proposed to accelerate the\nconvergence of matrix sensing problem compared to vanilla gradient descent,\nincorporating preconditioning terms $ (L^\\top L + \\lambda I)^{-1} $ and $\n(R^\\top R + \\lambda I)^{-1} $ into the original gradient. However, these\nmethods require careful tuning of the damping parameter $\\lambda$ and are\nsensitive to initial points and step size. To address these limitations, we\npropose the alternating preconditioned gradient descent (APGD) algorithm, which\nalternately updates the two factor matrices, eliminating the need for the\ndamping parameter and enabling faster convergence with larger step sizes. We\ntheoretically prove that APGD achieves near-optimal error convergence at a\nlinear rate, starting from arbitrary random initializations. Through extensive\nexperiments, we validate our theoretical results and demonstrate that APGD\noutperforms other methods, achieving the fastest convergence rate. Notably,\nboth our theoretical analysis and experimental results illustrate that APGD\ndoes not rely on the initialization procedure, making it more practical and\nversatile.",
        "We employ a covariant formalism to study the evolution of cosmological\nperturbations during a first-order phase transition, addressing in particular\ntheir gauge dependence that have been overlooked so far. Our results reveal\nthat non-covariant treatments employed in previous studies can substantially\noverestimate the production of primordial black holes and scalar-induced\ngravitational waves. Once gauge dependencies are properly accounted for, we\nfind that both effects occur at significantly lower levels than previously\nestimated.",
        "Microstructure of materials is often characterized through image analysis to\nunderstand processing-structure-properties linkages. We propose a largely\nautomated framework that integrates unsupervised and supervised learning\nmethods to classify micrographs according to microstructure phase\/class and,\nfor multiphase microstructures, segments them into different homogeneous\nregions. With the advance of manufacturing and imaging techniques, the\nultra-high resolution of imaging that reveals the complexity of microstructures\nand the rapidly increasing quantity of images (i.e., micrographs) enables and\nnecessitates a more powerful and automated framework to extract materials\ncharacteristics and knowledge. The framework we propose can be used to\ngradually build a database of microstructure classes relevant to a particular\nprocess or group of materials, which can help in analyzing and\ndiscovering\/identifying new materials. The framework has three steps: (1)\nsegmentation of multiphase micrographs through a recently developed score-based\nmethod so that different microstructure homogeneous regions can be identified\nin an unsupervised manner; (2) {identification and classification of}\nhomogeneous regions of micrographs through an uncertainty-aware supervised\nclassification network trained using the segmented micrographs from Step $1$\nwith their identified labels verified via the built-in uncertainty\nquantification and minimal human inspection; (3) supervised segmentation (more\npowerful than the segmentation in Step $1$) of multiphase microstructures\nthrough a segmentation network trained with micrographs and the results from\nSteps $1$-$2$ using a form of data augmentation. This framework can iteratively\ncharacterize\/segment new homogeneous or multiphase materials while expanding\nthe database to enhance performance. The framework is demonstrated on various\nsets of materials and texture images.",
        "A simple expression for the non-equilibrium distribution function in\nultra-fast transient processes is proposed. Postulating its dependence on\ntemporal derivatives of the equilibrium integrals of motion, non-equilibrium\nanalogues of the thermodynamic relationships are derived and the conditions\nthat maximize the non-equilibrium entropy are identified. A rigorous threshold\nbetween ``slow\" and ``fast\" processes is suggested, identifying the range of\napplicability of classical quasi-equilibrium description. The proposed theory\nis validated by deriving the known law of inertial heat conduction, which\naccounts for finite speed of thermal propagation. Finally, a new expression for\nthe non-equilibrium work is derived, revealing two kinds of pressure that\nemerge in fast non-equilibrium.",
        "This paper examines the Higgs particle self-coupling and its implications for\nelectroweak symmetry breaking in the Standard Model. We review the current\nexperimental constraints on the Higgs trilinear coupling and discuss the\nchallenges in measuring it precisely. The potential consequences of deviations\nfrom the Standard Model prediction are explored, including the possibility of\nnew physics. We then consider an alternative ultraviolet complete electroweak\ntheory based on finite quantum field theory, which does not require spontaneous\nsymmetry breaking or a non-zero vacuum expectation value. The predictions of\nthis model for particle masses and the stability of the electroweak vacuum are\ncompared to the standard Higgs mechanism. Finally, we review the SM derivation\nof the W-boson mass, its dependence on radiative corrections and its\nexperimentally determined value.",
        "The advent of high-average-power, ultrafast ytterbium-based lasers allows us\nto generate laser filaments at repetition rates ranging from 10s of kHz up to\n100s of kHz. At such high repetition rates, the inter-pulse time lies below the\ntime required for the total diffusion of the deposited heat by each laser\npulse, leading to cumulative hydrodynamic effects that have so far been rarely\nstudied. Here, we present, to the best of our knowledge, the first experimental\ntime-resolved measurements of these dynamics in air for laser repetition rates\nbetween 1 kHz and 100 kHz. We measure the change in the air refractive index\ncaused by the localized heat deposition and the length of the\nfilament-generated plasma channel, with which we can infer the corresponding\nchange in air density. We observe that at repetition rates above 10 kHz,\nstationary density depletions with vanishing dynamics emerge. Our findings are\nof wide relevance for the fields of high-repetition-rate laser filamentation\nand its applications, as well as THz generation from laser-induced plasma\nsources.",
        "Stokesian Dynamics (SD) is a numerical framework used for simulating\nhydrodynamic interactions in particle suspensions at low Reynolds number. It\ncombines far-field approximations with near-field lubrication corrections,\noffering a balance between accuracy and efficiency. This work reviews SD and\nprovides a perspective on future directions for this approach. We outline the\nmathematical foundations, the method's strengths and weaknesses, and the\ncomputational challenges that need to be overcome to work with SD effectively.\nWe also discuss recent advancements that improve the algorithm's efficiency,\nincluding the use of iterative solvers and matrix-free approaches. In addition,\nwe highlight the limitations of making stronger, albeit more cost-effective\napproximations to studying hydrodynamic interactions in dense suspensions than\nmade in SD, such as the two-body Rotne-Prager-Yamakawa (RPY) approximation. To\novercome these issues, we propose a hybrid framework that replaces SD's full\nmany-body computations with a neural network trained on SD data. That is, we\ncorrect the RPY approximation, while avoiding costly matrix inversions. We\ndemonstrate the potential of this method on a simple system, where we find a\nclose match to SD data while algorithmically outperforming RPY. Our work\nprovides an outlook on the way in which large-scale simulations of particle\nsuspensions can be performed in the foreseeable future.",
        "Quenched disorder in a solid state system can result in Anderson localization\nwhere electrons are exponentially localized and the system behaves like an\ninsulator. In this study, we investigate the effect of a DC electric field on\nAnderson localization. The study highlights the case of a one-dimensional\ninsulator chain with on-site disorder when a DC electric field is applied\nthroughout the chain. We study spectral properties of an Anderson localized\nsystem in equilibrium and out-of-equilibrium using a full lattice\nnonequilibrium Green's function method in the steady-state limit. Tuning the\ndisorder and the electric field strength results in the creation of exponential\nLifshitz tails near the band edge by strongly localized levels. These Lifshtiz\ntails create effects like insulator-to-metal transitions and contribute to\nnon-local hopping. The electric field causes gradual delocalization of the\nsystem and Anderson localization crossing over to Wannier Stark ladders at very\nstrong fields. Our study makes a comparison with the coherent potential\napproximation (CPA) highlighting some major differences and similarities in the\nphysics of disorder.",
        "We study the problem of PAC learning $\\gamma$-margin halfspaces in the\npresence of Massart noise. Without computational considerations, the sample\ncomplexity of this learning problem is known to be\n$\\widetilde{\\Theta}(1\/(\\gamma^2 \\epsilon))$. Prior computationally efficient\nalgorithms for the problem incur sample complexity $\\tilde{O}(1\/(\\gamma^4\n\\epsilon^3))$ and achieve 0-1 error of $\\eta+\\epsilon$, where $\\eta<1\/2$ is the\nupper bound on the noise rate. Recent work gave evidence of an\ninformation-computation tradeoff, suggesting that a quadratic dependence on\n$1\/\\epsilon$ is required for computationally efficient algorithms. Our main\nresult is a computationally efficient learner with sample complexity\n$\\widetilde{\\Theta}(1\/(\\gamma^2 \\epsilon^2))$, nearly matching this lower\nbound. In addition, our algorithm is simple and practical, relying on online\nSGD on a carefully selected sequence of convex losses.",
        "Assessing the prevalence of atmospheres on rocky planets around M-dwarf stars\nis a top priority of exoplanet science. High-energy activity from M-dwarfs can\ndestroy the atmospheres of these planets, which could explain the lack of\natmosphere detections to date. Volcanic outgassing has been proposed as a\nmechanism to replenish the atmospheres of tidally-heated rocky planets. L 98-59\nb, a sub-Earth transiting a nearby M dwarf, was recently identified as the most\npromising exoplanet to detect a volcanic atmosphere. We present the\ntransmission spectrum of L 98-59 b from four transits observed with JWST\nNIRSpec G395H. Although the airless model provides an adequate fit to the data\nbased on its $\\chi^2$, an SO$_2$ atmosphere is preferred by 3.6$\\sigma$ over a\nflat line in terms of the Bayesian evidence. Such an atmosphere would likely be\nin a steady state where volcanism balances escape. If so, L 98-59 b must\nexperience at least eight times as much volcanism and tidal heating per unit\nmass as Io. If volcanism is driven by runaway melting of the mantle, we predict\nthe existence of a subsurface magma ocean in L 98-59 b extending up to $R_p\\sim\n60-90\\%$. An SO$_2$-rich volcanic atmosphere on L 98-59 b would be indicative\nof an oxidized mantle with an oxygen fugacity of $f\\rm{O}_2>IW+2.7$, and it\nwould imply that L 98-59 b must have retained some of its volatile endowment\ndespite its proximity to its star. Our findings suggest that volcanism may\nrevive secondary atmospheres on tidally heated rocky planets around M-dwarfs.",
        "In this paper, we characterize all Artinian complete intersection\n$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In\naddition, we prove that such\n  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong\nLefschetz property.",
        "In the recent past, the work in the area of vanishing coefficients of\ninfinite $q$-products has been taken to the forefront. Weaving the same thread\nas Ramanujan, Richmond, Szekeres, Andrews, Alladi, Gordon, Mc Laughlin, Baruah,\nKaur, Tang, we further prove vanishing coefficients in arithmetic progressions\nmoduli 5, 7, 11, 13, 19, 21, 23 and 29 of the following four families of\ninfinite products, where $\\{X_{a,b,sm,km,u,v}(n)\\}_{n\\geq n_0}$,\n$\\{Y_{a,b,sm,km,u,v}(n)\\}_{n\\geq n_0}$, $\\{Z_{a,b,sm,km,u,v}(n)\\}_{n\\geq n_0}$\nand $\\{W_{a,b,sm,km,u,v}(n) \\}_{n\\geq n_0}$ are defined by \\begin{align*}\n\\sum_{n\\geq\nn_0}^{\\infty}X_{a,b,sm,km,u,v}(n)q^n:=&(q^{a},q^{sm-a};q^{sm})_{infty}^u(q^{b},q^{km-b};q^{km})_{infty}^v,\n\\\\ \\sum_{n\\geq\nn_0}^{\\infty}Y_{a,b,sm,km,u,v}(n)q^n:=&(q^{a},q^{sm-a};q^{sm})_{infty}^u(-q^{b},-q^{km-b};q^{km})_{infty}^v,\n\\\\ \\sum_{n\\geq\nn_0}^{\\infty}Z_{a,b,sm,km,u,v}(n)q^n:=&(-q^{a},-q^{sm-a};q^{sm})_{infty}^u(q^{b},q^{km-b};q^{km})_{infty}^v,\\\\\n\\sum_{n\\geq\nn_0}^{\\infty}W_{a,b,sm,km,u,v}(n)q^n:=&(-q^{a},-q^{sm-a};q^{sm})_{infty}^u(-q^{b},-q^{km-b};q^{km})_{infty}^v,\n\\end{align*} here $a, b, s, k, u$ and $v$ are chosen in such a way that the\ninfinite products in the right-hand side of the above are convergent and $n_0$\nis an integer (possibly negative or zero) depending on $a, b, s, k, u$ and $v$.\nThe proof uses the Jacobi triple product identity and the properties of\nRamanujan general theta function.",
        "Quantum machine learning is among the most exciting potential applications of\nquantum computing. However, the vulnerability of quantum information to\nenvironmental noises and the consequent high cost for realizing fault tolerance\nhas impeded the quantum models from learning complex datasets. Here, we\nintroduce AdaBoost.Q, a quantum adaptation of the classical adaptive boosting\n(AdaBoost) algorithm designed to enhance learning capabilities of quantum\nclassifiers. Based on the probabilistic nature of quantum measurement, the\nalgorithm improves the prediction accuracy by refining the attention mechanism\nduring the adaptive training and combination of quantum classifiers. We\nexperimentally demonstrate the versatility of our approach on a programmable\nsuperconducting processor, where we observe notable performance enhancements\nacross various quantum machine learning models, including quantum neural\nnetworks and quantum convolutional neural networks. With AdaBoost.Q, we achieve\nan accuracy above 86% for a ten-class classification task over 10,000 test\nsamples, and an accuracy of 100% for a quantum feature recognition task over\n1,564 test samples. Our results demonstrate a foundational tool for advancing\nquantum machine learning towards practical applications, which has broad\napplicability to both the current noisy and the future fault-tolerant quantum\ndevices.",
        "We verify that elliptic K3 surfaces and algebraic groups have many rational\npoints over function fields, i.e., they are geometrically special in the sense\nof Javanpeykar-Rousseau. We also show that under additional assumptions, this\ngeometric specialness persists under removal of closed subsets of codimension\nat least two.",
        "Consider a finite field $\\mathbb F_q$, $q=p^d$, where $p$ is an odd number.\nLet $M=(E,r)$ be a regular matroid; denote by ${\\mathcal B}$ the family of its\nbases, $\\bar s(M;\\alpha)=\\sum_{B\\in {\\mathcal B}}\\prod_{e\\not\\in B} \\alpha_e$,\nwhere ${\\alpha_e\\in \\mathbb F_q}$, $\\alpha_e\\neq 0$. Let a subset $A\\equiv\nA(\\alpha)$ in $E$ have the maximal cardinality and satisfy the condition $\\bar\ns(M|A;\\alpha)\\neq 0$, while ${r^*}(\\alpha)=|A|-r(E)$. Let us represent the\nvalue of the characteristic polynomial of the matroid $M$ at the point $q$ as\nthe linear combination of Legendre symbols with respect to $\\bar\ns(M|A;\\alpha)$, whose coefficients are modulo equal to $1\/q^{r^*(\\alpha)\/2}$.\nThis representation generalizes the formula for a flow polynomial of a graph\nwhich was obtained by us earlier. The latter formula is an analog of the\nso-called $\\alpha$-representation of vacuum Feynman amplitudes in the case of a\nfinite field, which has inspired the Kontsevich conjecture (1997). The\n$\\alpha$-representation technique is also applicable for expressing the number\nof Tait colorings for a cubic biconnected planar graph in terms of principal\nminors of the matrix of faces of this graph.",
        "In this article we consider a modification of the Stein's spherical maximal\noperator of complex order $\\alpha$ on ${\\mathbb R^n}$: $$ {\\mathfrak\nM}^\\alpha_{[1,2]} f(x) =\\sup\\limits_{t\\in [1,2]} \\big| {1\\over \\Gamma(\\alpha) }\n\\int_{|y|\\leq 1} \\left(1-|y|^2 \\right)^{\\alpha -1} f(x-ty) dy\\big|. $$ We show\nthat when $n\\geq 2$, suppose $\\|{\\mathfrak M}^{\\alpha}_{[1,2]} f\n\\|_{L^q({\\mathbb R^n})} \\leq C\\|f \\|_{L^p({\\mathbb R^n})}$ holds for some\n$\\alpha\\in \\mathbb{C}$, $p,q\\geq1$, then we must have that $q\\geq p$ and $${\\rm\nRe}\\,\\alpha\\geq \\sigma_n(p,q):=\\max\\left\\{\\frac{1}{p}-\\frac{n}{q},\\\n\\frac{n+1}{2p}-\\frac{n-1}{2}\\left(\\frac{1}{q}+1\\right),\\frac{n}{p}-n+1\\right\\}.$$\n  Conversely, we show that ${\\mathfrak M}^\\alpha_{[1,2]}$ is bounded from\n$L^p({\\mathbb R^n})$ to $L^q({\\mathbb R^n})$ provided that $q\\geq p$ and ${\\rm\nRe}\\,\\alpha>\\sigma_2(p,q)$ for $n=2$; and ${\\rm\nRe}\\,\\alpha>\\max\\left\\{\\sigma_n(p,q), 1\/(2p)- (n-2)\/(2q) -(n-1)\/4\\right\\}$ for\n$n>2$. The range of $\\alpha,p$ and $q$ is almost optimal in the case either\n$n=2$, or $\\alpha=0$, or $(p,q)$ lies in some regions for $n>2$."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Pediatric low-grade glioma: State-of-the-art and ongoing challenges",
    "start_abstract":"Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
      ],
      "abstract":[
        "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "The Complex Magnetic Field of the Extreme Galactic Center: PRIMA Science\n  Potential",
        "FoundationStereo: Zero-Shot Stereo Matching",
        "Self-Evaluation for Job-Shop Scheduling",
        "Quantifying the Speed-Up from Non-Reversibility in MCMC Tempering\n  Algorithms",
        "When is dataset cartography ineffective? Using training dynamics does\n  not improve robustness against Adversarial SQuAD",
        "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models",
        "Generation of Frequency-Tunable Shaped Single Microwave Photons Using a\n  Fixed-Frequency Superconducting Qubit",
        "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
        "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models",
        "$\\beta$-delayed neutron spectroscopy of $^{85, 86}$As with MONSTER at\n  IGISOL",
        "Auxiliary-field quantum Monte Carlo method with quantum selected\n  configuration interaction",
        "Exotic spherical flexible octahedra and counterexamples to the Modified\n  Bellows Conjecture",
        "Is Relevance Propagated from Retriever to Generator in RAG?",
        "Understanding colors of Dufaycolor: Can we recover them using historical\n  colorimetric and spectral data?",
        "Light-by-Light scattering in ultraperipheral heavy ion collisions:\n  Estimating inelastic contributions",
        "Toward Copyright Integrity and Verifiability via Multi-Bit Watermarking\n  for Intelligent Transportation Systems",
        "Mimicking How Humans Interpret Out-of-Context Sentences Through\n  Controlled Toxicity Decoding",
        "The Forestry of Adversarial Totient Iterations",
        "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
        "3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic\n  Segmentation in Autonomous Driving",
        "SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack\n  Segmentation in Structures",
        "Partition Tree Weighting for Non-Stationary Stochastic Bandits",
        "A time-dependent inverse source problem for a semilinear\n  pseudo-parabolic equation with Neumann boundary condition",
        "Path-Adaptive Matting for Efficient Inference Under Various\n  Computational Cost Constraints",
        "Non-adiabatic linear response in open quantum systems",
        "Flexible Exoskeleton Control Based on Binding Alignment Strategy and\n  Full-arm Coordination Mechanism",
        "Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for\n  Perspective-Specific Summarization of Clinical Q&A Forums",
        "Social inequality and cultural factors impact the awareness and reaction\n  during the cryptic transmission period of pandemic",
        "A Globally Convergent Method for Computing B-stationary Points of\n  Mathematical Programs with Equilibrium Constraints"
      ],
      "abstract":[
        "The Central Molecular Zone (CMZ) of the Galactic Center (GC) region of the\nMilky Way contains a substantial fraction of the molecular mass of the Galaxy\n>10e7 solar masses yet exhibits an order of magnitude lower star formation\nefficiency (SFE) than expected given the high densities found in this region.\nThere are multiple possible explanations for the depressed SFE in the CMZ, like\nfeedback, strong turbulence, longer free-fall timescales, and high magnetic\nfield strengths. It is currently unclear which of these mechanisms is the\ndominant inhibitor of star formation in the CMZ. It is important to understand\nthe star formation process in the extreme environment of the CMZ because it is\nthe only Galactic nuclear region we are able to study at high spatial\nresolutions with current observatories. One way to determine the relative\nimportance of the different SFE inhibiting mechanisms is through multi-spatial\nand multi-frequency polarimetric observations of the CMZ. Such observations\nwill provide insight into the behavior of the magnetic field in this unique\nenvironment. These observations will complement radio observations of\nnon-thermal structures revealing the magnetic field morphology and\npolarization. The PRobe far--Infrared Mission for Astrophysics (PRIMA) will be\nuniquely capable of contributing to such explorations by providing unique\nresolutions and frequencies for polarimetric observations. The PRIMAger\ninstrument will yield polarimetric observations covering the wavelength range\n80 -- 261 um with beam sizes ranging from 11 -- 28'', capabilities that\ncomplement existing and upcoming observatories.",
        "Tremendous progress has been made in deep stereo matching to excel on\nbenchmark datasets through per-domain fine-tuning. However, achieving strong\nzero-shot generalization - a hallmark of foundation models in other computer\nvision tasks - remains challenging for stereo matching. We introduce\nFoundationStereo, a foundation model for stereo depth estimation designed to\nachieve strong zero-shot generalization. To this end, we first construct a\nlarge-scale (1M stereo pairs) synthetic training dataset featuring large\ndiversity and high photorealism, followed by an automatic self-curation\npipeline to remove ambiguous samples. We then design a number of network\narchitecture components to enhance scalability, including a side-tuning feature\nbackbone that adapts rich monocular priors from vision foundation models to\nmitigate the sim-to-real gap, and long-range context reasoning for effective\ncost volume filtering. Together, these components lead to strong robustness and\naccuracy across domains, establishing a new standard in zero-shot stereo depth\nestimation. Project page: https:\/\/nvlabs.github.io\/FoundationStereo\/",
        "Combinatorial optimization problems, such as scheduling and route planning,\nare crucial in various industries but are computationally intractable due to\ntheir NP-hard nature. Neural Combinatorial Optimization methods leverage\nmachine learning to address these challenges but often depend on sequential\ndecision-making, which is prone to error accumulation as small mistakes\npropagate throughout the process. Inspired by self-evaluation techniques in\nLarge Language Models, we propose a novel framework that generates and\nevaluates subsets of assignments, moving beyond traditional stepwise\napproaches. Applied to the Job-Shop Scheduling Problem, our method integrates a\nheterogeneous graph neural network with a Transformer to build a policy model\nand a self-evaluation function. Experimental validation on challenging,\nwell-known benchmarks demonstrates the effectiveness of our approach,\nsurpassing state-of-the-art methods.",
        "We investigate the increase in efficiency of simulated and parallel tempering\nMCMC algorithms when using non-reversible updates to give them \"momentum\". By\nmaking a connection to a certain simple discrete Markov chain, we show that,\nunder appropriate assumptions, the non-reversible algorithms still exhibit\ndiffusive behaviour, just on a different time scale. We use this to argue that\nthe optimally scaled versions of the non-reversible algorithms are indeed more\nefficient than the optimally scaled versions of their traditional reversible\ncounterparts, but only by a modest speed-up factor of about 42%.",
        "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.",
        "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
        "Scaling up a superconducting quantum computer will likely require quantum\ncommunication between remote chips, which can be implemented using an itinerant\nmicrowave photon in a transmission line. To realize high-fidelity\ncommunication, it is essential to control the frequency and temporal shape of\nthe microwave photon. In this work, we demonstrate the generation of\nfrequency-tunable shaped microwave photons without resorting to any\nfrequency-tunable circuit element. We develop a framework which treats a\nmicrowave resonator as a band-pass filter mediating the interaction between a\nsuperconducting qubit and the modes in the transmission line. This\ninterpretation allows us to stimulate the photon emission by an off-resonant\ndrive signal. We characterize how the frequency and temporal shape of the\ngenerated photon depends on the frequency and amplitude of the drive signal. By\nmodulating the drive amplitude and frequency, we achieve a frequency tunability\nof 40 MHz while maintaining the photon mode shape time-symmetric.Through\nmeasurements of the quadrature amplitudes of the emitted photons, we\ndemonstrate consistently high state and process fidelities around 95\\% across\nthe tunable frequency range. Our hardware-efficient approach eliminates the\nneed for additional biasing lines typically required for frequency tuning,\noffering a simplified architecture for scalable quantum communication.",
        "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
        "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.",
        "The $\\beta$-delayed neutron emission in the $^{85, 86}$As $\\beta$-decays has\nbeen measured at the Ion Guide Isotope Separator On Line facility of the\nAccelerator Laboratory of the University of Jyv\\\"askyl\\\"a. The complete\n$\\beta$-decays have been studied with a complex setup that consists of a\nplastic scintillator for $\\beta$-particles, MONSTER -- the MOdular Neutron\ntime-of-flight SpectromeTER -- for neutrons, and a high-purity germanium and\nfour LaBr$_3$ crystals for $\\gamma$-rays. The $\\beta$-delayed neutron energy\ndistributions have been determined by unfolding the time-of-flight spectra with\nan innovative methodology based on the iterative Bayesian unfolding method and\naccurate Monte Carlo simulations. The results obtained for $^{85}$As are in\nexcellent agreement with the existing evaluated data, validating the proposed\nmethodology. In the case of $^{86}$As, a stronger neutron intensity at higher\nenergies than previously predicted is discovered.",
        "We propose using the wave function generated by the quantum selected\nconfiguration interaction (QSCI) method as the trial wave function in phaseless\nauxiliary-field quantum Monte Carlo (ph-AFQMC). In the QSCI framework,\nelectronic configurations are sampled from the quantum state realized on a\nquantum computer. These configurations serve as basis states for constructing\nan effective Hamiltonian, which is then diagonalized to obtain the\ncorresponding eigenstate. Using this wave function, ph-AFQMC is performed to\nrecover the dynamical electron correlation across the whole orbital space. The\nuse of the QSCI trial wave function is expected to improve the feasibility of\nthe quantum-classical (QC) hybrid quantum Monte Carlo approach [Nature, 603,\n416 (2022)]. We call this integrated approach QC-QSCI-AFQMC, or QSCI-AFQMC for\nshort. This method is validated across several molecular systems. For H2O and a\nlinear H4 chain, we achieved chemical accuracy in most investigations relative\nto full configuration interaction while utilizing superconducting quantum\ncomputers at Osaka University and RIKEN. Additionally, the application of\nQSCI-AFQMC to the O-H bond dissociation in an organic molecule highlights the\ncomplementary synergy between capturing static correlation on quantum hardware\nand incorporating dynamical correlation via classical post-processing. For the\nN2, when QSCI-AFQMC is executed with a noiseless simulator, it ranks among the\nmost accurate methods compared to various multireference electronic structure\ntheories. Although the proposed method is demonstrated using small active\nspaces on current quantum devices, the concept is not limited to few-qubit\nproblems. The QSCI-AFQMC can compete with state-of-the-art classical\ncomputational techniques, particularly in larger active spaces, displaying\nconsiderable potential for resolving classically intractable problems in\nquantum chemistry.",
        "In 2014 the author showed that in the three-dimensional spherical space,\nalongside with three classical types of flexible octahedra constructed by\nBricard, there exists a new type of flexible octahedra, which was called\nexotic. In the present paper we give a geometric construction for exotic\nflexible octahedra, describe their configuration spaces, and calculate their\nvolumes. We show that the volume of an exotic flexible octahedron is\nnonconstant during the flexion, and moreover the volume remains nonconstant if\nwe replace any set of vertices of the octahedron with their antipodes. So\nexotic flexible octahedra are counterexamples to the Modified Bellows\nConjecture proposed by the author in 2015.",
        "Retrieval Augmented Generation (RAG) is a framework for incorporating\nexternal knowledge, usually in the form of a set of documents retrieved from a\ncollection, as a part of a prompt to a large language model (LLM) to\npotentially improve the performance of a downstream task, such as question\nanswering. Different from a standard retrieval task's objective of maximising\nthe relevance of a set of top-ranked documents, a RAG system's objective is\nrather to maximise their total utility, where the utility of a document\nindicates whether including it as a part of the additional contextual\ninformation in an LLM prompt improves a downstream task. Existing studies\ninvestigate the role of the relevance of a RAG context for knowledge-intensive\nlanguage tasks (KILT), where relevance essentially takes the form of answer\ncontainment. In contrast, in our work, relevance corresponds to that of topical\noverlap between a query and a document for an information seeking task.\nSpecifically, we make use of an IR test collection to empirically investigate\nwhether a RAG context comprised of topically relevant documents leads to\nimproved downstream performance. Our experiments lead to the following\nfindings: (a) there is a small positive correlation between relevance and\nutility; (b) this correlation decreases with increasing context sizes (higher\nvalues of k in k-shot); and (c) a more effective retrieval model generally\nleads to better downstream RAG performance.",
        "Dufaycolor, an additive color photography process produced from 1935 to the\nlate 1950s, represents one of the most advanced iterations of this technique.\nThis paper presents ongoing research and development of an open-source\nColor-Screen tool designed to reconstruct the original colors of additive color\nphotographs. We discuss the incorporation of historical measurements of dyes\nused in the production of the color-screen filter (r\\'eseau) to achieve\naccurate color recovery.",
        "The current state-of-the-art theoretical estimations lead to cross-sections\nfor $AA \\to \\gamma \\gamma AA$ which are somewhat smaller than the measured ones\nby the ATLAS and CMS Collaborations, which motivates the searching and\ncalculation of subleading corrections disregarded in these previous studies. In\nthis paper, we estimate the contribution of inelastic channels to the Light -\nby - Light (LbL) scattering in ultraperipheral collisions of heavy ions\n(UPHICs), in which one or both of the incident nuclei dissociate ($A A \\to\n\\gamma \\gamma X Y$ where $X, Y = A, A'$) due to the photon emission. These new\nmechanisms are related to extra emissions that are rather difficult to identify\nat the LHC and can be mistakenly interpreted as enhanced $\\gamma \\gamma \\to\n\\gamma \\gamma$ scattering compared to the Standard Model result. We include\nprocesses of coupling of photons to individual nucleons (protons and neutrons)\nin addition to coherent coupling to the whole nuclei (called standard approach\nhere). Both elastic (nucleon in the ground state) and inelastic (nucleon in an\nexcited state) in the couplings of photons to nucleons are taken into account.\nThe inelastic nucleon fluxes are calculated using CT18qed photon in nucleon\nPDFs. The inelastic photon fluxes are shown and compared to standard photon\nfluxes in the nucleus. In addition, we show the ratio of the inelastic\ncorrections to the standard contribution as a function of diphoton invariant\nmass and photon rapidity difference. We find the maximal effect of the\ninelastic corrections at $M_{\\gamma \\gamma} \\sim$ 14 GeV for the ATLAS rapidity\nand transverse momentum acceptance. Furthermore, the inelastic contribution\nincreases gradually with photon rapidity difference. Our results indicate that\nthe inelastic contributions can increase locally by 10-15 \\% the traditional\n(no nuclear excitation) predictions for the LbL scattering in UPCs.",
        "Intelligent transportation systems (ITS) use advanced technologies such as\nartificial intelligence to significantly improve traffic flow management\nefficiency, and promote the intelligent development of the transportation\nindustry. However, if the data in ITS is attacked, such as tampering or\nforgery, it will endanger public safety and cause social losses. Therefore,\nthis paper proposes a watermarking that can verify the integrity of copyright\nin response to the needs of ITS, termed ITSmark. ITSmark focuses on functions\nsuch as extracting watermarks, verifying permission, and tracing tampered\nlocations. The scheme uses the copyright information to build the multi-bit\nspace and divides this space into multiple segments. These segments will be\nassigned to tokens. Thus, the next token is determined by its segment which\ncontains the copyright. In this way, the obtained data contains the custom\nwatermark. To ensure the authorization, key parameters are encrypted during\ncopyright embedding to obtain cipher data. Only by possessing the correct\ncipher data and private key, can the user entirely extract the watermark.\nExperiments show that ITSmark surpasses baseline performances in data quality,\nextraction accuracy, and unforgeability. It also shows unique capabilities of\npermission verification and tampered location tracing, which ensures the\nsecurity of extraction and the reliability of copyright verification.\nFurthermore, ITSmark can also customize the watermark embedding position and\nproportion according to user needs, making embedding more flexible.",
        "Interpretations of a single sentence can vary, particularly when its context\nis lost. This paper aims to simulate how readers perceive content with varying\ntoxicity levels by generating diverse interpretations of out-of-context\nsentences. By modeling toxicity, we can anticipate misunderstandings and reveal\nhidden toxic meanings. Our proposed decoding strategy explicitly controls\ntoxicity in the set of generated interpretations by (i) aligning interpretation\ntoxicity with the input, (ii) relaxing toxicity constraints for more toxic\ninput sentences, and (iii) promoting diversity in toxicity levels within the\nset of generated interpretations. Experimental results show that our method\nimproves alignment with human-written interpretations in both syntax and\nsemantics while reducing model prediction uncertainty.",
        "We give a closed-form expression for\n$\\varphi(1+\\varphi(2+\\varphi(3+...+\\varphi(n)$, where $\\varphi$ is Euler's\ntotient function. More generally, for an integer sequence $A=\\{a_j\\}$ we study\nthe value of\n$A^\\varphi(n)=\\varphi(a_1+\\varphi(a_2+\\varphi(a_3+...+\\varphi(a_n)$ when $A$ is\nthe perfect squares or the perfect cubes. We show $A^\\varphi(n)$ is bounded for\nall sequences considered. We also present the Arboreal Algorithm which can\nsometimes determine a closed form of $A^\\varphi(n)$ using tree-like structures.",
        "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https:\/\/github.com\/zsq259\/Plan-over-Graph.",
        "Domain generalization aims to find ways for deep learning models to maintain\ntheir performance despite significant domain shifts between training and\ninference datasets. This is particularly important for models that need to be\nrobust or are costly to train. LiDAR perception in autonomous driving is\nimpacted by both of these concerns, leading to the emergence of various\napproaches. This work addresses the challenge by proposing a geometry-based\napproach, leveraging the sequential structure of LiDAR sensors, which sets it\napart from the learning-based methods commonly found in the literature. The\nproposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic\nSegmentation (LSS). Through extensive experimentation on seven datasets, it is\ndemonstrated to be a state-of-the-art approach, outperforming both naive and\nother domain generalization methods.",
        "Pixel-level segmentation of structural cracks across various scenarios\nremains a considerable challenge. Current methods encounter challenges in\neffectively modeling crack morphology and texture, facing challenges in\nbalancing segmentation quality with low computational resource usage. To\novercome these limitations, we propose a lightweight Structure-Aware Vision\nMamba Network (SCSegamba), capable of generating high-quality pixel-level\nsegmentation maps by leveraging both the morphological information and texture\ncues of crack pixels with minimal computational cost. Specifically, we\ndeveloped a Structure-Aware Visual State Space module (SAVSS), which\nincorporates a lightweight Gated Bottleneck Convolution (GBC) and a\nStructure-Aware Scanning Strategy (SASS). The key insight of GBC lies in its\neffectiveness in modeling the morphological information of cracks, while the\nSASS enhances the perception of crack topology and texture by strengthening the\ncontinuity of semantic information between crack pixels. Experiments on crack\nbenchmark datasets demonstrate that our method outperforms other\nstate-of-the-art (SOTA) methods, achieving the highest performance with only\n2.8M parameters. On the multi-scenario dataset, our method reached 0.8390 in F1\nscore and 0.8479 in mIoU. The code is available at\nhttps:\/\/github.com\/Karl1109\/SCSegamba.",
        "This paper considers a generalisation of universal source coding for\ninteraction data, namely data streams that have actions interleaved with\nobservations. Our goal will be to construct a coding distribution that is both\nuniversal \\emph{and} can be used as a control policy. Allowing for action\ngeneration needs careful treatment, as naive approaches which do not\ndistinguish between actions and observations run into the self-delusion problem\nin universal settings. We showcase our perspective in the context of the\nchallenging non-stationary stochastic Bernoulli bandit problem. Our main\ncontribution is an efficient and high performing algorithm for this problem\nthat generalises the Partition Tree Weighting universal source coding technique\nfor passive prediction to the control setting.",
        "In this paper, we study the inverse problem for determining an unknown\ntime-dependent source coefficient in a semilinear pseudo-parabolic equation\nwith variable coefficients and Neumann boundary condition. This unknown source\nterm is recovered from the integral measurement over the domain $\\Omega$. Based\non Rothe's method, the existence and uniqueness of a weak solution, under\nsuitable assumptions on the data, is established. A numerical time-discrete\nscheme for the unique weak solution and the unknown source coefficient is\ndesigned, and the convergence of the approximations is proved. Some numerical\nexperiments are presented to support the obtained theoretical results.",
        "In this paper, we explore a novel image matting task aimed at achieving\nefficient inference under various computational cost constraints, specifically\nFLOP limitations, using a single matting network. Existing matting methods\nwhich have not explored scalable architectures or path-learning strategies,\nfail to tackle this challenge. To overcome these limitations, we introduce\nPath-Adaptive Matting (PAM), a framework that dynamically adjusts network paths\nbased on image contexts and computational cost constraints. We formulate the\ntraining of the computational cost-constrained matting network as a bilevel\noptimization problem, jointly optimizing the matting network and the path\nestimator. Building on this formalization, we design a path-adaptive matting\narchitecture by incorporating path selection layers and learnable connect\nlayers to estimate optimal paths and perform efficient inference within a\nunified network. Furthermore, we propose a performance-aware path-learning\nstrategy to generate path labels online by evaluating a few paths sampled from\nthe prior distribution of optimal paths and network estimations, enabling\nrobust and efficient online path learning. Experiments on five image matting\ndatasets demonstrate that the proposed PAM framework achieves competitive\nperformance across a range of computational cost constraints.",
        "Adiabatic theorem and non-adiabatic corrections have been widely applied in\nmodern quantum technology. Recently, non-adiabatic linear response theory has\nbeen developed to probe the many-body correlations in closed systems. In this\nwork, we generalize the non-adiabatic linear response theory to open quantum\nmany-body systems. We find that the linear deviation from steady states is\nmemory-less, similar to the closed system. The linear response of observables\nwhile ramping the Hamiltonian is still related to the derivative of the\nretarded Green's function. However, as one ramps the dissipation, the\ncorresponding coefficient is determined by a new high-order correlation\nfunction in the steady state. Our work gives a neat result and generalizes the\ntool of ramping dynamics to study the many-body correlations in open quantum\nsystems.",
        "In rehabilitation, powered, and teleoperation exoskeletons, connecting the\nhuman body to the exoskeleton through binding attachments is a common\nconfiguration. However, the uncertainty of the tightness and the donning\ndeviation of the binding attachments will affect the flexibility and comfort of\nthe exoskeletons, especially during high-speed movement. To address this\nchallenge, this paper presents a flexible exoskeleton control approach with\nbinding alignment and full-arm coordination. Firstly, the sources of the force\ninteraction caused by donning offsets are analyzed, based on which the\ninteractive force data is classified into the major, assistant, coordination,\nand redundant component categories. Then, a binding alignment strategy (BAS) is\nproposed to reduce the donning disturbances by combining different force data.\nFurthermore, we propose a full-arm coordination mechanism (FCM) that focuses on\ntwo modes of arm movement intent, joint-oriented and target-oriented, to\nimprove the flexible performance of the whole exoskeleton control during\nhigh-speed motion. In this method, we propose an algorithm to distinguish the\ntwo intentions to resolve the conflict issue of the force component. Finally, a\nseries of experiments covering various aspects of exoskeleton performance\n(flexibility, adaptability, accuracy, speed, and fatigue) were conducted to\ndemonstrate the benefits of our control framework in our full-arm exoskeleton.",
        "The PerAnsSumm 2025 challenge focuses on perspective-aware healthcare answer\nsummarization (Agarwal et al., 2025). This work proposes a few-shot learning\nframework using a Snorkel-BART-SVM pipeline for classifying and summarizing\nopen-ended healthcare community question-answering (CQA). An SVM model is\ntrained with weak supervision via Snorkel, enhancing zero-shot learning.\nExtractive classification identifies perspective-relevant sentences, which are\nthen summarized using a pretrained BART-CNN model. The approach achieved 12th\nplace among 100 teams in the shared task, demonstrating computational\nefficiency and contextual accuracy. By leveraging pretrained summarization\nmodels, this work advances medical CQA research and contributes to clinical\ndecision support systems.",
        "The World Health Organization (WHO) declared the COVID-19 outbreak a Public\nHealth Emergency of International Concern (PHEIC) on January 31, 2020. However,\nrumors of a \"mysterious virus\" had already been circulating in China in\nDecember 2019, possibly preceding the first confirmed COVID-19 case.\nUnderstanding how awareness about an emerging pandemic spreads through society\nis vital not only for enhancing disease surveillance, but also for mitigating\ndemand shocks and social inequities, such as shortages of personal protective\nequipment (PPE) and essential supplies. Here we leverage a massive e-commerce\ndataset comprising 150 billion online queries and purchase records from 94\nmillion people to detect the traces of early awareness and public response\nduring the cryptic transmission period of COVID-19. Our analysis focuses on\nidentifying information gaps across different demographic cohorts, revealing\nsignificant social inequities and the role of cultural factors in shaping\nawareness diffusion and response behaviors. By modeling awareness diffusion in\nheterogeneous social networks and analyzing online shopping behavior, we\nuncover the evolving characteristics of vulnerable populations. Our findings\nexpand the theoretical understanding of awareness spread and social inequality\nin the early stages of a pandemic, highlighting the critical importance of\ne-commerce data and social network data in effectively and timely addressing\nfuture pandemic challenges. We also provide actionable recommendations to\nbetter manage and mitigate dynamic social inequalities in public health crises.",
        "This paper introduces a method that globally converges to B-stationary points\nof mathematical programs with equilibrium constraints (MPECs) in a finite\nnumber of iterations. B-stationarity is necessary for optimality and means that\nno feasible first-order direction improves the objective. Given a feasible\npoint of an MPEC, B-stationarity can be certified by solving a linear program\nwith equilibrium constraints (LPEC) constructed at this point. The proposed\nmethod solves a sequence of LPECs, which either certify B-stationarity or\nprovide an active-set estimate for the complementarity constraints, and\nnonlinear programs (NLPs) - referred to as branch NLPs (BNLPs) - obtained by\nfixing the active set in the MPEC. A BNLP is more regular than the MPEC, easier\nto solve, and with the correct active set, its solution coincides with the\nsolution of the MPEC. The method has two phases: the first phase identifies a\nfeasible BNLP or certifies local infeasibility, and the second phase solves a\nfinite sequence of BNLPs until a B-stationary point of the MPEC is found. The\npaper provides a detailed convergence analysis and discusses implementation\ndetails. In addition, extensive numerical experiments and an open-source\nsoftware implementation are provided. The experiments demonstrate that the\nproposed method is more robust and faster than relaxation-based methods, while\nalso providing a certificate of B-stationarity without requiring the usual\ntoo-restrictive assumptions."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks",
    "start_abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
      ],
      "abstract":[
        "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Automated Microsolvation for Minimum Energy Path Construction in\n  Solution",
        "Giant Uncompensated Magnon Spin Currents in X-type Magnets",
        "Hedging with Sparse Reward Reinforcement Learning",
        "Construction A Lattice Design Based on the Truncated Union Bound",
        "Data-Aided Regularization of Direct-Estimate Combiner in Distributed\n  MIMO Systems",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Coverage errors for Student's t confidence intervals comparable to those\n  in Hall (1988)",
        "Evidence of Athermal Metastable Phase in a Halide Perovskite: Optically\n  Tracked Thermal-Breach Memory",
        "Do We Need to Verify Step by Step? Rethinking Process Supervision from a\n  Theoretical Perspective",
        "The bound and resonant states of $D^{(*)}D^{(*)}$ and\n  $D^{(*)}\\bar{D}^{(*)}$ with the complex scaling method",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "Fastest mixing reversible Markov chain on friendship graph: Trade-off\n  between transition probabilities among friends and convergence rate",
        "From Paramagnet to Dipolar Topological Order via Duality and Dipolar SPT",
        "Engagement Zones for a Turn Constrained Pursuer",
        "Benefits of Early Stopping in Gradient Descent for Overparameterized\n  Logistic Regression",
        "Time-Varying Causal Survival Learning",
        "Design and Analysis of a Concatenated Code for Intersymbol Interference\n  Wiretap Channels",
        "Mixed-state learnability transitions in monitored noisy quantum dynamics",
        "Limit Theorems for One-Dimensional Homogenized Diffusion Processes",
        "Quantifying and Visualizing the Microscopic Degrees of Freedom of Grain\n  Boundaries in the Wigner-Seitz Cell of the Displacement-Shift-Complete\n  Lattice",
        "Titan's Fluvial and Lacustrine Landscapes",
        "Claw-free cubic graphs are (1, 1, 1, 3)-packing edge-colorable",
        "Dynamics of test particles and scalar perturbations around\n  Ay\\'{o}n-Beato-Garc\\'{i}a black hole coupled with cloud of strings",
        "The complex Liouville string: the gravitational path integral",
        "Spin density matrix for neutral $\\rho$ mesons in a pion gas in linear\n  response theory",
        "Single pion resonant production in BSM scenarios: cross sections and\n  amplitudes",
        "Auxiliary dynamical mean-field approach for Anderson-Hubbard model with\n  off-diagonal disorder",
        "Model Categories and the Higher Riemann-Hilbert Correspondence",
        "Taylor-Couette flow with split endcaps: preparatory hydrodynamic study\n  for upcoming DRESDYN-MRI experiment"
      ],
      "abstract":[
        "Describing chemical reactions in solution on a molecular level is a\nchallenging task due to the high mobility of weakly interacting solvent\nmolecules which requires configurational sampling. For instance, polar and\nprotic solvents can interact strongly with solutes and may interfere in\nreactions. However, to define and identify representative arrangements of\nsolvent molecules modulating a transition state is a non-trivial task. Here, we\npropose to monitor their active participation in the decaying normal mode at a\ntransition state, which defines active solvent molecules. Moreover, it is\ndesirable to prepare a low-dimensional microsolvation model in a well-defined,\nfully automated, high-throughput, and easy-to-deploy fashion, which we propose\nto derive in a stepwise protocol. First, transition state structures are\noptimized in a sufficiently solvated quantum-classical hybrid model, which are\nthen subjected to a re-definition of a then reduced quantum region. From the\nreduced model, minimally microsolvated structures are extracted that contain\nonly active solvent molecules. Modeling the remaining solvation effects is\ndeferred to a continuum model. To establish an easy-to-use free-energy model,\nwe combine the standard thermochemical gas-phase model with a correction for\nthe cavity entropy in solution. We assess our microsolvation and free-energy\nmodels for methanediol formation from formaldehyde, for the hydration of carbon\ndioxide (which we consider in a solvent mixture to demonstrate the versatility\nof our approach), and, finally, for the chlorination of phenol with\nhypochlorous acid.",
        "Magnon spin currents in insulating magnets are useful for low-power\nspintronics. However, in magnets stacked by antiferromagnetic (AFM) exchange\ncoupling, which have recently aroused significant interest for potential\napplications in spintronics, these currents are largely counteracted by\nopposite magnetic sublattices, thus suppressing their net effect. Contrary to\nthis common observation, here, we show that magnets with X-type AFM stacking,\nwhere opposite magnetic sublattices form orthogonal intersecting chains,\nsupport giant magnon spin currents with minimal compensation. Our model\nHamiltonian calculations predict magnetic chain locking of magnon spin currents\nin these X-type magnets, significantly reducing their compensation ratio. In\naddition, the one-dimensional nature of the chain-like magnetic sublattices\nenhances magnon spin conductivities surpassing those of two-dimensional\nferromagnets and canonical altermagnets. Notably, uncompensated X-type magnets,\nsuch as odd-layer antiferromagnets and ferrimagnets, can exhibit magnon spin\ncurrents polarized opposite to those expected by their net magnetization. These\nunprecedented properties of X-type magnets, combined with their inherent\nadvantages resulting from AFM coupling, offer a promising new path for\nlow-power high-performance spintronics.",
        "Derivatives, as a critical class of financial instruments, isolate and trade\nthe price attributes of risk assets such as stocks, commodities, and indices,\naiding risk management and enhancing market efficiency. However, traditional\nhedging models, constrained by assumptions such as continuous trading and zero\ntransaction costs, fail to satisfy risk control requirements in complex and\nuncertain real-world markets.\n  With advances in computing technology and deep learning, data-driven trading\nstrategies are becoming increasingly prevalent. This thesis proposes a\nderivatives hedging framework integrating deep learning and reinforcement\nlearning. The framework comprises a probabilistic forecasting model and a\nhedging agent, enabling market probability prediction, derivative pricing, and\nhedging.\n  Specifically, we design a spatiotemporal attention-based probabilistic\nfinancial time series forecasting Transformer to address the scarcity of\nderivatives hedging data. A low-rank attention mechanism compresses\nhigh-dimensional assets into a low-dimensional latent space, capturing\nnonlinear asset relationships. The Transformer models sequential dependencies\nwithin this latent space, improving market probability forecasts and\nconstructing an online training environment for downstream hedging tasks.\n  Additionally, we incorporate generalized geometric Brownian motion to develop\na risk-neutral pricing approach for derivatives. We model derivatives hedging\nas a reinforcement learning problem with sparse rewards and propose a behavior\ncloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This\npretraining-finetuning framework significantly enhances the hedging agent's\nperformance. Numerical experiments in the U.S. and Chinese financial markets\ndemonstrate our method's superiority over traditional approaches.",
        "This paper considers $n= 128$ dimensional construction A lattice design,\nusing binary codes with known minimum Hamming distance and codeword\nmultiplicity, the number of minimum weight codeword. A truncated theta series\nof the lattice is explicitly given to obtain the truncated union bound to\nestimate the word error rate under maximum likelihood decoding. The best\ncomponent code is selected by minimizing the required volume-to-noise ratio\n(VNR) for a target word error rate $P_e$. The estimate becomes accurate for\n$P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH\ncodes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is\nachieved compared to that by the classic balanced distance rule and the equal\nerror probability rule. The $(128, 106, 8)$ EBCH code gives the best-known\n$n=128$ construction A lattice at $P_e= 10^{-5}$.",
        "This paper explores the data-aided regularization of the direct-estimate\ncombiner in the uplink of a distributed multiple-input multiple-output system.\nThe network-wide combiner can be computed directly from the pilot signal\nreceived at each access point, eliminating the need for explicit channel\nestimation. However, the sample covariance matrix of the received pilot signal\nthat is used in its computation may significantly deviate from the actual\ncovariance matrix when the number of pilot symbols is limited. To address this,\nwe apply a regularization to the sample covariance matrix using a shrinkage\ncoefficient based on the received data signal. Initially, the shrinkage\ncoefficient is determined by minimizing the difference between the sample\ncovariance matrices obtained from the received pilot and data signals. Given\nthe limitations of this approach in interference-limited scenarios, the\nshrinkage coefficient is iteratively optimized using the sample mean squared\nerror of the hard-decision symbols, which is more closely related to the actual\nsystem's performance, e.g., the symbol error rate (SER). Numerical results\ndemonstrate that the proposed regularization of the direct-estimate combiner\nsignificantly enhances the SER, particularly when the number of pilot symbols\nis limited.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "Table 1 of Hall (1988) contains asymptotic coverage error formulas for some\nnonparametric approximate 95% confidence intervals for the mean based on $n$\nIID samples. The table includes an entry for an interval based on the central\nlimit theorem using Gaussian quantiles and the Gaussian maximum likelihood\nvariance estimate. It is missing an entry for the very widely used Student $t$\nconfidence intervals. This note makes a mild numerical correction for the\nGaussian entry and provides an entry for the Student $t$ intervals. For\nskewness $\\gamma$ and kurtosis $\\kappa$, the corrected Gaussian formula is\n$0.14\\kappa -2.16\\gamma^2-3.42$ and the formula for the $t$ intervals is\n$0.14\\kappa -2.16\\gamma^2$. The impetus to revisit this estimate arose from the\nsurprisingly robust performance of Student's t statistic in randomized\nquasi-Monte Carlo sampling.",
        "Halide perovskite materials have been extensively studied in the last decade\nbecause of their impressive optoelectronic properties. However, their one\ncharacteristic that is uncommon for semiconductors is that many undergo\nthermally induced structural phase transitions. The transition is hysteretic,\nwith the hysteresis window marking the boundary of the metastable phase. We\nhave discovered that in methylammonium lead iodide, this hysteretic metastable\nphase is athermal, meaning it shows almost no temporal phase evolution under\nisothermal conditions. We also show that a large number of distinguishable\nmetastable states can be prepared following different thermal pathways.\nFurthermore, under a reversible thermal perturbation, the states in the\nmetastable phase either show return-point memory or undergo a systematic\nnonrecoverable phase evolution, depending on the thermal history and the sign\nof the temperature perturbation. Since the phase fraction can be probed with\nextreme sensitivity via luminescence, we have an optically retrievable memory\nthat reliably records any breach in temperature stability. Such thermal-breach\nmemory in athermal martensites, of which there are numerous examples, may be\nuseful for tagging packages requiring strict temperature control during\ntransportation or preservation.",
        "As large language models have evolved, it has become crucial to distinguish\nbetween process supervision and outcome supervision -- two key reinforcement\nlearning approaches to complex reasoning tasks. While process supervision\noffers intuitive advantages for long-term credit assignment, the precise\nrelationship between these paradigms has remained an open question.\nConventional wisdom suggests that outcome supervision is fundamentally more\nchallenging due to the trajectory-level coverage problem, leading to\nsignificant investment in collecting fine-grained process supervision data.\n  In this paper, we take steps towards resolving this debate. Our main theorem\nshows that, under standard data coverage assumptions, reinforcement learning\nthrough outcome supervision is no more statistically difficult than through\nprocess supervision, up to polynomial factors in horizon. At the core of this\nresult lies the novel Change of Trajectory Measure Lemma -- a technical tool\nthat bridges return-based trajectory measure and step-level distribution shift.\nFurthermore, for settings with access to a verifier or a rollout capability, we\nprove that any policy's advantage function can serve as an optimal process\nreward model, providing a direct connection between outcome and process\nsupervision. These findings suggest that the empirically observed performance\ngap -- if any -- between outcome and process supervision likely stems from\nalgorithmic limitations rather than inherent statistical difficulties,\npotentially transforming how we approach data collection and algorithm design\nfor reinforcement learning.",
        "We perform a systematic study of the possible molecular states composed of a\npair of heavy mesons such as $D^{(*)}D^{(*)}$, $D^{(*)}\\bar{D}^{(*)}$ in the\nframework of the one-boson-exchange model. The exchanged bosons include the\npseudoscalar, scalar and vector mesons($\\pi$, $\\sigma$, $\\rho$, $\\omega$). We\nuse the Bonn approximation to get the interaction potential of\none-boson-exchange model, then apply the complex scaling method to calculate\nthe bound and resonant states. The results indicate that the $D^{(*)}D^{(*)}$\nand $D^{(*)}\\bar{D}^{(*)}$ system can not only form several bound states, but\nalso a P-wave resonant state. The hadron molecular state model can explain the\nstructure of $T_{cc}^+$ as a bound state $DD^{*}$ with quantum number $I(J^P) =\n0(1^+)$. In addition, we also discovered other bound and resonant states, which\nhave the potential to be observed experimentally.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "A long-standing goal of social network research has been to alter the\nproperties of network to achieve the desired outcome. In doing so, DeGroot's\nconsensus model has served as the popular choice for modeling the information\ndiffusion and opinion formation in social networks. Achieving a trade-off\nbetween the cost associated with modifications made to the network and the\nspeed of convergence to the desired state has shown to be a critical factor.\nThis has been treated as the Fastest Mixing Markov Chain (FMMC) problem over a\ngraph with given transition probabilities over a subset of edges. Addressing\nthis multi-objective optimization problem over the friendship graph, this paper\nhas provided the corresponding Pareto optimal points or the Pareto frontier. In\nthe case of friendship graph with at least three blades, it is shown that the\nPareto frontier is reduced to a global minimum point which is same as the\noptimal point corresponding to the minimum spanning tree of the friendship\ngraph, i.e., the star topology. Furthermore, a lower limit for transition\nprobabilities among friends has been provided, where values higher than this\nlimit do not have any impact on the convergence rate.",
        "A scheme for the adaptive preparation of a topological state with dipole\nsymmetry, dubbed the dipolar topological state (dTS), which serves as an\nexample of translation symmetry-enriched topological phase, is proposed. The\nmidcircuit state emerging during the preparation process is identified as a\ntwo-dimensional symmetry-protected topological (SPT) state protected by dipole\nbundle symmetry alongside charge and 1-form symmetries. The non-trivial\nboundary modes of the dipolar SPT state exhibiting the spontaneous breaking of\ncharge and dipole bundle symmetries are analyzed. The duality map between the\nparamagnetic state and the dipolar topological state is established in the\nframework of the {\\it simultaneous gauging} of two charge symmetries and one\ndipole symmetry that cannot be reduced as sequential gauging of the individual\nsymmetry. Leveraging this duality, we work out the phase diagram of the dipolar\ntopological state under perturbations by various transverse fields.",
        "This work derives two basic engagement zone models, describing regions of\npotential risk or capture for a mobile vehicle by a pursuer. The pursuer is\nmodeled as having turn-constraints rather than simple motion. Turn-only\n(C-Paths) and turn-straight (CS-Paths) paths are considered for the pursuer of\nlimited range. Following the derivation, a simulation of a vehicle avoiding the\npursuer's engagement zone is provided.",
        "In overparameterized logistic regression, gradient descent (GD) iterates\ndiverge in norm while converging in direction to the maximum $\\ell_2$-margin\nsolution -- a phenomenon known as the implicit bias of GD. This work\ninvestigates additional regularization effects induced by early stopping in\nwell-specified high-dimensional logistic regression. We first demonstrate that\nthe excess logistic risk vanishes for early-stopped GD but diverges to infinity\nfor GD iterates at convergence. This suggests that early-stopped GD is\nwell-calibrated, whereas asymptotic GD is statistically inconsistent. Second,\nwe show that to attain a small excess zero-one risk, polynomially many samples\nare sufficient for early-stopped GD, while exponentially many samples are\nnecessary for any interpolating estimator, including asymptotic GD. This\nseparation underscores the statistical benefits of early stopping in the\noverparameterized regime. Finally, we establish nonasymptotic bounds on the\nnorm and angular differences between early-stopped GD and $\\ell_2$-regularized\nempirical risk minimizer, thereby connecting the implicit regularization of GD\nwith explicit $\\ell_2$-regularization.",
        "This work bridges the gap between staggered adoption designs and survival\nanalysis to estimate causal effects in settings with time-varying treatments,\naddressing a fundamental challenge in medical research exemplified by the\nStanford Heart Transplant study. In medical interventions, particularly organ\ntransplantation, the timing of treatment varies significantly across patients\ndue to factors such as donor availability and patient readiness, introducing\npotential bias in treatment effect estimation if not properly accounted for. We\nidentify conditions under which staggered adoption assumptions can justify the\nuse of survival analysis techniques for causal inference with time-varying\ntreatments. By establishing this connection, we enable the use of existing\nsurvival analysis methods while maintaining causal interpretability.\nFurthermore, we enhance estimation performance by incorporating double machine\nlearning methods, improving efficiency when handling complex relationships\nbetween patient characteristics and survival outcomes. Through both simulation\nstudies and application to heart transplant data, our approach demonstrates\nsuperior performance compared to traditional methods, reducing bias and\noffering theoretical guarantees for improved efficiency in survival analysis\nsettings.",
        "We propose a two-stage concatenated coding scheme for reliable and\ninformation-theoretically secure communication over intersymbol interference\nwiretap channels. Motivated by the theoretical coding strategies that achieve\nthe secrecy capacity, our scheme integrates low-density parity-check (LDPC)\ncodes in the outer stage, forming a nested structure of wiretap codes, with\ntrellis codes in the inner stage to improve achievable secure rates. The\ntrellis code is specifically designed to transform the uniformly distributed\ncodewords produced by the LDPC code stage into a Markov process, achieving\ntight lower bounds on the secrecy capacity. We further estimate the information\nleakage rate of the proposed coding scheme using an upper bound. To meet the\nweak secrecy criterion, we optimize degree distributions of the irregular LDPC\ncodes at the outer stage, essentially driving the estimated upper bound on the\ninformation leakage rate to zero.",
        "We consider learnability transitions in monitored quantum systems that\nundergo noisy evolution, subject to a global strong symmetry -- i.e., in\naddition to the measuring apparatus, the system can interact with an unobserved\nenvironment, but does not exchange charge with it. As in the pure-state\nsetting, we find two information-theoretic phases -- a sharp (fuzzy) phase in\nwhich an eavesdropper can rapidly (slowly) learn the symmetry charge. However,\nbecause the dynamics is noisy, both phases can be simulated efficiently using\ntensor networks. Indeed, even when the true dynamics is unitary, introducing\nnoise by hand allows an eavesdropper to efficiently learn the symmetry charge\nfrom local measurements, as we demonstrate. We identify the fuzzy phase in this\nsetting as a mixed-state phase that exhibits spontaneous strong-to-weak\nsymmetry breaking.",
        "We present two limit theorems, a mean ergodic and a central limit theorem,\nfor a specific class of one-dimensional diffusion processes that depend on a\nsmall-scale parameter $\\varepsilon$ and converge weakly to a homogenized\ndiffusion process in the limit $\\varepsilon \\rightarrow 0$. In these results,\nwe allow for the time horizon to blow up such that $T_\\varepsilon \\rightarrow\n\\infty$ as $\\varepsilon \\rightarrow 0$. The novelty of the results arises from\nthe circumstance that many quantities are unbounded for $\\varepsilon\n\\rightarrow 0$, so that formerly established theory is not directly applicable\nhere and a careful investigation of all relevant $\\varepsilon$-dependent terms\nis required. As a mathematical application, we then use these limit theorems to\nprove asymptotic properties of a minimum distance estimator for parameters in a\nhomogenized diffusion equation.",
        "We introduce a grain boundary (GB) translation vector, $\\textbf{t}^{WS}$, to\ndescribe and quantify the domain of the microscopic degrees of freedom of GBs.\nIt has long been recognized that for fixed macroscopic degrees of freedom of a\nGB there exists a large multiplicity of states characterized by different\nrelative grain translations. More recently another degree of freedom, $[n]$,\nthe number of GB atoms, has emerged and is now recognized as an equally\nimportant component of GB structural multiplicity. In this work, we show that\nall GB microstates can be uniquely characterized by their value of\n$\\textbf{t}^{WS}$, which is located within the Wigner-Seitz (WS) cell of the\nDisplacement-Shift-Complete lattice (DSCL) of the GB. The GB translation vector\ncaptures information about both the translation state and the number of GB\natoms. We show that the density of GB microstates inside the cell of the DSCL\nis not uniform and can form clusters that correspond to different GB phases.\nThe vectors connecting the centers of the clusters correspond to the Burgers\nvectors of GB phase junctions, which can be predicted without building the\njunctions. Using $\\textbf{t}^{WS}$, we quantify GB excess shear and argue that\nit is defined up to a DSCL vector, which has implications for thermodynamic\nequilibrium conditions. Additionally, this work generalizes the definition of\nthe number of GB atoms $[n]$ to asymmetric boundaries.",
        "In this chapter we begin with a review of Titan's fluvial and lacustrine\nlandscapes as observed with Cassini remote sensing data, and what the many\ndiscoveries have revealed about Titan's surface materials and climate. Yet\nCassini remote sensing data are coarse, topographic data are largely lacking,\nand the absence of in situ field measurements means we have little\nunderstanding of what the surface is composed of. At present, our knowledge of\nTitan's hydrology is comparable to that of Mars in the 1970's during the Viking\nera. Fortunately, the coming decades promise many new and exciting discoveries\nthat can be achieved through Earth-based experiments, numerical modeling, and a\ncontinued commitment to the exploration of Titan by future missions, including\nboth Dragonfly and orbiting assets. We therefore close the chapter with a\ndiscussion about what can be done with the current Cassini data and how new\ndata, from both Dragonfly and a potential future orbiter, would allow us to\nleverage Titan to help solve some of the largest problems both here on Earth\nand on hydrologic planets and exoplanets more generally.",
        "For a non-decreasing positive integer sequence $S = (s_{1}, \\dots, s_{k})$,\nan $S$-packing edge-coloring of a graph $G$ is a partition of the edge set of\n$G$ into subsets $E_{1}, \\dots, E_{k}$ such that for each $1 \\leq i \\leq k$,\nthe distance between any two distinct edges $e_{1}, e_{2} \\in E_{i}$ is at\nleast $s_{i} + 1$. Hocquard et al. conjectured that cubic graphs, except for\nthe Petersen and Tietze graphs, admit $(1, 1, 1, 3)$-packing edge-colorings. In\nthis paper, we prove that every claw-free cubic graph admits such a coloring.",
        "In this paper, we investigate the geodesic motion and scalar perturbations of\nthe Ay\\'on-Beato-Garc\\'ia (ABG) black hole (BH) coupled with a cloud of strings\n(CS). By employing the effective potential approach, we analyze the\ntrajectories of massless and massive test particles around this regular BH\nsolution. The interplay between nonlinear electrodynamics (NLED) and CS\nsignificantly modifies the spacetime geometry, resulting in distinct dynamical\nproperties for test particles. The behavior of null and timelike geodesics,\nincluding their stability and escape conditions, is thoroughly examined. In\naddition to the geodesic analysis, we study scalar perturbations by deriving\nthe Klein-Gordon equation for massless scalar fields in this spacetime. The\nresulting Schr\\\"odinger-like wave equation reveals an effective potential that\ndepends intricately on the NLED and CS parameters. Furthermore, we compute the\ngreybody factors (GFs) to explore the energy transmission and absorption\nproperties of theBH. Our results demonstrate that the NLED parameter $g$ and\nthe CS parameter $\\alpha$ play pivotal roles in modulating the GFs, influencing\nthe energy spectrum of scalar radiation. These results provide significant\nunderstanding of the observational characteristics of the ABG NLED-CS BH in\nastrophysical contexts.",
        "We give a rigorous definition of sine dilaton gravity in terms of the\nworldsheet theory of the complex Liouville string arXiv:2409.17246. The latter\nhas a known exact solution that we leverage to explore the gravitational path\nintegral of sine dilaton gravity - a quantum deformation of dS JT gravity that\nadmits both AdS$_2$ and dS$_2$ vacua. We uncover that the gravitational path\nintegral receives contributions from new saddles describing transitions between\nvacua in a third-quantized picture. We also discuss the sphere and disk\npartition function in this context and contrast our findings with other recent\nwork on this theory.",
        "We calculate the spin density matrix for neutral $\\rho$ mesons from the\nspectral function and thermal shear tensor by Kubo formula in the linear\nresponse theory, which contributes to the $\\gamma$ correlator for the CME\nsearch. We derive the spectral function of neutral $\\rho$ mesons with\n$\\rho\\pi\\pi$ and $\\rho\\rho\\pi\\pi$ interactions using the Dyson-Schwinger\nequation. The thermal shear tensor contribution is obtained from the Kubo\nformula in the linear response theory. We numerically calculate $\\rho_{00}-1\/3$\nand $\\mathrm{Re}\\rho_{-1,1}$ using the simulation results for the thermal shear\ntensor by the hydrodynamical model, which are of the order\n$10^{-3}\\sim10^{-2}$.",
        "We present a comprehensive theoretical framework describing single pion\nresonant production through inelastic dark fermion-nucleon interactions\nmediated by resonances in the GeV-scale regime. Building upon the Rein-Sehgal\napproach, we derive differential cross sections for processes in which an\nincoming dark fermion scatters off a nucleon, exciting a resonance that\nsubsequently decays into a nucleon and a pion. Our formulation accommodates\nvarious mediator types-namely, dark photons with vector and axial couplings, as\nwell as scalar and pseudoscalar mediators-thereby extending the conventional\napproach that Rein, Sehgal and Berger performed for neutrino interactions.\nTransition amplitudes for the nucleon-to-resonance conversion are computed\nusing the relativistic harmonic-oscillator quark model from Feynman, Kislinger\nand Ravndal, while a Breit-Wigner prescription is employed to incorporate\nfinite resonance widths. This framework offers a robust tool for interpreting\nexperimental data in dark sector and matter searches and represents a\ncontribution to elucidate the role of resonances in GeV-scale phenomenology.",
        "This work reports a theoretical framework that combines the auxiliary\ncoherent potential approximation (ACPA-DMFT) with dynamical mean-field theory\nto study strongly correlated and disordered electronic systems with both\ndiagonal and off-diagonal disorders. In this method, by introducing an\nauxiliary coupling space with extended local degree of freedom,the diagonal and\noff-diagonal disorders are treated in a unified and self-consistent framework\nof coherent potential approximation, within which the dynamical mean-field\ntheory is naturally combined to handle the strongly correlated Anderson-Hubbard\nmodel. By using this approach, we compute matsubara Green's functions for a\nsimple cubic lattice at finite temperatures and derive impurity spectral\nfunctions through the maximum entropy method. Our results reveal the critical\ninfluence of off-diagonal disorder on Mott-type metal-insulator transitions.\nSpecifically, a reentrant phenomenon is identified, where the system\ntransitions between insulating and metallic states under varying interaction\nstrengths. The ACPA-DMFT method provides an efficient and robust computational\nmethod for exploring the intricate interplay of disorder and strong\ncorrelations.",
        "We construct a new model structure on the category of dg presheaves over a\ntopological space $X$, obtained through the right Bousfield localization of the\nlocal projective model structure. The motivation for this construction arises\nfrom the study of the homotopy theory underlying higher Riemann-Hilbert\ncorrespondence theorems, as developed by Chuang, Holstein, and Lazarev.\n  Let $X$ be a smooth manifold. We prove the existence of a zig-zag of Quillen\nequivalences between the category of dg modules over the de Rham algebra and\nthe category of dg presheaves of vector spaces over $X$. In the case where $X$\nis a complex manifold, we obtain an analogous result, where the de Rham algebra\nis replaced by the Dolbeault algebra. In both settings, we equip the categories\nof modules with model structures of the second kind, whose homotopy categories\nare, in general, finer invariants than those given by quasi-isomorphisms.\n  Finally, we introduce a singular analogue of this equivalence, stating it as\na zig-zag of Quillen equivalences between the category of dg contramodules over\nthe singular cochain algebra $C^{*}(X)$ and dg presheaves. At the level of\nhomotopy categories, this establishes an equivalence between the contraderived\ncategory of $C^{*}(X)$-contramodules and the homotopy category of dg\npresheaves.",
        "Magnetorotational instability (MRI) is of great importance in astrophysical\ndisks, driving angular momentum transport and accretion of matter onto a\ncentral object. A Taylor-Couette (TC) flow between two coaxial cylinders\nsubject to an axial magnetic field is a preferred setup for MRI-experiments. A\nmain challenge in those experiments has been to minimize the effects of axial\nboundaries, or endcaps, which substantially alter the flow structure compared\nto the axially unbounded idealized case. Understanding the influence of endcaps\non the flow stability is crucial for the unambiguous experimental\nidentification of MRI. In this paper, we examine the hydrodynamic evolution of\na TC flow in the presence of split endcap rims up to Reynolds number $Re =$\n$2\\times 10^5$. At this $Re$, the flow deviates from the ideal TC flow profile,\nresulting in about $15\\%$ deviation in angular velocity at the mid-height of\nthe cylinders. Aside from turbulent fluctuations caused by shearing instability\nat the endcaps, the bulk flow remains axially independent and exhibits Rayleigh\nstability. We characterize the scaling of the Ekman and Stewartson boundary\nlayer thickness with respect to $Re$. We also study the effect of changing the\nrotation ratio of the cylinders $\\mu$ on the flow at large $Re$ and show that\nTC experiments can be conducted for larger $\\mu \\sim 0.5$ to safely ensure the\nhydrodynamic stability of the flow in the upcoming DRESDYN-MRI experiment. In\nall configurations considered, the modification of the flow profile by the\nendcaps further decreases the required critical threshold for the onset of MRI\nthat can facilitate its detection in future experiments."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema",
    "start_abstract":"This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Residual connections provably mitigate oversmoothing in graph neural\n  networks",
        "Artificial intelligence for objective assessment of acrobatic movements:\n  How to apply machine learning for identifying tumbling elements in cheer\n  sports",
        "Cost-Efficient Continual Learning with Sufficient Exemplar Memory",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Dynamic Basis Function Generation for Network Revenue Management",
        "Sink-free orientations: a local sampler with applications",
        "Exploring coronal abundances of M dwarfs at moderate activity levels",
        "Counterdiabatic Driving with Performance Guarantees",
        "AI Explainability for Power Electronics: From a Lipschitz Continuity\n  Perspective",
        "Nuclear Spin Induced Transparency",
        "RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on\n  Deep Learning and Big Data Technology",
        "The Role of Planetary-Scale Waves on the Stratospheric Superrotation in\n  Titan's Atmosphere",
        "An examination of the extended Hong-Ou-Mandel effect and considerations\n  for experimental detection",
        "Supercritical phase transition on the Toeplitz algebra of $\\mathbb\n  N^\\times \\ltimes \\mathbb Z$",
        "Experimental Test of Nonlocality Limits from Relativistic Independence",
        "Golden Ratio Weighting Prevents Model Collapse",
        "Comment on \"Quantum coherence between mass eigenstates of a neutrino\n  cannot be destroyed by its mass-momentum entanglement\"",
        "Quantum interference with time-frequency modes and multiple-photons\n  generated by a silicon nitride microresonator",
        "UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance",
        "Investigation of physical and electrical properties of a suboxide layer\n  at Si\/Si-hexafluoride interface",
        "SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized\n  Pipeline for Automated Extraction in the Higher Education Science Domain",
        "Revisiting Robust RAG: Do We Still Need Complex Robust Training in the\n  Era of Powerful LLMs?",
        "Neutron relative effectiveness factors in Boron Neutron Capture Therapy:\n  estimation of their values from the secondary charged particles and\n  evaluation of weighted kerma factors for a standard tissue",
        "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages",
        "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
        "SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning",
        "Two-Timescale Channel Estimation for RIS-Aided Near-Field Communications",
        "Improving Value-based Process Verifier via Structural Prior Injection",
        "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation,\n  Negative Demonstration, and Adaptive Sampling"
      ],
      "abstract":[
        "Graph neural networks (GNNs) have achieved remarkable empirical success in\nprocessing and representing graph-structured data across various domains.\nHowever, a significant challenge known as \"oversmoothing\" persists, where\nvertex features become nearly indistinguishable in deep GNNs, severely\nrestricting their expressive power and practical utility. In this work, we\nanalyze the asymptotic oversmoothing rates of deep GNNs with and without\nresidual connections by deriving explicit convergence rates for a normalized\nvertex similarity measure. Our analytical framework is grounded in the\nmultiplicative ergodic theorem. Furthermore, we demonstrate that adding\nresidual connections effectively mitigates or prevents oversmoothing across\nseveral broad families of parameter distributions. The theoretical findings are\nstrongly supported by numerical experiments.",
        "Over the past four decades, cheerleading has evolved from a sideline activity\nat major sporting events into a professional, competitive sport with growing\nglobal popularity. Evaluating tumbling elements in cheerleading relies on both\nobjective measures and subjective judgments, such as difficulty and execution\nquality. However, the complexity of tumbling - encompassing team synchronicity,\nground interactions, choreography, and artistic expression - makes objective\nassessment challenging. Artificial intelligence (AI) has revolutionized various\nscientific fields and industries through precise data-driven analyses, yet\ntheir application in acrobatic sports remains limited despite significant\npotential for enhancing performance evaluation and coaching. This study\ninvestigates the feasibility of using an AI-based approach with data from a\nsingle inertial measurement unit to accurately identify and objectively assess\ntumbling elements in standard cheerleading routines. A sample of 16\nparticipants (13 females, 3 males) from a Division I collegiate cheerleading\nteam wore a single inertial measurement unit at the dorsal pelvis. Over a\n4-week seasonal preparation period, 1102 tumbling elements were recorded during\nregular practice sessions. Using triaxial accelerations and rotational speeds,\nvarious ML algorithms were employed to classify and evaluate the execution of\ntumbling manoeuvres. Results indicate that certain machine learning models can\neffectively identify different tumbling elements despite inter-individual\nvariability and data noise, achieving high accuracy. These findings demonstrate\nthe significant potential for integrating AI-driven assessments into\ncheerleading and other acrobatic sports, providing objective metrics that\ncomplement traditional judging methods.",
        "Continual learning (CL) research typically assumes highly constrained\nexemplar memory resources. However, in many real-world scenarios-especially in\nthe era of large foundation models-memory is abundant, while GPU computational\ncosts are the primary bottleneck. In this work, we investigate CL in a novel\nsetting where exemplar memory is ample (i.e., sufficient exemplar memory).\nUnlike prior methods designed for strict exemplar memory constraints, we\npropose a simple yet effective approach that directly operates in the model's\nweight space through a combination of weight resetting and averaging\ntechniques. Our method achieves state-of-the-art performance while reducing the\ncomputational cost to a quarter or third of existing methods. These findings\nchallenge conventional CL assumptions and provide a practical baseline for\ncomputationally efficient CL applications.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "This paper introduces an algorithm that dynamically generates basis functions\nto approximate the value function in Network Revenue Management. Unlike\nexisting algorithms sampling the parameters of new basis functions, this\nNonlinear Incremental Algorithm (NLIAlg) iteratively refines the value function\napproximation by optimizing these parameters. For larger instances, the\nTwo-Phase Incremental Algorithm (2PIAlg) modifies NLIAlg to leverage the\nefficiency of LP solvers. It reduces the size of a large-dimensional nonlinear\nproblem and transforms it into an LP by fixing the basis function parameters,\nwhich are then optimized in a second phase using the flow imbalance ideas from\nAdelman and Klabjan (2012). This marks the first application of these\ntechniques in a stochastic setting. The algorithms can operate in two modes:\n(1) Standalone mode, to construct a value function approximation from scratch,\nand (2) Add-on mode, to refine an existing approximation. Our numerical\nexperiments indicate that while NLIAlg and 2PIAlg in standalone mode are only\nfeasible for small-scale problems, the heuristic version of 2PIAlg (H-2PIAlg)\nin add-on mode, using the Affine Approximation and exponential ridge basis\nfunctions, can handle extremely large instances that may cause benchmark\nnetwork revenue management methods to run out of memory. In these scenarios,\nH-2PIAlg delivers substantially better policies and upper bounds than the\nAffine Approximation. Furthermore, H-2PIAlg achieves higher average revenues in\npolicy simulations compared to network revenue management benchmarks in\ninstances with limited capacity.",
        "For sink-free orientations in graphs of minimum degree at least $3$, we show\nthat there is a deterministic approximate counting algorithm that runs in time\n$O((n^{73}\/\\varepsilon^{72})\\log(n\/\\varepsilon))$, a near-linear time sampling\nalgorithm, and a randomised approximate counting algorithm that runs in time\n$O((n\/\\varepsilon)^2\\log(n\/\\varepsilon))$, where $n$ denotes the number of\nvertices of the input graph and $0<\\varepsilon<1$ is the desired accuracy. All\nthree algorithms are based on a local implementation of the sink popping method\n(Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling\nframework (Guo, Jerrum, and Liu, 2019).",
        "Main sequence stars of spectral types F, G, and K with low to moderate\nactivity levels exhibit a recognizable pattern known as the first ionization\npotential effect (FIP effect), where elements with lower first ionization\npotentials are more abundant in the stellar corona than in the photosphere. In\ncontrast, high activity main sequence stars such as AB Dor (K0), active\nbinaries, and M dwarfs exhibit an inverse pattern known as iFIP. We aim to\ndetermine whether or not the iFIP pattern persists in moderate-activity M\ndwarfs. We used XMM-Newton to observe the moderately active M dwarf HD 223889\nthat has an X-ray surface flux of log FX,surf = 5.26, the lowest for an M dwarf\nstudied so far for coronal abundance patterns. We used low-resolution CCD\nspectra of the star to calculate the strength of the FIP effect quantified by\nthe FIP bias (Fbias) to assess the persistence of the iFIP effect in M dwarfs.\nOur findings reveal an iFIP effect similar to that of another moderately active\nbinary star, GJ 338 AB, with a comparable error margin. The results hint at a\npossible plateau in the Teff-Fbias diagram for moderately active M dwarfs.\nTargeting stars with low coronal activity that have a coronal temperature\nbetween 2 MK and 4 MK is essential for refining our understanding of (i)FIP\npatterns and their causes.",
        "Counterdiabatic (CD) driving has the potential to speed up adiabatic quantum\nstate preparation by suppressing unwanted excitations. However, existing\napproaches either require intractable classical computations or are based on\napproximations which do not have performance guarantees. We propose and analyze\na non-variational, system-agnostic CD expansion method and analytically show\nthat it converges exponentially quickly in the expansion order. In finite\nsystems, the required resources scale inversely with the spectral gap, which we\nargue is asymptotically optimal. To extend our method to the thermodynamic\nlimit and suppress errors stemming from high-frequency transitions, we leverage\nfinite-time adiabatic protocols. In particular, we show that a time determined\nby the quantum speed limit is sufficient to prepare the desired ground state,\nwithout the need to optimize the adiabatic trajectory. Numerical tests of our\nmethod on the quantum Ising chain show that our method can outperform\nstate-of-the-art variational CD approaches.",
        "Lifecycle management of power converters continues to thrive with emerging\nartificial intelligence (AI) solutions, yet AI mathematical explainability\nremains unexplored in power electronics (PE) community. The lack of theoretical\nrigor challenges adoption in mission-critical applications. Therefore, this\nletter proposes a generic framework to evaluate mathematical explainability,\nhighlighting inference stability and training convergence from a Lipschitz\ncontinuity perspective. Inference stability governs consistent outputs under\ninput perturbations, essential for robust real-time control and fault\ndiagnosis. Training convergence guarantees stable learning dynamics,\nfacilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware\nlearning rate selection strategy is introduced to accelerate convergence while\nmitigating overshoots and oscillations. The feasibility of the proposed\nLipschitz-oriented framework is demonstrated by validating the mathematical\nexplainability of a state-of-the-art physics-in-architecture neural network,\nand substantiated through empirical case studies on dual-active-bridge\nconverters. This letter serves as a clarion call for the PE community to\nembrace mathematical explainability, heralding a transformative era of\ntrustworthy and explainable AI solutions that potentially redefine the future\nof power electronics.",
        "Electromagnetically induced transparency (EIT) is an important quantum\noptical phenomenon which provides a crucial tool for light manipulation.\nHowever, typically the transparency window is broad, limited by the coherence\ntime of the metastable state. Here we show that extremely narrow transparency\nwindow can be realized using nuclear spin induced transparency (NSIT), which is\nachieved by combining optical field, magnetic field and the spin-exchange\ninteraction between noble-gas nuclear spins and alkali-metal electronic spins.\nThe width of the NSIT window can be several orders of magnitude smaller than\nthat of conventional EIT, and even reaches sub-mHz range due to the long\ncoherence time of nuclear spins. The scheme holds great potential for\napplications in slow light and magnetic field sensing.",
        "Multi-object tracking (MOT) in UAV-based video is challenging due to\nvariations in viewpoint, low resolution, and the presence of small objects.\nWhile other research on MOT dedicated to aerial videos primarily focuses on the\nacademic aspect by developing sophisticated algorithms, there is a lack of\nattention to the practical aspect of these systems. In this paper, we propose a\nnovel real-time MOT framework that integrates Apache Kafka and Apache Spark for\nefficient and fault-tolerant video stream processing, along with\nstate-of-the-art deep learning models YOLOv8\/YOLOv10 and BYTETRACK\/BoTSORT for\naccurate object detection and tracking. Our work highlights the importance of\nnot only the advanced algorithms but also the integration of these methods with\nscalable and distributed systems. By leveraging these technologies, our system\nachieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set\nwhile maintaining a real-time processing speed of 28 FPS on a single GPU. Our\nwork demonstrates the potential of big data technologies and deep learning for\naddressing the challenges of MOT in UAV applications.",
        "We analyze simulation results from the TitanWRF global circulation model to\nunderstand the mechanisms that maintain the equatorial superrotation in Titan's\nstratosphere. We find that the eddies associated with wave activities can\ntransport angular momentum upgradient to zonal flow, leading to acceleration of\nthe equatorial superrotation. The dominant wave modes identified in this study\nare consistent with previous studies, with zonal wavenumber 1 being the major\ncontributor to the prograde acceleration. Despite the same conclusion of\nmaintenance of equatorial superrotation via wave-mean interactions, we find\nthat the way waves interact with the zonal flow in TitanWRF is slightly\ndifferent from some other studies. We confirm our previous findings that in\nTitanWRF this occurs primarily during a dozen or so annual, short-duration (a\nfew Titan sols) angular momentum \"transfer events,\" which have a repeatable\nseasonal pattern but differ slightly in timing and magnitude between years.\nThis is not the case in the Titan Atmosphere Model (TAM), which found milder\nangular momentum transfers that produced the strongest acceleration of\nsuperrotation around solstice in the upper stratosphere and more continuous\nyear-around acceleration in the lower stratosphere. Despite differences in\nangular momentum transfer across models, we further find that, similar to the\nTAM wave analysis results, eddies generated by Rossby-Kelvin instabilities may\nbe the major source of prograde angular momentum for the equatorial\nsuperrotation, although TitanWRF may also include contributions from the\nabsorption of vertically propagating equatorial Kelvin waves. This differs from\nour previous work, which suggested barotropic waves were responsible for\nTitanWRF's solsticial transfer event.",
        "In recent works we have explored a multi-photon extension of the celebrated\ntwo-photon Hong-Ou-Mandel (HOM) effect in which the quantum amplitudes for a\ntwo-photon input to a lossless, balanced 50:50 beamsplitter (BS) undergoes\ncomplete destructive interference. In the extended Hong-Ou-Mandel (eHOM) effect\nthe multi-photon scattering of photons from the two input ports to the two\noutput ports of the BS for Fock number basis input states (FS)\n$|n,m\\rangle_{12}$ exhibit complete destructive interference pairwise within\nthe quantum amplitudes containing many scattering components, generalizing the\ntwo-photon HOM effect. This has profound implications for arbitrary bipartite\nphotonic input states constructed from such basis states: if the input state to\none input port of the BS is of odd parity, i.e. constructed from only of odd\nnumbers of photons, then regardless of the input state to the second 50:50 BS\nport, there will be a central nodal line (CNL) of zeros in the joint output\nprobability distribution along the main diagonal for coincidence detection. The\nfirst goal of this present work is to show diagrammatically how the extended\nHOM effect can be seen as a succession of multi-photon HOM effects when the\nlatter is viewed as a pairwise cancellation of mirror image scattering\namplitudes. The second goal of this work is to explore considerations for the\nexperimental realization of the extended Hong-Ou-Mandel effect. We examine the\ncase of a single photon interfering with a coherent state (an idealized laser)\non a balanced 50:50 beamsplitter and consider prospects for experimental\ndetection of the output destructive interference by including additional\neffects such as imperfect detection efficiency, spatio-temporal mode functions,\nand time delay between the detected output photons.",
        "We study the high-temperature equilibrium for the C*-algebra $\\mathcal\nT(\\mathbb N^\\times \\ltimes \\mathbb Z)$ recently considered by an Huef, Laca and\nRaeburn. We show that the simplex of KMS$_\\beta$ states at each inverse\ntemperature $\\beta$ in the critical interval $(0,1]$ is a Bauer simplex whose\nspace of extreme points is homeomorphic to $\\mathbb N \\sqcup\\{\\infty\\}$. This\nis in contrast to the uniqueness of equilibrium at high temperature observed in\npreviously considered systems arising from number theory. We also establish a\nconnection between the phase transitions on quotients of our system and the\nBost-Connes phase transition.",
        "Quantum correlations, like entanglement, represent the characteristic trait\nof quantum mechanics, and pose essential issues and challenges to the\ninterpretation of this pillar of modern physics. Although quantum correlations\nare largely acknowledged as a major resource to achieve quantum advantage in\nmany tasks of quantum technologies, their full quantitative description and the\naxiomatic basis underlying them are still under investigation. Previous works\nsuggested that the origin of nonlocal correlations is grounded in principles\ncapturing (from outside the quantum formalism) the essence of quantum\nuncertainty. In particular, the recently-introduced principle of Relativistic\nIndependence gave rise to a new bound intertwining local and nonlocal\ncorrelations. Here we test such a bound by realizing together sequential and\njoint weak measurements on entangled photon pairs, allowing to simultaneously\nquantify both local and nonlocal correlations by measuring incompatible\nobservables on the same quantum system without collapsing its state, a task\ntypically forbidden in the traditional (projective) quantum measurement\nframework. Our results demonstrate the existence of a fundamental limit on the\nextent of quantum correlations, shedding light on the profound role of\nuncertainty in both enabling and balancing them.",
        "Recent studies identified an intriguing phenomenon in recursive generative\nmodel training known as model collapse, where models trained on data generated\nby previous models exhibit severe performance degradation. Addressing this\nissue and developing more effective training strategies have become central\nchallenges in generative model research. In this paper, we investigate this\nphenomenon theoretically within a novel framework, where generative models are\niteratively trained on a combination of newly collected real data and synthetic\ndata from the previous training step. To develop an optimal training strategy\nfor integrating real and synthetic data, we evaluate the performance of a\nweighted training scheme in various scenarios, including Gaussian distribution\nestimation and linear regression. We theoretically characterize the impact of\nthe mixing proportion and weighting scheme of synthetic data on the final\nmodel's performance. Our key finding is that, across different settings, the\noptimal weighting scheme under different proportions of synthetic data\nasymptotically follows a unified expression, revealing a fundamental trade-off\nbetween leveraging synthetic data and generative model performance. Notably, in\nsome cases, the optimal weight assigned to real data corresponds to the\nreciprocal of the golden ratio. Finally, we validate our theoretical results on\nextensive simulated datasets and a real tabular dataset.",
        "In arXiv:2410.21850, I proved that the quantum coherence between the mass\neigenstates of a neutrino will be destroyed if they are correlated with\ndifferent momenta. In arXiv:2411.01190, James M. Cline claimed that I had made\nthe unrealistic assumption that the neutrino is always in a nearly exact energy\neigenstate, and ignored the spatial dependence of the wavefunction in my paper.\nHowever, I did not assume that the neutrino is in a nearly exact eigenstate of\nenergy anywhere in my paper, and the wavefunction I wrote in the position\nrepresentation has a spatial dependence. The argumentation of arXiv:2411.01190\nis based on misinterpreting my claim, and on ignoring the critical fact that\nthe neutrino's wavepacket has a finite size and the detector has a large\nvolume.",
        "We demonstrate bipartite gaussian boson sampling with squeezed light in 6\nmixed time-frequency modes. Non-degenerate two-mode squeezing is generated in\ntwo time-bins from a silicon nitride microresonator with simultaneous high\nspectral purity (>0.86(3)) and indistinguishability (0.985(2)). An unbalanced\ninterferometer embedding electro-optic modulators, which is stabilized by\nexploiting the continuous energy-time entanglement of the generated photon\npairs, controls time and frequency-bin modes. We measure 144 collision-free\nevents with 4 photons at the output, achieving a fidelity >0.98 with the\ntheoretical probability distribution. We use this result to identify the\nsimilarity between families of isomorphic graphs with 6 vertices, and present\nan approach for the realization of universal operations on time-frequency\nmodes.",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet code generation remains a major challenge. Current\napproaches for obtaining high-quality code data primarily focus on (i)\ncollecting large-scale pre-training data and (ii) synthesizing instruction data\nthrough prompt engineering with powerful models. While pre-training data faces\nquality consistency issues, instruction-based synthesis suffers from limited\ninstruction diversity and inherent biases of LLMs. To address this gap, we\nintroduce UnitCoder, a systematic pipeline leveraging model-generated unit\ntests to both guide and validate the code generation process. Combined with\nlarge-scale package-based retrieval from pre-training corpus, we generate a\ndataset of 500K+ verifiable programs containing diverse API calls. Evaluations\non multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that\nmodels fine-tuned on our synthetic data exhibit consistent performance\nimprovements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and\n28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work\npresents a scalable approach that leverages model-generated unit tests to guide\nthe synthesis of high-quality code data from pre-training corpora,\ndemonstrating the potential for producing diverse and high-quality\npost-training data at scale. All code and data will be released\n(https:\/\/github.com).",
        "The silicon suboxide SiOx (x<2.0) offers promising industrial application\npossibilities ranging from electrodes in lithium-ion batteries, which are used\nwidely in electrical vehicles and portable devices to sensing applications.\nTherefore, a low cost, environmental friendly and high performance silicon\noxide materials are required for an appropriate operation of any electronic\ngadget. In this work, we report on the physical and electrical properties of a\nsuboxide layer of up to 1 {\\mu}m, which was grown on silicon during the\nformation of a dielectric layer, namely the ammonium silicon hexafluoride. It\nis a stable oxide exhibiting light emission from 400 nm to 1700 nm offering\nscalable and cost-effective large area processing capability. The measurement\nresults reveal interesting properties, which are required to be understood\nclearly before proceeding with any suitable application. The results have been\nanalyzed using state-of-the-art physical and electrical characterization\ntechniques such as ellipsometry, AFM, SEM, FTIR, photoluminescence lifetime and\nresistive switching measurements to determine structural, optical and\nelectrical properties. At 300 K the carrier lifetime measurements reveal the\nlifetime values ranging from about few tens of picosecond up to 4500\npicoseconds. Scanning probe analysis indicate a surface roughness of about 30\nAngstr\\\"om. Resistive memory forming was observed also in these layers at\nrelatively low power thresholds. We provide a comprehensive description of the\nphysical and electrical properties in order to clarify the origin of the\nobserved features. The wavelength dependent real {\\epsilon}_1 ({\\omega}) and\nthe imaginary {\\epsilon}_2 ({\\omega}) dielectric functions provided useful\ninsights on optical properties. A lookout is given for the possible\napplications of this special SiOx dielectric oxide layer.",
        "Recent breakthroughs in large language models (LLMs) exemplified by the\nimpressive mathematical and scientific reasoning capabilities of the o1 model\nhave spotlighted the critical importance of high-quality training data in\nadvancing LLM performance across STEM disciplines. While the mathematics\ncommunity has benefited from a growing body of curated datasets, the scientific\ndomain at the higher education level has long suffered from a scarcity of\ncomparable resources. To address this gap, we present SCP-116K, a new\nlarge-scale dataset of 116,756 high-quality problem-solution pairs,\nautomatically extracted from heterogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach involves stringent filtering to\nensure the scientific rigor and educational level of the extracted materials,\nwhile maintaining adaptability for future expansions or domain transfers. By\nopenly releasing both the dataset and the extraction pipeline, we seek to\nfoster research on scientific reasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier to replicating the successes of\nadvanced models like o1 in the broader science community. We believe SCP-116K\nwill serve as a critical resource, catalyzing progress in high-level scientific\nreasoning tasks and promoting further innovations in LLM development. The\ndataset and code are publicly available at\nhttps:\/\/github.com\/AQA6666\/SCP-116K-open.",
        "Retrieval-augmented generation (RAG) systems often suffer from performance\ndegradation when encountering noisy or irrelevant documents, driving\nresearchers to develop sophisticated training strategies to enhance their\nrobustness against such retrieval noise. However, as large language models\n(LLMs) continue to advance, the necessity of these complex training methods is\nincreasingly questioned. In this paper, we systematically investigate whether\ncomplex robust training strategies remain necessary as model capacity grows.\nThrough comprehensive experiments spanning multiple model architectures and\nparameter scales, we evaluate various document selection methods and\nadversarial training techniques across diverse datasets. Our extensive\nexperiments consistently demonstrate that as models become more powerful, the\nperformance gains brought by complex robust training methods drop off\ndramatically. We delve into the rationale and find that more powerful models\ninherently exhibit superior confidence calibration, better generalization\nacross datasets (even when trained with randomly selected documents), and\noptimal attention mechanisms learned with simpler strategies. Our findings\nsuggest that RAG systems can benefit from simpler architectures and training\nstrategies as models become more powerful, enabling more scalable applications\nwith minimal complexity.",
        "The average Relative Biological effectiveness (RBE) factors for neutron\nirradiation in the context of a BNCT treatment are studied. This research\nconsiders the various interactions and secondary particles of each process and\nestimates the RBE based on the damage induced in tissues by all of these\nparticles. A novel concept of estimating the biological dose by means of\nweighted kerma factors is introduced. These weighted kerma factors include the\nRBE of each energy deposition based on an RBE-LET relationship for secondary\ncharged particles and can be directly incorporated in weighted dose\ncalculations from Monte Carlo simulations. Furthermore, the dependence of the\nneutron weighting factor on neutron energy for standard soft tissue is\ndiscussed.",
        "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps:\/\/github.com\/AfriHate\/AfriHate",
        "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps:\/\/quanhaol.github.io\/magicmotion-site.",
        "Federated learning (FL) enables collaborative model training while preserving\ndata privacy, but its decentralized nature exposes it to client-side data\npoisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global\nmodel performance. While numerous proposed defenses claim substantial\neffectiveness, their evaluation is typically done in isolation with limited\nattack strategies, raising concerns about their validity. Additionally,\nexisting studies overlook the mutual effectiveness of defenses against both\nDPAs and MPAs, causing fragmentation in this field. This paper aims to provide\na unified benchmark and analysis of defenses against DPAs and MPAs, clarifying\nthe distinction between these two similar but slightly distinct domains. We\npresent a systematic taxonomy of poisoning attacks and defense strategies,\noutlining their design, strengths, and limitations. Then, a unified comparative\nevaluation across FL algorithms and data heterogeneity is conducted to validate\ntheir individual and mutual effectiveness and derive key insights for design\nprinciples and future research. Along with the analysis, we frame our work to a\nunified benchmark, FLPoison, with high modularity and scalability to evaluate\n15 representative poisoning attacks and 17 defense strategies, facilitating\nfuture research in this domain. Code is available at\nhttps:\/\/github.com\/vio1etus\/FLPoison.",
        "In this paper, we investigate the channel estimation problem in\nreconfigurable intelligent surface (RIS)-aided near-field communication\nsystems, where the extremely large number of RIS elements imposes considerable\npilot overhead and computational complexity. To address this, we employ a\ntwo-timescale channel estimation strategy that exploits the asymmetric\ncoherence times of the RIS-base station (BS) channel and the User-RIS channel.\nWe derive a time-scaling property indicating that for any two effective\nchannels within the longer coherence time, one effective channel can be\nrepresented as the product of a vector, referred to as the small-timescale\neffective channel, and the other effective channel. By utilizing the estimated\neffective channel along with processed observations from our piecewise beam\ntraining, we present an efficient method for estimating subsequent\nsmall-timescale effective channels. We theoretically establish the efficacy of\nthe proposed RIS design and demonstrate, through simulations, the superiority\nof our channel estimation method in terms of pilot overhead and computational\ncomplexity compared to existing methods across various realistic channel\nmodels.",
        "In the Large Language Model(LLM) reasoning scenario, people often estimate\nstate value via Monte Carlo sampling. Though Monte Carlo estimation is an\nelegant method with less inductive bias, noise and errors are inevitably\nintroduced due to the limited sampling. To handle the problem, we inject the\nstructural prior into the value representation and transfer the scalar value\ninto the expectation of a pre-defined categorical distribution, representing\nthe noise and errors from a distribution perspective. Specifically, by treating\nthe result of Monte Carlo sampling as a single sample from the prior\nground-truth Binomial distribution, we quantify the sampling error as the\nmismatch between posterior estimated distribution and ground-truth\ndistribution, which is thus optimized via distribution selection optimization.\nWe test the performance of value-based process verifiers on Best-of-N task and\nBeam search task. Compared with the scalar value representation, we show that\nreasonable structural prior injection induced by different objective functions\nor optimization methods can improve the performance of value-based process\nverifiers for about 1$\\sim$2 points at little-to-no cost. We also show that\nunder different structural prior, the verifiers' performances vary greatly\ndespite having the same optimal solution, indicating the importance of\nreasonable structural prior injection.",
        "Many-shot jailbreaking circumvents the safety alignment of large language\nmodels by exploiting their ability to process long input sequences. To achieve\nthis, the malicious target prompt is prefixed with hundreds of fabricated\nconversational turns between the user and the model. These fabricated exchanges\nare randomly sampled from a pool of malicious questions and responses, making\nit appear as though the model has already complied with harmful instructions.\nIn this paper, we present PANDAS: a hybrid technique that improves many-shot\njailbreaking by modifying these fabricated dialogues with positive\naffirmations, negative demonstrations, and an optimized adaptive sampling\nmethod tailored to the target prompt's topic. Extensive experiments on AdvBench\nand HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS\nsignificantly outperforms baseline methods in long-context scenarios. Through\nan attention analysis, we provide insights on how long-context vulnerabilities\nare exploited and show how PANDAS further improves upon many-shot jailbreaking."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
      ],
      "abstract":[
        "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Divisibility Relations Between Ring Homomorphisms and Surjective Group\n  Homomorphisms in Finite Cyclic Structures",
        "Moduli spaces of twisted maps to smooth pairs",
        "Parallel Collisionless Shocks in strongly Magnetized Electron-Ion\n  Plasma. I. Temperature anisotropies",
        "Probing Coherences and Itinerant Magnetism in a Dipolar Lattice Gas",
        "$NJ\/\\psi$ and $N\\eta_c$ interactions from lattice QCD",
        "The Ridge Integration Method and its Application to Molecular Sieving,\n  Demonstrated for Gas Purification via Graphdiyne Membranes",
        "Overcoming Quantum Metrology Singularity through Sequential Measurements",
        "Late-time growth weakly affects the significance of high-redshift\n  massive galaxies",
        "Weierstrass representations of discrete constant mean curvature surfaces\n  in isotropic space",
        "Online Optimization with Unknown Time-varying Parameters",
        "Exactness and the topology of the space of invariant random equivalence\n  relations",
        "Cusps and fundamental domains for congruence subgroups",
        "$H^\\infty$-control for a class of boundary controlled hyperbolic PDEs",
        "Accelerating Expansion of the Universe in Modified Symmetric\n  Teleparallel Gravity",
        "Neutron versus proton scattering on exotic nuclei: the $^9$He example",
        "Higgs near-criticality at future colliders",
        "A new mass estimate method with hydrodynamical atmospheres for very\n  massive WNh stars",
        "Recent Topics on Linear Dynamics",
        "Theory of Two-level Tunneling Systems in Superconductors",
        "Reinforced Galton--Watson processes III: Empirical offspring\n  distributions",
        "On Dirac and Motzkin problem in discrete geometry",
        "On Malliavin differentiability and absolute continuity of\n  one-dimensional doubly perturbed diffusion processes",
        "Online Assortment and Price Optimization Under Contextual Choice Models",
        "Empirical risk minimization algorithm for multiclass classification of\n  S.D.E. paths",
        "Bosonisation and BTZ Black Hole Microstates",
        "Response of liquid metal in a fusion reactor blanket to rapid variation\n  of magnetic field during a transient plasma event",
        "A practical review on promoting connectivity in topology optimization",
        "Learning Energy-Based Models by Self-normalising the Likelihood",
        "From Weyl Anomaly to Defect Supersymmetric R\\'enyi Entropy and Casimir\n  Energy"
      ],
      "abstract":[
        "In this article, we delve into the intricate relationship between the number\nof ring homomorphisms and surjective group homomorphisms between two finite\ncyclic structures, specifically $\\mathbb{Z}_m$ and $\\mathbb{Z}_n$. We\ndemonstrate that the number of ring homomorphisms from $\\mathbb{Z}_m$ to\n$\\mathbb{Z}_n$ is a divisor of the number of surjective group homomorphisms\nfrom $\\mathbb{Z}_m$ to $\\mathbb{Z}_n$, provided that $n$ is not of the form $2\n\\cdot \\alpha$, where each prime factor $p$ of $\\alpha$ satisfies $p \\equiv 3\n\\pmod{4}$.",
        "We study moduli spaces of twisted maps to a smooth pair in arbitrary genus,\nand give geometric explanations for previously known comparisons between\norbifold and logarithmic Gromov--Witten invariants. Namely, we study the space\nof twisted maps to the universal target and classify its irreducible components\nin terms of combinatorial\/tropical information. We also introduce natural\nmorphisms between these moduli spaces for different rooting parameters and\ncompute their degree on various strata. Combining this with additional\nhypotheses on the discrete data, we show these degrees are monomial of degree\nbetween $0$ and $\\max(0,2g-1)$ in the rooting parameter. We discuss the virtual\ntheory of the moduli spaces, and relate our polynomiality results to work of\nTseng and You on the higher genus orbifold Gromov--Witten invariants of smooth\npairs, recovering their results in genus $1$. We discuss what is needed to\ndeduce arbitrary genus comparison results using the previous sections. We\nconclude with some geometric examples, starting by re-framing the original\ngenus $1$ example of Maulik in this new formalism.",
        "Collisionless electron-ion shocks are fundamental to astrophysical plasmas,\nyet their behavior in strong magnetic fields remains poorly understood. Using\nParticle-in-Cell (PIC) simulations with the SHARP-1D3V code, we investigate the\nrole of the ion magnetization parameter $\\sigma_i$ in parallel shock\ntransitions. Strongly magnetized converging flows ($\\sigma_i > 1$) exhibit\nlower density compression ratios ($R \\sim 2$), smaller entropy jumps, and\nsuppressed particle acceleration, while maintaining pressure anisotropy\nstability due to conserved perpendicular temperatures across the shock,\nalongside increased parallel temperatures. In contrast, weakly magnetized\nshocks drive downstream mirror and firehose instabilities due to ion\ntemperature anisotropy, which are suppressed in strongly magnetized cases.\nAdditionally, weakly magnetized shocks exhibit the onset of a supra-thermal\npopulation induced by shock-drift acceleration, with most of the upstream\nkinetic energy thermalized for both electrons and ions in the downstream\nregion. Our results demonstrate that perpendicular temperatures for both\nspecies are conserved in strongly magnetized cases and highlight deviations\nfrom standard ideal magnetohydrodynamic (MHD) behavior. These findings provide\ncritical insights into the role of magnetic fields in parallel collisionless\nastrophysical shocks.",
        "We report on the study of itinerant magnetism of lattice-trapped magnetic\natoms, driven by magnetic dipole-dipole interactions, in the low-entropy and\nclose-to-unit filling regime. We have used advanced dynamical decoupling\ntechniques to efficiently suppress the sensitivity to magnetic field\nfluctuations. We have thus measured the spin coherence of an itinerant spin 3\nBose dipolar gas throughout a quantum phase transition from a superfluid phase\nto a Mott insulating phase. In the superfluid phase, a metastable ferromagnetic\nbehavior is observed below a dynamical instability which occurs at lattice\ndepths below the phase transition. In the insulating phase, the thermalization\ntowards a paramagnetic state is driven by an interplay between intersite and\nsuperexchange interactions.",
        "The interaction between nucleon and charmonia ($J\/\\psi$ and $\\eta_c$) is\nexpected to deepen our understanding of various aspects in nonperturbative QCD\nranging from the origin of nucleon mass to $J\/\\psi$ mass modification in\nnuclear medium and properties of hidden-charm pentaquark states. Here, we\npresent the low-energy $NJ\/\\psi$ and $N\\eta_c$ interactions based on ($2+1$)\nflavor lattice QCD simulations with nearly physical pion mass $m_\\pi=146$ MeV.\nThe interactions, extracted from the spacetime correlations of the nucleon and\ncharmonium system by using the HAL QCD method, are found to be attractive in\nall distances and manifest a characteristic long-range tail consistent with the\ntwo-pion exchange interaction. The resulting scattering lengths are around\n$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ\/\\psi$ with spin $3\/2$, with spin $1\/2$,\nand $N\\eta_c$, respectively. Our results are orders of magnitude larger than\nthose from the photoproduction experiments assuming the vector meson dominance.",
        "Eyring theory provides a convenient approximation to the rate of a chemical\nreaction, as it uses only local information evaluated near extremal points of a\ngiven potential energy surface. However, in cases of pronounced anharmonicity\nand particularly low-lying vibrational frequencies, deviations from the correct\nreaction rate can become substantial. Molecular Dynamics simulations, on the\nother hand, are very costly at higher levels of theory, and of limited use\nsince molecular reactions are `rare' events and hence statistically less\naccessible. In this article, we present an alternative description for problems\nof gas separation and storage via two-dimensional materials such as porous\ngraphene or flat metal-organic frameworks. Taking geometric advantage of the\ntypical problem setting, our method is based on a statistical analysis of\nmolecular trajectories near the so-called `ridge', a hypersurface which divides\nthe reaction volume into a reactant and a product side. It allows for more\nrealistic predictions of permeabilities and selectivities, e.g. derived from\ndensity functional theory, but without the considerable costs of a full\nmolecular dynamics simulation on the corresponding Born-Oppenheimer potential\nenergy surface. We test our method on the example of methane separation from\nnitrogen and carbon dioxide via a graphdiyne membrane.",
        "The simultaneous estimation of multiple unknown parameters is the most\ngeneral scenario in quantum sensing. Quantum multi-parameter estimation theory\nprovides fundamental bounds on the achievable precision of simultaneous\nestimation. However, these bounds can become singular (no finite bound exists)\nin multi-parameter sensing due to parameter interdependencies, limited probe\naccessibility, and insufficient measurement outcomes. Here, we address the\nsingularity issue in quantum sensing through a simple mechanism based on a\nsequential measurement strategy. This sensing scheme overcomes the singularity\nconstraint and enables the simultaneous estimation of multiple parameters with\na local and fixed measurement throughout the sensing protocol. This is because\nsequential measurements, involving consecutive steps of local measurements\nfollowed by probe evolution, inherently produce correlated measurement data\nthat grows exponentially with the number of sequential measurements. Finally,\nthrough two different examples, namely a strongly correlated probe and a\nlight-matter system, we demonstrate how such singularities are reflected when\ninferring the unknown parameters through Bayesian estimation.",
        "Recent observations by the James Webb Space Telescope have revealed massive\ngalaxies at very high redshift ($z\\simeq 7-15$). The question of whether the\nexistence of such galaxies is expected in the corresponding JWST surveys has\nreceived a lot of attention, though the answer straddles areas of cosmology and\ncomplex astrophysical details of high-redshift galaxy formation. The growth\nrate of density fluctuations determines the amplitude of overdensities that\ncollapse to form galaxies. Late-time modifications of growth, combined with\nmeasurements at both $z\\sim 1$ from large-scale structure and $z\\sim 1000$ from\nthe cosmic microwave background, affect the predictions for the abundance of\nfirst galaxies in the universe. In this paper, we point out that the late-time\ngrowth rate of structure affects the statistical significance of high-redshift,\nhigh-mass objects very weakly. Consequently, if the existence and abundance of\nthese objects are confirmed to be unexpected, the variations in the late-time\ngrowth history are unlikely to explain these anomalies.",
        "In this paper, we obtain Weierstrass representations for discrete constant\nmean curvature surfaces in isotropic 3-space, and use this to construct\nexamples with discrete closed-form parametrizations.",
        "In this paper, we study optimization problems where the cost function\ncontains time-varying parameters that are unmeasurable and evolve according to\nlinear, yet unknown, dynamics. We propose a solution that leverages control\ntheoretic tools to identify the dynamics of the parameters, predict their\nevolution, and ultimately compute a solution to the optimization problem. The\nidentification of the dynamics of the time-varying parameters is done online\nusing measurements of the gradient of the cost function. This system\nidentification problem is not standard, since the output matrix is known and\nthe dynamics of the parameters must be estimated in the original coordinates\nwithout similarity transformations. Interestingly, our analysis shows that,\nunder mild conditions that we characterize, the identification of the\nparameters dynamics and, consequently, the computation of a time-varying\nsolution to the optimization problem, requires only a finite number of\nmeasurements of the gradient of the cost function. We illustrate the\neffectiveness of our algorithm on a series of numerical examples.",
        "We characterize exactness of a countable group $\\Gamma$ in terms of invariant\nrandom equivalence relations (IREs) on $\\Gamma$. Specifically, we show that\n$\\Gamma$ is exact if and only if every weak limit of finite IREs is an amenable\nIRE. In particular, for exact groups this implies amenability of the restricted\nrerooting relation associated to the ideal Bernoulli Voronoi tessellation, the\ndiscrete analog of the ideal Poisson Voronoi tesselation.",
        "We characterize the cusp classes and their widths for the congruence\nsubgroups $\\Gamma(N), \\Gamma_1(N)$ and $\\Gamma_0(N)$. We relate the cusp\nclasses of $\\Gamma_0(N)$ with those produced by the connected fundamental\ndomain in the previous work of Nie and Parent. By further studying the\ninteresting functions $M$ and $W$ on ${\\mathbb Z}\/N$, we establish an identity\nrelating the widths.",
        "A solution to the suboptimal $H^\\infty$-control problem is given for a class\nof hyperbolic partial differential equations (PDEs). The first result of this\nmanuscript shows that the considered class of PDEs admits an equivalent\nrepresentation as an infinite-dimensional discrete-time system. Taking\nadvantage of this, this manuscript shows that it is equivalent to solve the\nsuboptimal $H^\\infty$-control problem for a finite-dimensional discrete-time\nsystem whose matrices are derived from the PDEs. After computing the solution\nto this much simpler problem, the solution to the original problem can be\ndeduced easily. In particular, the optimal compensator solution to the\nsuboptimal $H^\\infty$-control problem is governed by a set of hyperbolic PDEs,\nactuated and observed at the boundary. We illustrate our results with a\nboundary controlled and boundary observed vibrating string.",
        "In the last century, theoretical and experimental developments have\nestablished the General Relativity theory as the most successful theory for\ndescribing the gravitational phenomenon. On the other hand, in the last two\ndecades, multiple observational probes have strongly favored the discovery of\nthe acceleration of cosmic expansion. The observational enhancement and\ndevelopment in precision cosmology indicate a requirement to go beyond General\nRelativity and to search for an alternate description that can resolve the\npersistent issues. In Chapter 1, we highlight some important elements of\nobservational cosmology. In Chapters 2 and 3, we investigate the f(Q) gravity\nin the presence of viscosity in the cosmic fluid. In Chapters 4 and 5, we\nexplore the constraints on the various classes of non-linear f(Q) gravity\nmodels in both coincident and non-coincident formalism, respectively. In\nChapter 6, we present a covariant formulation and energy balance equation for\nthe f(Q,T) gravity, which is an extension of f(Q) gravity. Finally, in Chapter\n7, we briefly summarize the outcomes of the present thesis and the future\nscope.",
        "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5\/2$ and $T=3\/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5\/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5\/2$ resonances may be strongly affected by the\nisobaric-partner $T=3\/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated.",
        "The so-called metastability bound on the Higgs mass suggests that the\nsmallness of the Higgs mass may be a byproduct of the metastability of the\nelectroweak vacuum. A significantly strong bound requires new physics capable\nof lowering the scale where the Higgs quartic coupling turns negative through\nrenormalization group effects, without destabilizing the electroweak vacuum\nentirely. We analyze in this context the low-scale Majoron model of neutrino\nmasses, which automatically contains two key elements for a viable scenario:\nheavy fermions to lower the instability scale and a extended scalar sector to\nstabilize the potential and achieve realistic lifetimes for the electroweak\nvacuum. We show how the metastability bound can be generalized to theories with\nmultiple scalars and present an efficient way of calculating the tunneling rate\nin such potentials. We also demonstrate that FCC will probe regions of the\nparameter space relevant for metastability: large regions of the fermionic\nsector at FCC-ee and some reach to the scalar sector at FCC-hh.",
        "Very massive stars with masses over 100 Msun are key objects in the Universe\nfor our understanding of chemical and energetic feedback in the Universe, but\ntheir evolution and fate are almost entirely determined by their wind mass\nloss. We aim to determine the mass of the most massive star known in the Local\nGroup R136a1. For this we compute the first hydrodynamically consistent\nnon-local thermodynamical equilibrium atmosphere models for both R136a1 (WN5h)\nas well as the binary system R144 (WN5\/6h+WN6\/7h) in the Tarantula nebula.\nUsing the Potsdam Wolf-Rayet code, we simultaneously empirically derive and\ntheoretically predict mass-loss rates and wind velocities. By fitting synthetic\nspectra derived from these models to multi-wavelength observations, we\nconstrain the stellar and wind properties of R144 and R136a1. We first\ndetermine the clumping stratification required by our hydro-models to fit the\nspectra of R144 by using the available dynamical mass estimates for the two\ncomponents. We then utilise this clumping stratification in hydrodynamic models\nof R136a1 and estimate a mass of $M_\\mathrm{Hydro}$ of 233 Msun. Remarkably,\nthe estimated mass is close to and entirely consistent with chemical\nhomogeneous mass relations. This present-day mass of 233 Msun provides a lower\nlimit to the initial stellar mass, that could be far higher due to previous\nwind mass loss.",
        "Notes from a course on linear dynamics given by the author at the University\nof Da Nang in January 2024.",
        "We develop a field theory formulation for the interaction of an ensemble of\ntwo-level tunneling systems (TLS) with the electronic states of a\nsuperconductor. Predictions for the impact of two-level tunneling systems on\nsuperconductivity are presented, including $T_c$ and spectrum of quasiparticle\nstates for conventional BCS superconductors. We show that non-magnetic TLS\nimpurities in conventional s-wave superconductors can act as pair-breaking or\npair-enhancing defects depending on the level population of the distribution of\nTLS impurities. We present calculations of the enhancement of\nsuperconductivity, both $T_c$ and the order parameter, for TLS defects in\nthermal equilibrium with the electrons and lattice. The scattering of\nquasiparticles by TLS impurities leads to sub-gap states below the bulk\nexcitation gap, $\\Delta$, as well as resonances in the continuum above\n$\\Delta$. The energies and spectral weights of these states depend on the\ndistribution of tunnel splittings, while the spectral weights are particularly\nsensitive to the level occupation of the TLS impurities. Under microwave\nexcitation, or decoupling from the thermal bath, a nonequilibrium level\npopulation of the TLS distribution generates subgap quasiparticle states near\nthe Fermi level which contribute to dissipation and thus degrade the\nperformance of superconducting devices at low temperatures.",
        "Reinforced Galton--Watson processes describe the dynamics of a population\nwhere reproduction events are reinforced, in the sense that offspring numbers\nof forebears can be repeated randomly by descendants. More specifically, the\nevolution depends on the empirical offspring distribution of each individual\nalong its ancestral lineage. We are interested here in asymptotic properties of\nthe empirical distributions observed in the population, such as concentration,\nevanescence and persistence. For this, we incorporate tools from the theory of\nlarge deviations to our preceding analysis [arXiv:2306.02476,arXiv:2310.19030].",
        "Dirac and Motzkin conjectured that any set X of $n$ non-collinear points in\nthe plane has an element incident with at least $\\lceil \\frac{n}{2} \\rceil$\nlines spanned by X. In this paper we prove that any set X of $n$ non-collinear\npoints in the plane, distributed on three lines passing through a common point,\nhas an element incident with at least $\\lceil \\frac{n}{2} \\rceil$ lines spanned\nby X.",
        "In this paper, we establish Malliavin differentiability and absolute\ncontinuity for $\\alpha, \\beta$-doubly perturbed diffusion process with\nparameters $\\alpha <1$ and $\\beta <1$ such that $|\\rho| < 1$, where $ \\rho : =\n\\frac{\\alpha\\beta}{(1-\\alpha)(1-\\beta)}$. Furthermore, under some regularity\nconditions on the coefficients, we prove that the solution $X_t$ has a smooth\ndensity for all $t\\in(0, t_0)$ for some finite number $t_0>0$. Our results\nrecover earlier works by Yue and Zhang (2015) and Xue, Yue and Zhang (2016),\nand the proofs are based on the techniques of the Malliavin calculus.",
        "We consider an assortment selection and pricing problem in which a seller has\n$N$ different items available for sale. In each round, the seller observes a\n$d$-dimensional contextual preference information vector for the user, and\noffers to the user an assortment of $K$ items at prices chosen by the seller.\nThe user selects at most one of the products from the offered assortment\naccording to a multinomial logit choice model whose parameters are unknown. The\nseller observes which, if any, item is chosen at the end of each round, with\nthe goal of maximizing cumulative revenue over a selling horizon of length $T$.\nFor this problem, we propose an algorithm that learns from user feedback and\nachieves a revenue regret of order $\\widetilde{O}(d \\sqrt{K T} \/ L_0 )$ where\n$L_0$ is the minimum price sensitivity parameter. We also obtain a lower bound\nof order $\\Omega(d \\sqrt{T}\/ L_0)$ for the regret achievable by any algorithm.",
        "We address the multiclass classification problem for stochastic diffusion\npaths, assuming that the classes are distinguished by their drift functions,\nwhile the diffusion coefficient remains common across all classes. In this\nsetting, we propose a classification algorithm that relies on the minimization\nof the L 2 risk. We establish rates of convergence for the resulting predictor.\nNotably, we introduce a margin assumption under which we show that our\nprocedure can achieve fast rates of convergence. Finally, a simulation study\nhighlights the numerical performance of our classification algorithm.",
        "When the boundary dynamics of \\(AdS_3\\) gravity is governed by the collective\nfield theory Hamiltonian proposed by Jevicki and Sakita, its asymptotic\nsymmetry algebra becomes the centerless \\(U(1)\\) Kac-Moody algebra. We quantize\nthis system using the quantum bosonization of relativistic free fermions and\nrelate these to the dynamical fields of \\(AdS_3\\) gravity. This leads to a\ncorrespondence where different bulk configurations correspond to distinct\nstates (particle-hole pair excitations) in the fermionic Hilbert space. This\nmapping allows us to construct BTZ black hole microstates, represented by Young\ndiagrams of irreducible \\(U(\\infty)\\) representations. Notably, the logarithm\nof the microstate degeneracy exactly reproduces the classical entropy of the\nBTZ black hole.",
        "Transient plasma events, such as plasma disruptions, are anticipated in the\nfuture magnetic-confinement nuclear fusion reactors. The events are accompanied\nby a rapid change in the magnetic field generated by the plasma current and,\naccordingly, induction of strong eddy currents and Lorentz forces within the\nreactor structure. This work targets processes within liquid-metal components\nof the reactor's breeding blankets. Order-of-magnitude analysis and numerical\nsimulations are performed to understand the response of liquid metal to a\nrapidly changing magnetic field and to evaluate the accuracy of commonly used\nsimplifying model assumptions. The response is found to consist of two stages:\nan initial brief stage ($\\sim 1$ ms) characterized by a rapid increase in the\ninduced currents, forces, and fluid velocity; and a subsequent stage, which is\ntriggered by the growing velocity of the metal and marked by reversals of\nLorentz force, and oscillations and decreases in the amplitude of the induced\nfields. The transition to the second stage sets the upper limit of the velocity\n($\\sim 0.5$ m\/s in our tests), to which an initially quiescent metal can be\naccelerated during the event. The simulations indicate that many widely used\nmodel assumptions, such as the negligible role of Joule dissipation in the heat\nbalance and the constancy of physical property coefficients, remain valid\nduring the response. However, the assumption of liquid metal incompressibility\nis found to be questionable due to the potential significant effects of\npressure waves.",
        "Topology optimization facilitates the automated design of high-performance\nstructures across various engineering fields but, if unconstrained, often\nproduces designs that are complex and difficult to manufacture. A key attribute\nof the resulting designs is connectivity, which involves controlling the\npresence of solid and\/or void islands of material. This manuscript provides a\ncomprehensive overview of existing connectivity constraints developed for\ncontinuous design representations and highlights their advantages and\nlimitations in influencing design outcomes and performance. The review further\nincludes a practical comparison of five different connectivity constraints\nusing a topology optimization framework for sandwich panels that balances\nacoustic and structural performance. With Pareto-front analyses, the\nconstraints are evaluated based on computational cost, monotonicity, parameter\ndependency, and their impact on the optimized designs, their performance, and\nunderlying dynamics. From the comparison, practical insights and rule of thumbs\nhave been derived. The findings emphasize the critical role of selecting\nappropriate connectivity constraints, given their significant effect on the\noptimization results.",
        "Training an energy-based model (EBM) with maximum likelihood is challenging\ndue to the intractable normalisation constant. Traditional methods rely on\nexpensive Markov chain Monte Carlo (MCMC) sampling to estimate the gradient of\nlogartihm of the normalisation constant. We propose a novel objective called\nself-normalised log-likelihood (SNL) that introduces a single additional\nlearnable parameter representing the normalisation constant compared to the\nregular log-likelihood. SNL is a lower bound of the log-likelihood, and its\noptimum corresponds to both the maximum likelihood estimate of the model\nparameters and the normalisation constant. We show that the SNL objective is\nconcave in the model parameters for exponential family distributions. Unlike\nthe regular log-likelihood, the SNL can be directly optimised using stochastic\ngradient techniques by sampling from a crude proposal distribution. We validate\nthe effectiveness of our proposed method on various density estimation tasks as\nwell as EBMs for regression. Our results show that the proposed method, while\nsimpler to implement and tune, outperforms existing techniques.",
        "We present a closed-form expression for the contribution of surface defects\nto the supersymmetric R\\'enyi entropy in six-dimensional $(2,0)$ theories. Our\nresults show that this defect contribution is a linear function of $1\/n$ and is\ndirectly proportional to $2b-d_2$, where $b$ and $d_2$ are the surface defect\nWeyl anomaly coefficients. We also derive a closed-form expression for the\ndefect contribution to the supersymmetric Casimir energy, which simplifies to\n$-d_2$ (up to a proportionality constant) in the chiral algebra limit."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel",
    "start_abstract":"Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2",
        "b0"
      ],
      "title":[
        "Shannon capacity of signal transduction for multiple independent receptors",
        "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
      ],
      "abstract":[
        "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
        "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
      ],
      "categories":[
        "eess.SP",
        "cs.SY"
      ]
    },
    "list":{
      "title":[
        "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings",
        "Separate surface and bulk topological Anderson localization transitions\n  in disordered axion insulators",
        "Harmonic And Transposition Constraints Arising From The Use Of The\n  Roland TR-808 Bass Drum",
        "Quantifying Point Contributions: A Lightweight Framework for Efficient\n  and Effective Query-Driven Trajectory Simplification",
        "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
        "A Hybrid Model\/Data-Driven Solution to Channel, Position and Orientation\n  Tracking in mmWave Vehicular Systems",
        "Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset\n  Electroencephalography Encoding with Quantum Machine Learning",
        "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic\n  Data Generation and Fairness Algorithms",
        "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation",
        "Dango: A Mixed-Initiative Data Wrangling System using Large Language\n  Model",
        "6GStarLab -- A CubeSat Mission to support the development and\n  standardization of Non-Terrestrial Networks towards 6G",
        "Hints of Primordial Magnetic Fields at Recombination and Implications\n  for the Hubble Tension",
        "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in\n  Citation Network",
        "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention",
        "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models",
        "Grammar and Gameplay-aligned RL for Game Description Generation with\n  LLMs",
        "Classical elasticity meets quantum complexity: A connection from the\n  holographic lens",
        "Contact value theorem for electric double layers with modulated surface\n  charge density",
        "Leader-follower formation enabled by pressure sensing in free-swimming\n  undulatory robotic fish",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Three-stage dynamics of nonlinear pulse amplification in ultrafast\n  mid-infrared fiber amplifier with anomalous dispersion",
        "DNN-Powered MLOps Pipeline Optimization for Large Language Models: A\n  Framework for Automated Deployment and Resource Management",
        "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration",
        "Singular leaning coefficients and efficiency in learning theory",
        "Topological derivative approach for deep neural network architecture\n  adaptation",
        "Robust Moving-horizon Estimation for Nonlinear Systems: From Perfect to\n  Imperfect Optimization",
        "Brown dwarf number density in the JWST COSMOS-Web field",
        "Elemental and angular fragmentation cross section measurements with the\n  FOOT experiment"
      ],
      "abstract":[
        "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps:\/\/github.com\/ZeroNLP\/SEA.",
        "In topological phases of matter for which the bulk and boundary support\ndistinct electronic gaps, there exists the possibility of decoupled mobility\ngaps in the presence of disorder. This is in analogy with the well-studied\nproblem of realizing separate or concomitant bulk-boundary criticality in\nconventional Landau theory. Using a three-dimensional axion insulator having\nclean, gapped surfaces with $e^2\/2h$ quantized Hall conductance, we show the\nbulk and surface mobility gap evolve differently in the presence of disorder.\nThe decoupling of the bulk and surface topology yields a regime that realizes a\ntwo-dimensional, unquantized anomalous Hall metal in the Gaussian unitary\nensemble (GUE), which shares some spectral and response properties akin to the\nsurface states of a conventional three-dimensional (3D) topological insulator.\nThe generality of these results as well as extensions to other insulators and\nsuperconductors is discussed.",
        "The study investigates hip-hop music producer Scott Storch's approach to\ntonality, where the song's key is transposed to fit the Roland TR-808 bass drum\ninstead of tuning the drums to the song's key. This process, involving the\nadjustment of all tracks except the bass drum, suggests significant production\nmotives. The primary constraint stems from the limited usable pitch range of\nthe TR-808 bass drum if its characteristic sound is to be preserved. The\nresearch examines drum tuning practices, the role of the Roland TR-808 in\nmusic, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples\nreveals their characteristics and their integration into modern genres like\ntrap and hip-hop. The study also considers the impact of loudspeaker frequency\nresponse and human ear sensitivity on bass drum perception. The findings\nsuggest that Storch's method prioritizes the spectral properties of the bass\ndrum over traditional pitch values to enhance the bass response. The need to\nmaintain the unique sound of the TR-808 bass drum underscores the importance of\nspectral formants and register in contemporary popular music production.",
        "As large volumes of trajectory data accumulate, simplifying trajectories to\nreduce storage and querying costs is increasingly studied. Existing proposals\nface three main problems. First, they require numerous iterations to decide\nwhich GPS points to delete. Second, they focus only on the relationships\nbetween neighboring points (local information) while neglecting the overall\nstructure (global information), reducing the global similarity between the\nsimplified and original trajectories and making it difficult to maintain\nconsistency in query results, especially for similarity-based queries. Finally,\nthey fail to differentiate the importance of points with similar features,\nleading to suboptimal selection of points to retain the original trajectory\ninformation.\n  We propose MLSimp, a novel Mutual Learning query-driven trajectory\nsimplification framework that integrates two distinct models: GNN-TS, based on\ngraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates\nthe importance of a point according to its globality, capturing its correlation\nwith the entire trajectory, and its uniqueness, capturing its differences from\nneighboring points. It also incorporates attention mechanisms in the GNN\nlayers, enabling simultaneous data integration from all points within the same\ntrajectory and refining representations, thus avoiding iterative processes.\nDiff-TS generates amplified signals to enable the retention of the most\nimportant points at low compression rates. Experiments involving eight\nbaselines on three databases show that MLSimp reduces the simplification time\nby 42%--70% and improves query accuracy over simplified trajectories by up to\n34.6%.",
        "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
        "Channel tracking in millimeter wave (mmWave) vehicular systems is crucial for\nmaintaining robust vehicle-to-infrastructure (V2I) communication links, which\ncan be leveraged to achieve high accuracy vehicle position and orientation\ntracking as a byproduct of communication. While prior work tends to simplify\nthe system model by omitting critical system factors such as clock offsets,\nfiltering effects, antenna array orientation offsets, and channel estimation\nerrors, we address the challenges of a practical mmWave multiple-input\nmultiple-output (MIMO) communication system between a single base station (BS)\nand a vehicle while tracking the vehicle's position and orientation (PO)\nconsidering realistic driving behaviors. We first develop a channel tracking\nalgorithm based on multidimensional orthogonal matching pursuit (MOMP) with\nfactoring (F-MOMP) to reduce computational complexity and enable\nhigh-resolution channel estimates during the tracking stage, suitable for PO\nestimation. Then, we develop a network called VO-ChAT (Vehicle\nOrientation-Channel Attention for orientation Tracking), which processes the\nchannel estimate sequence for orientation prediction. Afterward, a weighted\nleast squares (WLS) problem that exploits the channel geometry is formulated to\ncreate an initial estimate of the vehicle's 2D position. A second network named\nVP-ChAT (Vehicle Position-Channel Attention for position Tracking) refines the\ngeometric position estimate. VP-ChAT is a Transformer inspired network\nprocessing the historical channel and position estimates to provide the\ncorrection for the initial geometric position estimate. The proposed solution\nis evaluated using raytracing generated channels in an urban canyon\nenvironment. For 80% of the cases it achieves a 2D position tracking accuracy\nof 26 cm while orientation errors are kept below 0.5 degree.",
        "Electroencephalography (EEG) is widely used in neuroscience and clinical\nresearch for analyzing brain activity. While deep learning models such as\nEEGNet have shown success in decoding EEG signals, they often struggle with\ndata complexity, inter-subject variability, and noise robustness. Recent\nadvancements in quantum machine learning (QML) offer new opportunities to\nenhance EEG analysis by leveraging quantum computing's unique properties. In\nthis study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a\nhybrid neural network incorporating quantum layers into EEGNet, to investigate\nits generalization ability across multiple EEG datasets. Our evaluation spans a\ndiverse set of cognitive and motor task datasets, assessing QEEGNet's\nperformance in different learning scenarios. Experimental results reveal that\nwhile QEEGNet demonstrates competitive performance and maintains robustness in\ncertain datasets, its improvements over traditional deep learning methods\nremain inconsistent. These findings suggest that hybrid quantum-classical\narchitectures require further optimization to fully leverage quantum advantages\nin EEG processing. Despite these limitations, our study provides new insights\ninto the applicability of QML in EEG research and highlights challenges that\nmust be addressed for future advancements.",
        "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
        "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
        "Data wrangling is a time-consuming and challenging task in a data science\npipeline. While many tools have been proposed to automate or facilitate data\nwrangling, they often misinterpret user intent, especially in complex tasks. We\npropose Dango, a mixed-initiative multi-agent system for data wrangling.\nCompared to existing tools, Dango enhances user communication of intent by\nallowing users to demonstrate on multiple tables and use natural language\nprompts in a conversation interface, enabling users to clarify their intent by\nanswering LLM-posed multiple-choice clarification questions, and providing\nmultiple forms of feedback such as step-by-step natural language explanations\nand data provenance to help users evaluate the data wrangling scripts. We\nconducted a within-subjects user study with 38 participants and demonstrated\nthat Dango's features can significantly improve intent clarification, accuracy,\nand efficiency in data wrangling. Furthermore, we demonstrated the\ngeneralizability of Dango by applying it to a broader set of data wrangling\ntasks.",
        "The emergence of the Non-Terrestrial Network (NTN) concept in the last years\nhas revolutionized the space industry. This novel network architecture composed\nof aircraft and spacecraft is currently being standardized by the 3GPP. This\nstandardization process follows dedicated phases in which experimentation of\nthe technology is needed. Although some missions have been conducted to\ndemonstrate specific and service-centric technologies, a open flexible in-orbit\ninfrastructure is demanded to support this standardization process. This work\npresents the 6GStarLab mission, which aims to address this gap. Specifically,\nthis mission envisions to provide a 6U CubeSat as the main in-orbit\ninfrastructure in which multiple technology validations can be uploaded. The\nconcept of this mission is depicted. Additionally, this work presents the\ndetails of the satellite platform and the payload. This last one is designed to\nenable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-,\nand Ka-bands) and an optical terminal. The launch of the satellite is scheduled\nfor Q2 2025, and it will contribute to the standardization of future NTN\narchitectures.",
        "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
        "With the explosive growth of academic literature, effectively evaluating the\nknowledge value of literature has become quite essential. However, most of the\nexisting methods focus on modeling the entire citation network, which is\nstructurally complex and often suffers from long sequence dependencies when\ndealing with text embeddings. Thus, they might have low efficiency and poor\nrobustness in different fields. To address these issues, a novel knowledge\nevaluation method is proposed, called EMK-KEN. The model consists of two\nmodules. Specifically, the first module utilizes MetaFP and Mamba to capture\nsemantic features of node metadata and text embeddings to learn contextual\nrepresentations of each paper. The second module utilizes KAN to further\ncapture the structural information of citation networks in order to learn the\ndifferences in different fields of networks. Extensive experiments based on ten\nbenchmark datasets show that our method outperforms the state-of-the-art\ncompetitors in effectiveness and robustness.",
        "Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and\/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.",
        "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.",
        "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.",
        "In this work, we explore the effects of shear deformations in a wide class of\nholographic amorphous solids. It is found that both the shear stress and the\ncomplexity of formation grow with the increase of the shear strain. Notably, in\nthe regime of very large shear, they exhibit coordinated behavior and adhere to\na universal scaling relation, uncovering a surprising connection between two\nseemingly unrelated aspects of amorphous systems. Furthermore, our findings\nalso provide a counterexample to the previous understanding that the complexity\nscales linearly with the Bekenstein-Hawking entropy for large static black\nholes.",
        "The contact value theorem was originally derived for Coulomb fluids of mobile\ncharged particles in thermal equilibrium, in the presence of interfaces\ncarrying a {\\em uniform} surface charge density and in the absence of\ndielectric discontinuities. It relates the pressure (the effective force)\nbetween two parallel electric double layers to the particle number density and\nthe surface charge density at the interface, separately for each of the two\nelectric double layers. In this paper, we generalise the contact value theorem\nto electric double layers with interfaces carrying a {\\em modulated} surface\ncharge density. The derivation is based on balance of forces exerted on\ninterfaces. The relevance of particular terms of the contact value theorem is\ntested on an exactly solvable two-dimensional Coulomb system with counterions\nonly at the coupling constant $\\Gamma=2$.",
        "Fish use their lateral lines to sense flows and pressure gradients, enabling\nthem to detect nearby objects and organisms. Towards replicating this\ncapability, we demonstrated successful leader-follower formation swimming using\nflow pressure sensing in our undulatory robotic fish ($\\mu$Bot\/MUBot). The\nfollower $\\mu$Bot is equipped at its head with bilateral pressure sensors to\ndetect signals excited by both its own and the leader's movements. First, using\nexperiments with static formations between an undulating leader and a\nstationary follower, we determined the formation that resulted in strong\npressure variations measured by the follower. This formation was then selected\nas the desired formation in free swimming for obtaining an expert policy. Next,\na long short-term memory neural network was used as the control policy that\nmaps the pressure signals along with the robot motor commands and the Euler\nangles (measured by the onboard IMU) to the steering command. The policy was\ntrained to imitate the expert policy using behavior cloning and Dataset\nAggregation (DAgger). The results show that with merely two bilateral pressure\nsensors and less than one hour of training data, the follower effectively\ntracked the leader within distances of up to 200 mm (= 1 body length) while\nswimming at speeds of 155 mm\/s (= 0.8 body lengths\/s). This work highlights the\npotential of fish-inspired robots to effectively navigate fluid environments\nand achieve formation swimming through the use of flow pressure feedback.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Nonlinear pulse amplification in optical fiber, with capability of breaking\nthe gain-bandwidth limitation, is a key technique for high-energy, ultrafast\npulse generation. In the longer wavelength region (including 1.55 {\\mu}m, 2\n{\\mu}m and 2.8 {\\mu}m) where the gain fiber has normally strong anomalous\ndispersion, the nonlinear amplification process over fiber exhibits more\ncomplicated dynamics than that of its 1-{\\mu}m counterpart, and the underlying\nmechanism of the nonlinear pulse propagation process in high-gain anomalous\nfiber is still elusive so far. Here, we demonstrate an in-depth study on the\nnonlinear amplification process in high-gain ultrafast mid-infrared fiber,\nproviding clear physical understanding on the debate of adiabatic soliton\ncompression. We unveil that under the high-gain condition, the ultrafast pulse\nlaunched into the anomalous gain fiber experiences successively three distinct\nstages, named as the balance between linear and nonlinear chirp,\nhigh-order-soliton-like pulse compression and pulse splitting due to high-order\neffects. While a relatively-clean ultrafast pulse can be obtained immediately\nafter the high-order-soliton-like compression stage, excessive gain fiber\nlength could hardly enhance further the pulse peak power due to soliton\nsplitting. Our findings can provide several critical guidelines for designing\nhigh-power ultrafast fiber amplifiers at near- and mid-infrared wavelengths.",
        "The exponential growth in the size and complexity of Large Language Models\n(LLMs) has introduced unprecedented challenges in their deployment and\noperational management. Traditional MLOps approaches often fail to efficiently\nhandle the scale, resource requirements, and dynamic nature of these models.\nThis research presents a novel framework that leverages Deep Neural Networks\n(DNNs) to optimize MLOps pipelines specifically for LLMs. Our approach\nintroduces an intelligent system that automates deployment decisions, resource\nallocation, and pipeline optimization while maintaining optimal performance and\ncost efficiency. Through extensive experimentation across multiple cloud\nenvironments and deployment scenarios, we demonstrate significant improvements:\n40% enhancement in resource utilization, 35% reduction in deployment latency,\nand 30% decrease in operational costs compared to traditional MLOps approaches.\nThe framework's ability to adapt to varying workloads and automatically\noptimize deployment strategies represents a significant advancement in\nautomated MLOps management for large-scale language models. Our framework\nintroduces several novel components including a multi-stream neural\narchitecture for processing heterogeneous operational metrics, an adaptive\nresource allocation system that continuously learns from deployment patterns,\nand a sophisticated deployment orchestration mechanism that automatically\nselects optimal strategies based on model characteristics and environmental\nconditions. The system demonstrates robust performance across various\ndeployment scenarios, including multi-cloud environments, high-throughput\nproduction systems, and cost-sensitive deployments. Through rigorous evaluation\nusing production workloads from multiple organizations, we validate our\napproach's effectiveness in reducing operational complexity while improving\nsystem reliability and cost efficiency.",
        "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582\/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61\/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.",
        "Singular learning models with non-positive Fisher information matrices\ninclude neural networks, reduced-rank regression, Boltzmann machines, normal\nmixture models, and others. These models have been widely used in the\ndevelopment of learning machines. However, theoretical analysis is still in its\nearly stages. In this paper, we examine learning coefficients, which indicate\nthe general learning efficiency of deep linear learning models and three-layer\nneural network models with ReLU units. Finally, we extend the results to\ninclude the case of the Softmax function.",
        "This work presents a novel algorithm for progressively adapting neural\nnetwork architecture along the depth. In particular, we attempt to address the\nfollowing questions in a mathematically principled way: i) Where to add a new\ncapacity (layer) during the training process? ii) How to initialize the new\ncapacity? At the heart of our approach are two key ingredients: i) the\nintroduction of a ``shape functional\" to be minimized, which depends on neural\nnetwork topology, and ii) the introduction of a topological derivative of the\nshape functional with respect to the neural network topology. Using an optimal\ncontrol viewpoint, we show that the network topological derivative exists under\ncertain conditions, and its closed-form expression is derived. In particular,\nwe explore, for the first time, the connection between the topological\nderivative from a topology optimization framework with the Hamiltonian from\noptimal control theory. Further, we show that the optimality condition for the\nshape functional leads to an eigenvalue problem for deep neural architecture\nadaptation. Our approach thus determines the most sensitive location along the\ndepth where a new layer needs to be inserted during the training phase and the\nassociated parametric initialization for the newly added layer. We also\ndemonstrate that our layer insertion strategy can be derived from an optimal\ntransport viewpoint as a solution to maximizing a topological derivative in\n$p$-Wasserstein space, where $p>= 1$. Numerical investigations with fully\nconnected network, convolutional neural network, and vision transformer on\nvarious regression and classification problems demonstrate that our proposed\napproach can outperform an ad-hoc baseline network and other architecture\nadaptation strategies. Further, we also demonstrate other applications of\ntopological derivative in fields such as transfer learning.",
        "Robust stability of moving-horizon estimators is investigated for nonlinear\ndiscrete-time systems that are detectable in the sense of incremental\ninput\/output-to-state stability and are affected by disturbances. The estimate\nof a moving-horizon estimator stems from the on-line solution of a\nleast-squares minimization problem at each time instant. The resulting\nstability guarantees depend on the optimization tolerance in solving such\nminimization problems. Specifically, two main contributions are established:\n(i) the robust stability of the estimation error, while supposing to solve\nexactly the on-line minimization problem; (ii) the practical robust stability\nof the estimation error with state estimates obtained by an imperfect\nminimization. Finally, the construction of such robust moving-horizon\nestimators and the performances resulting from the design based on the\ntheoretical findings are showcased with two numerical examples.",
        "Brown dwarfs are failed stars with very low mass (13 to 75 $M_J$), and an\neffective temperature lower than 2500 K. Thus, they play a key role in\nunderstanding the gap in the mass function between stars and planets. However,\ndue to their faint nature, previous searches are inevitably limited to the\nsolar neighbourhood (20 pc). To improve our knowledge of the low mass part of\nthe initial stellar mass function and the star formation history of the Milky\nWay, it is crucial to find more distant brown dwarfs. Using James Webb Space\nTelescope (JWST) COSMOS-Web data, this study seeks to enhance our comprehension\nof the physical characteristics of brown dwarfs situated at a distance of kpc\nscale. The exceptional sensitivity of the JWST enables the detection of brown\ndwarfs that are up to 100 times more distant than those discovered in the\nearlier all-sky infrared surveys. The large area coverage of the JWST\nCOSMOS-Web survey allows us to find more distant brown dwarfs than earlier JWST\nstudies with smaller area coverages. To capture prominent water absorption\nfeatures around 2.7 $\\mu$m, we apply two colour criteria,\nF115W-F277W+1<F277W-F444W and F277W-F444W>0.9. We then select point sources by\nCLASS_STAR, FLUX_RADIUS, and SPREAD_MODEL criteria. Faint sources are visually\nchecked to exclude possibly extended sources. We conduct SED fitting and MCMC\nsimulations to determine their physical properties and associated\nuncertainties. Our search reveals 25 T-dwarf and 2 Y-dwarf candidates, more\nthan any previous JWST brown dwarf searches. They are located from 0.3 kpc to 4\nkpc away from the Earth. The cumulative number count of our brown dwarf\ncandidates is consistent with the prediction from a standard double exponential\nmodel. Three of our brown dwarf candidates were detected by HST, with\ntransverse velocities $12\\pm5$ km s$^{-1}$, $12\\pm4$ km s$^{-1}$, and $17\\pm6$\nkm s$^{-1}$.",
        "The FOOT (FragmentatiOn Of Target) experiment was proposed to measure double\ndifferential nuclear fragmentation cross sections in angle and kinetic energy\nof the produced fragments in beam-target settings, interesting for\nhadrontherapy and space radioprotection applications. In particular, FOOT\nmeasures projectile and target fragmentations in the kinetic energy range\nbetween $200 \\text{MeV\/u}$ and $800 \\text{MeV\/u}$. In this contribution,\ndifferential cross section measurements of a $400 \\text{MeV\/u}$ $^{16}$O beam\non a Carbon and a polyethylene target with data acquired at GSI (Darmstadt,\nGermany) beam accelerator facility are presented, along with the extraction of\nthe first total fragmentation cross section for a Hydrogen target within the\nFOOT experiment."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Shannon capacity of signal transduction for multiple independent receptors",
    "start_abstract":"Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h",
        "Gravitational lensing by non-self-intersecting vortons",
        "Construction of self-similar energy forms and singularity of Sobolev\n  spaces on Laakso-type fractal spaces",
        "Exact Bayesian inference for Markov switching diffusions",
        "Thermoelectrically Elevated Hydrogel Evaporation for Personal Cooling\n  under Extreme Heat Stress",
        "Predicting House Rental Prices in Ghana Using Machine Learning",
        "Forward and Inverse Problems in Nonlinear Acoustics",
        "Moist Vortex Dynamics of Axisymmetric Tropical Cyclones Before Reaching\n  Symmetric Neutrality -- Part I: A Generalized Tangential Wind Formula",
        "Trajectories of light beams in a Kerr metric: the influence of the\n  rotation of an observer on the shadow of a black hole",
        "Matter Couplings in Supergravity. The first 10 years",
        "Crossover from Wannier-Stark localization to charge density waves for\n  interacting spinless fermions in one dimension",
        "Two categories of UV-upturn galaxies revealed by semi-analytic models",
        "Global boundedness in the higher-dimensional fully parabolic chemotaxis\n  with weak singular sensitivity and logistic source",
        "Unique extremality of affine maps on plane domains",
        "Star formation in low brightness galaxies and in the extended gaseous\n  disks of normal galaxies"
      ],
      "abstract":[
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc.",
        "We present exact solutions to the Nambu-Goto equations for thin vortons\nstabilized by chiral currents. The solutions describe a class of\nnon-self-intersecting, stationary loops with arbitrary shapes. In addition to\nthe trivial circular and the Kibble-Turok vortons, we also derive a\ntwo-parameter family that incorporates the first, second, and third harmonic\nmodes. We found that, in general, the vorton's constraints allow for\nconstructing families of solutions with arbitrary harmonic modes. We further\ninvestigate the gravitational lensing effects associated with these solutions\nunder the weak-field and thin-lens approximations. For circular vortons, the\nlensing exhibits a sharp discontinuity separating two regions with distinctly\ndifferent distortions. The corresponding Einstein ring co-exist alongside an\nalmost undistorted source image. This effect is significantly amplified in the\ncase of non-circular vortons, highlighting their potential observational\nsignatures.",
        "We construct self-similar $p$-energy forms as normalized limits of\ndiscretized $p$-energies on a rich class of Laakso-type fractal spaces.\nCollectively, we refer to them as IGS-fractals, where IGS stands for\n(edge-)iterated graph systems. We propose this framework as a rich source of\n\"toy models\" that can be consulted for tackling challenging questions that are\nnot well understood on most other fractal spaces. Supporting this, our\nframework uncovers a novel analytic phenomenon, which we term as singularity of\nSobolev spaces. This means that the associated Sobolev spaces\n$\\mathscr{F}_{p_1}$ and $\\mathscr{F}_{p_2}$ for distinct $p_1,p_2 \\in\n(1,\\infty)$ intersect only at constant functions. We provide the first example\nof a self-similar fractal on which this singularity phenomenon occurs for all\npairs of distinct exponents. In particular, we show that the Laakso diamond\nspace is one such example.",
        "We give the first exact Bayesian methodology for the problem of inference in\ndiscretely observed regime switching diffusions. We design an MCMC and an MCEM\nalgorithm that target the exact posterior of diffusion parameters and the\nlatent regime process. The algorithms are exact in the sense that they target\nthe correct posterior distribution of the continuous model, so that the errors\nare due to Monte Carlo only. Switching diffusion models extend ordinary\ndiffusions by allowing for jumps in instantaneous drift and volatility. The\njumps are driven by a latent, continuous time Markov switching process. We\nillustrate the method on numerical examples, including an empirical analysis of\nthe method's scalability in the length of the time series, and find that it is\ncomparable in computational cost with discrete approximations while avoiding\ntheir shortcomings.",
        "Hydrogel evaporative cooling has recently emerged as an appealing strategy\nfor personal cooling. However, with the increasing prevalence of extreme heat\nevents featuring high temperatures (above 40$^{\\circ}$C) and relative humidity\n(RH$> 30\\%$), hydrogel alone may not achieve thermal comfort under most\nconditions, as it must be maintained at a sufficiently high temperature-often\nexceeding the skin comfort temperature ($\\sim$35.8$^{\\circ}$C)-to achieve\neffective evaporation in hot, humid environments. This study integrates\nthermoelectric devices (TEDs) with hydrogels to create a personal cooling\nsolution suited to extreme heat. TEDs pump heat away from the skin, maintaining\nit at a comfortable temperature, while simultaneously raising the temperature\nof a hydrogel layer positioned on top of the TEDs to enhance its evaporation\nrate. The TED-hydrogel tandem device outperforms TEDs or hydrogel alone in\nextreme conditions (up to 55$^{\\circ}$C and RH$> 30\\%$). Furthermore, the\nactive temperature control enabled by the TEDs allows the system to adapt to\nvarying thermal loads and environmental conditions. With a manageable hydrogel\nand battery weight, this cooling system can operate for over six hours. These\nresults demonstrate the potential of hybrid evaporative and thermoelectric\ncooling as an efficient, adaptable, and sustainable personal cooling solution\nto combat extreme heat.",
        "This study investigates the efficacy of machine learning models for\npredicting house rental prices in Ghana, addressing the need for accurate and\naccessible housing market information. Utilising a comprehensive dataset of\nrental listings, we trained and evaluated various models, including CatBoost,\nXGBoost, and Random Forest. CatBoost emerged as the best-performing model,\nachieving an $R^2$ of 0.876, demonstrating its ability to effectively capture\ncomplex relationships within the housing market. Feature importance analysis\nrevealed that location-based features, number of bedrooms, bathrooms, and\nfurnishing status are key drivers of rental prices. Our findings provide\nvaluable insights for stakeholders, including real estate professionals,\ninvestors, and policymakers, while also highlighting opportunities for future\nresearch, such as incorporating temporal data and exploring regional\nvariations.",
        "The importance of ultrasound is well established in the imaging of human\ntissue. In order to enhance image quality by exploiting nonlinear effects,\nrecently techniques such as harmonic imaging and nonlinearity parameter\ntomography have been put forward. As soon as the pressure amplitude exceeds a\ncertain bound, the classical linear wave equation loses its validity and more\ngeneral nonlinear versions have to be used. Another characteristic property of\nultrasound propagation in human tissue is frequency power law attenuation,\nleading to fractional derivative damping models in time domain. In this\ncontribution we will first of all dwell on modeling nonlinearity on the one\nhand and fractional damping on the other hand. Moreover we will give an idea on\nthe challenges in the analysis of the resulting PDEs and discuss some parameter\nasymptotics. Finally, we address a relevant inverse problems in this context,\nthe above mentioned task of nonlinearity parameter imaging, which leads to a\ncoefficient identification problem for a quasilinear wave equation.",
        "The potential intensity (PI) theory of tropical cyclones (TCs) provides a\nreasonable estimate of the maximum intensity of a steady-state TC in a\nquiescent environment. The traditional PI theory relies on the symmetric\nneutrality (SN) assumption, where the isolines of absolute angular momentum (M)\nare parallel to the saturation entropy (s*) surfaces within the eyewall\nupdraft. When the SN is not valid, there is currently no quantitative theory\nthat explicitly describes how these surfaces directly relate to the maximum\ntangential wind (vmax) near the surface. In this study, the PI theory is\nrevisited without making the SN assumption. Under non-SN conditions, it is\nfound that the balance between the centrifugal torque and baroclinic torque\nprovides a strong constraint on the vortex structure and balanced intensity.\nSpecifically, it is shown that the gradient between s* and M along constant\ntemperature (T) throughout the saturated eyewall determines the structure of\nthe M surface and the balanced intensity at the low level. The same technique\ncan be applied to obtain the generalized terms that correspond to the\nunbalanced component. It is shown that this generalized vmax equation is the\nnatural extension of the traditional PI formula into the non-symmetric\nneutrality regime. Verifying against axisymmetric simulations, it is shown that\nthe generalized vmax equation can accurately quantify the various contributions\nto vmax during the rapid intensification process. The implications of these\nfindings on the TC rapid intensification, such as the TC inner-core structure\nand the upper-tropospheric mixing process, are examined.",
        "This paper investigates the trajectories of light beams in a Kerr metric,\nwhich describes the gravitational field in the neighborhood of a rotating black\nhole. After reduction by cyclic coordinates, this problem reduces to analysis\nof a Hamiltonian system with two degrees of freedom. A bifurcation diagram is\nconstructed and a classification is made of the types of trajectories of the\nsystem according to the values of first integrals. Relations describing the\nboundary of the shadow of the black hole are obtained for a stationary observer\nwho rotates with an arbitrary angular velocity about the axis of rotation of\nthe black hole.",
        "Following the initial construction of pure supergravity in 1976, various\nmethods were developed to couple supergravity with supersymmetric matter. This\ncontribution to 'Half a century of supergravity' provides a personal\nperspective on the key steps, techniques and results developed in the first\ndecade. These developments form the foundation for numerous applications in\nphenomenology, cosmology and string theories, while also revealing intriguing\nmathematical structures.",
        "We study spinless fermions on a finite chain with nearest-neighbor repulsion\nand in the presence of a Wannier-Stark linearly-varying electric field\npotential. In the absence of the interaction, the eigenstates are localized for\nthe system's sizes larger than the localization length. We present several\nanalytical expressions for the localization length, which is proportional to\nthe inverse of the electric field. Using the density matrix renormalization\ngroup numerical technique, we observe that the ground state exhibits a decrease\nof the occupation on the chain sites from the `bulk', with occupation 1, to the\nvacuum, with occupation 0. The width of this intermediate `edge' region is also\ninversely proportional to the electric field, increasing linearly with the\nstrength of the nearest-neighbor repulsion. For strong interactions, the\noccupations in the intermediate region exhibit a charge density wave. We also\npresent the local density of states for sites in the `edge' region. For the\nnon-interacting case, the spectrum shows an increasing energy-localized\nstructure as the field is increased, which is a consequence of the uniform\nenergy distribution of the localized states (Wannier-Stark ladder). This\nstructure survives for small interactions, and it smears out in the strongly\ninteracting limit. Experimental variations of the slope of the potential (the\nelectric field) on cold atom chains may test these predictions.",
        "UV-upturn galaxies are characterized by unusually excessive flux in the\nfar-ultraviolet (FUV) band, observed in some elliptical galaxies and the bulges\nof disk galaxies. We examine UV-upturn galaxies within the semi-analytic model\nGABE, which embeds the formation of extreme horizontal branch (EHB) stars --\nproposed as key candidates responsible for the UV-upturn phenomenon. We have\nanalyzed all related physical processes, including stellar evolution, initial\nmass functions (IMFs), dust attenuation, galaxy age, metallicity, and binary\nfractions, in an effort to determine which processes play significant roles.\nOur findings reveal two categories of UV-upturn galaxies in the semi-analytic\nmodel, each with distinct formation channels: old metal-rich quenched\nelliptical galaxies, which are intrinsic UV-upturn galaxies induced by EHB\nstars within their old stellar populations, and dusty star-forming galaxies,\nwhich are relatively young and may also be photometrically identified as\nUV-upturn galaxies when accounting for dust attenuation. Dust attenuation\ncontributes to 20%-60% of the UV-upturn galaxies, depending on the specific\ndust attenuation models adopted. With the binary star formation model of EHB\nstars, both of these formation channels exhibit strong preferences for high\nstellar metallicity. The high-mass end slope of the IMFs is found to have a\nmarginal effect, indicating that a universal IMF is adequate for studying the\nUV-upturn phenomenon.",
        "We consider the following chemotaxis system under homogeneous Neumann\nboundary conditions in a smooth, open, bounded domain $\\Omega \\subset\n\\mathbb{R}^n$ with $n \\geq 3$: \\begin{equation*}\n  \\begin{cases}\n  u_t = \\Delta u - \\chi \\nabla \\cdot \\left( \\frac{u}{v^k} \\nabla v \\right) + ru\n- \\mu u^2, & \\text{in } \\Omega \\times (0,T_{\\rm max}),\n  v_t = \\Delta v - \\alpha v + \\beta u, & \\text{in } \\Omega \\times (0,T_{\\rm\nmax}),\n  \\end{cases} \\end{equation*} where $k \\in (0,1)$, and $\\chi, r, \\mu, \\alpha,\n\\beta$ are positive parameters. In this paper, we demonstrate that for suitably\nsmooth initial data, the problem admits a unique nonnegative classical solution\nthat remains globally bounded in time when $\\mu$ is sufficiently large.",
        "We prove that affine maps are uniquely extremal quasiconformal maps on the\ncomplement of a well distribute set in the complex plane answering a conjecture\nfrom \\cite{markovic}. We construct the required Reich sequence using Bergman\nprojections, and meromorphic partitions of unity.",
        "We analyze the available observational data on the radial distribution of gas\nand young stellar populations in the disks of low surface brightness (LSB)\ngalaxies and in the outer regions or the extended disks of normal brightness\n(HSB) galaxies. These cases involve star formation under special conditions of\nlow volume and surface gas density. There is no well-defined boundary between\nthese subgroups of galaxies that we consider, but in non-dwarf LSB galaxies the\nrate of current star formation within the wide range of radial distances\nappears to be higher compared to the outer disks of most of HSB galaxies at\nsimilar values of the surface gas density. The factors that could stimulate the\ncompression of the rarefied gas at the periphery of galaxies are briefly\ndiscussed. Attention is drawn to the idea that the densities of LSB disks\nestimated from their brightness may be underestimated."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT",
    "start_abstract":"Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h",
        "Gravitational lensing by non-self-intersecting vortons",
        "Construction of self-similar energy forms and singularity of Sobolev\n  spaces on Laakso-type fractal spaces",
        "Exact Bayesian inference for Markov switching diffusions",
        "Thermoelectrically Elevated Hydrogel Evaporation for Personal Cooling\n  under Extreme Heat Stress",
        "Predicting House Rental Prices in Ghana Using Machine Learning",
        "Forward and Inverse Problems in Nonlinear Acoustics",
        "Moist Vortex Dynamics of Axisymmetric Tropical Cyclones Before Reaching\n  Symmetric Neutrality -- Part I: A Generalized Tangential Wind Formula",
        "Trajectories of light beams in a Kerr metric: the influence of the\n  rotation of an observer on the shadow of a black hole",
        "Matter Couplings in Supergravity. The first 10 years",
        "Crossover from Wannier-Stark localization to charge density waves for\n  interacting spinless fermions in one dimension",
        "Two categories of UV-upturn galaxies revealed by semi-analytic models",
        "Global boundedness in the higher-dimensional fully parabolic chemotaxis\n  with weak singular sensitivity and logistic source",
        "Unique extremality of affine maps on plane domains",
        "Star formation in low brightness galaxies and in the extended gaseous\n  disks of normal galaxies"
      ],
      "abstract":[
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc.",
        "We present exact solutions to the Nambu-Goto equations for thin vortons\nstabilized by chiral currents. The solutions describe a class of\nnon-self-intersecting, stationary loops with arbitrary shapes. In addition to\nthe trivial circular and the Kibble-Turok vortons, we also derive a\ntwo-parameter family that incorporates the first, second, and third harmonic\nmodes. We found that, in general, the vorton's constraints allow for\nconstructing families of solutions with arbitrary harmonic modes. We further\ninvestigate the gravitational lensing effects associated with these solutions\nunder the weak-field and thin-lens approximations. For circular vortons, the\nlensing exhibits a sharp discontinuity separating two regions with distinctly\ndifferent distortions. The corresponding Einstein ring co-exist alongside an\nalmost undistorted source image. This effect is significantly amplified in the\ncase of non-circular vortons, highlighting their potential observational\nsignatures.",
        "We construct self-similar $p$-energy forms as normalized limits of\ndiscretized $p$-energies on a rich class of Laakso-type fractal spaces.\nCollectively, we refer to them as IGS-fractals, where IGS stands for\n(edge-)iterated graph systems. We propose this framework as a rich source of\n\"toy models\" that can be consulted for tackling challenging questions that are\nnot well understood on most other fractal spaces. Supporting this, our\nframework uncovers a novel analytic phenomenon, which we term as singularity of\nSobolev spaces. This means that the associated Sobolev spaces\n$\\mathscr{F}_{p_1}$ and $\\mathscr{F}_{p_2}$ for distinct $p_1,p_2 \\in\n(1,\\infty)$ intersect only at constant functions. We provide the first example\nof a self-similar fractal on which this singularity phenomenon occurs for all\npairs of distinct exponents. In particular, we show that the Laakso diamond\nspace is one such example.",
        "We give the first exact Bayesian methodology for the problem of inference in\ndiscretely observed regime switching diffusions. We design an MCMC and an MCEM\nalgorithm that target the exact posterior of diffusion parameters and the\nlatent regime process. The algorithms are exact in the sense that they target\nthe correct posterior distribution of the continuous model, so that the errors\nare due to Monte Carlo only. Switching diffusion models extend ordinary\ndiffusions by allowing for jumps in instantaneous drift and volatility. The\njumps are driven by a latent, continuous time Markov switching process. We\nillustrate the method on numerical examples, including an empirical analysis of\nthe method's scalability in the length of the time series, and find that it is\ncomparable in computational cost with discrete approximations while avoiding\ntheir shortcomings.",
        "Hydrogel evaporative cooling has recently emerged as an appealing strategy\nfor personal cooling. However, with the increasing prevalence of extreme heat\nevents featuring high temperatures (above 40$^{\\circ}$C) and relative humidity\n(RH$> 30\\%$), hydrogel alone may not achieve thermal comfort under most\nconditions, as it must be maintained at a sufficiently high temperature-often\nexceeding the skin comfort temperature ($\\sim$35.8$^{\\circ}$C)-to achieve\neffective evaporation in hot, humid environments. This study integrates\nthermoelectric devices (TEDs) with hydrogels to create a personal cooling\nsolution suited to extreme heat. TEDs pump heat away from the skin, maintaining\nit at a comfortable temperature, while simultaneously raising the temperature\nof a hydrogel layer positioned on top of the TEDs to enhance its evaporation\nrate. The TED-hydrogel tandem device outperforms TEDs or hydrogel alone in\nextreme conditions (up to 55$^{\\circ}$C and RH$> 30\\%$). Furthermore, the\nactive temperature control enabled by the TEDs allows the system to adapt to\nvarying thermal loads and environmental conditions. With a manageable hydrogel\nand battery weight, this cooling system can operate for over six hours. These\nresults demonstrate the potential of hybrid evaporative and thermoelectric\ncooling as an efficient, adaptable, and sustainable personal cooling solution\nto combat extreme heat.",
        "This study investigates the efficacy of machine learning models for\npredicting house rental prices in Ghana, addressing the need for accurate and\naccessible housing market information. Utilising a comprehensive dataset of\nrental listings, we trained and evaluated various models, including CatBoost,\nXGBoost, and Random Forest. CatBoost emerged as the best-performing model,\nachieving an $R^2$ of 0.876, demonstrating its ability to effectively capture\ncomplex relationships within the housing market. Feature importance analysis\nrevealed that location-based features, number of bedrooms, bathrooms, and\nfurnishing status are key drivers of rental prices. Our findings provide\nvaluable insights for stakeholders, including real estate professionals,\ninvestors, and policymakers, while also highlighting opportunities for future\nresearch, such as incorporating temporal data and exploring regional\nvariations.",
        "The importance of ultrasound is well established in the imaging of human\ntissue. In order to enhance image quality by exploiting nonlinear effects,\nrecently techniques such as harmonic imaging and nonlinearity parameter\ntomography have been put forward. As soon as the pressure amplitude exceeds a\ncertain bound, the classical linear wave equation loses its validity and more\ngeneral nonlinear versions have to be used. Another characteristic property of\nultrasound propagation in human tissue is frequency power law attenuation,\nleading to fractional derivative damping models in time domain. In this\ncontribution we will first of all dwell on modeling nonlinearity on the one\nhand and fractional damping on the other hand. Moreover we will give an idea on\nthe challenges in the analysis of the resulting PDEs and discuss some parameter\nasymptotics. Finally, we address a relevant inverse problems in this context,\nthe above mentioned task of nonlinearity parameter imaging, which leads to a\ncoefficient identification problem for a quasilinear wave equation.",
        "The potential intensity (PI) theory of tropical cyclones (TCs) provides a\nreasonable estimate of the maximum intensity of a steady-state TC in a\nquiescent environment. The traditional PI theory relies on the symmetric\nneutrality (SN) assumption, where the isolines of absolute angular momentum (M)\nare parallel to the saturation entropy (s*) surfaces within the eyewall\nupdraft. When the SN is not valid, there is currently no quantitative theory\nthat explicitly describes how these surfaces directly relate to the maximum\ntangential wind (vmax) near the surface. In this study, the PI theory is\nrevisited without making the SN assumption. Under non-SN conditions, it is\nfound that the balance between the centrifugal torque and baroclinic torque\nprovides a strong constraint on the vortex structure and balanced intensity.\nSpecifically, it is shown that the gradient between s* and M along constant\ntemperature (T) throughout the saturated eyewall determines the structure of\nthe M surface and the balanced intensity at the low level. The same technique\ncan be applied to obtain the generalized terms that correspond to the\nunbalanced component. It is shown that this generalized vmax equation is the\nnatural extension of the traditional PI formula into the non-symmetric\nneutrality regime. Verifying against axisymmetric simulations, it is shown that\nthe generalized vmax equation can accurately quantify the various contributions\nto vmax during the rapid intensification process. The implications of these\nfindings on the TC rapid intensification, such as the TC inner-core structure\nand the upper-tropospheric mixing process, are examined.",
        "This paper investigates the trajectories of light beams in a Kerr metric,\nwhich describes the gravitational field in the neighborhood of a rotating black\nhole. After reduction by cyclic coordinates, this problem reduces to analysis\nof a Hamiltonian system with two degrees of freedom. A bifurcation diagram is\nconstructed and a classification is made of the types of trajectories of the\nsystem according to the values of first integrals. Relations describing the\nboundary of the shadow of the black hole are obtained for a stationary observer\nwho rotates with an arbitrary angular velocity about the axis of rotation of\nthe black hole.",
        "Following the initial construction of pure supergravity in 1976, various\nmethods were developed to couple supergravity with supersymmetric matter. This\ncontribution to 'Half a century of supergravity' provides a personal\nperspective on the key steps, techniques and results developed in the first\ndecade. These developments form the foundation for numerous applications in\nphenomenology, cosmology and string theories, while also revealing intriguing\nmathematical structures.",
        "We study spinless fermions on a finite chain with nearest-neighbor repulsion\nand in the presence of a Wannier-Stark linearly-varying electric field\npotential. In the absence of the interaction, the eigenstates are localized for\nthe system's sizes larger than the localization length. We present several\nanalytical expressions for the localization length, which is proportional to\nthe inverse of the electric field. Using the density matrix renormalization\ngroup numerical technique, we observe that the ground state exhibits a decrease\nof the occupation on the chain sites from the `bulk', with occupation 1, to the\nvacuum, with occupation 0. The width of this intermediate `edge' region is also\ninversely proportional to the electric field, increasing linearly with the\nstrength of the nearest-neighbor repulsion. For strong interactions, the\noccupations in the intermediate region exhibit a charge density wave. We also\npresent the local density of states for sites in the `edge' region. For the\nnon-interacting case, the spectrum shows an increasing energy-localized\nstructure as the field is increased, which is a consequence of the uniform\nenergy distribution of the localized states (Wannier-Stark ladder). This\nstructure survives for small interactions, and it smears out in the strongly\ninteracting limit. Experimental variations of the slope of the potential (the\nelectric field) on cold atom chains may test these predictions.",
        "UV-upturn galaxies are characterized by unusually excessive flux in the\nfar-ultraviolet (FUV) band, observed in some elliptical galaxies and the bulges\nof disk galaxies. We examine UV-upturn galaxies within the semi-analytic model\nGABE, which embeds the formation of extreme horizontal branch (EHB) stars --\nproposed as key candidates responsible for the UV-upturn phenomenon. We have\nanalyzed all related physical processes, including stellar evolution, initial\nmass functions (IMFs), dust attenuation, galaxy age, metallicity, and binary\nfractions, in an effort to determine which processes play significant roles.\nOur findings reveal two categories of UV-upturn galaxies in the semi-analytic\nmodel, each with distinct formation channels: old metal-rich quenched\nelliptical galaxies, which are intrinsic UV-upturn galaxies induced by EHB\nstars within their old stellar populations, and dusty star-forming galaxies,\nwhich are relatively young and may also be photometrically identified as\nUV-upturn galaxies when accounting for dust attenuation. Dust attenuation\ncontributes to 20%-60% of the UV-upturn galaxies, depending on the specific\ndust attenuation models adopted. With the binary star formation model of EHB\nstars, both of these formation channels exhibit strong preferences for high\nstellar metallicity. The high-mass end slope of the IMFs is found to have a\nmarginal effect, indicating that a universal IMF is adequate for studying the\nUV-upturn phenomenon.",
        "We consider the following chemotaxis system under homogeneous Neumann\nboundary conditions in a smooth, open, bounded domain $\\Omega \\subset\n\\mathbb{R}^n$ with $n \\geq 3$: \\begin{equation*}\n  \\begin{cases}\n  u_t = \\Delta u - \\chi \\nabla \\cdot \\left( \\frac{u}{v^k} \\nabla v \\right) + ru\n- \\mu u^2, & \\text{in } \\Omega \\times (0,T_{\\rm max}),\n  v_t = \\Delta v - \\alpha v + \\beta u, & \\text{in } \\Omega \\times (0,T_{\\rm\nmax}),\n  \\end{cases} \\end{equation*} where $k \\in (0,1)$, and $\\chi, r, \\mu, \\alpha,\n\\beta$ are positive parameters. In this paper, we demonstrate that for suitably\nsmooth initial data, the problem admits a unique nonnegative classical solution\nthat remains globally bounded in time when $\\mu$ is sufficiently large.",
        "We prove that affine maps are uniquely extremal quasiconformal maps on the\ncomplement of a well distribute set in the complex plane answering a conjecture\nfrom \\cite{markovic}. We construct the required Reich sequence using Bergman\nprojections, and meromorphic partitions of unity.",
        "We analyze the available observational data on the radial distribution of gas\nand young stellar populations in the disks of low surface brightness (LSB)\ngalaxies and in the outer regions or the extended disks of normal brightness\n(HSB) galaxies. These cases involve star formation under special conditions of\nlow volume and surface gas density. There is no well-defined boundary between\nthese subgroups of galaxies that we consider, but in non-dwarf LSB galaxies the\nrate of current star formation within the wide range of radial distances\nappears to be higher compared to the outer disks of most of HSB galaxies at\nsimilar values of the surface gas density. The factors that could stimulate the\ncompression of the rarefied gas at the periphery of galaxies are briefly\ndiscussed. Attention is drawn to the idea that the densities of LSB disks\nestimated from their brightness may be underestimated."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment",
        "Revolutionizing Gravitational Potential Analysis: From Clairaut to Lie\n  Groups",
        "Number of partitions of modular integers (with an Appendix by P.\n  Deligne)",
        "A Proof of Lieb--Wehrl Entropy conjecture for $SU(N,1)$",
        "A Bayesian Non-linear Mixed-Effects Model for Accurate Detection of the\n  Onset of Cognitive Decline in Longitudinal Aging Studies",
        "Rationalizability and Monotonocity in Games with Incomplete Information",
        "Predicting the detectability of sulphur-bearing molecules in the solid\n  phase with simulated spectra of JWST instruments",
        "Dialectics of antimicrobial peptides I: common mechanisms of offensive\n  and protecting roles of the peptides",
        "Assessment of the January 2025 Los Angeles County wildfires: A\n  multi-modal analysis of impact, response, and population exposure",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Normalizing flows for SU($N$) gauge theories employing singular value\n  decomposition",
        "White's conjecture for matroids and inner projections",
        "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of\n  Model Prediction and Scientific Understanding of Soil Organic Carbon",
        "Constructing reducibly geometrically finite subgroups of the mapping\n  class group",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain"
      ],
      "abstract":[
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies.",
        "This letter introduces an advanced novel theory for calculating non-linear\nNewtonian hydrostatic perturbations in the density, shape, and gravitational\nfield of fluid stars and planets subjected to external tidal and rotational\nforces. The theory employs a Lie group approach using exponential mappings to\nderive exact differential equations for large gravitational field perturbations\nand the shape function, which describes the finite deformation of the body's\nfigure. This approach lays the foundation for the precise analytic\ndetermination and numerical computation of the induced body's multipole moments\nand Love numbers with any desired degree of accuracy.",
        "For integers $n,k,s$, we give a formula for the number $T(n,k,s)$ of order\n$k$ subsets of the ring $\\mathbb{Z}\/n\\mathbb{Z}$ whose sum of elements is $s$\nmodulo $n$. To do so, we describe explicitly a sequence of matrices $M(k)$, for\npositive integers $k$, such that the size of $M(k)$ is the number of divisors\nof $k$, and for two coprime integers $k_{1},k_{2}$, the matrix $M(k_{1}k_{2})$\nis the Kronecker product of $M(k_{1})$ and $M(k_{2})$. For $s=0, 1, 2$, and for\n$s=k\/2$ when $k$ is even, the sequences $T(n,k,s)$ are related to the number of\nnecklaces with $k$ black beads and $n-k$ white beads, and to Lyndon words. This\nwork begins with empirical determinations of $M(k)$ up to $k=10000$, from which\nwe infer a closed formula that encompasses many entries in the Encyclopedia of\nInteger Sequences. Its proof comes from work on Ramanujan sums, by Ramanathan,\nwith a generalization to wider problems linked to representation theory and\nrecently described by Deligne.",
        "We investigate the sharp functional inequalities for the coherent state\ntransforms of $SU(N,1)$. These inequalities are rooted in Wehrl's definition of\nsemiclassical entropy and his conjecture about its minimum value. Lieb resolved\nthis conjecture in 1978, posing a similar question for Bloch coherent states of\n$SU(2)$. The $SU(2)$ conjecture was settled by Lieb and Solovej in 2014, and\nthe conjecture was extended for a wide class of Lie groups. The generalized\nLieb conjecture has been resolved for several Lie groups, including $SU(N),\\,\nN\\geq2$, $SU(1,1)$, and its $AX+B$ subgroup. With sharp functional inequalities\nfor the coherent state transforms of the group $SU(N,1)$, we confirm this\nLieb-Wehrl entropy conjecture for $SU(N,1),\\, N\\geq2$. Additionally, we explore\nthe Faber-Krahn inequality, which applies to the short-time Fourier transform\nwith a Gaussian window. This inequality was previously proven by Nicola and\nTilli and later extended by Ramos and Tilli to the wavelet transform. In this\npaper, we further extend this result within the framework of the Bergman space\n$\\mathcal A_{\\alpha}$.",
        "Change-point models are frequently considered when modeling phenomena where a\nregime shift occurs at an unknown time. In ageing research, these models are\ncommonly adopted to estimate of the onset of cognitive decline. Yet commonly\nused models present several limitations. Here, we present a Bayesian non-linear\nmixed-effects model based on a differential equation designed for longitudinal\nstudies to overcome some limitations of classical change point models used in\nageing research. We demonstrate the ability of the proposed model to avoid\nbiases in estimates of the onset of cognitive impairment in a simulated study.\nFinally, the methodology presented in this work is illustrated by analysing\nresults from memory tests from older adults who participated in the English\nLongitudinal Study of Ageing.",
        "This paper examines games with strategic complements or substitutes and\nincomplete information, where players are uncertain about the opponents'\nparameters. We assume that the players' beliefs about the opponent's parameters\nare selected from some given set of beliefs. One extreme is the case where\nthese sets only contain a single belief, representing a scenario where the\nplayers' actual beliefs about the parameters are commonly known among the\nplayers. Another extreme is the situation where these sets contain all possible\nbeliefs, representing a scenario where the players have no information about\nthe opponents' beliefs about parameters. But we also allow for intermediate\ncases, where these sets contain some, but not all, possible beliefs about the\nparameters. We introduce an assumption of weakly increasing differences that\ntakes both the choice belief and parameter belief of a player into account.\nUnder this assumption, we demonstrate that greater choice-parameter beliefs\nleads to greater optimal choices. Moreover, we show that the greatest and least\npoint rationalizable choice of a player is increasing in their parameter, and\nthese can be determined through an iterative procedure. In each round of the\niterative procedure, the lowest surviving choice is optimal for the lowest\nchoice-parameter belief, while the greatest surviving choice is optimal for the\nhighest choice-parameter belief.",
        "To date, gas phase observations of sulphur in dense interstellar environments\nhave only constrained the molecular carriers of 1% of its predicted cosmic\nabundance. An additional 5% is known to be locked up in molecular solids in\ndense clouds, leaving the main reservoir of depleted sulphur in the solid phase\nunknown. The spectral resolution and sensitivity of the JWST could make a\nsubstantial difference in detecting part of this missing sulphur, with its\nwavelength coverage that includes vibrational absorption features of the\nS-carriers H2S, OCS, SO2, CS2, SO, CS, and S8. The aim of this study is to\ndetermine whether these molecules may be viable candidates for detection. We\ncarried out new laboratory measurements of the IR absorption spectra of CS2 and\nS8 to update the IR band strength of the most intense CS2 absorption feature at\n6.8 {\\mu}m, as well as to determine that of S8 at 20.3 {\\mu}m for the first\ntime. These data, along with values previously reported in the literature,\nallow us to evaluate which S-bearing species could be potentially detected with\nJWST in interstellar ices. Taking the literature abundances of the major ice\nspecies determined by previous IR observations towards starless cores, LYSOs\nand MYSOs, we generated simulated IR spectra using the characteristics of the\ninstruments on the JWST. Thus, we have been able to establish a case study for\nthree stages of the star formation process. We conclude that the detection of\nS-bearing molecules remains challenging. Despite these obstacles, the detection\nof H2S and potentially SO2 should be possible in regions with favourable\nphysical and chemical conditions. In contrast, S8 would remain undetected.\nAlthough the sensitivity of JWST is insufficient to determine the sulphur\nbudget in the solid state, the detection of an additional icy sulphur compound\n(H2S, SO2) would enable us to elevate our knowledge of sulphur chemistry.",
        "Antimicrobial peptides (AMPs) have intrigued researchers for decades due to\nthe contradiction between their high potential against resistant bacteria and\nthe inability to find a structure-function relationship for the development of\nan effective and non-toxic agent. In the present study and the companion paper\n[Phys. Rev. E (2024)], we performed a comprehensive experimental and\ntheoretical analysis of various aspects of AMP-membrane interactions and\nAMP-induced pore formation. Using the well-known melittin and magainin as\nexamples, we showed, using patch-clamp and fluorescence measurements, that\nthese peptides, even at nanomolar concentrations, modify the membrane by making\nit permeable to protons (and, possibly, water), but not to ions, and protect\nthe membrane from large pore formation after subsequent addition of 20-fold\nhigher concentrations of AMPs. This protective effect is independent of the\nmembrane side (or both sides) of the peptide addition and is determined by the\npeptide-induced deformations of the membrane. Peptides create small,\nH+-permeable pores that incessantly connect the opposing membrane leaflets,\nallowing translocation of peptides and lipids and thus preventing further\ngeneration of large lateral pressure\/tension imbalance. At the same time, such\nan imbalance is a key to the formation of peptide-induced pores at high AMP\nconcentrations, with the main contribution coming from single ion-conducting\nevents rather than stable channel-like structures. Therefore, our results\nsuggest that lowering the AMP concentration, which is a common principle to\nreduce toxicity, may actually make bacteria resistant to AMP. However, a\nprotective pre-treatment with nanomolar concentrations of peptides may be the\nkey to protect eukaryotic cells from the high concentrations of AMPs.",
        "This study presents a comprehensive analysis of four significant California\nwildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts\nthrough multiple dimensions, including land cover change, jurisdictional\nmanagement, structural damage, and demographic vulnerability. Using the\nChebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the\nextent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares.\nOur analysis revealed that shrubland ecosystems were consistently the most\naffected, comprising 57.4-75.8% of burned areas across all events. The\njurisdictional assessment demonstrated varying management complexities, from\nsingular authority (98.7% in the Palisades Fire) to distributed management\nacross multiple agencies. A structural impact analysis revealed significant\ndisparities between urban interface fires (Eaton: 9,869 structures; Palisades:\n8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17\nstructures). The demographic analysis showed consistent gender distributions,\nwith 50.9% of the population identified as female and 49.1% as male.\nWorking-age populations made up the majority of the affected populations,\nranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods.\nThe study identified strong correlations between urban interface proximity,\nstructural damage, and population exposure. The Palisades and Eaton fires\naffected over 20,000 people each, compared to fewer than 500 in rural events.\nThese findings offer valuable insights for the development of targeted wildfire\nmanagement strategies, particularly in wildland urban interface zones, and\nemphasize the need for age- and gender-conscious approaches in emergency\nresponse planning.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "We present a progress report on the use of normalizing flows for generating\ngauge field configurations in pure SU(N) gauge theories. We discuss how the\nsingular value decomposition can be used to construct gauge-invariant\nquantities, which serve as the building blocks for designing gauge-equivariant\ntransformations of SU(N) gauge links. Using this novel approach, we build\nrepresentative models for the SU(3) Wilson action on a \\( 4^4 \\) lattice with\n\\( \\beta = 1 \\). We train these models and provide an analysis of their\nperformance, highlighting the effectiveness of the new technique for\ngauge-invariant transformations. We also provide a comparison between the\nefficiency of the proposed algorithm and the spectral flow of Wilson loops.",
        "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties.",
        "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels.",
        "In this article, we consider qualified notions of geometric finiteness in\nmapping class groups called parabolically geometrically finite (PGF) and\nreducibly geometrically finite (RGF). We examine several constructions of\nsubgroups and determine when they produce a PGF or RGF subgroup. These results\nprovide a variety of new examples of PGF and RGF subgroups. Firstly, we\nconsider the right-angled Artin subgroups constructed by Koberda and\nClay--Leininger--Mangahas, which are generated by high powers of given elements\nof the mapping class group. We give conditions on the supports of these\nelements that imply the resulting right-angled Artin subgroup is RGF. Secondly,\nwe prove combination theorems which provide conditions for when a collection of\nreducible subgroups, or sufficiently deep finite-index subgroups thereof,\ngenerate an RGF subgroup.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b33",
    "start_title":"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network",
    "start_abstract":"We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment",
        "Revolutionizing Gravitational Potential Analysis: From Clairaut to Lie\n  Groups",
        "Number of partitions of modular integers (with an Appendix by P.\n  Deligne)",
        "A Proof of Lieb--Wehrl Entropy conjecture for $SU(N,1)$",
        "A Bayesian Non-linear Mixed-Effects Model for Accurate Detection of the\n  Onset of Cognitive Decline in Longitudinal Aging Studies",
        "Rationalizability and Monotonocity in Games with Incomplete Information",
        "Predicting the detectability of sulphur-bearing molecules in the solid\n  phase with simulated spectra of JWST instruments",
        "Dialectics of antimicrobial peptides I: common mechanisms of offensive\n  and protecting roles of the peptides",
        "Assessment of the January 2025 Los Angeles County wildfires: A\n  multi-modal analysis of impact, response, and population exposure",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Normalizing flows for SU($N$) gauge theories employing singular value\n  decomposition",
        "White's conjecture for matroids and inner projections",
        "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of\n  Model Prediction and Scientific Understanding of Soil Organic Carbon",
        "Constructing reducibly geometrically finite subgroups of the mapping\n  class group",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain"
      ],
      "abstract":[
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies.",
        "This letter introduces an advanced novel theory for calculating non-linear\nNewtonian hydrostatic perturbations in the density, shape, and gravitational\nfield of fluid stars and planets subjected to external tidal and rotational\nforces. The theory employs a Lie group approach using exponential mappings to\nderive exact differential equations for large gravitational field perturbations\nand the shape function, which describes the finite deformation of the body's\nfigure. This approach lays the foundation for the precise analytic\ndetermination and numerical computation of the induced body's multipole moments\nand Love numbers with any desired degree of accuracy.",
        "For integers $n,k,s$, we give a formula for the number $T(n,k,s)$ of order\n$k$ subsets of the ring $\\mathbb{Z}\/n\\mathbb{Z}$ whose sum of elements is $s$\nmodulo $n$. To do so, we describe explicitly a sequence of matrices $M(k)$, for\npositive integers $k$, such that the size of $M(k)$ is the number of divisors\nof $k$, and for two coprime integers $k_{1},k_{2}$, the matrix $M(k_{1}k_{2})$\nis the Kronecker product of $M(k_{1})$ and $M(k_{2})$. For $s=0, 1, 2$, and for\n$s=k\/2$ when $k$ is even, the sequences $T(n,k,s)$ are related to the number of\nnecklaces with $k$ black beads and $n-k$ white beads, and to Lyndon words. This\nwork begins with empirical determinations of $M(k)$ up to $k=10000$, from which\nwe infer a closed formula that encompasses many entries in the Encyclopedia of\nInteger Sequences. Its proof comes from work on Ramanujan sums, by Ramanathan,\nwith a generalization to wider problems linked to representation theory and\nrecently described by Deligne.",
        "We investigate the sharp functional inequalities for the coherent state\ntransforms of $SU(N,1)$. These inequalities are rooted in Wehrl's definition of\nsemiclassical entropy and his conjecture about its minimum value. Lieb resolved\nthis conjecture in 1978, posing a similar question for Bloch coherent states of\n$SU(2)$. The $SU(2)$ conjecture was settled by Lieb and Solovej in 2014, and\nthe conjecture was extended for a wide class of Lie groups. The generalized\nLieb conjecture has been resolved for several Lie groups, including $SU(N),\\,\nN\\geq2$, $SU(1,1)$, and its $AX+B$ subgroup. With sharp functional inequalities\nfor the coherent state transforms of the group $SU(N,1)$, we confirm this\nLieb-Wehrl entropy conjecture for $SU(N,1),\\, N\\geq2$. Additionally, we explore\nthe Faber-Krahn inequality, which applies to the short-time Fourier transform\nwith a Gaussian window. This inequality was previously proven by Nicola and\nTilli and later extended by Ramos and Tilli to the wavelet transform. In this\npaper, we further extend this result within the framework of the Bergman space\n$\\mathcal A_{\\alpha}$.",
        "Change-point models are frequently considered when modeling phenomena where a\nregime shift occurs at an unknown time. In ageing research, these models are\ncommonly adopted to estimate of the onset of cognitive decline. Yet commonly\nused models present several limitations. Here, we present a Bayesian non-linear\nmixed-effects model based on a differential equation designed for longitudinal\nstudies to overcome some limitations of classical change point models used in\nageing research. We demonstrate the ability of the proposed model to avoid\nbiases in estimates of the onset of cognitive impairment in a simulated study.\nFinally, the methodology presented in this work is illustrated by analysing\nresults from memory tests from older adults who participated in the English\nLongitudinal Study of Ageing.",
        "This paper examines games with strategic complements or substitutes and\nincomplete information, where players are uncertain about the opponents'\nparameters. We assume that the players' beliefs about the opponent's parameters\nare selected from some given set of beliefs. One extreme is the case where\nthese sets only contain a single belief, representing a scenario where the\nplayers' actual beliefs about the parameters are commonly known among the\nplayers. Another extreme is the situation where these sets contain all possible\nbeliefs, representing a scenario where the players have no information about\nthe opponents' beliefs about parameters. But we also allow for intermediate\ncases, where these sets contain some, but not all, possible beliefs about the\nparameters. We introduce an assumption of weakly increasing differences that\ntakes both the choice belief and parameter belief of a player into account.\nUnder this assumption, we demonstrate that greater choice-parameter beliefs\nleads to greater optimal choices. Moreover, we show that the greatest and least\npoint rationalizable choice of a player is increasing in their parameter, and\nthese can be determined through an iterative procedure. In each round of the\niterative procedure, the lowest surviving choice is optimal for the lowest\nchoice-parameter belief, while the greatest surviving choice is optimal for the\nhighest choice-parameter belief.",
        "To date, gas phase observations of sulphur in dense interstellar environments\nhave only constrained the molecular carriers of 1% of its predicted cosmic\nabundance. An additional 5% is known to be locked up in molecular solids in\ndense clouds, leaving the main reservoir of depleted sulphur in the solid phase\nunknown. The spectral resolution and sensitivity of the JWST could make a\nsubstantial difference in detecting part of this missing sulphur, with its\nwavelength coverage that includes vibrational absorption features of the\nS-carriers H2S, OCS, SO2, CS2, SO, CS, and S8. The aim of this study is to\ndetermine whether these molecules may be viable candidates for detection. We\ncarried out new laboratory measurements of the IR absorption spectra of CS2 and\nS8 to update the IR band strength of the most intense CS2 absorption feature at\n6.8 {\\mu}m, as well as to determine that of S8 at 20.3 {\\mu}m for the first\ntime. These data, along with values previously reported in the literature,\nallow us to evaluate which S-bearing species could be potentially detected with\nJWST in interstellar ices. Taking the literature abundances of the major ice\nspecies determined by previous IR observations towards starless cores, LYSOs\nand MYSOs, we generated simulated IR spectra using the characteristics of the\ninstruments on the JWST. Thus, we have been able to establish a case study for\nthree stages of the star formation process. We conclude that the detection of\nS-bearing molecules remains challenging. Despite these obstacles, the detection\nof H2S and potentially SO2 should be possible in regions with favourable\nphysical and chemical conditions. In contrast, S8 would remain undetected.\nAlthough the sensitivity of JWST is insufficient to determine the sulphur\nbudget in the solid state, the detection of an additional icy sulphur compound\n(H2S, SO2) would enable us to elevate our knowledge of sulphur chemistry.",
        "Antimicrobial peptides (AMPs) have intrigued researchers for decades due to\nthe contradiction between their high potential against resistant bacteria and\nthe inability to find a structure-function relationship for the development of\nan effective and non-toxic agent. In the present study and the companion paper\n[Phys. Rev. E (2024)], we performed a comprehensive experimental and\ntheoretical analysis of various aspects of AMP-membrane interactions and\nAMP-induced pore formation. Using the well-known melittin and magainin as\nexamples, we showed, using patch-clamp and fluorescence measurements, that\nthese peptides, even at nanomolar concentrations, modify the membrane by making\nit permeable to protons (and, possibly, water), but not to ions, and protect\nthe membrane from large pore formation after subsequent addition of 20-fold\nhigher concentrations of AMPs. This protective effect is independent of the\nmembrane side (or both sides) of the peptide addition and is determined by the\npeptide-induced deformations of the membrane. Peptides create small,\nH+-permeable pores that incessantly connect the opposing membrane leaflets,\nallowing translocation of peptides and lipids and thus preventing further\ngeneration of large lateral pressure\/tension imbalance. At the same time, such\nan imbalance is a key to the formation of peptide-induced pores at high AMP\nconcentrations, with the main contribution coming from single ion-conducting\nevents rather than stable channel-like structures. Therefore, our results\nsuggest that lowering the AMP concentration, which is a common principle to\nreduce toxicity, may actually make bacteria resistant to AMP. However, a\nprotective pre-treatment with nanomolar concentrations of peptides may be the\nkey to protect eukaryotic cells from the high concentrations of AMPs.",
        "This study presents a comprehensive analysis of four significant California\nwildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts\nthrough multiple dimensions, including land cover change, jurisdictional\nmanagement, structural damage, and demographic vulnerability. Using the\nChebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the\nextent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares.\nOur analysis revealed that shrubland ecosystems were consistently the most\naffected, comprising 57.4-75.8% of burned areas across all events. The\njurisdictional assessment demonstrated varying management complexities, from\nsingular authority (98.7% in the Palisades Fire) to distributed management\nacross multiple agencies. A structural impact analysis revealed significant\ndisparities between urban interface fires (Eaton: 9,869 structures; Palisades:\n8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17\nstructures). The demographic analysis showed consistent gender distributions,\nwith 50.9% of the population identified as female and 49.1% as male.\nWorking-age populations made up the majority of the affected populations,\nranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods.\nThe study identified strong correlations between urban interface proximity,\nstructural damage, and population exposure. The Palisades and Eaton fires\naffected over 20,000 people each, compared to fewer than 500 in rural events.\nThese findings offer valuable insights for the development of targeted wildfire\nmanagement strategies, particularly in wildland urban interface zones, and\nemphasize the need for age- and gender-conscious approaches in emergency\nresponse planning.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "We present a progress report on the use of normalizing flows for generating\ngauge field configurations in pure SU(N) gauge theories. We discuss how the\nsingular value decomposition can be used to construct gauge-invariant\nquantities, which serve as the building blocks for designing gauge-equivariant\ntransformations of SU(N) gauge links. Using this novel approach, we build\nrepresentative models for the SU(3) Wilson action on a \\( 4^4 \\) lattice with\n\\( \\beta = 1 \\). We train these models and provide an analysis of their\nperformance, highlighting the effectiveness of the new technique for\ngauge-invariant transformations. We also provide a comparison between the\nefficiency of the proposed algorithm and the spectral flow of Wilson loops.",
        "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties.",
        "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels.",
        "In this article, we consider qualified notions of geometric finiteness in\nmapping class groups called parabolically geometrically finite (PGF) and\nreducibly geometrically finite (RGF). We examine several constructions of\nsubgroups and determine when they produce a PGF or RGF subgroup. These results\nprovide a variety of new examples of PGF and RGF subgroups. Firstly, we\nconsider the right-angled Artin subgroups constructed by Koberda and\nClay--Leininger--Mangahas, which are generated by high powers of given elements\nof the mapping class group. We give conditions on the supports of these\nelements that imply the resulting right-angled Artin subgroup is RGF. Secondly,\nwe prove combination theorems which provide conditions for when a collection of\nreducible subgroups, or sufficiently deep finite-index subgroups thereof,\ngenerate an RGF subgroup.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation",
    "start_abstract":"In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b36",
        "b33"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
        "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
      ],
      "categories":[
        "cs.CV",
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity\n  Grouping",
        "Coded Deep Learning: Framework and Algorithm",
        "Universal programmable and self-configuring optical filter",
        "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review",
        "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
        "Contracting low degree points on curves",
        "Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction",
        "A study of the Antlion Random Walk",
        "Evolving Skeletons: Motion Dynamics in Action Recognition",
        "B-Call: Integrating Ideological Position and Political Cohesion in\n  Legislative Voting Models",
        "Foliated Plateau problems, geometric rigidity and equidistribution of\n  closed $k$-surfaces",
        "Evidence for J\/$\\psi$ suppression in incoherent photonuclear production",
        "A Machine Learning Approach for Design of Frequency Selective Surface\n  based Radar Absorbing Material via Image Prediction",
        "Detection of chiral spin fluctuations driven by frustration in Mott\n  insulators",
        "Norms in equivariant homotopy theory",
        "MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language\n  Models for Biomedical In-Context Learning",
        "Robust Conformal Outlier Detection under Contaminated Reference Data",
        "Local damage detection in rolling element bearings based on a Single\n  Ensemble Empirical Mode Decomposition",
        "Single-crystalline CrSb(0001) thin films grown by dc magnetron\n  co-sputtering",
        "Parental Guidance: Efficient Lifelong Learning through Evolutionary\n  Distillation",
        "A Fully Self-Synchronized Control for Hybrid Series-Parallel\n  Electronized Power Networks",
        "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond",
        "Navigating Gender Disparities in Communication Research Leadership:\n  Academic Recognition, Career Development, and Compensation",
        "ASKAP and VLASS search for a radio-continuum counterpart of\n  ultra-high-energy neutrino event KM3-230213A",
        "Effective textures from a $[SU(3)]^3$ flavored scalar sector",
        "MAUCell: An Adaptive Multi-Attention Framework for Video Frame\n  Prediction"
      ],
      "abstract":[
        "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https:\/\/github.com\/AIGNLAI\/GDDSG.",
        "The success of deep learning (DL) is often achieved with large models and\nhigh complexity during both training and post-training inferences, hindering\ntraining in resource-limited settings. To alleviate these issues, this paper\nintroduces a new framework dubbed ``coded deep learning'' (CDL), which\nintegrates information-theoretic coding concepts into the inner workings of DL,\nto significantly compress model weights and activations, reduce computational\ncomplexity at both training and post-training inference stages, and enable\nefficient model\/data parallelism. Specifically, within CDL, (i) we first\npropose a novel probabilistic method for quantizing both model weights and\nactivations, and its soft differentiable variant which offers an analytic\nformula for gradient calculation during training; (ii) both the forward and\nbackward passes during training are executed over quantized weights and\nactivations, eliminating most floating-point operations and reducing training\ncomplexity; (iii) during training, both weights and activations are entropy\nconstrained so that they are compressible in an information-theoretic sense\nthroughout training, thus reducing communication costs in model\/data\nparallelism; and (iv) the trained model in CDL is by default in a quantized\nformat with compressible quantized weights, reducing post-training inference\nand storage complexity. Additionally, a variant of CDL, namely relaxed CDL\n(R-CDL), is presented to further improve the trade-off between validation\naccuracy and compression though requiring full precision in training with other\nadvantageous features of CDL intact. Extensive empirical results show that CDL\nand R-CDL outperform the state-of-the-art algorithms in DNN compression in the\nliterature.",
        "We propose an approach to integrated optical spectral filtering that allows\narbitrary programmability, can compensate automatically for imperfections in\nfilter fabrication, allows multiple simultaneous and separately programmable\nfilter functions on the same input, and can configure itself automatically to\nthe problem of interest, for example to filter or reject multiple arbitrarily\nchosen frequencies. The approach exploits splitting the input light into an\narray of multiple waveguides of different lengths that then feed a programmable\ninterferometer array that can also self-configure. It can give spectral\nresponse similar to arrayed waveguide gratings but offers many other filtering\nfunctions, as well as supporting other structures based on non-redundant arrays\nfor precise spectral filtering. Simultaneous filtering also allows, for the\nfirst time to our knowledge, an automatic measurement of the temporal coherency\nmatrix and physical separation into the Karhunen-Lo\\`eve expansion of\ntemporally partially coherent light fields.",
        "In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1\/\\sqrt{n}$.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
        "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https:\/\/huggingface.co\/datasets\/bytedance-research\/ToolHop.",
        "The main result of this article is that all but finitely many points of small\nenough degree on a curve can be written as a pullback of a smaller degree\npoint. The main theorem has several corollaries that yield improvements on\nresults of Kadets and Vogt, Khawaja and Siksek, and Vojta under a slightly\nstronger assumption on the degree of the points.",
        "Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.",
        "This paper combines two significant areas of political science research:\nmeasuring individual ideological position and cohesion. Although both\napproaches help analyze legislative behaviors, no unified model currently\nintegrates these dimensions. To fill this gap, the paper proposes a methodology\ncalled B-Call that combines ideological positioning with voting cohesion,\ntreating votes as random variables. The model is empirically validated using\nroll-call data from the United States, Brazil, and Chile legislatures, which\nrepresent diverse legislative dynamics. The analysis aims to capture the\ncomplexities of voting and legislative behaviors, resulting in a\ntwo-dimensional indicator. This study addresses gaps in current legislative\nvoting models, particularly in contexts with limited party control.",
        "In this note, we survey recent advances in the study of dynamical properties\nof the space of surfaces with constant curvature in three-dimensional manifolds\nof negative sectional curvature. We interpret this space as a two-dimensional\nanalogue of the geodesic flow and explore the extent to which the thermodynamic\nproperties of the latter can be generalized to the surface setting.\nAdditionally, we apply this theory to derive geometric rigidity results,\nincluding the rigidity of the hyperbolic marked area spectrum.",
        "According to quantum chromodynamics, at sufficiently high energy, the\nstructure of hadrons reveals a dynamic equilibrium between gluon splitting and\ngluon recombination -- a phenomenon known as saturation. The process of\ndiffractive photonuclear production of a J\/$\\psi$ vector meson provides a\ndirect insight into the gluon composition of hadrons. The J\/$\\psi$ production\nas a function of momentum transferred in the interaction, quantified by the\nMandelstam-$t$ variable, serves as an excellent probe for studying the\nstructure of hadrons within the impact-parameter plane, because different\nranges in $t$ are sensitive to the dynamics of the gluon field at varying\nspatial size scales. The ALICE collaboration has measured the energy dependence\nof incoherent photonuclear production of J\/$\\psi$ mesons off lead ions, at\n$\\sqrt{s_{\\rm NN}} = 5.02$ TeV, for three Mandelstam-$t$ intervals. The energy\ndependence of the photonuclear cross section at the highest $|t|$ range\nmeasured, $(0.81< |t| <1.44)$ GeV$^2$, is sensitive to subnucleonic structures\nof the Pb target. The increase of the cross section with energy at large $|t|$\nshows evidence of suppression with respect to the increase seen at low $|t|$.\nThe observed pattern of the energy evolution in data is similar to that of\ngluon saturation models.",
        "The paper presents an innovative methodology for designing frequency\nselective surface (FSS) based radar absorbing materials using machine learning\n(ML) technique. In conventional electromagnetic design, unit cell dimensions of\nFSS are used as input and absorption coefficient is then predicted for a given\ndesign. In this paper, absorption coefficient is considered as input to ML\nmodel and image of FSS unit cell is predicted. Later, this image is used for\ngenerating the FSS unit cell parameters. Eleven different ML models are studied\nover a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e.\n(a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid\nsearch regression, (d) Random Forest regression, (e) Decision tree\nclassification, and (f) Decision tree regression) show training accuracy more\nthan 90%. The absorption coefficients with varying frequencies of these\npredicted images are subsequently evaluated using commercial electromagnetic\nsolver. The performance of these ML models is encouraging, and it can be used\nfor accelerating design and optimization of high performance FSS based radar\nabsorbing material for advanced electromagnetic applications in future.",
        "Topologically ordered states, such as chiral spin liquids, have been proposed\nas candidates that host fractionalized excitations. However, detecting chiral\ncharacter or proximity to these non-trivial states remains a challenge.\nResonant Raman scattering can be a powerful tool for detecting chiral\nfluctuations, as the $A_{2g}$ channel probes excitations with broken\ntime-reversal symmetry and local chiral order. Here, we use exact\ndiagonalization to characterize the resonant $A_{2g}$ channel, alongside\ntwo-magnon scattering in $B_{1g}$ and $E_g$ channels, for the Hubbard model on\nlattices with increasing levels of geometric spin frustration, where tuning the\nincident energy near the Mott gap reveals strong chiral spin excitation\nintensity. Increased spin frustration in the Mott insulator results in an\noverall softening of the Raman $A_{2g}$ response, indicating a tendency toward\nlow energy chiral-chiral fluctuations in Mott insulators with magnetic\nfrustration and proximity to chiral spin liquid states that can potentially be\ntuned by external perturbations.",
        "We show that the $\\infty$-category of normed algebras in genuine $G$-spectra,\nas introduced by Bachmann-Hoyois, is modelled by strictly commutative algebras\nin $G$-symmetric spectra for any finite group $G$. We moreover provide an\nanalogous description of Schwede's ultra-commutative global ring spectra in\nhigher categorical terms.\n  Using these new descriptions, we exhibit the $\\infty$-category of\nultra-commutative global ring spectra as a partially lax limit of the\n$\\infty$-categories of genuine $G$-spectra for varying $G$, in analogy with the\nnon-multiplicative comparison of Nardin, Pol, and the second author.\n  Along the way, we establish various new results in parametrized higher\nalgebra, which we hope to be of independent interest.",
        "Objective: To optimize in-context learning in biomedical natural language\nprocessing by improving example selection. Methods: We introduce a novel\nmulti-mode retrieval-augmented generation (MMRAG) framework, which integrates\nfour retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2)\nTop Mode, retrieving the most relevant examples based on similarity; (3)\nDiversity Mode, ensuring variation in selected examples; and (4) Class Mode,\nselecting category-representative examples. This study evaluates MMRAG on three\ncore biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction\n(RE), and Text Classification (TC). The datasets used include BC2GM for gene\nand protein mention recognition (NER), DDI for drug-drug interaction extraction\n(RE), GIT for general biomedical information extraction (RE), and HealthAdvice\nfor health-related text classification (TC). The framework is tested with two\nlarge language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever,\nMedCPT, BGE-Large) to assess performance across different retrieval strategies.\nResults: The results from the Random mode indicate that providing more examples\nin the prompt improves the model's generation performance. Meanwhile, Top mode\nand Diversity mode significantly outperform Random mode on the RE (DDI) task,\nachieving an F1 score of 0.9669, a 26.4% improvement. Among the three\nretrievers tested, Contriever outperformed the other two in a greater number of\nexperiments. Additionally, Llama 2 and Llama 3 demonstrated varying\ncapabilities across different tasks, with Llama 3 showing a clear advantage in\nhandling NER tasks. Conclusion: MMRAG effectively enhances biomedical\nin-context learning by refining example selection, mitigating data scarcity\nissues, and demonstrating superior adaptability for NLP-driven healthcare\napplications.",
        "Conformal prediction is a flexible framework for calibrating machine learning\npredictions, providing distribution-free statistical guarantees. In outlier\ndetection, this calibration relies on a reference set of labeled inlier data to\ncontrol the type-I error rate. However, obtaining a perfectly labeled inlier\nreference set is often unrealistic, and a more practical scenario involves\naccess to a contaminated reference set containing a small fraction of outliers.\nThis paper analyzes the impact of such contamination on the validity of\nconformal methods. We prove that under realistic, non-adversarial settings,\ncalibration on contaminated data yields conservative type-I error control,\nshedding light on the inherent robustness of conformal methods. This\nconservativeness, however, typically results in a loss of power. To alleviate\nthis limitation, we propose a novel, active data-cleaning framework that\nleverages a limited labeling budget and an outlier detection model to\nselectively annotate data points in the contaminated reference set that are\nsuspected as outliers. By removing only the annotated outliers in this\n``suspicious'' subset, we can effectively enhance power while mitigating the\nrisk of inflating the type-I error rate, as supported by our theoretical\nanalysis. Experiments on real datasets validate the conservative behavior of\nconformal methods under contamination and show that the proposed data-cleaning\nstrategy improves power without sacrificing validity.",
        "A Single Ensemble Empirical Mode Decomposition (SEEMD) is proposed for\nlocating the damage in rolling element bearings. The SEEMD does not require a\nnumber of ensembles from the addition or subtraction of noise every time while\nprocessing the signals. The SEEMD requires just a single sifting process of a\nmodified raw signal to reduce the computation time significantly. The other\nadvantage of the SEEMD method is its success in dealing with non-Gaussian or\nnon-stationary perturbing signals. In SEEMD, initially, a fractional Gaussian\nnoise (FGN) is added to the raw signal to emphasize on high frequencies of the\nsignal. Then, a convoluted white Gaussian noise is multiplied to the resulting\nsignal which changes the spectral content of the signal which helps in\nextraction of the weak periodic signal. Finally, the obtained signal is\ndecomposed by using a single sifting process. The proposed methodology is\napplied to the raw signals obtained from the mining industry. These signals are\ndifficult to analyze since cyclic impulsive components are obscured by noise\nand other interference. Based on the results, the proposed method can\neffectively detect the fault where the signal of interest (SOI) has been\nextracted with good quality.",
        "The recent discovery of altermagnetism has sparked renewed interest in the\ngrowth of epitaxial films of the NiAs-phase polymorph of CrSb. This paper\ndescribes the magnetron sputtering-based fabrication and characterization of\nhigh-quality single crystalline CrSb(0001) thin films supported by an\nisostructural non-magnetic PtSb buffer. X-ray diffraction and scanning\ntransmission electron microscopy show that the films are phase-pure and possess\na very high crystalline quality (mosaicity ~0.05 deg), while also being free of\nextended crystallographic defects. Both scanning electron microscopy and atomic\nforce microscopy confirm their smooth and homogeneous topography. Additionally,\nthe elemental composition of our films was found to be close to stoichiometric\nvia electron probe microanalysis and X-ray fluorescence. Thus, the developed\nsamples represent an ideal platform for further investigation of the material\nproperties of CrSb.",
        "Developing robotic agents that can perform well in diverse environments while\nshowing a variety of behaviors is a key challenge in AI and robotics.\nTraditional reinforcement learning (RL) methods often create agents that\nspecialize in narrow tasks, limiting their adaptability and diversity. To\novercome this, we propose a preliminary, evolution-inspired framework that\nincludes a reproduction module, similar to natural species reproduction,\nbalancing diversity and specialization. By integrating RL, imitation learning\n(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents\ncontinuously through complex tasks. This approach promotes adaptability,\ninheritance of useful traits, and continual learning. Agents not only refine\ninherited skills but also surpass their predecessors. Our initial experiments\nshow that this method improves exploration efficiency and supports open-ended\nlearning, offering a scalable solution where sparse reward coupled with diverse\nterrain environments induces a multi-task setting.",
        "The hybrid series-parallel system is the final form of the power\nelectronics-enabled power system, which combines the advantages of both series\nand parallel connections. Although self-synchronization of parallel-type and\nseries-type systems is well known, self-synchronization of hybrid systems\nremains unrevealed. To fill in this gap, a fully self-synchronized control for\nhybrid series-parallel system is proposed in this paper. Based on the\nself-synchronization mechanism of power angle in parallel-type system and power\nfactor angle in series-type system, a decentralized control strategy by\nintegration of power droop and power factor angle droop can realize\nself-synchronization and power balancing of each module in the hybrid system.",
        "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.",
        "This study examines gender disparities in communication research through\ncitation metrics, authorship patterns, team composition, and faculty salaries.\nUsing data from 62,359 papers across 121 communication journals, we find that\nwhile female authors are increasingly represented, citation gaps persist, with\nsole-authored papers by women receiving fewer citations than those by men,\nespecially in smaller teams. Team composition analysis reveals a tendency\ntoward gender homophily, with single-gender teams being more common. In top\nU.S. communication journals, female authors face underrepresentation and\ncitation disparities favoring male authors. Salary analysis from leading U.S.\npublic universities shows that female faculty earn lower salaries at the\nAssistant Professor level, though disparities lessen at higher ranks. These\nfindings highlight the need for greater efforts to promote gender equity\nthrough inclusive collaboration, equitable citation practices, and fair\ncompensation.",
        "We present the results of an Australian Square Kilometre Array Pathfinder\n(ASKAP) 944 MHz and Very Large Array Sky Survey (VLASS) 3~GHz search for a\nradio-continuum counterpart of the recent ultra-high-energy (UHE) neutrino\nevent, KM3-230213A. Using (ASKAP), we catalog 1052 radio sources within the\n1.5$^\\circ$ radius search area (68% certainty region) around the particle's\ncalculated origin, 10 of which we classify as blazar candidates based on their\nradio spectra. The most prominent radio source in the search area is the nearby\nspiral galaxy UGCA 127 (nicknamed Phaedra, From Greek: $\\phi\\alpha\ni\\delta\\rho\\alpha$, a Cretan princess of Greek Mythology, derived from\nPhaidros, Greek: ${\\phi}{\\alpha}{\\iota}{\\delta}{\\rho}o{\\varsigma}$, meaning\n'bright'.). Its non-thermal radio spectrum classifies it as a non-blazar active\ngalactic nucleus (AGN). We also present an extended radio source, WISEA\nJ061715.89-075455.4 (nicknamed Hebe, From Greek: $H{\\beta}{\\eta}$, the Greek\ngoddess of youth.), located only ~7' from the geometric center of the search\narea, with a very unusual highly polarized compact component. Finally, we\npresent a strong radio source, EMU J062248-072246 (nicknamed Narcissus, From\nGreek $N{\\alpha}{\\rho}{\\kappa}{\\iota}{\\sigma}{\\sigma}o{\\zeta}$ was a\nself-absorbed hunter from Thespiae in Boeotia.), which has a maximum\nself-absorption spectral slope of +2.5 at low frequencies, and exhibits ~25%\nflux density variability over the ~5-year VLASS 3~GHz survey.",
        "Current constraints on flavor-changing neutral currents (FCNCs) strongly\nindicate that any new physics emerging at the 1-10 TeV scale must adhere to the\nMinimal Flavor Violation (MFV) principle, where Yukawa couplings are the sole\nsources of flavor violation. In this work, we present a model inspired by a\ngauged $SU(3)$ flavor symmetry that dynamically generates leptonic Yukawa\nmatrices through effective operators. The model incorporates a scalar sector\nwith two sets of flavons, characterized by their vacuum expectation values\n(VEVs), which govern the suppression scale of the Yukawa couplings and the\nhierarchy of neutrino masses. By leveraging phenomenologically viable Yukawa\ntextures, we derive restrictions on the flavon VEVs and demonstrate the\ncompatibility of the model with experimental neutrino oscillation data.\nFurthermore, the model predicts at least one neutrino mass to be strongly\nsuppressed, consistent with the normal mass ordering and experimental upper\nbounds. This framework provides a robust mechanism for dynamically generating\nneutrino masses and mixing while addressing key challenges in leptonic flavor\nphysics, such as FCNC suppression and CP-violating phases.",
        "Temporal sequence modeling stands as the fundamental foundation for video\nprediction systems and real-time forecasting operations as well as anomaly\ndetection applications. The achievement of accurate predictions through\nefficient resource consumption remains an ongoing issue in contemporary\ntemporal sequence modeling. We introduce the Multi-Attention Unit (MAUCell)\nwhich combines Generative Adversarial Networks (GANs) and spatio-temporal\nattention mechanisms to improve video frame prediction capabilities. Our\napproach implements three types of attention models to capture intricate motion\nsequences. A dynamic combination of these attention outputs allows the model to\nreach both advanced decision accuracy along with superior quality while\nremaining computationally efficient. The integration of GAN elements makes\ngenerated frames appear more true to life therefore the framework creates\noutput sequences which mimic real-world footage. The new design system\nmaintains equilibrium between temporal continuity and spatial accuracy to\ndeliver reliable video prediction. Through a comprehensive evaluation\nmethodology which merged the perceptual LPIPS measurement together with classic\ntests MSE, MAE, SSIM and PSNR exhibited enhancing capabilities than\ncontemporary approaches based on direct benchmark tests of Moving MNIST, KTH\nAction, and CASIA-B (Preprocessed) datasets. Our examination indicates that\nMAUCell shows promise for operational time requirements. The research findings\ndemonstrate how GANs work best with attention mechanisms to create better\napplications for predicting video sequences."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study",
    "start_abstract":"In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "What is a cell type, really? The quest to categorize life's myriad forms."
      ],
      "abstract":[
        "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Teacher-student training improves accuracy and efficiency of machine\n  learning inter-atomic potentials",
        "Partitions of unity and barycentric algebras",
        "Adiabatic Pumping of Orbital Magnetization by Spin Precession",
        "Non-positive energy quasidistributions in coherent collision models",
        "New properties of length-extremals in free step-2 rank-4 Carnot groups",
        "Covariant photon current",
        "PyClustrPath: An efficient Python package for generating clustering\n  paths with GPU acceleration",
        "Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced\n  Physical Demand",
        "Period Analysis of Eclipsing Cataclysmic Variable Stars",
        "OpenGERT: Open Source Automated Geometry Extraction with Geometric and\n  Electromagnetic Sensitivity Analyses for Ray-Tracing Propagation Models",
        "Black holes inside cosmic voids",
        "Poisson Vertex Algebras and Three-Dimensional Gauge Theory",
        "Investigating the Effects of Atmospheric Stratification on Coronal\n  Active Region Field Modelling",
        "Spall failure of alumina at high-strain rates using femtosecond laser\n  experiments and high-fidelity molecular dynamics simulations",
        "Bounded conciseness in the space of marked groups",
        "An open-closed Deligne-Mumford field theory associated to a Lagrangian\n  submanifold",
        "2-Adic quantum mechanics, continuous-time quantum walks, and the space\n  discreteness",
        "Photometric Decision-Making During the Dawn Choruses of Cicadas",
        "Self-ion irradiation effects on nanoindentation-induced plasticity of\n  crystalline iron: A joint experimental and computational study",
        "Exploring Large Language Models (LLMs) through interactive Python\n  activities",
        "Towards robust gravitational wave detections from individual\n  supermassive black hole binaries",
        "Elucidating the Dark Energy and Dark Matter Phenomena Within the\n  Scale-Invariant Vacuum (SIV) Paradigm",
        "Synthetic Data for Portfolios: A Throw of the Dice Will Never Abolish\n  Chance",
        "Misconceptions in Neutrino Oscillations in presence of a non-Unitary\n  Mixing",
        "Kernels, Distances, and Bridges",
        "Fabrication of Fibers with Complex Features Using Thermal Drawing of\n  3D-Printed Preforms",
        "Stueckelberg field and Cosmology",
        "Helfrich cylinders -- instabilities, bifurcations and amplitude\n  equations",
        "An exposition of recent list-size bounds of FRS Codes"
      ],
      "abstract":[
        "Machine learning inter-atomic potentials (MLIPs) are revolutionizing the\nfield of molecular dynamics (MD) simulations. Recent MLIPs have tended towards\nmore complex architectures trained on larger datasets. The resulting increase\nin computational and memory costs may prohibit the application of these MLIPs\nto perform large-scale MD simulations. Here, we present a teacher-student\ntraining framework in which the latent knowledge from the teacher (atomic\nenergies) is used to augment the students' training. We show that the\nlight-weight student MLIPs have faster MD speeds at a fraction of the memory\nfootprint compared to the teacher models. Remarkably, the student models can\neven surpass the accuracy of the teachers, even though both are trained on the\nsame quantum chemistry dataset. Our work highlights a practical method for\nMLIPs to reduce the resources required for large-scale MD simulations.",
        "Barycentric coordinates provide solutions to the problem of expressing an\nelement of a compact convex set as a convex combination of a finite number of\nextreme points of the set. They have been studied widely within the geometric\nliterature, typically in response to the demands of interpolation, numerical\nanalysis and computer graphics. In this note we bring an algebraic perspective\nto the problem, based on barycentric algebras. We focus on the discussion of\nrelations between different subclasses of partitions of unity, one arising in\nthe context of barycentric coordinates, based on the tautological map\nintroduced by Guessab.",
        "We propose adiabatic pumping of orbital magnetization driven by coherent spin\nprecession, facilitating the rectification of this precession. The orbital\nmagnetization originates from the adiabatic evolution of valence electrons with\na topological bulk contribution expressed as a Chern-Simons form. When the\nprecession cone angle of spin $\\mathbf{S}$ is small, the resulting\nmagnetization is proportional to $\\mathbf{S}\\times \\dot{\\mathbf{S}}$,\ncontributing to the magnon Zeeman effect. With a large cone angle, the\nmagnetization can reach its natural unit, $e\/T$, in an antiferromagnetic\ntopological insulator with $e$ as the elementary charge and $T$ as the\nprecession period. This significant magnetization is related to the global\nproperties of the electronic geometric phases in the parameter space spanned by\n$\\mathbf{S}$ and momentum $\\mathbf{k}$. When the pumped magnetization is\ninhomogeneous, induced by spin textures or electronic topological phase\ndomains, a dissipationless charge current is also pumped. At last, we discuss\nthe boundary contributions from the spin-driving edge states, which are\nintricately linked to the gauge-dependent quantum uncertainty of the\nChern-Simons form.",
        "We determine the Kirkwood-Dirac quasiprobability (KDQ) distribution\nassociated to the stochastic instances of internal energy variations for the\nquantum system and environment particles in coherent Markovian collision\nmodels. In the case the interactions between the quantum system and the\nparticles do not conserve energy, the KDQ of the non-energy-preserving\nstochastic work is also derived. These KDQ distributions can account for\nnon-commutativity, and return the unperturbed average values and variances for\na generic interaction-time, and generic local initial states of the quantum\nsystem and environment particles. Using this nonequilibrium-physics approach,\nwe certify the conditions under which the collision process of the model\nexhibits quantum traits, and we quantify the rate of energy exchanged by the\nquantum system by looking at the variance of the KDQ energy distributions.\nFinally, we propose an experimental test of our results on a superconducting\nquantum circuit implementing a qubit system, with microwave photons\nrepresenting the environment particles.",
        "In the free, step-2, rank-4 sub-Riemannian Carnot group, we give a clean\nexpression for length-extremals, we provide an explicit equation for conjugate\npoints, we relate it with the conjectured cut-locus of the origin. Finally, we\ngive some upper estimates for the cut-time of extremals.",
        "An inhomogeneous continuity equation for the photon four-current operator,\n$\\widehat{J}_{p}$, was derived in [M. Hawton, Phys. Rev. A, 109, 062221\n(2024)]. If the electromagnetic potential operator, $\\widehat{A}% =\\left(\n\\widehat{\\phi}\/c,\\widehat{\\mathbf{A}}\\right) $, is covariant then\n$\\widehat{J}_{p}$ is covariant and the continuity equation is invariant. Here\nwe start with the standard Lagrangian in a Lorentz invariant gauge and quantize\nboth transverse and longitudinal modes. The scalar potential\n$\\widehat{\\phi}=c\\widehat{A}_{\\Vert}$ is not independently second quantized, so\nall modes have positive definite norm. The continuity equation is generalized\nby separating the material source current into a nonabsorbing term describing\npropagation in a lossless transmission line and localized single photon\nemission and detection terms that do not require nonlocal separation of\ntransverse and longitudinal modes.",
        "Convex clustering is a popular clustering model without requiring the number\nof clusters as prior knowledge. It can generate a clustering path by\ncontinuously solving the model with a sequence of regularization parameter\nvalues. This paper introduces {\\it PyClustrPath}, a highly efficient Python\npackage for solving the convex clustering model with GPU acceleration. {\\it\nPyClustrPath} implements popular first-order and second-order algorithms with a\nclean modular design. Such a design makes {\\it PyClustrPath} more scalable to\nincorporate new algorithms for solving the convex clustering model in the\nfuture. We extensively test the numerical performance of {\\it PyClustrPath} on\npopular clustering datasets, demonstrating its superior performance compared to\nthe existing solvers for generating the clustering path based on the convex\nclustering model. The implementation of {\\it PyClustrPath} can be found at:\nhttps:\/\/github.com\/D3IntOpt\/PyClustrPath.",
        "We present a hybrid brain-machine interface (BMI) that integrates\nsteady-state visually evoked potential (SSVEP)-based EEG and facial EMG to\nimprove multimodal control and mitigate fatigue in assistive applications.\nTraditional BMIs relying solely on EEG or EMG suffer from inherent limitations;\nEEG-based control requires sustained visual focus, leading to cognitive\nfatigue, while EMG-based control induces muscular fatigue over time. Our system\ndynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP\nsignals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize\ncontrol based on task demands. In a virtual turtle navigation task, the hybrid\nsystem achieved task completion times comparable to an EMG-only approach, while\n90% of users reported reduced or equal physical demand. These findings\ndemonstrate that multimodal BMI systems can enhance usability, reduce strain,\nand improve long-term adherence in assistive technologies.",
        "We have performed a study of the orbital properties of seven eclipsing\ncataclysmic variable (CV) binary systems by analyzing photometric time series\nfrom the Transiting Exoplanet Survey Satellite (TESS). We employed Python code\nto determine the eclipse epochs and orbital periods for each system, and\nconstructed O-C diagrams from observed and predicted eclipse epochs. By\nanalyzing the O-C diagrams of our target CVs, we have constrained values for\nchanges in orbital period with time. Our targets include a sample of sources\nfrom each class of non-magnetic, eclipsing CVs: dwarf novae variables, Z Cam\ntype, and U Gem subclasses. We include in our study classical novae variables,\nnova-like variables (including the VY Scl and UX UMa subclasses), and recurrent\nnovae variable stars. We approached this project with goals of developing time\nseries analysis techniques for future undergraduate-level studies of eclipsing\nCVs, and how they may contribute to the understanding of their orbital\nevolution.",
        "Accurate RF propagation modeling in urban environments is critical for\ndeveloping digital spectrum twins and optimizing wireless communication\nsystems. We introduce OpenGERT, an open-source automated Geometry Extraction\ntool for Ray Tracing, which collects and processes terrain and building data\nfrom OpenStreetMap, Microsoft Global ML Building Footprints, and USGS elevation\ndata. Using the Blender Python API, it creates detailed urban models for\nhigh-fidelity simulations with NVIDIA Sionna RT. We perform sensitivity\nanalyses to examine how variations in building height, position, and\nelectromagnetic material properties affect ray-tracing accuracy. Specifically,\nwe present pairwise dispersion plots of channel statistics (path gain, mean\nexcess delay, delay spread, link outage, and Rician K-factor) and investigate\nhow their sensitivities change with distance from transmitters. We also\nvisualize the variance of these statistics for selected transmitter locations\nto gain deeper insights. Our study covers Munich and Etoile scenes, each with\n10 transmitter locations. For each location, we apply five types of\nperturbations: material, position, height, height-position, and all combined,\nwith 50 perturbations each. Results show that small changes in permittivity and\nconductivity minimally affect channel statistics, whereas variations in\nbuilding height and position significantly alter all statistics, even with\nnoise standard deviations of 1 meter in height and 0.4 meters in position.\nThese findings highlight the importance of precise environmental modeling for\naccurate propagation predictions, essential for digital spectrum twins and\nadvanced communication networks. The code for geometry extraction and\nsensitivity analyses is available at github.com\/serhatadik\/OpenGERT\/.",
        "This study examines the gravitational and thermodynamic properties of static,\nspherically symmetric black holes within cosmic voids -- vast underdense\nregions of the universe. By deriving a novel solution based on a universal\ndensity profile for voids, we analyze its spacetime structure, which reveals\ntwo horizons: One of the black hole and the other related to the de Sitter-like\nbehavior. As the void approaches a perfect vacuum, the black hole horizon\ndiminishes, tending to that of the Schwarzschild solution, while the outer\nhorizon increases. We also study the solution stability via sound speed of the\nfluid, as well as the thermodynamic properties, including Hawking temperature,\nevaporation time, entropy, and specific heat. Our results show that as the void\nempties, the Hawking temperature rises, shortening evaporation times. The\nentropy follows the area's law and specific heat exhibits a minimum for a given\nblack hole size, indicating a thermal transition and highlighting the role of\nvoids in the black hole evolution. These findings offer new insights into the\nrelationship between local gravitational collapse and large-scale cosmic\nstructure, enhancing our understanding of the black hole behavior in underdense\nenvironments. We also provide a glimpse of a potential thermodynamic\ninteraction between the event horizon and the cosmological horizon.",
        "We introduce a mixed holomorphic-topological gauge theory in three dimensions\nassociated to a (freely generated) Poisson vertex algebra. The\n$\\lambda$-bracket of the PVA plays the role of the structure constants of the\ngauge algebra and the gauge invariance of the theory holds if and only if the\n$\\lambda$-bracket Jacobi identity is satisfied. We show that the\nholomorphic-topological symmetry of the theory enhances to full topological\nsymmetry if the Poisson vertex algebra contains a Virasoro element. We outline\nexamples associated to PVAs of $\\mathcal{W}$-type and demonstrate their\nconnections to various versions of $3d$ gravity. We expect the\nthree-dimensional Poisson sigma model to play an important role in the\ndeformation quantization of Poisson vertex algebras.",
        "Understanding the evolution of the complex magnetic fields found in solar\nactive regions is an active area of research. There are numerous models for\nsuch fields which range in their complexity due to the number of known physical\neffects included in them, the one common factor being they all extrapolate the\nfield up from the photosphere. In this study we focus on the fact that, above\nthe photosphere, and below the corona, lies the relatively cool and dense\nchromosphere -- which is often neglected in coronal models due to it being\ncomparatively thin and difficult hard to model. We isolate and examine the\neffect including this boundary layer has on a 2.5D class of driven MHD models\nof an active region eruption. We find that it can result in significant changes\nto the dynamics of an erupting field far higher in the atmosphere than the\nchromosphere itself, generally delaying eruption and increasing the magnetic\nenergy released in each eruption. We also test whether these effects can be\napproximated using a variation of the more computationally efficient\nmagnetofrictional model, finding a number of simple adaptations of the standard\nmagnetofrictional model capture the effect the chromospheric stratification\nwell.",
        "Ceramic materials are widely used in high-strain-rate applications due to\ntheir exceptional strength-to-weight ratio. However, under these extreme\nconditions, spall failure becomes a critical concern, which is driven by a\nlarge hydrostatic tensile stress state. This study introduces a novel two-laser\nsetup to generate controlled hydrostatic stress states at specific locations\nwithin test specimens. By inducing and manipulating shock wave interactions, we\nachieve large hydrostatic compressive and tensile stresses at very\nhigh-strain-rates, enabling the controlled nucleation and growth of nanovoids\nleading to spall failure. Our experiments demonstrate that shock wave\ninterference can precisely trigger spallation at arbitrary locations in the\nspecimen thickness. To further validate our approach, we investigate alumina\nspall failure using molecular dynamics (MD) simulations with a custom-designed\ngraph neural network potential. The MD results show strong agreement with\nexperimentally estimated spall strength. These findings highlight the potential\nof the two-laser technique as a powerful tool for studying the early stages of\nspall failure in ceramics, paving the way for advanced materials testing\nmethodologies.",
        "We prove that bounded conciseness is a closed property in the space of marked\ngroups. As a consequence, we reformulate a conjecture of Fern\\'andez-Alcober\nand Shumyatsky [7] about conciseness in the class of residually finite groups.",
        "Let $L \\subset X$ be a compact embedded Lagrangian in a compact symplectic\nmanifold. We present the moduli spaces of holomorphic maps of arbitrary genus\nwith boundary on $L$ as a global Kuranishi chart, generalising the work of\nAbouzaid-McLean-Smith and Hirschi-Swaminathan. We use this to define an\nopen-closed Deligne-Mumford theory whose open genus zero part is the Fukaya\n$A_\\infty$ algebra associated to $L$, and whose closed part gives the\nGromov--Witten theory of $X$. Combined with results of Costello, this has\napplications in obtaining Gromov--Witten invariants from the Fukaya category.",
        "Using techniques of p-adic analysis, it is possible to formulate a rigorous\nversion of the quantum mechanics (QM), in the sense of Dirac-von Neumann,\nconsistent with the existence of the Planck length. Such a model cannot be\nformulated if we use R^{3} as a model for physical space. The experimental\ntestability of physical theories at the Planck scale is currently impossible.\nHere, we provide an indirect, theoretical argument that shows that the p-adic\nQM has physical content. We show that a large class of Schr\\\"odinger equations\ndescribes the scaling limits of continuous-time quantum walks on graphs\n(stochastic automata). These quantum walks appear as fundamental tools in\nquantum computing. We conjecture that this interpretation is valid in a general\nframework. The `new theory' does not have Lorentz symmetry, and the Einstein\ncausality is violated. This fact does not contradict the so-called\nno-communication theorem; such a result requires as a primary hypothesis that\nR^{4} be a valid model for space-time at the Planck scale. Thus, the\nno-communication theorem under the discreteness of the space is an open\nproblem.",
        "We report the first quantitative study of the onset of dawn choruses of\ncicadas in several natural habitats. A time-frequency analysis of the\nacoustical signals is used to define an order parameter for the development of\ncollective singing. The ensemble of recordings reveals that the chorus onset\ntimes accurately track the changing sunrise times over the course of many\nweeks, occurring within civil twilight at a solar elevation of -$3.8^\\circ \\pm\n0.2^\\circ$. Despite day-to-day variations in the amplitude of fully developed\nchoruses, the order parameter data collapse to a common sigmoidal curve when\nscaled by those amplitudes and shifted by the onset time, revealing a\ncharacteristic rise time of ~60 s for a chorus to reach saturation amplitude.\nThe results are used to obtain the cumulative distribution function of singing\nas a function of ground illumination, from which is obtained a generalized\nsusceptibility which exhibits a narrow peak with a half-width of $\\sim\\! 12\\%$.\nThe variance of the order parameter exhibits a similar peak, suggesting that a\ngeneralized fluctuation-dissipation theorem holds for this system. A model of\ndecision-making under ramps of a control parameter is developed and can achieve\na quantitative match to the data. It suggest that sharpness of the\nsusceptibility peak reflects cooperative decision-making arising from acoustic\ncommunication.",
        "In this paper, experimental work is supported by multi-scale numerical\nmodeling to investigate nanomechanical response of pristine and ion irradiated\nwith Fe2+ ions with energy 5 MeV high purity iron specimens by nanoindentation\nand Electron Backscatter Diffraction. The appearance of a sudden displacement\nburst that is observed during the loading process in the load-displacement\ncurves is connected with increased shear stress in a small subsurface volume\ndue to dislocation slip activation and mobilization of pre-existing\ndislocations by irradiation. The molecular dynamics (MD) and 3D-discrete\ndislocation dynamics (3D-DDD) simulations are applied to model geometrically\nnecessary dislocations (GNDs) nucleation mechanisms at early stages of\nnanoindentation test; providing an insight to the mechanical response of the\nmaterial and its plastic instability and are in a qualitative agreement with\nGNDs density mapping images. Finally, we noted that dislocations and defects\nnucleated are responsible the material hardness increase, as observed in\nrecorded load-displacement curves and pop-ins analysis.",
        "This paper presents an approach to introduce physics students to the basic\nconcepts of Large Language Models (LLMs) using Python-based activities in\nGoogle Colab. The teaching strategy integrates active learning strategies and\ncombines theoretical ideas with practical, physics-related examples. Students\nengage with key technical concepts, such as word embeddings, through hands-on\nexploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot\nof attention in 2019 for its ability to generate coherent and plausible text\nfrom simple prompts.\n  The activities highlight how words acquire meaning and how LLMs predict\nsubsequent tokens by simulating simplified scenarios related to physics. By\nfocusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles\nunderlying modern LLMs, such as semantic representation and contextual\nprediction. Through interactive experimenting in Google Colab, students observe\nthe relationship between model parameters (such as temperature) in GPT-2 and\noutput behaviour, understand scaling laws relating data quantity to model\nperformance, and gain practical insights into the predictive capabilities of\nLLMs. This approach allows students to begin to understand how these systems\nwork by linking them to physics concepts - systems that will shape their\nacademic studies, professional careers and roles in society.",
        "The recent discovery of the stochastic gravitational-wave background via\npulsar timing arrays will likely be followed by the detection of individual\nblack hole binaries that stand out above the background. However, to\nconfidently claim the detection of an individual binary, we need not only more\nand better data, but also more sophisticated analysis techniques. In this\npaper, we develop two new approaches that can help us more robustly ascertain\nif a candidate found by a search algorithm is indeed an individual supermassive\nblack hole binary. One of these is a coherence test that directly compares the\nfull signal model to an incoherent version of that. The other is a model\nscrambling approach that builds null distributions of our detection statistic\nand compares that with the measured value to quantify our confidence in signal\ncoherence. Both of these rely on finding the coherence between pulsars\ncharacteristic to gravitational waves from a binary system. We test these\nmethods on simple simulated datasets and find that they work well in correctly\nidentifying both true gravitational waves and false positives. However, as\nexpected for such a flexible and simple signal model, confidently identifying\nsignal coherence is significantly harder than simply finding a candidate in\nmost scenarios. Our analyses also indicate that the confidence with which we\ncan identify a true signal depends not only on the signal-to-noise ratio, but\nalso on the number of contributing pulsars and the amount of frequency\nevolution shown by the signal.",
        "The enigmatic phenomenon of dark energy (DE) is regarded as the elusive\nentity driving the accelerated expansion of our Universe. A plausible candidate\nfor DE is the non-zero Einstein Cosmological Constant $\\Lambda_{E}$ manifested\nas a constant energy density of the vacuum, yet it seemingly defies\ngravitational effects. In this work, we interpret the non-zero $\\Lambda_{E}$\nthrough the lens of scale-invariant cosmology. We revisit the conformal scale\nfactor $\\lambda$ and its defining equations within the Scale-Invariant Vacuum\n(SIV) paradigm. Furthermore, we address the profound problem of the missing\nmass across galactic and extragalactic scales by deriving an MOND-like\nrelation, $g \\sim \\sqrt{a_0\\,g_N}$, within the SIV context. Remarkably, the\nvalues obtained for $\\Lambda_{E}$ and the MOND fundamental acceleration, $a_0$,\nalign with observed magnitudes, specifically, $a_0 \\approx 10^{-10} \\,\n\\mathrm{m} \\, \\mathrm{s}^{-2}$ and $\\Lambda_{E} \\approx 1.8 \\times 10^{-52} \\,\n\\mathrm{m}^{-2}$. Moreover, we propose a novel early dark energy term,\n$\\tilde{T}_{\\mu\\nu} \\sim \\kappa H$, within the SIV paradigm, which holds\npotential relevance for addressing the Hubble tension.\n  Keywords: cosmology; theory; dark energy; dark matter; MOND; Weyl integrable\ngeometry.",
        "Simulation methods have always been instrumental in finance, and data-driven\nmethods with minimal model specification, commonly referred to as generative\nmodels, have attracted increasing attention, especially after the success of\ndeep learning in a broad range of fields. However, the adoption of these models\nin financial applications has not kept pace with the growing interest, probably\ndue to the unique complexities and challenges of financial markets. This paper\naims to contribute to a deeper understanding of the limitations of generative\nmodels, particularly in portfolio and risk management. To this end, we begin by\npresenting theoretical results on the importance of initial sample size, and\npoint out the potential pitfalls of generating far more data than originally\navailable. We then highlight the inseparable nature of model development and\nthe desired use case by touching on a paradox: generic generative models\ninherently care less about what is important for constructing portfolios (in\nparticular the long-short ones). Based on these findings, we propose a pipeline\nfor the generation of multivariate returns that meets conventional evaluation\nstandards on a large universe of US equities while being compliant with\nstylized facts observed in asset returns and turning around the pitfalls we\npreviously identified. Moreover, we insist on the need for more delicate\nevaluation methods, and suggest, through an example of mean-reversion\nstrategies, a method designed to identify poor models for a given application\nbased on regurgitative training, i.e. retraining the model using the data it\nhas itself generated, which is commonly referred to in statistics as\nidentifiability.",
        "Deviations from unitarity of the CKM matrix in the quark sector are\nconsidered excellent windows to probe physics beyond the Standard Model. In its\nleptonic counterpart, the PMNS matrix, these searches are particularly\nmotivated, as the new physics needed to generate neutrino masses often leads to\nnon-unitary mixing among the standard neutrinos. It is then interesting to\nconsider how neutrino oscillations are affected in such scenario. This simple\nquestion is, however, subject to several subtleties: What is the correct way to\ndefine oscillation probabilities for a non-unitary mixing matrix? Do these\nprobabilities add up to one? Does a non-unitary mixing matrix lead to\nobservable flavor transitions at zero distance? What is the interplay between\nunitarity constraints obtained from neutrino oscillations and from electroweak\nprecision data? This work aims to shed light on these issues and to clarify the\ncorresponding misconceptions commonly found in the literature. We also compile\nupdated bounds from neutrino oscillation searches to compare with those from\nflavour and electroweak precision observables.",
        "The purpose of this paper is to study more general real-valued functions of\ntwo variables than just metrics on a set X. We concentrate mainly on the\nclasses of distances and almost distances. We also introduce the notion of a\nbridge on the disjoint union of two sets and show that it induces a symmetric\ndistance on the disjoint union.",
        "High-aspect-ratio polymer materials are widely utilized in applications\nranging from everyday materials such as clothing to specialized equipment in\nindustrial and medical fields. Traditional fabrication methods, such as\nextrusion and molding, face challenges in integrating diverse materials and\nachieving complex geometries. Additionally, these methods are limited in their\nability to provide low-cost and rapid prototyping, which are critical for\nresearch and development processes. In this work, we investigated the use of\ncommercially available 3D printers to fabricate fiber preforms, which were\nsubsequently thermally drawn into fibers. By optimizing 3D printing parameters,\nwe achieved the fabrication of fibers with diameters as small as 200 um having\ncomplex shapes, with features down to a few microns. We demonstrated the\nversatility of this method by fabricating fibers from diverse set of materials,\nsuch as fibers with different stiffnesses and fibers with magnetic\ncharacteristics, which are beneficial for developing tendon-driven and\nmagnetically actuated robotic fibers. In addition, by designing novel preform\ngeometries, we produced tapered fibers and fibers with interlocking mechanisms,\nalso tailored for use in medical steerable catheter applications. These\nadvancements highlight the scalability and versatility of this approach,\noffering a robust platform for producing high-precision polymer fibers for\ndiverse applications.",
        "Stueckelberg introduced an axion like scalar field to provide mass to the\ngauge electromagnetic field without breaking gauge invariance. This can be\nconsidered as a precursor to the spontaneously broken abelian Higgs model. We\nwill consider its role in cosmology to provide a novel candidate to the dark\nmatter question. In addition its implications to deeper issues will be pointed\nout.",
        "Combining local bifurcation analysis with numerical continuation and\nbifurcation methods we study bifurcations from cylindrical vesicles described\nby the Helfrich equation with volume and area constraints, with a prescribed\nperiodicity along the cylindrical axis. The bifurcating solutions are in two\nmain classes, axisymmetric (pearling), and non-axisymmetric (coiling, buckling,\nand wrinkling), and depending on the spontaneous curvature and the prescribed\nperiodicity along the cylinder axis we obtain different stabilities of the\nbifurcating branches, and different secondary bifurcations.",
        "In the last year, there have been some remarkable improvements in the\ncombinatorial list-size bounds of Folded Reed Solomon codes and multiplicity\ncodes. Starting from the work on Kopparty, Ron-Zewi, Saraf and Wootters (SIAM\nJ. Comput. 2023) (and subsequent simplifications due to Tamo (IEEE Trans.\nInform. Theory 2024), we have had dramatic improvements in the list-size bounds\nof FRS codes due to Srivastava (SODA 2025) and Chen & Zhang (STOC 2025). In\nthis note, we give a short exposition of these three results (Tamo, Srivastava\nand Chen-Zhang)."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"What is a cell type, really? The quest to categorize life's myriad forms.",
    "start_abstract":"The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
      ],
      "abstract":[
        "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based\n  Tiling Generation",
        "A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning",
        "Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf",
        "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization",
        "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "Parallelizing Multi-objective A* Search",
        "The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes",
        "Algorithmical Aspects of Some Bio Inspired Operations",
        "Proximal Flow Inspired Multi-Step Methods",
        "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization",
        "Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving",
        "One-Loop QCD Corrections to $\\bar{u}d \\rightarrow t\\bar{t}W$ at\n  $\\mathcal{O}(\\varepsilon^2)$",
        "Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches",
        "Realization of Two-dimensional Discrete Time Crystals with Anisotropic\n  Heisenberg Coupling",
        "Dissipative quantum phase transitions monitored by current fluctuations",
        "High-frequency coronal Alfv\\'enic waves observed with DKIST\/Cryo-NIRSP",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Controlling Large Language Models Through Concept Activation Vectors",
        "Publish on Ping: A Better Way to Publish Reservations in Memory\n  Reclamation for Concurrent Data Structures",
        "Design and Benchmarks for Emulating Kondo Dynamics on a Quantum Chip",
        "A note on the Sauvageot density principle",
        "Web Phishing Net (WPN): A scalable machine learning approach for\n  real-time phishing campaign detection",
        "Prompting in the Dark: Assessing Human Performance in Prompt Engineering\n  for Data Labeling When Gold Labels Are Absent",
        "An Analysis for Reasoning Bias of Language Models with Small\n  Initialization",
        "Disordered Weyl semimetal as an array of coupled Hubbard chains",
        "Process-based Self-Rewarding Language Models",
        "Isogeny graphs with level structures arrising from the Verschiebung map",
        "Labeling abelian varieties over finite fields"
      ],
      "abstract":[
        "As an artistic aid in tiled level design, Constraint Based Tiling Generation\n(CBTG) algorithms can help to automatically create level realizations from a\nset of tiles and placement constraints. Merrell's Modify in Blocks Model\nSynthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as\nConstraint Based Tiling Generation (CBTG) algorithms that work well for many\nscenarios but have limitations in problem size, problem setup and solution\nbiasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling\nGeneration algorithm, that can handle large problem sizes, requires minimal\nassumptions for setup and can help mitigate solution biasing. POMS attempts to\nresolve indeterminate grid regions by trying to progressively realize\nsub-blocks, performing a stochastic boundary erosion on previously resolved\nregions should sub-block resolution fail. We highlight the results of running a\nreference implementation on different tile sets and discuss a tile correlation\nlength, implied by the tile constraints, and its role in choosing an\nappropriate block size to aid POMS in successfully finding grid realizations.",
        "Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.",
        "In this paper, we give an explicit chain map, which induces the algebra\nisomorphism between the Hochschild cohomology ${\\bf HH}^{\\bullet}(B)$ and the\n$H$-invariant subalgebra ${\\bf H}^{\\bullet}(A, B)^{H}$ under two mild\nhypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$\nis an $H$-Galois extension of $A$. In particular, the smash product $B=A\\#H$\nalways satisfies the mild hypotheses. The isomorphism between ${\\bf\nHH}^{\\bullet}(A\\#H)$ and ${\\bf H}^{\\bullet}(A, A\\#H)^{H}$ generalizes the\nclassical result of group actions. As an application, Hochschild cohomology and\ncup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin\nHopf algebra are computed.",
        "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.",
        "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
        "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
        "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension.",
        "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
        "This thesis investigates three biologically inspired operations:\nprefix-suffix duplication, bounded prefix-suffix duplication, and\nprefix-suffix-square completion. Duplication, a common genetic mutation,\ninvolves repeating DNA sequences and is modeled here as formal operations on\nwords. The prefix-suffix duplication generates non-context-free languages, even\nfrom simple initial words. To better reflect biological processes, we propose a\nbounded variant that limits duplication length, resolving unsolved problems and\naligning with biochemical realities.\n  We also introduce the prefix-suffix-square completion operation, which\ngenerates squares at sequence ends. This operation enables the generation of\ninfinite words such as Fibonacci, Period-doubling, and Thue-Morse, which\ncontain squares but avoid higher exponent repetitions, highlighting unique\nstructural properties. In contrast, prefix-suffix duplication cannot generate\ncertain infinite words, such as Thue-Morse, but can produce cube-free words.\n  Additionally, we address the detection of gapped repeats and\npalindromes-structures important in DNA and RNA analysis. These involve\nrepeating or reversed factors flanking a central gap. Previous studies imposed\nconstraints on gap length or arm-gap relationships; we extend this by solving\nthe problem in three novel settings. This work advances theoretical insights\ninto biologically inspired operations and their computational applications in\ngenetic modeling.",
        "We investigate a family of approximate multi-step proximal point methods,\nframed as implicit linear discretizations of gradient flow. The resulting\nmethods are multi-step proximal point methods, with similar computational cost\nin each update as the proximal point method. We explore several optimization\nmethods where applying an approximate multistep proximal points method results\nin improved convergence behavior. We also include convergence analysis for the\nproposed method in several problem settings: quadratic problems, general\nproblems that are strongly or weakly (non)convex, and accelerated results for\nalternating projections.",
        "We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.",
        "The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems",
        "We present a computation of the one-loop QCD corrections to top-quark pair\nproduction in association with a $W$ boson, including terms up to order\n$\\varepsilon^2$ in dimensional regularization. Providing a first glimpse into\nthe complexity of the corresponding two-loop amplitude, this result is a first\nstep towards a description of this process at next-to-next-to-leading order\n(NNLO) in QCD. We perform a tensor decomposition and express the corresponding\nform factors in terms of a basis of independent special functions with compact\nrational coefficients, providing a structured framework for future\ndevelopments. In addition, we derive an explicit analytic representation of the\nform factors, valid up to order $\\varepsilon^0$, expressed in terms of\nlogarithms and dilogarithms. For the complete set of special functions\nrequired, we obtain a semi-numerical solution based on generalized power series\nexpansion.",
        "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
        "A discrete time crystal (DTC) is the paradigmatic example of a phase of\nmatter that occurs exclusively in systems out of equilibrium. This phenomenon\nis characterized by the spontaneous symmetry breaking of discrete\ntime-translation and provides a rich playground to study a fundamental question\nin statistical physics: what mechanism allows for driven quantum systems to\nexhibit emergent behavior that deviates from their counterparts with\ntime-independent evolution? Unlike equilibrium phases, DTCs exhibit macroscopic\nmanifestations of coherent quantum dynamics, challenging the conventional\nnarrative that thermodynamic behavior universally erases quantum signatures.\nHowever, due to the difficulty of simulating these systems with either\nclassical or quantum computers, previous studies have been limited to a set of\nmodels with Ising-like couplings -- and mostly only in one dimension -- thus\nprecluding our understanding of the existence (or not) of DTCs in models with\ninteractions that closely align with what occurs in nature. In this work, by\ncombining the latest generation of IBM quantum processors with state-of-the-art\ntensor network methods, we are able to demonstrate the existence of a DTC in a\ntwo-dimensional system governed by anisotropic Heisenberg interactions. Our\ncomprehensive analysis reveals a rich phase diagram encompassing spin-glass,\nergodic, and time-crystalline phases, highlighting the tunability of these\nphases through multiple control parameters. Crucially, our results emphasize\nthe interplay of initialization, interaction anisotropy, and driving protocols\nin stabilizing the DTC phase. By extending the study of Floquet matter beyond\nsimplified models, we lay the groundwork for exploring how driven systems\nbridge the gap between quantum coherence and emergent non-equilibrium\nthermodynamics.",
        "Dissipative phase transitions (DPT) are defined by sudden changes in the\nphysical properties of nonequilibrium open quantum systems and they present\ncharacteristics that have no analogue in closed and thermal systems. Several\nmethods to detect and characterize DPT have been suggested in the literature,\nthe most famous of which -- the $\\textit{Liouvillian gap}$ -- can be derived\nfrom a spectral analysis of the Liouvillian super-operator that governs the\ncomplex interplay between coherent and dissipative dynamics. Here, we consider\nthe $\\textit{output current}$, defined as the average total quantum jumps per\nunit time between the open quantum system and the environment. We propose that\noutput current fluctuations, and in particular their dynamical correlations,\ntheir power spectrum, and their characteristic timescale can provide valuable\ninformation about DPT, confirming a dramatic change of behavior at the critical\npoint. We validate our proposal using the dissipative XYZ model and the\nnonlinear driven-dissipative Kerr model, showing good agreement with previous\nestimates of the location of the critical point. Compared to previous\napproaches, our proposal could be already experimentally tested in optical\nsystems, providing a practical method to detect criticality in quantum open\nsystems.",
        "The presence and nature of low-frequency (0.1-10~mHz) Alfv\\'enic waves in the\ncorona has been established over the last decade, with many of these results\ncoming from coronagraphic observations of the infrared Fe XIII line. The\nCryo-NIRSP instrument situated at DKIST has recently begun acquiring science\nquality data of the same Fe XIII line, with at least a factor of 9 improvement\nin spatial resolution, a factor 30 increase in temporal resolution and an\nincrease in signal-to-noise, when compared to the majority of previously\navailable data. Here we present an analysis of 1~s cadence sit-and-stare data\nfrom Cryo-NIRSP, examining the Doppler velocity fluctuations associated with\nthe Fe XIII 1074~nm coronal line. We are able to confirm previous results of\nAlfv\\'enic waves in the corona as well as explore a new frequency regime. The\ndata reveals that the power law behaviour of the Doppler velocity power\nspectrum extends to higher frequencies. This result appears to challenge some\nmodels of photospheric-driven Alfv\\'enic waves that predict a lack of high\nfrequency wave power in the corona due to strong chromospheric damping.\nMoreover, the high-frequency waves do not transport as much energy as their\nlow-frequency counterparts, with less time-averaged energy per frequency\ninterval. We are also able to confirm the incompressible nature of the\nfluctuations with little coherence between the line amplitude and Doppler\nvelocity time-series.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.",
        "Safe memory reclamation techniques that utilize per read reservations, such\nas hazard pointers, often cause significant overhead in traversals of linked\nconcurrent data structures. This is primarily due to the need to announce a\nreservation, and fence to enforce appropriate ordering, before each read. In\nread-intensive workloads, this overhead is amplified because, even if\nrelatively little memory reclamation actually occurs, the full overhead of\nreserving records is still incurred while traversing data structures.\n  In this paper, we propose a novel memory reclamation technique by combining\nPOSIX signals and delayed reclamation, introducing a publish-on-ping approach.\nThis method eliminates the need to make reservations globally visible before\nuse. Instead, threads privately track which records they are accessing, and\nshare this information on demand with threads that intend to reclaim memory.\nThe approach can serve as a drop-in replacement for hazard pointers and hazard\neras. Furthermore, the capability to retain reservations during traversals in\ndata structure operations and publish them on demand facilitates the\nconstruction of a variant of hazard pointers (EpochPOP). This variant uses\nepochs to approach the performance of epoch-based reclamation in the common\ncase where threads are not frequently delayed (while retaining the robustness\nof hazard pointers).\n  Our publish-on-ping implementations based on hazard pointers (HP) and hazard\neras, when applied to various data structures, exhibit significant performance\nimprovements. The improvements across various workloads and data structures\nrange from 1.2X to 4X over the original HP, up to 20% compared to a heavily\noptimized HP implementation similar to the one in the Folly open-source\nlibrary, and up to 3X faster than hazard eras. EpochPOP delivers performance\nsimilar to epoch-based reclamation while providing stronger guarantees.",
        "Motivated by recent advances in digital quantum simulation and the overall\nprospective of solving correlated many-electron problems using quantum\nalgorithms, we design a gate-based quantum circuit that emulates the dynamics\nof the Kondo impurity model. We numerically determine the impurity\nmagnetization, entanglement between impurity and fermionic sites and energy as\na function of time (i.e.~circuit depth) for various initial states and find\nuniversal long-time dynamics. We complement the numerical simulations for\nmoderate system size with an asymptotically exact analytical solution that is\neffective in the limit of large system sizes and for starting states\ncorresponding to a filled Fermi sea. This work opens up the perspective of\nstudying the dynamics of electronic quantum many-body states on quantum chips\nof the NISQ era.",
        "In this short note, we address a gap in the proof of Sauvageot's density\nprinciple, which was pointed out in a paper by Nelson-Venkatesh.",
        "Phishing is the most prevalent type of cyber-attack today and is recognized\nas the leading source of data breaches with significant consequences for both\nindividuals and corporations. Web-based phishing attacks are the most frequent\nwith vectors such as social media posts and emails containing links to phishing\nURLs that once clicked on render host systems vulnerable to more sinister\nattacks. Research efforts to detect phishing URLs have involved the use of\nsupervised learning techniques that use large amounts of data to train models\nand have high computational requirements. They also involve analysis of\nfeatures derived from vectors including email contents thus affecting user\nprivacy. Additionally, they suffer from a lack of resilience against evolution\nof threats especially with the advent of generative AI techniques to bypass\nthese systems as with AI-generated phishing URLs. Unsupervised methods such as\nclustering techniques have also been used in phishing detection in the past,\nhowever, they are at times unscalable due to the use of pair-wise comparisons.\nThey also lack high detection rates while detecting phishing campaigns. In this\npaper, we propose an unsupervised learning approach that is not only fast but\nscalable, as it does not involve pair-wise comparisons. It is able to detect\nentire campaigns at a time with a high detection rate while preserving user\nprivacy; this includes the recent surge of campaigns with targeted phishing\nURLs generated by malicious entities using generative AI techniques.",
        "Millions of users prompt large language models (LLMs) for various tasks, but\nhow good are people at prompt engineering? Do users actually get closer to\ntheir desired outcome over multiple iterations of their prompts? These\nquestions are crucial when no gold-standard labels are available to measure\nprogress. This paper investigates a scenario in LLM-powered data labeling,\n\"prompting in the dark,\" where users iteratively prompt LLMs to label data\nwithout using manually-labeled benchmarks. We developed PromptingSheet, a\nGoogle Sheets add-on that enables users to compose, revise, and iteratively\nlabel data through spreadsheets. Through a study with 20 participants, we found\nthat prompting in the dark was highly unreliable-only 9 participants improved\nlabeling accuracy after four or more iterations. Automated prompt optimization\ntools like DSPy also struggled when few gold labels were available. Our\nfindings highlight the importance of gold labels and the needs, as well as the\nrisks, of automated support in human prompt engineering, providing insights for\nfuture tool design.",
        "Transformer-based Large Language Models (LLMs) have revolutionized Natural\nLanguage Processing by demonstrating exceptional performance across diverse\ntasks. This study investigates the impact of the parameter initialization scale\non the training behavior and task preferences of LLMs. We discover that smaller\ninitialization scales encourage models to favor reasoning tasks, whereas larger\ninitialization scales lead to a preference for memorization tasks. We validate\nthis reasoning bias via real datasets and meticulously designed anchor\nfunctions. Further analysis of initial training dynamics suggests that specific\nmodel components, particularly the embedding space and self-attention\nmechanisms, play pivotal roles in shaping these learning biases. We provide a\ntheoretical framework from the perspective of model training dynamics to\nexplain these phenomena. Additionally, experiments on real-world language tasks\ncorroborate our theoretical insights. This work enhances our understanding of\nhow initialization strategies influence LLM performance on reasoning tasks and\noffers valuable guidelines for training models.",
        "We demonstrate that a disordered magnetic Weyl semimetal may be mapped onto a\ntwo-dimensional array of coupled replicated Hubbard chains, where the Hubbard\n$U$ is directly related to the variance of the disorder potential. This is a\nthree-dimensional generalization of a similar mapping of the two-dimensional\nquantum Hall plateau transition to a one-dimensional Hubbard chain. We\ndemonstrate that this mapping leads to the conclusion that the Weyl semimetal\nbecomes a diffusive metal with a nonzero density of states at arbitrarily weak\ndisorder, in agreement with recent work. We also discuss the absence of\nlocalization in strongly disordered Weyl semimetals from the viewpoint of this\nmapping.",
        "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
        "We enhance an isogeny graph of elliptic curves by incorporating level\nstructures defined by bases of the kernels of iterates of the Verschiebung map.\nWe extend several previous results on isogeny graphs with level structures\ndefined by geometric points to these graphs. Firstly, we prove that these\ngraphs form $\\mathbb{Z}_p$-towers of graph coverings as the power of the\nVerschiebung map varies. Secondly, we prove that the connected components of\nthese graphs display a volcanic structure.",
        "We describe a deterministic process to associate a practical, permanent label\nto isomorphism classes of abelian varieties defined over finite fields and the\npolarizations they admit, for use in the mathematical literature."
      ]
    }
  },
  {
    "id":2411.01144,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images.",
    "start_abstract":"Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods.",
    "start_categories":[
      "Lung Ultrasound"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A Simple Framework for Contrastive Learning of Visual Representations"
      ],
      "abstract":[
        "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Invisible Labor: The Backbone of Open Source Software",
        "CardioTabNet: A Novel Hybrid Transformer Model for Heart Disease\n  Prediction using Tabular Medical Data",
        "A Multiple Transferable Neural Network Method with Domain Decomposition\n  for Elliptic Interface Problems",
        "Clinical Inspired MRI Lesion Segmentation",
        "Generalized quantum two level model and its application in astrophysics",
        "Algebras of analytic functionals and homological epimorphisms",
        "Formation of filaments and feathers in disc galaxies: Is self-gravity\n  enough?",
        "The State of Post-Hoc Local XAI Techniques for Image Processing:\n  Challenges and Motivations",
        "Network-centric optimal hybrid sensing hole recovery and self-healing in\n  IPV6 WSNs",
        "A spinless crystal for a high-performance solid-state $^{229}$Th nuclear\n  clock",
        "Learned Bayesian Cram\\'er-Rao Bound for Unknown Measurement Models Using\n  Score Neural Networks",
        "Ludwig-Soret microscopy with vibrational photothermal effect",
        "Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of\n  Phase Transitions in Weight Space",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Spectroscopic signatures of biexcitons: A case study in\n  Ruddlesden-Popper lead-halides",
        "The Layered Catalan Monoids: Structure and Determinants",
        "Beyond Interaction Patterns: Assessing Claims of Coordinated Inter-State\n  Information Operations on Twitter\/X",
        "On the time constant of high dimensional first passage percolation,\n  revisited",
        "Assessing Teamwork Dynamics in Software Development Projects",
        "A Lightweight and Secure Deep Learning Model for Privacy-Preserving\n  Federated Learning in Intelligent Enterprises",
        "Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates\n  Algorithm for Protecting Neural Networks",
        "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks",
        "Supersymmetric scale-separated AdS$_3$ vacua of type IIB",
        "MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic\n  Computed Tomography",
        "Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised\n  Disentanglement",
        "CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified\n  Intermediate Representation",
        "The Quantum Internet (Technical Version)",
        "Spectral properties from an efficient analytical representation of the\n  $GW$ self-energy within a multipole approximation",
        "The Economics of Large Language Models: Token Allocation, Fine-Tuning,\n  and Optimal Pricing"
      ],
      "abstract":[
        "Invisible labor is an intrinsic part of the modern workplace, and includes\nlabor that is undervalued or unrecognized such as creating collaborative\natmospheres. Open source software (OSS) is software that is viewable, editable\nand shareable by anyone with internet access. Contributors are mostly\nvolunteers, who participate for personal edification and because they believe\nin the spirit of OSS rather than for employment. Volunteerism often leads to\nhigh personnel turnover, poor maintenance and inconsistent project management.\nThis in turn, leads to a difficulty with sustainability long term. We believe\nthat the key to sustainable management is the invisible labor that occurs\nbehind the scenes. It is unclear how OSS contributors think about the invisible\nlabor they perform or how that affects OSS sustainability. We interviewed OSS\ncontributors and asked them about their invisible labor contributions,\nleadership departure, membership turnover and sustainability. We found that\ninvisible labor is responsible for good leadership, reducing contributor\nturnover, and creating legitimacy for the project as an organization.",
        "The early detection and prediction of cardiovascular diseases are crucial for\nreducing the severe morbidity and mortality associated with these conditions\nworldwide. A multi-headed self-attention mechanism, widely used in natural\nlanguage processing (NLP), is operated by Transformers to understand feature\ninteractions in feature spaces. However, the relationships between various\nfeatures within biological systems remain ambiguous in these spaces,\nhighlighting the necessity of early detection and prediction of cardiovascular\ndiseases to reduce the severe morbidity and mortality with these conditions\nworldwide. We handle this issue with CardioTabNet, which exploits the strength\nof tab transformer to extract feature space which carries strong understanding\nof clinical cardiovascular data and its feature ranking. As a result,\nperformance of downstream classical models significantly showed outstanding\nresult. Our study utilizes the open-source dataset for heart disease prediction\nwith 1190 instances and 11 features. In total, 11 features are divided into\nnumerical (age, resting blood pressure, cholesterol, maximum heart rate, old\npeak, weight, and fasting blood sugar) and categorical (resting ECG, exercise\nangina, and ST slope). Tab transformer was used to extract important features\nand ranked them using random forest (RF) feature ranking algorithm. Ten\nmachine-learning models were used to predict heart disease using selected\nfeatures. After extracting high-quality features, the top downstream model (a\nhyper-tuned ExtraTree classifier) achieved an average accuracy rate of 94.1%\nand an average Area Under Curve (AUC) of 95.0%. Furthermore, a nomogram\nanalysis was conducted to evaluate the model's effectiveness in cardiovascular\nrisk assessment. A benchmarking study was conducted using state-of-the-art\nmodels to evaluate our transformer-driven framework.",
        "The transferable neural network (TransNet) is a two-layer shallow neural\nnetwork with pre-determined and uniformly distributed neurons in the hidden\nlayer, and the least-squares solvers can be particularly used to compute the\nparameters of its output layer when applied to the solution of partial\ndifferential equations. In this paper, we integrate the TransNet technique with\nthe nonoverlapping domain decomposition and the interface conditions to develop\na novel multiple transferable neural network (Multi-TransNet) method for\nsolving elliptic interface problems, which typically contain discontinuities in\nboth solutions and their derivatives across interfaces. We first propose an\nempirical formula for the TransNet to characterize the relationship between the\nradius of the domain-covering ball, the number of hidden-layer neurons, and the\noptimal neuron shape. In the Multi-TransNet method, we assign each subdomain\none distinct TransNet with an adaptively determined number of hidden-layer\nneurons to maintain the globally uniform neuron distribution across the entire\ncomputational domain, and then unite all the subdomain TransNets together by\nincorporating the interface condition terms into the loss function. The\nempirical formula is also extended to the Multi-TransNet and further employed\nto estimate appropriate neuron shapes for the subdomain TransNets, greatly\nreducing the parameter tuning cost. Additionally, we propose a normalization\napproach to adaptively select the weighting parameters for the terms in the\nloss function. Ablation studies and extensive experiments with comparison tests\non different types of elliptic interface problems with low to high contrast\ndiffusion coefficients in two and three dimensions are carried out to\nnumerically demonstrate the superior accuracy, efficiency, and robustness of\nthe proposed Multi-TransNet method.",
        "Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting\npathological tissues in various diseases. Different MRI sequences have\ndifferent contrast mechanisms and sensitivities for different types of lesions,\nwhich pose challenges to accurate and consistent lesion segmentation. In\nclinical practice, radiologists commonly use the sub-sequence feature, i.e. the\ndifference between post contrast-enhanced T1-weighted (post) and\npre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we\npropose a residual fusion method to learn subsequence representation for MRI\nlesion segmentation. Specifically, we iteratively and adaptively fuse features\nfrom pre- and post-contrast sequences at multiple resolutions, using dynamic\nweights to achieve optimal fusion and address diverse lesion enhancement\npatterns. Our method achieves state-of-the-art performances on BraTS2023\ndataset for brain tumor segmentation and our in-house breast MRI dataset for\nbreast lesion segmentation. Our method is clinically inspired and has the\npotential to facilitate lesion segmentation in various applications.",
        "Complicated time-dependent curved spacetime and electric field are involved\nin many astrophysical situations, including the early universe, Hawking\nradiation, the Schwinger effect, and gravitational pair production. In this\nLetter, a generalized quantum two-level model (GQTLM) is developed, which is\napplicable to arbitrary time-dependent curved spacetime and electric field. The\nmodel is found to be consistent with quantum kinetic theory, and is\ncharacterized by its simplicity and versatility. The momentum distribution of\nparticles and the effects of gravitational distortions can be correctly\ndescribed. Quantum properties concerning vortex structures, such as the\nintrinsic orbital angular momentum of particles and antiparticles can also be\nconveniently calculated. The model is expected to significantly advance the\nquantum exploration of the universe. It could refine the prediction of\nprimordial gravitational waves and relevant non-Gaussian signals, extend the\ncalculation of Hawking radiation to general black hole configurations, help to\ndistinguish neutron stars from strange quark stars, and elucidate the\ngravitational pair production mechanism.",
        "It has been proved by the author [arXiv: 2404.19433] that the Arens-Michael\nenvelope of a solvable Lie algebra is a homological epimorphism. We show here\nthat for algebras of analytic functionals on a connected complex Lie group the\nanalogous statement is satisfied without the assumption of solvability, and\nfurthermore the completion homomorphisms of a more general form are also\nhomological epimorphisms, including the envelope with respect to the class of\nBanach PI-algebras.",
        "Context. Dense filaments\/feathers are kpc-scale dusty features present in\nnearby main sequence galaxies. Distinct from the spiral arms, filaments\nconstitute a major portion of dense gas concentration. They are expected to\nplay an important role in star formation and are known to harbour star-forming\nregions and H II regions.\n  Aims. We explore the origin of filaments\/feathers in disc galaxies via global\ngravitational instability.\n  Methods. We conduct a parameter study using three-dimensional hydrodynamical\nsimulations of isolated disc galaxies that are isothermal, self-gravitating and\ninitialised in equilibrium. Our galaxies are uniquely characterised by two\ndimensionless parameters, the Toomre $Q$ and the rotational Mach number,\n$\\mathcal{M}_{\\rm c} = v_{\\rm c}\/c_{\\rm s}$ (ratio of circular velocity to\nsound speed). We carry out simulations covering a wide range in both.\n  Results. We find that galaxies with $Q = 1$ form filaments within a single\nrotation, while galaxies with $Q \\geq 2$ do not. These filaments are kpc long\nand are semi-regularly spaced along the azimuth. Their morphology, density\ncontrast and formation timescale vary with $\\mathcal{M}_{\\rm c}$, with filament\nspacing and instability onset time both inversely proportional to\n$\\mathcal{M}_{\\rm c}$ and the density contrast increasing with\n$\\mathcal{M}_{\\rm c}$. However, their growth rates in all $Q = 1$ galaxies are\n$\\sim 0.5~\\Omega$, where $\\Omega$ is the angular frequency. We compare the\nfilament spacing in our simulations with the ones from JWST\/MIRI and HST\nobservations of nearby galaxies and find them in agreement.\n  Conclusions. Our study suggests that self-gravity and rotation are sufficient\nto form filaments, even in the absence of spiral arms or magnetic fields. Their\nmorphologies are primarily determined by $\\mathcal{M}_{\\rm c}$, which\nparametrises the importance of thermal versus rotational support.",
        "As complex AI systems further prove to be an integral part of our lives, a\npersistent and critical problem is the underlying black-box nature of such\nproducts and systems. In pursuit of productivity enhancements, one must not\nforget the need for various technology to boost the overall trustworthiness of\nsuch AI systems. One example, which is studied extensively in this work, is the\ndomain of Explainable Artificial Intelligence (XAI). Research works in this\nscope are centred around the objective of making AI systems more transparent\nand interpretable, to further boost reliability and trust in using them. In\nthis work, we discuss the various motivation for XAI and its approaches, the\nunderlying challenges that XAI faces, and some open problems that we believe\ndeserve further efforts to look into. We also provide a brief discussion of\nvarious XAI approaches for image processing, and finally discuss some future\ndirections, to hopefully express and motivate the positive development of the\nXAI research space.",
        "In our earlier work, Network-Centric Optimal Hybrid Mobility for IPv6\nwireless sensor networks, in which the work sought to control mobility of\nsensor nodes from an external network was proposed. It was a major improvement\non earlier works such as Cluster Sensor Proxy Mobile IPv6 (CSPMIPv6) and\nNetwork of Proxies (NoP). In this work, the Network-Centric optimal hybrid\nmobility scenario was used to detect and fill sensing holes occurring as a\nresult damaged or energy depleted sensing nodes. Various sensor networks\nself-healing and recovery, and deployment algorithms such as Enhanced Virtual\nForces Algorithm with Boundary Forces (EVFA-B); Coverage - Aware Sensor\nAutomation protocol (CASA); Sensor Self-Organizing Algorithm (SSOA); VorLag and\nthe use of the use of anchor and relay nodes were reviewed. With node density\nthresholds set for various scenarios, the recovery efficiency using various\nparameters were measured. Comparably, our method provides the most efficient\nnode relocation and self-healing mechanism for sensor networks. Compared to\nSensor Self-Organizing Algorithm (SSOA), Hybrid Mobile IP showed superiority in\ncoverage, shorter period of recovery, less computational cost and lower energy\ndepletion. With processing and mobility costs shifted to the external network,\nHybrid Mobile IP extends the life span of the network.",
        "Solid-state $^{229}$Th nuclear clocks require a host material whose band gap\nis larger than the 8.4 eV nuclear transition energy. As such, excitation of the\n$^{229}$Th nuclear state has so far only been demonstrated in metal fluorides,\nspecifically CaF$_2$, LiSrAlF$_6$, and ThF$_4$, where the large\nelectronegativity of the halogen leads to sufficient band gaps. However, it is\nexpected that the nuclear magnetic moment of the fluorine gives rise to a\nleading order broadening mechanism that limits the clock stability. Here, we\nuse concepts of molecular design to identify a polyatomic anion, SO$_4^{2-}$,\nthat is both nuclear spin free and of sufficient electron affinity to result in\na high band gap metal sulfate system. Using state-of-the-art calculations, we\nfind that the band gap of Th(SO$_4$)$_2$ is approximately 9 eV, large enough\nfor direct laser excitation of $^{229}$Th. Low concentrations of $^{229}$Th in\nthe otherwise spinless $^{232}$Th(SO$_4$)$_2$ crystal mitigate\n$^{229}$Th-$^{229}$Th interactions. Furthermore, the introduction of $^{229}$Th\ndoes not modify the material band gap nor introduce electronic states\nassociated with nuclear quenching. By removing one of the primary sources of\nnuclear line broadening in the crystal, the nuclear magnetic dipole-dipole\ninteraction, a nuclear clock with instability as low as $\\sigma =\n4.6\\times10^{-23}\/\\sqrt{\\tau}$, where ${\\tau}$ is the averaging time, may be\nrealized. This is roughly six orders of magnitude lower than previously thought\npossible.",
        "The Bayesian Cram\\'er-Rao bound (BCRB) is a crucial tool in signal processing\nfor assessing the fundamental limitations of any estimation problem as well as\nbenchmarking within a Bayesian frameworks. However, the BCRB cannot be computed\nwithout full knowledge of the prior and the measurement distributions. In this\nwork, we propose a fully learned Bayesian Cram\\'er-Rao bound (LBCRB) that\nlearns both the prior and the measurement distributions. Specifically, we\nsuggest two approaches to obtain the LBCRB: the Posterior Approach and the\nMeasurement-Prior Approach. The Posterior Approach provides a simple method to\nobtain the LBCRB, whereas the Measurement-Prior Approach enables us to\nincorporate domain knowledge to improve the sample complexity and\n{interpretability}. To achieve this, we introduce a Physics-encoded score\nneural network which enables us to easily incorporate such domain knowledge\ninto a neural network. We {study the learning} errors of the two suggested\napproaches theoretically, and validate them numerically. We demonstrate the two\napproaches on several signal processing examples, including a linear\nmeasurement problem with unknown mixing and Gaussian noise covariance matrices,\nfrequency estimation, and quantized measurement. In addition, we test our\napproach on a nonlinear signal processing problem of frequency estimation with\nreal-world underwater ambient noise.",
        "Vibrational microscopy provides label-free, bond-selective chemical contrast\nby detecting molecular vibrations, making it invaluable for biomedical\nresearch. While conventional methods rely on the direct detection of Raman\nscattering or infrared absorption, recently developed vibrational photothermal\n(ViP) microscopy achieves chemical contrast indirectly through refractive index\n(RI) changes. This indirect approach enables unique imaging capabilities beyond\ntraditional chemical imaging. Here, we introduce a novel application of ViP\nmicroscopy: label-free intracellular thermophoretic (Soret) imaging, which\nvisualizes biomolecular transport driven by temperature gradients. ViP-induced\nSoret (ViPS) imaging leverages a steady-state temperature distribution\ngenerated by optical heating through vibrational photothermal effect, combined\nwith time-resolved RI imaging via optical diffraction tomography (ODT). Using\nViPS imaging, we measured thermophoretic behavior in living COS7 cells,\ndetermining intracellular diffusion and Soret coefficients. Notably, we\nobserved a reversed direction of molecular transport (negative Soret effect) in\nthe cytoplasm compared to the nucleus, possibly driven by\nthermophoresis-induced diffusiophoresis. Furthermore, time-lapse imaging under\nCO2-depleted conditions revealed a remarkable reduction in thermophoretic\nactivity, suggesting glass formation during the dying process, likely due to\npolymer aggregation. ViPS imaging represents a new frontier in intracellular\nthermophoretic studies, expanding the capabilities of vibrational microscopy.",
        "Neural quantum states (NQS) have emerged as a powerful tool for approximating\nquantum wavefunctions using deep learning. While these models achieve\nremarkable accuracy, understanding how they encode physical information remains\nan open challenge. In this work, we introduce adiabatic fine-tuning, a scheme\nthat trains NQS across a phase diagram, leading to strongly correlated weight\nrepresentations across different models. This correlation in weight space\nenables the detection of phase transitions in quantum systems by analyzing the\ntrained network weights alone. We validate our approach on the transverse field\nIsing model and the J1-J2 Heisenberg model, demonstrating that phase\ntransitions manifest as distinct structures in weight space. Our results\nestablish a connection between physical phase transitions and the geometry of\nneural network parameters, opening new directions for the interpretability of\nmachine learning models in physics.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Exciton-exciton interactions are fundamental to the light-emitting properties\nof semiconductors, influencing applications from lasers to quantum light\nsources. In this study, we investigate the spectroscopic signatures and binding\nenergy of biexcitons in a metal halide two-dimensional Ruddlesden-Popper\nstructure, which is known for hosting distinct excitonic resonances with unique\nlattice coupling. Using three spectroscopic techniques - photoluminescence (PL)\nand two variations of two-dimensional electronic spectroscopy (2DES) - we map\ncoherent one-quantum and two-quantum correlations to gain deeper insight into\nthe biexciton characteristics. While PL spectroscopy is hindered by spectral\nbroadening and reabsorption, 2DES provides a more accurate characterization,\nrevealing multiple biexciton states and uncovering a mixed biexciton species\narising from exciton cross-coupling. These findings highlight the importance of\nadvanced spectroscopic approaches in accurately determining biexciton binding\nenergies and offer new perspectives on many-body interactions in\nexciton-polarons within layered perovskites.",
        "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
        "Social media platforms have become key tools for coordinated influence\noperations, enabling state actors to manipulate public opinion through\nstrategic, collective actions. While previous research has suggested\ncollaboration between states, such research failed to leverage state-of-the-art\ncoordination indicators or control datasets. In this study, we investigate\ninter-state coordination by analyzing multiple online behavioral traces and\nusing sophisticated coordination detection models. By incorporating a control\ndataset to differentiate organic user activity from coordinated efforts, our\nfindings reveal no evidence of inter-state coordination. These results\nchallenge earlier claims and underscore the importance of robust methodologies\nand control datasets in accurately detecting online coordination.",
        "In [2], it was claimed that the time constant $\\mu_{d}(e_{1})$ for the\nfirst-passage percolation model on $\\mathbb Z^{d}$ is $\\mu_{d}(e_{1}) \\sim \\log\nd\/(2ad)$ as $d\\to \\infty$, if the passage times $(\\tau_{e})_{e\\in \\mathbb\nE^{d}}$ are i.i.d., with a common c.d.f. $F$ satisfying\n$\\left|\\frac{F(x)}{x}-a\\right| \\le \\frac{C}{|\\log x|}$ for some constants $a,\nC$ and sufficiently small $x$.\n  However, the proof of the upper bound, namely, Equation (2.1) in [2]\n\\begin{align} \\limsup_{d\\to\\infty} \\frac{\\mu_{d}(e_{1})ad}{\\log d} \\le\n\\frac{1}{2} \\end{align} is incorrect. In this article, we provide a different\napproach that establishes this inequality. As a side product of this new\nmethod, we also show that the variance of the non-backtracking passage time to\nthe first hyperplane is of order $o\\big((\\log d\/d)^{2}\\big)$ as $d\\to \\infty$\nin the case of the when the edge weights are exponentially distributed.",
        "This study investigates teamwork dynamics in student software development\nprojects through a mixed-method approach combining quantitative analysis of\nGitLab commit logs and qualitative survey data. We analyzed individual\ncontributions across six project phases, comparing self-reported and actual\ncontributions to measure discrepancies. Additionally, a survey captured\ninsights on team leadership, conflict resolution, communication practices, and\nworkload perceptions. Findings reveal that teams with minimal contribution\ndiscrepancies achieved higher project grades and exam pass rates. In contrast,\nteams with more significant discrepancies experienced lower performance,\npotentially due to role clarity and communication issues. These results\nunderscore the value of shared leadership, structured conflict resolution, and\nregular feedback in fostering effective teamwork, offering educators strategies\nto enhance collaboration in software engineering education through\nself-reflection and balanced workload allocation.",
        "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).",
        "Neural network models implemented in embedded devices have been shown to be\nsusceptible to side-channel attacks (SCAs), allowing recovery of proprietary\nmodel parameters, such as weights and biases. There are already available\ncountermeasure methods currently used for protecting cryptographic\nimplementations that can be tailored to protect embedded neural network models.\nShuffling, a hiding-based countermeasure that randomly shuffles the order of\ncomputations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm\nis used. In this paper, we propose a design of an SCA-secure version of the\nFisher-Yates algorithm. By integrating the masking technique for modular\nreduction and Blakely's method for modular multiplication, we effectively\nremove the vulnerability in the division operation that led to side-channel\nleakage in the original version of the algorithm. We experimentally evaluate\nthat the countermeasure is effective against SCA by implementing a correlation\npower analysis attack on an embedded neural network model implemented on ARM\nCortex-M4. Compared to the original proposal, the memory overhead is $2\\times$\nthe biggest layer of the network, while the time overhead varies from $4\\%$ to\n$0.49\\%$ for a layer with $100$ and $1000$ neurons, respectively.",
        "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
        "I construct supersymmetric, parametrically scale-separated AdS$_3$ vacua of\ntype IIB string theory. These arise as compactifications with orientifold\nplanes on specific seven-dimensional solvmanifolds admitting co-closed\n$G_2$-structures, preserving minimal supersymmetry. There are solutions that\ninclude either one set or four sets of intersecting O5-planes in the smeared\napproximation, and parametric scale separation can be achieved by tuning\nunbounded fluxes to infinity. Additionally, the putative holographic field\ntheory operators that are dual to the lightest scalars in the gravitational\ntheory have integer conformal dimensions at tree level, aligning with other\nscale-separated models of type II string theory.",
        "Chinese acupuncture practitioners primarily depend on muscle memory and\ntactile feedback to insert needles and accurately target acupuncture points, as\nthe current workflow lacks imaging modalities and visual aids. Consequently,\nnew practitioners often learn through trial and error, requiring years of\nexperience to become proficient and earn the trust of patients. Medical\nstudents face similar challenges in mastering this skill. To address these\nchallenges, we developed an innovative system, MRUCT, that integrates\nultrasonic computed tomography (UCT) with mixed reality (MR) technology to\nvisualize acupuncture points in real-time. This system offers offline image\nregistration and real-time guidance during needle insertion, enabling them to\naccurately position needles based on anatomical structures such as bones,\nmuscles, and auto-generated reference points, with the potential for clinical\nimplementation. In this paper, we outline the non-rigid registration methods\nused to reconstruct anatomical structures from UCT data, as well as the key\ndesign considerations of the MR system. We evaluated two different 3D user\ninterface (3DUI) designs and compared the performance of our system to\ntraditional workflows for both new practitioners and medical students. The\nresults highlight the potential of MR to enhance therapeutic medical practices\nand demonstrate the effectiveness of the system we developed.",
        "The imitation of voice, targeted on specific speech attributes such as timbre\nand speaking style, is crucial in speech generation. However, existing methods\nrely heavily on annotated data, and struggle with effectively disentangling\ntimbre and style, leading to challenges in achieving controllable generation,\nespecially in zero-shot scenarios. To address these issues, we propose Vevo, a\nversatile zero-shot voice imitation framework with controllable timbre and\nstyle. Vevo operates in two core stages: (1) Content-Style Modeling: Given\neither text or speech's content tokens as input, we utilize an autoregressive\ntransformer to generate the content-style tokens, which is prompted by a style\nreference; (2) Acoustic Modeling: Given the content-style tokens as input, we\nemploy a flow-matching transformer to produce acoustic representations, which\nis prompted by a timbre reference. To obtain the content and content-style\ntokens of speech, we design a fully self-supervised approach that progressively\ndecouples the timbre, style, and linguistic content of speech. Specifically, we\nadopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We\ntreat the vocabulary size of the VQ-VAE codebook as the information bottleneck,\nand adjust it carefully to obtain the disentangled speech representations.\nSolely self-supervised trained on 60K hours of audiobook speech data, without\nany fine-tuning on style-specific corpora, Vevo matches or surpasses existing\nmethods in accent and emotion conversion tasks. Additionally, Vevo's\neffectiveness in zero-shot voice conversion and text-to-speech tasks further\ndemonstrates its strong generalization and versatility. Audio samples are\navailable at https:\/\/versavoice.github.io.",
        "Geospatial imaging leverages data from diverse sensing modalities-such as EO,\nSAR, and LiDAR, ranging from ground-level drones to satellite views. These\nheterogeneous inputs offer significant opportunities for scene understanding\nbut present challenges in interpreting geometry accurately, particularly in the\nabsence of precise ground truth data. To address this, we propose\nCrossModalityDiffusion, a modular framework designed to generate images across\ndifferent modalities and viewpoints without prior knowledge of scene geometry.\nCrossModalityDiffusion employs modality-specific encoders that take multiple\ninput images and produce geometry-aware feature volumes that encode scene\nstructure relative to their input camera positions. The space where the feature\nvolumes are placed acts as a common ground for unifying input modalities. These\nfeature volumes are overlapped and rendered into feature images from novel\nperspectives using volumetric rendering techniques. The rendered feature images\nare used as conditioning inputs for a modality-specific diffusion model,\nenabling the synthesis of novel images for the desired output modality. In this\npaper, we show that jointly training different modules ensures consistent\ngeometric understanding across all modalities within the framework. We validate\nCrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,\ndemonstrating its effectiveness in generating accurate and consistent novel\nviews across multiple imaging modalities and perspectives.",
        "Following the emergence of quantum computing, the subsequent quantum\nrevolution will be that of interconnecting individual quantum computers at\nglobal level. In the same way that classical computers only realised their full\npotential with the emergence of the internet, a fully realised quantum internet\nis the next stage of evolution for quantum computation. This work examines in\ndetail how the quantum internet would evolve in practice, focusing not only on\nthe technology itself but also on the implications it will have economically\nand politically. We present both original ideas, as well as an extensive review\nof relevant and related background material. This work begins with a\ndescription of classical networks before introducing the key concepts behind\nquantum networks, such as quantum internet protocols, quantum cryptography, and\ncloud quantum computing. The work is divided into technical sections (requiring\nonly a basic knowledge of the notation of quantum mechanics), for those\ninterested in mathematical details, as well as non-technical sections for those\nseeking a more general understanding. We target this work very broadly at\nquantum and classical computer scientists, classical computer systems, software\nand network engineers, physicists, economists, artists, musicians, and those\njust generally curious about the future of quantum technologies and what they\nmight bring to humanity.",
        "We propose an efficient analytical representation of the frequency-dependent\n$GW$ self-energy $\\Sigma$ via a multipole approximation (MPA-$\\Sigma$). The\nmultipole-Pad\\'e model for the self-energy is interpolated from a small set of\nnumerical evaluations of $\\Sigma$ in the complex frequency plane, similarly to\nthe previously multipole representation developed for the screened Coulomb\ninteraction (MPA-$W$) [Phys. Rev. B \\textbf{104}, 115157 (2021)]. We show that,\nlikewise MPA-$W$, an appropriate choice of frequency sampling in MPA-$\\Sigma$\nis critical to guarantee computational efficiency and high accuracy. The\ncombined MPA-$W$ and MPA-$\\Sigma$ scheme considerably reduces the cost of\nfull-frequency self-energy calculations, especially for spectral band\nstructures over a wide energy range. Crucially, MPA-$\\Sigma$ enables a\nmultipole representation for the interacting Green's function $G$ (MPA-$G$),\nproviding a straightforward evaluation of all the spectral properties, and a\nmore general way to define the renormalization factor $Z$. We validate the\nMPA-$\\Sigma$ and MPA-$G$ approaches for diverse systems: bulk Si, Na and Cu,\nmonolayer MoS$_2$, the NaCl ion-pair and the F$_2$ molecule. Moreover, we\nintroduce toy MPA-$\\Sigma$\/$G$ models to examine the quasiparticle picture in\ndifferent regimens of weak and strong correlation. With these models, we expose\nlimitations in defining $Z$ from the local derivative of $\\Sigma$.",
        "We develop an economic framework to analyze the optimal pricing and product\ndesign of Large Language Models (LLM). Our framework captures several key\nfeatures of LLMs: variable operational costs of processing input and output\ntokens; the ability to customize models through fine-tuning; and\nhigh-dimensional user heterogeneity in terms of task requirements and error\nsensitivity. In our model, a monopolistic seller offers multiple versions of\nLLMs through a menu of products. The optimal pricing structure depends on\nwhether token allocation across tasks is contractible and whether users face\nscale constraints. Users with similar aggregate value-scale characteristics\nchoose similar levels of fine-tuning and token consumption. The optimal\nmechanism can be implemented through menus of two-part tariffs, with higher\nmarkups for more intensive users. Our results rationalize observed industry\npractices such as tiered pricing based on model customization and usage levels."
      ]
    }
  },
  {
    "id":2411.01144,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A Simple Framework for Contrastive Learning of Visual Representations",
    "start_abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images."
      ],
      "abstract":[
        "Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods."
      ],
      "categories":[
        "Lung Ultrasound"
      ]
    },
    "list":{
      "title":[
        "The 200 Gbps Challenge: Imagining HL-LHC analysis facilities",
        "Cryoscope: A Cryogenic Infrared Survey Telescope in Antarctica",
        "Thermal investigation of bistability in high index doped silica\n  integrated ring resonators",
        "Affineness on Noetherian graded rings, algebras and Hopf algebras",
        "Genuine Multipartite Nonlocality sharing under sequential measurement",
        "Hamiltonian Learning at Heisenberg Limit for Hybrid Quantum Systems",
        "Diagnosing Quantum Many-body Chaos in Non-Hermitian Quantum Spin Chain\n  via Krylov Complexity",
        "Gamma-Ray Bursts Calibrated from the Observational $H(z)$ Data in\n  Artificial Neural Network Framework",
        "Exploring the link between galaxy assembly and dark matter halo assembly\n  in IllustrisTNG: Insights from the Mutual Information",
        "Stability in affine logic",
        "Kink breathers on a traveling wave background in the defocusing modified\n  Korteweg--de Vries equation",
        "CoRe: Coherency Regularization for Hierarchical Time Series",
        "Long-time asymptotics of 3-solitary waves for the damped nonlinear\n  Klein-Gordon equation",
        "Some Problems on Intrinsically Harmonic Forms",
        "Stress-induced phase transformations in Ti-15Mo alloy at elevated\n  temperature",
        "Quantitative Theory for Critical Conditions of Like-Charge Attraction\n  Between Polarizable Spheres",
        "A Probabilistic Parking Process and Labeled IDLA",
        "Enhancement of sensitivity near exceptional points in dissipative\n  qubit-resonator systems",
        "Pulsation Properties of Blazhko and Non-Blazhko RRab Stars",
        "AMPEL workflows for LSST: Modular and reproducible real-time photometric\n  classification",
        "Score-Preserving Targeted Maximum Likelihood Estimation",
        "Distinguishing Cause from Effect with Causal Velocity Models",
        "A method to optimize antipodal coloring span of graphs and its\n  application",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Linear Optics to Scalable Photonic Quantum Computing",
        "A New Proof of Sub-Gaussian Norm Concentration Inequality",
        "Forecasting Italian daily electricity generation disaggregated by\n  geographical zones and energy sources using coherent forecast combination",
        "Quark-Antiquark Potential as a Probe for Holographic Phase Transitions",
        "Enhanced Vascular Flow Simulations in Aortic Aneurysm via\n  Physics-Informed Neural Networks and Deep Operator Networks"
      ],
      "abstract":[
        "The IRIS-HEP software institute, as a contributor to the broader HEP Python\necosystem, is developing scalable analysis infrastructure and software tools to\naddress the upcoming HL-LHC computing challenges with new approaches and\nparadigms, driven by our vision of what HL-LHC analysis will require. The\ninstitute uses a \"Grand Challenge\" format, constructing a series of\nincreasingly large, complex, and realistic exercises to show the vision of\nHL-LHC analysis. Recently, the focus has been demonstrating the IRIS-HEP\nanalysis infrastructure at scale and evaluating technology readiness for\nproduction.\n  As a part of the Analysis Grand Challenge activities, the institute executed\na \"200 Gbps Challenge\", aiming to show sustained data rates into the event\nprocessing of multiple analysis pipelines. The challenge integrated teams\ninternal and external to the institute, including operations and facilities,\nanalysis software tools, innovative data delivery and management services, and\nscalable analysis infrastructure. The challenge showcases the prototypes -\nincluding software, services, and facilities - built to process around 200 TB\nof data in both the CMS NanoAOD and ATLAS PHYSLITE data formats with test\npipelines.\n  The teams were able to sustain the 200 Gbps target across multiple pipelines.\nThe pipelines focusing on event rate were able to process at over 30 MHz. These\ntarget rates are demanding; the activity revealed considerations for future\ntesting at this scale and changes necessary for physicists to work at this\nscale in the future. The 200 Gbps Challenge has established a baseline on\ntoday's facilities, setting the stage for the next exercise at twice the scale.",
        "We present Cryoscope--a new 50 deg$^2$ field-of-view, 1.2 m aperture,\n$K_{dark}$ survey telescope to be located at Dome C, Antarctica. Cryoscope has\nan innovative optical-thermal design wherein the entire telescope is\ncryogenically cooled. Cryoscope also explores new detector technology to\ncost-effectively tile the full focal plane. Leveraging the dark Antarctic sky\nand minimizing telescope thermal emission, Cryoscope achieves unprecedented\ndeep, wide, fast and red observations, matching and exceeding volumetric survey\nspeeds from the Ultraviolet Explorer, Vera Rubin Observatory, Nancy Grace Roman\nSpace Telescope, SPHEREx, and NEO Surveyor. By providing coverage beyond\nwavelengths of 2 $\\mu$m, we aim to create the most comprehensive dynamic movie\nof the most obscured reaches of the Universe. Cryoscope will be a dedicated\ndiscovery engine for electromagnetic emission from coalescing compact binaries,\nEarth-like exoplanets orbiting cold stars, and multiple facets of time-domain,\nstellar and solar system science. In this paper, we describe the scientific\ndrivers and technical innovations for this new discovery engine operating in\nthe $K_{dark}$ passband, why we choose to deploy it in Antarctica, and the\nstatus of a fifth-scale prototype designed as a Pathfinder to retire\ntechnological risks prior to full-scale implementation. We plan to deploy the\nCryoscope Pathfinder to Dome C in December 2026 and the full-scale telescope by\n2030.",
        "The utilization and engineering of thermo-optic effects have found broad\napplications in integrated photonic devices, facilitating efficient light\nmanipulation to achieve various functionalities. Here, we perform both an\nexperimental characterization and theoretical analysis of these effects in\nintegrated micro-ring resonators in high index doped silica (HIDS), which has\nhad many applications in integrated photonics and nonlinear optics. By fitting\nthe experimental results with theory, we obtain fundamental parameters that\ncharacterize their thermo-optic performance, including the thermo-optic\ncoefficient, the efficiency for the optically induced thermo-optic process, and\nthe thermal conductivity. The characteristics of these parameters are compared\nto those of other materials commonly used for integrated photonic platforms,\nsuch as silicon, silicon nitride, and silica. These results offer a\ncomprehensive insight into the thermo-optic properties of HIDS based devices.\nUnderstanding these properties is essential for efficiently controlling and\nengineering them in many practical applications.",
        "In this note, we show that every Noetherian graded ring with an affine degree\nzero part is affine. As a result, a Noetherian graded Hopf algebra whose degree\nzero component is a commutative or a cocommutative Hopf subalgebra is affine.\nMoreover, we show that the braided Hopf algebra of a Noetherian graded Hopf\nalgebra is affine.",
        "The study of quantum nonlocality sharing has garnered significant attention,\nparticularly for two-qubit and three-qubit entangled systems. In this paper, we\nextend the investigation to $n$-qubit Greenberger-Horne-Zeilinger (GHZ)\nsystems, analyzing nonlocality sharing under unbiased unsharp measurements.\nEmploying the Seevink and Svetlichny inequalities, we explore both unilateral\nand multilateral sequential measurement scenarios. In the unilateral scenario,\nwe derive the range for which an observer's multiple copies can share genuine\n$n$-partite nonlocality with single copies of the remaining parties. In the\nmultilateral scenario, we identify the maximum number of independent observers\non $m$ sides who can share genuine $n$-partite nonlocality with other parties.\nA crucial aspect of our results is that all findings stem from a measurement\nstrategy where each sequential observer utilizes unbiased unsharp measurements.\nAs a specific case, for the four-qubit maximally entangled GHZ state, we\ndemonstrate that at most two copies of an observer (e.g., Alice) can share\nnonlocality in the unilateral sequential measurement scenario. However, in the\nmultilateral scenario, no additional sharing is possible compared to the\nunilateral case. This finding highlights the significance of unsharp\nmeasurements in optimizing the recycling of qubits for generating quantum\nnonlocality.",
        "Hybrid quantum systems with different particle species are fundamental in\nquantum materials and quantum information science. In this work, we demonstrate\nthat Hamiltonian learning in hybrid spin-boson systems can achieve the\nHeisenberg limit. Specifically, we establish a rigorous theoretical framework\nproving that, given access to an unknown hybrid Hamiltonian system, our\nalgorithm can estimate the Hamiltonian coupling parameters up to root mean\nsquare error (RMSE) $\\epsilon$ with a total evolution time scaling as $T \\sim\n\\mathcal{O}(\\epsilon^{-1})$ using only $\\mathcal{O}({\\rm\npolylog}(\\epsilon^{-1}))$ measurements. Furthermore, it remains robust against\nsmall state preparation and measurement (SPAM) errors. In addition, we also\nprovide an alternative algorithm based on distributed quantum sensing, which\nsignificantly reduces the maximum evolution time per measurement. To validate\nour method, we apply it to the generalized Dicke model for Hamiltonian learning\nand the spin-boson model for spectrum learning, demonstrating its efficiency in\npractical quantum systems. These results provide a scalable and robust\nframework for precision quantum sensing and Hamiltonian characterization in\nhybrid quantum platforms.",
        "We investigate the phase transitions from chaotic to non-chaotic dynamics in\na quantum spin chain with a local non-Hermitian disorder, which can be realized\nwith a Rydberg atom array setting. As the disorder strength increases, the\nemergence of non-chaotic dynamics is qualitatively captured through the\nsuppressed growth of Krylov complexity, and quantitatively identified through\nthe reciprocity breaking of Krylov space. We further find that the localization\nin Krylov space generates another transition in the weak disorder regime,\nsuggesting a weak ergodicity breaking. Our results closely align with\nconventional methods, such as the entanglement entropy and complex level\nspacing statistics, and pave the way to explore non-Hermitian phase transitions\nusing Krylov complexity and associated metrics.",
        "In this paper, we calibrate the luminosity relation of gamma-ray bursts\n(GRBs) from an Artificial Neural Network (ANN) framework for reconstructing the\nHubble parameter \\unboldmath{$H(z)$} from the latest observational Hubble data\n(OHD) obtained with the cosmic chronometers method in a cosmology-independent\nway. We consider the physical relationships between the data to introduce the\ncovariance matrix and KL divergence of the data to construct the loss function\nand calibrate the Amati relation ($E_{\\rm p}$--$E_{\\rm iso}$) by selecting the\noptimal ANN model with the A219 sample and the J220 sample at low redshift.\nCombining the Pantheon+ sample of type Ia supernovae (SNe Ia) and Baryon\nacoustic oscillations (BAOs) with GRBs at high redshift in the Hubble diagram\nwith Markov Chain Monte Carlo numerical method, we find that the $\\Lambda$CDM\nmodel is preferred over the $w$CDM and CPL models with the joint constraints by\nthe Akaike Information Criterion (AIC) and Bayesian Information Criterion\n(BIC).",
        "We employed Mutual Information (MI) analysis to investigate the relationship\nbetween galaxy properties and the assembly history of their host dark matter\n(DM) haloes from the IllustrisTNG simulations. Focusing on central and\nsatellite galaxies with stellar masses between $10^{9} \\, - \\, 10^{11.5}\\,\nh^{-1} M_\\odot$, we examined the correlation between halo assembly time and\ngalaxy assembly time, specific star formation rate (sSFR), color $(g-i)$, and\ngalaxy formation efficiency $F_\\star$. Our results indicate a strong\ncorrelation between $F_\\star$ and the halo assembly time for low-mass central\ngalaxies, suggesting a co-evolutionary relationship. In contrast, sSFR and\ncolor $(g-i)$ exhibit weaker correlations with halo assembly time, indicating\nthat additional factors should influence these galaxy properties. Satellite\ngalaxies show negligible correlation between their properties and halo assembly\ntime, highlighting the impact of environmental processes on their evolution. We\nfurther extended our analysis to cluster observables, including the magnitude\ngap, the satellite richness, and the distances to the satellites. Although\nthese cluster properties display weak overall correlations with halo assembly\ntime, the richness consistently increases with stellar mass. This trend\nsuggests that richness is more closely linked to formation history in more\nmassive haloes, where satellite accretion dominates the growth of their host DM\nhaloes. These findings establish $F_\\star$ as a more sensitive indicator of\nhalo assembly history than colour $(g-i)$, sSFR, or cluster observables,\noffering new insights into the complex interplay between galaxy evolution and\nthe hierarchical growth of their host dark matter haloes.",
        "We develop foundational aspects of stability theory in affine logic. On the\none hand, we prove appropriate affine versions of many classical results,\nincluding definability of types, existence of non-forking extensions, and other\nfundamental properties of forking calculus. Most notably, stationarity holds\nover arbitrary sets (in fact, every type is Lascar strong). On the other hand,\nwe prove that stability is preserved under direct integrals of measurable\nfields of structures. We deduce that stability in the extremal models of an\naffine theory implies stability of the theory. We also deduce that the affine\npart of a stable continuous logic theory is affinely stable, generalising the\nresult of preservation of stability under randomisations.",
        "We characterize a general traveling periodic wave of the defocusing mKdV\n(modified Korteweg--de Vries) equation by using a quotient of products of\nJacobi's elliptic theta functions. Compared to the standing periodic wave of\nthe defocusing NLS (nonlinear Schr\\\"{o}dinger) equation, these solutions are\nspecial cases of Riemann's theta function of genus two. Based on our\ncharacterization, we derive a new two-parameter solution form which defines a\ngeneral three-parameter solution form with the scaling transformation.\nEigenfunctions of the Lax system for the general traveling periodic wave are\nalso characterized as quotients of products of Jacobi's theta functions. As the\nmain outcome of our analytical computations, we derive a new solution of the\ndefocusing mKdV equation which describes the kink breather propagating on a\ngeneral traveling wave background.",
        "Hierarchical time series forecasting presents unique challenges, particularly\nwhen dealing with noisy data that may not perfectly adhere to aggregation\nconstraints. This paper introduces a novel approach to soft coherency in\nhierarchical time series forecasting using neural networks. We present a\nnetwork coherency regularization method, which we denote as CoRe (Coherency\nRegularization), a technique that trains neural networks to produce forecasts\nthat are inherently coherent across hierarchies, without strictly enforcing\naggregation constraints. Our method offers several key advantages. (1) It\nprovides theoretical guarantees on the coherency of forecasts, even for\nout-of-sample data. (2) It is adaptable to scenarios where data may contain\nerrors or missing values, making it more robust than strict coherency methods.\n(3) It can be easily integrated into existing neural network architectures for\ntime series forecasting. We demonstrate the effectiveness of our approach on\nmultiple benchmark datasets, comparing it against state-of-the-art methods in\nboth coherent and noisy data scenarios. Additionally, our method can be used\nwithin existing generative probabilistic forecasting frameworks to generate\ncoherent probabilistic forecasts. Our results show improved generalization and\nforecast accuracy, particularly in the presence of data inconsistencies. On a\nvariety of datasets, including both strictly hierarchically coherent and noisy\ndata, our training method has either equal or better accuracy at all levels of\nthe hierarchy while being strictly more coherent out-of-sample than existing\nsoft-coherency methods.",
        "We consider the damped nonlinear Klein-Gordon equation: \\begin{align*}\n\\partial_{t}^2u-\\Delta u+2\\alpha \\partial_{t}u+u-|u|^{p-1}u=0, \\ & (t,x) \\in\n\\mathbb{R} \\times \\mathbb{R}^d, \\end{align*} where $\\alpha>0$, $1\\leq d\\leq 5$\nand energy sub-critical exponents $p>2$. In this paper, we prove that\n3-solitary waves behave as if the three solitons are on a line. Furthermore,\nthe solitary waves have alternative signs and their distances are of order\n$\\log{t}$.",
        "In this short note we recall the definition of intrinsically harmonic forms,\nsome known results and some open problems.",
        "Controlled mechanical loading was applied to Ti-15Mo alloy during annealing\nat 550 {\\deg}C. Massive formation of the $\\omega_{\\textrm{iso}}$ phase from the\nparent $\\beta$-phase occurred during annealing at 550 {\\deg}C without external\nstress or with stress well below the yield stress. Moreover, a massive $\\alpha$\nphase precipitation takes place under simultaneous annealing and plastic\ndeformation. Plastic deformation plays a key role in $\\beta\\rightarrow\\alpha$\ntransformation and achieving refined $\\alpha+\\beta$ type microstructure\nresulted in improved mechanical properties. Studying phase transformations\nduring plastic deformation is critical for understanding and optimizing\nthermomechanical processing of metastable $\\beta$-Ti alloys.",
        "Despite extensive experimental and theoretical efforts, a concise\nquantitative theory to predict the occurrence of like-charge attraction (LCA)\nbetween polarizable spheres remains elusive. In this work, we first derive a\nnovel three-point image formula, based on a key observation that connects the\nclassical Neumann's image principle with the incomplete beta function. This\napproach naturally yields simple yet precise critical conditions for LCA, with\na relative discrepancy of less than $1\\%$ compared to numerical simulations,\nvalidated across diverse parameter settings. The obtained critical conditions\nmay provide physical insights into various processes potentially involving LCA,\nsuch as self-assembly, crystallization, and phase separation across different\nlength scales. Additionally, the new image formula is directly applicable to\nenhance the efficiency of polarizable force field calculations involving\npolarizable spheres.",
        "We introduce and study a new probabilistic variant of the classical parking\nprotocol of Konheim and Weiss [29], which is closely related to Internal\nDiffusion Limited Aggregation, or IDLA, introduced in 1991 by Diaconis and\nFulton [15]. In particular, we show that if one runs our parking protocol\nstarting with a parking function whose outcome permutation (in the sense of the\nclassical parking process of Konheim and Weiss) is the identity permutation,\nthen we can compute the exact probability that all of the cars park.\nFurthermore, we compute the expected time it takes for the protocol to complete\nassuming all of the cars park, and prove that the parking process is negatively\ncorrelated. We also study statistics of uniformly random weakly increasing\nparking functions, a subset of parking functions whose outcome is the identity\npermutation. We give the distribution of the last entry, along with the\nprobability that a specific set of cars is lucky, and the expected number of\nlucky cars.",
        "Dissipation usually plays a negative role in quantum metrological\ntechnologies, which aim to improve measurement precision by leveraging quantum\neffects that are vulnerable to environment-induced decoherence. Recently, it\nhas been demonstrated that dissipation can actually be used as a favorable\nresource for enhancing the susceptibility of signal detection. However,\ndemonstrations of such enhancement for detecting physical quantities in open\nquantum systems are still lacking. Here we propose and demonstrate a protocol\nfor realizing such non-Hermitian quantum sensors for probing the coupling\nbetween a qubit and a resonator subjecting to energy dissipations. The\nexcitation-number conversion associated with the no-jump evolution trajectory\nenables removal of the noisy outcomes with quantum jumps, implementing the\nexceptional point (EP), where the Rabi splitting exhibits a divergent behavior\nin response to a tiny variation of the effective coupling. The sensitivity\nenhancement near the EP is confirmed by both theoretical calculation and\nexperimental measurement.",
        "In this study, we conduct a comparative analysis of the properties of Blazhko\nand non-Blazhko RRab stars. We identified 1054 non-Blazhko and 785 Blazhko RRab\nstars in the photometric data observed by K2 mission, which, combined with\nthose 37 stars observed in the original Kepler field, constituted our study\nsample. Using the Fourier Decomposition method, we calculated the pulsation\nparameters, including phase differences and amplitude ratios, for these RRab\nstars, revealing significant discrepancies in the pulsation parameters between\nBlazhko and non-Blazhko RRab stars. However, distinguishing between Blazhko and\nNon-Blazhko RRab stars based on Fourier parameters remains challenging due to\nthe significant overlap in their distributions. By cross-matching our sample\nwith the LRS of LAMOST DR12, we identified 147 Blazhko and 111 non-Blazhko RRab\nstars, which exhibit similar metallicity distributions. Furthermore,\ncross-matching with Gaia DR3 data yielded 766 Blazhko and 950 non-Blazhko RRab\nstars, showing differences in color indices but not in absolute magnitudes. Our\nfindings suggested the Blazhko effect is linked to pulsation parameters and\ncolors, rather than metallicities or absolute magnitude.",
        "Modern time-domain astronomical surveys produce high throughput data streams\nwhich require tools for processing and analysis. This will be critical for\nprograms making full use of the alert stream from the Vera Rubin Observatory\n(VRO), where spectroscopic labels will only be available for a small subset of\nall transients. In this context, the AMPEL toolset can work as a code-to-data\nplatform for the development of efficient, reproducible and flexible workflows\nfor real-time astronomical application.\n  We here introduce three different AMPEL channels constructed to highlight\ndifferent uses of alert streams: to rapidly find infant transients (SNGuess),\nto provide unbiased transient samples for follow-up (FollowMe) and to deliver\nfinal transient classifications (FinalBet). These pipelines already contain\nplaceholders for mechanisms which will be essential for the optimal usage of\nVRO alerts: combining different classifiers, including host galaxy information,\npopulation priors and sampling non-gaussian photometric redshift distributions.\nBased on the ELAsTiCC simulation, all three channels are already working at a\nhigh level: SNGuess correctly tags 99% of all young supernovae, FollowMe\nillustrates how an unbiased subset of alerts can be selected for spectroscopic\nfollow-up in the context of cosmological probes and FinalBet includes priors to\nachieve successful classifications for >~80% of all extragalactic transients.\n  The fully functional workflows presented here are all public and can be used\nas starting points for any group wishing to optimize pipelines for their\nspecific VRO science programs. AMPEL is designed to allow this to be done in\naccordance with FAIR principles: both software and results can be easily shared\nand results reproduced. The code-to-data environment ensures that models\ndeveloped this way can be directly applied to the real-time LSST stream parsed\nby AMPEL.",
        "Targeted maximum likelihood estimators (TMLEs) are asymptotically optimal\namong regular, asymptotically linear estimators. In small samples, however, we\nmay be far from \"asymptopia\" and not reap the benefits of optimality. Here we\npropose a variant (score-preserving TMLE; SP-TMLE) that leverages an initial\nestimator defined as the solution of a large number of possibly data-dependent\nscore equations. Instead of targeting only the efficient influence function in\nthe TMLE update to knock out the plug-in bias, we also target the\nalready-solved scores. Solving additional scores reduces the remainder term in\nthe von-Mises expansion of our estimator because these scores may come close to\nspanning higher-order influence functions. The result is an estimator with\nbetter finite-sample performance. We demonstrate our approach in simulation\nstudies leveraging the (relaxed) highly adaptive lasso (HAL) as our initial\nestimator. These simulations show that in small samples SP-TMLE has reduced\nbias relative to plug-in HAL and reduced variance relative to vanilla TMLE,\nblending the advantages of the two approaches. We also observe improved\nestimation of standard errors in small samples.",
        "Bivariate structural causal models (SCM) are often used to infer causal\ndirection by examining their goodness-of-fit under restricted model classes. In\nthis paper, we describe a parametrization of bivariate SCMs in terms of a\ncausal velocity by viewing the cause variable as time in a dynamical system.\nThe velocity implicitly defines counterfactual curves via the solution of\ninitial value problems where the observation specifies the initial condition.\nUsing tools from measure transport, we obtain a unique correspondence between\nSCMs and the score function of the generated distribution via its causal\nvelocity. Based on this, we derive an objective function that directly\nregresses the velocity against the score function, the latter of which can be\nestimated non-parametrically from observational data. We use this to develop a\nmethod for bivariate causal discovery that extends beyond known model classes\nsuch as additive or location scale noise, and that requires no assumptions on\nthe noise distributions. When the score is estimated well, the objective is\nalso useful for detecting model non-identifiability and misspecification. We\npresent positive results in simulation and benchmark experiments where many\nexisting methods fail, and perform ablation studies to examine the method's\nsensitivity to accurate score estimation.",
        "In this article, we study radio \\(k\\)-colorings of simple connected graphs\n\\(G\\) with diameter \\(d\\), where a radio \\(k\\)-coloring \\(g\\) assigns\nnon-negative integers to \\(V(G)\\) (vertices of \\(G\\)) such that \\(|g(u) - g(v)|\n\\geq 1 + k - d(u, v)\\) for any two vertices \\(u, v\\) with \\(1 \\leq k \\leq d\\).\nThe span of a radio \\(k\\)-coloring \\(g\\), expressed by \\(rc_k(g)\\), is the\nmaximum integer assigned by \\(g\\), and the radio \\(k\\)-chromatic number\n\\(rc_k(G)\\) is the minimum span among all radio \\(k\\)-colorings of \\(G\\). A\ncoloring \\(g\\) is minimal if \\(rc_k(g) = rc_k(G)\\). When \\(k = d-1\\), this\ncoloring is known as the antipodal coloring, and \\(rc_{d-1}(G)\\) referred to as\nthe antipodal number, is denoted by \\(ac(G)\\). We derive a sufficient condition\nfor an antipodal coloring to be minimal and apply this criterion to determine\nthe antipodal number of the generalized Petersen graph \\(GP(n,1)\\) for all\n\\(n\\) except when \\(n \\equiv 2 \\pmod{8}\\), and for toroidal grids \\(T_{r,s} =\nC_r \\square C_s\\) when \\(rs\\) is even. Additionally, we establish a lower bound\nfor \\(ac(T_{r,s})\\) when \\(rs\\) is odd.",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Recent advancements in quantum photonics have driven significant progress in\nphotonic quantum computing (PQC), addressing challenges in scalability,\nefficiency, and fault tolerance. Experimental efforts have focused on\nintegrated photonic platforms utilizing materials such as silicon photonics and\nlithium niobate to enhance performance. Parameters like photon loss rates,\ncoupling efficiencies, and fidelities have been pivotal, with state-of-the-art\nsystems achieving coupling efficiencies above 90% and photon\nindistinguishability exceeding 99%. Quantum error correction schemes have\nreduced logical error rates to below $10^{-3}$, marking a step toward\nfault-tolerant PQC. Photon generation has also advanced with deterministic\nsources, such as quantum dots, achieving brightness levels exceeding $10^6$\nphoton pairs\/s\/mW and time-bin encoding enabling scalable entanglement.\nHeralded single-photon sources now exhibit purities above 99%, driven by\ninnovations in fabrication techniques. High-efficiency photon detectors, such\nas superconducting nanowire single-photon detectors (SNSPDs), have demonstrated\ndetection efficiencies exceeding 98%, dark count rates below 1 Hz, and timing\njitters as low as 15 ps, ensuring precise photon counting and manipulation.\nMoreover, demonstrations of boson sampling with over 100 photons underscore the\ngrowing computational power of photonic systems, surpassing classical limits.\nThe integration of machine learning has optimized photonic circuit design,\nwhile frequency multiplexing and time-bin encoding have increased system\nscalability. Together, these advances bridge the gap between theoretical\npotential and practical implementation, positioning PQC as a transformative\ntechnology for computing, communication, and quantum sensing.",
        "We present a new proof of the sub-Gaussian norm concentration inequality. Our\nproof is based on an averaged version of the moment generating function termed\nthe averaged moment generating function. Compared with the widely adopted\n$\\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration\ninequality, our method does not rely on the union bound and promises a tighter\nconcentration bound.",
        "A novel approach is applied for improving forecast accuracy and achieving\ncoherence in forecasting the Italian daily energy generation time series. In\nhierarchical frameworks such as national energy generation disaggregated by\ngeographical zones and energy sources, independently generated base forecasts\noften result in inconsistencies across the constraints. We deal with this issue\nthrough a coherent balanced multi-task forecast combination approach, which\ncombines unbiased forecasts from multiple experts while ensuring coherence.\nApplied to the daily Italian electricity generation data, our method shows\nsuperior accuracy compared to single-task base and combined forecasts, and a\nstate-of-the-art single-expert reconciliation technique, demonstrating to be an\neffective approach to forecasting linearly constrained multiple time series.",
        "In the recent paper (Phys.Rev.Lett. 133 (2024) 12, 121601), a higher-order\nphase transition between the planar, charged, 5-dimensional\nReissner-Nordstr\\\"om-Anti-de Sitter black hole and a hairy black hole solution\nof the type IIB supergravity was investigated. Here, we set out to investigate\nthese two phases of the theory by means of the holographic probe that describes\na quark-antiquark in the dual gauge theory. We show that the study of the\nquark-antiquark potential turns out to be a useful method to investigate the\nchange of behavior at different values of the parameter that controls the phase\ntransition, this parameter being the ratio between the chemical potential and\nthe temperature. In other words, the string serves as a probe to detect the\nphase transition.",
        "Due to the limited accuracy of 4D Magnetic Resonance Imaging (MRI) in\nidentifying hemodynamics in cardiovascular diseases, the challenges in\nobtaining patient-specific flow boundary conditions, and the computationally\ndemanding and time-consuming nature of Computational Fluid Dynamics (CFD)\nsimulations, it is crucial to explore new data assimilation algorithms that\noffer possible alternatives to these limitations. In the present work, we study\nPhysics-Informed Neural Networks (PINNs), Deep Operator Networks (DeepONets),\nand their Physics-Informed extensions (PI-DeepONets) in predicting vascular\nflow simulations in the context of a 3D Abdominal Aortic Aneurysm (AAA)\nidealized model. PINN is a technique that combines deep neural networks with\nthe fundamental principles of physics, incorporating the physics laws, which\nare given as partial differential equations, directly into loss functions used\nduring the training process. On the other hand, DeepONet is designed to learn\nnonlinear operators from data and is particularly useful in studying parametric\npartial differential equations (PDEs), e.g., families of PDEs with different\nsource terms, boundary conditions, or initial conditions. Here, we adapt the\napproaches to address the particular use case of AAA by integrating the 3D\nNavier-Stokes equations (NSE) as the physical laws governing fluid dynamics. In\naddition, we follow best practices to enhance the capabilities of the models by\neffectively capturing the underlying physics of the problem under study. The\nadvantages and limitations of each approach are highlighted through a series of\nrelevant application cases. We validate our results by comparing them with CFD\nsimulations for benchmark datasets, demonstrating good agreements and\nemphasizing those cases where improvements in computational efficiency are\nobserved."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images",
    "start_abstract":"Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "HI-MaNGA: Results from (21cm-HI) single-dish observations of MaNGA\n  Survey Galaxies",
        "\\'El\\'ements de comptage sur les g\\'en\\'erateurs du groupe modulaire et\n  les $\\lambda$-quiddit\\'es",
        "Dark Energy Survey Year 6 Results: Synthetic-source Injection Across the\n  Full Survey Using Balrog",
        "On the Commuting Problem of Toeplitz Operators on the Harmonic Bergman\n  Space",
        "Generalization Performance of Hypergraph Neural Networks",
        "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
        "Properties of the one-component Coulomb gas on a sphere with two\n  macroscopic external charges",
        "Norm-one points in convex combinations of relatively weakly open subsets\n  of the unit ball in the spaces $L_1(\\mu,X)$",
        "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "Deriving motivic coactions and single-valued maps at genus zero from\n  zeta generators",
        "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to\n  Counteract Alpha Decay",
        "Relative knot probabilities in confined lattice polygons",
        "Global branching of solutions to ODEs and integrability",
        "Kink dynamics for the Yang-Mills field in an extremal\n  Reissner-Nordstr\\\"om black hole",
        "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party\n  Conversation Contexts",
        "70 MW-level picosecond mid-infrared radiation generation by difference\n  frequency generation in AgGaS2, BaGa4Se7, LiGaSe2, and LiGaS2",
        "A life in Mathematical Analysis: a conversation with Luigi Rodino",
        "Higher Riemann-Hilbert correspondence for foliations",
        "Performance of Practical Quantum Oblivious Key Distribution",
        "Human-Like Robot Impedance Regulation Skill Learning from Human-Human\n  Demonstrations",
        "Transient Chirality in the Gelation of Adhesive Spinner Monolayers",
        "Normal and inverse magnetocaloric effects in structurally disordered\n  Laves phase Y$_{1-x}$Gd$_{x}$Co$_{2}$ (0 $\\leq$ x $\\leq$ 1) compounds",
        "Keeping up with dynamic attackers: Certifying robustness to adaptive\n  online data poisoning",
        "Unified Multivariate Ordinal Model for analysis of sensory attributes",
        "Analog QAOA with Bayesian Optimisation on a neutral atom QPU",
        "Can one size fit all?: Measuring Failure in Multi-Document Summarization\n  Domain Transfer",
        "$S$, $T$, $U$ Parameters in The B-LSSM",
        "IRIS: An Immersive Robot Interaction System",
        "A Label-Free High-Precision Residual Moveout Picking Method for Travel\n  Time Tomography based on Deep Learning"
      ],
      "abstract":[
        "In a poster presentation for IAU Symposium 392: \"Neutral hydrogen in and\naround galaxies in the SKA era\", we gave an overview of the HI-MaNGA project\nwhich is working to obtain complementary information about the cold gas\n(neutral hydrogen traced by the radio 21cm line) content of Mapping Nearby\nGalaxies at Apache Point Observatory (MaNGA) sample galaxies. MaNGA, part of\nthe fourth incarnation of the Sloan Digital Sky Surveys (SDSS-IV), obtained\nspatially resolved spectral maps for 10,000 nearby galaxies selected to create\na representative sample out of the SDSS Main Galaxy Sample. MaNGA data have\nprovided a census of the stellar and ionized gas content of these galaxies, as\nwell as kinematics of both stars and gas. Adding HI information via the\nHI-MaNGA program, which has observed or collected 21cm line data for 70% of the\nfull MaNGA sample, has been crucial for a number of applications, but\nespecially understanding the physical mechanisms that regulate gas accretion,\nand through that star formation and quenching of star formation. This\nconference proceedings article accompanies the release of the DR3 version of\nHI-MaNGA data.",
        "The aim of this article is to count the $n$-tuples of positive integers\n$(a_{1},\\ldots,a_{n})$ solutions of the equation $\\begin{pmatrix} a_{n} & -1\n\\\\[4pt] 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1} & -1 \\\\[4pt] 1 & 0\n\\end{pmatrix} \\cdots \\begin{pmatrix} a_{1} & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}=\\pm\nM$ when $M$ is equal to the generators of the modular group $S=\\begin{pmatrix}\n0 & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}$ and $T=\\begin{pmatrix} 1 & 1 \\\\[4pt] 0 & 1\n\\end{pmatrix}$. To count these elements, we will study the\n$\\lambda$-quiddities, which are the solutions of the equation in the case\n$M=Id$ (related to Coxeter's friezes), whose last component is fixed.",
        "Synthetic source injection (SSI), the insertion of sources into pixel-level\non-sky images, is a powerful method for characterizing object detection and\nmeasurement in wide-field, astronomical imaging surveys. Within the Dark Energy\nSurvey (DES), SSI plays a critical role in characterizing all necessary\nalgorithms used in converting images to catalogs, and in deriving quantities\nneeded for the cosmology analysis, such as object detection rates, galaxy\nredshift estimation, galaxy magnification, star-galaxy classification, and\nphotometric performance. We present here a source injection catalog of $146$\nmillion injections spanning the entire 5000 deg$^2$ DES footprint, generated\nusing the Balrog SSI pipeline. Through this sample, we demonstrate that the DES\nYear 6 (Y6) image processing pipeline provides accurate estimates of the object\nproperties, for both galaxies and stars, at the percent-level, and we highlight\nspecific regimes where the accuracy is reduced. We then show the consistency\nbetween SSI and data catalogs, for all galaxy samples developed within the weak\nlensing and galaxy clustering analyses of DES Y6. The consistency between the\ntwo catalogs also extends to their correlations with survey observing\nproperties (seeing, airmass, depth, extinction, etc.). Finally, we highlight a\nnumber of applications of this catalog to the DES Y6 cosmology analysis. This\ndataset is the largest SSI catalog produced at this fidelity and will serve as\na key testing ground for exploring the utility of SSI catalogs in upcoming\nsurveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time.",
        "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
        "Hypergraph neural networks have been promising tools for handling learning\ntasks involving higher-order data, with notable applications in web graphs,\nsuch as modeling multi-way hyperlink structures and complex user interactions.\nYet, their generalization abilities in theory are less clear to us. In this\npaper, we seek to develop margin-based generalization bounds for four\nrepresentative classes of hypergraph neural networks, including\nconvolutional-based methods (UniGCN), set-based aggregation (AllDeepSets),\ninvariant and equivariant transformations (M-IGN), and tensor-based approaches\n(T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in\nwhich hypergraph structure and spectral norms of the learned weights can affect\nthe generalization bounds, where the key technical challenge lies in developing\nnew perturbation analysis for hypergraph neural networks, which offers a\nrigorous understanding of how variations in the model's weights and hypergraph\nstructure impact its generalization behavior. Our empirical study examines the\nrelationship between the practical performance and theoretical bounds of the\nmodels over synthetic and real-world datasets. One of our primary observations\nis the strong correlation between the theoretical bounds and empirical loss,\nwith statistically significant consistency in most cases.",
        "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
        "The one-component Coulomb gas on the sphere, consisting on $N$ unit charges\ninteracting via a logarithmic potential, and in the presence of two external\ncharges each of strength proportional to $N$, is considered. There are two\nspherical caps naturally associated with the external charges, giving rise to\ntwo distinct phases depending on them not overlapping (post-critical) or\noverlapping (pre-critical). The equilibrium measure in the post-critical phase\nis known from earlier work. We determine the equilibrium measure in the\npre-critical phase using a particular conformal map, with the parameters\ntherein specified in terms of a root of a certain fourth order polynomial. This\nis used to determine the exact form of the electrostatic energy for the\npre-critical phase. Using a duality relation from random matrix theory, the\npartition function for the Coulomb gas at the inverse temperature $\\beta = 2$\ncan be expanded for large $N$ in the post-critical phase, and in a scaling\nregion of the post and pre-critical boundary. For the pre-critical phase, the\nduality identity implies a relation between two electrostatic energies, one for\nthe present sphere system, and the other for a certain constrained log-gas\nrelating to the Jacobi unitary ensemble.",
        "In a paper published in 2020 in Studia Mathematica, Abrahamsen et al. proved\nthat in the real space $L_1(\\mu)$, where $\\mu$ is a non-zero $\\sigma$-finite\n(countably additive non-negative) measure, norm-one elements in finite convex\ncombinations of relatively weakly open subsets of the unit ball are interior\npoints of these convex combinations in the relative weak topology. In this\npaper that result is generalised by proving that the same is true in the (real\nor complex) Lebesgue--Bochner spaces $L_1(\\mu,X)$ where $X$ is a weakly\nuniformly rotund Banach space.",
        "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
        "Multiple polylogarithms are equipped with rich algebraic structures including\nthe motivic coaction and the single-valued map which both found fruitful\napplications in high-energy physics. In recent work arXiv:2312.00697, the\ncurrent authors presented a conjectural reformulation of the motivic coaction\nand the single-valued map via zeta generators, certain operations on\nnon-commuting variables in suitable generating series of multiple\npolylogarithms. In this work, the conjectures of the reference will be proven\nfor multiple polylogarithms that depend on any number of variables on the\nRiemann sphere.",
        "Alpha mining, a critical component in quantitative investment, focuses on\ndiscovering predictive signals for future asset returns in increasingly complex\nfinancial markets. However, the pervasive issue of alpha decay, where factors\nlose their predictive power over time, poses a significant challenge for alpha\nmining. Traditional methods like genetic programming face rapid alpha decay\nfrom overfitting and complexity, while approaches driven by Large Language\nModels (LLMs), despite their promise, often rely too heavily on existing\nknowledge, creating homogeneous factors that worsen crowding and accelerate\ndecay. To address this challenge, we propose AlphaAgent, an autonomous\nframework that effectively integrates LLM agents with ad hoc regularizations\nfor mining decay-resistant alpha factors. AlphaAgent employs three key\nmechanisms: (i) originality enforcement through a similarity measure based on\nabstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor\nalignment via LLM-evaluated semantic consistency between market hypotheses and\ngenerated factors, and (iii) complexity control via AST-based structural\nconstraints, preventing over-engineered constructions that are prone to\noverfitting. These mechanisms collectively guide the alpha generation process\nto balance originality, financial rationale, and adaptability to evolving\nmarket conditions, mitigating the risk of alpha decay. Extensive evaluations\nshow that AlphaAgent outperforms traditional and LLM-based methods in\nmitigating alpha decay across bull and bear markets, consistently delivering\nsignificant alpha in Chinese CSI 500 and US S&P 500 markets over the past four\nyears. Notably, AlphaAgent showcases remarkable resistance to alpha decay,\nelevating the potential for yielding powerful factors.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "We consider a natural generalisation of the Painlev\\'e property and use it to\nidentify the known integrable cases of the Lane-Emden equation with a real\npositive index. We classify certain first-order ordinary differential equations\nwith this property and find necessary conditions for a large family of\nsecond-order equations. We consider ODEs such that, given any simply connected\ndomain $\\Omega$ not containing fixed singularities of the equation, the Riemann\nsurface of any solution obtained by analytic continuation along curves in\n$\\Omega$ has a finite number of sheets over $\\Omega$.",
        "Considered in this work is the Yang-Mills field in an extremal\nReissner-Nordstr\\\"om black hole, a physically motivated mathematical model\nintroduced by Bizo\\'n and Kahl. The kink is a fundamental, strongly unstable\nstationary solution in this non-perturbative, variable coefficients model, with\na polynomial tail and no explicit form. In this paper, we introduce and extend\nseveral virial techniques, adapt them to the inhomogeneous medium setting, and\nconstruct a finite codimensional manifold of the energy space where the kink is\nasymptotically stable. In particular, we handle, using virial techniques, the\nemergence of a weak threshold resonance in the description of the stable\nmanifold.",
        "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities\/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis.",
        "Comparative study of nonlinear crystals for picosecond difference frequency\ngeneration in mid-IR is presented. Nonlinear crystals of AgGaS$_2$,\nBaGa$_4$Se$_7$, LiGaSe$_2$, and LiGaS$_2$ were studied. Samples of AgGaS$_2$,\nBaGa$_4$Se$_7$, LiGaSe$_2$, and LiGaS$_2$ were tested in thee sets having\nlengths of 2, 4, or 8 mm. In order to investigate the dependence of efficiency\non the crystal length, three sets of crystals with lengths of 2, 4, or 8 mm\nwere tested. The developed tunable DFG system was driven by the 1.03 $\\mu$m,\n1.8 ps, Yb:YAG thin-disk laser system operated at the repetition rate of 10 or\n100 Hz. As the best result, picosecond mid-IR pulses at a wavelength of $\\sim$7\n$\\mu$m with the energy up to 130 $\\mu$J corresponding to the peak power of\n$\\sim$72 MW were generated using the 8 mm long LiGaS$_2$ crystal. Using the\nBaGa$_4$Se$_7$ crystal, DFG tunability in the wavelength range from 6 up to 13\n$\\mu$m was achieved.",
        "This note is the transcription of an interview with Professor Luigi Rodino,\non the occasion of the ISAAC-ICMAM Conference of Analysis in Developing\nCountries (December 2, 2024 - Bogot\\`a), that was dedicated to him. Luigi\nRodino is at present Emeritus Professor at the University of Turin, and a\nmember of the Accademia delle Scienze di Torino.",
        "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Humans are experts in collaborating with others physically by regulating\ncompliance behaviors based on the perception of their partner states and the\ntask requirements. Enabling robots to develop proficiency in human\ncollaboration skills can facilitate more efficient human-robot collaboration\n(HRC). This paper introduces an innovative impedance regulation skill learning\nframework for achieving HRC in multiple physical collaborative tasks. The\nframework is designed to adjust the robot compliance to the human partner\nstates while adhering to reference trajectories provided by human-human\ndemonstrations. Specifically, electromyography (EMG) signals from human muscles\nare collected and analyzed to extract limb impedance, representing compliance\nbehaviors during demonstrations. Human endpoint motions are captured and\nrepresented using a probabilistic learning method to create reference\ntrajectories and corresponding impedance profiles. Meanwhile, an LSTMbased\nmodule is implemented to develop task-oriented impedance regulation policies by\nmapping the muscle synergistic contributions between two demonstrators.\nFinally, we propose a wholebody impedance controller for a human-like robot,\ncoordinating joint outputs to achieve the desired impedance and reference\ntrajectory during task execution. Experimental validation was conducted through\na collaborative transportation task and two interactive Tai Chi pushing hands\ntasks, demonstrating superior performance from the perspective of interactive\nforces compared to a constant impedance control method.",
        "Active systems of self-rotating elements inherently exhibit chirality, making\nthem of fundamental interest due to parity violation. Using large-scale\nhydrodynamic simulations, we investigate the gelation of adhesive spinners\nconfined to quasi-2D monolayers at low Reynolds numbers. Unlike the coarsening\ndynamics of passive colloids, spinner gelation follows a different pathway,\ndisplaying structural chirality during the early stages of aggregation.\nHowever, this chirality dissipates upon dynamical arrest, resulting in a final\ngel structure that resembles a conventional colloidal gel. As a result, we find\nno sign of odd mechanical responses. Nonetheless, the elastic modulus and\ngelation time remain tunable through spinning activity, providing a new avenue\nfor the bottom-up design of programmable soft materials.",
        "Magnetic and magnetocaloric properties of Y$_{1-x}$Gd$_{x}$Co$_{2}$\ncompounds, where x = 0.2, 0.4, 0.6, 0.8 and 1.0, were investigated\nexperimentally and theoretically. Crystal structures were characterized by\nX-ray diffraction (Rietveld analysis) and investigated samples possess the\nMgCu$_{2}$-type single phase with Fd-3m space group. Melt-spinning process\nintroduced a chemical and topological disorder, which directly affected the\nmagnetic properties. Refrigerant capacity (RC), strictly connected to the full\nwidth at half maximum $\\delta$TFWHM of the $\\Delta$S$_M$(T) curve and the\nmaximum of magnetic entropy changes $\\Delta$S$_{Mpk}$(T)(T,$\\Delta$H),\nincreases from 29 to 148 J\/kg with replacement of Y by Gd atoms from x = 0.2 to\nx = 0.8. RC and $\\delta$TFWHM indicate the presence of disorder. Temperature\ndependences of magnetic entropy change $\\Delta$S$_M$(T,$\\Delta$H) and RC were\nmeasured in as-quenched and annealed state for Y$_{0.4}$Gd$_{0.6}$Co$_{2}$.\nThis particular composition was chosen for detailed investigation mainly due to\nits Curie point (T$_C$ = 282 K), which is close to the room temperature. After\nisothermal annealing ($\\tau_a$ = 60 min, Ta = 700$^o$C) RC decreased from 122\nto 104 J\/kg, which clearly indicates the homogenization of the heat treated\nsample. Furthermore, observed inverse magnetocaloric effect is associated with\nthe presence of antiferromagnetically coupled Gd and Co magnetic moments. The\nphase transition temperature increases with increasing Gd content from 74 to\n407 K for Y$_{0.8}$Gd$_{0.2}$Co$_{2}$ and GdCo2, respectively. Within the\nFPLO-LDA DFT method, the non-magnetic ground state for YCo$_{2}$ and the\nmagnetic ground state for GdCo$_{2}$ are predicted in agreement with\nexperiment. The dependence of calculated total and species-resolved magnetic\nmoments on Gd concentration reasonably agrees with available experimental data.",
        "The rise of foundation models fine-tuned on human feedback from potentially\nuntrusted users has increased the risk of adversarial data poisoning,\nnecessitating the study of robustness of learning algorithms against such\nattacks. Existing research on provable certified robustness against data\npoisoning attacks primarily focuses on certifying robustness for static\nadversaries who modify a fraction of the dataset used to train the model before\nthe training algorithm is applied. In practice, particularly when learning from\nhuman feedback in an online sense, adversaries can observe and react to the\nlearning process and inject poisoned samples that optimize adversarial\nobjectives better than when they are restricted to poisoning a static dataset\nonce, before the learning algorithm is applied. Indeed, it has been shown in\nprior work that online dynamic adversaries can be significantly more powerful\nthan static ones. We present a novel framework for computing certified bounds\non the impact of dynamic poisoning, and use these certificates to design robust\nlearning algorithms. We give an illustration of the framework for the mean\nestimation and binary classification problems and outline directions for\nextending this in further work. The code to implement our certificates and\nreplicate our results is available at\nhttps:\/\/github.com\/Avinandan22\/Certified-Robustness.",
        "Experiments involving sensory analysis of foods and beverages are beneficial\nfor selecting healthy products and assessing the preferences of potential\nconsumers. They are generally planned in incomplete blocks, and their\nattributes, such as aroma, colour, and flavour, are evaluated using a 9-point\nhedonic scale, characterising an ordinal variable response. Also, the\ngeneralised logit model with random effects for panellists is one of the\nappropriate models to relate the multivariate response to the covariates. This\nstudy aims to present a method for analysing sensory attributes through a\nunified multivariate model. Due to the nature of the variable, each separate\nmodel already corresponds to a multivariate analysis, so our proposal would\nincorporate a complete analysis with solely one model. This proposal is based\non multivariate methods for categorical data and maximum likelihood theory. Our\nmethod was evaluated through a simulation study, in which we consider three\ndistinct formulations with two attributes to represent various formulation\nselection scenarios via mixed discrete models. The simulated results\ndemonstrated overall concordance rates exceeding 80\\% for the unified model\ncompared to the separate models. Moreover, as motivation is presented, a study\nof 13 prebiotic beverages based on cashew nut almonds added to grape juice,\nwith 130 potential consumers. The attributes evaluated were overall impression,\naroma, Body, sweetness and flavour, using a 9-point hedonic scale. The selected\nunified model considering all attributes was the non-proportional odds\nmixed-effect model. According to this model, the prebiotic beverage\nformulations most likely to be accepted were: 8\\% sugar and 40\\% grape juice\n($F_4$), 6\\% sugar and 44\\% grape juice ($F_6$), and 9\\% sugar and 30\\% grape\njuice ($F_{13}$). The unified analysis and computational time showed the\nadvantages of this proposal.",
        "This study explores the implementation of the Quantum Approximate\nOptimisation Algorithm (QAOA) in its analog form using a neutral atom quantum\nprocessing unit to solve the Maximum Independent Set problem. The analog QAOA\nleverages the natural encoding of problem Hamiltonians by Rydberg atom\ninteractions, while employing Bayesian Optimisation to navigate the\nquantum-classical parameter space effectively under the constraints of hardware\nnoise and resource limitations. We evaluate the approach through a combination\nof simulations and experimental runs on Pasqal's first commercial quantum\nprocessing unit, Orion Alpha, demonstrating effective parameter optimisation\nand noise mitigation strategies, such as selective bitstring discarding and\ndetection error corrections. Results show that a limited number of measurements\nstill allows for a quick convergence to a solution, making it a viable solution\nfor resource-efficient scenarios.",
        "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.",
        "Using the pinch technique, we compute the one-loop vertices of weak\ninteractions in the B-LSSM and incorporate their pinch contributions into the\ngauge boson self-energies. Compared to the definitions of the $S$, $T$, and $U$\nparameters in the Standard Model based on the $SU(2)_L\\otimes U(1)_Y$ group,\nthe corresponding parameters in the B-LSSM are modified. We provide these\nredefined $S$, $T$, and $U$ parameters and demonstrate the convergence of the\nresults. In the framework of the low-energy effective Lagrangian for weak\ninteractions, the $S$, $T$, and $U$ parameters can be expressed as functions of\ncertain parameters in the B-LSSM. The updated experimental and fitting results\nconstrain the parameter space of the B-LSSM strongly.",
        "This paper introduces IRIS, an immersive Robot Interaction System leveraging\nExtended Reality (XR), designed for robot data collection and interaction\nacross multiple simulators, benchmarks, and real-world scenarios. While\nexisting XR-based data collection systems provide efficient and intuitive\nsolutions for large-scale data collection, they are often challenging to\nreproduce and reuse. This limitation arises because current systems are highly\ntailored to simulator-specific use cases and environments. IRIS is a novel,\neasily extendable framework that already supports multiple simulators,\nbenchmarks, and even headsets. Furthermore, IRIS is able to include additional\ninformation from real-world sensors, such as point clouds captured through\ndepth cameras. A unified scene specification is generated directly from\nsimulators or real-world sensors and transmitted to XR headsets, creating\nidentical scenes in XR. This specification allows IRIS to support any of the\nobjects, assets, and robots provided by the simulators. In addition, IRIS\nintroduces shared spatial anchors and a robust communication protocol that\nlinks simulations between multiple XR headsets. This feature enables multiple\nXR headsets to share a synchronized scene, facilitating collaborative and\nmulti-user data collection. IRIS can be deployed on any device that supports\nthe Unity Framework, encompassing the vast majority of commercially available\nheadsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and\nthe HoloLens 2. IRIS showcased its versatility across a wide range of\nreal-world and simulated scenarios, using current popular robot simulators such\nas MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study\nevaluates IRIS on a data collection task for the LIBERO benchmark. The study\nshows that IRIS significantly outperforms the baseline in both objective and\nsubjective metrics.",
        "Residual moveout (RMO) provides critical information for travel time\ntomography. The current industry-standard method for fitting RMO involves\nscanning high-order polynomial equations. However, this analytical approach\ndoes not accurately capture local saltation, leading to low iteration\nefficiency in tomographic inversion. Supervised learning-based image\nsegmentation methods for picking can effectively capture local variations;\nhowever, they encounter challenges such as a scarcity of reliable training\nsamples and the high complexity of post-processing. To address these issues,\nthis study proposes a deep learning-based cascade picking method. It\ndistinguishes accurate and robust RMOs using a segmentation network and a\npost-processing technique based on trend regression. Additionally, a data\nsynthesis method is introduced, enabling the segmentation network to be trained\non synthetic datasets for effective picking in field data. Furthermore, a set\nof metrics is proposed to quantify the quality of automatically picked RMOs.\nExperimental results based on both model and real data demonstrate that,\ncompared to semblance-based methods, our approach achieves greater picking\ndensity and accuracy."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
      ],
      "abstract":[
        "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Sharp stability for critical points of the Sobolev inequality in the\n  absence of bubbling",
        "Free energy profiles for chemical reactions in solution from\n  high-dimensional neural network potentials: The case of the Strecker\n  synthesis",
        "Efficient Multivariate Robust Mean Estimation Under Mean-Shift\n  Contamination",
        "Mathematical Modelling of Mechanotransduction via RhoA Signalling\n  Pathways",
        "Nonparametric Smoothing of Directional and Axial Data",
        "The EFT Bootstrap at Finite $M_{PL}$",
        "Preconditioning for a Cahn-Hilliard-Navier-Stokes model for morphology\n  formation in organic solar cells",
        "Four-quark scatterings in QCD III",
        "Vacuum stress between conducting plates: the curved spacetime version",
        "Euler--Poincar\\'e reduction and the Kelvin--Noether theorem for discrete\n  mechanical systems with advected parameters and additional dynamics",
        "On the Prescribed Ricci Curvature of Noncompact Homogeneous Spaces with\n  Two Isotropy Summands",
        "Active bacterial baths in droplets",
        "Seeing Stereotypes",
        "Supercell environments using GridRad-Severe and the HRRR: Addressing\n  discrepancies between prior tornado datasets",
        "Indigenous Mathematics I. Smoke Telegraphy",
        "Calibration of the Polarimetric GNSS-R Sensor in the Rongowai Mission",
        "Nonlinear Spectroscopy as a Magnon Breakdown Diagnosis and its Efficient\n  Simulation",
        "Perspectives on Quantum Friction, Self-Propulsion, and Self-Torque",
        "Towards Quantitative Interpretation of 3D Atomic Force Microscopy at\n  Solid-Liquid Interfaces",
        "The Simons Observatory: Validation of reconstructed power spectra from\n  simulated filtered maps for the Small Aperture Telescope survey",
        "The Southern Twenty-centimetre All-sky Polarization Survey (STAPS):\n  survey description and maps",
        "Visualization of Organ Movements Using Automatic Region Segmentation of\n  Swallowing CT",
        "Pad\\'e metrics for black hole perturbations and light rings",
        "Bottomonium meson spectrum with quenched and unquenched quark models",
        "Sparse Hyperparametric Itakura-Saito NMF via Bi-Level Optimization",
        "Bounded powers of edge ideals: regularity and linear quotients",
        "Elastic Plateau-Rayleigh instability in soft cylinders: Surface\n  elasticity and periodic beading",
        "Efficient evaluation of real-time path integrals",
        "MolSpectra: Pre-training 3D Molecular Representation with Multi-modal\n  Energy Spectra"
      ],
      "abstract":[
        "When $u$ is close to a single Talenti bubble $v$ of the $p$-Sobolev\ninequality, we show that\n  \\begin{equation*}\n  \\|Du-Dv\\|_{L^p(\\mathbb{R}^n)}^{\\max\\{1,p-1\\}}\\le C \\|-{\\rm\ndiv}(|Du|^{p-2}Du)-|u|^{p^*-2}u\\|_{W^{-1,q}(\\mathbb{R}^n)}, \\end{equation*}\nwhere $C=C(n,p)>0$. This estimate provides a sharp stability estimate for the\nStruwe-type decomposition in the single bubble case, generalizing the result of\nCiraolo, Figalli, and Maggi \\cite{CFM2018} (focusing on the case $p=2$) to the\narbitrary $p$. Also, in the Sobolev setting, this answers an open problem\nraised by Zhou and Zou in \\cite[Remark 1.17]{ZZ2023}.",
        "Machine learning potentials (MLPs) have become a popular tool in chemistry\nand materials science as they combine the accuracy of electronic structure\ncalculations with the high computational efficiency of analytic potentials.\nMLPs are particularly useful for computationally demanding simulations such as\nthe determination of free energy profiles governing chemical reactions in\nsolution, but to date such applications are still rare. In this work we show\nhow umbrella sampling simulations can be combined with active learning of\nhigh-dimensional neural network potentials (HDNNPs) to construct free energy\nprofiles in a systematic way. For the example of the first step of Strecker\nsynthesis of glycine in aqueous solution we provide a detailed analysis of the\nimproving quality of HDNNPs for datasets of increasing size. We find that next\nto the typical quantification of energy and force errors with respect to the\nunderlying density functional theory data also the long-term stability of the\nsimulations and the convergence of physical properties should be rigorously\nmonitored to obtain reliable and converged free energy profiles of chemical\nreactions in solution.",
        "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1\/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings.",
        "We derive and simulate a mathematical model for mechanotransduction related\nto the Rho GTPase signalling pathway. The model addresses the bidirectional\ncoupling between signalling processes and cell mechanics. A numerical method\nbased on bulk-surface finite elements is proposed for the approximation of the\ncoupled system of nonlinear reaction-diffusion equations, defined inside the\ncell and on the cell membrane, and the equations of elasticity. Our simulation\nresults illustrate novel emergent features such as the strong dependence of the\ndynamics on cell shape, a threshold-like response to changes in substrate\nstiffness, and the fact that coupling mechanics and signalling can lead to the\nrobustness of cell deformation to larger changes in substrate stiffness,\nensuring mechanical homeostasis in agreement with experiments.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "We explore the impact of loop effects on positivity in effective field\ntheories emerging in the infrared from unitary and causal microscopic dynamics.\nFocusing on massless particles coupled to gravity, we address the treatment of\nforward-limit divergences from loop discontinuities and establish necessary\nconditions for maintaining computational control in perturbation theory. While\nloop effects remain small, ensuring consistency in our approach leads to a\nsignificant impact on bounds, even at tree level.",
        "We present a model for the morphology evolution of printed organic solar\ncells which occurs during the drying of a mixture of polymer, the non-fullerene\nacceptor and the solvent. Our model uses a phase field approach coupled to a\nNavier-Stokes equation describing the macroscopic movement of the fluid.\nAdditionally, we incorporate the evaporation process of the solvent using an\nAllen-Cahn equation.\n  The model is discretized using a finite-element approach with a semi-implicit\ndiscretization in time. The resulting (non)linear systems are coupled and of\nlarge dimensionality. We present a preconditioned iterative scheme to solve\nthem robustly with respect to changes in the discretization parameters. We\nillustrate that the preconditioned solver shows parameter-robust iteration\nnumbers and that the model qualitatively captures the behavior of the film\nmorphology during drying.",
        "We study the full infrared dynamics of 2+1 flavour QCD with the functional\nrenormalisation group approach. We resolve self-consistently the glue dynamics\nas well as the dynamics of chiral symmetry breaking. The computation hosts no\nphenomenological parameter or external input. The only ultraviolet input\nparameters are the physical ones in QCD: the light and strange quark masses.\nThey are adjusted to the physical ratios of the pion and kaon masses, divided\nby the pion decay constant. The results for other observables of current\nfirst-principles computations are in quantitative agreement with the physical\nones. This work completes the series of papers, initiated and furthered in\n[1,2], on dynamical chiral symmetry breaking and the emergence of mesonic bound\nstates within the functional renormalisation group. As a first application we\ndiscuss the formation of light mesonic bound states. Amongst other applications\nsuch as the phase structure of QCD, the current work paves the way for studying\nQCD parton distribution functions within the functional renormalisation group\napproach to first-principles QCD.",
        "Brown and Maclay \\cite{Brown} found the energy-momentum tensor for the\nCasimir effect of parallel plates in 1969. We find its curved spacetime version\nin a static background using the point splitting regularization method.\nPrevious results in the literature are reinforced and some consequences\ndiscussed.",
        "The Euler--Poincar\\'e equations, firstly introduced by Henri Poincar\\'e in\n1901, arise from the application of Lagrangian mechanics to systems on Lie\ngroups that exhibit symmetries, particularly in the contexts of classical\nmechanics and fluid dynamics. These equations have been extended to various\nsettings, such as semidirect products, advected parameters, and field theory,\nand have been widely applied to mechanics and physics. In this paper, we\nintroduce the discrete Euler--Poincar\\'e reduction for discrete Lagrangian\nsystems on Lie groups with advected parameters and additional dynamics,\nutilizing the group difference map technique. Specifically, the group\ndifference map is defined using either the Cayley transform or the matrix\nexponential. The continuous and discrete Kelvin--Noether theorems are extended\naccordingly, that account for Kelvin--Noether quantities of the corresponding\ncontinuous and discrete Euler--Poincar\\'e equations. As an application, we show\nboth continuous and discrete Euler--Poincar\\'e formulations about the dynamics\nof underwater vehicles, followed by numerical simulations. Numerical results\nillustrate the scheme's ability to preserve geometric properties over extended\ntime intervals, highlighting its potential for practical applications in the\ncontrol and navigation of underwater vehicles, as well as in other domains.",
        "This work studies simply connected, noncompact $G\/H$ in which $G$ is\nsemi-simple, $H$ is connected, and $G\/H$ has two irreducible summands. Here, we\nclassify all such spaces and we provide solutions to the so-called Prescribed\nRicci Curvature problem for all such spaces.",
        "Suspensions of self-propelled objects represent a novel paradigm in colloidal\nscience. In such active baths traditional concepts, such as Brownian motion,\nfluctuation-dissipation relations, and work extraction from heat reservoirs,\nmust be extended beyond the conventional framework of thermal baths. Unlike\nthermal baths, which are characterized by a single parameter, the temperature,\nthe fundamental descriptors of an active bath remain elusive, especially in\nconfined environments. In this study, buoyant, passive tracers are employed as\ngeneralized probes to investigate an active bath comprising motile bacteria\nconfined within a droplet. We demonstrate that momentum transfer from the bath\nto the tracer can be effectively described as colored noise, characterized by\ntemporal memory and an enhanced effective diffusivity significantly larger\ncompared to thermal Brownian motion values. Using a stochastic analytical\nframework, we extract the temporal memory and diffusivity parameters that\ndefine such an active bath. Notably, the diffusivity scales linearly with\nbacterial concentration, modulated by a factor representing the role of\nconfinement, expressed as the ratio of the confining radius to the probe\nradius. This finding, while still awaiting a complete theoretical explanation,\noffers new insights into the transport properties of confined active baths and\npaves the way for a deeper understanding of active emulsions driven by confined\nactive matter.",
        "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
        "Storm-relative helicity (SRH) is an important ingredient in supercell\ndevelopment, as well as mesocyclone intensity, and is linked to tornadogenesis\nand tornado potential. Derived from the storm-relative wind profile, SRH is\ncomposed of both the vertical wind shear and storm-relative flow. Recent\nstudies have come to conflicting findings regarding whether shallower or deeper\nlayers of SRH have more skill in tornado forecasting. Possible causes of this\ndiscrepancy include the use of observed versus model-based proximity soundings,\nas well as whether the storm-relative wind profile is determined via observed\nversus estimated storm motions. This study uses a new dataset of objectively\nidentified supercells, with observed storm motions, paired with high-resolution\nmodel analyses to address the discrepancies among prior studies. Unlike in\nprevious model-based tornado environmental datasets, the present approach\nreveals substantive differences in storm-relative flow, vertical wind shear,\nand SRH within the low-to-mid-levels between nontornadic and tornadic\nsupercells. Using observed storm motions for storm-relative variables further\nmagnifies differences in the low-to-mid-level storm-relative winds between\nnontornadic and tornadic supercells, ultimately leading to deeper layers of SRH\nhaving more forecast skill than near-ground SRH. Thus, the combination of a\nhigher-resolution model analyses, which better represents the near-storm\nenvironment, with observed storm motions appears to explain why many past\ntornado climatologies using model-based environmental analyses have failed to\nfind significant differences in the storm-relative wind profile. These results\nhelp bridge the gap between previous studies that employed coarser model-based\nanalyses with those that aggregated observed soundings from field projects.",
        "This article is the first in an occasional series for the Australian\nMathematical Society Gazette on diverse aspects and topics of Indigenous\nmathematical knowledge. This is an important, but neglected, part of the\nmathematical heritage of humankind, and as such is the concern of the\nmathematics community as a whole. It is hoped that this and future articles may\nhelp to inspire mathematics researchers, students, and educators at tertiary\nand school levels who are seeking to widen their mathematical horizons and\ndevelop course and research materials of broad cultural relevance.\n  I would like to honour the Mithaka peoples of the Kurrawoolben and\nKirrenderri (Diamantina) and Nooroondinna (Georgina) river channel country of\nsouth-western Qld, Australia. The material in this article does not involve\nculturally restricted knowledge or images, and is shared with respect for the\nMithaka ancestors and their descendants.",
        "Polarimetric GNSS-R systems, equipped with an additional polarization\nchannel, offer enhanced capabilities for separating vegetation and surface\nscattering effects, thereby improving GNSS-R land remote sensing applications\nsuch as soil moisture retrieval in vegetated and forested areas and biomass\nestimation. However, the effectiveness of these applications relies on accurate\ncalibration of the polarimetric GNSS-R sensor. In the Rongowai mission, a newly\ndeveloped Next Generation GNSS-R Receiver (NGRx) is installed on a domestic Air\nNew Zealand airplane to collect data during its commercial flights. The NGRx\nprocesses multi-GNSS satellite signals simultaneously and utilizes a\ndual-channel (LHCP and RHCP) antenna, thereby improving spatial coverage and\nretrieval accuracy. The dual-polarized antenna also provides the possibility to\nexamine the polarimetric GNSS-R system. In this article, a new methodology is\ndeveloped to calibrate the Level-1 power measurement and the on-board antenna\ncross-pol gain by comparing measurements from inland lakes and ocean with\nmodeled results. The calibration results in a 34% decrease in the uncertainty\nin co-pol reflectivity retrieval. The retrieved cross-pol and co-pol\nreflectivity after calibration are examined by their statistical distribution\nand spatial mapping with 1.5 km resolution, with multi-land surface types and\nincidence angles. These results validate the effectiveness of the calibration\nmethod and pave the way for future terrestrial science applications.",
        "Identifying quantum spin liquids, magnon breakdown, or fractionalized\nexcitations in quantum magnets is an ongoing challenge due to the ambiguity of\npossible origins of excitation continua occurring in linear response probes.\nRecently, it was proposed that techniques measuring higher-order response, such\nas two-dimensional coherent spectroscopy (2DCS), could resolve such\nambiguities. Numerically simulating nonlinear response functions can, however,\nbe computationally very demanding. We present an efficient Lanczos-based method\nto compute second-order susceptibilities $\\chi^{2}\\omega_t,\\omega_\\tau)$\ndirectly in the frequency domain. Applying this to extended Kitaev models\ndescribing $\\alpha$-RuCl$_3$, we find qualitatively different nonlinear\nresponses between intermediate magnetic field strengths and the high-field\nregime. To put these results into context, we derive the general 2DCS response\nof partially-polarized magnets within the linear spin-wave approximation,\nestablishing that $\\chi^2(\\omega_t,\\omega_\\tau)$ is restricted to a distinct\nuniversal form if the excitations are conventional magnons. Deviations from\nthis form, as predicted in our (Lanczos-based) simulations for\n$\\alpha$-RuCl$_3$, can hence serve in 2DCS experiments as direct criteria to\ndetermine whether an observed excitation continuum is of conventional\ntwo-magnon type or of different nature.",
        "This paper provides an overview of the nonequilibrium fluctuational forces\nand torques acting on a body either in motion or at rest relative to another\nbody or the thermal vacuum blackbody radiation. For a moving body, a retarding\nforce emerges, called quantum or Casimir friction, which in vacuum was first\npredicted by Einstein and Hopf in 1907. Moreover, if a stationary body is not\nin thermal equilibrium with the blackbody vacuum, a self-propulsive force or\ntorque can appear, resulting in a potentially observable linear or angular\nterminal velocity, even after thermalization.",
        "Three-dimensional atomic force microscopy (3D-AFM) has been a powerful tool\nto probe the atomic-scale structure of solid-liquid interfaces. As a nanoprobe\nmoves along the 3D volume of interfacial liquid, the probe-sample interaction\nforce is sensed and mapped, providing information on not only the solid\nmorphology, but also the liquid density distribution. To date 3D-AFM force maps\nof a diverse set of solid-liquid interfaces have been recorded, revealing\nremarkable force oscillations that are typically attributed to solvation layers\nor electrical double layers. However, despite the high resolution down to\nsub-angstrom level, quantitative interpretation of the 3D force maps has been\nan outstanding challenge. Here we will review the technical details of 3D-AFM\nand the existing approaches for quantitative data interpretation. Based on\nevidences in recent literature, we conclude that the perturbation-induced AFM\nforce paradoxically represents the intrinsic, unperturbed liquid density\nprofile. We will further discuss how the oscillatory force profiles can be\nattributed to the probe-modulation of the liquid configurational entropy, and\nhow the quantitative, atomic-scale liquid density distribution can be derived\nfrom the force maps.",
        "We present a transfer function-based method to estimate angular power spectra\nfrom filtered maps for cosmic microwave background (CMB) surveys. This is\nespecially relevant for experiments targeting the faint primordial\ngravitational wave signatures in CMB polarisation at large scales, such as the\nSimons Observatory (SO) small aperture telescopes. While timestreams can be\nfiltered to mitigate the contamination from low-frequency noise, usual methods\nthat calculate the mode coupling at individual multipoles can be challenging\nfor experiments covering large sky areas or reaching few-arcminute resolution.\nThe method we present here, although approximate, is more practical and faster\nfor larger data volumes. We validate it through the use of simulated\nobservations approximating the first year of SO data, going from half-wave\nplate-modulated timestreams to maps, and using simulations to estimate the\nmixing of polarisation modes induced by an example of time-domain filtering. We\nshow its performance through an example null test and with an end-to-end\npipeline that performs inference on cosmological parameters, including the\ntensor-to-scalar ratio $r$. The performance demonstration uses simulated\nobservations at multiple frequency bands. We find that the method can recover\nunbiased parameters for our simulated noise levels.",
        "We present data processing and verification of the Southern Twenty-centimetre\nAll-sky Polarization Survey (STAPS) conducted with Murriyang, the Parkes 64-m\ntelescope. The survey covers the sky area of -89<Dec<0 and the frequency range\nof 1.3-1.8 GHz split into 1-MHz channels. STAPS was observed commensally with\nthe S-band Polarization All-Sky Survey (S-PASS). The survey is composed of long\nazimuth scans, which allows us to absolutely calibrate Stokes Q and U with the\ndata processing procedure developed for S-PASS. We obtain I, Q, and U maps in\nboth flux density scale (Jy\/beam) and main beam brightness temperature scale\n(K), for the 301 frequency channels with sufficiently good data. The\ntemperature scale is tied to the Global Magneto-ionic Medium Survey (GMIMS)\nhigh-band north sky survey conducted with the Dominion Radio Astrophysical\nObservatory 26-m telescope. All the STAPS maps are smoothed to a common\nresolution of 20 arcmin. The rms noise per channel ranges from about 16 mK to 8\nmK for I, and from about 8 mK to 5 mK for Q and U at frequencies from 1.3 to\n1.8 GHz. The rms noise in Q and U varies with declination and reaches minimum\nat declination of -89 degree. We also run rotation measure (RM) synthesis and\nRM clean to obtain peak polarized intensity and Faraday depth maps. The whole\nSTAPS data processing is validated by comparing flux densities of compact\nsources, pixel flux density versus pixel flux density for Cen A, pixel\ntemperature versus pixel temperature for the entire survey area, and RMs of\nextragalactic sources between STAPS and other measurements. The uncertainty of\nthe flux density scale is less than 10%. STAPS delivers an L-band (20 cm)\nmulti-frequency polarization view of the Galaxy, and will help advance our\nunderstanding of the Galactic magnetic field and magnetized interstellar\nmedium.",
        "This study presents the first report on the development of an artificial\nintelligence (AI) for automatic region segmentation of four-dimensional\ncomputer tomography (4D-CT) images during swallowing. The material consists of\n4D-CT images taken during swallowing. Additionally, data for verifying the\npracticality of the AI were obtained from 4D-CT images during mastication and\nswallowing. The ground truth data for the region segmentation for the AI were\ncreated from five 4D-CT datasets of swallowing. A 3D convolutional model of\nnnU-Net was used for the AI. The learning and evaluation method for the AI was\nleave-one-out cross-validation. The number of epochs for training the nnU-Net\nwas 100. The Dice coefficient was used as a metric to assess the AI's region\nsegmentation accuracy. Regions with a median Dice coefficient of 0.7 or higher\nincluded the bolus, bones, tongue, and soft palate. Regions with a Dice\ncoefficient below 0.7 included the thyroid cartilage and epiglottis. Factors\nthat reduced the Dice coefficient included metal artifacts caused by dental\ncrowns in the bolus and the speed of movement for the thyroid cartilage and\nepiglottis. In practical verification of the AI, no significant misrecognition\nwas observed for facial bones, jaw bones, or the tongue. However, regions such\nas the hyoid bone, thyroid cartilage, and epiglottis were not fully delineated\nduring fast movement. It is expected that future research will improve the\naccuracy of the AI's region segmentation, though the risk of misrecognition\nwill always exist. Therefore, the development of tools for efficiently\ncorrecting the AI's segmentation results is necessary. AI-based visualization\nis expected to contribute not only to the deepening of motion analysis of\norgans during swallowing but also to improving the accuracy of swallowing CT by\nclearly showing the current state of its precision.",
        "Most distinguishing features of black holes and their mimickers are\nconcentrated near the horizon. In contrast, astrophysical observations and\ntheoretical considerations primarily constrain the far-field geometry. In this\nwork we develop tools to effectively describe both, using the two-point Pad\\'e\napproximation to construct interpolating metrics connecting the near and\nfar-field. We extend our previous work by computing the quasinormal modes of\ngravitational perturbations for static, spherically symmetric metrics that\ndeviate from Schwarzschild spacetime. Even at the lowest order, this approach\ncompares well with existing methods in both accuracy and applicability.\nAdditionally, we show that the lowest-order interpolating metric reliably\npredicts light ring locations. It closely matches exact results, even when\nunsuitable for quasinormal frequency calculations.",
        "An open question in hadronic phenomenology concerns the ``unquenching\"\neffects of higher Fock space components on the leading Fock space description\nof hadrons. We address this by making a comparison of the bottomonium spectrum\nas computed with the relativized Godfrey-Isgur quark model and an unquenched\ncoupled channel model driven by the ``$^3P_0$\" mechanism of hadronic decay. Our\nresults show that both models can describe the spectrum well, indicating that\nthe influence of coupled channel effects can be largely absorbed into the\nparameters of the quenched quark model. This conclusion is reinforced by a\nperturbative calculation that shows that the spin-dependence of mass splittings\ndue to mixing with the continuum recapitulates quenched quark model\nspin-dependent interactions. We also show that softening of the quark-antiquark\nwavefunction due to continuum mixing improves the description of vector\nbottomonium decay constants. Together, these results illustrate and\nsubstantiate the surprising robustness of simple constituent quark model\ndescriptions of hadrons.",
        "The selection of penalty hyperparameters is a critical aspect in Nonnegative\nMatrix Factorization (NMF), since these values control the trade-off between\nthe reconstruction accuracy and the adherence to desired constraints. In this\nwork, we focus on an NMF problem involving the Itakura-Saito (IS) divergence,\neffective for extracting low spectral density components from spectrograms of\nmixed signals, enhanced with sparsity constraints. We propose a new algorithm\ncalled SHINBO, which introduces a bi-level optimization framework to\nautomatically and adaptively tune the row-dependent penalty hyperparameters,\nenhancing the ability of IS-NMF to isolate sparse, periodic signals against\nnoise. Experimental results showed SHINBO ensures precise spectral\ndecomposition and demonstrates superior performance in both synthetic and\nreal-world applications. For the latter, SHINBO is particularly useful, as\nnoninvasive vibration-based fault detection in rolling bearings, where the\ndesired signal components often reside in high-frequency subbands but are\nobscured by stronger, spectrally broader noise. By addressing the critical\nissue of hyperparameter selection, SHINBO advances the state-of-the-art in\nsignal recovery for complex, noise-dominated environments.",
        "Let $S=K[x_1, \\ldots,x_n]$ denote the polynomial ring in $n$ variables over a\nfield $K$ and let $I \\subset S$ be a monomial ideal. For a vector\n$\\mathfrak{c}\\in\\mathbb{N}^n$, we set $I_{\\mathfrak{c}}$ to be the ideal\ngenerated by monomials belonging to $I$ whose exponent vectors are\ncomponentwise bounded above by $\\mathfrak{c}$. Also, let\n$\\delta_{\\mathfrak{c}}(I)$ be the largest integer $k$ such that\n$(I^k)_{\\mathfrak{c}}\\neq 0$. It is shown that for every graph $G$ with edge\nideal $I(G)$, the ideal $(I(G)^{\\delta_{\\mathfrak{c}}(I)})_{\\mathfrak{c}}$ is a\npolymatroidal ideal. Moreover, we show that for each integer $s=1, \\ldots\n\\delta_{\\mathfrak{c}}(I(G))$, the Castelnuovo--Mumford regularity of\n$(I(G)^s)_{\\mathfrak{c}}$ is bounded above by $\\delta_{\\mathfrak{c}}(I(G))+s$.",
        "The Plateau-Rayleigh instability shows that a cylindrical fluid flow can be\ndestabilized by surface tension. Similarly, capillary forces can make an\nelastic cylinder unstable when the elastocapillary length is comparable to the\ncylinder's radius. While existing models predict a single isolated bulge as the\nresult of an instability, experiments reveal a periodic sequence of bulges\nspaced out by thinned regions, a phenomenon known as beading instability. Most\nmodels assume that surface tension is independent of the deformation of the\nsolid, neglecting variations due to surface stretch.\n  In this work, we assume that surface tension arises from the deformation of\nmaterial particles near the free surface, treating it as a pre-stretched\nelastic surface surrounding the body. Using the theoretical framework proposed\nby Gurtin and Murdoch, we show that a cylindrical solid can undergo a\nmechanical instability with a finite critical wavelength if the body is\nsufficiently soft or axially stretched. Post-buckling numerical simulations\nreveal a morphology in qualitative agreement with experimental observations.\nPeriod-halving secondary bifurcations are also observed. The results of this\nresearch have broad implications for soft materials, biomechanics, and\nmicrofabrication applications where surface tension plays a crucial role.",
        "The Feynman path integral has revolutionized modern approaches to quantum\nphysics. Although the path integral formalism has proven very successful and\nspawned several approximation schemes, the direct evaluation of real-time path\nintegrals is still extremely expensive and numerically delicate due to its\nhigh-dimensional and oscillatory nature. We propose an efficient method for the\nnumerical evaluation of the real-time world-line path integral for theories\nwhere the potential is dominated by a quadratic at infinity. This is done by\nrewriting the high-dimensional oscillatory integral in terms of a series of\nlow-dimensional oscillatory integrals, that we efficiently evaluate with\nPicard-Lefschetz theory or approximate with the eikonal approximation.\nSubsequently, these integrals are stitched together with a series of fast\nFourier transformations to recover the lattice regularized Feynman path\nintegral. Our method directly applies to problems in quantum mechanics, the\nword-line quantization of quantum field theory, and quantum gravity.",
        "Establishing the relationship between 3D structures and the energy states of\nmolecular systems has proven to be a promising approach for learning 3D\nmolecular representations. However, existing methods are limited to modeling\nthe molecular energy states from classical mechanics. This limitation results\nin a significant oversight of quantum mechanical effects, such as quantized\n(discrete) energy level structures, which offer a more accurate estimation of\nmolecular energy and can be experimentally measured through energy spectra. In\nthis paper, we propose to utilize the energy spectra to enhance the\npre-training of 3D molecular representations (MolSpectra), thereby infusing the\nknowledge of quantum mechanics into the molecular representations.\nSpecifically, we propose SpecFormer, a multi-spectrum encoder for encoding\nmolecular spectra via masked patch reconstruction. By further aligning outputs\nfrom the 3D encoder and spectrum encoder using a contrastive objective, we\nenhance the 3D encoder's understanding of molecules. Evaluations on public\nbenchmarks reveal that our pre-trained representations surpass existing methods\nin predicting molecular properties and modeling dynamics."
      ]
    }
  },
  {
    "id":2412.02695,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"EEG data for ADHD \/ Control children",
    "start_abstract":"Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors. EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes. Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child\u2019s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child\u2019s performance (i.e. response speed).",
    "start_categories":[
      "Neurotherapeutics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Deep Residual Learning for Image Recognition"
      ],
      "abstract":[
        "Deeper neural networks are more difficult to train. We present a residual learning framework ease the training of that substantially deeper than those used previously. explicitly reformulate layers as functions with reference layer inputs, instead unreferenced functions. provide comprehensive empirical evidence showing these easier optimize, and can gain accuracy from considerably increased depth. On ImageNet dataset we evaluate nets depth up 152 - 8\u00d7 VGG [40] but still having lower complexity. An ensemble achieves 3.57% error on test set. This result won 1st place ILSVRC 2015 classification task. also analysis CIFAR-10 100 1000 layers. The representations is central importance for many visual recognition tasks. Solely due our extremely deep representations, obtain 28% relative improvement COCO object detection dataset. Deep foundations submissions & competitions1, where places tasks detection, localization, segmentation."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "De Sitter Horizon Edge Partition Functions",
        "Reducing Simulation Effort for RIS Optimization using an Efficient\n  Far-Field Approximation",
        "Minerva: A Programmable Memory Test Benchmark for Language Models",
        "Connecting the Unconnectable through Feedback",
        "Language Models for Automated Classification of Brain MRI Reports and\n  Growth Chart Generation",
        "Spider's webs and sharp $L^p$ bounds for the Hardy--Littlewood maximal\n  operator on Gromov hyperbolic spaces",
        "Polarisation conversion and optical meron topologies in anisotropic\n  epsilon-near-zero metamaterials",
        "Censor Resistant Instruction Independent Obfuscation for Multiple\n  Programs",
        "Bell Inequality Violation of Light Quarks in Back-to-Back Dihadron Pair\n  Production at Lepton Colliders",
        "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment",
        "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
        "Intelligent Framework for Human-Robot Collaboration: Safety, Dynamic\n  Ergonomics, and Adaptive Decision-Making",
        "X-Dyna: Expressive Dynamic Human Image Animation",
        "One Model to Train them All: Hierarchical Self-Distillation for Enhanced\n  Early Layer Embeddings",
        "Parametric Hypersensitivity and Transport in the Steady-State\n  Open-System Holstein Model",
        "Exploring strong electronic correlations in the breathing kagome metal\n  Fe$_3$Sn",
        "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared\n  Imaging",
        "Multi-agent coordination via communication partitions",
        "Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial\n  CDW Ground State",
        "The MAGPI Survey: the kinematic morphology-density relation (or lack\n  thereof) and the Hubble sequence at $z\\sim0.3$",
        "Egoistic MDS-based Rigid Body Localization",
        "Collision Risk Quantification and Conflict Resolution in Trajectory\n  Tracking for Acceleration-Actuated Multi-Robot Systems",
        "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance\n  Software Engineering?",
        "Se-HiLo: Noise-Resilient Semantic Communication with High-and-Low\n  Frequency Decomposition",
        "Embedding of a Discrete Lattice Structure in a Smooth Manifold",
        "Nontrapping Tunable Topological Photonic Memory",
        "Exploring the Finite-Temperature Behavior of Rydberg Atom Arrays: A\n  Tensor Network Approach",
        "CopyJudge: Automated Copyright Infringement Identification and\n  Mitigation in Text-to-Image Diffusion Models",
        "Causality Enhanced Origin-Destination Flow Prediction in Data-Scarce\n  Cities"
      ],
      "abstract":[
        "One-loop $S^{d+1}$ path integrals were shown to factorize into two parts: a\nbulk thermal ideal gas partition function in a $dS_{d+1}$ static patch and an\nedge partition function associated with degrees of freedom living on $S^{d-1}$.\nHere, we analyze the $\\mathfrak{so}(d)$ structure of the edge partition\nfunctions for massive and massless totally symmetric tensors of arbitrary rank\nin any $d\\geq 3$. For linearized Einstein gravity on $S^{d+1}$, we find that\nthe edge partition function receives contributions from shift-symmetric vector\nand scalar fields on $S^{d-1}$ that nonlinearly realize the isometry group\n$SO(d+2)$ of $S^{d+1}$, suggesting a possible interpretation in terms of an\nembedded $S^{d-1}$ brane.",
        "Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously\nintroduced method is effective, but time-consuming, because multiport impedance\nor scatter matrices are required for each transmitter and receiver position,\nwhich generally must be obtained through full-wave simulation. Herein, a simple\nand efficient far-field approximation is introduced, to extrapolate scatter\nmatrices for arbitrary receiver and transmitter positions from only a single\nsimulation while still maintaining high accuracy suitable for optimization\npurposes. This is demonstrated through comparisons of the optimized capacitance\nvalues and further supported by empirical measurements.",
        "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs.",
        "Reliable uplink connectivity remains a persistent challenge for IoT devices,\nparticularly those at the cell edge, due to their limited transmit power and\nsingle-antenna configurations. This paper introduces a novel framework aimed at\nconnecting the unconnectable, leveraging real-time feedback from access points\n(APs) to enhance uplink coverage without increasing the energy consumption of\nIoT devices. At the core of this approach are feedback channel codes, which\nenable IoT devices to dynamically adapt their transmission strategies based on\nAP decoding feedback, thereby reducing the critical uplink SNR required for\nsuccessful communication. Analytical models are developed to quantify the\ncoverage probability and the number of connectable APs, providing a\ncomprehensive understanding of the system's performance. Numerical results\nvalidate the proposed method, demonstrating substantial improvements in\ncoverage range and connectivity, particularly for devices at the cell edge,\nwith up to a 51% boost in connectable APs. Our approach offers a robust and\nenergy-efficient solution to overcoming uplink coverage limitations, enabling\nIoT networks to connect devices in challenging environments.",
        "Clinically acquired brain MRIs and radiology reports are valuable but\nunderutilized resources due to the challenges of manual analysis and data\nheterogeneity. We developed fine-tuned language models (LMs) to classify brain\nMRI reports as normal (reports with limited pathology) or abnormal, fine-tuning\nBERT, BioBERT, ClinicalBERT, and RadBERT on 44,661 reports. We also explored\nthe reasoning capabilities of a leading LM, Gemini 1.5-Pro, for normal report\ncategorization. Automated image processing and modeling generated brain growth\ncharts from LM-classified normal scans, comparing them to human-derived charts.\nFine-tuned LMs achieved high classification performance (F1-Score >97%), with\nunbalanced training mitigating class imbalance. Performance was robust on\nout-of-distribution data, with full text outperforming summary (impression)\nsections. Gemini 1.5-Pro showed a promising categorization performance,\nespecially with clinical inference. LM-derived brain growth charts were nearly\nidentical to human-annotated charts (r = 0.99, p < 2.2e-16). Our LMs offer\nscalable analysis of radiology reports, enabling automated classification of\nbrain MRIs in large datasets. One application is automated generation of brain\ngrowth charts for benchmarking quantitative image features. Further research is\nneeded to address data heterogeneity and optimize LM reasoning.",
        "In this paper we prove that if $1<a\\leq b<a^2$ and $X$ is a locally doubling\n$\\delta$-hyperbolic complete connected length metric measure space with\n$(a,b)$-pinched exponential growth at infinity, then the centred\nHardy--Littlewood maximal operator $\\mathcal M$ is bounded on $L^p(X)$ for all\n$p>\\tau$, and it is of weak type $(\\tau,\\tau)$, where $\\tau := \\log_ab$. A key\nstep in the proof is a new structural theorem for Gromov hyperbolic spaces with\n$(a,b)$-pinched exponential growth at infinity, consisting in a discretisation\nof $X$ by means of certain graphs, introduced in this paper and called spider's\nwebs, with ``good connectivity properties\". Our result applies to trees with\nbounded geometry, and Cartan--Hadamard manifolds of pinched negative curvature,\nproviding new boundedness results in these settings. The index $\\tau$ is\noptimal in the sense that if $p<\\tau$, then there exists $X$ satisfying the\nassumptions above such that $\\mathcal M$ is not of weak type $(p,p)$.\nFurthermore, if $b>a^2$, then there are examples of spaces $X$ satisfying the\nassumptions above such that $\\mathcal M$ bounded on $L^p(X)$ if and only if\n$p=\\infty$.",
        "Plasmonic metamaterials provide a flexible platform for light manipulation\nand polarisation management, thanks to their engineered optical properties with\nexotic dispersion regimes. Here, we exploit the enhanced spin-orbit coupling\ninduced by the strong anisotropy of plasmonic nanorod metamaterials to control\nthe polarisation of vector vortex beams and generate complex field structures\nwith meron topology. Modifying the degree of ellipticity of the input\npolarisation, we show how the observed meron topology can be additionally\nmanipulated. Flexible control of the state of polarisation of vortex beams is\nimportant in optical manipulation, communications, metrology and quantum\ntechnologies.",
        "This work builds upon and optimizes our prior research on obfuscation as\ninstruction decorrelation which achieves multiple program obfuscation.\nLeveraging this infrastructure, we further achieve the property of\nsensor-resistant computation.",
        "Spin correlations between particles produced at colliders provide valuable\ninsights for quantum information studies. While traditional studies of quantum\ninformation at colliders are typically limited to massive particles with\nperturbative decay, we propose an innovative method to explore the Bell\ninequality in massless quark pair systems by analyzing the azimuthal\ncorrelations in back-to-back $\\pi^+\\pi^-$ dihadron pair production at lepton\ncolliders. Revisiting the Belle data, we have shown the potential to detect\nBell inequality violation of light quarks by introducing an additional angular\ncut, achieving a significance of 2.5 $\\sigma$ even in the worst-case scenario\nof 100% correlated systematic uncertainties in each bins. The significance\nsubstantially exceeds $5\\sigma$ when considering uncorrelated systematic\nuncertainties. Our approach opens avenues for exploring spin quantum\ninformation in the non-perturbative aspect and leverages existing data for\nquantum information research.",
        "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
        "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https:\/\/github.com\/cxliu0314\/CoLMDriver.",
        "The integration of collaborative robots into industrial environments has\nimproved productivity, but has also highlighted significant challenges related\nto operator safety and ergonomics. This paper proposes an innovative framework\nthat integrates advanced visual perception technologies, real-time ergonomic\nmonitoring, and Behaviour Tree (BT)-based adaptive decision-making. Unlike\ntraditional methods, which often operate in isolation or statically, our\napproach combines deep learning models (YOLO11 and SlowOnly), advanced tracking\n(Unscented Kalman Filter) and dynamic ergonomic assessments (OWAS), offering a\nmodular, scalable and adaptive system. Experimental results show that the\nframework outperforms previous methods in several aspects: accuracy in\ndetecting postures and actions, adaptivity in managing human-robot\ninteractions, and ability to reduce ergonomic risk through timely robotic\ninterventions. In particular, the visual perception module showed superiority\nover YOLOv9 and YOLOv8, while real-time ergonomic monitoring eliminated the\nlimitations of static analysis. Adaptive role management, made possible by the\nBehaviour Tree, provided greater responsiveness than rule-based systems, making\nthe framework suitable for complex industrial scenarios. Our system\ndemonstrated a 92.5\\% accuracy in grasping intention recognition and\nsuccessfully classified ergonomic risks with real-time responsiveness (average\nlatency of 0.57 seconds), enabling timely robotic",
        "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for\nanimating a single human image using facial expressions and body movements\nderived from a driving video, that generates realistic, context-aware dynamics\nfor both the subject and the surrounding environment. Building on prior\napproaches centered on human pose control, X-Dyna addresses key shortcomings\ncausing the loss of dynamic details, enhancing the lifelike qualities of human\nvideo animations. At the core of our approach is the Dynamics-Adapter, a\nlightweight module that effectively integrates reference appearance context\ninto the spatial attentions of the diffusion backbone while preserving the\ncapacity of motion modules in synthesizing fluid and intricate dynamic details.\nBeyond body pose control, we connect a local control module with our model to\ncapture identity-disentangled facial expressions, facilitating accurate\nexpression transfer for enhanced realism in animated scenes. Together, these\ncomponents form a unified framework capable of learning physical human motion\nand natural scene dynamics from a diverse blend of human and scene videos.\nComprehensive qualitative and quantitative evaluations demonstrate that X-Dyna\noutperforms state-of-the-art methods, creating highly lifelike and expressive\nanimations. The code is available at https:\/\/github.com\/bytedance\/X-Dyna.",
        "Deploying language models often requires handling model size vs. performance\ntrade-offs to satisfy downstream latency constraints while preserving the\nmodel's usefulness. Model distillation is commonly employed to reduce model\nsize while maintaining acceptable performance. However, distillation can be\ninefficient since it involves multiple training steps. In this work, we\nintroduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters,\nuseful for multiple tasks within the scope of code retrieval.\nMODULARSTARENCODER is trained with a novel self-distillation mechanism that\nsignificantly improves lower-layer representations-allowing different portions\nof the model to be used while still maintaining a good trade-off in terms of\nperformance. Our architecture focuses on enhancing text-to-code and\ncode-to-code search by systematically capturing syntactic and semantic\nstructures across multiple levels of representation. Specific encoder layers\nare targeted as exit heads, allowing higher layers to guide earlier layers\nduring training. This self-distillation effect improves intermediate\nrepresentations, increasing retrieval recall at no extra training cost. In\naddition to the multi-exit scheme, our approach integrates a repository-level\ncontextual loss that maximally utilizes the training context window, further\nenhancing the learned representations. We also release a new dataset\nconstructed via code translation, seamlessly expanding traditional text-to-code\nbenchmarks with code-to-code pairs across diverse programming languages.\nExperimental results highlight the benefits of self-distillation through\nmulti-exit supervision.",
        "We demonstrate that the nonequilibrium steady state (NESS) of an open-system\nHolstein model with linear bias displays extreme sensitivity to the closed\nsystem parameters. This sensitivity is shown to correspond to avoided crossings\nin the closed system spectrum, as previously demonstrated in the Rabi model. We\nthen develop a kinetic model to analyze the effects of environmental parameters\non NESS hypersensitivity. This reveals that hypersensitivity only exists in\nintermediate environmental parameter regimes, a prediction that is verified\nnumerically. The inherent spatial character of the Holstein model offers a\nnatural connection to transport, revealing that transport properties in the\nsteady-state regime can be optimized by simultaneously coordinating the closed-\nand open-system parameters.",
        "Kagome metals have emerged as pivotal materials in condensed matter physics\ndue to their unique geometric arrangement and intriguing electronic properties.\nUnderstanding the origin of magnetism in these materials, particularly in iron\nrich Fe-Sn binary compounds like Fe$_3$Sn, holds a significant importance, as\nthey represent potential candidates for permanent magnets with a high Curie\ntemperature and a strong magnetic anisotropy. In the present study, we employ\ndensity-functional theory and dynamical mean-field theory to analyze the\nelectronic structure and magnetic properties of Fe$_3$Sn. Our investigation\nreveals the presence of several nearly-flat bands and Weyl nodes at low\nexcitation energies. The inclusion of local correlation effects is shown to\npush these features even closer to the Fermi energy, which may be important for\ntheir manipulation via external stimuli. Regarding magnetism, the Hubbard-like\ninteraction leads to an increase of orbital polarization at the expenses of a\nminor reduction of the spin moment. The magnetic anisotropy energy exhibits a\nstrong dependence on the particular choice of the Coulomb interaction\nparameters. Additionally, our detailed analysis of the interatomic exchange\ninteractions indicates a significant contribution from the antisymmetric\nexchange, i.e. the Dzyaloshinskii-Moriya interaction, which showcases the\nexistence of magnetic chirality in the system. Overall, our investigation\nhighlights a strong interplay between the flat bands near the Fermi level, the\nlocal Coulomb interaction and the triangular geometry of the lattice, which\nplays a crucial role in driving the magnetic properties of this material.",
        "Thermal imaging is often compromised by dynamic, complex degradations caused\nby hardware limitations and unpredictable environmental factors. The scarcity\nof high-quality infrared data, coupled with the challenges of dynamic,\nintricate degradations, makes it difficult to recover details using existing\nmethods. In this paper, we introduce thermal degradation simulation integrated\ninto the training process via a mini-max optimization, by modeling these\ndegraded factors as adversarial attacks on thermal images. The simulation is\ndynamic to maximize objective functions, thus capturing a broad spectrum of\ndegraded data distributions. This approach enables training with limited data,\nthereby improving model performance.Additionally, we introduce a\ndual-interaction network that combines the benefits of spiking neural networks\nwith scale transformation to capture degraded features with sharp spike signal\nintensities. This architecture ensures compact model parameters while\npreserving efficient feature representation. Extensive experiments demonstrate\nthat our method not only achieves superior visual quality under diverse single\nand composited degradation, but also delivers a significant reduction in\nprocessing when trained on only fifty clear images, outperforming existing\ntechniques in efficiency and accuracy. The source code will be available at\nhttps:\/\/github.com\/LiuZhu-CV\/DEAL.",
        "Coordinating the behaviour of self-interested agents in the presence of\nmultiple Nash equilibria is a major research challenge for multi-agent systems.\nPre-game communication between all the players can aid coordination in cases\nwhere the Pareto-optimal payoff is unique, but can lead to deadlocks when there\nare multiple payoffs on the Pareto frontier. We consider a communication\npartition, where only players within the same coalition can communicate with\neach other, and they can establish an agreement (a coordinated joint-action) if\nit is envy-free, credible, and Pareto-optimal. We show that under a natural\nassumption about symmetry, certain communication partitions can induce social\noptimal outcomes in singleton congestion games. This game is a reasonable model\nfor a decentralised, anonymous system where players are required to choose from\na range of identical resources, and incur costs that are increasing and convex\nin the total number of players sharing the same resource. The communication\npartition can be seen as a mechanism for inducing efficient outcomes in this\ncontext.",
        "We evaluate the Hall coefficient characterising magnetotransport in an\nintercalated graphite CaC$_6$ with the Fermi surface reconstructed by an\nuniaxial charge density wave from closed pockets to open sheets. As the typical\norder parameter, corresponding to the pseudo-gap in electronic spectrum and\nconsequently to spacing between electron trajectories in reciprocal space, is\nof the order of $10^2$K, magnetic breakdown in strong experimentally achievable\nfields of the order of 10T is inevitable. The classical expressions for the\ncomponents of the magnetoconductivity tensor are strongly modified by magnetic\nfield-assisted over-gap tunneling causing quantum interference. Due to magnetic\nbreakdown, all magnetoconductivity components undergo strong quantum\noscillations reflected in the Hall coefficient. In their nature, these are\ndifferent than standard Shubnikov de Haas oscillations which would not appear\nin a system with an open Fermi surface.",
        "This work presents visual morphological and dynamical classifications for 637\nspatially resolved galaxies, most of which are at intermediate redshift\n($z\\sim0.3$), in the Middle-Ages Galaxy Properties with Integral field\nspectroscopy (MAGPI) Survey. For each galaxy, we obtain a minimum of 11\nindependent visual classifications by knowledgeable classifiers. We use an\nextension of the standard Dawid-Skene bayesian model introducing\nclassifier-specific confidence parameters and galaxy-specific difficulty\nparameters to quantify classifier confidence and infer reliable statistical\nconfidence estimates. Selecting sub-samples of 86 bright ($r<20$ mag)\nhigh-confidence ($>0.98$) morphological classifications at redshifts ($0.2 \\le\nz \\le0.4$), we confirm the full range of morphological types is represented in\nMAGPI as intended in the survey design. Similarly, with a sub-sample of 82\nbright high-confidence stellar kinematic classifications, we find that the\nrotating and non-rotating galaxies seen at low redshift are already in place at\nintermediate redshifts. We \\textit{do not} find evidence that the kinematic\nmorphology-density relation seen at $z\\sim0$ is established at $z\\sim0.3$. We\nsuggest that galaxies without obvious stellar rotation are dynamically\npre-processed sometime before $z\\sim0.3$ within lower mass groups before\njoining denser environments.",
        "We consider a novel anchorless rigid body localization (RBL) suitable for\napplication in autonomous driving (AD), in so far as the algorithm enables a\nrigid body to egoistically detect the location (relative translation) and\norientation (relative rotation) of another body, without knowledge of the shape\nof the latter, based only on a set of measurements of the distances between\nsensors of one vehicle to the other. A key point of the proposed method is that\nthe translation vector between the two-bodies is modeled using the\ndouble-centering operator from multidimensional scaling (MDS) theory, enabling\nthe method to be used between rigid bodies regardless of their shapes, in\ncontrast to conventional approaches which require both bodies to have the same\nshape. Simulation results illustrate the good performance of the proposed\ntechnique in terms of root mean square error (RMSE) of the estimates in\ndifferent setups.",
        "One of the pivotal challenges in a multi-robot system is how to give\nattention to accuracy and efficiency while ensuring safety. Prior arts cannot\nstrictly guarantee collision-free for an arbitrarily large number of robots or\nthe results are considerably conservative. Smoothness of the avoidance\ntrajectory also needs to be further optimized. This paper proposes an\naccelerationactuated simultaneous obstacle avoidance and trajectory tracking\nmethod for arbitrarily large teams of robots, that provides a nonconservative\ncollision avoidance strategy and gives approaches for deadlock avoidance. We\npropose two ways of deadlock resolution, one involves incorporating an\nauxiliary velocity vector into the error function of the trajectory tracking\nmodule, which is proven to have no influence on global convergence of the\ntracking error. Furthermore, unlike the traditional methods that they address\nconflicts after a deadlock occurs, our decision-making mechanism avoids the\nnear-zero velocity, which is much more safer and efficient in crowed\nenvironments. Extensive comparison show that the proposed method is superior to\nthe existing studies when deployed in a large-scale robot system, with minimal\ninvasiveness.",
        "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software\nengineering tasks from Upwork, valued at \\$1 million USD total in real-world\npayouts. SWE-Lancer encompasses both independent engineering tasks--ranging\nfrom \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks,\nwhere models choose between technical implementation proposals. Independent\ntasks are graded with end-to-end tests triple-verified by experienced software\nengineers, while managerial decisions are assessed against the choices of the\noriginal hired engineering managers. We evaluate model performance and find\nthat frontier models are still unable to solve the majority of tasks. To\nfacilitate future research, we open-source a unified Docker image and a public\nevaluation split, SWE-Lancer Diamond\n(https:\/\/github.com\/openai\/SWELancer-Benchmark). By mapping model performance\nto monetary value, we hope SWE-Lancer enables greater research into the\neconomic impact of AI model development.",
        "Semantic communication has emerged as a transformative paradigm in\nnext-generation communication systems, leveraging advanced artificial\nintelligence (AI) models to extract and transmit semantic representations for\nefficient information exchange. Nevertheless, the presence of unpredictable\nsemantic noise, such as ambiguity and distortions in transmitted\nrepresentations, often undermines the reliability of received information.\nConventional approaches primarily adopt adversarial training with noise\ninjection to mitigate the adverse effects of noise. However, such methods\nexhibit limited adaptability to varying noise levels and impose additional\ncomputational overhead during model training. To address these challenges, this\npaper proposes Noise-Resilient \\textbf{Se}mantic Communication with\n\\textbf{Hi}gh-and-\\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image\ntransmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization\n(FSQ) based noise-resilient module, which bypasses adversarial training by\nenforcing encoded representations within predefined spaces to enhance noise\nresilience. While FSQ improves robustness, it compromise representational\ndiversity. To alleviate this trade-off, we adopt a transformer-based\nhigh-and-low frequency decomposition module that decouples image\nrepresentations into high-and-low frequency components, mapping them into\nseparate FSQ representation spaces to preserve representational diversity.\nExtensive experiments demonstrate that Se-HiLo achieves superior noise\nresilience and ensures accurate semantic communication across diverse noise\nenvironments.",
        "I propose a mathematical framework for embedding an unshaped discrete lattice\n$L$ on a smooth manifold $M$. This framework simplifies complex concepts in\npure mathematics and physics by connecting discrete lattice structures with\ncontinuous geometric interpretations through practical embeddings.",
        "We propose a novel topological photonic memory that encodes information\nthrough dynamically controllable Chern numbers in a two-band topological\nphotonic system. Utilizing a honeycomb lattice photonic crystal, the memory\nleverages topologically protected edge states that remain robust against\nfabrication imperfections and environmental perturbations. By applying a\nsynthetic time-dependent magnetic field, we achieve real-time tunability of the\nChern number, enabling rapid and efficient memory switching without the need\nfor light-trapping mechanisms. Our computational study evaluates critical\nperformance metrics, including write speed, read stabilization time, energy gap\nstability, and nonadiabatic transition probabilities. The results demonstrate\nthat the system supports GHz-range write speeds (approximately 1-10 GHz), with\nstable data retention due to the large energy gap between bands. The system\nenables scalable multi-bit memory encoding based on quantized Chern numbers and\nexhibits superior speed, fault tolerance, and robustness compared to\nconventional photonic memory architectures. This work introduces a scalable,\nhigh-speed, and nontrapping optical memory paradigm, paving the way for future\napplications in quantum information processing and optical communication\ntechnologies.",
        "Rydberg atom arrays have emerged as a powerful platform for experimental\nresearch and a challenging subject for theoretical investigation in quantum\nscience. In this study, we investigate the finite-temperature properties of\ntwo-dimensional square-lattice Rydberg atom arrays using the projected\nentangled pair states (PEPS) method. By analyzing the thermal behavior of\nsystems in the checkerboard and striated phases, we extract critical exponents\nand identify phase transition characteristics. Our results confirm that the\ncheckerboard phase transition belongs to the 2D Ising universality class, while\nthe striated phase exhibits critical exponents that deviate from known\nuniversality classes, possibly due to finite-size effects. These findings\nprovide theoretical insights into the thermal stability of quantum phases in\nRydberg atom arrays and offer valuable guidance for future experimental\nefforts.",
        "Assessing whether AI-generated images are substantially similar to\ncopyrighted works is a crucial step in resolving copyright disputes. In this\npaper, we propose CopyJudge, an automated copyright infringement identification\nframework that leverages large vision-language models (LVLMs) to simulate\npractical court processes for determining substantial similarity between\ncopyrighted images and those generated by text-to-image diffusion models.\nSpecifically, we employ an abstraction-filtration-comparison test framework\nwith multi-LVLM debate to assess the likelihood of infringement and provide\ndetailed judgment rationales. Based on the judgments, we further introduce a\ngeneral LVLM-based mitigation strategy that automatically optimizes infringing\nprompts by avoiding sensitive expressions while preserving the non-infringing\ncontent. Besides, our approach can be enhanced by exploring non-infringing\nnoise vectors within the diffusion latent space via reinforcement learning,\neven without modifying the original prompts. Experimental results show that our\nidentification method achieves comparable state-of-the-art performance, while\noffering superior generalization and interpretability across various forms of\ninfringement, and that our mitigation method could more effectively mitigate\nmemorization and IP infringement without losing non-infringing expressions.",
        "Accurate origin-destination (OD) flow prediction is of great importance to\ndeveloping cities, as it can contribute to optimize urban structures and\nlayouts. However, with the common issues of missing regional features and\nlacking OD flow data, it is quite daunting to predict OD flow in developing\ncities. To address this challenge, we propose a novel Causality-Enhanced OD\nFlow Prediction (CE-OFP), a unified framework that aims to transfer urban\nknowledge between cities and achieve accuracy improvements in OD flow\npredictions across data-scarce cities. In specific, we propose a novel\nreinforcement learning model to discover universal causalities among urban\nfeatures in data-rich cities and build corresponding causal graphs. Then, we\nfurther build Causality-Enhanced Variational Auto-Encoder (CE-VAE) to\nincorporate causal graphs for effective feature reconstruction in data-scarce\ncities. Finally, with the reconstructed features, we devise a knowledge\ndistillation method with a graph attention network to migrate the OD prediction\nmodel from data-rich cities to data-scare cities. Extensive experiments on two\npairs of real-world datasets validate that the proposed CE-OFP remarkably\noutperforms state-of-the-art baselines, which can reduce the RMSE of OD flow\nprediction for data-scarce cities by up to 11%."
      ]
    }
  },
  {
    "id":2412.02695,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Deep Residual Learning for Image Recognition",
    "start_abstract":"Deeper neural networks are more difficult to train. We present a residual learning framework ease the training of that substantially deeper than those used previously. explicitly reformulate layers as functions with reference layer inputs, instead unreferenced functions. provide comprehensive empirical evidence showing these easier optimize, and can gain accuracy from considerably increased depth. On ImageNet dataset we evaluate nets depth up 152 - 8\u00d7 VGG [40] but still having lower complexity. An ensemble achieves 3.57% error on test set. This result won 1st place ILSVRC 2015 classification task. also analysis CIFAR-10 100 1000 layers. The representations is central importance for many visual recognition tasks. Solely due our extremely deep representations, obtain 28% relative improvement COCO object detection dataset. Deep foundations submissions & competitions1, where places tasks detection, localization, segmentation.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "EEG data for ADHD \/ Control children"
      ],
      "abstract":[
        "Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors. EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes. Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child\u2019s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child\u2019s performance (i.e. response speed)."
      ],
      "categories":[
        "Neurotherapeutics"
      ]
    },
    "list":{
      "title":[
        "Classical and Deep Reinforcement Learning Inventory Control Policies for\n  Pharmaceutical Supply Chains with Perishability and Non-Stationarity",
        "Void spin distribution as a powerful probe of $\\sigma_{8}$",
        "Social Choice Rules with Responsibility for Individual Skills",
        "Hadron Identification Prospects With Granular Calorimeters",
        "Mie resonances in optical trapping: Their role in kinematics and\n  back-action",
        "The typical structure of dense claw-free graphs",
        "Granulation and Convectional Driving on Stellar Surfaces",
        "11 New Transiting Brown Dwarfs and Very Low Mass Stars from TESS",
        "Any theory that admits a Wigner's Friend type multi-agent paradox is\n  logically contextual",
        "A structural instability drives the VO2 metal-insulator transition",
        "The large time asymptotics of nonlinear multichannel Schroedinger\n  equations",
        "White Gaussian Noise Generation with a Vacuum State Quantum Entropy\n  Source Chip",
        "Non-Reversible Langevin Algorithms for Constrained Sampling",
        "Approche non-invariante de la correspondance de Jacquet-Langlands :\n  analyse spectrale",
        "A simple method to enlarge a basin of attraction using a memristive\n  function",
        "Simulating quantum circuits with arbitrary local noise using Pauli\n  Propagation",
        "Zeptosecond to attosecond dynamics in atoms and possibility of\n  generating a zeptosecond light source",
        "The MeerKAT Fornax Survey V. H i kinematics and Fornax cluster\n  membership of the dwarf galaxy ESO 358-60",
        "Giant Topological Hall Effect Across Wide Temperature in Pt\/NiCo2O4\n  Heterostructure",
        "Black hole accretion and radiation variability in GRMHD simulations with\n  Rezzolla-Zhidenko spacetime",
        "Perturbative solutions for compact objects in (2+1)-dimensional\n  Bopp-Podolsky electrodynamics",
        "Three-dimensional flat band in ultra-thin Kagome metal Mn3Sn film",
        "SIGGRAPH: G: Improved Projective Dynamics Global Using Snapshots-based\n  Reduced Bases",
        "Mathematical analysis of the gradients in deep learning",
        "Continuously tunable anomalous Hall crystals in rhombohedral heptalayer\n  graphene",
        "Spherical black hole perturbations in EFT of scalar-tensor gravity with\n  timelike scalar profile",
        "Background processes in Higgs decay to Z gamma",
        "Late Time Cosmic Acceleration in Modified Gravity and Gauss-Bonnet\n  Cosmology",
        "3D Electron Diffraction as GIWAXS Alternative for Quantitative\n  Structural Characterization of Organic Solar Cells"
      ],
      "abstract":[
        "We study inventory control policies for pharmaceutical supply chains,\naddressing challenges such as perishability, yield uncertainty, and\nnon-stationary demand, combined with batching constraints, lead times, and lost\nsales. Collaborating with Bristol-Myers Squibb (BMS), we develop a realistic\ncase study incorporating these factors and benchmark three\npolicies--order-up-to (OUT), projected inventory level (PIL), and deep\nreinforcement learning (DRL) using the proximal policy optimization (PPO)\nalgorithm--against a BMS baseline based on human expertise. We derive and\nvalidate bounds-based procedures for optimizing OUT and PIL policy parameters\nand propose a methodology for estimating projected inventory levels, which are\nalso integrated into the DRL policy with demand forecasts to improve\ndecision-making under non-stationarity. Compared to a human-driven policy,\nwhich avoids lost sales through higher holding costs, all three implemented\npolicies achieve lower average costs but exhibit greater cost variability.\nWhile PIL demonstrates robust and consistent performance, OUT struggles under\nhigh lost sales costs, and PPO excels in complex and variable scenarios but\nrequires significant computational effort. The findings suggest that while DRL\nshows potential, it does not outperform classical policies in all numerical\nexperiments, highlighting 1) the need to integrate diverse policies to manage\npharmaceutical challenges effectively, based on the current state-of-the-art,\nand 2) that practical problems in this domain seem to lack a single policy\nclass that yields universally acceptable performance.",
        "We present a numerical proof of the concept that the void spin distributions\ncan in principle provide a tight constraint on the amplitude of matter density\nfluctuation on the scale of $8\\,h^{-1}{\\rm Mpc}$ ($\\sigma_{8}$) without being\nseverely deteriorated by the degeneracies of $\\sigma_{8}$ with cold dark matter\ndensity parameter multiplied by the dimensionless Hubble parameter square\n($\\Omega_{\\rm cdm}h^{2}$), total neutrino mass ($M_{\\nu}$) and dark energy\nequation of state ($w$). Applying the Void-Finder algorithm to a total of $15$\nAbacusSummit $N$-body simulations of $15$ different cosmological models, we\nidentify the giant voids to measure their spins, defined as the magnitudes of\nrescaled specific angular momenta of void halos. The $15$ cosmologies include\nthe Planck $\\Lambda$CDM and $14$ non-Planck models, each of which differs among\none another only in one of $\\{\\sigma_{8},\\ \\Omega_{\\rm cdm}h^{2},\\ M_{\\nu},\\\nw\\}$. The probability density distribution of void spins is determined for each\nmodel and found to be well approximated by the generalized Gamma distribution\nwith two characteristic parameters, $k$ and $\\theta$. It turns out that the\nbest-fit values of $k$ and $\\theta$ exhibit very sensitive dependences only on\n$\\sigma_{8}$, being almost insensitive to $\\Omega_{\\rm cdm}h^{2}$, $M_{\\nu}$,\n$w$. This exclusive $\\sigma_{8}$-dependence of the void spin distributions is\nconfirmed to be robust against the variation of the mass and number cuts of\nvoid halos. We also test an observational feasibility of estimating the void\nspins from real data on the galaxy redshifts.",
        "This paper examines normatively acceptable criteria for evaluating social\nstates when individuals are responsible for their skills or productivity and\nthese factors should be accounted for. We consider social choice rules over\nsets of feasible utility vectors \\`a la Nash's (1950) bargaining problem.\nFirst, we identify necessary and sufficient conditions for choice rules to be\nrationalized by welfare orderings or functions over ability-normalized utility\nvectors. These general results provide a foundation for exploring novel choice\nrules with the normalization and providing their axiomatic foundations. By\nadding natural axioms, we propose and axiomatize a new class of choice rules,\nwhich can be viewed as combinations of three key principles: distribution\naccording to individuals' abilities, utilitarianism, and egalitarianism.\nFurthermore, we show that at the axiomatic level, this class of choice rules is\nclosely related to the classical bargaining solution introduced by Kalai and\nSmorodinsky (1975).",
        "In this work we consider the problem of determining the identity of hadrons\nat high energies based on the topology of their energy depositions in dense\nmatter, along with the time of the interactions. Using GEANT4 simulations of a\nhomogeneous lead tungstate calorimeter with high transverse and longitudinal\nsegmentation, we investigated the discrimination of protons, positive pions,\nand positive kaons at 100 GeV. The analysis focuses on the impact of\ncalorimeter granularity by progressively merging detector cells and extracting\nfeatures like energy deposition patterns andtiming information. Two machine\nlearning approaches, XGBoost and fully connected deep neural networks, were\nemployed to assess the classification performance across particle pairs. The\nresults indicate that fine segmentation improves particle discrimination, with\nhigher granularity yielding more detailed characterization of energy showers.\nAdditionally, the results highlight the importance of shower radius, energy\nfractions, and timing variables in distinguishing particle types. The XGBoost\nmodel demonstrated computational efficiency and interpretability advantages\nover deep learning for tabular data structures, while achieving similar\nclassification performance. This motivates further work required to combine\nhigh- and low-level feature analysis, e.g., using convolutional and graph-based\nneural networks, and extending the study to a broader range of particle\nenergies and types.",
        "The heating rate plays a crucial role in the decoherence of the harmonic\nmotion of an optically levitated nanoparticle. The values of this rate vary\ndepending on both the scattering photon rate and the kinetic energy acquired\nthrough individual photon recoils. While the combined roles of these factors\nhave been extensively studied, the energy transfer per recoil has not been\nexplicitly examined. This energy transfer is often approximated using a linear\ndipole model with coefficients $\\{1\/5, 2\/5, 7\/5\\}$ which applies in the\nRayleigh limit. In this work, we analyze the evolution of energy transfer per\nphoton recoil for low-absorption dielectric nanospheres with diameters ranging\nfrom 2 nm to 500 nm. Using a far-field approximation, we demonstrate that the\nKerker condition, which enhances the alignment between incident and scattered\nwavevectors, may significantly reduce the energy transferred per recoil.\nAlthough this reduction is counterbalanced by the increasing scattering rate,\nfor an individual scattering event, the reduction of recoil suggests an\nintrinsic suppression of back-action. Our results reveal a potential\nenhancement in the accuracy of estimations in tabletop experiments involving\nMie particles of the considered sizes and provide guidance for the selection of\noptimal probe sizes and materials. Our interpretation of recoil reduction as a\nmanifestation of back-action suppression indicates the potential for quantum\nnondemolition (QND) measurements by tailoring scattered radiation patterns with\nmetamaterials.",
        "We analyze the asymptotic number and typical structure of claw-free graphs at\nconstant edge densities. The first of our main results is a formula for the\nasymptotics of the logarithm of the number of claw-free graphs of edge density\n$\\gamma \\in (0,1)$. We show that the problem exhibits a second-order phase\ntransition at edge density $\\gamma^\\ast=\\frac{5-\\sqrt{5}}{4}$. The asymptotic\nformula arises by solving a variational problem over graphons. For\n$\\gamma\\geq\\gamma^\\ast$ there is a unique optimal graphon, while for\n$\\gamma<\\gamma^\\ast$ there is an infinite set of optimal graphons. By analyzing\nmore detailed structure, we prove that for $\\gamma<\\gamma^\\ast$, there is in\nfact a unique graphon $W$ such that almost all claw-free graphs at edge density\n$\\gamma$ are close in cut metric to $W$. We also analyze the probability of\nclaw-freeness in the Erd\\H{o}s-R\\'enyi random graph $G(n,p)$ for constant $p$,\nobtaining a formula for the large-deviation rate function for claw-freeness. In\nthis case, the problem exhibits a first-order phase transition at\n$p^\\ast=\\frac{3-\\sqrt{5}}{2}$, separating distinct structural regimes. At the\ncritical point $p^\\ast$, the corresponding graphon variational problem has\ninfinitely many solutions, and we again pinpoint a unique optimal graphon that\ndescribes the typical structure of $G(n,p^\\ast)$ conditioned on being\nclaw-free.",
        "Surface convection is important for the presence of magnetic activity at\nstars. So far, this convection is thought to be a result of heating from below,\nwhere convection cells rise and break up. New models reveal that surface\nconvection is instead strongly driven by cooling from above. We compare two\nsimulations of surface convection, one with a significant heating from below\nand one without. We obtain surface convection in both cases, and they show\nsimilar granulation patterns. The deep convection driven by heating from below\nis still evolving and asymptotically approaches a steady-state solution. We\nfind that convection from below is not needed at all to form typical\nphotospheric granulation. This indicates the possibility of a surface dynamo\nacting on stars without a convecting envelope. Even stars without a convecting\nenvelope could therefore exhibit stronger magnetic and coronal activity than\nexpected so far.",
        "We present the discovery of 11 new transiting brown dwarfs and low-mass\nM-dwarfs from NASA's TESS mission: TOI-2844, TOI-3122, TOI-3577, TOI-3755,\nTOI-4462, TOI-4635, TOI-4737, TOI-4759, TOI-5240, TOI-5467, and TOI-5882. They\nconsist of 5 brown dwarf companions and 6 very low mass stellar companions\nranging in mass from $25 M_{\\rm J}$ to $128 M_{\\rm J}$. We used a combination\nof photometric time-series, spectroscopic, and high resolution imaging\nfollow-up as a part of the TESS Follow-up Observing Program (TFOP) in order to\ncharacterize each system. With over 50 transiting brown dwarfs confirmed, we\nnow have a large enough sample to directly test different formation and\nevolutionary scenarios. We provide a renewed perspective on the transiting\nbrown dwarf desert and its role in differentiating between planetary and\nstellar formation mechanisms. Our analysis of the eccentricity distribution for\nthe transiting brown dwarf sample does not support previous claims of a\ntransition between planetary and stellar formation at $\\sim42$ $M_{\\rm J}$. We\nalso contribute a first look into the metallicity distribution of transiting\ncompanions in the range $7 - 150$ $M_{\\rm J}$, showing that this too does not\nsupport a $\\sim42$ $M_{\\rm J}$ transition. Finally, we also detect a\nsignificant lithium absorption feature in one of the brown dwarf hosts\n(TOI-5882) but determine that the host star is likely old based on rotation,\nkinematic, and photometric measurements. We therefore claim that TOI-5882 may\nbe a candidate for planetary engulfment.",
        "Wigner's Friend scenarios push the boundaries of quantum theory by modeling\nagents, along with their memories storing measurement outcomes, as physical\nquantum systems. Extending these ideas beyond quantum theory, we ask: in which\nphysical theories, and under what assumptions, can agents who are reasoning\nlogically about each other's measurement outcomes encounter apparent paradoxes?\nTo address this, we prove a link between Wigner's Friend type multi-agent\nparadoxes and contextuality in general theories: if agents who are modeled\nwithin a physical theory come to a contradiction when reasoning using that\ntheory (under certain assumptions on how they reason and describe\nmeasurements), then the theory must admit contextual correlations of a logical\nform. This also yields a link between the distinct fundamental concepts of\nHeisenberg cuts and measurement contexts in general theories, and in\nparticular, implies that the quantum Frauchiger-Renner paradox is a proof of\nlogical contextuality. Moreover, we identify structural properties of such\nparadoxes in general theories and specific to quantum theory. For instance, we\ndemonstrate that theories admitting behaviors corresponding to extremal\nvertices of n-cycle contextuality scenarios admit Wigner's Friend type\nparadoxes without post-selection, and that any quantum Wigner's Friend paradox\nbased on the n-cycle scenario must necessarily involve post-selection. Further,\nwe construct a multi-agent paradox based on a genuine contextuality scenario\ninvolving sequential measurements on a single system, showing that Bell\nnon-local correlations between distinct subsystems are not necessary for\nWigner's Friend paradoxes. Our work offers an approach to investigate the\nstructure of physical theories and their information-theoretic resources by\nmeans of deconstructing the assumptions underlying multi-agent physical\nparadoxes.",
        "VO2 features concomitant structural and metal-insulator transitions. This\nposes a challenge for understanding the underlying mechanism: is the transition\ntriggered by a structural or by an electronic instability? Here, we address\nthis question by studying pre-transitional fluctuations in the metallic state.\nBy measuring resonant diffuse X-ray scattering we find no evidence that spatial\nfluctuations of d-electrons are any different from those of vanadium ion cores.\nThat is, charge and lattice remain coupled as they fluctuate jointly towards\nthe insulating phase, strongly supporting the case that the VO2 metal-insulator\ntransition is triggered by a structural instability. Our work offers a novel\napproach to solve similar problems in other strongly correlated systems.",
        "We consider the Schroedinger equation with a general interaction term, which\nis localized in space. The interaction may be x, t dependent and non-linear.\nPurely non-linear parts of the interaction are localized via the radial Sobolev\nembedding. Under the assumption of radial symmetry and boundedness in H1(R3) of\nthe solution, uniformly in time. we prove it is asymptotic in L2 (and H1) in\nthe strong sense, to a free wave and a weakly localized solution. The general\nproperties of the localized solutions are derived. The proof is based on the\nintroduction of phase-space analysis of the nonlinear dispersive dynamics and\nrelies on a new class of (exterior) a priory propagation estimates. This\napproach allows a unified analysis of general linear time-dependent potentials\nand non-linear interactions.",
        "White Gaussian noise (WGN) is widely used in communication system testing,\nphysical modeling, Monte Carlo simulations, and electronic countermeasures. WGN\ngeneration relies heavily on random numbers. In this work, we present an\nimplementation of WGN generation utilizing a quantum entropy source chip for\nthe first time. A photonic integrated chip based on the vacuum state scheme\ngenerates quantum random numbers at a real-time output rate of up to 6.4 Gbps.\nA hardware-based inversion method converts uniform quantum random numbers into\nGaussian random numbers using the inverse cumulative distribution function.\nSubsequently, the WGN signal is generated through a digital-to-analog converter\nand amplifiers. The WGN generator is characterized by a bandwidth of 230 MHz, a\ncrest factor as high as 6.2, and an adjustable peak-to-peak range of 2.5 V.\nThis work introduces a novel approach to WGN generation with information-theory\nprovable quantum random numbers to enhance system security.",
        "We consider the constrained sampling problem where the goal is to sample from\na target distribution on a constrained domain. We propose skew-reflected\nnon-reversible Langevin dynamics (SRNLD), a continuous-time stochastic\ndifferential equation with skew-reflected boundary. We obtain non-asymptotic\nconvergence rate of SRNLD to the target distribution in both total variation\nand 1-Wasserstein distances. By breaking reversibility, we show that the\nconvergence is faster than the special case of the reversible dynamics. Based\non the discretization of SRNLD, we propose skew-reflected non-reversible\nLangevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error\nfrom SRNLD, and convergence guarantees to the target distribution in\n1-Wasserstein distance. We show better performance guarantees than the\nprojected Langevin Monte Carlo in the literature that is based on the\nreversible dynamics. Numerical experiments are provided for both synthetic and\nreal datasets to show efficiency of the proposed algorithms.",
        "This is the second article in a two-part series presenting a new proof\ncomparing the non-invariant trace formula for a general linear group with that\nof one of its inner forms. In this article, we focus on the spectral side of\nthe trace formula. We complete the proof of the global Jacquet-Langlands\ncorrespondence using the non-invariant trace formula and examine its arithmetic\nimplications. Furthermore, we define the notion of non-invariant spectral\ntransfer of a test function and show that it coincides with the non-invariant\ngeometric transfer introduced in our first article. This provides a positive\nanswer to a conjecture of Arthur and extends a well-known theorem of Kazhdan\nwithin our framework.",
        "This study presents an innovative approach to chaotic attractor stabilization\nintroducing a memristor in discrete dynamical systems. Using the H\\'enon map as\na test case, we replace a system parameter with a memristive function governed\nby a sigmoid activation function. The method relies on leveraging attractors\nwith larger basins of attraction to attract the orbits and guide them towards\nthe desired chaotic attractor. The effectiveness of the method is confirmed\nthrough numerical simulations, showing substantial enhancement in attractor\nstability without requiring explicit parameter control.",
        "We present a polynomial-time classical algorithm for estimating expectation\nvalues of arbitrary observables on typical quantum circuits under any\nincoherent local noise, including non-unital or dephasing. Although previous\nresearch demonstrated that some carefully designed quantum circuits affected by\nnon-unital noise cannot be efficiently simulated, we show that this does not\napply to average-case circuits, as these can be efficiently simulated using\nPauli-path methods. Specifically, we prove that, with high probability over the\ncircuit gates choice, Pauli propagation algorithms with tailored truncation\nstrategies achieve an inversely polynomially small simulation error. This\nresult holds for arbitrary circuit topologies and for any local noise, under\nthe assumption that the distribution of each circuit layer is invariant under\nsingle-qubit random gates. Under the same minimal assumptions, we also prove\nthat most noisy circuits can be truncated to an effective logarithmic depth for\nthe task of {estimating} expectation values of observables, thus generalizing\nprior results to a significantly broader class of circuit ensembles. We further\nnumerically validate our algorithm with simulations on a $6\\times6$ lattice of\nqubits under the effects of amplitude damping and dephasing noise, as well as\nreal-time dynamics on an $11\\times11$ lattice of qubits affected by amplitude\ndamping.",
        "In nuclear collisions, nuclear bremsstrahlung can cause nuclear Coulomb\nexcitation via photon exchange in the projectile as well as the target nuclei.\nSuch a process originating in nuclear timescales (zeptoseconds) can also\ninfluence the atomic phenomenon, which can be observed if it is delayed at\nleast by a few attoseconds as atomic timescales $\\ge$ an attosecond. We have\nfound that this may happen due to a mechanism called the Eisenbud-Wigner-Smith\n(EWS)time delay process. We have estimated EWS time delays in atomic collisions\nutilizing the non-relativistic version of random phase approximation with\nexchange as well as Hartree-Fock methods. We present three representative\ncollision systems through which one can experimentally observe the phenomena in\nattosecond timescales even though they originate from nuclear bremsstrahlung\nradiation occurring in zeptoseconds. Thus the present work represents an\ninvestigation of parallels between two neighboring areas of physics: atomic and\nnuclear physics. Furthermore the present work suggests the possibilities for\natomic physics research near the Coulomb barrier energies, where the nuclear\nbremsstrahlung can be used as a zeptosecond x-ray source.",
        "The MeerKAT Fornax Survey (MFS) is a large survey project mapping the HI in\nthe Fornax cluster. Most of the cluster members detected in HI show significant\nsigns of interaction with the intra-cluster medium or other galaxies. The\ngalaxy ESO 358-60 however stands out as its large HI disk appears regular and\nundisturbed. A possible explanation for this undisturbed disk is that the\ngalaxy is not in Fornax.\n  We analyze the HI distribution within and around ESO 358-60 based on the MFS\nobservations. We visually inspect the low resolution data in order to study the\nHI disk from the center to its outskirts and look for low column density gas\nthat could reveal recent interactions. We then construct a detailed\nparameterization of the HI disk by fitting a tilted ring model to the high\nresolution data cube. We use the fitted rotational velocity to place the galaxy\non the baryonic Tully-Fisher relationship. By equating the galaxy's HI and 3.6\n$\\mu$m fluxes to the thus retrieved baryonic mass, we obtain a redshift\nindependent distance.\n  Our modeling confirms the regularity of the HI disk in ESO 358-60 but also\nshows that the galaxy contains a significant line of sight warp and contains\nradial motions, of the order of 10 km s$^{-1}$, that cover the extent of the\noptical disk. From the modeling we obtain a velocity V$_{\\rm flat} = $48.1\n$\\pm$ 1.4 km s$^{-1}$ for the best fit rotation curve. This leads to a distance\nfrom the baryonic Tully-Fisher relation of 9.4 $\\pm$ 2.5 Mpc,$\\sim$ 10 Mpc less\nthan the distance to the Fornax cluster. This distance not only fits better\nwith V$_{\\rm flat}$ but also with the overall HI distribution of low mass\ngalaxies and the fact that the galaxy appears undisturbed and reasonably\nsymmetric. At 9.4 Mpc ESO 358-60 cannot be a member of the Fornax cluster but\nis a foreground field galaxy.",
        "Topological Hall effect (THE), a quantum phenomenon arising from emergent\nmagnetic field generated by topological spin texture, is a key method for\ndetecting non-coplanar spin structures like skyrmions in magnetic materials.\nHere, we investigate a bilayer structure of Pt and conducting ferrimagnet\nNiCo2O4 (NCO) of perpendicular magnetic anisotropy and demonstrate giant THE\nacross a temperature range 2 - 350 K. The absence of THE in single-layer Pt and\nNCO, as well as in Pt\/Cu\/NCO, suggests its interfacial origin. The maximum THE\noccurring just before the NCO coercive field indicates its connection to\nmagnetic nucleation centers, which are topologically equivalent to skyrmions.\nThe large normalized THE, based on the emergent-field model, points to a high\npopulation density of small nucleation centers. This aligns with the\nunresolvable domain structures during magnetization reversal, even though clear\ndomain structures are detected after zero-field cooling. These results\nestablish heavy metal\/NCO as a promising system for exploring topological spin\nstructures.",
        "The Event Horizon Telescope (EHT) has revealed the horizon-scale radiation of\nSagittarius A* (Sgr A*), our galaxy's central supermassive black hole, offering\na new platform to test gravitational theories. The next step involves studying\naccretion flows and spacetime structures near black holes using EHT time\nvariability data and GRMHD simulations. We study accretion dynamics in\nspherically symmetric black hole spacetimes deviating from general relativity,\nusing 2D GRMHD simulations with Rezzolla-Zhidenko spacetime. This study\nsystematically investigates how light curve variability amplitudes from\nnon-Kerr GRMHD simulations depend on Schwarzschild spacetime deviations, based\non the constraints from weak gravitational fields and Sgr A*'s shadow size. We\nfind that the dynamics of accretion flows systematically depend on the\ndeviation. In spacetimes with a deeper gravitational potential, fluid and\nAlfv\\'en velocities consistently decrease relative to the Schwarzschild metric,\nindicating weaker dynamical behavior. We also examine the influence of\nspacetime deviations on radiation properties by computing luminosity\nfluctuations at 230 GHz using general relativistic radiative transfer\nsimulations, in line with EHT observations. The amplitude of these fluctuations\nexhibits a systematic dependence on the deviation parameters, decreasing for\ndeeper gravitational potentials compared to the Schwarzschild metric. These\nfeatures are validated using one of the theoretically predicted metrics, the\nHayward metric, a model that describes nonsingular black holes. This\ncharacteristic is expected to have similar effects in future comprehensive\nsimulations that include more realistic accretion disk models and electron\ncooling in the future, potentially aiding in distinguishing black hole\nsolutions that explain the variability of Sgr A*.",
        "We investigate the space-time geometry generated by compact objects in\n(2+1)-dimensional Bopp-Podolsky electrodynamics. Inspired by previous studies\nwhere the Bopp-Podolsky field acts as a source for spherically symmetric\nsolutions, we revisit this question within the lower-dimensional (2+1)\nframework. Using a perturbative approach, we derive a charged BTZ-like black\nhole solution and compute corrections up to second order in a perturbative\nexpansion valid far from the horizon. Our analysis suggests that the\nnear-horizon and inner structure of the solution remain unaltered, indicating\nthat no new non-black hole objects emerge in this regime. In particular, we do\nnot find evidence of wormhole solutions in the (2+1)-dimensional version of\nthis theory.",
        "Flat bands with small energy dispersion can give rise to strongly correlated\nelectronic and topological phases, especially when located at the Fermi level.\nWhilst flat bands have been experimentally realized in two-dimensional (2D)\ntwisted van der Waals heterostructures, they are highly sensitive to twist\nangle, necessitating complex fabrication techniques. Geometrically frustrated\nkagome lattices have emerged as an attractive platform as they natively host\nflat bands that have been observed experimentally in quasi-2D bulk-crystal\nkagome metals. An outstanding experimental question is whether flat bands can\nbe realized in atomically thin metals, with opportunities for stronger\nelectron-electron interactions through tuning of the surrounding dielectric\nenvironment. Here we use angle-resolved photoelectron spectroscopy, scanning\ntunnelling microscopy and band structure calculations to show that ultra-thin\nfilms of the kagome metal Mn3Sn host a robust dispersionless flat band with a\nbandwidth of 50 meV. Furthermore, we demonstrate chemical tuning of the flat\nband to near the Fermi level via manganese defect engineering. The realization\nof tunable kagome-derived flat bands in an ultra-thin kagome metal, represents\na promising platform to study strongly correlated and topological phenomena,\nwith applications in quantum computing, spintronics and low-energy electronics.",
        "We propose a snapshots-based method to compute reduction subspaces for\nphysics-based simulations. Our method is applicable to any mesh with some\nartistic prior knowledge of the solution and only requires a record of existing\nsolutions during, for instance, the range-of-motion test that is required\nbefore approving a mesh character for an application. Our subspaces span a\nwider range of motion, especially large deformations, and rotations by default.\nCompared to the state-of-the-art, we achieve improved numerical stability,\ncomputational efficiency, and more realistic simulations with a smaller\nsub-space.",
        "Deep learning algorithms -- typically consisting of a class of deep\nartificial neural networks (ANNs) trained by a stochastic gradient descent\n(SGD) optimization method -- are nowadays an integral part in many areas of\nscience, industry, and also our day to day life. Roughly speaking, in their\nmost basic form, ANNs can be regarded as functions that consist of a series of\ncompositions of affine-linear functions with multidimensional versions of\nso-called activation functions. One of the most popular of such activation\nfunctions is the rectified linear unit (ReLU) function $\\mathbb{R} \\ni x\n\\mapsto \\max\\{ x, 0 \\} \\in \\mathbb{R}$. The ReLU function is, however, not\ndifferentiable and, typically, this lack of regularity transfers to the cost\nfunction of the supervised learning problem under consideration. Regardless of\nthis lack of differentiability issue, deep learning practioners apply SGD\nmethods based on suitably generalized gradients in standard deep learning\nlibraries like {\\sc TensorFlow} or {\\sc Pytorch}. In this work we reveal an\naccurate and concise mathematical description of such generalized gradients in\nthe training of deep fully-connected feedforward ANNs and we also study the\nresulting generalized gradient function analytically. Specifically, we provide\nan appropriate approximation procedure that uniquely describes the generalized\ngradient function, we prove that the generalized gradients are limiting\nFr\\'echet subgradients of the cost functional, and we conclude that the\ngeneralized gradients must coincide with the standard gradient of the cost\nfunctional on every open sets on which the cost functional is continuously\ndifferentiable.",
        "The interplay of electronic interactions and nontrivial topology can give\nrise to a wealth of exotic quantum states. A notable example is the formation\nof Wigner crystals driven by strong electron-electron interactions. When these\nelectronic crystals emerge in a parent band carrying a large Berry curvature,\nthey can exhibit topologically nontrivial properties as anomalous Hall\ncrystals, spontaneously breaking both continuous translational symmetry and\ntime-reversal symmetry. Here, we report the experimental observation of tunable\nanomalous Hall crystals in rhombohedral heptalayer graphene moir\\'e\nsuperlattices. At filling factors near one electron per moir\\'e unit cell\n(v=1), we identify a series of incommensurate Chern insulators with a Chern\nnumber of C=1. Furthermore, we observe spontaneous time-reversal symmetry\nbreaking spanning the entire filling range from v=1 to v=2, manifesting as\nanomalous Hall effects with pronounced magnetic hysteresis. Notably, anomalous\nHall crystals with a high Chern number C=3 are observed over generic fillings\nranging from v=1.5 to v=2. These anomalous Hall crystals are incommensurate\nwith the moir\\'e superlattice and exhibit dispersive fan diagrams consistent\nwith the Streda formula, with their positions continuously tunable through\ndisplacement fields. Remarkably, these partially filled Chern insulators\ndisplay Chern numbers distinct from their parent bands. Our findings\ndemonstrate the rich variety of electronic crystalline states in rhombohedral\ngraphene moir\\'e superlattices, offering valuable insights into the strongly\ncorrelated topological phases.",
        "We study linear even-parity perturbations of static and spherically symmetric\nblack holes with a timelike scalar profile by use of the effective field theory\n(EFT) approach. For illustrative purposes, we consider a simple subclass of the\nEFT that accommodates ghost condensate, namely the k-essence model along with\nthe so-called scordatura term, and focus on the spherical (monopole)\nperturbations about an approximately stealth Schwarzschild solution. The\nscordatura effect is introduced to avoid the strong coupling problem that\ntypically happens in the scalar sector around stealth solutions with a timelike\nscalar profile. We argue that the scalar perturbation is decoupled from the\nmetric perturbations at the leading order in the scordatura effect under a\nparticular gauge choice. We stress that this is an important step in\nunderstanding the dynamics of even-parity perturbations, paving the way towards\nderiving a set of master equations -- the generalized Zerilli and the\nscalar-field equations -- for generic multipoles.",
        "The ATLAS and CMS Collaborations reported that the observed number of Higgs\nboson decays into a $Z$ boson and a photon is $\\mu = 2.2 \\pm 0.7$ times higher\nthan predicted by the Standard Model. Initially, this discrepancy was\nattributed to a modification of the $HZ\\gamma$ vertex. In the $H \\to Z\\gamma$\nprocess, this decay is reconstructed from $H \\to \\ell\\ell\\gamma$, where $\\ell$\nrepresents either an electron or a muon. In this study, an investigation is\nconducted to examine this anomaly by exploring potential additional background\ncontributions to $H \\to \\ell\\ell\\gamma$ from various subprocesses within and\nbeyond the Standard Model.",
        "This thesis focuses on late-time cosmic acceleration within modified theories\nof gravity, using various observational data sets and statistical analysis. The\nUniverse is assumed to be spatially homogeneous and isotropic and is described\nby the Friedmann Lema\\^{i}tre Robertson Walker metric. The late-time\nacceleration of the Universe has posed a significant challenge to contemporary\ncosmology. General relativity addresses this by introducing the cosmological\nconstant, forming the basis of the standard cosmological model ($\\Lambda$CDM).\nHowever, this model has limitations, leading cosmologists to explore\nalternative explanations for late-time acceleration. These alternatives range\nfrom models involving a dynamic dark fluid known as dark energy, to large-scale\nmodifications of gravitational interaction, known as modified gravity. The\nformulation of general relativity fundamentally changed our understanding of\ngravitation, redefining gravity as a manifestation of the curvature of\nspacetime rather than a force as described by Newton. Despite its success,\ngeneral relativity has shown incompatibilities with observations, necessitating\nthe introduction of dark matter and dark energy....",
        "We demonstrate elastically filtered 3D Electron Diffraction (3D ED) as a\npowerful alternative technique to Grazing Incidence Wide-Angle X-ray Scattering\n(GIWAXS) for quantitatively characterizing the structure of organic\nsemiconductor films. Using a model material system of solvent vapor annealed\nDRCN5T:PC71BM thin film, which is employed in organic solar cells (OSCs), we\nextract the structural data obtained from 3D ED and compare with that from\nGIWAXS, utilizing both laboratory and synchrotron X-ray sources. Quantitative\nevaluation of the datasets in terms of peak positions, peak widths and\nmosaicity revealed good agreement between both techniques, qualifying 3D ED as\nan alternative tool for analyzing highly beam-sensitive organic thin films.\nFurthermore, the respective advantages and limitations of 3D ED and GIWAXS are\ndiscussed, emphasizing the unique capability of 3D ED to integrate seamlessly\nwith the diverse imaging and spectroscopic modalities in modern TEM. This\nintegration enriches the techniques of structural characterization of OSCs,\npaving the way for deeper insights into their structural properties and\nultimately their performance."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Language Models are Few-Shot Learners",
    "start_abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Long non-coding RNAs: definitions, functions, challenges and recommendations"
      ],
      "abstract":[
        "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Secure Quantum Key Distribution with Room-Temperature Quantum Emitter",
        "Plebanski complex",
        "Extended string-net models with all anyons at finite temperature",
        "Geodesic cycles on the Sphere: $t$-designs and Marcinkiewicz-Zygmund\n  Inequalities",
        "Minimax Optimality of Classical Scaling Under General Noise Conditions",
        "The Effect of the Non-Abelian Quantum Metric on Superfluidity",
        "Global well-posedness and optimal time-decay of 3D full compressible\n  Navier-Stokes system",
        "Origin of switchable quasiparticle-interference chirality in\n  loop-current phase of kagome metals measured by scanning-tunneling-microscopy",
        "Learning the P2D Model for Lithium-Ion Batteries with SOH Detection",
        "Dot to dot: high-$z$ little red dots in $M_{\\rm bh}$-$M_{\\rm \\star}$\n  diagrams with galaxy-morphology-specific scaling relations and nuclear star\n  clusters",
        "Knoop: Practical Enhancement of Knockoff with Over-Parameterization for\n  Variable Selection",
        "Spectral properties of bottomonium at high temperature: a systematic\n  investigation",
        "Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65\n  observed with Chandra",
        "Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers",
        "Ground state and magnetic transitions of the orthorhombic\n  antiferromagnet CaCo$_2$TeO$_6$",
        "Clustered Flexible Calibration Plots For Binary Outcomes Using Random\n  Effects Modeling",
        "A Theory of Chaordic Economics: How Artificial Intelligence and\n  Blockchain Transform Businesses, Economies and Societies",
        "Absence of dehydration due to superionic transition at Earth's\n  core-mantle boundary",
        "Optimizing realistic continuous-variable quantum teleportation with\n  non-Gaussian resources",
        "On the origin of compressive turbulence in protoclumps in high redshift\n  disks",
        "A pioneering experiment combining single-antenna and aperture-synthesis\n  data to measure Faraday rotation with GMIMS and the CGPS",
        "Thermal conductance at superradiant phase transition in quantum Rabi\n  model",
        "Science of a coffee cup: a physicist walks into a bar...",
        "Unfolding the Six-Dimensional Tensor Multiplet",
        "Quantum Electrodynamics from Quantum Cellular Automata, and the Tension\n  Between Symmetry, Locality and Positive Energy",
        "Integrative Analysis of High-dimensional RCT and RWD Subject to\n  Censoring and Hidden Confounding",
        "Diffusion Models for Inverse Problems in the Exponential Family",
        "Comparative study on radiation resistance of WTaCrV high-entropy alloy\n  and tungsten in helium-containing conditions",
        "Movement Dynamics in Elite Female Soccer Athletes: The Quantile Cube\n  Approach"
      ],
      "abstract":[
        "On-demand generation of single photons from solid-state quantum emitters is a\nkey building block for future quantum networks, particularly quantum key\ndistribution (QKD) systems, by enabling higher secure key rates (SKR) and lower\nquantum bit error rates (QBER). In this work, we demonstrate the B92 protocol\nbased on single photons from defects in hexagonal boron nitride (hBN). The\nresults show a sifted key rate (SiKR) of 17.5 kbps with a QBER of 6.49 % at a\ndynamic polarization encoding rate of 40 MHz. Finite-key analysis yields a SKR\nof 7 kbps, as one of the highest SKR obtained for any room-temperature single\nphoton source. Our results highlight the potential of hBN defects in advancing\nquantum communication technologies.",
        "As is very well-known, linearisation of the instanton equations on a\n4-manifold gives rise to an elliptic complex of differential operators, the\ntruncated (twisted) Hodge complex $\\Lambda^0(\\mathfrak{g}) \\to\n\\Lambda^1(\\mathfrak{g})\\to \\Lambda^2_+(\\mathfrak{g})$. Moreover, the\nlinearisation of the full YM equations also fits into this framework, as it is\ngiven by the second map followed by its adjoint. We define and study properties\nof what we call the Pleba\\'nski complex. This is a differential complex that\narises by linearisation of the equations implying that a Riemannian 4-manifold\nis hyper-K\\\"ahler. We recall that these are most naturally stated as the\ncondition that there exists a perfect $\\Sigma^i\\wedge \\Sigma^j\\sim\\delta^{ij}$\ntriple $\\Sigma^i, i=1,2,3$ of 2-forms that are closed $d\\Sigma^i=0$. The\nRiemannian metric is encoded by the 2-forms $\\Sigma^i$. We show that what\nresults is an elliptic differential complex $TM \\to S\\to E\\times \\Lambda^1 \\to\nE$, where $S$ is the tangent space to the space of perfect triples, and\n$E=\\mathbb{R}^3$. We also show that, as in the case with instanton equations,\nthe full Einstein equations $Ric=0$ also fit into this framework, their\nlinearisation being given by the second map followed by its adjoint. Our second\nresult concerns the elliptic operator that the Pleba\\'nski complex defines. In\nthe case of the instanton complex, operators appearing in the complex\nsupplemented with their adjoints assemble to give the Dirac operator. We show\nhow the same holds true for the Pleba\\'nski complex. Supplemented by suitable\nadjoints, operators assemble into an elliptic operator that squares to the\nLaplacian and is given by the direct sum of two Dirac operators.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "A geodesic cycle is a closed curve that connects finitely many points along\ngeodesics. We study geodesic cycles on the sphere in regard to their role in\nequal-weight quadrature rules and approximation.",
        "We establish the consistency of classical scaling under a broad class of\nnoise models, encompassing many commonly studied cases in literature. Our\napproach requires only finite fourth moments of the noise, significantly\nweakening standard assumptions. We derive convergence rates for classical\nscaling and establish matching minimax lower bounds, demonstrating that\nclassical scaling achieves minimax optimality in recovering the true\nconfiguration even when the input dissimilarities are corrupted by noise.",
        "The quantum geometric tensor, which encodes the full geometric information of\nquantum states in projective Hilbert space, plays a crucial role in condensed\nmatter physics. In this work, we examine the effect of the non-Abelian quantum\nmetric -- the real part of the non-Abelian quantum geometric tensor -- on the\nsuperfluid weight in time-reversal symmetric systems. For conventional $s$-wave\npairing, we demonstrate that the superfluid weight includes a contribution\nproportional to the trace of the non-Abelian quantum metric. Notably, this\ncontribution remains significant even when the total Chern number of a set of\ndegenerate bands is zero and can exceed the conventional contribution, as\nconfirmed using lattice models. Ab initio density functional theory (DFT)\ncalculations for MoS$_2$ and TiSe$_2$ further corroborate these findings,\nrevealing that the non-Abelian quantum metric accounts for up to 20% of the\nsuperfluid weight in MoS$_2$ and 50% in TiSe$_2$. Our results provide new\ninsights into the nontrivial relationship between the geometric properties of\nquantum states and superconductivity, opening avenues for further exploration\nin topological and superconducting materials.",
        "In this paper, we investigate the global well-posedness and optimal\ntime-decay of classical solutions for the 3-D full compressible Navier-Stokes\nsystem, which is given by the motion of the compressible viscous and\nheat-conductive gases. First of all, we study the global well-posedness of the\nCauchy problem to the system when the initial data is small enough. Secondly,\nwe show the optimal decay rates of the higher-order spatial derivatives of the\n$\\dot{H}^{-s}$ $\\left(0\\leq s<\\frac{3}{2}\\right)$ negative Sobolev norms.\nFinally, under the assumption that the initial data is bounded in $L^{1}$-norm,\nwe establish the upper and lower bounds of the optimal decay rates for the\nclassical solutions.",
        "The chiral loop-current (LC) phase in kagome metals AV3Sb5 (A = Cs, Rb, K)\nhas attracted considerable attention as a novel quantum state driven by\nelectron correlations. Scanning tunneling microscopy (STM) experiments have\nprovided strong evidence for the chiral LC phase through the detection of\nchirality in the quasiparticle interference (QPI) signal. However, the\nfundamental relationship between ``QPI chirality'' and ``LC chirality'' remains\nunexplored. For instance, the QPI signal is unchanged even when all LC orders\nare inverted. Furthermore, only the chiral LC order cannot induce QPI\nchirality. At present, the true essence of kagome metals that we should learn\nfrom the remarkable QPI experiments remains elusive. To address this, we\ninvestigate the origin of the QPI signal in the LC phase using a large\nunit-cell tight-binding model for kagome metals. The LC phase gives rise to a\n$Z_3$ nematic phase, characterized by three distinct directors, under the\nStar-of-David bond order. Our findings demonstrate that the QPI chirality\ninduced by a single impurity at site Z, denoted as $\\chi_Z$, can take values of\n$\\pm1$ (chiral) or 0 (achiral), depending on the direction of the $Z_3$ nematic\norder. Prominent QPI chirality originates from extremely dilute impurities\n($\\lesssim$0.1%) in the present mechanism. Notably, $\\chi_Z$ ($=\\pm1$, 0)\nchanges smoothly with minimal free-energy barriers by applying a small magnetic\nfield $B_z$, accompanied by a switching of the $Z_3$ nematic director. This\nstudy provides a comprehensive explanation for the observed ``$B_z$-switchable\nQPI chirality'' in regions with dilute impurities, offering fundamental insight\ninto the chiral LC in kagome metals.",
        "Lithium ion batteries are widely used in many applications. Battery\nmanagement systems control their optimal use and charging and predict when the\nbattery will cease to deliver the required output on a planned duty or driving\ncycle. Such systems use a simulation of a mathematical model of battery\nperformance. These models can be electrochemical or data-driven.\nElectrochemical models for batteries running at high currents are\nmathematically and computationally complex. In this work, we show that a\nwell-regarded electrochemical model, the Pseudo Two Dimensional (P2D) model,\ncan be replaced by a computationally efficient Convolutional Neural Network\n(CNN) surrogate model fit to accurately simulated data from a class of random\ndriving cycles. We demonstrate that a CNN is an ideal choice for accurately\ncapturing Lithium ion concentration profiles. Additionally, we show how the\nneural network model can be adjusted to correspond to battery changes in State\nof Health (SOH).",
        "High-redshift little red dots (LRDs) detected with the James Webb Space\nTelescope are considered the cores of emerging galaxies. For the first time, we\ncompare LRDs in $M_{\\rm bh}$-$M_{\\star}$ diagrams with an array of $z=0$\ngalaxy-morphology-dependent scaling relations, along with the $M_{\\rm\nbh}$-$M_{\\rm \\star,nsc}$ relation for nuclear star clusters. The $M_{\\rm\nbh}$-$M_{\\rm \\star,sph}$ relations for spheroidal stellar systems are\ncharacterised by a nearly parallel set of quasi-quadratic (or steeper)\ndistributions that are known to trace the `punctuated equilibrium' of galaxies,\nreflecting their stepwise growth in black hole mass and merger-built\nbulge\/spheroid mass. We show that LRDs are not equivalent to nuclear star\nclusters, with the latter having higher $M_{\\rm bh}\/M_{\\star}$ ratios. However,\nthe least massive LRDs exhibit similar $M_{\\rm bh}$ and $M_{\\rm \\star,gal}$\nvalues as ultracompact dwarf (UCD) galaxies. We show that the LRDs span the\n$M_{\\rm bh}$-$M_{\\rm \\star,gal}$ diagram from UCD galaxies to primaeval\nlenticular galaxies. In contrast, spiral galaxies and the subset of\nmajor-merger-built early-type galaxies define offset relations. Additionally,\nwe observe that low-$z$ galaxies with active galactic nuclei align with the\nsteep black hole scaling relations for disc galaxies defined by primarily\ninactive galaxies with directly measured black hole masses. Collectively, this\nhighlights the benefits of considering galaxy morphology, which reflects their\naccretion and merger history, to understand the coevolution of galaxies and\ntheir black holes.",
        "Variable selection plays a crucial role in enhancing modeling effectiveness\nacross diverse fields, addressing the challenges posed by high-dimensional\ndatasets of correlated variables. This work introduces a novel approach namely\nKnockoff with over-parameterization (Knoop) to enhance Knockoff filters for\nvariable selection. Specifically, Knoop first generates multiple knockoff\nvariables for each original variable and integrates them with the original\nvariables into an over-parameterized Ridgeless regression model. For each\noriginal variable, Knoop evaluates the coefficient distribution of its\nknockoffs and compares these with the original coefficients to conduct an\nanomaly-based significance test, ensuring robust variable selection. Extensive\nexperiments demonstrate superior performance compared to existing methods in\nboth simulation and real-world datasets. Knoop achieves a notably higher Area\nunder the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for\neffectively identifying relevant variables against the ground truth by\ncontrolled simulations, while showcasing enhanced predictive accuracy across\ndiverse regression and classification tasks. The analytical results further\nbackup our observations.",
        "We investigate spectral features of bottomonium at high temperature, in\nparticular the thermal mass shift and width of ground state S-wave and P-wave\nstate. We employ and compare a range of methods for determining these features\nfrom lattice NRQCD correlators, including direct correlator analyses\n(multi-exponential fits and moments of spectral functions), linear methods\n(Backus-Gilbert, Tikhonov and HLT methods), and Bayesian methods for spectral\nfunction reconstruction (MEM and BR). We comment on the reliability and\nlimitations of the various methods.",
        "4U 0114+65 is a high-mass X-ray binary system formed by the luminous\nsupergiant B1Ia, known as V{*} V662 Cas, and one of the slowest rotating\nneutron stars (NS) with a spin period of about 2.6 hours. This fact provides a\nrare opportunity to study interesting details of the accretion within each\nindividual pulse of the compact object. In this paper, we analyze 200 ks of\nChandra grating data, divided into 9 uninterrupted observations around the\norbit. The changes in the circumstellar absorption column through the orbit\nsuggest an orbital inclination of $\\sim$ $40^{\\circ}$ with respect to the\nobserver and a companion mass-loss rate of $\\sim$ 8.6 10$^{-7}$ solar masses\nyr$^{-1}$. The peaks of the NS pulse show a large pulse-to-pulse variability.\nThree of them show an evolution from a brighter regime to a weaker one. We\npropose that the efficiency of Compton cooling in this source fluctuates\nthroughout an accumulation cycle. After significant depletion of matter within\nthe magnetosphere, since the settling velocity is $\\sim \\times$ 2 times lower\nthan the free-fall velocity, the source gradually accumulates matter until the\ndensity exceeds a critical threshold. This increase in density triggers a\ntransition to a more efficient Compton cooling regime, leading to a higher mass\naccretion rate and consequently to an increased brightness.",
        "Parameter shift rules (PSRs) are key techniques for efficient gradient\nestimation in variational quantum eigensolvers (VQEs). In this paper, we\npropose its Bayesian variant, where Gaussian processes with appropriate kernels\nare used to estimate the gradient of the VQE objective. Our Bayesian PSR offers\nflexible gradient estimation from observations at arbitrary locations with\nuncertainty information and reduces to the generalized PSR in special cases. In\nstochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the\nreuse of observations in previous steps, which accelerates the optimization\nprocess. Furthermore, the accessibility to the posterior uncertainty, along\nwith our proposed notion of gradient confident region (GradCoRe), enables us to\nminimize the observation costs in each SGD step. Our numerical experiments show\nthat the VQE optimization with Bayesian PSR and GradCoRe significantly\naccelerates SGD and outperforms the state-of-the-art methods, including\nsequential minimal optimization.",
        "We report the systematic synthesis, crystal structure, magnetization, and\npowder neutron diffraction of single crystalline and polycrystalline\nCaCo$_2$TeO$_6$ samples. CaCo$_2$TeO$_6$ crystallizes in an orthorhombic\nstructure with $Pnma$ space group, featuring chains of edge-shared CoO$_6$\noctahedra arranged in a honeycomb pattern. Two antiferromagnetic transitions\nare observed at $T$$_{N1}$ = 14.4 K and $T$$_{N2}$ = 16.2 K, corresponding to\ntwo long-range magnetic orders with propagation vectors of $\\bf{k}$$_1$ = (0,\n0, 0) and $\\bf{k}$$_2$ = (0.125, 0, 0.25), respectively. The ground state is\ndetermined as a canted up-up-down-down zigzag spin configuration along the $c$\naxis, wherein the magnetic moments of Co1 and Co2 ions are 3.4(1) and\n2.1(1)$\\mu$$_B$, respectively. Successive spin-flop transitions appear with the\nincreasing magnetic field applied along the easy axis ($c$ axis), accompanied\nby depression of the antiferromagnetic orders and enhancement of residual\nmagnetic entropy. The field-induced spin-disordered state suggests that\nCaCo$_2$TeO$_6$ may be an ideal candidate for studying frustrated magnetism.",
        "Evaluation of clinical prediction models across multiple clusters, whether\ncenters or datasets, is becoming increasingly common. A comprehensive\nevaluation includes an assessment of the agreement between the estimated risks\nand the observed outcomes, also known as calibration. Calibration is of utmost\nimportance for clinical decision making with prediction models and it may vary\nbetween clusters. We present three approaches to take clustering into account\nwhen evaluating calibration. (1) Clustered group calibration (CG-C), (2) two\nstage meta-analysis calibration (2MA-C) and (3) mixed model calibration (MIX-C)\ncan obtain flexible calibration plots with random effects modelling and\nproviding confidence and prediction intervals. As a case example, we externally\nvalidate a model to estimate the risk that an ovarian tumor is malignant in\nmultiple centers (N = 2489). We also conduct a simulation study and synthetic\ndata study generated from a true clustered dataset to evaluate the methods. In\nthe simulation and the synthetic data analysis MIX-C gave estimated curves\nclosest to the true overall and center specific curves. Prediction interval was\nbest for 2MA-C with splines. Standard flexible calibration worked likewise in\nterms of calibration error when sample size is limited. We recommend using\n2MA-C (splines) to estimate the curve with the average effect and the 95% PI\nand MIX-C for the cluster specific curves, specially when sample size per\ncluster is limited. We provide ready-to-use code to construct summary flexible\ncalibration curves with confidence and prediction intervals to assess\nheterogeneity in calibration across datasets or centers.",
        "Dee Hock, the founder of Visa, coined the term 'chaordic' to describe\nsimultaneously chaotic and ordered systems. Based on his reasoning, we\nintroduce the Theory of Chaordic Economics to explain how economic systems are\ntransformed by two disruptive technologies: namely Artificial Intelligence and\nBlockchain. Artificial intelligence can generate novel output through\nalgorithmic yet rather unpredictable processes. Blockchain creates\ndeterministic results without central authorities and relies on elaborated\nprotocols that prescribe how consensus can be reached within a network of\npeers. The amalgamation of chaos and order produces chaordic economic systems\nand can yield hitherto unthinkable economic structures.",
        "The properties and stability of hydrous phases are key to unraveling the\nmysteries of the water cycle in Earth's interior. Under the deep lower mantle\nconditions, hydrous phases transition into a superionic state. However, the\ninfluence of the superionic effect on their stability and dehydration processes\nremains poorly understood. Using ab initio calculations and deep-learning\npotential molecular dynamics simulations, we discovered a doubly superionic\ntransition in delta-AlOOH, characterized by the highly diffusive behavior of\nionic hydrogen and aluminum within the oxygen sub-lattice. These highly\ndiffusive elements contribute significant external entropy into the system,\nresulting in exceptional thermostability. Free energy calculations indicate\nthat dehydration is energetically and kinetically unfavorable when water exists\nin a superionic state under core-mantle boundary (CMB) conditions.\nConsequently, water can accumulate in the deep lower mantle over Earth's\nhistory. This deep water reservoir plays a crucial role in the global deep\nwater and hydrogen cycles.",
        "In this work, we investigate the performance of non-Gaussian entangled\nresources in continuous-variable quantum teleportation within a realistic\nsetting. We describe the characteristic functions of three distinct entangled\nresources, a two-mode squeezed vacuum state, a two-mode photon-subtracted\nsqueezed state, and a two-mode photon-added squeezed state. We extend the\ntheoretical analysis by Yang et al. to include the realistic experimental\nconditions such as photon losses, imperfect measurements which typically affect\ncontinuous-variable quantum teleportation. Our results demonstrate that even in\nnon-ideal situations, the photon-subtracted squeezed state outperforms the\nother two resources in the low squeezing regime, keeping fidelity above the\nclassical threshold that suggests the robustness of photon-subtracted squeezed\nstate in practical teleportation applications. We further analyze the EPR\ncorrelations of these entangled resources, revealing that the photon-subtracted\nsqueezed state exhibits stronger EPR correlations than the original two-mode\nsqueezed vacuum state and the two-mode photon-added squeezed state. This study\nmerges theoretical models with realistic imperfections and utilizes\nnon-Gaussian entanglement into high-fidelity quantum teleportation.",
        "The giant, star forming clumps in gas-rich, high redshift disks are commonly\nassumed to form due to gravitational instabilities, in which protoclumps have a\nToomre-$Q$ parameter less than unity. However, some cosmological simulations\nshow that clumps can form in regions where $Q\\gg1$. In these simulations, there\nis an excess of compressive modes of turbulence that lead to gravitational\ncollapse of regions that were not supposed to gravitationally collapse,\naccording to linear theory. In contrast, sites of clump formation in isolated\nsimulations do not show this excess, hinting that the origin may be external.\nWe explore two external mechanisms that can induce compressive modes of disk\nturbulence in protoclumps, namely, compressive tides exerted by the\ncosmological environment and the direct driving by inflowing streams. We\ncorrelate the local strength of compressive tides and the amount of fresh\nstream material with protoclump regions in zoom-in cosmological simulations.\nThe local strength of compressive tides is derived from the tidal tensor. The\nlocal strength of incoming streams is derived from the fractional presence of\nthe stream compared to the average. We find that the tidal field in protoclumps\ntends to be over-compressive while random patches in the disk show diverging\ntides. In particular, in $25\\%$ of the protoclumps, the tidal field is fully\ncompressive, while no random patch resides in regions of fully compressive\ntides. In addition, protoclumps tend to reside in regions where the fraction of\nincoming stream mass is 2-10 times larger than the average at the same\ngalactocentric radius. Both compressive tides and inflowing streams are\ncorrelated with the protoclumps and can thus serve as the drivers of excessive\ncompressive turbulence that can initiate clump formation. This constitutes a\nnew, non-linear mode of violent disk instabilities in high-$z$ galaxies.",
        "Structures in the magneto-ionic medium exist across a wide range of angular\nsizes due to large-scale magnetic fields coherent over the Galactic spiral arms\ncombined with small-scale fluctuations in the magnetic field and electron\ndensity resulting from energy injection processes such as supernovae. For the\nfirst time, we produce diffuse Galactic synchrotron emission Faraday rotation\nmaps covering all spatial scales down to $3'$ resolution for Galactic magnetic\nfield studies. These maps complement standard total and polarized intensity\nmaps combining single-antenna and interferometric data that have been produced,\nfor example, for the Canadian Galactic Plane Survey (CGPS). Such combined maps\nhave sensitivity to large scales from the single-antenna component, and the\nresolution from the interferometric component. We combine Global Magneto-Ionic\nMedium Survey High-Band-North (GMIMS-HBN) single-antenna and CGPS\naperture-synthesis polarization data after spatial filtering, producing Stokes\n$Q$ and $U$ maps individually for the four CGPS frequency channels. We\ncalculate Rotation Measures (RM) for all pixels using a linear fit to\npolarization angle versus wavelength squared. The RM maps show magnetic field\nstructures on the full range of scales probed by this dataset. Regions of\nsmooth polarized emission require the large-scale sensitivity of the\nsingle-antenna data to illuminate the Faraday depths, while the\naperture-synthesis data reveal small-scale variability in the Faraday rotation.\nDespite the limitation of the 35 MHz bandwidth of the CGPS, we demonstrate that\nuseful information on Faraday rotation structures can be obtained by combining\nsingle-antenna and aperture-synthesis data, highlighting the important synergy\nbetween future broadband interferometric and single-antenna polarization\nsurveys.",
        "The quantum Rabi model exhibits a superradiant phase transition when the\ncoupling becomes strong, even though it involves only two components: a\ntwo-level atom and a single bosonic mode. This phase transition is referred to\nas a few-body quantum phase transition, in contrast to conventional phase\ntransitions in many-body systems. In this work, we investigate heat transport\nacross an atom embedded in bosonic modes, modeled by the quantum Rabi model,\nbetween two thermal baths. We found a manifestation of the superradiant phase\ntransition in the thermal conductance, which represents the linear response to\na temperature bias. Our work can be helpful for the development of quantum heat\ndevices utilizing controllable few-body phase transitions.",
        "... and annoys everyone with unsolicited experiments. The present paper\nproposes a short pedagogical review of the various phenomena that can be\nobserved in a coffee cup with little to no equipment. The physical domains\nspanned include acoustics, optics and, of course, fluid mechanics. The variety\nof experimental and theoretical techniques introduced throughout the paper\nmakes it suitable for a broad audience. For each topic, we first propose an\nexperimental realization before introducing a minimal model to explain the\nobservations. We end each section by discussing more advanced works existing in\nthe literature as well as related applications. We provide detailed\nexperimental procedures and videos of the experiments that can be freely used\nfor teaching purposes. The phenomena presented here also show remarkable\nefficiency as icebreakers for morning coffee in laboratories or conferences.",
        "We derive a manifestly superconformally covariant unfolded formulation of the\nfree (2,0) tensor multiplet. The unfolded system consists of an abelian\ntwo-form and an infinite-dimensional, chiral Weyl zero-form realized using\nsuperoscillators. The construction of the cocycle gluing these forms on a\ngeneral superconformal background goes one step beyond previous results in\nsuper-Poincar\\'e backgrounds.",
        "We show that free QED is equivalent to the continuous-space-and-time limit of\nFermi and Bose lattice quantum cellular automata theories derived from quantum\nrandom walks satisfying simple symmetry and unitarity conditions. In doing so\nwe define the Fermi and Bose theories in a unified manner using the usual\nfermion internal space but a boson internal space that is six-dimensional. We\nshow that the reduction to a two-dimensional boson internal space (two helicity\nstates arising from spin-1 plus the photon transversality condition) comes from\nrestricting the quantum cellular automaton theory to positive energies. We\nbriefly examine common symmetries of quantum cellular automata, and how\ntime-reversal symmetry demands the existence of negative-energy solutions.\nThese solutions produce a tension in coupling the Fermi and Bose theories, in\nwhich the strong locality of quantum cellular automata seems to require a\nnonzero amplitude to produce negative-energy states, leading to an unphysical\ncascade of negative-energy particles. However, we show in a 1D model that by\nextending interactions over a larger (but finite) range it is possible to\nexponentially suppress the production of negative-energy particles to the point\nwhere they can be neglected.",
        "In this study, we focus on estimating the heterogeneous treatment effect\n(HTE) for survival outcome. The outcome is subject to censoring and the number\nof covariates is high-dimensional. We utilize data from both the randomized\ncontrolled trial (RCT), considered as the gold standard, and real-world data\n(RWD), possibly affected by hidden confounding factors. To achieve a more\nefficient HTE estimate, such integrative analysis requires great insight into\nthe data generation mechanism, particularly the accurate characterization of\nunmeasured confounding effects\/bias. With this aim, we propose a\npenalized-regression-based integrative approach that allows for the\nsimultaneous estimation of parameters, selection of variables, and\nidentification of the existence of unmeasured confounding effects. The\nconsistency, asymptotic normality, and efficiency gains are rigorously\nestablished for the proposed estimate.\n  Finally, we apply the proposed method to estimate the HTE of lobar\/sublobar\nresection on the survival of lung cancer patients. The RCT is a multicenter\nnon-inferiority randomized phase 3 trial, and the RWD comes from a clinical\noncology cancer registry in the United States. The analysis reveals that the\nunmeasured confounding exists and the integrative approach does enhance the\nefficiency for the HTE estimation.",
        "Diffusion models have emerged as powerful tools for solving inverse problems,\nyet prior work has primarily focused on observations with Gaussian measurement\nnoise, restricting their use in real-world scenarios. This limitation persists\ndue to the intractability of the likelihood score, which until now has only\nbeen approximated in the simpler case of Gaussian likelihoods. In this work, we\nextend diffusion models to handle inverse problems where the observations\nfollow a distribution from the exponential family, such as a Poisson or a\nBinomial distribution. By leveraging the conjugacy properties of exponential\nfamily distributions, we introduce the evidence trick, a method that provides a\ntractable approximation to the likelihood score. In our experiments, we\ndemonstrate that our methodology effectively performs Bayesian inference on\nspatially inhomogeneous Poisson processes with intensities as intricate as\nImageNet images. Furthermore, we demonstrate the real-world impact of our\nmethodology by showing that it performs competitively with the current\nstate-of-the-art in predicting malaria prevalence estimates in Sub-Saharan\nAfrica.",
        "W and W-based high-entropy alloys (HEAs) are promising candidates for\nplasma-facing materials in fusion reactors. While irradiation studies on W have\nrevealed a tendency for helium (He) bubble formation and radiation-induced\ndefects, investigations of WTaCrV HEA have demonstrated superior radiation\nresistance, whether under He+ irradiation or heavy ion irradiation. To assess\nmaterial performance under conditions relevant to fusion reactors -\ncharacterized by fast neutrons and gas production from transmutation reactions\n- complex irradiation environments need to be modeled. Using molecular dynamics\nsimulations, we examined defect evolution in W and equimolar WTaCrV HEA with\nand without preexisting He atoms under cascade overlap conditions up to 0.2 dpa\nat 300 K. In W, dislocation loops and large interstitial clusters formed\nreadily, with increasing He content leading to higher dislocation densities and\nthe formation of polygonal interstitial networks. In contrast, the WTaCrV alloy\nexhibited strong resistance to the formation of dislocation loops and large\ninterstitial clusters but was more susceptible to the formation of bubbles at\nhigher He concentrations. Bubble growth was driven by helium trapping at\nvacancy sites and the coalescence of smaller bubbles. Larger bubbles remained\nstable against cascade overlap, limiting further growth by coalescence.",
        "This paper presents an innovative adaptation of existing methodology to\ninvestigate external load in elite female soccer athletes using GPS-derived\nmovement data from 23 matches. We developed a quantitative framework to examine\nvelocity, acceleration, and movement angle across game halves, enabling\ntransparent and meaningful performance insights. By constructing a quantile\ncube to quantify movement patterns, we segmented athletes' movements into\ndistinct velocity, acceleration, and angle quantiles. Statistical analysis\nrevealed significant differences in movement distributions between match halves\nfor individual athletes. Principal Component Analysis (PCA) identified\nanomalous games with unique movement dynamics, particularly at the start and\nend of the season. Dirichlet-multinomial regression further explored how\nfactors like athlete position, playing time, and game characteristics\ninfluenced movement profiles. This approach provides a structured method for\nanalyzing movement dynamics, revealing external load variations over time and\noffering insights into performance optimization. The integration of these\nstatistical techniques demonstrates the potential of data-driven strategies to\nenhance athlete monitoring in soccer."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Long non-coding RNAs: definitions, functions, challenges and recommendations",
    "start_abstract":"Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Language Models are Few-Shot Learners"
      ],
      "abstract":[
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Adaptive Drift Compensation for Soft Sensorized Finger Using Continual\n  Learning",
        "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
        "A Survey of Internet Censorship and its Measurement: Methodology,\n  Trends, and Challenges",
        "Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective",
        "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search",
        "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot\n  Trajectories",
        "Density-Functional Perturbation Theory with Numeric Atom-Centered\n  Orbitals",
        "Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and\n  Power Interference in Multi-User OSaaS Networks",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
        "Study of Nucleon Charge-Exchange Processes at $^{12}$C Fragmentation\n  with an Energy of 300 MeV\/Nucleon",
        "Global Picard Spectra and Borel Parametrized Algebra",
        "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
        "Microscopic investigation of wobbling motion in even-even nuclei",
        "Improvement of Data Analytics Techniques in Reflection High Energy\n  Electron Diffraction to Enable Machine Learning",
        "Scalar behavior for a complex multi-soliton arising in blow-up for a\n  semilinear wave equation",
        "Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms\n  Radiologist MRI Interpretation: A Multi-Center Study",
        "3D-grids are not transducible from planar graphs",
        "RIS-Aided Fluid Antenna Array-Mounted UAV Networks",
        "Behavioral Homophily in Social Media via Inverse Reinforcement Learning:\n  A Reddit Case Study",
        "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics",
        "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes",
        "Potential Contribution of Young Pulsar Wind Nebulae to Galactic\n  High-Energy Neutrino Emission",
        "Estimating Parameters of Structural Models Using Neural Networks",
        "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models",
        "HoloGest: Decoupled Diffusion and Motion Priors for Generating\n  Holisticly Expressive Co-speech Gestures",
        "Orthogonal Representation Learning for Estimating Causal Quantities",
        "AI-Powered Noisy Quantum Emulation: Generalized Gate-Based Protocols for\n  Hardware-Agnostic Simulation"
      ],
      "abstract":[
        "Strain sensors are gaining popularity in soft robotics for acquiring tactile\ndata due to their flexibility and ease of integration. Tactile sensing plays a\ncritical role in soft grippers, enabling them to safely interact with\nunstructured environments and precisely detect object properties. However, a\nsignificant challenge with these systems is their high non-linearity,\ntime-varying behavior, and long-term signal drift. In this paper, we introduce\na continual learning (CL) approach to model a soft finger equipped with\npiezoelectric-based strain sensors for proprioception. To tackle the\naforementioned challenges, we propose an adaptive CL algorithm that integrates\na Long Short-Term Memory (LSTM) network with a memory buffer for rehearsal and\nincludes a regularization term to keep the model's decision boundary close to\nthe base signal while adapting to time-varying drift. We conduct nine different\nexperiments, resetting the entire setup each time to demonstrate signal drift.\nWe also benchmark our algorithm against two other methods and conduct an\nablation study to assess the impact of different components on the overall\nperformance.",
        "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
        "Internet censorship limits the access of nodes residing within a specific\nnetwork environment to the public Internet, and vice versa. During the last\ndecade, techniques for conducting Internet censorship have been developed\nfurther. Consequently, methodology for measuring Internet censorship had been\nimproved as well. In this paper, we firstly provide a survey of Internet\ncensorship techniques. Secondly, we survey censorship measurement methodology,\nincluding a coverage of available datasets. In cases where it is beneficial, we\nbridge the terminology and taxonomy of Internet censorship with related\ndomains, namely traffic obfuscation and information hiding. We cover both,\ntechnical and human aspects, as well as recent trends, and challenges.",
        "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
        "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.",
        "Diffusion models excel at creating images and videos thanks to their\nmultimodal generative capabilities. These same capabilities have made diffusion\nmodels increasingly popular in robotics research, where they are used for\ngenerating robot motion. However, the stochastic nature of diffusion models is\nfundamentally at odds with the precise dynamical equations describing the\nfeasible motion of robots. Hence, generating dynamically admissible robot\ntrajectories is a challenge for diffusion models. To alleviate this issue, we\nintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to\ngenerate provably admissible trajectories of black-box robotic systems using\ndiffusion models. A sequence of states is a dynamically admissible trajectory\nif each state of the sequence belongs to the reachable set of its predecessor\nby the robot's equations of motion. To generate such trajectories, our\ndiffusion policies project their predictions onto a dynamically admissible\nmanifold during both training and inference to align the objective of the\ndenoiser neural network with the dynamical admissibility constraint. The\nauto-regressive nature of these projections along with the black-box nature of\nrobot dynamics render these projections immensely challenging. We thus enforce\nadmissibility by iteratively sampling a polytopic under-approximation of the\nreachable set of a state onto which we project its predicted successor, before\niterating this process with the projected successor. By producing accurate\ntrajectories, this projection eliminates the need for diffusion models to\ncontinually replan, enabling one-shot long-horizon trajectory planning. We\ndemonstrate that our framework generates higher quality dynamically admissible\nrobot trajectories through extensive simulations on a quadcopter and various\nMuJoCo environments, along with real-world experiments on a Unitree GO1 and\nGO2.",
        "This paper represents one contribution to a larger Roadmap article reviewing\nthe current status of the FHI-aims code. In this contribution, the\nimplementation of density-functional perturbation theory in a numerical\natom-centered framework is summarized. Guidelines on usage and links to\ntutorials are provided.",
        "We present a framework to identify and mitigate rogue OOK signals and\nuser-generated power interference in a multi-user Optical-Spectrum-as-a-Service\nnetwork. Experimental tests on the OpenIreland-testbed achieve up to 89%\ndetection rate within 10 seconds of an interference event.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
        "The search of reactions with nucleon charge-exchange was performed on the\nFRAGM fragment-separator of the TWAC accelerator complex at fragmentation of\ncarbon nuclei with an energy of 300 MeV\/nucleon on a thin beryllium target. The\nexperimental setup, located at an angle of 3.5 degrees to the incident beam,\nhad high momentum resolution. Differential cross sections were measured for\n$^{11}$Be, $^{12}$B and $^{12}$Be as function of the nuclei momentum. The\nexperimental data were compared with theoretical predictions of various models\nof nucleus-nucleus interactions and other experimental results. Measurements of\nnucleon charge exchange processes in this energy region was carried out for the\nfirst time. New results were obtained to test theoretical models of\nnucleus-nucleus interactions.",
        "We answer a question of Schwede on the existence of global Picard spectra\nassociated to his ultra-commutative global ring spectra; given an\nultra-commutative global ring spectrum $R$, we show there exists a global\nspectrum $\\mathrm{pic}_\\mathrm{eq}(R)$ assembling the Picard spectra of all\nunderlying $G$-equivariant ring spectra $\\mathrm{res}_G R$ of $R$ into one\nobject, in that for all finite groups $G$, the genuine fixed points are given\nby $\\mathrm{pic}_\\mathrm{eq}(R)^G \\simeq\n\\mathrm{pic}(\\mathrm{Mod}_{\\mathrm{res}_G R}(\\mathrm{Sp}_G))$.\n  Along the way, we develop a generalization of Borel-equivariant objects in\nthe setting of parametrized higher algebra. We use this to assemble the\nsymmetric monoidal categories of $G$-spectra for all finite groups $G$ together\nwith all restrictions and norms into a single `normed global category', and\nbuild a comparison functor which allows us to import ultra-commutative\n$G$-equivariant or global ring spectra into the setting of parametrized higher\nalgebra.",
        "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
        "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode.",
        "Perovskite oxides such as LaFeO$_3$ are a well-studied family of materials\nthat possess a wide range of useful and novel properties. Successfully\nsynthesizing perovskite oxide samples usually requires a significant number of\ngrowth attempts and a detailed film characterization on each sample to find the\noptimal growth window of a material. The most common real-time \\textit{in situ}\ndiagnostic technique available during molecular beam epitaxy (MBE) synthesis is\nreflection high-energy electron diffraction (RHEED). Conventional use of RHEED\nallows a highly experienced operator to determine growth rate by monitoring\nintensity osciallations and make some qualitative observations during growth,\nsuch as recognizing the sample has become amorphous or recognizing that large\nislands have formed on the surface. However, due to a lack of theoretical\nunderstanding of the diffraction patterns, finer, more precise levels of\nobservations are challenging. To address these limitations, we implement new\ndata analytics techniques in the growth of three LaFeO$_3$ samples on Nb-doped\nSrTiO$_3$ by MBE. These techniques improve our ability to perform unsupervised\nmachine learning using principal component analysis (PCA) and k-means\nclustering by using drift correction to overcome sample or stage motion during\ngrowth and intensity transformations that highlight more subtle features in the\nimages such as Kikuchi bands. With this approach, we enable the first\ndemonstration of PCA and k-means across multiple samples, allowing for\nquantitative comparison of RHEED videos for two LaFeO$_3$ film samples. These\ncapabilities set the stage for real-time processing of RHEED data during growth\nto enable machine learning-accelerated film synthesis.",
        "This paper deals with blow-up for the complex-valued semilinear wave equation\nwith power nonlinearity in dimension 1. Up to a rotation of the solution in the\ncomplex plane, we show that near a characteristic blow-up point, the solution\nbehaves exactly as in the real-valued case. Namely, up to a rotation in the\ncomplex plane, the solution decomposes into a sum of a finite number of\ndecoupled solitons with alternate signs. The main novelty of our proof is a\nresolution of a complex-valued first order Toda system governing the evolution\nof the positions and the phases of the solitons.",
        "Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target\nsuspicious prostate lesions. This has led to artificial intelligence (AI)\napplications improving MRI-based detection of clinically significant prostate\ncancer (CsPCa). However, MRI-detected lesions must still be mapped to\ntransrectal ultrasound (TRUS) images during biopsy, which results in missing\nCsPCa. This study systematically evaluates a multimodal AI framework\nintegrating MRI and TRUS image sequences to enhance CsPCa identification. The\nstudy included 3110 patients from three cohorts across two institutions who\nunderwent prostate biopsy. The proposed framework, based on the 3D UNet\narchitecture, was evaluated on 1700 test cases, comparing performance to\nunimodal AI models that use either MRI or TRUS alone. Additionally, the\nproposed model was compared to radiologists in a cohort of 110 patients. The\nmultimodal AI approach achieved superior sensitivity (80%) and Lesion Dice\n(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared\nto radiologists, the multimodal model showed higher specificity (88% vs. 78%)\nand Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings\ndemonstrate the potential of multimodal AI to improve CsPCa lesion targeting\nduring biopsy and treatment planning, surpassing current unimodal models and\nradiologists; ultimately improving outcomes for prostate cancer patients.",
        "We prove that the class of 3D-grids is cannot be transduced from planar\ngraphs, and more generally, from any class of graphs of bounded Euler genus. To\nprove our result, we introduce a new structural tool called slice\ndecompositions, and show that every graph class transducible from a class of\ngraphs of bounded Euler genus is a perturbation of a graph class that admits\nslice decompositions.",
        "This paper investigates reconfigurable intelligent surface (RIS)-assisted\nunmanned aerial vehicle (UAV) downlink networks with fluid antennas (FA), where\nRIS enables non-line-of-sight (NLoS) transmissions. Moreover, the FA is\nequipped on the UAV offering dynamic antenna position adjustment, enhancing\nspatial diversity besides UAV deployment. We aim at total downlink rate\nmaximization while ensuring minimum user rate requirement. We consider joint\noptimization of active UAV beamforming, passive RIS beamforming, UAV deployment\nand FA position adjustment. To address the complex problem, we propose\nbeamfomring for RIS\/UAV and FA-UAV deployment (BRAUD) scheme by employing\nalternative optimization, successive convex approximation (SCA) and sequential\nrank-one constraint relaxation (SROCR) method for the decomposed subproblems.\nSimulation results demonstrate the effectiveness of RIS-FA-UAV, achieving the\nhighest rate among existing architectures without FA\/UAV\/RIS deployment and\nwithout proper beamforming. Moreover, BRAUD achieves the highest rate among\nbenchmarks of drop-rank method, heuristic optimizations and conventional\nzero-forcing beamforming as well as random method.",
        "Online communities play a critical role in shaping societal discourse and\ninfluencing collective behavior in the real world. The tendency for people to\nconnect with others who share similar characteristics and views, known as\nhomophily, plays a key role in the formation of echo chambers which further\namplify polarization and division. Existing works examining homophily in online\ncommunities traditionally infer it using content- or adjacency-based\napproaches, such as constructing explicit interaction networks or performing\ntopic analysis. These methods fall short for platforms where interaction\nnetworks cannot be easily constructed and fail to capture the complex nature of\nuser interactions across the platform. This work introduces a novel approach\nfor quantifying user homophily. We first use an Inverse Reinforcement Learning\n(IRL) framework to infer users' policies, then use these policies as a measure\nof behavioral homophily. We apply our method to Reddit, conducting a case study\nacross 5.9 million interactions over six years, demonstrating how this approach\nuncovers distinct behavioral patterns and user roles that vary across different\ncommunities. We further validate our behavioral homophily measure against\ntraditional content-based homophily, offering a powerful method for analyzing\nsocial media dynamics and their broader societal implications. We find, among\nothers, that users can behave very similarly (high behavioral homophily) when\ndiscussing entirely different topics like soccer vs e-sports (low topical\nhomophily), and that there is an entire class of users on Reddit whose purpose\nseems to be to disagree with others.",
        "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation.",
        "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
        "Pulsar wind nebulae (PWNe), especially the young ones, are among the most\nenergetic astrophysical sources in the Galaxy. It is usually believed that the\nspin-down energy injected from the pulsars is converted into magnetic field and\nrelativistic electrons, but the possible presence of proton acceleration inside\nPWNe cannot be ruled out. Previous works have estimated the neutrino emission\nfrom PWNe using various source catalogs measured in gamma-rays. However, such\nresults rely on the sensitivity of TeV gamma-ray observations and may omit the\ncontribution by unresolved sources. Here we estimate the potential neutrino\nemission from a synthetic population of PWNe in the Galaxy with a focus on the\nones that are still in the free expansion phase. In the calculation, we model\nthe temporal evolution of the free-expanding PWNe and consider the transport of\nprotons inside the PWNe. The Crab nebula is treated as a standard template for\nyoung PWNe to evaluate some model parameters, such as the energy conversion\nfraction of relativistic protons and the target gas density for the hadronic\nprocess, which are relevant to neutrino production. In the optimistic case, the\nneutrino flux from the simulated young PWNe may constitute to 5% of the\nmeasured flux by IceCube around 100 TeV. At higher energy around 1 PeV, the\nneutrino emission from the population highly depends on the injection spectral\nshape, and also on the emission of the nearby prominent sources.",
        "We study an alternative use of machine learning. We train neural nets to\nprovide the parameter estimate of a given (structural) econometric model, for\nexample, discrete choice or consumer search. Training examples consist of\ndatasets generated by the econometric model under a range of parameter values.\nThe neural net takes the moments of a dataset as input and tries to recognize\nthe parameter value underlying that dataset. Besides the point estimate, the\nneural net can also output statistical accuracy. This neural net estimator\n(NNE) tends to limited-information Bayesian posterior as the number of training\ndatasets increases. We apply NNE to a consumer search model. It gives more\naccurate estimates at lighter computational costs than the prevailing approach.\nNNE is also robust to redundant moment inputs. In general, NNE offers the most\nbenefits in applications where other estimation approaches require very heavy\nsimulation costs. We provide code at: https:\/\/nnehome.github.io.",
        "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.",
        "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps:\/\/cyk990422.github.io\/HoloGest.github.io\/.",
        "Representation learning is widely used for estimating causal quantities\n(e.g., the conditional average treatment effect) from observational data. While\nexisting representation learning methods have the benefit of allowing for\nend-to-end learning, they do not have favorable theoretical properties of\nNeyman-orthogonal learners, such as double robustness and quasi-oracle\nefficiency. Also, such representation learning methods often employ additional\nconstraints, like balancing, which may even lead to inconsistent estimation. In\nthis paper, we propose a novel class of Neyman-orthogonal learners for causal\nquantities defined at the representation level, which we call OR-learners. Our\nOR-learners have several practical advantages: they allow for consistent\nestimation of causal quantities based on any learned representation, while\noffering favorable theoretical properties including double robustness and\nquasi-oracle efficiency. In multiple experiments, we show that, under certain\nregularity conditions, our OR-learners improve existing representation learning\nmethods and achieve state-of-the-art performance. To the best of our knowledge,\nour OR-learners are the first work to offer a unified framework of\nrepresentation learning methods and Neyman-orthogonal learners for causal\nquantities estimation.",
        "Quantum computer emulators model the behavior and error rates of specific\nquantum processors. Without accurate noise models in these emulators, it is\nchallenging for users to optimize and debug executable quantum programs prior\nto running them on the quantum device, as device-specific noise is not properly\naccounted for. To overcome this challenge, we introduce a general protocol to\napproximate device-specific emulators without requiring pulse-level control. By\napplying machine learning to data obtained from gate set tomography, we\nconstruct a device-specific emulator by predicting the noise model input\nparameters that best match the target device. We demonstrate the effectiveness\nof our protocol's emulator in estimating the unitary coupled cluster energy of\nthe H$_2$ molecule and compare the results with those from actual quantum\nhardware. Remarkably, our noise model captures device noise with high accuracy,\nachieving a mean absolute difference of just 0.3\\% in expectation value\nrelative to the state-vector simulation."
      ]
    }
  },
  {
    "id":2411.17702,
    "research_type":"basic",
    "start_id":"b5",
    "start_title":"Patient Contrastive Learning: a Performant, Expressive, and Practical Approach to ECG Modeling.",
    "start_abstract":"Supervised machine learning applications in health care are often limited due to a scarcity of labeled training data. To mitigate this effect small sample size, we introduce pre-training approach, Patient Contrastive Learning Representations (PCLR), which creates latent representations ECGs from large number unlabeled examples. The resulting expressive, performant, and practical across wide spectrum clinical tasks. We develop PCLR using system with over 3.2 million 12-lead ECGs, demonstrate substantial improvements multiple new tasks when there fewer than 5,000 labels.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram"
      ],
      "abstract":[
        "Asymptomatic left ventricular dysfunction (ALVD) is present in 3-6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1-4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction \u226435%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG-a ubiquitous, low-cost test-permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD."
      ],
      "categories":[
        "Cardio"
      ]
    },
    "list":{
      "title":[
        "On singular problems associated with mixed operators under mixed\n  boundary conditions",
        "Effects of altermagnetic order, strain and doping on the optical and\n  vibrational properties of RuO$_2$",
        "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
        "Depth-Bounds for Neural Networks via the Braid Arrangement",
        "Ducci Matrices in $p$-adic Context",
        "Einstein's Cat -- A Thought Experiment Against Anti-Relativist Claims",
        "Involutions of spherical 3-manifolds",
        "Book I of Euclid's Elements and application of areas",
        "Maximal $L_p$-regularity for fractional problem driven by non-autonomous\n  forms",
        "Logarithmic Non-Abelian Hodge Theory for curves in prime characteristic",
        "Nonlinear Einstein-Power-Yang-Mills AdS Black Holes: From Quantum\n  Tunneling to Aschenbach Effect",
        "Observation of the Dirac Dispersions in Co-doped CaFe2As2",
        "Anticoncentration in Clifford Circuits and Beyond: From Random Tensor\n  Networks to Pseudo-Magic States",
        "Phases and critical transport of the SU(N) Hofstadter-Hubbard model on\n  the triangular lattice",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "IQPopt: Fast optimization of instantaneous quantum polynomial circuits\n  in JAX",
        "Physics-informed neural networks for phase-resolved data assimilation\n  and prediction of nonlinear ocean waves",
        "Intervals in Dyck paths and the wreath conjecture",
        "Quantifying Quantumness in (A)dS spacetimes with Unruh-DeWitt Detector",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Dynamic Structures of Knowledge Production: Citation Rates in Hydrogen\n  Technologies",
        "Spin nematic order and superconductivity in $J_1$-$J_2$ Kondo lattice\n  model on square lattice",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Space-Dependent Fractional Evolution Equations: A New Approach",
        "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
        "Assessing the value of advanced computing infrastructure for supporting\n  research: new tools to inform research policy",
        "Revisiting the outer-weakly convex domination number in graph products",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Premixed flame quenching distance between cold walls: effects of flow\n  and Lewis number"
      ],
      "abstract":[
        "In this paper, we study the following singular problem associated with mixed\noperators (the combination of the classical Laplace operator and the fractional\nLaplace operator) under mixed boundary conditions \\begin{equation*} \\label{1}\n  \\left\\{\n  \\begin{aligned}\n  \\mathcal{L}u &= g(u), \\quad u > 0 \\quad \\text{in} \\quad \\Omega,\n  u &= 0 \\quad \\text{in} \\quad U^c,\n  \\mathcal{N}_s(u) &= 0 \\quad \\text{in} \\quad \\mathcal{N},\n  \\frac{\\partial u}{\\partial \\nu} &= 0 \\quad \\text{in} \\quad \\partial \\Omega\n\\cap \\overline{\\mathcal{N}},\n  \\end{aligned}\n  \\right.\n  \\tag{$P_\\lambda$} \\end{equation*}\n  where $U= (\\Omega \\cup {\\mathcal{N}} \\cup\n(\\partial\\Omega\\cap\\overline{\\mathcal{N}}))$, $\\Omega \\subseteq \\mathbb{R}^N$\nis a non empty open set, $\\mathcal{D}$, $\\mathcal{N}$ are open subsets of\n$\\mathbb{R}^N\\setminus{\\bar{\\Omega }}$ such that ${\\mathcal{D}} \\cup\n{\\mathcal{N}}= \\mathbb{R}^N\\setminus{\\bar{\\Omega}}$, $\\mathcal{D} \\cap\n{\\mathcal{N}}= \\emptyset $ and $\\Omega\\cup \\mathcal{N}$ is a bounded set with\nsmooth boundary, $\\lambda >0$ is a real parameter and\n  $\\mathcal{L}= -\\Delta+(-\\Delta)^{s},~ \\text{for}~s \\in (0, 1).$\n  Here $g(u)=u^{-q}$ or $g(u)= \\lambda u^{-q}+ u^p$ with $0<q<1<p\\leq 2^*-1$.\nWe study $(P_\\lambda)$ to derive the existence of weak solutions along with its\n$L^\\infty$-regularity. Moreover, some Sobolev-type variational inequalities\nassociated with these weak solutions are established.",
        "RuO$_2$, one of the most widely studied transition metal oxides, was recently\npredicted to host a novel form of collinear magnetic order referred to as\naltermagnetism. In this study we combine experiment (reflectance,\ntransmittance, ellipsometry and Raman measurements) and first-principles\ncalculations to elucidate the potential role of altermagnetic order, strain and\ndoping on the optical and vibrational properties of RuO$_2$ grown on TiO$_2$\n(001), (101) and (110) substrates. The combination of experiment and theory in\nthis study surprisingly indicates RuO$_2$ is in fact best described if one\nassumes the nonmagnetic state. Calculations of the altermagnetic state leads to\npoor agreement with the measured optical and vibrational properties of RuO$_2$.",
        "Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset.",
        "We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.",
        "In this paper, we mutuate the concept of Ducci matrices to the $p$-adic\nsetting, generalizing the classical Ducci sequences to the framework of\n$p$-adic numbers. The classical Ducci operator, which iteratively computes the\nabsolute differences of neighboring elements in a sequence or matrix, is\nredefined using the $p$-adic absolute value $| \\cdot |_p$. We investigate the\ndynamics of $p$-adic Ducci sequences for matrices over $\\mathbb{Q}_p$, focusing\non their convergence and periodicity properties.",
        "When faced with overwhelming evidence supporting the reality of time\ndilation, confirmed in particular by the Hafele-Keating experiment, some\nanti-relativists reluctantly concede that time dilation applies to light\nclocks. However, they argue that the Theory of Relativity remains flawed,\nclaiming that time dilation applies to light clocks only, not to massive\nobjects. They assert that atomic clocks, which operate based on microwave\nradiation, merely create the illusion that the Hafele-Keating experiment\nconfirms the theory. To refute this misconception, we introduce a thought\nexperiment inspired by Schrodinger's cat, in which the fate of Einstein's cat\ndepends on a \"Sync-or-Die clock\", an imaginary device that tests the\nsynchronization between a light clock and a mechanical clock, potentially\ntriggering the release of poison. By analyzing this scenario from both the\ninertial frame where the device is at rest and another in which it moves at\nconstant velocity, we demonstrate that time dilation must apply to the\nmechanical clock in exactly the same way as it does to the light clock,\nhighlighting the universality of relativistic time dilation.",
        "We classify involutions acting on spherical 3-manifolds up to conjugacy. Our\ngeometric approach provides insights into numerous topological properties of\nthese involutions.",
        "We work through Book I of Euclid's Elements with our focus on application of\nareas (I.42, I.44, I.45). We summarize alternate constructions from medieval\neditions of Euclid's elements and ancient and medieval commentaries. We remark\nthat Euclid's proof of I.44 involves a seldom commented on use of\nsuperposition, but that several medieval editions of Euclid give constructions\nthat avoid the use of superposition. This use of superposition is also avoided\nin Ralph Abraham's \"VCE: The Visual Constructions of Euclid\" C#12, C#12B at\nhttp:\/\/www.visual-euclid.org\/vce\/contents.html\n  We collate the figures with the digitized editions of Euclid at (P)\nBiblioteca Apostolica Vaticana (BAV), Vat. gr. 190, (F) Florence, Biblioteca\nMedicea Laurenziana, Plut. 28.03, (B) Bodleian, MS. D'Orville 301, (V)\n\\\"Osterreichische Nationalbibliothek, Cod. Phil. gr. 31 Han",
        "We investigate the maximal $L_p$-regularity in J.L. Lions' problem involving\na time-fractional derivative and a non-autonomous form $a(t;\\cdot,\\cdot)$ on a\nHilbert space $H$. This problem says whether the maximal $L_p$-regularity in\n$H$ hold when $t \\mapsto a(t ; u, v)$ is merely continuous or even merely\nmeasurable. We prove the maximal $L_p$-regularity results when the coefficients\nsatisfy general Dini-type continuity conditions. In particular, we construct a\ncounterexample to negatively answer this problem, indicating the minimal\nH\\\"{o}lder-scale regularity required for positive results.",
        "For a curve C and a reductive group G in prime characteristic, we relate the\nde Rham moduli of logarithmic G-connections on C to the Dolbeault moduli of\nlogarithmic G-Higgs bundles on the Frobenius twist of C. We name this result\nthe Log-p-NAHT. It is a logarithmic version of Chen-Zhu's characteristic p Non\nAbelian Hodge Theorem (p-NAHT). In contrast to the no pole case, the two moduli\nstacks in the log case are not isomorphic etale locally over the Hitchin base.\nInstead, they differ by an Artin-Schreier type Galois cover of the base. In\ncontrast to the case over the complex numbers, where some parabolic\/parahoric\ndata are needed to specify the boundary behavior of the tame harmonic metrics,\nno parabolic\/parahoric data are needed in Log-p-NAHT. We also establish a\nsemistable version of the Log-p-NAHT, and deduce several geometric and\ncohomological consequences. In particular, when G=GL_r, the Log-p-NAHT induces\nan embedding of the intersection cohomology of the degree d Dolbeault moduli to\nthat of the degree pd de Rham moduli, and the embedding is an isomorphism when\nr is coprime to d and p>r.",
        "This study investigates the thermodynamic and quantum properties of\nEinstein-Power-Yang-Mills (EPYM) black holes in an Anti-de Sitter background,\nfocusing on the effects of the nonlinear Yang-Mills charge parameter $\\gamma$.\nWe derive the metric function, analyze Hawking radiation through boson\ntunneling, and calculate thermodynamic properties including temperature and\nphase transitions. The quantum tunneling of $W^+$ bosons is examined using the\nWKB approximation and Hamilton-Jacobi formalism, revealing how nonlinearity\nmodifies the radiation spectrum. We compute the effective potential governing\nphoton orbits and null geodesics, demonstrating significant alterations in\nlight behavior in strong gravitational fields. Additionally, we explore the\nAschenbach effect, showing that this phenomenon, which is typically associated\nwith rotating black holes, can emerge in spherically symmetric EPYM spacetimes\nbecause of non-linear field interactions. Our results may yield observational\nmarkers that can be identified with instruments such as the Event Horizon\nTelescope and upcoming gravitational wave detectors.",
        "We performed an angle-resolved photoemission spectroscopy (ARPES) study of\nthe electronic structure of the CaFe$_2$As$_2$ 122-iron pnictide, a parent\ncompound, and two iron-based superconductors CaFe$_{2-x}$Co$_x$As$_2$ ($x =\n0.07$ and 0.15). We studied the band structure of this system across the phase\ndiagram with the transition from the orthorhombic spin density wave (SDW) phase\nto the tetragonal paramagnetic phase. We observed characteristic features of\nthe electronic structures corresponding to the antiferromagnetic phase in the\nparent compound and the samples with low cobalt concentration ($x = 0.07$). For\nhighly doped systems ($x = 0.15$), the measurements revealed the concentric\nbranches of the Fermi surface, which are associated with paramagnetic and\nsuperconducting 122-iron pnictides. We found the existence of Dirac cones\nlocated at 30 meV below Fermi energy for nonsuperconducting CaFe$_2$As$_2$ and\nsuperconducting CaFe$_{1.93}$Co$_{0.07}$As$_2$ orthorhombic SDW systems.",
        "Anticoncentration describes how an ensemble of quantum states spreads over\nthe allowed Hilbert space, leading to statistically uniform output probability\ndistributions. In this work, we investigate the anticoncentration of random\nClifford circuits toward the overlap distribution of random stabilizer states.\nUsing exact analytical techniques and extensive numerical simulations based on\nClifford replica tensor networks, we demonstrate that random Clifford circuits\nfully anticoncentrate in logarithmic circuit depth, namely higher-order moments\nof the overlap distribution converge to those of random stabilizer states.\nMoreover, we investigate the effect of introducing a controlled number of\nnon-Clifford (magic) resources into Clifford circuits. We show that inserting a\npolylogarithmic in qudit number of $T$-states is sufficient to drive the\noverlap distribution toward the Porter-Thomas statistics, effectively\nrecovering full quantum randomness. In short, this fact presents doped tensor\nnetworks and shallow Clifford circuits as pseudo-magic quantum states. Our\nresults clarify the interplay between Clifford dynamics, magic resource\ninjection, and quantum complexity, with implications for quantum circuit\nsampling and benchmarking of computational quantum advantage.",
        "We study phases and transitions of a triangular Hubbard model subject to\ncommensurate magnetic field, called the Hofstadter-Hubbard model. At filling\none fermion per site, for the number of fermion flavors 2 <= N <= 4, we\nidentify three distinct phases and calculate critical interaction strength from\nself-consistent mean-field approximation. Integer quantum Hall, chiral spin\nliquid, and valence bond solid (or stripe) states could be realized upon\nvarying the Hubbard interaction U. We study the critical transport behavior\nusing quantum Boltzmann equations for general N for the putative continuous\ntransition from quantum Hall states to chiral spin liquid. The critical\nbehavior serves as strong signatures of the critical theory and consequently of\nthe existence of chiral spin liquid as the proximate phase.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "IQPopt is a software package designed to optimize large-scale instantaneous\nquantum polynomial circuits on classical hardware. By exploiting an efficient\nclassical simulation algorithm for expectation value estimation, circuits with\nthousands of qubits and millions of gates can be optimized, provided the\nrelevant objective function has an efficient description in terms of Pauli-Z\ntype observables. Since sampling from instantaneous quantum polynomial circuits\nis widely believed to be hard for classical computers, this provides a method\nto identify powerful circuit instances before deployment and sampling on\nquantum hardware, where computational advantages may exist. The package\nleverages automatic differentiation in JAX, can be accelerated with access to\nhardware accelerators such as graphics processing units, and contains a\ndedicated module that can be used to train and evaluate quantum generative\nmodels via the maximum mean discrepancy.",
        "The assimilation and prediction of phase-resolved surface gravity waves are\ncritical challenges in ocean science and engineering. Potential flow theory\n(PFT) has been widely employed to develop wave models and numerical techniques\nfor wave prediction. However, traditional wave prediction methods are often\nlimited. For example, most simplified wave models have a limited ability to\ncapture strong wave nonlinearity, while fully nonlinear PFT solvers often fail\nto meet the speed requirements of engineering applications. This computational\ninefficiency also hinders the development of effective data assimilation\ntechniques, which are required to reconstruct spatial wave information from\nsparse measurements to initialize the wave prediction. To address these\nchallenges, we propose a novel solver method that leverages physics-informed\nneural networks (PINNs) that parameterize PFT solutions as neural networks.\nThis provides a computationally inexpensive way to assimilate and predict wave\ndata. The proposed PINN framework is validated through comparisons with\nanalytical linear PFT solutions and experimental data collected in a laboratory\nwave flume. The results demonstrate that our approach accurately captures and\npredicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover,\nthe PINN can infer the fully nonlinear velocity potential throughout the entire\nfluid volume solely from surface elevation measurements, enabling the\ncalculation of fluid velocities that are difficult to measure experimentally.",
        "Let $\\iota_{k}(m,l)$ denote the total number of intervals of length $m$\nacross all Dyck paths of semilength $k$ such that each interval contains\nprecisely $l$ falls. We give the formula for $\\iota_{k}(m,l)$ and show that\n$\\iota_{k}(k,l)=\\binom{k}{l}^2$. Motivated by this, we propose two stronger\nvariants of the wreath conjecture due to Baranyai for $n=2k+1$.",
        "Probing quantumness in curved spacetime is regarded as one of fundamental and\nimportant topics in the framework of relativistic quantum information. In this\nwork, we focus on the theoretical feasibility of probing quantum properties in\nde Sitter (dS) and Anti-de Sitter (AdS) spacetimes via detectors. By employing\nthe Unruh-DeWitt detector coupled with a massless scalar field, which is\ntreated as an open system, quantum uncertainty and quantum coherence in both dS\nand AdS spacetimes are investigated. Our analysis reveals that the acceleration\nin dS spacetime and the boundary conditions in AdS spacetime significantly\nimpact the detector's evolution in the initial stage. Notably, both of the\nuncertainty and coherence will oscillate with the initial state being in a\nsuperposition state, however the high temperature is able to suppress their\noscillation. Interestingly, it is found that the constant values of the final\nuncertainty and coherence are identical as those in dS and AdS spacetimes,\nwhich are determined by the ratio of energy gap to temperature. Hence, the\ncurrent exploration offers insight into quantumness in dS and AdS spacetimes,\nand might be helpful to facilitate the curved-spacetime-based quantum\ninformation processing.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We explore a dynamic patent citation network model to explain the established\nlink between network structure and technological improvement rate. This model,\na type of survival model, posits that the *dynamic* network structure\ndetermines the *constant* improvement rate, requiring consistent structural\nreproduction over time. The model's hazard rate, the probability of a patent\nbeing cited, represents \"knowledge production,\" reflecting the output of new\npatents given existing ones. Analyzing hydrogen technology patents, we find\ndistinct subdomain knowledge production rates, but consistent development\nacross subdomains. \"Distribution\" patents show the lowest production rate,\nsuggesting dominant \"distribution\" costs in $H_2$ pricing. Further modeling\nshows Katz-centrality predicts knowledge production, outperforming subdomain\nclassification. Lower Katz centrality in \"distribution\" suggests inherent\norganizational differences in invention. Exploitative learning\n(within-subdomain citations) correlates with higher patenting opportunity\ncosts, potentially explaining slower \"distribution\" development, as high\ninvestment needs may incentivize monopolization over knowledge sharing.",
        "We investigate competition and cooperation of magnetic frustration and the\nKondo effect in the $J_1$-$J_2$ Kondo lattice model on the square lattice at\nzero temperature. In this model, the frustrated interactions $J_1,J_2$ between\nthe localized spins stabilize spin nematic orders, while the Kondo coupling\nfavors local spin singlets. Using the slave boson mean field approximation, we\nfind that the spin nematic order remains stable against small Kondo coupling,\nand the localized spins and the conduction electrons are effectively decoupled.\nOn the other hand, a standard Fermi liquid state is formed for sufficiently\nstrong Kondo interactions. Furthermore, in an intermediate region with moderate\nKondo coupling, the spin nematic order and the Kondo effect coexist, and\nsuperconducting pairing of the conduction electrons is induced by the spinon\npairing. We discuss the ground state phase diagram and nature of the quantum\nphase transitions between the different superconducting states.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "Inspired by the works of \\cite{baz2} and \\cite{kian}, this study develops an\nabstract framework for analyzing differential equations with space-dependent\nfractional time derivatives and bounded operators. Within this framework, we\nestablish existence and uniqueness results for solutions in both linear and\nsemilinear settings. Our findings provide deeper insights into how spatially\nvarying fractional derivatives influence the behavior of differential\nequations, shedding light on their mathematical properties and potential\napplications.",
        "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
        "Purpose: How much to invest in research facilities has long been a question\nin research policy and practice in higher education. This matter is\ntime-sensitive due to critical financial challenges at institutions in the USA,\nwith signs of significant problems in Europe. The purpose of this report is to\npresent new techniques for assessment of one particular type of research\ninfrastructure - computing facilities and staff that support research. These\nnew approaches are timely because of the ongoing financial crises which may\nmake it essential for institutions of higher education to make difficult\ndecisions regarding research infrastructure.\n  Principal results: We present recently developed methods for assessment of\nthe economic and scientific value of investment in advanced computing\nfacilities and services. Existing examples of these tools in use show that\ninvestment in advanced computing facilities and services contributes\nimportantly to positive financial and academic outcomes for institutions of\nhigher education. We present a format based on the Balanced Scorecard concept\nfor summarizing such information.\n  Conclusion: The methods presented here enable quantitative assessment of the\nrelationship between investment in computing facilities and research and\neducation outcomes. These methods should be of interest to research policy\ninvestigators and practitioners. The analysis methods described may be applied\nretroactively, making this report of potentially immediate value in setting\nresearch policies.",
        "Let $G = (V, E)$ be a simple undirected graph. A set $C \\subseteq V(G)$ is\nweakly convex of graph $G$ if for every two vertices $u,v\\in G$, there exists a\n$u-v$ geodesic whose vertices are in $C$. A set $C \\subseteq V$ is an\nouter-weakly convex dominating set if it is dominating set and every vertex not\nin $C$ is adjacent to some vertex in $C$ and a set $V(G)\\setminus C$ is weakly\nconvex. The outer-weakly convex domination number of graph $G$, denoted by\n$\\widetilde{ \\gamma}_{wcon}(G)$, is the minimum cardinality of an outer-weakly\nconvex dominating vertex set of graph $G$. In this paper, we determined the\nouter-weakly convex domination number of two graphs under the cartesian, strong\nand lexicographic products, and discuss some important combinatorial findings.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This study investigates the critical conditions for flame propagation in\nchannels with cold walls. We analyze the impact of the Lewis number and flow\namplitude ($A$) on the minimum channel width required to sustain a premixed\nflame. Our results span a wide range of Lewis numbers, encompassing both aiding\nand opposing flow conditions. Results are presented for both variable and\nconstant density models. A combined numerical approach, involving stationary\nand time-dependent simulations, is employed to determine quenching distances\nand solution stability. We find that smaller Lewis numbers and aiding flows ($A\n< 0$) facilitate flame propagation in narrower channels, while opposing flows\n($A > 0$) tend to destabilize the flame, promoting asymmetric solutions. For\nsufficiently large positive values of $A$, the quenching distance is determined\nby asymmetric solutions, rather than the typical symmetric ones."
      ]
    }
  },
  {
    "id":2411.17702,
    "research_type":"basic",
    "start_id":"b11",
    "start_title":"CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients",
    "start_abstract":"The healthcare industry generates troves of unlabelled physiological data. This data can be exploited via contrastive learning, a self-supervised pre-training method that encourages representations instances to similar one another. We propose family learning methods, CLOCS, across space, time, \\textit{and} patients show CLOCS consistently outperforms the state-of-the-art BYOL and SimCLR, when performing linear evaluation of, fine-tuning on, downstream tasks. also achieves strong generalization performance with only 25\\% labelled training Furthermore, our procedure naturally patient-specific used quantify patient-similarity.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram"
      ],
      "abstract":[
        "Asymptomatic left ventricular dysfunction (ALVD) is present in 3-6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1-4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction \u226435%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG-a ubiquitous, low-cost test-permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD."
      ],
      "categories":[
        "Cardio"
      ]
    },
    "list":{
      "title":[
        "On singular problems associated with mixed operators under mixed\n  boundary conditions",
        "Effects of altermagnetic order, strain and doping on the optical and\n  vibrational properties of RuO$_2$",
        "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
        "Depth-Bounds for Neural Networks via the Braid Arrangement",
        "Ducci Matrices in $p$-adic Context",
        "Einstein's Cat -- A Thought Experiment Against Anti-Relativist Claims",
        "Involutions of spherical 3-manifolds",
        "Book I of Euclid's Elements and application of areas",
        "Maximal $L_p$-regularity for fractional problem driven by non-autonomous\n  forms",
        "Logarithmic Non-Abelian Hodge Theory for curves in prime characteristic",
        "Nonlinear Einstein-Power-Yang-Mills AdS Black Holes: From Quantum\n  Tunneling to Aschenbach Effect",
        "Observation of the Dirac Dispersions in Co-doped CaFe2As2",
        "Anticoncentration in Clifford Circuits and Beyond: From Random Tensor\n  Networks to Pseudo-Magic States",
        "Phases and critical transport of the SU(N) Hofstadter-Hubbard model on\n  the triangular lattice",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "IQPopt: Fast optimization of instantaneous quantum polynomial circuits\n  in JAX",
        "Physics-informed neural networks for phase-resolved data assimilation\n  and prediction of nonlinear ocean waves",
        "Intervals in Dyck paths and the wreath conjecture",
        "Quantifying Quantumness in (A)dS spacetimes with Unruh-DeWitt Detector",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Dynamic Structures of Knowledge Production: Citation Rates in Hydrogen\n  Technologies",
        "Spin nematic order and superconductivity in $J_1$-$J_2$ Kondo lattice\n  model on square lattice",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Space-Dependent Fractional Evolution Equations: A New Approach",
        "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
        "Assessing the value of advanced computing infrastructure for supporting\n  research: new tools to inform research policy",
        "Revisiting the outer-weakly convex domination number in graph products",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Premixed flame quenching distance between cold walls: effects of flow\n  and Lewis number"
      ],
      "abstract":[
        "In this paper, we study the following singular problem associated with mixed\noperators (the combination of the classical Laplace operator and the fractional\nLaplace operator) under mixed boundary conditions \\begin{equation*} \\label{1}\n  \\left\\{\n  \\begin{aligned}\n  \\mathcal{L}u &= g(u), \\quad u > 0 \\quad \\text{in} \\quad \\Omega,\n  u &= 0 \\quad \\text{in} \\quad U^c,\n  \\mathcal{N}_s(u) &= 0 \\quad \\text{in} \\quad \\mathcal{N},\n  \\frac{\\partial u}{\\partial \\nu} &= 0 \\quad \\text{in} \\quad \\partial \\Omega\n\\cap \\overline{\\mathcal{N}},\n  \\end{aligned}\n  \\right.\n  \\tag{$P_\\lambda$} \\end{equation*}\n  where $U= (\\Omega \\cup {\\mathcal{N}} \\cup\n(\\partial\\Omega\\cap\\overline{\\mathcal{N}}))$, $\\Omega \\subseteq \\mathbb{R}^N$\nis a non empty open set, $\\mathcal{D}$, $\\mathcal{N}$ are open subsets of\n$\\mathbb{R}^N\\setminus{\\bar{\\Omega }}$ such that ${\\mathcal{D}} \\cup\n{\\mathcal{N}}= \\mathbb{R}^N\\setminus{\\bar{\\Omega}}$, $\\mathcal{D} \\cap\n{\\mathcal{N}}= \\emptyset $ and $\\Omega\\cup \\mathcal{N}$ is a bounded set with\nsmooth boundary, $\\lambda >0$ is a real parameter and\n  $\\mathcal{L}= -\\Delta+(-\\Delta)^{s},~ \\text{for}~s \\in (0, 1).$\n  Here $g(u)=u^{-q}$ or $g(u)= \\lambda u^{-q}+ u^p$ with $0<q<1<p\\leq 2^*-1$.\nWe study $(P_\\lambda)$ to derive the existence of weak solutions along with its\n$L^\\infty$-regularity. Moreover, some Sobolev-type variational inequalities\nassociated with these weak solutions are established.",
        "RuO$_2$, one of the most widely studied transition metal oxides, was recently\npredicted to host a novel form of collinear magnetic order referred to as\naltermagnetism. In this study we combine experiment (reflectance,\ntransmittance, ellipsometry and Raman measurements) and first-principles\ncalculations to elucidate the potential role of altermagnetic order, strain and\ndoping on the optical and vibrational properties of RuO$_2$ grown on TiO$_2$\n(001), (101) and (110) substrates. The combination of experiment and theory in\nthis study surprisingly indicates RuO$_2$ is in fact best described if one\nassumes the nonmagnetic state. Calculations of the altermagnetic state leads to\npoor agreement with the measured optical and vibrational properties of RuO$_2$.",
        "Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset.",
        "We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.",
        "In this paper, we mutuate the concept of Ducci matrices to the $p$-adic\nsetting, generalizing the classical Ducci sequences to the framework of\n$p$-adic numbers. The classical Ducci operator, which iteratively computes the\nabsolute differences of neighboring elements in a sequence or matrix, is\nredefined using the $p$-adic absolute value $| \\cdot |_p$. We investigate the\ndynamics of $p$-adic Ducci sequences for matrices over $\\mathbb{Q}_p$, focusing\non their convergence and periodicity properties.",
        "When faced with overwhelming evidence supporting the reality of time\ndilation, confirmed in particular by the Hafele-Keating experiment, some\nanti-relativists reluctantly concede that time dilation applies to light\nclocks. However, they argue that the Theory of Relativity remains flawed,\nclaiming that time dilation applies to light clocks only, not to massive\nobjects. They assert that atomic clocks, which operate based on microwave\nradiation, merely create the illusion that the Hafele-Keating experiment\nconfirms the theory. To refute this misconception, we introduce a thought\nexperiment inspired by Schrodinger's cat, in which the fate of Einstein's cat\ndepends on a \"Sync-or-Die clock\", an imaginary device that tests the\nsynchronization between a light clock and a mechanical clock, potentially\ntriggering the release of poison. By analyzing this scenario from both the\ninertial frame where the device is at rest and another in which it moves at\nconstant velocity, we demonstrate that time dilation must apply to the\nmechanical clock in exactly the same way as it does to the light clock,\nhighlighting the universality of relativistic time dilation.",
        "We classify involutions acting on spherical 3-manifolds up to conjugacy. Our\ngeometric approach provides insights into numerous topological properties of\nthese involutions.",
        "We work through Book I of Euclid's Elements with our focus on application of\nareas (I.42, I.44, I.45). We summarize alternate constructions from medieval\neditions of Euclid's elements and ancient and medieval commentaries. We remark\nthat Euclid's proof of I.44 involves a seldom commented on use of\nsuperposition, but that several medieval editions of Euclid give constructions\nthat avoid the use of superposition. This use of superposition is also avoided\nin Ralph Abraham's \"VCE: The Visual Constructions of Euclid\" C#12, C#12B at\nhttp:\/\/www.visual-euclid.org\/vce\/contents.html\n  We collate the figures with the digitized editions of Euclid at (P)\nBiblioteca Apostolica Vaticana (BAV), Vat. gr. 190, (F) Florence, Biblioteca\nMedicea Laurenziana, Plut. 28.03, (B) Bodleian, MS. D'Orville 301, (V)\n\\\"Osterreichische Nationalbibliothek, Cod. Phil. gr. 31 Han",
        "We investigate the maximal $L_p$-regularity in J.L. Lions' problem involving\na time-fractional derivative and a non-autonomous form $a(t;\\cdot,\\cdot)$ on a\nHilbert space $H$. This problem says whether the maximal $L_p$-regularity in\n$H$ hold when $t \\mapsto a(t ; u, v)$ is merely continuous or even merely\nmeasurable. We prove the maximal $L_p$-regularity results when the coefficients\nsatisfy general Dini-type continuity conditions. In particular, we construct a\ncounterexample to negatively answer this problem, indicating the minimal\nH\\\"{o}lder-scale regularity required for positive results.",
        "For a curve C and a reductive group G in prime characteristic, we relate the\nde Rham moduli of logarithmic G-connections on C to the Dolbeault moduli of\nlogarithmic G-Higgs bundles on the Frobenius twist of C. We name this result\nthe Log-p-NAHT. It is a logarithmic version of Chen-Zhu's characteristic p Non\nAbelian Hodge Theorem (p-NAHT). In contrast to the no pole case, the two moduli\nstacks in the log case are not isomorphic etale locally over the Hitchin base.\nInstead, they differ by an Artin-Schreier type Galois cover of the base. In\ncontrast to the case over the complex numbers, where some parabolic\/parahoric\ndata are needed to specify the boundary behavior of the tame harmonic metrics,\nno parabolic\/parahoric data are needed in Log-p-NAHT. We also establish a\nsemistable version of the Log-p-NAHT, and deduce several geometric and\ncohomological consequences. In particular, when G=GL_r, the Log-p-NAHT induces\nan embedding of the intersection cohomology of the degree d Dolbeault moduli to\nthat of the degree pd de Rham moduli, and the embedding is an isomorphism when\nr is coprime to d and p>r.",
        "This study investigates the thermodynamic and quantum properties of\nEinstein-Power-Yang-Mills (EPYM) black holes in an Anti-de Sitter background,\nfocusing on the effects of the nonlinear Yang-Mills charge parameter $\\gamma$.\nWe derive the metric function, analyze Hawking radiation through boson\ntunneling, and calculate thermodynamic properties including temperature and\nphase transitions. The quantum tunneling of $W^+$ bosons is examined using the\nWKB approximation and Hamilton-Jacobi formalism, revealing how nonlinearity\nmodifies the radiation spectrum. We compute the effective potential governing\nphoton orbits and null geodesics, demonstrating significant alterations in\nlight behavior in strong gravitational fields. Additionally, we explore the\nAschenbach effect, showing that this phenomenon, which is typically associated\nwith rotating black holes, can emerge in spherically symmetric EPYM spacetimes\nbecause of non-linear field interactions. Our results may yield observational\nmarkers that can be identified with instruments such as the Event Horizon\nTelescope and upcoming gravitational wave detectors.",
        "We performed an angle-resolved photoemission spectroscopy (ARPES) study of\nthe electronic structure of the CaFe$_2$As$_2$ 122-iron pnictide, a parent\ncompound, and two iron-based superconductors CaFe$_{2-x}$Co$_x$As$_2$ ($x =\n0.07$ and 0.15). We studied the band structure of this system across the phase\ndiagram with the transition from the orthorhombic spin density wave (SDW) phase\nto the tetragonal paramagnetic phase. We observed characteristic features of\nthe electronic structures corresponding to the antiferromagnetic phase in the\nparent compound and the samples with low cobalt concentration ($x = 0.07$). For\nhighly doped systems ($x = 0.15$), the measurements revealed the concentric\nbranches of the Fermi surface, which are associated with paramagnetic and\nsuperconducting 122-iron pnictides. We found the existence of Dirac cones\nlocated at 30 meV below Fermi energy for nonsuperconducting CaFe$_2$As$_2$ and\nsuperconducting CaFe$_{1.93}$Co$_{0.07}$As$_2$ orthorhombic SDW systems.",
        "Anticoncentration describes how an ensemble of quantum states spreads over\nthe allowed Hilbert space, leading to statistically uniform output probability\ndistributions. In this work, we investigate the anticoncentration of random\nClifford circuits toward the overlap distribution of random stabilizer states.\nUsing exact analytical techniques and extensive numerical simulations based on\nClifford replica tensor networks, we demonstrate that random Clifford circuits\nfully anticoncentrate in logarithmic circuit depth, namely higher-order moments\nof the overlap distribution converge to those of random stabilizer states.\nMoreover, we investigate the effect of introducing a controlled number of\nnon-Clifford (magic) resources into Clifford circuits. We show that inserting a\npolylogarithmic in qudit number of $T$-states is sufficient to drive the\noverlap distribution toward the Porter-Thomas statistics, effectively\nrecovering full quantum randomness. In short, this fact presents doped tensor\nnetworks and shallow Clifford circuits as pseudo-magic quantum states. Our\nresults clarify the interplay between Clifford dynamics, magic resource\ninjection, and quantum complexity, with implications for quantum circuit\nsampling and benchmarking of computational quantum advantage.",
        "We study phases and transitions of a triangular Hubbard model subject to\ncommensurate magnetic field, called the Hofstadter-Hubbard model. At filling\none fermion per site, for the number of fermion flavors 2 <= N <= 4, we\nidentify three distinct phases and calculate critical interaction strength from\nself-consistent mean-field approximation. Integer quantum Hall, chiral spin\nliquid, and valence bond solid (or stripe) states could be realized upon\nvarying the Hubbard interaction U. We study the critical transport behavior\nusing quantum Boltzmann equations for general N for the putative continuous\ntransition from quantum Hall states to chiral spin liquid. The critical\nbehavior serves as strong signatures of the critical theory and consequently of\nthe existence of chiral spin liquid as the proximate phase.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "IQPopt is a software package designed to optimize large-scale instantaneous\nquantum polynomial circuits on classical hardware. By exploiting an efficient\nclassical simulation algorithm for expectation value estimation, circuits with\nthousands of qubits and millions of gates can be optimized, provided the\nrelevant objective function has an efficient description in terms of Pauli-Z\ntype observables. Since sampling from instantaneous quantum polynomial circuits\nis widely believed to be hard for classical computers, this provides a method\nto identify powerful circuit instances before deployment and sampling on\nquantum hardware, where computational advantages may exist. The package\nleverages automatic differentiation in JAX, can be accelerated with access to\nhardware accelerators such as graphics processing units, and contains a\ndedicated module that can be used to train and evaluate quantum generative\nmodels via the maximum mean discrepancy.",
        "The assimilation and prediction of phase-resolved surface gravity waves are\ncritical challenges in ocean science and engineering. Potential flow theory\n(PFT) has been widely employed to develop wave models and numerical techniques\nfor wave prediction. However, traditional wave prediction methods are often\nlimited. For example, most simplified wave models have a limited ability to\ncapture strong wave nonlinearity, while fully nonlinear PFT solvers often fail\nto meet the speed requirements of engineering applications. This computational\ninefficiency also hinders the development of effective data assimilation\ntechniques, which are required to reconstruct spatial wave information from\nsparse measurements to initialize the wave prediction. To address these\nchallenges, we propose a novel solver method that leverages physics-informed\nneural networks (PINNs) that parameterize PFT solutions as neural networks.\nThis provides a computationally inexpensive way to assimilate and predict wave\ndata. The proposed PINN framework is validated through comparisons with\nanalytical linear PFT solutions and experimental data collected in a laboratory\nwave flume. The results demonstrate that our approach accurately captures and\npredicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover,\nthe PINN can infer the fully nonlinear velocity potential throughout the entire\nfluid volume solely from surface elevation measurements, enabling the\ncalculation of fluid velocities that are difficult to measure experimentally.",
        "Let $\\iota_{k}(m,l)$ denote the total number of intervals of length $m$\nacross all Dyck paths of semilength $k$ such that each interval contains\nprecisely $l$ falls. We give the formula for $\\iota_{k}(m,l)$ and show that\n$\\iota_{k}(k,l)=\\binom{k}{l}^2$. Motivated by this, we propose two stronger\nvariants of the wreath conjecture due to Baranyai for $n=2k+1$.",
        "Probing quantumness in curved spacetime is regarded as one of fundamental and\nimportant topics in the framework of relativistic quantum information. In this\nwork, we focus on the theoretical feasibility of probing quantum properties in\nde Sitter (dS) and Anti-de Sitter (AdS) spacetimes via detectors. By employing\nthe Unruh-DeWitt detector coupled with a massless scalar field, which is\ntreated as an open system, quantum uncertainty and quantum coherence in both dS\nand AdS spacetimes are investigated. Our analysis reveals that the acceleration\nin dS spacetime and the boundary conditions in AdS spacetime significantly\nimpact the detector's evolution in the initial stage. Notably, both of the\nuncertainty and coherence will oscillate with the initial state being in a\nsuperposition state, however the high temperature is able to suppress their\noscillation. Interestingly, it is found that the constant values of the final\nuncertainty and coherence are identical as those in dS and AdS spacetimes,\nwhich are determined by the ratio of energy gap to temperature. Hence, the\ncurrent exploration offers insight into quantumness in dS and AdS spacetimes,\nand might be helpful to facilitate the curved-spacetime-based quantum\ninformation processing.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We explore a dynamic patent citation network model to explain the established\nlink between network structure and technological improvement rate. This model,\na type of survival model, posits that the *dynamic* network structure\ndetermines the *constant* improvement rate, requiring consistent structural\nreproduction over time. The model's hazard rate, the probability of a patent\nbeing cited, represents \"knowledge production,\" reflecting the output of new\npatents given existing ones. Analyzing hydrogen technology patents, we find\ndistinct subdomain knowledge production rates, but consistent development\nacross subdomains. \"Distribution\" patents show the lowest production rate,\nsuggesting dominant \"distribution\" costs in $H_2$ pricing. Further modeling\nshows Katz-centrality predicts knowledge production, outperforming subdomain\nclassification. Lower Katz centrality in \"distribution\" suggests inherent\norganizational differences in invention. Exploitative learning\n(within-subdomain citations) correlates with higher patenting opportunity\ncosts, potentially explaining slower \"distribution\" development, as high\ninvestment needs may incentivize monopolization over knowledge sharing.",
        "We investigate competition and cooperation of magnetic frustration and the\nKondo effect in the $J_1$-$J_2$ Kondo lattice model on the square lattice at\nzero temperature. In this model, the frustrated interactions $J_1,J_2$ between\nthe localized spins stabilize spin nematic orders, while the Kondo coupling\nfavors local spin singlets. Using the slave boson mean field approximation, we\nfind that the spin nematic order remains stable against small Kondo coupling,\nand the localized spins and the conduction electrons are effectively decoupled.\nOn the other hand, a standard Fermi liquid state is formed for sufficiently\nstrong Kondo interactions. Furthermore, in an intermediate region with moderate\nKondo coupling, the spin nematic order and the Kondo effect coexist, and\nsuperconducting pairing of the conduction electrons is induced by the spinon\npairing. We discuss the ground state phase diagram and nature of the quantum\nphase transitions between the different superconducting states.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "Inspired by the works of \\cite{baz2} and \\cite{kian}, this study develops an\nabstract framework for analyzing differential equations with space-dependent\nfractional time derivatives and bounded operators. Within this framework, we\nestablish existence and uniqueness results for solutions in both linear and\nsemilinear settings. Our findings provide deeper insights into how spatially\nvarying fractional derivatives influence the behavior of differential\nequations, shedding light on their mathematical properties and potential\napplications.",
        "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
        "Purpose: How much to invest in research facilities has long been a question\nin research policy and practice in higher education. This matter is\ntime-sensitive due to critical financial challenges at institutions in the USA,\nwith signs of significant problems in Europe. The purpose of this report is to\npresent new techniques for assessment of one particular type of research\ninfrastructure - computing facilities and staff that support research. These\nnew approaches are timely because of the ongoing financial crises which may\nmake it essential for institutions of higher education to make difficult\ndecisions regarding research infrastructure.\n  Principal results: We present recently developed methods for assessment of\nthe economic and scientific value of investment in advanced computing\nfacilities and services. Existing examples of these tools in use show that\ninvestment in advanced computing facilities and services contributes\nimportantly to positive financial and academic outcomes for institutions of\nhigher education. We present a format based on the Balanced Scorecard concept\nfor summarizing such information.\n  Conclusion: The methods presented here enable quantitative assessment of the\nrelationship between investment in computing facilities and research and\neducation outcomes. These methods should be of interest to research policy\ninvestigators and practitioners. The analysis methods described may be applied\nretroactively, making this report of potentially immediate value in setting\nresearch policies.",
        "Let $G = (V, E)$ be a simple undirected graph. A set $C \\subseteq V(G)$ is\nweakly convex of graph $G$ if for every two vertices $u,v\\in G$, there exists a\n$u-v$ geodesic whose vertices are in $C$. A set $C \\subseteq V$ is an\nouter-weakly convex dominating set if it is dominating set and every vertex not\nin $C$ is adjacent to some vertex in $C$ and a set $V(G)\\setminus C$ is weakly\nconvex. The outer-weakly convex domination number of graph $G$, denoted by\n$\\widetilde{ \\gamma}_{wcon}(G)$, is the minimum cardinality of an outer-weakly\nconvex dominating vertex set of graph $G$. In this paper, we determined the\nouter-weakly convex domination number of two graphs under the cartesian, strong\nand lexicographic products, and discuss some important combinatorial findings.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This study investigates the critical conditions for flame propagation in\nchannels with cold walls. We analyze the impact of the Lewis number and flow\namplitude ($A$) on the minimum channel width required to sustain a premixed\nflame. Our results span a wide range of Lewis numbers, encompassing both aiding\nand opposing flow conditions. Results are presented for both variable and\nconstant density models. A combined numerical approach, involving stationary\nand time-dependent simulations, is employed to determine quenching distances\nand solution stability. We find that smaller Lewis numbers and aiding flows ($A\n< 0$) facilitate flame propagation in narrower channels, while opposing flows\n($A > 0$) tend to destabilize the flame, promoting asymmetric solutions. For\nsufficiently large positive values of $A$, the quenching distance is determined\nby asymmetric solutions, rather than the typical symmetric ones."
      ]
    }
  },
  {
    "id":2411.17702,
    "research_type":"basic",
    "start_id":"b2",
    "start_title":"Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram",
    "start_abstract":"Asymptomatic left ventricular dysfunction (ALVD) is present in 3-6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1-4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction \u226435%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG-a ubiquitous, low-cost test-permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD.",
    "start_categories":[
      "Cardio"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5",
        "b11"
      ],
      "title":[
        "Patient Contrastive Learning: a Performant, Expressive, and Practical Approach to ECG Modeling.",
        "CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients"
      ],
      "abstract":[
        "Supervised machine learning applications in health care are often limited due to a scarcity of labeled training data. To mitigate this effect small sample size, we introduce pre-training approach, Patient Contrastive Learning Representations (PCLR), which creates latent representations ECGs from large number unlabeled examples. The resulting expressive, performant, and practical across wide spectrum clinical tasks. We develop PCLR using system with over 3.2 million 12-lead ECGs, demonstrate substantial improvements multiple new tasks when there fewer than 5,000 labels.",
        "The healthcare industry generates troves of unlabelled physiological data. This data can be exploited via contrastive learning, a self-supervised pre-training method that encourages representations instances to similar one another. We propose family learning methods, CLOCS, across space, time, \\textit{and} patients show CLOCS consistently outperforms the state-of-the-art BYOL and SimCLR, when performing linear evaluation of, fine-tuning on, downstream tasks. also achieves strong generalization performance with only 25\\% labelled training Furthermore, our procedure naturally patient-specific used quantify patient-similarity."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "A Hadamard theorem in transversely affine geometry with applications to\n  affine orbifolds",
        "Aligning Instance-Semantic Sparse Representation towards Unsupervised\n  Object Segmentation and Shape Abstraction with Repeatable Primitives",
        "A non-degeneracy theorem for interacting electrons in one dimension",
        "RiskHarvester: A Risk-based Tool to Prioritize Secret Removal Efforts in\n  Software Artifacts",
        "RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation\n  Models",
        "Can LLM Agents Maintain a Persona in Discourse?",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Topological phase transition through tunable nearest-neighbor\n  interactions in a one-dimensional lattice",
        "VicSim: Enhancing Victim Simulation with Emotional and Linguistic\n  Fidelity",
        "Vision-Driven Prompt Optimization for Large Language Models in\n  Multimodal Generative Tasks",
        "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
        "MAD-BA: 3D LiDAR Bundle Adjustment -- from Uncertainty Modelling to\n  Structure Optimization",
        "Uniform bounds in excellent $\\mathbf{F}_p$-algebras and applications to\n  semi-continuity",
        "QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations\n  Using Neural Network Potentials",
        "A nonlocal degenerate macroscopic model of traffic dynamics with\n  saturated diffusion: modeling and calibration theory",
        "Constraints on Evolutions of Fundamental Constants from Clustering of\n  Fast Radio Burst Dispersion Measure",
        "Smart Sampling Strategies for Wireless Industrial Data Acquisition",
        "Spin-charge Kondo effect for a quantum dot with side coupled Majorana\n  zero mode",
        "Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer\n  Using Generative Artificial Intelligence",
        "Relative Tur\\'an densities of ordered graphs",
        "PhaseMO: Future-Proof, Energy-efficient, Adaptive Massive MIMO",
        "AdvSwap: Covert Adversarial Perturbation with High Frequency\n  Info-swapping for Autonomous Driving Perception",
        "Inference Scaling Reshapes AI Governance",
        "Bayesian inference from time series of allele frequency data using exact\n  simulation techniques",
        "Tensor parametric Hamiltonian operator inference",
        "Human-in-the-Loop Annotation for Image-Based Engagement Estimation:\n  Assessing the Impact of Model Reliability on Annotation Accuracy",
        "Global existence for multi-dimensional partially diffusive systems",
        "As Confidence Aligns: Exploring the Effect of AI Confidence on Human\n  Self-confidence in Human-AI Decision Making"
      ],
      "abstract":[
        "We introduce and investigate a novel notion of transversely affine foliation,\ncomparing and contrasting it to the previous ones in the literature. We then\nuse it to give an extension of the classic Hadamard's theorem from Riemannian\ngeometry to this setting. Our main result is a transversely affine version of a\nwell-known \"Hadamard-like\" theorem by J. Hebda for Riemannian foliations.\nAlternatively, our result can be viewed as a foliation-theoretic analogue of\nthe Hadamard's theorem for affine manifolds proven by Beem and Parker. Namely,\nwe show that under the transverse analogs of pseudoconvexity and disprisonment\nfor the family of geodesics in the transverse affine geometry, together with an\nabsence of transverse conjugate points, the universal cover of a manifold\nendowed with a transversely affine foliation whose leaves are compact and with\nfinite holonomy is diffeomorphic to the product of a contractible manifold with\nthe universal cover of a leaf. This also leads to a Beem-Parker-type\nHadamard-like theorem for affine orbifolds.",
        "Understanding 3D object shapes necessitates shape representation by object\nparts abstracted from results of instance and semantic segmentation. Promising\nshape representations enable computers to interpret a shape with meaningful\nparts and identify their repeatability. However, supervised shape\nrepresentations depend on costly annotation efforts, while current unsupervised\nmethods work under strong semantic priors and involve multi-stage training,\nthereby limiting their generalization and deployment in shape reasoning and\nunderstanding. Driven by the tendency of high-dimensional semantically similar\nfeatures to lie in or near low-dimensional subspaces, we introduce a one-stage,\nfully unsupervised framework towards semantic-aware shape representation. This\nframework produces joint instance segmentation, semantic segmentation, and\nshape abstraction through sparse representation and feature alignment of object\nparts in a high-dimensional space. For sparse representation, we devise a\nsparse latent membership pursuit method that models each object part feature as\na sparse convex combination of point features at either the semantic or\ninstance level, promoting part features in the same subspace to exhibit similar\nsemantics. For feature alignment, we customize an attention-based strategy in\nthe feature space to align instance- and semantic-level object part features\nand reconstruct the input shape using both of them, ensuring geometric\nreusability and semantic consistency of object parts. To firm up semantic\ndisambiguation, we construct cascade unfrozen learning on geometric parameters\nof object parts.",
        "In this paper, we show that the ground-state of many-body Schr\\\"odinger\noperators for electrons in one dimension is non-degenerate. More precisely, we\nconsider Schr\\\"odinger operators of the form $H_N(v,w) = -\\Delta + \\sum_{i\\neq\nj}^N w(x_i,x_j) + \\sum_{j=1}^N v(x_i)$ acting on $\\wedge^N\n\\mathrm{L}^2([0,1])$, where the external and interaction potentials $v$ and $w$\nbelong to a large class of distributions. In this setting, we show that the\nground-state of the system with Fermi statistics and local boundary conditions\nis non-degenerate and does not vanish on a set of positive measure. In the case\nof periodic and anti-periodic (or more general non-local) boundary conditions,\nwe show that the same result holds whenever the number of particles is odd and\neven, respectively. This non-degeneracy result seems to be new even for regular\npotentials $v$ and $w$. As an immediate application of this result, we prove\neigenvalue inequalities and the strong unique continuation property for\neigenfunctions of the single-particle one-dimensional operators $h(v) = -\\Delta\n+v$. In addition, we prove strict inequalities between the lowest eigenvalues\nof different self-adjoint realizations of $H_N(v,w)$.",
        "Since 2020, GitGuardian has been detecting checked-in hard-coded secrets in\nGitHub repositories. During 2020-2023, GitGuardian has observed an upward\nannual trend and a four-fold increase in hard-coded secrets, with 12.8 million\nexposed in 2023. However, removing all the secrets from software artifacts is\nnot feasible due to time constraints and technical challenges. Additionally,\nthe security risks of the secrets are not equal, protecting assets ranging from\nobsolete databases to sensitive medical data. Thus, secret removal should be\nprioritized by security risk reduction, which existing secret detection tools\ndo not support. The goal of this research is to aid software practitioners in\nprioritizing secrets removal efforts through our security risk-based tool. We\npresent RiskHarvester, a risk-based tool to compute a security risk score based\non the value of the asset and ease of attack on a database. We calculated the\nvalue of asset by identifying the sensitive data categories present in a\ndatabase from the database keywords in the source code. We utilized data flow\nanalysis, SQL, and ORM parsing to identify the database keywords. To calculate\nthe ease of attack, we utilized passive network analysis to retrieve the\ndatabase host information. To evaluate RiskHarvester, we curated RiskBench, a\nbenchmark of 1,791 database secret-asset pairs with sensitive data categories\nand host information manually retrieved from 188 GitHub repositories.\nRiskHarvester demonstrates precision of (95%) and recall (90%) in detecting\ndatabase keywords for the value of asset and precision of (96%) and recall\n(94%) in detecting valid hosts for ease of attack. Finally, we conducted a\nsurvey (52 respondents) to understand whether developers prioritize secret\nremoval based on security risk score. We found that 86% of the developers\nprioritized the secrets for removal with descending security risk scores.",
        "Referring remote sensing image segmentation is crucial for achieving\nfine-grained visual understanding through free-format textual input, enabling\nenhanced scene and object extraction in remote sensing applications. Current\nresearch primarily utilizes pre-trained language models to encode textual\ndescriptions and align them with visual modalities, thereby facilitating the\nexpression of relevant visual features. However, these approaches often\nstruggle to establish robust alignments between fine-grained semantic concepts,\nleading to inconsistent representations across textual and visual information.\nTo address these limitations, we introduce a referring remote sensing image\nsegmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual\nand textual encoding, employing both global and local textual semantics as\nfilters to generate referring-related visual activation features in the latent\nspace. These activated features then serve as input prompts for SAM, which\nrefines the segmentation masks through its robust visual generalization\ncapabilities. Experimental results on the RRSIS-D dataset demonstrate that\nRSRefSeg outperforms existing methods, underscoring the effectiveness of\nfoundational models in enhancing multimodal task comprehension. The code is\navailable at \\url{https:\/\/github.com\/KyanChen\/RSRefSeg}.",
        "Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High\/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "We investigate the phase diagram of a one-dimensional model of hardcore\nbosons or spinless fermions with tunable nearest-neighbor interactions. By\nintroducing alternating repulsive and attractive interactions on consecutive\nbonds, we show that the system undergoes a transition from a bond-ordered (BO)\nphase to a charge-density wave-II (CDW-II) phase as the attractive interaction\nstrength increases at a fixed repulsive interaction. For a specific interaction\npattern, the BO phase exhibits topological properties, which vanish when the\npattern is altered, leading to a transition from a topological BO phase to a\ntrivial BO phase through a gap-closing point where both interactions vanish. We\nidentify these phases using a combination of order parameters, topological\ninvariants, edge-state analysis and Thouless charge pumping. By extending our\nanalysis beyond half-filling, we explore the phase diagram across all densities\nand identify the superfluid (SF) and the pair-superfluid (PSF) phases,\ncharacterized by single-particle and bound-pair excitations at incommensurate\ndensities. The proposed model is experimentally realizable in platforms such as\nRydberg excited or ultracold atoms in optical lattices, offering a versatile\nframework to study such interplay between topology and interactions in\nlow-dimensional systems.",
        "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness.",
        "Vision generation remains a challenging frontier in artificial intelligence,\nrequiring seamless integration of visual understanding and generative\ncapabilities. In this paper, we propose a novel framework, Vision-Driven Prompt\nOptimization (VDPO), that leverages Large Language Models (LLMs) to dynamically\ngenerate textual prompts from visual inputs, guiding high-fidelity image\nsynthesis. VDPO combines a visual embedding prompt tuner, a textual instruction\ngenerator, and a vision generation module to achieve state-of-the-art\nperformance in diverse vision generation tasks. Extensive experiments on\nbenchmarks such as COCO and Sketchy demonstrate that VDPO consistently\noutperforms existing methods, achieving significant improvements in FID, LPIPS,\nand BLEU\/CIDEr scores. Additional analyses reveal the scalability, robustness,\nand generalization capabilities of VDPO, making it a versatile solution for\nin-domain and out-of-domain tasks. Human evaluations further validate the\npractical superiority of VDPO in generating visually appealing and semantically\ncoherent outputs.",
        "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
        "The joint optimization of sensor poses and 3D structure is fundamental for\nstate estimation in robotics and related fields. Current LiDAR systems often\nprioritize pose optimization, with structure refinement either omitted or\ntreated separately using representations like signed distance functions or\nneural networks. This paper introduces a framework for simultaneous\noptimization of sensor poses and 3D map, represented as surfels. A generalized\nLiDAR uncertainty model is proposed to address degraded or less reliable\nmeasurements in varying scenarios. Experimental results on public datasets\ndemonstrate improved performance over most comparable state-of-the-art methods.\nThe system is provided as open-source software to support further research.",
        "We study two important numerical invariants, Hilbert-Kunz multiplicity and\n$F$-signature, on the spectrum of a Noetherian $\\mathbf{F}_p$-algebra $R$ that\nis not necessarily $F$-finite. When $R$ is excellent, we show that the limits\ndefining the invariants are uniform. As a consequence, we show that the\n$F$-signature is lower semi-continuous, and the Hilbert-Kunz multiplicity is\nupper semi-continuous provided $R$ is locally equidimensional. Uniform\nconvergence is achieved via a uniform version of Cohen-Gabber theorem. We prove\nthe results under weaker conditions than excellence.",
        "Accurate prediction of protein-ligand binding affinities is crucial in drug\ndiscovery, particularly during hit-to-lead and lead optimization phases,\nhowever, limitations in ligand force fields continue to impact prediction\naccuracy. In this work, we validate relative binding free energy (RBFE)\naccuracy using neural network potentials (NNPs) for the ligands. We utilize a\nnovel NNP model, AceForce 1.0, based on the TensorNet architecture for small\nmolecules that broadens the applicability to diverse drug-like compounds,\nincluding all important chemical elements and supporting charged molecules.\nUsing established benchmarks, we show overall improved accuracy and correlation\nin binding affinity predictions compared with GAFF2 for molecular mechanics and\nANI2-x for NNPs. Slightly less accuracy but comparable correlations with OPLS4.\nWe also show that we can run the NNP simulations at 2 fs timestep, at least two\ntimes larger than previous NNP models, providing significant speed gains. The\nresults show promise for further evolutions of free energy calculations using\nNNPs while demonstrating its practical use already with the current generation.\nThe code and NNP model are publicly available for research use.",
        "In this work, we introduce a novel first-order nonlocal partial differential\nequation with saturated diffusion to describe the macroscopic behavior of\ntraffic dynamics. We show how the proposed model is better in comparison with\nexisting models in explaining the underlying driver behavior in real traffic\ndata. In doing so, we introduce a methodology for adjusting the parameters of\nthe proposed PDE with respect to the distribution of real datasets. In\nparticular, we conceptually and analytically elaborate on how such calibration\nconnects the solution of the PDE to the probability transition kernel proposed\nby the datasets.\n  The performance of the model is thoroughly investigated with respect to\nseveral metrics. More precisely, we study the capability of the model in\ncapturing the probability distribution realized by the datasets in the form of\nthe fundamental diagram. We show that the model is capable of approximating the\ndynamics of the evolution of the probability distribution. To this end, we\nevaluate the performance of the model with regard to the congestion formation\nand dissipation scenarios from various datasets.",
        "Constrained measurements of fundamental physical constants using astronomical\nobservational data represent a powerful method for investigating potential new\nphysics. In particular, the dispersion measure (DM) of fast radio bursts\n(FRBs), which probes the electron density along their propagation paths, may be\ninfluenced by the space-time variation of the fine-structure constant\n\\(\\alpha\\). In this study, we analyze the cross-correlation signal between\nforeground galaxies and the DM of background FRBs to constrain the evolution of\n\\(\\alpha\\). Assuming large-scale structure (LSS) galaxy surveys with the\ncapabilities of the China Space Station Telescope (CSST) at \\(z=0.15\\) and { a\nmock FRB survey with \\(N_{\\text{FRB}}=10^5\\) at \\(z=0.4\\), we test how well\n\\(\\alpha\\) variation can be constrained}, with a standard deviation of\n\\(\\sigma(\\Delta \\alpha \/ \\alpha) = 0.0007\\) at \\(z=0.15\\). Furthermore, taking\ninto account the nonminimal coupling between the scalar field and the\nelectromagnetic field, the variation in \\(\\alpha\\) can lead to the\nnon-conservation of photon number along geodesics. This would result in a\nviolation of the CDDR and affect the evolution of the Cosmic Microwave\nBackground (CMB) temperature. In this work, we { obtain constraints results} on\nthe CDDR parameter \\(\\eta\\) and the parameter \\(\\beta\\) governing CMB\ntemperature evolution at \\(z=0.15\\), yielding \\(\\sigma(\\eta) = 0.0004\\) and\n\\(\\sigma(\\beta) = 0.0006\\), respectively. Finally, we relate the variation in\n\\(\\alpha\\) to the time evolution of the proton-to-electron mass ratio, {\nreporting a standard deviation} of \\(\\sigma(\\Delta \\mu\/\\mu) = 0.002\\) at\n$z=0.15$. Future FRB surveys hold significant potential for advancing our\nunderstanding of the evolution of fundamental physical constants.",
        "In industrial environments, data acquisition accuracy is crucial for process\ncontrol and optimization. Wireless telemetry has proven to be a valuable tool\nfor improving efficiency in well-testing operations, enabling bidirectional\ncommunication and real-time control of downhole tools. However, high sampling\nfrequencies present challenges in telemetry, including data storage,\ntransmission, computational resource consumption, and battery life of wireless\ndevices. This study explores how optimizing data acquisition strategies can\nreduce aliasing effects and systematic errors while improving sampling rates\nwithout compromising measurement accuracy. A reduction of 80% in sampling\nfrequency was achieved without degrading measurement quality, demonstrating the\npotential for resource optimization in industrial environments.",
        "We investigate a minimal system consisting of a quantum dot coupled to a\nMajorana zero mode and a normal lead. We identify the underlying screening\nprocess as a novel spin-charge Kondo effect, where the low-energy spin and\ncharge degrees of freedom of the Majorana zero mode-quantum dot subsystem are\nfully screened by those in the normal lead, resulting in the formation of a\nspin-charge singlet. An effective low-energy model is derived, with charge\nfluctuations appropriately accounted for. This spin-charge Kondo effect is\nfound to be consistent with the spin-dependent Andreev\/normal boundary\nconditions induced by the Majorana zero mode. We demonstrate that the anomalous\nsubstructure in the spectrum and thermodynamic properties is closely tied to\nthe proportion of the charge component in the screening cloud. The spin-charge\nscreening cloud exhibits scaling behavior analogous to that of traditional\nKondo systems, though the sub-leading even-odd effect is subtly modified by the\nboundary conditions. These findings enhance our understanding of Kondo physics\nand resolve key debates on quantum dot nanostructures with Majorana zero modes.",
        "Full-Field Digital Mammography (FFDM) is the primary imaging modality for\nroutine breast cancer screening; however, its effectiveness is limited in\npatients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced\nSpectral Mammography (CESM), a second-level imaging technique, offers enhanced\naccuracy in tumor detection. Nonetheless, its application is restricted due to\nhigher radiation exposure, the use of contrast agents, and limited\naccessibility. As a result, CESM is typically reserved for select cases,\nleaving many patients to rely solely on FFDM despite the superior diagnostic\nperformance of CESM. While biopsy remains the gold standard for definitive\ndiagnosis, it is an invasive procedure that can cause discomfort for patients.\nWe introduce a multimodal, multi-view deep learning approach for virtual\nbiopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral\noblique views to classify lesions as malignant or benign. To address the\nchallenge of missing CESM data, we leverage generative artificial intelligence\nto impute CESM images from FFDM scans. Experimental results demonstrate that\nincorporating the CESM modality is crucial to enhance the performance of\nvirtual biopsy. When real CESM data is missing, synthetic CESM images proved\neffective, outperforming the use of FFDM alone, particularly in multimodal\nconfigurations that combine FFDM and CESM modalities. The proposed approach has\nthe potential to improve diagnostic workflows, providing clinicians with\naugmented intelligence tools to improve diagnostic accuracy and patient care.\nAdditionally, as a contribution to the research community, we publicly release\nthe dataset used in our experiments, facilitating further advancements in this\nfield.",
        "We introduce a modification of the Tur\\'an density of ordered graphs and\ninvestigate this graph parameter.",
        "The rapid proliferation of devices and increasing data traffic in cellular\nnetworks necessitate advanced solutions to meet these escalating demands.\nMassive MIMO (Multiple Input Multiple Output) technology offers a promising\napproach, significantly enhancing throughput, coverage, and spatial\nmulti-plexing. Despite its advantages, massive MIMO systems often lack flexible\nsoftware controls over hardware, limiting their ability to optimize operational\nexpenditure (OpEx) by reducing power consumption while maintaining performance.\nCurrent software-controlled methods, such as antenna muting combined with\ndigital beamforming and hybrid beamforming, have notable limitations. Antenna\nmuting struggles to maintain throughput and coverage, while hybrid beamforming\nfaces hardware constraints that restrict scalability and future-proofing. This\nwork presents PhaseMO, a versatile approach that adapts to varying network\nloads. PhaseMO effectively reduces power consumption in low-load scenarios\nwithout sacrificing coverage and overcomes the hardware limitations of hybrid\nbeamforming, offering a scalable and future-proof solution. We will show that\nPhaseMO can achieve up to 30% improvement in energy efficiency while avoiding\nabout 10% coverage reduction and 5dB increase in UE transmit power.",
        "Perception module of Autonomous vehicles (AVs) are increasingly susceptible\nto be attacked, which exploit vulnerabilities in neural networks through\nadversarial inputs, thereby compromising the AI safety. Some researches focus\non creating covert adversarial samples, but existing global noise techniques\nare detectable and difficult to deceive the human visual system. This paper\nintroduces a novel adversarial attack method, AdvSwap, which creatively\nutilizes wavelet-based high-frequency information swapping to generate covert\nadversarial samples and fool the camera. AdvSwap employs invertible neural\nnetwork for selective high-frequency information swapping, preserving both\nforward propagation and data integrity. The scheme effectively removes the\noriginal label data and incorporates the guidance image data, producing\nconcealed and robust adversarial samples. Experimental evaluations and\ncomparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can\nmake concealed attacks on common traffic targets. The generates adversarial\nsamples are also difficult to perceive by humans and algorithms. Meanwhile, the\nmethod has strong attacking robustness and attacking transferability.",
        "The shift from scaling up the pre-training compute of AI systems to scaling\nup their inference compute may have profound effects on AI governance. The\nnature of these effects depends crucially on whether this new inference compute\nwill primarily be used during external deployment or as part of a more complex\ntraining programme within the lab. Rapid scaling of inference-at-deployment\nwould: lower the importance of open-weight models (and of securing the weights\nof closed models), reduce the impact of the first human-level models, change\nthe business model for frontier AI, reduce the need for power-intense data\ncentres, and derail the current paradigm of AI governance via training compute\nthresholds. Rapid scaling of inference-during-training would have more\nambiguous effects that range from a revitalisation of pre-training scaling to a\nform of recursive self-improvement via iterated distillation and amplification.",
        "A central statistical problem in population genetics is to infer evolutionary\nand biological parameters such as the strength of natural selection and allele\nage from DNA samples extracted from a contemporary population. That all samples\ncome only from the present-day has long been known to limit statistical\ninference; there is potentially more information available if one also has\naccess to ancient DNA so that inference is based on a time-series of historical\nchanges in allele frequencies. We introduce a Markov Chain Monte Carlo (MCMC)\nmethod for Bayesian inference from allele frequency time-series data based on\nan underlying Wright--Fisher diffusion model of evolution, through which one\ncan infer the parameters of essentially any selection model including those\nwith frequency-dependent effects. The chief novelty is that we show this method\nto be exact in the sense that it is possible to augment the state space\nexplored by MCMC with the unobserved diffusion trajectory, even though the\ntransition function of this diffusion is intractable. Through careful design of\na proposal distribution, we describe an efficient method in which updates to\nthe trajectory and accept\/reject decisions are calculated without error. We\nillustrate the method on data capturing changes in coat colour over the past\n20,000 years, and find evidence to support previous findings that the mutant\nalleles ASIP and MC1R responsible for changes in coat color have experienced\nvery strong, possibly overdominant, selection and further provide estimates for\nthe ages of these genes.",
        "This work presents a tensor-based approach to constructing data-driven\nreduced-order models corresponding to semi-discrete partial differential\nequations with canonical Hamiltonian structure. By expressing parameter-varying\noperators with affine dependence as contractions of a generalized parameter\nvector against a constant tensor, this method leverages the operator inference\nframework to capture parametric dependence in the learned reduced-order model\nvia the solution to a convex, least-squares optimization problem. This leads to\na concise and straightforward implementation which compactifies previous\nparametric operator inference approaches and directly extends to learning\nparametric operators with symmetry constraints, a key feature required for\nconstructing structure-preserving surrogates of Hamiltonian systems. The\nproposed approach is demonstrated on both a (non-Hamiltonian) heat equation\nwith variable diffusion coefficient as well as a Hamiltonian wave equation with\nvariable wave speed.",
        "Human-in-the-loop (HITL) frameworks are increasingly recognized for their\npotential to improve annotation accuracy in emotion estimation systems by\ncombining machine predictions with human expertise. This study focuses on\nintegrating a high-performing image-based emotion model into a HITL annotation\nframework to evaluate the collaborative potential of human-machine interaction\nand identify the psychological and practical factors critical to successful\ncollaboration. Specifically, we investigate how varying model reliability and\ncognitive framing influence human trust, cognitive load, and annotation\nbehavior in HITL systems. We demonstrate that model reliability and\npsychological framing significantly impact annotators' trust, engagement, and\nconsistency, offering insights into optimizing HITL frameworks. Through three\nexperimental scenarios with 29 participants--baseline model reliability (S1),\nfabricated errors (S2), and cognitive bias introduced by negative framing\n(S3)--we analyzed behavioral and qualitative data. Reliable predictions in S1\nyielded high trust and annotation consistency, while unreliable outputs in S2\nled to increased critical evaluations but also heightened frustration and\nresponse variability. Negative framing in S3 revealed how cognitive bias\ninfluenced participants to perceive the model as more relatable and accurate,\ndespite misinformation regarding its reliability. These findings highlight the\nimportance of both reliable machine outputs and psychological factors in\nshaping effective human-machine collaboration. By leveraging the strengths of\nboth human oversight and automated systems, this study establishes a scalable\nHITL framework for emotion annotation and lays the foundation for broader\napplications in adaptive learning and human-computer interaction.",
        "In this work, we explore the global existence of strong solutions for a class\nof partially diffusive hyperbolic systems within the framework of critical\nhomogeneous Besov spaces. Our objective is twofold: first, to extend our recent\nfindings on the local existence presented in J.-P. Adogbo and R. Danchin. Local\nwell-posedness in the critical regularity setting for hyperbolic systems with\npartial diffusion. arXiv:2307.05981, 2024, and second, to refine and enhance\nthe analysis of Kawashima (S. Kawashima. Systems of a hyperbolic parabolic type\nwith applications to the equations of magnetohydrodynamics. PhD thesis, Kyoto\nUniversity, 1983).\n  To address the distinct behaviors of low and high frequency regimes, we\nemploy a hybrid Besov norm approach that incorporates different regularity\nexponents for each regime. This allows us to meticulously analyze the\ninteractions between these regimes, which exhibit fundamentally different\ndynamics.\n  A significant part of our methodology is based on the study of a Lyapunov\nfunctional, inspired by the work of Beauchard and Zuazua (K. Beauchard and E.\nZuazua. Large time asymptotics for partially dissipative hyperbolic system.\nArch. Rational Mech. Anal, 199:177-227, 2011.) and recent contributions (T.\nCrin-Barat and R. Danchin. Partially dissipative hyperbolic systems in the\ncritical regularity setting: the multi-dimensional case. J. Math. Pures Appl.\n(9), 165:1-41, 2022). To effectively handle the high-frequency components, we\nintroduce a parabolic mode with better smoothing properties, which plays a\ncentral role in our analysis.\n  Our results are particularly relevant for important physical systems, such as\nthe magnetohydrodynamics (MHD) system and the Navier-Stokes-Fourier equations.",
        "Complementary collaboration between humans and AI is essential for human-AI\ndecision making. One feasible approach to achieving it involves accounting for\nthe calibrated confidence levels of both AI and users. However, this process\nwould likely be made more difficult by the fact that AI confidence may\ninfluence users' self-confidence and its calibration. To explore these\ndynamics, we conducted a randomized behavioral experiment. Our results indicate\nthat in human-AI decision-making, users' self-confidence aligns with AI\nconfidence and such alignment can persist even after AI ceases to be involved.\nThis alignment then affects users' self-confidence calibration. We also found\nthe presence of real-time correctness feedback of decisions reduced the degree\nof alignment. These findings suggest that users' self-confidence is not\nindependent of AI confidence, which practitioners aiming to achieve better\nhuman-AI collaboration need to be aware of. We call for research focusing on\nthe alignment of human cognition and behavior with AI."
      ]
    }
  },
  {
    "id":2412.20007,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
    "start_abstract":"Deep learning tools have gained tremendous attention in applied machine learning. However such for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about uncertainty, but usually come with prohibitive computational cost. this paper we develop new theoretical casting dropout training deep neural networks (NNs) as approximate inference Gaussian processes. A direct result of theory gives us uncertainty NNs -- extracting information from existing that has been thrown away so far. This mitigates the problem representing without sacrificing either complexity or test accuracy. We perform an extensive study properties dropout's Various network architectures non-linearities are assessed on tasks classification, using MNIST example. show considerable improvement predictive log-likelihood RMSE compared state-of-the-art methods, finish by reinforcement",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions"
      ],
      "abstract":[
        "Training of neural networks for automated diagnosis pigmented skin lesions is hampered by the small size and lack diversity available datasets dermatoscopic images. We tackle this problem releasing HAM10000 (\"Human Against Machine with 10000 training images\") dataset. collected images from different populations acquired stored modalities. Given we had to apply acquisition cleaning methods developed semi-automatic workflows utilizing specifically trained networks. The final dataset consists 10015 which are released as a set academic machine learning purposes publicly through ISIC archive. This benchmark can be used comparisons human experts. Cases include representative collection all important diagnostic categories in realm lesions. More than 50% have been confirmed pathology, while ground truth rest cases was either follow-up, expert consensus, or confirmation in-vivo confocal microscopy."
      ],
      "categories":[
        "Data"
      ]
    },
    "list":{
      "title":[
        "On the origin of heating-induced stiffening and enthalpic reinforcement\n  in elastomeric nanocomposites",
        "General Classification, Invariance and Conservation Laws Analyses of\n  Nonlinear Fourth Order Wave and Nerve Membrane Equations with Dissipation",
        "Energy density and stress fields in quantum systems",
        "Beam splitters as controlled-Z gate for hybrid state",
        "On the zero capillarity limit for the Euler-Korteweg system",
        "Combinatorial construction of symplectic 6-manifolds via bifibration\n  structures",
        "Fiber-based mid-infrared frequency-swept laser at 50 MScans\/s via\n  frequency down-conversion of time-stretched pulses",
        "Geometric Flavours of Quantum Field Theory on a Cauchy Hypersurface:\n  Gaussian Analysis for the Hamiltonian Formalism and Applications to Cosmology",
        "Asymptotic Freedom in Parton Language: the Birth of Perturbative QCD",
        "From Target Tracking to Targeting Track -- Part III: Stochastic Process\n  Modeling and Online Learning",
        "Contextual Similarity Distillation: Ensemble Uncertainties with a Single\n  Model",
        "High-throughput computational screening of Heusler compounds with phonon\n  considerations for enhanced material discovery",
        "The self-interaction effects on the Kerr black hole superradiance and\n  their observational implications",
        "Constitutive Kolmogorov-Arnold Networks (CKANs): Combining Accuracy and\n  Interpretability in Data-Driven Material Modeling",
        "Dynamic Imprints of Colliding-wind Dust Formation from WR140",
        "Spectral gaps on thick part of moduli spaces",
        "Rewinding the byte trail of the White Whale",
        "Emergence of Fermi's Golden Rule in the Probing of a Quantum Many-Body\n  System",
        "Theory of Quantum-Enhanced Stimulated Raman Scattering",
        "Density fluctuation in the solar corona and solar wind: A comparative\n  analysis of radio-occultation observations and magnetohydrodynamic simulation",
        "A Milli-Kelvin Atomic Force Microscope Made of Glass",
        "Atomistic insights into solid solution strengthening: size misfit versus\n  stiffness misfit",
        "Enhancement of Electric Drive in Silicon Quantum Dots with Electric\n  Quadrupole Spin Resonance",
        "A strong-coupling effective-field theory for asymmetrically charged\n  plates with counterions only",
        "Skyrmions in Nanotechnology: Fundamental Properties, Experimental\n  Advances, and Emerging Applications",
        "Bayesian Selection for Efficient MLIP Dataset Selection",
        "Mapping strain and structural heterogeneities around bubbles in\n  amorphous ionically conductive Bi$_2$O$_3$",
        "Properties of Turbulent Convection and Large-Scale Flows in a Rotating\n  F-type Star Revealed by 3D Realistic Radiative Hydrodynamic Simulations",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets"
      ],
      "abstract":[
        "Despite a century of use, the mechanism of nanoparticle-driven mechanical\nreinforcement of elastomers is unresolved. A major hypothesis attributes it to\nglassy interparticle bridges, supported by an observed inversion of the\nvariation of the modulus E(T) on heating -- from entropic stiffening in\nelastomers to enthalpic softening in nanocomposites. Here, molecular\nsimulations reveal that this instead arises from a bulk-modulus mediated\ncompetition between elastomer and nanoparticulate networks over preferred\nnonequilibrium densities under deformation. A theory accounting for softening\nof the bulk modulus on heating quantitatively predicts the simulated E(T)\ninversion, suggesting that reinforcement is driven by a volume-competition\nmechanism unique to co-continuous systems of soft and rigid networks.",
        "We study the nonlinear wave equation for arbitrary function with fourth order\ndissipation. A special case that is analysed exclusively is the model of nerve\nmembranes; we consider this model, both, in the presence and absence of the\nfourth order dissipation. The equivalence transformations, Lie symmetries and a\ncomplete classification is presented. We also discuss the one dimensional\noptimal system in each case obtained via classification. The reduction of the\npartial differential equations (PDEs) is carried out and the forms of invariant\nsolutions are presented. The study also include the construction of\nconservation laws using the direct method. The invariant solutions and some\nspecial type of solutions including solitions are presented with their\ngraphical illustrations.",
        "There has been an enduring interest and controversy about whether or not one\ncan define physically meaningful energy density and stress fields, $e({\\bf r})$\nand $\\sigma_{\\alpha \\beta}({\\bf r})$ in quantum systems. A key issue is kinetic\nenergy since $\\frac{1}{2}|\\nabla \\Psi|^2$ and $-\\frac{1}{2}\\Psi\\nabla^2 \\Psi$\nlead to different densities, and analogous issues arise interaction energies.\nThis paper presents a resolution to the problems in two steps: 1) All effects\nof exchange and correlation are shown to be unique functions defined at each\npoint ${\\bf r}$; all issues of non-uniqueness involve only the density $n({\\bf\nr})$ and are equivalent to a single-particle problem with wavefunction $s({\\bf\nr}) = \\sqrt{n({\\bf r})\/N}$. 2) Forms for the latter terms are proposed based on\nthe nature of energy and stress, where energy has two distinct roles. Because\nthe energy determines the ground state itself through the variational\nprinciple, the appropriate density involves the terms in the hamiltonian:\n$-\\frac{1}{2}\\Psi\\nabla^2 \\Psi$ and interactions in terms of potentials acting\non the particles. This leads naturally to density functional theory with the\ninterpretation that the energy density $e({\\bf r})$ is equilibrated to minimize\nfluctuations with the same chemical potential throughout the system. On the\nother hand, the energy and stress (derivative of the energy with respect to\nstrain) are properties of the ground state, and simple examples to show that\nthe only acceptable expressions involve the combination $\\frac{1}{4}[|\\nabla\n\\Psi|^2 - \\Psi\\nabla^2 \\Psi]$, as derived by Schrodinger, Pauli and others, and\nCoulomb interactions in the Maxwell form in terms of electric fields, not\npotentials. Together these results lead to well-defined formulations of energy\ndensity and stress fields that are physically motivated and based on a clear\nset of arguments.",
        "We explore a scheme based on adding a nonlocal photon and subtracting some\nnumber of photons to entangle the initial single-mode squeezed vacuum (SMSV)\nstate with the photon state. In a realistic model of interaction of the SMSV\nstate with the photonic state on a beam splitter (BS) with changeable\ntransmissivity or reflectivity the hybrid entanglement is realized for any\nvalues of the squeezing of input SMSV state. Maximum hybrid entanglement is\nachieved at certain values of the squeezing and BS parameter, which can mean\nimplementation of a two-qubit controlled-Z (CZ-) operation using the BS with\nthe appropriate initialization of the input states. The success probability of\nthe gate, taking into account multiphoton outcomes in the measuring mode of the\nBS, is more than 0.3. We also propose to use new continuous variable (CV)\nstates of definite parity that could increase the success probability of\ngenerating maximal hybrid entanglement. We show sufficient robustness of the\ngenerated entanglement under photon number resolving detection with practical\nquantum efficiency.",
        "We study the Euler-Korteweg equations with a weak capillarity tensor. It\nformally converges to the Euler equations in the zero capillarity limit. Our\naim is two-fold : first we prove rigorously this limit in R d , d $\\ge$ 1, and\nobtain a more precise BKW expansion of the solution, second we initiate the\nstudy of the problem on the half space. In this case we obtain a priori\nestimates for the solutions that degenerate as the capillary coefficient\nconverges to zero, and we explain this degeneracy with the construction of a\n(formal) BKW expansion that exhibits boundary layers. The results on the full\nspace extend and improve a classical result of Grenier (1998) on the\nsemi-classical limit of nonlinear Schr{\\\"o}dinger equations. The analysis on\nthe half space is restricted to the case of quantum fluids with irrotational\nvelocity.",
        "A bifibration structure on a $6$-manifold is a map to either the complex\nprojective plane $\\mathbb{P}^2$ or a $\\mathbb{P}^1$-bundle over $\\mathbb{P}^1$,\nsuch that its composition with the projection to $\\mathbb{P}^1$ is a\n($6$-dimensional) Lefschetz fibration\/pencil, and its restriction to the\npreimage of a generic $\\mathbb{P}^1$-fiber is also a ($4$-dimensional)\nLefschetz fibration\/pencil. This object has been studied by Auroux, Katzarkov,\nSeidel, among others. From a pair consisting of a monodromy representation of a\nLefschetz fibration\/pencil on a $4$-manifold and a relation in a braid group,\nwhich are mutually compatible in an appropriate sense, we construct a\nbifibration structure on a closed symplectic $6$-manifold, producing the given\ncompatible pair as its monodromies. We further establish methods for computing\ntopological invariants of symplectic $6$-manifolds, including Chern numbers,\nfrom compatible pairs. Additionally, we provide an explicit example of a\ncompatible pair, conjectured to correspond to a bifibration structure derived\nfrom the degree-$2$ Veronese embedding of the $3$-dimensional complex\nprojective space. This example can be viewed as a higher-dimensional analogue\nof the lantern relation in the mapping class group of the four-punctured\nsphere. Our results not only extend the applicability of combinatorial\ntechniques to higher-dimensional symplectic geometry but also offer a unified\nframework for systematically exploring symplectic $6$-manifolds.",
        "Increasing the sweep rate of mid-infrared (MIR) frequency-swept sources\noffers significant potential for various high-speed spectroscopy-based\napplications. While continuous-wave frequency-swept lasers have achieved sweep\nrates up to 1 MHz, a recently demonstrated time-stretched ultrashort pulsed\nlaser has reached a significantly higher sweep rate, up to tens of MHz.\nHowever, the previous system relied on a bulky femtosecond optical parametric\noscillator and produced only ~30 discrete spectral elements due to the use of a\nfree-space time stretcher. In this work, we present a fiber-based\nfrequency-swept MIR source that utilizes the frequency down-conversion of\ntime-stretched near-infrared pulses, employing a compact mode-locked fiber\nlaser and telecommunication fiber. As a proof-of-concept demonstration, we\nperformed MIR spectroscopy of methane gas around 3.4 um at a rate of 50\nMSpectra\/s, capturing 220 spectral elements over a range of 19.0 cm-1. This\ncompact and robust high-speed MIR frequency-swept laser system holds the\npotential for deployment in field applications.",
        "This thesis explores Quantum Field Theory (QFT) on curved spacetimes using a\ngeometric Hamiltonian approach to the Schr\\\"odinger-like representation. In\nparticular it studies the theory of the scalar field described through its\nconfigurations over a Cauchy hypersurface. It is focused on mathematical\nconsistency based on analytic and geometric tools.\n  The mathematical aspects of Gaussian integration theory in\ninfinite-dimensional Topological Vector Spaces (TVS) are thoroughly reviewed.\nIt also reviews the complex and holomorphic versions of important results and\nconcepts of Gaussian integration. For example, the Wiener-It\\^o decomposition\ntheorem or the definition of Hida test functions.\n  The physical framework builds upon three interconnected levels: classical\nGeneral Relativity (GR), Classical Statistical Field Theory (CSFT), and QFT.\nThe work begins by extending the Koopman-van Hove (KvH) formalism of classical\nstatistical mechanics to CSFT. This description is based upon prequantization\ntheory. It reveals features inherent to both CSFT and QFT, that help delineate\nthe genuine quantum features of a theory.\n  Upon the prequantum program, the QFT of the scalar field is built mixing\nGeometric Quantization with the choice of Wick and Weyl orderings. Various\nquantum representations are introduced: the holomorphic, Schr\\\"odinger,\nfield-momentum, and antiholomorphic. The relation among them is studied using\nintegral transforms, including novel infinite-dimensional Fourier transforms.\nFrom a geometrical analysis, it is argued that a covariant time derivative that\nmodifies the evolution equations should be added to the Schr\\\"odinger equation.\nThis connection is unique and required by the geometrodynamical description\nofthe coupling of QFT and GR.\n  Finally, studying the free model on cosmological spacetimes it obtains\nparticle creation effects on a dynamical equation.",
        "I review the contributions of Giorgio Parisi to perturbative QCD.\nConcentrated in a decade, they mark the transition of the theory of strong\ninteractions from a set of loosely connected ideas based on models, to a\nquantum field theory that is now an integral part of the standard model of\nfundamental interactions. Parisi's contributions have established at a very\nearly stage ideas, methods and tools that are now standard, and in several\ncases anticipated results that only became prominent in the XXIst century.",
        "This is the third part of a series of studies that model the target\ntrajectory, which describes the target state evolution over continuous time, as\na sample path of a stochastic process (SP). By adopting a\ndeterministic-stochastic decomposition framework, we decompose the learning of\nthe trajectory SP into two sequential stages: the first fits the deterministic\ntrend of the trajectory using a curve function of time, while the second\nestimates the residual stochastic component through parametric learning of\neither a Gaussian process (GP) or Student's-$t$ process (StP). This leads to a\nMarkov-free data-driven tracking approach that produces the continuous-time\ntrajectory with minimal prior knowledge of the target dynamics. Notably, our\napproach explicitly models both the temporal correlations of the state sequence\nand of measurement noises through the SP framework. It does not only take\nadvantage of the smooth trend of the target but also makes use of the long-term\ntemporal correlation of both the data noise and the model fitting error.\nSimulations in four maneuvering target tracking scenarios have demonstrated its\neffectiveness and superiority in comparison with existing approaches.",
        "Uncertainty quantification is a critical aspect of reinforcement learning and\ndeep learning, with numerous applications ranging from efficient exploration\nand stable offline reinforcement learning to outlier detection in medical\ndiagnostics. The scale of modern neural networks, however, complicates the use\nof many theoretically well-motivated approaches such as full Bayesian\ninference. Approximate methods like deep ensembles can provide reliable\nuncertainty estimates but still remain computationally expensive. In this work,\nwe propose contextual similarity distillation, a novel approach that explicitly\nestimates the variance of an ensemble of deep neural networks with a single\nmodel, without ever learning or evaluating such an ensemble in the first place.\nOur method builds on the predictable learning dynamics of wide neural networks,\ngoverned by the neural tangent kernel, to derive an efficient approximation of\nthe predictive variance of an infinite ensemble. Specifically, we reinterpret\nthe computation of ensemble variance as a supervised regression problem with\nkernel similarities as regression targets. The resulting model can estimate\npredictive variance at inference time with a single forward pass, and can make\nuse of unlabeled target-domain data or data augmentations to refine its\nuncertainty estimates. We empirically validate our method across a variety of\nout-of-distribution detection benchmarks and sparse-reward reinforcement\nlearning environments. We find that our single-model method performs\ncompetitively and sometimes superior to ensemble-based baselines and serves as\na reliable signal for efficient exploration. These results, we believe,\nposition contextual similarity distillation as a principled and scalable\nalternative for uncertainty quantification in reinforcement learning and\ngeneral deep learning.",
        "High-throughput (HTP) $ab$ $initio$ calculations are performed on 27,865\nHeusler compositions, covering a broad range of regular, inverse, and\nhalf-Heusler compounds in both cubic and tetragonal phases. In addition to\nconventional stability metrics, such as formation energy, Hull distance, and\nmagnetic critical temperature $T_{\\mathrm{c}}$, phonon stability is assessed by\nsystematically conducting $ab$ $initio$ phonon calculations for over 8,000\ncompounds. The performance of $ab$ $initio$ stability criteria is\nsystematically assessed against 189 experimentally synthesized compounds, and\nmagnetic critical temperature calculations are validated using 59 experimental\ndata points. As a result, we identify 631 stable compounds as promising\ncandidates for further functional material exploration. Notably, 47 low-moment\nferrimagnets are identified, with their spin polarization and anomalous\nHall\/Nernst conductivity calculated to provide insights into potential\napplications in spintronics and energy harvesting. Furthermore, our analyses\nreveal linear relationship between $T_{\\mathrm{c}}$ and magnetization in 14\nsystems and correlations between stability and atomic properties such as atomic\nradius and ionization energy. The regular\/inverse structures preference in\n$X_2YZ$ compound and tetragonal distortion are also investigated for a broad\nHeusler family.",
        "Through the black hole (BH) superradiance, ultralight bosons can form dense\nclouds around rotating Kerr BHs. Certain ultralight bosons, such as axions and\naxion-like particles (promising dark matter candidates), naturally possess\nself-interactions, and thus may significantly modify the dynamics of the\nsuperradiance process. Previous studies on the detection or constraint of\nultralight bosons through superradiance have largely neglected the\nself-interaction effects of bosons. In this work, we investigate the formation\nand evolution of self-interacting boson clouds in the full Kerr spacetime\nduring BH superradiance. Using numerical methods, we compute the superradiant\ngrowth rate of boson clouds with self-interactions around Kerr BHs and\nquantitatively evaluate how the self-interaction strength of scalar bosons\naffects the growth rate. We also assess the evolution of the BH's mass and\nspin. Our results reveal that, in addition to the superradiance-imposed upper\nbound on the boson cloud mass, self-interactions of ultralight bosons introduce\na new, lower critical mass limit, beyond which the growth rate of the boson\ncloud approaches zero. This implies that the superradiance process terminates\nearlier when self-interactions are considered. Furthermore, we explore how\nself-interactions affect both the oscillation frequency of boson clouds in\ngravitational atoms and the frequency of gravitational wave (GW) emitted\nthrough cloud annihilation. The anticipated frequency shift could be detectable\nby the GW observatories. Given that self-interactions substantially alter the\nevolution of BH superradiance, their effects can significantly relax existing\nconstraints on scalar boson models derived from superradiance. Taking the spin\nmeasurements from GW190412 and GW190517 as examples, we discuss the impact of\nself-interactions on constraint results in details.",
        "Hybrid constitutive modeling integrates two complementary approaches for\ndescribing and predicting a material's mechanical behavior: purely data-driven\nblack-box methods and physically constrained, theory-based models. While\nblack-box methods offer high accuracy, they often lack interpretability and\nextrapolability. Conversely, physics-based models provide theoretical insight\nand generalizability but may not capture complex behaviors with the same\naccuracy. Traditionally, hybrid modeling has required a trade-off between these\naspects. In this paper, we show how recent advances in symbolic machine\nlearning, specifically Kolmogorov-Arnold Networks (KANs), help to overcome this\nlimitation. We introduce Constitutive Kolmogorov-Arnold Networks (CKANs) as a\nnew class of hybrid constitutive models. By incorporating a post-processing\nsymbolification step, CKANs combine the predictive accuracy of data-driven\nmodels with the interpretability and extrapolation capabilities of symbolic\nexpressions, bridging the gap between machine learning and physical modeling.",
        "Carbon-rich Wolf-Rayet binaries are a prominent source of carbonaceous dust\nthat contribute to the dust budget of galaxies. The \"textbook\" example of an\nepisodic dust producing WR binary, WR140 (HD193793), provides us with an ideal\nlaboratory for investigating the dust physics and kinematics in an extreme\nenvironment. This study is among the first to utilize two separate JWST\nobservations, from Cycle 1 ERS (July 2022) and Cycle 2 (Sept. 2023), to measure\nWR140's dust kinematics and confirm its morphology. To measure the proper\nmotions and projected velocities of the dust shells, we performed a novel PSF\nsubtraction to reduce the effects of the bright diffraction spikes and\ncarefully aligned the Cycle 2 to the Cycle 1 images. At 7.7 $\\mu$m, through the\nbright feature common to 16 dust shells (C1), we find an average dust shell\nproper motion of $390\\pm29$ mas yr$^{-1}$, which equates to a projected\nvelocity of $2714\\pm188$ km s$^{-1}$ at a distance of 1.64 kpc. Our measured\nspeeds are constant across all visible shells and consistent with previously\nreported dust expansion velocities. Our observations not only prove that these\ndusty shells are astrophysical (i.e., not associated with any PSF artifact) and\noriginate from WR140, but also confirm the \"clumpy\" morphology of the dust\nshells, in which identifiable substructures within certain shells persist for\nat least 14 months from one cycle to the next. These results support the\nhypothesis that clumping in the wind collision region is required for dust\nproduction in WR binaries.",
        "In this paper, we study spectral gaps of closed hyperbolic surfaces for large\ngenus. We show that for any fixed $k\\geq 1$, as the genus goes to infinity, the\nmaximum of $\\lambda_k-\\lambda_{k-1}$ over any thick part of the moduli space of\nclosed Riemann surfaces approaches the limit $\\frac{1}{4}$.",
        "Motivated by a popular code golf challenge, we review some key ideas from\ninformation theory and discuss how to efficiently compress a streaming file\nwith an acceptable error rate.",
        "Fermi's Golden Rule (FGR) is one of the most impactful formulas in quantum\nmechanics, providing a link between easy-to-measure observables - such as\ntransition rates - and fundamental microscopic properties - such as density of\nstates or spectral functions. Its validity relies on three key assumptions: the\nexistence of a continuum, an appropriate time window, and a weak coupling.\nUnderstanding the regime of validity of FGR is critical for the proper\ninterpretation of most spectroscopic experiments. While the assumptions\nunderlying FGR are straightforward to analyze in simple models, their\napplicability is significantly more complex in quantum many-body systems. Here,\nwe observe the emergence and breakdown of FGR, using a strongly interacting\nhomogeneous spin-$1\/2$ Fermi gas coupled to a radio-frequency (rf) field.\nMeasuring the transition probability into an outcoupled internal state, we map\nthe system's dynamical response diagram versus the rf-pulse duration $t$ and\nRabi frequency $\\Omega_0$. For weak drives, we identify three regimes: an\nearly-time regime where the transition probability takes off as $t^2$, an\nintermediate-time FGR regime, and a long-time non-perturbative regime. Beyond a\nthreshold Rabi frequency, Rabi oscillations appear. Our results provide a\nblueprint for the applicability of linear response theory to the spectroscopy\nof quantum many-body systems.",
        "Stimulated Raman scattering (SRS) is a powerful method for label-free imaging\nand spectroscopy of materials. Recent experiments have shown that\nquantum-enhanced Raman scattering can surpass the shot noise limit and improve\nthe sensitivity substantially. Here, we introduce a full theory of\nquantum-enhanced SRS based on the framework of quantum metrology. Our results\nenable the assessment of quantum-enhancements of arbitrary measurement\nstrategies and identify optimal measurement observables that extract maximal\ninformation about the signal. We use this to identify the optimal employment of\nsqueezed states in SRS, highlighting the potential to improve quantum gains\nbeyond those observed in recent experiments. Our work establishes the\ntheoretical foundation for understanding and approaching the quantum limits of\nprecision in SRS, and provide a tool to discuss nonlinear spectroscopy and\nimaging more broadly.",
        "Recent in-situ observations and numerical models indicated various types of\nmagnetohydrodynamic (MHD) waves contributing to the solar wind acceleration.\nAmong them is an MHD wave decomposition at distances closer than 50 $R_{\\odot}$\nusing data taken by the first perihelion pass of Parker Solar Probe (PSP).\nHowever, the underlying physical processes responsible for the formation of the\nsolar wind have not yet been observationally confirmed at distances closer than\n10 $R_{\\odot}$. We aim to infer the mode population of density fluctuations\nobserved by radio occultation, which has all been attributed to slow\nmagnetoacoustic waves. We compare the radio occultation observations conducted\nin 2016 using the JAXA's Venus orbiter Akatsuki with the MHD simulation. The\ntime-frequency analysis was applied to the density fluctuations observed by the\nradio occultation and those reproduced in the MHD model. The time-spatial\nspectrum of the density fluctuation in the model exhibits two components that\nare considered to be fast and slow magnetoacoustic waves. The fast\nmagnetoacoustic waves in the model tend to have periods shorter than the slow\nmagnetoacoustic waves, and the superposition of these modes has a broadened\nspectrum extending in the range of approximately 20$-$1000 s, which resembles\nthat of the observed waves. Based on this comparison, it is probable that the\ndensity oscillations observed by radio occultation include fast and slow\nmagnetoacoustic waves, and that fast magnetoacoustic waves are predominant at\nshort periods and slow magnetoacoustic waves are prevalent at long periods.\nThis is qualitatively similar to the results of the mode decomposition obtained\nfrom the PSP's first perihelion at more distance regions.",
        "Milli-Kelvin atomic force microscopy (mK-AFM) presents an ongoing\nexperimental challenge due to the intense vibrations in a cryogen-free dilution\nrefrigerator and the low cooling power available at mK temperatures. A viable\napproach is to make the system exceptionally rigid and thermally insulating to\ndecouple external vibrations and isolate heat dissipation from the piezo\nelements. Here, we present a low-cost and large scan-range mK-AFM that operates\nbelow 100 mK. All the essential parts of our mK-AFM, including the scanners,\ntip assembly, and microscope body, are custom-made of fused silica glass by\ntaking advantage of its high specific modulus, extremely low thermal expansion\ncoefficient, and excellent thermal insulation properties. We carefully balance\nthe scan range (25 ${\\mu}$m $\\times$ 25 ${\\mu}$m), heat dissipation, and\nstiffness of the system to reach optimal performance at mK temperatures.",
        "Used for centuries to enhance mechanical properties of materials, solid\nsolution strengthening (SSS) is a classical metallurgical method in which small\namounts of impurity elements are added to a base metal. Developed for dilute\nalloys, classical theories of SSS are presently challenged by the ongoing\nexplosive development of complex concentrated alloys (CCA) in which all\ncomponent elements are present in nearly equal fractions. Here we develop a\nmethod of computational alchemy in which interatomic interactions are modified\nto continuously and systematically vary two key parameters defining SSS, atomic\nsize misfit and elastic stiffness misfit, over a maximally wide range of misfit\nvalues. The resulting model alloys are subjected to massive Molecular Dynamics\nsimulations reproducing full complexity of plastic strength response in\nconcentrated single-phase body-centered cubic solid solutions. At variance with\nviews prevailing in the literature, our computational experiments show that\nstiffness misfit can contribute to SSS on par if not more than size misfit.\nFurthermore, depending on exactly how they are combined, the two misfits can\nresult in synergistic or antagonistic effect on alloy strengthening. In\ncontrast to real CCAs in which every constituent element comes with its\nspecific combination of atomic size and elastic stiffness, our alchemical model\nalloys sample the space of misfit parameters continuously thus augmenting the\nmuch more constrained and inevitably spotty experimental exploration of the CCA\ndesign space. Taking advantage of unique to our approach ability to define\nalloy misfit parameters, our computational study demonstrates how useful\ninsights can be gained from intentionally unrealistic alchemical models. Rather\nthan practical recommendation for alloy design, our computational experiments\nshould be regarded as a proving ground for further SSS theory development.",
        "Quantum computation with electron spin qubits requires coherent and efficient\nmanipulation of these spins, typically accomplished through the application of\nalternating magnetic or electric fields for electron spin resonance (ESR). In\nparticular, electrical driving allows us to apply localized fields on the\nelectrons, which benefits scale-up architectures. However, we have found that\nElectric Dipole Spin Resonance (EDSR) is insufficient for modeling the Rabi\nbehavior in recent experimental studies. Therefore, we propose that the\nelectron spin is being driven by a new method of electric spin qubit control\nwhich generalizes the spin dynamics by taking into account a quadrupolar\ncontribution of the quantum dot: electric quadrupole spin resonance (EQSR). In\nthis work, we explore the electric quadrupole driving of a quantum dot in\nsilicon, specifically examining the cases of 5 and 13 electron occupancies.",
        "We are interested in rationalizing the phenomenon of like-charge attraction\nbetween charged bodies, such as a pair of colloids, in the strong coupling\nregime. The two colloids are modelled as uniformly charged parallel plates,\nneutralized by mobile counterions. In an earlier work [Palaia et al., J. Phys.\nChem. B 126, 3143 (2022)], we developed an effective-field theory for symmetric\nplates, stemming from the ground-state description that holds at infinite\ncouplings. Here, we generalize the approach to the asymmetric case, where the\nplates bear charges of the same sign, but of different values. In the symmetric\nsituation, the mobile ions, which are localized in the vicinity of the two\nplates, share equally between both of them. Here, the sharing is non-trivial,\ndepending both on the coupling parameter and the distance between the plates.\nWe thus introduce a counterion occupation parameter, that is determined\nvariationally to ensure minimum of the free energy. The resulting analytical\nresults for the pressure as a function of the plate-plate distance $d$ agree\nwell with our Monte Carlo data, in a large interval of strong and intermediate\ncoupling constants $\\Xi$. We show in particular that within this description,\nthere exists a range of large distances at which the attractive pressure\nfeatures a $1\/d^2$ behavior.",
        "Skyrmions, topologically protected textures, have been observed in different\nfields of nanotechnology and have emerged as promising candidates for different\napplications due to their topological stability, low-power operation, and\ndynamic response to external stimuli. First introduced in particle physics,\nskyrmions have since been observed in different condensed matter fields,\nincluding magnetism, ferroelectricity, photonics, and acoustics. Their unique\ntopological properties enable robust manipulation and detection, paving the way\nfor innovative applications in room temperature sensing, storage, and\ncomputing. Recent advances in materials engineering and device integration have\ndemonstrated several strategies for an efficient manipulation of skyrmions,\naddressing key challenges in their practical implementation. In this review, we\nsummarize the state-of-the-art research on skyrmions across different\nplatforms, highlighting their fundamental properties and characteristics,\nrecent experimental breakthroughs, and technological potential. We present\nfuture perspectives and remaining challenges, emphasizing the interdisciplinary\nimpact of skyrmions on nanotechnology.",
        "The problem of constructing a dataset for MLIP development which gives the\nmaximum quality in the minimum amount of compute time is complex, and can be\napproached in a number of ways. We introduce a ``Bayesian selection\" approach\nfor selecting from a candidate set of structures, and compare the effectiveness\nof this method against other common approaches in the task of constructing\nideal datasets targeting Silicon surface energies. We show that the Bayesian\nselection method performs much better than Simple Random Sampling at this task\n(for example, the error on the (100) surface energy is 4.3x lower in the low\ndata regime), and is competitive with a variety of existing selection methods,\nusing ACE and MACE features.",
        "While amorphous materials are often approximated to have a statistically\nhomogeneous atomic structure, they frequently exhibit localized structural\nheterogeneity that challenges simplified models. This study uses 4D scanning\ntransmission electron microscopy to investigate the strain and structural\nmodifications around gas bubbles in amorphous Bi$_2$O$_3$ induced by argon\nirradiation. We present a method for determining strain fields surrounding\nbubbles that can be used to measure the internal pressure of the gas.\nCompressive strain is observed around the cavities, with higher-order\ncrystalline symmetries emerging near the cavity interfaces, suggesting\nparacrystalline ordering as a result of bubble coarsening. This ordering, along\nwith a compressive strain gradient, indicates that gas bubbles induce\nsignificant localized changes in atomic packing. By analyzing strain fields\nwith maximum compressive strains of 3\\%, we estimate a lower bound on the\ninternal pressure of the bubbles at 2.5 GPa. These findings provide insight\ninto the complex structural behavior of amorphous materials under stress,\nparticularly in systems with gas inclusions, and offer new methods for probing\nthe local atomic structure in disordered materials. Although considering\nstructural heterogeneity in amorphous systems is non-trivial, these features\nhave crucial impacts on material functionalities, such as mechanical strength,\nionic conductivity, and electronic mobility.",
        "The nonlinear coupling between stellar convection and rotation is of great\ninterest because it relates to understanding both stellar evolution and\nactivity. We investigated the influence of rotation and the Coriolis force on\nthe dynamics and thermodynamic structure of an F-type main-sequence star with a\nshallow outer convection zone. We performed a series of 3D radiative\nhydrodynamic simulations of a 1.47Msun star for different rotation rates\n(periods of rotation 1 and 14 days) and with computational domains placed at\nlatitudes of 0degrees (equator), 30degrees, and 60degrees. Because the star has\na relatively shallow convection zone (28.5 Mm thick or about 2.81% R*), we\nmodel its dynamics from the upper layers of the radiative zone, the whole\nconvection zone, and the low atmosphere. The simulation results show a weak\nshift of the ionization zones to the photosphere and a decrease of the stellar\nradius by about 29 km at the equator and about 58 km at higher latitudes in the\npresence of rotation with a period of 1 day. The models presented reveal the\nformation of radial differential rotation, meridional flows, latitude-dependent\nroll-like structures of convection, a tachocline, the presence of a\ngravity-darkening effect, and others. In this paper, we primarily discuss the\nproperties of the outer convection zone for different rotation rates. Detailed\nanalysis of the properties of the tachocline, the overshoot layer, and\nsmall-scale turbulence will be discussed in follow-on papers.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs."
      ]
    }
  },
  {
    "id":2412.20007,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",
    "start_abstract":"Training of neural networks for automated diagnosis pigmented skin lesions is hampered by the small size and lack diversity available datasets dermatoscopic images. We tackle this problem releasing HAM10000 (\"Human Against Machine with 10000 training images\") dataset. collected images from different populations acquired stored modalities. Given we had to apply acquisition cleaning methods developed semi-automatic workflows utilizing specifically trained networks. The final dataset consists 10015 which are released as a set academic machine learning purposes publicly through ISIC archive. This benchmark can be used comparisons human experts. Cases include representative collection all important diagnostic categories in realm lesions. More than 50% have been confirmed pathology, while ground truth rest cases was either follow-up, expert consensus, or confirmation in-vivo confocal microscopy.",
    "start_categories":[
      "Data"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      ],
      "abstract":[
        "Deep learning tools have gained tremendous attention in applied machine learning. However such for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about uncertainty, but usually come with prohibitive computational cost. this paper we develop new theoretical casting dropout training deep neural networks (NNs) as approximate inference Gaussian processes. A direct result of theory gives us uncertainty NNs -- extracting information from existing that has been thrown away so far. This mitigates the problem representing without sacrificing either complexity or test accuracy. We perform an extensive study properties dropout's Various network architectures non-linearities are assessed on tasks classification, using MNIST example. show considerable improvement predictive log-likelihood RMSE compared state-of-the-art methods, finish by reinforcement"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "$U(1)_A$ Breaking in Hot QCD in the Chiral Limit",
        "An approach for improving the distorted structured light in holographic\n  optical tweezers",
        "ChaosEater: Fully Automating Chaos Engineering with Large Language\n  Models",
        "Cute-Lock: Behavioral and Structural Multi-Key Logic Locking Using Time\n  Base Keys",
        "Exo-MerCat v2.0.0: updates and open-source release of the Exoplanet\n  Merged Catalog software",
        "CMB-S4: Foreground-Cleaning Pipeline Comparison for Measuring Primordial\n  Gravitational Waves",
        "DobLIX: A Dual-Objective Learned Index for Log-Structured Merge Trees",
        "Random quantum Ising model with three-spin couplings",
        "Online Housing Market",
        "Global existence of weak solutions to a cell migration and\n  (de)differentiation model with double haptotaxis in the context of tissue\n  regeneration",
        "Electronic structures and multi-orbital models of La$_3$Ni$_2$O$_7$ thin\n  films",
        "Amortized In-Context Bayesian Posterior Estimation",
        "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "Uncovering the Hidden Threat of Text Watermarking from Users with\n  Cross-Lingual Knowledge",
        "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a\n  Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
        "Bayesian Multifractal Image Segmentation",
        "Momentum tunnelling between nanoscale liquid flows",
        "Weihrauch problems as containers",
        "Homological stability for Hurwitz spaces and applications",
        "XTS mode revisited: high hopes for key scopes?",
        "Modality-Composable Diffusion Policy via Inference-Time\n  Distribution-level Composition",
        "Inverse Flow and Consistency Models",
        "Probing Spin-2 Ultralight Dark Matter with Space-based Gravitational\n  Wave Detectors in Millihertz",
        "Joint graphical model estimation using Stein-type shrinkage for fast\n  large scale network inference in scRNAseq data",
        "Predicting Air Temperature from Volumetric Urban Morphology with Machine\n  Learning",
        "Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction\n  in open-plan offices",
        "RG-Attn: Radian Glue Attention for Multi-modality Multi-agent\n  Cooperative Perception",
        "Private Federated Learning In Real World Application -- A Case Study"
      ],
      "abstract":[
        "We propose a simple instanton-based random matrix model of hot QCD that in\nthe quenched case precisely reproduces the distribution of the lowest lattice\noverlap Dirac eigenvalues. Even after including dynamical quarks the model can\nbe easily simulated in volumes and for quark masses that will be out of reach\nfor direct lattice simulations in the foreseeable future. Our simulations show\nthat quantities connected to the $U(1)_A$ and $SU(N_f)_A$ chiral symmetry are\ndominated by eigenvalues in a peak of the spectral density that becomes\nsingular at zero in the thermodynamic limit. This spectral peak turns out to be\nproduced by an ideal instanton gas. By generalizing Banks-Casher type integrals\nfor the singular spectral density, definite predictions can be given for\nphysical quantities that are essential to test chiral symmetry breaking, but\npresently impossible to compute reliably with direct lattice simulations.",
        "Optical tweezers have been widely used for optical manipulation of various\nparticles. At present, there are different type of optical tweezers. Among\nthem, holographic optical tweezers have attracted growing attention as a\npowerful tools for optical trapping, optical transportation and optical sorting\nin many fields, due to its excellent properties including great flexibility and\nhigh convenience. Experimentally, however, the structured light has been easily\ndistorted, which would lead to serious degradation of optical manipulation\nperformance. In this work, the distortion of structured light is theoretically\nanalyzed. In the following, the distortion of structured light are numerically\nsimulated and experimentally measured. It shows that the simulated results are\nin consistent with the experimental ones. Then, an approach for decreasing its\noptical distortion is proposed, and the results reveal that the distortion of\nstructured light can be effectively corrected. Accordingly, our study provides\na way for improving the distorted structured light, which is useful for\noptically manipulating various particles in optical tweezers.",
        "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools realize the automated execution of predefined\nCE experiments. However, defining these experiments and reconfiguring the\nsystem after the experiments still remain manual. To reduce the costs of the\nmanual operations, we propose \\textsc{ChaosEater}, a \\textit{system} for\nautomating the entire CE operations with Large Language Models (LLMs). It\npre-defines the general flow according to the systematic CE cycle and assigns\nsubdivided operations within the flow to LLMs. We assume systems based on\nInfrastructure as Code (IaC), wherein the system configurations and artificial\nfailures are managed through code. Hence, the LLMs' operations in our\n\\textit{system} correspond to software engineering tasks, including requirement\ndefinition, code generation and debugging, and testing. We validate our\n\\textit{system} through case studies on both small and large systems. The\nresults demonstrate that our \\textit{system} significantly reduces both time\nand monetary costs while completing reasonable single CE cycles.",
        "The outsourcing of semiconductor manufacturing raises security risks, such as\npiracy and overproduction of hardware intellectual property. To overcome this\nchallenge, logic locking has emerged to lock a given circuit using additional\nkey bits. While single-key logic locking approaches have demonstrated serious\nvulnerability to a wide range of attacks, multi-key solutions, if carefully\ndesigned, can provide a reliable defense against not only oracle-guided logic\nattacks, but also removal and dataflow attacks. In this paper, using time base\nkeys, we propose, implement and evaluate a family of secure multi-key logic\nlocking algorithms called Cute-Lock that can be applied both in RTL-level\nbehavioral and netlist-level structural representations of sequential circuits.\nOur extensive experimental results under a diverse range of attacks confirm\nthat, compared to vulnerable state-of-the-art methods, employing the Cute-Lock\nfamily drives attacking attempts to a dead end without additional overhead.",
        "Exoplanet research is at the forefront of contemporary astronomy\nrecommendations. As more and more exoplanets are discovered and vetted,\ndatabases and catalogs are built to collect information. Various resources are\navailable to scientists for this purpose, though every one of them has\ndifferent scopes and notations. In Alei et al. (2020) we described Exo-MerCat,\na script that collects information from multiple sources and creates a\nhomogenized table. In this manuscript, we announce the release of the\nExo-MerCat v2.0.0 script as an upgraded, tested, documented and open-source\nsoftware to produce catalogs. The main upgrades on the script concern: 1) the\naddition of the TESS Input Catalog and the K2 Input Catalog as input sources;\n2) the optimization of the main identifier queries; 3) a more complex merging\nof the entries from the input sources into the final catalog; 4) some\nquality-of-life improvements such as informative flags, more user-friendly\ncolumn headers, and log files; 5) the refactoring of the code in modules. We\ncompare the performance of Exo-MerCat v2.0.0 with the previous version and\nnotice a substantial improvement in the completeness of the sample, thanks to\nthe addition of new input sources, and its accuracy, because of the\noptimization of the script.",
        "We compare multiple foreground-cleaning pipelines for estimating the\ntensor-to-scalar ratio, $r$, using simulated maps of the planned CMB-S4\nexperiment within the context of the South Pole Deep Patch. To evaluate\nrobustness, we analyze bias and uncertainty on $r$ across various foreground\nsuites using map-based simulations. The foreground-cleaning methods include: a\nparametric maximum likelihood approach applied to auto- and cross-power spectra\nbetween frequency maps; a map-based parametric maximum-likelihood method; and a\nharmonic-space internal linear combination using frequency maps. We summarize\nthe conceptual basis of each method to highlight their similarities and\ndifferences. To better probe the impact of foreground residuals, we implement\nan iterative internal delensing step, leveraging a map-based pipeline to\ngenerate a lensing $B$-mode template from the Large Aperture Telescope\nfrequency maps. Our results show that the performance of the three approaches\nis comparable for simple and intermediate-complexity foregrounds, with\n$\\sigma(r)$ ranging from 3 to 5 $\\times 10^{-4}$. However, biases at the\n$1-2\\sigma$ level appear when analyzing more complex forms of foreground\nemission. By extending the baseline pipelines to marginalize over foreground\nresiduals, we demonstrate that contamination can be reduced to within\nstatistical uncertainties, albeit with a pipeline-dependent impact on\n$\\sigma(r)$, which translates to a detection significance between 2 and\n4$\\sigma$ for an input value of $r = 0.003$. These findings suggest varying\nlevels of maturity among the tested pipelines, with the auto- and\ncross-spectra-based approach demonstrating the best stability and overall\nperformance. Moreover, given the extremely low noise levels, mutual validation\nof independent foreground-cleaning pipelines is essential to ensure the\nrobustness of any potential detection.",
        "In this paper, we introduce DobLIX, a dual-objective learned index\nspecifically designed for Log-Structured Merge(LSM) tree-based key-value\nstores. Although traditional learned indexes focus exclusively on optimizing\nindex lookups, they often overlook the impact of data access from storage,\nresulting in performance bottlenecks. DobLIX addresses this by incorporating a\nsecond objective, data access optimization, into the learned index training\nprocess. This dual-objective approach ensures that both index lookup efficiency\nand data access costs are minimized, leading to significant improvements in\nread performance while maintaining write efficiency in real-world LSM-tree\nsystems. Additionally, DobLIX features a reinforcement learning agent that\ndynamically tunes the system parameters, allowing it to adapt to varying\nworkloads in real-time. Experimental results using real-world datasets\ndemonstrate that DobLIX reduces indexing overhead and improves throughput by\n1.19 to 2.21 times compared to state-of-the-art methods within RocksDB, a\nwidely used LSM-tree-based storage engine.",
        "We apply a real-space block renormalization group approach to study the\ncritical properties of the random transverse-field Ising spin chain with\nmultispin interactions. First we recover the known properties of the\ntraditional model with two-spin interactions by applying the renormalization\napproach for arbitrary size of the block. For the model with three-spin\ncouplings we calculate the critical point and demonstrate that the phase\ntransition is controlled by an infinite disorder fixed point. We have\ndetermined the typical correlation-length critical exponent, which seems to be\ndifferent from that of the random transverse Ising chain with nearest-neighbor\ncouplings. Thus this model represents a new infinite disorder universality\nclass.",
        "This paper studies an online variant of the celebrated housing market\nproblem, where each agent has a single house and seeks to exchange it for\nanother based on her preferences. In this online setting, agents may arrive and\ndepart at any time, meaning that not all agents are present on the housing\nmarket simultaneously. I extend the well known serial dictatorship and Gale s\ntop trading cycle mechanisms to this online scenario, aiming to retain their\ndesirable properties such as Pareto efficiency, individual rationality, and\nstrategy proofness. These extensions also seek to prevent agents from\nstrategically delaying their arrival or advancing their departure. I\ndemonstrate that achieving all of these properties simultaneously is impossible\nin the online context, and I present several variants that achieve different\nsubsets of these properties.",
        "We study a model for the spread and (de)differentiation of mesenchymal stem\ncells and chondrocytes in a scaffold whose fibers are coated with hyaluron. The\nchondrocytes produce new extracellular matrix, which, together with hyaluron,\nserves as haptotactic cue for the stem cell migration. We prove global\nexistence of weak solutions of the corresponding cross-diffusion system with\ndouble haptotaxis.",
        "The discovery of superconductivity with $T_c$ exceeding 40 K in\nLa$_3$Ni$_2$O$_7$ and (La,Pr)$_{3}$Ni$_2$O$_7$ thin films at ambient pressure\nmarks a significant breakthrough in nickelate superconductors. Using density\nfunctional theory (DFT), we propose the double-stacked two-orbital effective\nmodels for La$_3$Ni$_2$O$_7$ thin films, based on the Ni$-e_g$ orbitals. Our\nanalysis reveals the presence of three electron pockets\n$\\alpha,\\alpha^{\\prime},\\beta$ and two hole pockets $\\gamma,\\gamma^{\\prime}$ on\nthe Fermi surface, where the additional pockets $\\alpha^{\\prime}$ and\n$\\gamma^{\\prime}$ emerge due to inter-stack interactions. Furthermore, we\nconstruct higher-energy models incorporating O-$p$ orbitals to facilitate\nfurther investigations. The spin susceptibility, calculated within the random\nphase approximation (RPA), indicates enhanced magnetic correlations primarily\ndriven by nesting effects of the $\\gamma$ pocket, which is predominantly\ncontributed by the Ni$-d_{z^2}$ orbital. These models provide fundamental\nframework for further theoretical and experimental studies, offering critical\ninsights into the superconducting mechanism of La$_3$Ni$_2$O$_7$ thin films.",
        "Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows.",
        "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https:\/\/github.com\/ada-cheng\/CAT-Pruning",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "In this study, we delve into the hidden threats posed to text watermarking by\nusers with cross-lingual knowledge. While most research focuses on watermarking\nmethods for English, there is a significant gap in evaluating these methods in\ncross-lingual contexts. This oversight neglects critical adversary scenarios\ninvolving cross-lingual users, creating uncertainty regarding the effectiveness\nof cross-lingual watermarking. We assess four watermarking techniques across\nfour linguistically rich languages, examining watermark resilience and text\nquality across various parameters and attacks. Our focus is on a realistic\nscenario featuring adversaries with cross-lingual expertise, evaluating the\nadequacy of current watermarking methods against such challenges.",
        "In recent years, the rapid development of AI systems has brought about the\nbenefits of intelligent services but also concerns about security and\nreliability. By fostering appropriate user reliance on an AI system, both\ncomplementary team performance and reduced human workload can be achieved.\nPrevious empirical studies have extensively analyzed the impact of factors\nranging from task, system, and human behavior on user trust and appropriate\nreliance in the context of one-step decision making. However, user reliance on\nAI systems in tasks with complex semantics that require multi-step workflows\nremains under-explored. Inspired by recent work on task decomposition with\nlarge language models, we propose to investigate the impact of a novel\nMulti-Step Transparent (MST) decision workflow on user reliance behaviors. We\nconducted an empirical study (N = 233) of AI-assisted decision making in\ncomposite fact-checking tasks (i.e., fact-checking tasks that entail multiple\nsub-fact verification steps). Our findings demonstrate that human-AI\ncollaboration with an MST decision workflow can outperform one-step\ncollaboration in specific contexts (e.g., when advice from an AI system is\nmisleading). Further analysis of the appropriate reliance at fine-grained\nlevels indicates that an MST decision workflow can be effective when users\ndemonstrate a relatively high consideration of the intermediate steps. Our work\nhighlights that there is no one-size-fits-all decision workflow that can help\nobtain optimal human-AI collaboration. Our insights help deepen the\nunderstanding of the role of decision workflows in facilitating appropriate\nreliance. We synthesize important implications for designing effective means to\nfacilitate appropriate reliance on AI systems in composite tasks, positioning\nopportunities for the human-centered AI and broader HCI communities.",
        "Multifractal analysis (MFA) provides a framework for the global\ncharacterization of image textures by describing the spatial fluctuations of\ntheir local regularity based on the multifractal spectrum. Several works have\nshown the interest of using MFA for the description of homogeneous textures in\nimages. Nevertheless, natural images can be composed of several textures and,\nin turn, multifractal properties associated with those textures. This paper\nintroduces a Bayesian multifractal segmentation method to model and segment\nmultifractal textures by jointly estimating the multifractal parameters and\nlabels on images. For this, a computationally and statistically efficient\nmultifractal parameter estimation model for wavelet leaders is firstly\ndeveloped, defining different multifractality parameters to different regions\nof an image. Then, a multiscale Potts Markov random field is introduced as a\nprior to model the inherent spatial and scale correlations between the labels\nof the wavelet leaders. A Gibbs sampling methodology is employed to draw\nsamples from the posterior distribution of the parameters. Numerical\nexperiments are conducted on synthetic multifractal images to evaluate the\nperformance of the proposed segmentation approach. The proposed method achieves\nsuperior performance compared to traditional unsupervised segmentation\ntechniques as well as modern deep learning-based approaches, showing its\neffectiveness for multifractal image segmentation.",
        "The world of nanoscales in fluidics is the frontier where the continuum of\nfluid mechanics meets the atomic, and even quantum, nature of matter. While\nwater dynamics remains largely classical under extreme confinement, several\nexperiments have recently reported coupling between water transport and the\nelectronic degrees of freedom of the confining materials. This avenue prompts\nus to reconsider nanoscale hydrodynamic flows under the perspective of\ninteracting excitations, akin to condensed matter frameworks. Here we show,\nusing a combination of many-body theory and molecular simulations, that the\nflow of a liquid can induce the flow of another liquid behind a separating\nwall, at odds with the prediction of continuum hydrodynamics. We further show\nthat the range of this 'flow tunnelling' can be tuned through the solid's\nelectronic excitations, with a maximum occurring when these are at resonance\nwith the liquid's charge density fluctuations. Flow tunnelling is expected to\nplay a role in global transport across nanoscale fluidic networks, such as\nlamellar graphene oxide or MXene membranes. It further suggests exploiting the\nelectronic properties of the confining walls for manipulating liquids via their\ndielectric spectra, beyond the nature and characteristics of individual\nmolecules.",
        "We note that Weihrauch problems can be regarded as containers over the\ncategory of projective represented spaces and that Weihrauch reductions\ncorrespond exactly to container morphisms. We also show that Bauer's extended\nWeihrauch degrees and the posetal reflection of containers over partition\nassemblies are equivalent. Using this characterization, we show how a number of\noperators over Weihrauch degrees, such as the composition product, also arise\nnaturally from the abstract theory of polynomial functors.",
        "We show the homology of the Hurwitz space associated to an arbitrary finite\nrack stabilizes integrally in a suitable sense. We also compute the dominant\npart of its stable homology after inverting finitely many primes. This proves a\nconjecture of Ellenberg--Venkatesh--Westerland and improves upon our previous\nresults for non-splitting racks. We obtain applications to Malle's conjecture,\nthe Picard rank conjecture, and the Cohen--Lenstra--Martinet heuristics.",
        "This paper concisely summarizes the XTS block encryption mode for storage\nsector-based encryption applications and clarifies its limitations. In\nparticular, we aim to provide a unified basis for much needed discussions about\nthe newly proposed key scope change to the IEEE 1619 standard.",
        "Diffusion Policy (DP) has attracted significant attention as an effective\nmethod for policy representation due to its capacity to model\nmulti-distribution dynamics. However, current DPs are often based on a single\nvisual modality (e.g., RGB or point cloud), limiting their accuracy and\ngeneralization potential. Although training a generalized DP capable of\nhandling heterogeneous multimodal data would enhance performance, it entails\nsubstantial computational and data-related costs. To address these challenges,\nwe propose a novel policy composition method: by leveraging multiple\npre-trained DPs based on individual visual modalities, we can combine their\ndistributional scores to form a more expressive Modality-Composable Diffusion\nPolicy (MCDP), without the need for additional training. Through extensive\nempirical experiments on the RoboTwin dataset, we demonstrate the potential of\nMCDP to improve both adaptability and performance. This exploration aims to\nprovide valuable insights into the flexible composition of existing DPs,\nfacilitating the development of generalizable cross-modality, cross-domain, and\neven cross-embodiment policies. Our code is open-sourced at\nhttps:\/\/github.com\/AndyCao1125\/MCDP.",
        "Inverse generation problems, such as denoising without ground truth\nobservations, is a critical challenge in many scientific inquiries and\nreal-world applications. While recent advances in generative models like\ndiffusion models, conditional flow matching, and consistency models achieved\nimpressive results by casting generation as denoising problems, they cannot be\ndirectly used for inverse generation without access to clean data. Here we\nintroduce Inverse Flow (IF), a novel framework that enables using these\ngenerative models for inverse generation problems including denoising without\nground truth. Inverse Flow can be flexibly applied to nearly any continuous\nnoise distribution and allows complex dependencies. We propose two algorithms\nfor learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency\nModel (ICM). Notably, to derive the computationally efficient, simulation-free\ninverse consistency model objective, we generalized consistency training to any\nforward diffusion processes or conditional flows, which have applications\nbeyond denoising. We demonstrate the effectiveness of IF on synthetic and real\ndatasets, outperforming prior approaches while enabling noise distributions\nthat previous methods cannot support. Finally, we showcase applications of our\ntechniques to fluorescence microscopy and single-cell genomics data,\nhighlighting IF's utility in scientific problems. Overall, this work expands\nthe applications of powerful generative models to inversion generation\nproblems.",
        "Spin-2 ultralight dark matter (ULDM) is a viable dark matter candidate and it\ncan be constrained using gravitational wave (GW) observations. In this paper,\nwe investigate the detectability of spin-2 ULDM by space-based GW\ninterferometers. By considering a direct coupling between spin-2 ULDM and\nordinary matter, we derive the corresponding response functions and sensitivity\ncurves for various time-delay interferometry channels and calculate the optimal\nsensitivity curves for future millihertz GW detectors. Our results demonstrate\nthat the space-based detectors can place stringent constraints on the coupling\nconstant of spin-2 ULDM, reaching $\\alpha \\sim 10^{-10}$ around a mass of $m\n\\sim 10^{-17} \\rm eV$, surpassing current limits from ground-based detectors\nand pulsar timing arrays. Thus, the space-based GW detectors can serve as\npowerful tools not only for detecting GWs but also for probing fundamental\nproperties of ultralight dark matter.",
        "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
        "In this study, we firstly introduce a method that converts CityGML data into\nvoxels which works efficiently and fast in high resolution for large scale\ndatasets such as cities but by sacrificing some building details to overcome\nthe limitations of previous voxelization methodologies that have been\ncomputationally intensive and inefficient at transforming large-scale urban\nareas into voxel representations for high resolution. Those voxelized 3D city\ndata from multiple cities and corresponding air temperature data are used to\ndevelop a machine learning model. Before the model training, Gaussian blurring\nis implemented on input data to consider spatial relationships, as a result the\ncorrelation rate between air temperature and volumetric building morphology is\nalso increased after the Gaussian blurring. After the model training, the\nprediction results are not just evaluated with Mean Square Error (MSE) but some\nimage similarity metrics such as Structural Similarity Index Measure (SSIM) and\nLearned Perceptual Image Patch Similarity (LPIPS) that are able to detect and\nconsider spatial relations during the evaluation process. This trained model is\ncapable of predicting the spatial distribution of air temperature by using\nbuilding volume information of corresponding pixel as input. By doing so, this\nresearch aims to assist urban planners in incorporating environmental\nparameters into their planning strategies, thereby facilitating more\nsustainable and inhabitable urban environments.",
        "Open-plan offices are well-known to be adversely affected by acoustic issues.\nThis study aims to model acoustic dissatisfaction using measurements of room\nacoustics, sound environment during occupancy, and occupant surveys (n = 349)\nin 28 offices representing a diverse range of workplace parameters. As latent\nfactors, the contribution of $\\textit{lack of privacy}$ (LackPriv) was 25%\nhigher than $\\textit{noise disturbance}$ (NseDstrb) in predicting\n$\\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on\nsound pressure level (SPL) decay of speech ($L_{\\text{p,A,s,4m}}$ and\n$r_{\\text{C}}$) were better in predicting these factors than distraction\ndistance ($r_{\\text{D}}$) based on speech transmission index. This contradicts\nprevious findings, and the trends for SPL-based metrics in predicting AcDsat\nand LackPriv go against expectations based on ISO 3382-3. For sound during\noccupation, $L_{\\text{A,90}}$ and psychoacoustic loudness ($N_{\\text{90}}$)\npredicted AcDsat, and a SPL fluctuation metric ($M_{\\text{A,eq}}$) predicted\nLackPriv. However, these metrics were weaker predictors than ISO 3382-3\nmetrics. Medium-sized offices exhibited higher dissatisfaction than larger\n($\\geq$50 occupants) offices. Dissatisfaction varied substantially across\nparameters including ceiling heights, number of workstations, and years of\nwork, but not between offices with fixed seating compared to more flexible and\nactivity-based working configurations. Overall, these findings highlight the\ncomplexities in characterizing occupants' perceptions using instrumental\nacoustic measurements.",
        "Cooperative perception offers an optimal solution to overcome the perception\nlimitations of single-agent systems by leveraging Vehicle-to-Everything (V2X)\ncommunication for data sharing and fusion across multiple agents. However, most\nexisting approaches focus on single-modality data exchange, limiting the\npotential of both homogeneous and heterogeneous fusion across agents. This\noverlooks the opportunity to utilize multi-modality data per agent, restricting\nthe system's performance. In the automotive industry, manufacturers adopt\ndiverse sensor configurations, resulting in heterogeneous combinations of\nsensor modalities across agents. To harness the potential of every possible\ndata source for optimal performance, we design a robust LiDAR and camera\ncross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to\nboth intra-agent cross-modality fusion and inter-agent cross-modality fusion\nscenarios, owing to the convenient coordinate conversion by transformation\nmatrix and the unified sampling\/inversion mechanism. We also propose two\ndifferent architectures, named Paint-To-Puzzle (PTP) and\nCo-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP\naims for maximum precision performance and achieves smaller data packet size by\nlimiting cross-agent fusion to a single instance, but requiring all\nparticipants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents\nwith any configuration-LiDAR-only, camera-only, or LiDAR-camera-both,\npresenting more generalization ability. Our approach achieves state-of-the-art\n(SOTA) performance on both real and simulated cooperative perception datasets.\nThe code will be released at GitHub in early 2025.",
        "This paper presents an implementation of machine learning model training\nusing private federated learning (PFL) on edge devices. We introduce a novel\nframework that uses PFL to address the challenge of training a model using\nusers' private data. The framework ensures that user data remain on individual\ndevices, with only essential model updates transmitted to a central server for\naggregation with privacy guarantees. We detail the architecture of our app\nselection model, which incorporates a neural network with attention mechanisms\nand ambiguity handling through uncertainty management. Experiments conducted\nthrough off-line simulations and on device training demonstrate the feasibility\nof our approach in real-world scenarios. Our results show the potential of PFL\nto improve the accuracy of an app selection model by adapting to changes in\nuser behavior over time, while adhering to privacy standards. The insights\ngained from this study are important for industries looking to implement PFL,\noffering a robust strategy for training a predictive model directly on edge\ndevices while ensuring user data privacy."
      ]
    }
  },
  {
    "id":2411.05188,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Limitations of conventional magnetic resonance imaging as a predictor of death or disability following neonatal hypoxic-ischemic encephalopathy in the late hypothermia trial",
    "start_abstract":"Objective: To investigate if magnetic resonance imaging (MRI) is an accurate predictor for death or moderate-severe disability at 18-22 months of age among infants with neonatal encephalopathy in a trial of cooling initiated at 6-24 hours. Study design: Subgroup analysis of infants \u226536 weeks of gestation with moderate-severe neonatal encephalopathy randomized at 6-24 postnatal hours to hypothermia or usual care in a multicenter trial of late hypothermia. MRI scans were performed per each center's practice and interpreted by 2 central readers using the Eunice Kennedy Shriver National Institute of Child Health and Human Development injury score (6 levels, normal to hemispheric devastation). Neurodevelopmental outcomes were assessed at 18-22 months of age. Results: Of 168 enrollees, 128 had an interpretable MRI and were seen in follow-up (n = 119) or died (n = 9). MRI findings were predominantly acute injury and did not differ by cooling treatment. At 18-22 months, death or severe disability occurred in 20.3%. No infant had moderate disability. Agreement between central readers was moderate (weighted kappa 0.56, 95% CI 0.45-0.67). The adjusted odds of death or severe disability increased 3.7-fold (95% CI 1.8-7.9) for each increment of injury score. The area under the curve for severe MRI patterns to predict death or severe disability was 0.77 and the positive and negative predictive values were 36% and 100%, respectively. Conclusions: MRI injury scores were associated with neurodevelopmental outcome at 18-22 months among infants in the Late Hypothermia Trial. However, the results suggest caution when using qualitative interpretations of MRI images to provide prognostic information to families following perinatal hypoxia-ischemia.",
    "start_categories":[
      "Pediatrics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "What is being transferred in transfer learning?"
      ],
      "abstract":[
        "One desired capability for machines is the ability to transfer their knowledge of one domain another where data (usually) scarce. Despite ample adaptation learning in various deep applications, we yet do not understand what enables a successful and which part network responsible that. In this paper, provide new tools analyses address these fundamental questions. Through series on transferring block-shuffled images, separate effect feature reuse from low-level statistics show that some benefit comes latter. We present when training pre-trained weights, model stays same basin loss landscape different instances such are similar space close parameter space."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Single-Ferroelectric Memcapacitor-Based Time-Domain Content-Addressable\n  Memory for Highly Precise Distance Function Computation",
        "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected",
        "Exploring the encoding of linguistic representations in the\n  Fully-Connected Layer of generative CNNs for Speech",
        "A Note on Mixed Cages of Girth 5",
        "Modeling the impact of hospitalization-induced behavioral changes on\n  SARS-COV-2 spread in New York City",
        "Doped resonating valence bond states: How robust are the spin ice phases\n  in 3D Rydberg arrays",
        "FlakeRanker: Automated Identification and Prioritization of Flaky Job\n  Failure Categories",
        "$K_{2,3}$-induced minor-free graphs admit quasi-isometry with additive\n  distortion to graphs of tree-width at most two",
        "Cryogenic operation of silicon photomultiplier arrays",
        "GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign\n  Language Accessibility",
        "Invitation to the subpath number",
        "Mono-lepton Signature of a Neutrino-philic Dark Fermion at Hadron\n  Colliders",
        "Learning Accurate Models on Incomplete Data with Minimal Imputation",
        "Eruptive YSOs in Cygnus-X: a mid-infrared variability study with NEOWISE\n  and SPICY",
        "Declarative Application Management in the Fog. A bacteria-inspired\n  decentralised approach",
        "Chiral and deconfinement thermal transitions at finite quark spin\n  polarization in lattice QCD simulations",
        "Variational Quantum Optimization with Continuous Bandits",
        "AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding\n  Predictive Architecture for Autonomous Driving with LiDAR Data",
        "The Massive and Distant Clusters of $WISE$ Survey. XII. Exploring X-ray\n  AGN in Dynamically Active Massive Galaxy Clusters at z~1",
        "Spectral proper orthogonal decomposition using sub-Nyquist rate data",
        "Robotic CBCT Meets Robotic Ultrasound",
        "Finding Needles in Emb(a)dding Haystacks: Legal Document Retrieval via\n  Bagging and SVR Ensembles",
        "On Beating $2^n$ for the Closest Vector Problem",
        "A Supplement to the anticanonical Volumes of weak $\\mathbb{Q}$-Fano\n  threefolds of Picard rank two",
        "Interaction of Electromagnetic Radiation with Cometary Dust",
        "Various approaches to solving nonlinear equations",
        "Matrix weighted inequalities for fractional type integrals associated to\n  operators with new classes of weights",
        "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?",
        "Effects of particle elongation on dense granular flows down a rough\n  inclined plane"
      ],
      "abstract":[
        "Single ferroelectric memcapacitor-based time-domain (TD) content-addressable\nmemory (CAM) is proposed and experimentally demonstrated for high reliability\nand density. The proposed TD CAM features the symmetric capacitance-voltage\ncharacteristics of a ferroelectric memcapacitor with a gated p-i-n diode\nstructure. This CAM performs search operations based on the variable\ncapacitance of cells. The propagation delay of the TD CAM output signal is\nlinearly correlated with the Hamming distance (HD) between input and output\nvectors. The proposed TD CAM array exhibits exceptional reliability in HD\ncomputation and in-memory search tasks owing to this linearity, considerably\noutperforming the conventional nonlinear voltage-domain CAM.",
        "This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations.",
        "Interpretability work on the convolutional layers of CNNs has primarily\nfocused on computer vision, but some studies also explore correspondences\nbetween the latent space and the output in the audio domain. However, it has\nnot been thoroughly examined how acoustic and linguistic information is\nrepresented in the fully connected (FC) layer that bridges the latent space and\nconvolutional layers. The current study presents the first exploration of how\nthe FC layer of CNNs for speech synthesis encodes linguistically relevant\ninformation. We propose two techniques for exploration of the fully connected\nlayer. In Experiment 1, we use weight matrices as inputs into convolutional\nlayers. In Experiment 2, we manipulate the FC layer to explore how\nsymbolic-like representations are encoded in CNNs. We leverage the fact that\nthe FC layer outputs a feature map and that variable-specific weight matrices\nare temporally structured to (1) demonstrate how the distribution of learned\nweights varies between latent variables in systematic ways and (2) demonstrate\nhow manipulating the FC layer while holding constant subsequent model\nparameters affects the output. We ultimately present an FC manipulation that\ncan output a single segment. Using this technique, we show that lexically\nspecific latent codes in generative CNNs (ciwGAN) have shared lexically\ninvariant sublexical representations in the FC-layer weights, showing that\nciwGAN encodes lexical information in a linguistically principled manner.",
        "A mixed regular graph is a graph where every vertex has $z$ incoming arcs,\n$z$ outgoing arcs, and $r$ edges; furthermore, if it has girth $g$, we say that\nthe graph is a \\emph{$[z,r;g]$-mixed graph}. A \\emph{$[z,r;g]$-mixed cage} is a\n$[z,r;g]$-mixed graph with the smallest possible order. In this note, we give a\nfamily of $[z,q;5]$-mixed graphs for $q\\geq 7$ power of prime and $q-1\\leq\n4z+R$ with $z\\geq 1$ and $R \\in \\{1,\\ldots,5\\}$. This provides better upper\nbounds on the order of mixed cages until this moment.",
        "A novel behavior-epidemiology model, which considers $n$ heterogeneous\nbehavioral groups based on level of risk tolerance and distinguishes behavioral\nchanges by social and disease-related motivations (such as peer-influence and\nfear of disease-related hospitalizations), is developed. In addition to\nrigorously analyzing the basic qualitative features of this model, a special\ncase is considered where the total population is stratified into two groups:\nrisk-averse (Group 1) and risk-tolerant (Group 2). The two-group behavior model\nhas three disease-free equilibria in the absence of disease, and their\nstability is analyzed using standard linearization and the properties of\nMetzler-stable matrices. Furthermore, the two-group model was calibrated and\nvalidated using daily hospitalization data for New York City during the first\nwave, and the calibrated model was used to predict the data for the second\nwave. Numerical simulations of the calibrated two-group behavior model showed\nthat while the dynamics of the SARS-CoV-2 pandemic during the first wave was\nlargely influenced by the behavior of the risk-tolerant individuals, the\ndynamics during the second wave was influenced by the behavior of individuals\nin both groups. It was also shown that disease-motivated behavioral changes had\ngreater influence in significantly reducing SARS-CoV-2 morbidity and mortality\nthan behavior changes due to the level of peer or social influence or pressure.\nFinally, it is shown that the initial proportion of members in the community\nthat are risk-averse (i.e., the proportion of individuals in Group 1 at the\nbeginning of the pandemic) and the early and effective implementation of\nnon-pharmaceutical interventions have major impacts in reducing the size and\nburden of the pandemic (particularly the total SARS-CoV-2 mortality in New York\nCity during the second wave).",
        "Rydberg blockade effect provides a convenient platform for simulating locally\nconstrained many-body systems, such as quantum dimer models and quantum loop\nmodels, especially their novel phases like topological orders and gapless\nquantum spin ice (QSI) phases. To discuss the possible phase diagram containing\ndifferent QSIs in 3D Rydberg arrays, here, we have constructed an extended\nRokhsar-Kivelson (RK) Hamiltonian with equal-weight-superposition ground state\nin different fillings at the RK point. Therefore, both the perfect QSIs with\nfixed local dimer filling and their monomer-doped states can be simulated\ndirectly by Monte Carlo sampling. Using single mode approximation, the\nexcitations of dimers and monomers have also been explored in different\nfillings. We find that, in the thermodynamical limit, even doping a small\namount of monomers can disrupt the topological structure and lead to the\nexistence of off-diagonal long-range order. However, in a finite size (as in\ncold-atom experiment), the property of QSI will be kept in a certain region\nlike a crossover after doping. The phase diagram containing different QSIs and\noff-diagonal order phases is proposed.",
        "This document presents the artifact associated with the ICSE SEIP 25 paper\ntitled On the Diagnosis of Flaky Job Failures: Understanding and Prioritizing\nFailure Categories. The original paper identifies and analyzes 46 distinct\ncategories of flaky job failures that developers encounter, using Recency (R),\nFrequency (F), and Monetary (M) measures. In addition, it uses an RFM\nclustering model to identify and prioritize the most wasteful and persistent.\nThe original paper only discusses the rankings and evolution of the top 20\ncategories in the results. This artifact contains (1) the regex and scripts\nused to automate the labeling process for RQ1, (2) complete analysis results,\nincluding the ranking of all 46 categories by cost in RQ2 and the evolution of\nthese categories over time in RQ3, and (3) the RFM dataset and scripts used to\ncreate the RFM clustering model for prioritization in RQ4. In addition, we\nengineered the labeling tool and the RFM-based prioritization methodology in a\ncommand-line interface (CLI) called FLAKERANKER to facilitate reuse and\nrepurposing in future studies.",
        "A graph $H$ is an induced minor of a graph $G$ if $H$ can be obtained from\n$G$ by a sequence of edge contractions and vertex deletions. Otherwise, $G$ is\n$H$-induced minor-free. In this paper, we prove that $K_{2,3}$-induced\nminor-free graphs admit a quasi-isometry with additive distortion to graphs\nwith tree-width at most two. Our result implies that a recent conjecture of\nNguyen et al. [Coarse tree-width (2025)] holds for $K_{2,3}$-induced minor-free\ngraphs.",
        "The LHCb experiment at CERN has been upgraded for the Run 3 operation of the\nLarge Hadron Collider (LHC). A new concept of tracking detector based on\nScintillating Fibres (SciFi) read out with multichannel silicon\nphotomultipliers (SiPMs) was installed during its upgrade. One of the main\nchallenges the SciFi tracker will face during the Run 4 operation of the LHC is\nthe higher radiation environment due to fast neutrons, where the SiPMs are\nlocated. To cope with the increase in radiation, cryogenic cooling with liquid\nnitrogen is being investigated as a possible solution to mitigate the\nperformance degradation of the SiPMs induced by radiation damage. Thus, a\ndetailed performance study of different layouts of SiPM arrays produced by\nFondazione Bruno Kessler (FBK) and Hamamatsu Photonics K.K. is being carried\nout. These SiPMs have been designed to operate at cryogenic temperatures.\nSeveral SiPMs have been tested in a dedicated cryogenic setup down to 100 K.\nKey performance parameters such as breakdown voltage, dark count rate, photon\ndetection efficiency, gain and direct cross-talk are characterized as a\nfunction of the temperature. The main results of this study are going to be\npresented here.",
        "The Greek Language Multimodal Lip Reading with Integrated Sign Language\nAccessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and\nmultimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals.\nDeveloped from the FEELIT project [2], it integrates high-resolution audio,\nvideo, textual transcriptions, and Greek Sign Language translations for\napplications like real-time sign language translation and enhanced subtitle\nsynchronization. While its primary focus is on promoting inclusivity in the\nGreek tourism sector, its adaptability extends to education, healthcare, and\npublic services. Future advancements will enhance word-level precision and\nscalability to additional languages, supported by advanced AI methodologies and\ncollaborations with diverse stakeholders. This dataset underscores the\ntransformative potential of multimodal resources in bridging communication\ngaps, fostering innovation, and setting a benchmark for ethical AI and\ninclusive technologies.",
        "In this paper we count all the subpaths of a given graph G; including the\nsubpaths of length zero, and we call this quantity the subpath number of G. The\nsubpath number is related to the extensively studied number of subtrees, as it\ncan be considered as counting subtrees with the additional requirement of\nmaximum degree being two. We first give the explicit formula for the subpath\nnumber of trees and unicyclic graphs. We show that among connected graphs on\nthe same number of vertices, the minimum of the subpath number is attained for\nany tree and the maximum for the complete graph. Further, we show that the\ncomplete bipartite graph with partite sets of almost equal size maximizes the\nsubpath number among all bipartite graphs. The explicit formula for cycle\nchains, i.e. graphs in which two consecutive cycles share a single edge, is\nalso given. This family of graphs includes the unbranched catacondensed\nbenzenoids which implies a possible application of the result in chemistry. The\npaper is concluded with several directions for possible further research where\nseveral conjectures are provided.",
        "Searching for dark matter at high-energy colliders and direct detection\nexperiments can effectively cover nearly the entire mass range from the MeV to\nthe TeV scale. In this paper, we focus on four-fermion contact interactions\nformulated within the framework of Effective Field Theory. Specifically, we\npresent a detailed analysis of mono-lepton production at the LHC. Our results\ndemonstrate that tensor operators exhibit superior sensitivity in the\nmono-lepton channel, constraining energy scales up to 3\\,TeV for a nearly\nmassless dark fermion using current LHC data. Moreover, these operators mediate\nboth spin-independent and spin-dependent absorption processes in nuclear\ntargets. A systematic comparison of constraints between direct detection\nexperiments and collider measurements reveals the LHC's distinct advantage in\nexploring sub-GeV dark matter candidates while maintaining competitive\nsensitivity at the TeV scale. Notably, direct detection experiments such as\nSuper-Kamiokande and Borexino achieve complementary constraints in the\n10-100\\,TeV mass range through their unique capabilities: utilization of light\nnuclei targets, large exposure volumes, and distinctive features of the recoil\nenergy spectra.",
        "Missing data often exists in real-world datasets, requiring significant time\nand effort for imputation to learn accurate machine learning (ML) models. In\nthis paper, we demonstrate that imputing all missing values is not always\nnecessary to achieve an accurate ML model. We introduce the concept of minimal\ndata imputation, which ensures accurate ML models trained over the imputed\ndataset. Implementing minimal imputation guarantees both minimal imputation\neffort and optimal ML models. We propose algorithms to find exact and\napproximate minimal imputation for various ML models. Our extensive experiments\nindicate that our proposed algorithms significantly reduce the time and effort\nrequired for data imputation.",
        "The mass accretion process controls pre-main-sequence evolution, although its\nintrinsic instability has yet to be fully understood, especially towards the\nprotostellar stage. In this work, we have undertaken a thorough examination of\nthe mid-infrared variability of Spitzer-selected YSOs in the Cygnus-X\nstar-forming region over the last decade, using the NEOWISE time series. This\nwork compares two groups of young stars: embedded Class I objects, and the more\nevolved flat-spectrum\/Class II sources. We report on 48 candidate eruptive\nvariables within these groups, including 14 with characteristics that resemble\nthe photometric behaviour of FUors. We also include an additional 20 YSOs,\nwhich are of a less certain categorisation. We find the candidate FUors to be\nan order of magnitude more common among the younger Class I systems than more\nevolved objects. A large number of the identified short-duration eruptive YSOs\ndisplay mid-infrared colour behaviour that is redder-when-brighter, which\ncontrasts with optically bright outbursts seen in YSOs. Finally, we note the\nunusual long-term rising behaviours of four Class I YSOs, with rise timescales\nlonger than five years, which is far slower than 6-12 month timescale for the\nmajority of optically discovered FUors. Additionally, our broader investigation\nof MIR variability for embedded class I YSOs shows that there is a higher\nincidence of high amplitude variability for these stars, than is seen in class\nII sources. This holds true for all variable class I YSOs, not just the\neruptive sources.",
        "Orchestrating next gen applications over hterogeneous resources along the\nCloud-IoT continuum calls for new strategies and tools to enable scalable and\napplication-specific managements. Inspired by the self-organisation\ncapabilities of bacteria colonies, we propose a declarative, fully\ndecentralised application management solution, targeting pervasive\nopportunistic Cloud-IoT infrastructures. We present acustomisable declarative\nimplementation of the approach and validate its scalability through simulation\nover motivating scenarios, also considering end-user's mobility and the\npossibility to enforce application-specific management policies for different\nclasses of applications.",
        "We study the effect of finite spin quark density on the chiral and\ndeconfinement thermal transitions using numerical simulations of lattice QCD\nwith two dynamical light quarks. The finite spin density is introduced by the\nquark spin potential in the canonical formulation of the spin operator. We show\nthat both chiral and deconfinement temperatures are decreasing functions of the\nspin potential. We determine the parabolic curvatures of transition\ntemperatures in a limit of physical quark masses.",
        "We introduce a novel approach to variational Quantum algorithms (VQA) via\ncontinuous bandits. VQA are a class of hybrid Quantum-classical algorithms\nwhere the parameters of Quantum circuits are optimized by classical algorithms.\nPrevious work has used zero and first order gradient based methods, however\nsuch algorithms suffer from the barren plateau (BP) problem where gradients and\nloss differences are exponentially small. We introduce an approach using\nbandits methods which combine global exploration with local exploitation. We\nshow how VQA can be formulated as a best arm identification problem in a\ncontinuous space of arms with Lipschitz smoothness. While regret minimization\nhas been addressed in this setting, existing methods for pure exploration only\ncover discrete spaces. We give the first results for pure exploration in a\ncontinuous setting and derive a fixed-confidence, information-theoretic,\ninstance specific lower bound. Under certain assumptions on the expected\npayoff, we derive a simple algorithm, which is near-optimal with respect to our\nlower bound. Finally, we apply our continuous bandit algorithm to two VQA\nschemes: a PQC and a QAOA quantum circuit, showing that we significantly\noutperform the previously known state of the art methods (which used gradient\nbased methods).",
        "As opposed to human drivers, current autonomous driving systems still require\nvast amounts of labeled data to train. Recently, world models have been\nproposed to simultaneously enhance autonomous driving capabilities by improving\nthe way these systems understand complex real-world environments and reduce\ntheir data demands via self-supervised pre-training. In this paper, we present\nAD-L-JEPA (aka Autonomous Driving with LiDAR data via a Joint Embedding\nPredictive Architecture), a novel self-supervised pre-training framework for\nautonomous driving with LiDAR data that, as opposed to existing methods, is\nneither generative nor contrastive. Our method learns spatial world models with\na joint embedding predictive architecture. Instead of explicitly generating\nmasked unknown regions, our self-supervised world models predict Bird's Eye\nView (BEV) embeddings to represent the diverse nature of autonomous driving\nscenes. Our approach furthermore eliminates the need to manually create\npositive and negative pairs, as is the case in contrastive learning. AD-L-JEPA\nleads to simpler implementation and enhanced learned representations. We\nqualitatively and quantitatively demonstrate high-quality of embeddings learned\nwith AD-L-JEPA. We furthermore evaluate the accuracy and label efficiency of\nAD-L-JEPA on popular downstream tasks such as LiDAR 3D object detection and\nassociated transfer learning. Our experimental evaluation demonstrates that\nAD-L-JEPA is a plausible approach for self-supervised pre-training in\nautonomous driving applications and is the best available approach\noutperforming SOTA, including most recently proposed Occupancy-MAE [1] and ALSO\n[2]. The source code of AD-L-JEPA is available at\nhttps:\/\/github.com\/HaoranZhuExplorer\/AD-L-JEPA-Release.",
        "We present an analysis of the cluster X-ray morphology and active galactic\nnucleus (AGN) activity in nine $z\\sim1$ galaxy clusters from the Massive and\nDistant Clusters of $WISE$ Survey (MaDCoWS) observed with $Chandra$. Using\nphoton asymmetry ($A_{\\text{phot}}$) to quantify X-ray morphologies, we find\nevidence that the four most dynamically disturbed clusters are likely to be\nmergers. Employing a luminosity cut of $7.6\\times10^{42}$ erg\/s to identify AGN\nin the 0.7-7.0 keV, we show that the majority of these clusters host excess AGN\ncompared to the local field. We use the cumulative number-count ($\\log N-\\log\nS$) model to predict AGN incidence in cluster isophotes under this luminosity\ncut. Our analysis finds evidence (at $> 2\\sigma$) of a positive correlation\nbetween AGN surface densities and photon asymmetry, suggesting that a disturbed\ncluster environment plays a pivotal role in regulating AGN triggering. Studying\nAGN incidence in cluster X-ray isophotes equivalent in area to $1.0r_{500}$, we\nfind that the AGN space density inversely scales with cluster mass as $\\sim\nM^{-0.5^{+0.18}_{-0.18}}$ at the 3.18$\\sigma$ level. Finally, when we\nseparately explore the cluster mass dependence of excess AGN surface density in\ndisturbed and relaxed clusters, we see tentative evidence that the two\nmorphologically distinct sub-populations exhibit diverging trends, especially\nnear the outskirts, likely due to cluster merger-driven AGN\ntriggering\/suppression.",
        "Modal decomposition methods are important for characterizing the\nlow-dimensional dynamics of complex systems, including turbulent flows.\nDifferent methods have varying data requirements and produce modes with\ndifferent properties. Spectral proper orthogonal decomposition (SPOD) produces\northogonal, energy-ranked spatial modes at discrete temporal frequencies for\nstatistically stationary flows. However, SPOD requires long stretches of\nsequential, uniformly sampled, time-resolved data. These data requirements\nlimit SPOD's use in experimental settings where the maximum capture rate of a\ncamera is often slower than the Nyquist sampling rate required to resolve the\nhighest turbulent frequencies. However, if two PIV systems operate in tandem,\npairs of data can be acquired that are arbitrarily close in time. The dynamic\nmode decomposition (DMD) uses this pairwise data to resolve frequencies up to\nthe Nyquist frequency associated with the small time step within a pair.\nHowever, these modes do not form an orthonormal basis and have no set ranking.\nThe present work attempts to compute SPOD modes from pairwise data with a small\ntime step but with large gaps between pairs. We use DMD on pairwise data to\nestimate segment-wise, uniformly sampled series that can then be used to\nestimate the SPOD modes, intending to resolve frequencies between the gap and\npair Nyquist limits. The method is tested on numerically obtained data of the\nlinearized complex Ginzburg-Landau equation, as well as a Mach 0.4 isothermal\nturbulent jet. For the jet, pairwise SPOD can accurately de-alias the SPOD\nspectrum and estimate mode shapes at frequencies up to St = 1.0 while using\nover 90% less data.",
        "The multi-modality imaging system offers optimal fused images for safe and\nprecise interventions in modern clinical practices, such as computed tomography\n- ultrasound (CT-US) guidance for needle insertion. However, the limited\ndexterity and mobility of current imaging devices hinder their integration into\nstandardized workflows and the advancement toward fully autonomous intervention\nsystems. In this paper, we present a novel clinical setup where robotic cone\nbeam computed tomography (CBCT) and robotic US are pre-calibrated and\ndynamically co-registered, enabling new clinical applications. This setup\nallows registration-free rigid registration, facilitating multi-modal guided\nprocedures in the absence of tissue deformation. First, a one-time\npre-calibration is performed between the systems. To ensure a safe insertion\npath by highlighting critical vasculature on the 3D CBCT, SAM2 segments vessels\nfrom B-mode images, using the Doppler signal as an autonomously generated\nprompt. Based on the registration, the Doppler image or segmented vessel masks\nare then mapped onto the CBCT, creating an optimally fused image with\ncomprehensive detail. To validate the system, we used a specially designed\nphantom, featuring lesions covered by ribs and multiple vessels with simulated\nmoving flow. The mapping error between US and CBCT resulted in an average\ndeviation of 1.72+-0.62 mm. A user study demonstrated the effectiveness of\nCBCT-US fusion for needle insertion guidance, showing significant improvements\nin time efficiency, accuracy, and success rate. Needle intervention performance\nimproved by approximately 50% compared to the conventional US-guided workflow.\nWe present the first robotic dual-modality imaging system designed to guide\nclinical applications. The results show significant performance improvements\ncompared to traditional manual interventions.",
        "We introduce a retrieval approach leveraging Support Vector Regression (SVR)\nensembles, bootstrap aggregation (bagging), and embedding spaces on the German\nDataset for Legal Information Retrieval (GerDaLIR). By conceptualizing the\nretrieval task in terms of multiple binary needle-in-a-haystack subtasks, we\nshow improved recall over the baselines (0.849 > 0.803 | 0.829) using our\nvoting ensemble, suggesting promising initial results, without training or\nfine-tuning any deep learning models. Our approach holds potential for further\nenhancement, particularly through refining the encoding models and optimizing\nhyperparameters.",
        "The Closest Vector Problem (CVP) is a computational problem in lattices that\nis central to modern cryptography. The study of its fine-grained complexity has\ngained momentum in the last few years, partly due to the upcoming deployment of\nlattice-based cryptosystems in practice. A main motivating question has been if\nthere is a $(2-\\varepsilon)^n$ time algorithm on lattices of rank $n$, or\nwhether it can be ruled out by SETH.\n  Previous work came tantalizingly close to a negative answer by showing a\n$2^{(1-o(1))n}$ lower bound under SETH if the underlying distance metric is\nchanged from the standard $\\ell_2$ norm to other $\\ell_p$ norms. Moreover,\nbarriers toward proving such results for $\\ell_2$ (and any even $p$) were\nestablished.\n  In this paper we show \\emph{positive results} for a natural special case of\nthe problem that has hitherto seemed just as hard, namely\n$(0,1)$-$\\mathsf{CVP}$ where the lattice vectors are restricted to be sums of\nsubsets of basis vectors (meaning that all coefficients are $0$ or $1$). All\nprevious hardness results applied to this problem, and none of the previous\nalgorithmic techniques could benefit from it. We prove the following results,\nwhich follow from new reductions from $(0,1)$-$\\mathsf{CVP}$ to weighted\nMax-SAT and minimum-weight $k$-Clique.\n  1. An $O(1.7299^n)$ time algorithm for exact $(0,1)$-$\\mathsf{CVP}_2$ in\nEuclidean norm, breaking the natural $2^n$ barrier, as long as the absolute\nvalue of all coordinates in the input vectors is $2^{o(n)}$.\n  2. A computational equivalence between $(0,1)$-$\\mathsf{CVP}_p$ and\nMax-$p$-SAT for all even $p$.\n  3. The minimum-weight-$k$-Clique conjecture from fine-grained complexity and\nits numerous consequences (which include the APSP conjecture) can now be\nsupported by the hardness of a lattice problem, namely\n$(0,1)$-$\\mathsf{CVP}_2$.",
        "We show that for a weak $\\mathbb{Q}$-Fano threefold $X$\n($\\mathbb{Q}$-factorial with terminal singularities and $-K_X$ is nef and big)\nof Picard rank $\\rho(X)\\leq 2$, either $-K_X^3\\leq 64$ or $-K_X^3=72$ and\n$X=\\mathbb{P}_{\\mathbb{P}^2}(\\mathcal{O}_{\\mathbb{P}^2}\\oplus\\mathcal{O}_{\\mathbb{P}^2}(3))$.\nThis is supplementary to the previous work in arXiv:2501.12555.",
        "The chapter overviews the recent developments in the remote sensing of\ncometary dust using visible, near-infrared, and thermal-infrared radiation, as\nwell as interaction of the dust with electromagnetic radiation, which affects\nthe dynamics of dust particles. It considers photometric, polarimetric, and\nspectral studies of cometary dust, focusing on those observables and\ncorrelations between them that allow revealing the composition, size, and\nstructure of the dust particles. The analysis includes the observed brightness\nand polarization phase curves, color and polarimetric color of the cometary\ndust, and near- and thermal-infrared spectra. Special attention is paid to the\nrole of gas contamination in the polarimetric and photometric data. A review of\nmodeling efforts to interpret the observational results is also provided,\ndescribing the most popular (and some novel) techniques used in the computer\nmodeling of light scattering by dust particles with a focus on modeling the\nmost complex type of cometary particles: fluffy and porous agglomerates. The\nchapter also considers how properties of the dust particles affect their\nphotoelectric emission and their response to the radiation pressure and\nradiative torque, including alignment and fragmentation of particles. Results\nof computer and some laboratory modeling are analyzed for their consistency\nwith the observational and in situ data. Also discussed is how the modeling\nresults can be combined with in situ data for better characterization of the\ncometary dust.",
        "Modelling real world systems frequently requires the solution of systems of\nnonlinear equations. A number of approaches have been suggested and developed\nfor this computational problem. However, it is also possible to attempt\nsolutions using more general nonlinear least squares or function minimization\ntechniques. There are concerns, nonetheless, that we may fail to find\nsolutions, or that the process will be inefficient. Examples are presented with\nR with the goal of providing guidance on the solution of nonlinear equations\nproblems.",
        "Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a\nnon-negative self-adjoint operator on $L^2(\\mathbb{R}^d)$. Assume that the\nkernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for\nall $N>0$, there are constants $c,C>0$ such that \\begin{align}\\label{upper\nbound}\n|p_t(x,y)|\\leq\\frac{C}{t^{d\/2}}e^{-\\frac{|x-y|^2}{ct}}\\Big(1+\\frac{\\sqrt{t}}{\\rho(x)}+\n\\frac{\\sqrt{t}}{\\rho(y)}\\Big)^{-N} \\end{align} holds for all\n$x,y\\in\\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix\nweighted inequalities for fractional type integrals associated to $L$ with new\nclasses of matrix weights, which are nontrivial extension of the results\nestablished by Li, Rahm and Wick [23]. Next, we give new two-weight bump\nconditions with Young functions satisfying wider conditions for fractional type\nintegrals associated to $L$, which cover the result obtained by Cruz-Uribe,\nIsralowitz and Moen [6]. We point out that the new classes of matrix weights\nand bump conditions are larger and weaker than the classical ones given in [17]\nand [6], respectively. As applications, our results can be applied to settings\nof magnetic Schr\\\"{o}dinger operator, Laguerre operators, etc.",
        "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.",
        "Granular materials in nature are nearly always non-spherical, but particle\nshape effects in granular flow remain largely elusive. This study uses discrete\nelement method simulations to investigate how elongated particle shapes affect\nthe mobility of dense granular flows down a rough incline. For a range of\nsystematically varied particle length-to-diameter aspect ratios (AR), we run\nsimulations with various flow thicknesses $h$ and slope angles $\\theta$ to\nextract the well-known $h_\\textrm{stop}(\\theta)$ curves (below which the flow\nceases) and the $Fr$-$h\/h_\\textrm{stop}$ relations following Pouliquen's\napproach, where $Fr=u\/\\sqrt{gh}$ is the Froude number, $u$ is the mean flow\nvelocity, and $g$ is the gravitational acceleration. The slope $\\beta$ of the\n$Fr$-$h\/h_\\textrm{stop}$ relations shows an intriguing S-shaped dependence on\nAR, with two plateaus at small and large AR, respectively, transitioning with a\nsharp increase. We understand this S-shaped dependence by examining statistics\nof particle orientation, alignment, and hindered rotation. We find that the\nrotation ability of weakly elongated particles ($\\textrm{AR}\\lesssim1.3$)\nremains similar to spheres, leading to the first plateau in the $\\beta$-AR\nrelation, whereas the effects of particle orientation saturates beyond\n$\\textrm{AR}\\approx2.0$, explaining the second plateau. An empirical sigmoidal\nfunction is proposed to capture this non-linear dependence. The findings are\nexpected to enhance our understanding of how particle shape affects the flow of\ngranular materials from both the flow- and particle-scale perspectives."
      ]
    }
  },
  {
    "id":2411.05188,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"What is being transferred in transfer learning?",
    "start_abstract":"One desired capability for machines is the ability to transfer their knowledge of one domain another where data (usually) scarce. Despite ample adaptation learning in various deep applications, we yet do not understand what enables a successful and which part network responsible that. In this paper, provide new tools analyses address these fundamental questions. Through series on transferring block-shuffled images, separate effect feature reuse from low-level statistics show that some benefit comes latter. We present when training pre-trained weights, model stays same basin loss landscape different instances such are similar space close parameter space.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Limitations of conventional magnetic resonance imaging as a predictor of death or disability following neonatal hypoxic-ischemic encephalopathy in the late hypothermia trial"
      ],
      "abstract":[
        "Objective: To investigate if magnetic resonance imaging (MRI) is an accurate predictor for death or moderate-severe disability at 18-22 months of age among infants with neonatal encephalopathy in a trial of cooling initiated at 6-24 hours. Study design: Subgroup analysis of infants \u226536 weeks of gestation with moderate-severe neonatal encephalopathy randomized at 6-24 postnatal hours to hypothermia or usual care in a multicenter trial of late hypothermia. MRI scans were performed per each center's practice and interpreted by 2 central readers using the Eunice Kennedy Shriver National Institute of Child Health and Human Development injury score (6 levels, normal to hemispheric devastation). Neurodevelopmental outcomes were assessed at 18-22 months of age. Results: Of 168 enrollees, 128 had an interpretable MRI and were seen in follow-up (n = 119) or died (n = 9). MRI findings were predominantly acute injury and did not differ by cooling treatment. At 18-22 months, death or severe disability occurred in 20.3%. No infant had moderate disability. Agreement between central readers was moderate (weighted kappa 0.56, 95% CI 0.45-0.67). The adjusted odds of death or severe disability increased 3.7-fold (95% CI 1.8-7.9) for each increment of injury score. The area under the curve for severe MRI patterns to predict death or severe disability was 0.77 and the positive and negative predictive values were 36% and 100%, respectively. Conclusions: MRI injury scores were associated with neurodevelopmental outcome at 18-22 months among infants in the Late Hypothermia Trial. However, the results suggest caution when using qualitative interpretations of MRI images to provide prognostic information to families following perinatal hypoxia-ischemia."
      ],
      "categories":[
        "Pediatrics"
      ]
    },
    "list":{
      "title":[
        "Known Unknowns: Out-of-Distribution Property Prediction in Materials and\n  Molecules",
        "Overcoming experimental obstacles in two-dimensional spectroscopy of a\n  single molecule",
        "Benchmarking Selected Density Functionals and Dispersion Corrections for\n  MOF-5 and its Derivatives",
        "Quantum-Corrected Hawking Radiation from Near-Extremal Kerr-Newman Black\n  Holes",
        "Current-linear emergent induction of pinned skyrmion textures in an\n  oxide bilayer",
        "Superadditivity at Large Charge",
        "Mapping strain and structural heterogeneities around bubbles in\n  amorphous ionically conductive Bi$_2$O$_3$",
        "Enumeration of polyhedra with triangular and hexagonal faces and three\n  faces around each vertex",
        "Universal Semantic Embeddings of Chemical Elements for Enhanced\n  Materials Inference and Discovery",
        "Extending the Bridge Connecting Chiral Lagrangians and QCD Gaussian\n  Sum-Rules for Low-Energy Hadronic Physics",
        "The Pseudo-Dimension of Contracts",
        "Nonlinear eigenvalue problems for a class of quasilinear operator on\n  complete Riemannian manifolds",
        "Statistical Inference of the Matthews Correlation Coefficient for\n  Multiclass Classification",
        "Study of Mass Transport in the Anode of a Proton Exchange Membrane Fuel\n  Cell with a New Hydrogen Flow-Rate Modulation Technique",
        "ATP requirements for growth reveal the bioenergetic impact of\n  mitochondrial symbiosis",
        "On Volume Minimization in Conformal Regression",
        "Low-Complexity Detection of Multiple Preambles in the Presence of\n  Mobility and Delay Spread",
        "Constraints on fast radio burst population from the first CHIME\/FRB\n  catalog from Hierarchical Bayesian Inference",
        "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following",
        "Benchmarking Quantum Reinforcement Learning",
        "Magnetically Induced Current Density from Numerical Positional\n  Derivatives of Nucleus Independent Chemical Shifts",
        "Kovacs-like memory effect in a sheared colloidal glass: role of\n  non-affine flows",
        "Bridging Critical Gaps in Convergent Learning: How Representational\n  Alignment Evolves Across Layers, Training, and Distribution Shifts",
        "Caribou -- A versatile data acquisition system for silicon pixel\n  detector prototyping",
        "Elimination of substrate-induced FMR linewidth broadening in the\n  epitaxial system YIG-GGG by microstructuring",
        "Optical Nuclear Electric Resonance in LiNa: Selective Addressing of\n  Nuclear Spins Through Pulsed Lasers",
        "Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via\n  Activation-Level Gaussian Processes",
        "How far are two symmetric matrices from commuting? With an application\n  to object characterisation and identification in metal detection",
        "On the Hilbert depth of the quotient ring of the edge ideal of a star\n  graph"
      ],
      "abstract":[
        "Discovery of high-performance materials and molecules requires identifying\nextremes with property values that fall outside the known distribution.\nTherefore, the ability to extrapolate to out-of-distribution (OOD) property\nvalues is critical for both solid-state materials and molecular design. Our\nobjective is to train predictor models that extrapolate zero-shot to higher\nranges than in the training data, given the chemical compositions of solids or\nmolecular graphs and their property values. We propose using a transductive\napproach to OOD property prediction, achieving improvements in prediction\naccuracy. In particular, the True Positive Rate (TPR) of OOD classification of\nmaterials and molecules improved by 3x and 2.5x, respectively, and precision\nimproved by 2x and 1.5x compared to non-transductive baselines. Our method\nleverages analogical input-target relations in the training and test sets,\nenabling generalization beyond the training target support, and can be applied\nto any other material and molecular tasks.",
        "Two-dimensional electronic spectroscopy provides information on coupling and\nenergy transfer between excited states on ultrafast timescales. Only recently,\nincoherent fluorescence detection has made it possible to combine this method\nwith single-molecule optical spectroscopy to reach the ultimate limit of\nsensitivity. The main obstacle has been the low number of photons detected due\nto limited photostability. Here we discuss the key experimental choices that\nallowed us to overcome these obstacles: broadband acousto-optic modulation,\naccurate phase-locked loops, photon-counting lock-in detection, delay stage\nlinearization, and detector dead-time compensation. We demonstrate how the\nacquired photon stream data can be used to post-select detection events\naccording to specific criteria.",
        "Accurate computational predictions of metal-organic frameworks (MOFs) and\ntheir properties is crucial for discovering optimal compositions and applying\nthem in relevant technological areas. This work benchmarks density functional\ntheory (DFT) approaches, including semi-local, meta-GGA, and hybrid functionals\nwith various dispersion corrections, on MOF-5 and three of its computationally\npredicted derivatives, analyzing structural, electronic, and vibrational\nproperties. Our results underline the importance of explicitly treating van der\nWaals interactions for an accurate description of structural and vibrational\nproperties, and indicate the meta-GGA functional R2SCAN as the best balance\nbetween accuracy and efficiency for characterizing the electronic structure of\nthese systems, in view of future high-throughput screening studies on MOFs.",
        "Near-extremal black holes have a long AdS$_2$ throat in their near-horizon\nregion. Quantum fluctuations in the throat region are effectively governed by a\nquantum version of Jackiw-Teitelboim gravity with matter and are strongly\ncoupled at low temperatures. We investigate how these quantum fluctuations\naffect the spectrum of emission of particles during Hawking radiation. We\nsystematically consider the cases of Kerr and Kerr-Newman black holes for\nemission of scalar particles and discuss photon and graviton emission from the\nKerr background. We find that at very low temperatures the quantum fluctuations\nradically change the nature of particle emission. Unlike the generic\nsuppression of particle emission in the spherically symmetric\nReissner-Nordstr\\\"om case, we uncover that for particles with non-vanishing\nangular momentum, the quantum-corrected emission can be substantially enhanced\nwith respect to the standard semiclassical result.",
        "Emergent electromagnetic induction (EEMI) induced through current-driven spin\ndynamics was recently predicted and subsequently observed in helical spin\nmagnets, opening a new direction in spintronics and paving the way towards\nfurther miniaturization of electronic circuit elements. In contrast to\nconventional inductors consisting of coil-like structures whose inductance $L$\nshows a linear dependence on the cross-section $A$, emergent inductors exhibit\nan inverse ($\\propto {A}^{-1}$) proportionality, favorable for the\nminiaturization of electrical devices. However, the expected current-linear\nresponse of the EEMI voltage has not been demonstrated. Magnetic skyrmions hold\npromise as a simple platform to study the conceptual foundations of EEMI from\ncurrent-driven spin dynamics. We fabricated devices of thin film bilayers of\nferromagnetic SrRuO$_3$ and paramagnetic SrIrO$_3$, which are known to host\ninterfacial N\\'eel skyrmions detected by the appearance of a topological\nHall-effect (THE). A large, positive and current-linear inductive response is\nfound to accompany the THE. In our experiment, the current-induced dynamics of\npinned magnetic skyrmions creates a voltage both parallel and perpendicular to\nthe applied electric current flow, corresponding to longitudinal and transverse\ninduction, respectively. This is the first report of transverse EEMI,\nindicating an angle of $80^{\\circ}$ between skyrmion motion and the applied\ncurrent. Our observation of current-linear longitudinal and transverse EEMI is\na hallmark of pinned dynamics of magnetic skyrmion textures in oxide\nheterostructures.",
        "The weak gravity conjecture has been invoked to conjecture that the\ndimensions of charged operators in a CFT should obey a superadditivity relation\n(sometimes referred to as convexity). In this paper, we study superadditivity\nof the operator spectrum in theories expanded about the semi-classical saddle\npoint that dominates correlators of large charge operators. We explore this in\ntwo contexts. The first is a model with two scalar fields that carry different\ncharges, at a non-trivial Wilson-Fisher fixed point. A careful analysis of the\nsemi-classics for this two field model demonstrates that 'quantum' violations\nof superadditivity (those not forbidden by the conjecture) persist in the large\ncharge regime. We then turn to study the general properties of CFTs at large\ncharge as bottom-up EFTs. By a trial and error procedure we come up with a\nseemingly consistent family of examples violating the conjecture. In so doing\nthe presence of a genuine dilaton field appears necessary. On the one hand our\nresult demonstrates that the superadditivity conjecture cannot be proven purely\non the basis of a bottom-up analysis. On the other hand, the need for a\ndilaton, with the corresponding infinite fine tuning, indicates the\nconjecture-violating EFTs are unlikely to be UV completable.",
        "While amorphous materials are often approximated to have a statistically\nhomogeneous atomic structure, they frequently exhibit localized structural\nheterogeneity that challenges simplified models. This study uses 4D scanning\ntransmission electron microscopy to investigate the strain and structural\nmodifications around gas bubbles in amorphous Bi$_2$O$_3$ induced by argon\nirradiation. We present a method for determining strain fields surrounding\nbubbles that can be used to measure the internal pressure of the gas.\nCompressive strain is observed around the cavities, with higher-order\ncrystalline symmetries emerging near the cavity interfaces, suggesting\nparacrystalline ordering as a result of bubble coarsening. This ordering, along\nwith a compressive strain gradient, indicates that gas bubbles induce\nsignificant localized changes in atomic packing. By analyzing strain fields\nwith maximum compressive strains of 3\\%, we estimate a lower bound on the\ninternal pressure of the bubbles at 2.5 GPa. These findings provide insight\ninto the complex structural behavior of amorphous materials under stress,\nparticularly in systems with gas inclusions, and offer new methods for probing\nthe local atomic structure in disordered materials. Although considering\nstructural heterogeneity in amorphous systems is non-trivial, these features\nhave crucial impacts on material functionalities, such as mechanical strength,\nionic conductivity, and electronic mobility.",
        "We give an exact count of the number of trivalent graphs whose faces all have\n3 or 6 sides, or equivalently, the number of polyhedra with triangular and\nhexagonal faces and three faces around each vertex. The count is given in terms\nof the prime factorization of the number of vertices. We also enumerate graphs\nof this type with mirror symmetry, with 3-fold rotational symmetry, and with\nboth types of symmetry.",
        "We present a framework for generating universal semantic embeddings of\nchemical elements to advance materials inference and discovery. This framework\nleverages ElementBERT, a domain-specific BERT-based natural language processing\nmodel trained on 1.29 million abstracts of alloy-related scientific papers, to\ncapture latent knowledge and contextual relationships specific to alloys. These\nsemantic embeddings serve as robust elemental descriptors, consistently\noutperforming traditional empirical descriptors with significant improvements\nacross multiple downstream tasks. These include predicting mechanical and\ntransformation properties, classifying phase structures, and optimizing\nmaterials properties via Bayesian optimization. Applications to titanium\nalloys, high-entropy alloys, and shape memory alloys demonstrate up to 23%\ngains in prediction accuracy. Our results show that ElementBERT surpasses\ngeneral-purpose BERT variants by encoding specialized alloy knowledge. By\nbridging contextual insights from scientific literature with quantitative\ninference, our framework accelerates the discovery and optimization of advanced\nmaterials, with potential applications extending beyond alloys to other\nmaterial classes.",
        "It has previously been demonstrated that the mesonic fields in chiral\nLagrangians can be related to the quark-level operators of QCD sum-rules via\nenergy-independent (constant) scale factor matrices constrained by chiral\nsymmetry. This leads to universal scale factors for each type of chiral nonet\nrelated to quark-antiquark ($q\\bar q$) operators and four-quark ($qq\\bar q\\bar\nq$) operators. Motivated by these successful demonstrations of scale-factor\nuniversality for the $K_0^*$ isodoublet and $a_0$ isotriplet scalar mesons, a\nrevised Gaussian QCD sum-rule methodology is developed that enables the\nextension to higher-dimensional isospin sectors, including the possibility of\nmixing with glueball components. Moreover, to extract non-perturbative\ninformation about a resonance stemming from the final state interactions of its\ndecay products, a background-resonance interference approximation is developed\nand shown to provide an excellent description of both $\\pi K$ scattering\namplitude data and $\\pi\\eta$ scattering calculations. This background-resonance\ninterference approximation inspires new resonance models as ingredients in the\nscale-factor analysis connecting chiral Lagrangians and QCD Gaussian sum-rules.\nUsing the revised Gaussian QCD sum-rule methodology, key properties of the\nscale factors are examined for the $K_0^*$ isodoublet and $a_0$ isotriplet\nscalar mesons for a sequence of increasingly sophisticated resonance models.\nGaussian sum-rules are demonstrated to have sufficient resolution to\ndistinguish between different resonance models, and it is shown that the\nbackground-resonance interference approximation not only describes $\\{\\pi\nK,\\pi\\eta\\}$ scattering, but leads to the best universality and\nenergy-independence properties of the scale factors.",
        "Algorithmic contract design studies scenarios where a principal incentivizes\nan agent to exert effort on her behalf. In this work, we focus on settings\nwhere the agent's type is drawn from an unknown distribution, and formalize an\noffline learning framework for learning near-optimal contracts from sample\nagent types. A central tool in our analysis is the notion of pseudo-dimension\nfrom statistical learning theory. Beyond its role in establishing upper bounds\non the sample complexity, pseudo-dimension measures the intrinsic complexity of\na class of contracts, offering a new perspective on the tradeoffs between\nsimplicity and optimality in contract design. Our main results provide\nessentially optimal tradeoffs between pseudo-dimension and representation error\n(defined as the loss in principal's utility) with respect to linear and bounded\ncontracts. Using these tradeoffs, we derive sample- and time-efficient learning\nalgorithms, and demonstrate their near-optimality by providing almost matching\nlower bounds on the sample complexity. Conversely, for unbounded contracts, we\nprove an impossibility result showing that no learning algorithm exists.\n  Finally, we extend our techniques in three important ways. First, we provide\nrefined pseudo-dimension and sample complexity guarantees for the combinatorial\nactions model, revealing a novel connection between the number of critical\nvalues and sample complexity. Second, we extend our results to menus of\ncontracts, showing that their pseudo-dimension scales linearly with the menu\nsize. Third, we adapt our algorithms to the online learning setting, where we\nshow that, a polynomial number of type samples suffice to learn near-optimal\nbounded contracts. Combined with prior work, this establishes a formal\nseparation between expert advice and bandit feedback for this setting.",
        "In this manuscript, we study the nonlinear eigenvalue problem on complete\nRiemannian manifolds with Ricci curvature bounded from below, to find the\nunknowns $\\lambda$ and $u$, such that\n  $$\n  Qu + \\lambda f(u) = 0\n  $$\n  where $\\lambda$ is an eigenvalue of $u$, with respect to the quasilinear\noperator $Qu = \\operatorname{div} (\\mathcal{F}(u^2, |\\nabla u|^2)\\nabla u)$ and\nnonlinar function $f(\\cdot)\\neq 0$. We generalize the Cheng--Yau gradient\nestimate in \\cite{shen2025feasibilitynashmoseriterationchengyautype} and\ndemonstrate that under certain conditions, a non-zero eigenvalue gives rise to\nunbounded eigenfunction $u$. Our new result also covers more quasilinear\nequations like $p$-porous medium equation (\\textit{i.e.} $\\Delta_p u^q =\n\\lambda u^r$), and generally,\n$\\Delta_{p}\\left(\\sum_{i=1}^{m}a_iu^{q_i}\\right)+\\lambda u^r = 0$.",
        "Classification problems are essential statistical tasks that form the\nfoundation of decision-making across various fields, including patient\nprognosis and treatment strategies for critical conditions. Consequently,\nevaluating the performance of classification models is of significant\nimportance, and numerous evaluation metrics have been proposed. Among these,\nthe Matthews correlation coefficient (MCC), also known as the phi coefficient,\nis widely recognized as a reliable metric that provides balanced measurements\neven in the presence of class imbalance. However, with the increasing\nprevalence of multiclass classification problems involving three or more\nclasses, macro-averaged and micro-averaged extensions of MCC have been\nemployed, despite a lack of clear definitions or established references for\nthese extensions. In the present study, we propose a formal framework for MCC\ntailored to multiclass classification problems using macro-averaged and\nmicro-averaged approaches. Moreover, discussions on the use of these extended\nMCCs for multiclass problems often rely solely on point estimates, potentially\noverlooking the statistical significance and reliability of the results. To\naddress this gap, we introduce several methods for constructing asymptotic\nconfidence intervals for the proposed metrics. Furthermore, we extend these\nmethods to include the construction of asymptotic confidence intervals for\ndifferences in the proposed metrics, specifically for paired study designs. The\nutility of our methods is evaluated through comprehensive simulations and\nreal-world data analyses.",
        "Hydrogen transport in the anode of a proton-exchange membrane fuel cell\n(PEMFC) has been studied with a modulation technique relating the hydrogen\nflow-rate $(\\tilde{Q}_{H2})$ and the faradaic current $(\\tilde{I})$, called\n$\\textit{Current-modulated Hydrogen flow-rate Spectroscopy}$ (CH2S). A simple\nanalytical expression for the transfer function, $H(j{\\omega})=n \\, F \\,\n\\tilde{Q}_{H2} \\mathbin{\/} \\tilde{I}$, is provided, showing a skewed semicircle\nin Nyquist representation ($-H^{''}$ vs. $H^{'}$), extending from $H^{'}=0\\:$\nto $H^{'}=1$, and with the maximum frequency at\n${\\omega}_{max}=2.33\\,(D_{H2}\\mathbin{\/}{L_{i}}^{2})$, where $D_{H2}$ is the\neffective hydrogen diffusivity and $L_i$ the thickness of the anode gas\ndiffusion layer (GDL). The expression for CH2S is also calculated with an\nexisting reversible chemical reaction in the GDL. Experimental results under\ndifferent operation conditions show two transport processes limiting the anode\nreaction, one attributed to molecular diffusion through the partially saturated\nGDL, and the other to the microporous layer (MPL), or its interfaces with GDL\nor with the catalyst layer (CL). CH2S provides the hydrogen diffusivities\n$(D_{H2,i})$ associated to each process under the different conditions. Current\ndensity decreases slightly the diffusivity of the GDL, while it becomes\nactivated in the MPL; using two GDLs in the anode improves both GDL and MPL\ndiffusivities; humidification decreases the diffusivity in both, GDL and MPL;\nfinally, a superhydrophobic anodic CL prepared by electrospray improves\nhydrogen diffusivity in GDL and MPL.",
        "Studies by microbiologists from the 1970s provided robust estimates for the\nenergy supply and demand of a prokaryotic cell. The amount of ATP needed to\nsupport growth was calculated from the chemical composition of the cell and\nknown enzymatic pathways that synthesize its constituents from known substrates\nin culture. Starting in 2015, geneticists and evolutionary biologists began\ninvestigating the bioenergetic role of mitochondria at eukaryote origin and\nenergy in metazoan evolution using their own, widely trusted but hitherto\nunvetted model for the costs of growth in terms of ATP per cell. The more\nrecent model contains, however, a severe and previously unrecognized error that\nsystematically overestimates the ATP cost of amino acid synthesis up to 200\nfold. The error applies to all organisms studied by such models and leads to\nconspicuously false inferences, for example that the synthesis of an average\namino acid in humans requires 30 ATP, which no biochemistry textbook will\nconfirm. Their ATP cost calculations would require that Escherichia coli\nobtains roughly 100 ATP per glucose and that mammals obtain roughly 240 ATP per\nglucose, propositions that invalidate evolutionary inferences so based. By\ncontrast, established methods for estimating the ATP cost of microbial growth\nshow that the first mitochondrial endosymbionts could have easily doubled the\nhosts available ATP pool, provided that genes for growth on environmental amino\nacids were transferred from the mitochondrial symbiont to the archaeal host and\nthat the host for mitochondrial origin was an autotroph using the acetyl-CoA\npathway.",
        "We study the question of volume optimality in split conformal regression, a\ntopic still poorly understood in comparison to coverage control. Using the fact\nthat the calibration step can be seen as an empirical volume minimization\nproblem, we first derive a finite-sample upper-bound on the excess volume loss\nof the interval returned by the classical split method. This important quantity\nmeasures the difference in length between the interval obtained with the split\nmethod and the shortest oracle prediction interval. Then, we introduce EffOrt,\na methodology that modifies the learning step so that the base prediction\nfunction is selected in order to minimize the length of the returned intervals.\nIn particular, our theoretical analysis of the excess volume loss of the\nprediction sets produced by EffOrt reveals the links between the learning and\ncalibration steps, and notably the impact of the choice of the function class\nof the base predictor. We also introduce Ad-EffOrt, an extension of the\nprevious method, which produces intervals whose size adapts to the value of the\ncovariate. Finally, we evaluate the empirical performance and the robustness of\nour methodologies.",
        "Current wireless infrastructure is optimized to support downlink\napplications. This paper anticipates the emergence of applications where\nengineering focus shifts from downlink to uplink. The current paradigm of\nscheduling users on reserved uplink resources is not able to deal efficiently\nwith unpredictable traffic patterns. As a result, 3GPP introduced the 2-step\nRACH as a mechanism to enable grant-free (random) initial access. The first of\nthe two steps is preamble detection in a RACH slot, and in this paper we\ndescribe a low-complexity algorithm for simultaneous detection of multiple\npreambles in the presence of mobility and delay spread. We provide a pathway to\nstandards adoption by choosing ZC sequences as preambles, as ZC sequences\nalready appear in 5G standards. We construct preambles by using the discrete\nZak transform to pass from a ZC sequence of length MN in the TD to a\nquasi-periodic MxN array in the DD domain. There are MN quasi-periodic Dirac\npulses, each corresponding to a Zak-OTFS carrier waveform, and the ZC preamble\nis simply the corresponding sum of Zak-OTFS carrier waveforms. We detect\nmultiple preambles in the presence of mobility and delay spread by sampling the\nreceived signal on the MxN period grid in the DD domain. We approach detection\nas a compressed sensing problem. We represent a preamble as a column of length\nMN in the DD domain and apply discrete shifts in delay and Doppler to produce a\nblock with O(MN) columns in the compressed sensing matrix. The superposition of\nmultiple preambles determines a block sparse sum of columns in the sensing\nmatrix. The correlation properties of ZC sequences result in a highly\nstructured compressed sensing matrix, making it possible to identify\nconstituent preambles using OST, which has complexity O(M^3N^3). In this paper,\nwe describe an algorithm with complexity that is O(M^2N^2) in the size of an\nindividual column.",
        "Fast Radio Bursts (FRBs) have emerged as one of the most dynamic areas of\nresearch in astronomy and cosmology. Despite increasing number of FRBs have\nbeen reported, the exact origin of FRBs remains elusive. Investigating the\nintrinsic distributions of FRBs could provide valuable insights into their\npossible origins and enhance the power of FRBs a cosmological probe. In this\npaper, we propose a hierarchical Bayesian inference approach combining with\nseveral viable models to investigate the population information of FRBs\nreleased in the CHIME catalog 1. By utilizing this method, we aim to uncover\nthe underlying patterns and characteristics of the FRB population. Taking into\naccount the uncertainties and complex relationships within the data. We find\nthat the distribution of FRBs does not trace the history of star formation, and\nthere is evidence that the FRB population has time delay with respect to the\nhistory of star formation.",
        "Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.",
        "Quantum Reinforcement Learning (QRL) has emerged as a promising research\nfield, leveraging the principles of quantum mechanics to enhance the\nperformance of reinforcement learning (RL) algorithms. However, despite its\ngrowing interest, QRL still faces significant challenges. It is still uncertain\nif QRL can show any advantage over classical RL beyond artificial problem\nformulations. Additionally, it is not yet clear which streams of QRL research\nshow the greatest potential. The lack of a unified benchmark and the need to\nevaluate the reliance on quantum principles of QRL approaches are pressing\nquestions. This work aims to address these challenges by providing a\ncomprehensive comparison of three major QRL classes: Parameterized Quantum\nCircuit based QRL (PQC-QRL) (with one policy gradient (QPG) and one Q-Learning\n(QDQN) algorithm), Free Energy based QRL (FE-QRL), and Amplitude Amplification\nbased QRL (AA-QRL). We introduce a set of metrics to evaluate the QRL\nalgorithms on the widely applicable benchmark of gridworld games. Our results\nprovide a detailed analysis of the strengths and weaknesses of the QRL classes,\nshedding light on the role of quantum principles in QRL and paving the way for\nfuture research in this field.",
        "Instead of computing magneticallly induced (MI) current densities (CD) via\nthe wave function and their quatum mechanical definition one can also use the\ndifferential form of the Amp\\`ere-Maxwell law to obtain them from spatial\nderivatives of the induced magnetic field. In magnetic molecular response\ncalculations, the latter can be done by numerical derivativation of the so\ncalled ``nucleus-independent chemical shifts'' (NICS) which are avaialable to\nmany standard quantum chemical programs. The resulting numerical MICD data is\nin contrast to other numerically obtained MICDs computed via the wave function\nroute, virtually divergence-free.",
        "Memory effect reflects a system's ability to encode, retain and retrieve\ninformation about its past. Such effects are essentially an out-of-equilibrium\nphenomenon providing insight into the complex structural and dynamical behavior\nof the system. Kovacs effect is one such memory effect that is traditionally\nassociated with thermal history. Although studies on the Kovacs-like memory\neffect have been extended to mechanical perturbations such as\ncompression-decompression, whether such effects can also be observed under\nvolume-conserving perturbations like shear, remains unclear. Combining\nexperiments, simulations and linear response theory we demonstrate Kovacs-like\nmemory effect in a sheared colloidal glass. Moreover, we explore the influence\nof non-linear perturbations and establish a correlation between the deviation\nfrom linear response prediction and microscopic non-affine flows generated due\nto such large deformations in affecting the memory effect. Our study not only\nextends Kovacs-like memory effect in the domain of volume-conserving mechanical\nperturbations, it also highlights the importance of the nature of underlying\nmicroscopic flows in controlling the memory effect in amorphous matter.",
        "Understanding convergent learning -- the extent to which artificial and\nbiological neural networks develop similar representations -- is crucial for\nneuroscience and AI, as it reveals shared learning principles and guides\nbrain-like model design. While several studies have noted convergence in early\nand late layers of vision networks, key gaps remain. First, much existing work\nrelies on a limited set of metrics, overlooking transformation invariances\nrequired for proper alignment. We compare three metrics that ignore specific\nirrelevant transformations: linear regression (ignoring affine\ntransformations), Procrustes (ignoring rotations and reflections), and\npermutation\/soft-matching (ignoring unit order). Notably, orthogonal\ntransformations align representations nearly as effectively as more flexible\nlinear ones, and although permutation scores are lower, they significantly\nexceed chance, indicating a robust representational basis. A second critical\ngap lies in understanding when alignment emerges during training. Contrary to\nexpectations that convergence builds gradually with task-specific learning, our\nfindings reveal that nearly all convergence occurs within the first epoch --\nlong before networks achieve optimal performance. This suggests that shared\ninput statistics, architectural biases, or early training dynamics drive\nconvergence rather than the final task solution. Finally, prior studies have\nnot systematically examined how changes in input statistics affect alignment.\nOur work shows that out-of-distribution (OOD) inputs consistently amplify\ndifferences in later layers, while early layers remain aligned for both\nin-distribution and OOD inputs, suggesting that this alignment is driven by\ngeneralizable features stable across distribution shifts. These findings fill\ncritical gaps in our understanding of representational convergence, with\nimplications for neuroscience and AI.",
        "Caribou is a versatile data acquisition system used in multiple collaborative\nframeworks (CERN EP R&D, DRD3, AIDAinnova, Tangerine) for laboratory and\ntest-beam qualification of novel silicon pixel detector prototypes. The system\nis built around a common hardware, firmware and software stack shared accross\ndifferent projects, thereby drastically reducing the development effort and\ncost. It consists of a custom Control and Readout (CaR) board and a commercial\nXilinx Zynq System-on-Chip (SoC) platform. The SoC platform runs a full Yocto\ndistribution integrating the custom software framework (Peary) and a custom\nFPGA firmware built within a common firmware infrastructure (Boreal). The CaR\nboard provides a hardware environment featuring various services such as\npowering, slow-control, and high-speed data links for the target detector\nprototype. Boreal and Peary, in turn, offer firmware and software architectures\nthat enable seamless integration of control and readout for new devices. While\nthe first version of the system used a SoC platform based on the ZC706\nevaluation board, migration to a Zynq UltraScale+ architecture is progressing\ntowards the support of the ZCU102 board and the ultimate objective of\nintegrating the SoC functionality directly into the CaR board, eliminating the\nneed for separate evaluation boards. This paper describes the Caribou system,\nfocusing on the latest project developments and showcasing progress and future\nplans across its hardware, firmware, and software components.",
        "Modern quantum technologies and hybrid quantum systems offer the opportunity\nto utilize magnons on the level of single excitations. Long lifetimes, low\ndecoherence rates, and a strong coupling rate to other subsystems propose the\nferrimagnet yttrium iron garnet (YIG), grown on a gadolinium gallium garnet\n(GGG) substrate, as a suitable platform to host magnonic quantum states.\nHowever, the magnetic damping at cryogenic temperatures significantly increases\ndue to the paramagnetic character and the highly inhomogeneous stray field of\nGGG, as recent experiments and simulations pointed out. Here, we report on\ntemperature dependent ferromagnetic resonance (FMR) spectroscopy studies in\nYIG-GGG thin-films with different sample geometries. We experimentally\ndemonstrate how to eliminate the asymmetric stray field-induced linewidth\nbroadening via microstructuring of the YIG film. Additionally, our experiments\nreveal evidence of a non-Gilbert like behavior of the linewidth at cryogenic\ntemperatures, independent of the inhomogeneous GGG stray field.",
        "Optical nuclear electric resonance (ONER), a recently proposed protocol for\nnuclear spin manipulation in atomic systems via short laser pulses with MHz\nrepetition rate, exploits the coupling between the nuclear quadrupole moment of\na suitable atom and the periodic modulations of the electric field gradient\ngenerated by an optically stimulated electronic excitation. In this theory\npaper, we extend the scope of ONER from atomic to molecular systems and show\nthat molecular vibrations do not interfere with our protocol. Exploring the\ndiatomic molecule LiNa as a first benchmark system, our investigation showcases\nthe robustness with respect to molecular vibration, and the ability to address\nand manipulate each of the two nuclear spins independently, simply by adjusting\nthe repetition rate of a pulsed laser. Our findings suggest that it might be\npossible to shift complicated spin manipulation tasks required for quantum\ncomputing into the time domain by pulse-duration encoded laser signals.",
        "Uncertainty quantification in neural networks through methods such as\nDropout, Bayesian neural networks and Laplace approximations is either prone to\nunderfitting or computationally demanding, rendering these approaches\nimpractical for large-scale datasets. In this work, we address these\nshortcomings by shifting the focus from uncertainty in the weight space to\nuncertainty at the activation level, via Gaussian processes. More specifically,\nwe introduce the Gaussian Process Activation function (GAPA) to capture\nneuron-level uncertainties. Our approach operates in a post-hoc manner,\npreserving the original mean predictions of the pre-trained neural network and\nthereby avoiding the underfitting issues commonly encountered in previous\nmethods. We propose two methods. The first, GAPA-Free, employs empirical kernel\nlearning from the training data for the hyperparameters and is highly efficient\nduring training. The second, GAPA-Variational, learns the hyperparameters via\ngradient descent on the kernels, thus affording greater flexibility. Empirical\nresults demonstrate that GAPA-Variational outperforms the Laplace approximation\non most datasets in at least one of the uncertainty quantification metrics.",
        "Examining the extent to which measurements of rotation matrices are close to\neach other is challenging due measurement noise. To overcome this, data is\ntypically smoothed and Riemannian and Euclidean metrics are applied. However,\nif rotation matrices are not directly measured and are instead formed by\neigenvectors of measured symmetric matrices, this can be problematic if the\nassociated eigenvalues are close. In this work, we propose novel semi-metrics\nthat can be used to approximate the Riemannian metric for small angles. Our new\nresults do not require eigenvector information and are beneficial for measured\ndatasets. There are also issues when using comparing rotational data arising\nfrom computational simulations and it is important that the impact of the\napproximations on the computed outputs is properly assessed to ensure that the\napproximations made and the finite precision arithmetic are not unduly\npolluting the results. In this work, we examine data arising from object\ncharacterisation in metal detection using the complex symmetric rank two\nmagnetic polarizability tensor (MPT) description, we rigorously analyse the\neffects of our numerical approximations and apply our new approximate measures\nof distance to the commutator of the real and imaginary parts of the MPT to\nthis application. Our new approximate measures of distance provide additional\nfeature information, which is invariant of the object orientation, to aid with\nobject identification using machine learning classifiers. We present Bayesian\nclassification examples to demonstrate the success of our approach.",
        "Let $S_n=K[x_1,\\ldots,x_n,y]$ and $I_n=(x_1y,x_2y,\\ldots,x_ny)\\subset S_n$ be\nthe edge ideal of star graph. We prove that $\\operatorname{hdepth}(S_n\/I_n)\\geq\n\\left\\lceil \\frac{n}{2} \\right\\rceil + \\left\\lfloor \\sqrt{n} \\right\\rfloor -\n2$. Also, we show that for any $\\varepsilon>0$, there exists some integer\n$A=A(\\varepsilon)\\geq 0$ such that $\\operatorname{hdepth}(S_n\/I_n)\\leq\n\\left\\lceil \\frac{n}{2} \\right\\rceil + \\left\\lfloor \\varepsilon n \\right\\rfloor\n+ A - 2$. We deduce that $\\lim\\limits_{n\\to\\infty}\n\\frac{1}{n}\\operatorname{hdepth}(S_n\/I_n) = \\frac{1}{2}$."
      ]
    }
  },
  {
    "id":2411.19345,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"On the use of preclinical imaging to explore the principles of brain function in rodent models and their relevance for illnesses of the human mind",
    "start_abstract":"Dear Editor, We recently published in Translational Psychiatry an article that examine the strategies for evaluating brain function at the wholebrain level [1]. In this review, we covered several methods, from functional MRI to functional ultrasound to calcium imaging. For each technique, we wrote a brief history of its development, the physical notion, some key applications, its potentials, and its limitations. We concluded that methods for imaging the rodent brain at the network level are growing and will advance our understanding of brain function. A commentary by Zhuo and colleagues further enhances the complexity of addressing the issue of \u201ctranslation\u201d from animal models to patients for the discipline of psychiatry [2]. They propose that the approaches employed to develop an animal model for a psychiatric disease need to be thoroughly scrutinized and, perhaps, revised. For example, most rodent models of mental diseases are to-date established using a simple pharmacological infusion [3] and\/or psychosocial stimulation [4]. The key concern posed, however, is how these manipulations change the brain\u2019s structure and function, and whether these models genuinely reflect the pathophysiology of human mental illnesses. Especially since it is difficult to evaluate whether one can speak of inverse inference from rodents to humans. This is a true and acceptable statement. However, this is exactly what preclinical imaging aims to deliver. By mapping the dynamic responses of brain networks in animal models and compare them, if possible, with those reported in clinical studies, we can obtain quantitative data and parameters to establish whether our models are effectively translational [5]. If these metrics demonstrate temporal and spatial similarity in network-level modifications as those observed in humans, we can pursue further inquiry utilizing more intrusive and more specific methods for brain recordings in animal models. Otherwise, we must have the confidence and the correctness to move forward and attempt other solutions. Two recent examples. In 2019 we established a causal association between activity of the noradrenergic nucleus locus coeruleus (LC) and the engagement of numerous large-scale brain networks in mice, in particular of the salience and amygdala networks [6]. In addition, we could link network-changes with direct markers of norepinephrine (NE) turnover and with the distribution of NE receptors over the entire brain. The hypothesis that specific brain networks dynamics are related to LC activity and to NE receptor density derives from stress-research and pharmacological studies in humans [7, 8]. However, since it is impossible to selectively stimulate LC in people, it has remained a hypothesis for more than a decade. Our preclinical work helped confirm this causal relationship and this has direct implications for interpreting the results of clinical imaging studies on stress and anxiety behavior. More recently, the Gozzi lab described how chronic local neuronal suppression via overexpression of a potassium channel or acute silencing via chemogenetics result in a paradoxical hyperconnectivity [9]; an intriguing finding often reported in humans after stroke [10] and in early stages of Alzheimer\u2019s disease [11], but never truly understood. Using in vivo electrophysiology, they showed local inhibition improves low frequency (0.1\u20134 Hz) oscillatory power via suppression of neuronal activity not phaselocked to slow rhythms, resulting in increased slow and \u03b4 band coherence between areas that display fMRI overconnectivity. These data present causal evidence that cortical inactivation can counterintuitively augment fMRI connectivity via greater, lesslocalized slow oscillatory processes. Once again, this could be only achieved by combining functional MRI and electrophysiology with neuromodulation in animal models. These and other examples give a peek of what the future of preclinical imaging might look like: a field of research capable of delivering causal explanations to the hypotheses presented by human neuroscience, neurology and psychiatry. Lastly, I would argue against statements like \u201cthe computational complexity of human brains is billions of times that of mouse brain\u201d. While this may be true from a numerical standpoint of mere neuronal counts, preclinical neuroimaging\u2019s objective should not be per se to map every single neuron in real time but of identifying the general neural and cellular principles governing the assembly of brain networks and its breakdown in brain disorders. The field is relatively new but is moving fast and has already produced some important insights. The future is challenging and will require time, devotion and an optimal synergy between engineering, chemistry, biology, and computer science. If the community will be patient and supportive enough, there will be further important discoveries in the future.",
    "start_categories":[
      "Psychiatry"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "GAN\uff08Generative Adversarial Nets\uff09"
      ],
      "abstract":[
        "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: model G that captures the data distribution, and discriminative D estimates probability sample came from training rather than G. The procedure is to maximize of making mistake. This corresponds minimax two-player game. In space arbitrary functions D, unique solution exists, with recovering distribution equal \u00bd everywhere. case where are defined by multilayer perceptrons, entire system can be trained backpropagation. There no need any Markov chains or unrolled approximate inference networks during either generation samples. Experiments demonstrate potential through qualitative quantitative evaluation generated"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Multi-messenger observations in the Einstein Telescope era: binary\n  neutron star and black hole - neutron star mergers",
        "On invex functions with same {\\eta} in single and multivalued nonsmooth\n  optimization with Clarke's subdifferential",
        "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
        "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set",
        "Emulating Retrieval Augmented Generation via Prompt Engineering for\n  Enhanced Long Context Comprehension in LLMs",
        "Flexible and Efficient Grammar-Constrained Decoding",
        "Efficient Risk-sensitive Planning via Entropic Risk Measures",
        "CER: Confidence Enhanced Reasoning in LLMs",
        "Quasimodular Asymptotics of Spherical Integrals",
        "An Improved Deep Learning Model for Word Embeddings Based Clustering for\n  Large Text Datasets",
        "Transformer-Based Nonlinear Transform Coding for Multi-Rate CSI\n  Compression in MIMO-OFDM Systems",
        "ACF-Monotonicity Formula on RCD(0,N) Metric Measure Cones",
        "Generative AI for Vision: A Comprehensive Study of Frameworks and\n  Applications",
        "From Natural Language to Extensive-Form Game Representations",
        "Spherically symmetric electrically counterpoised dust either collapses\n  or disperses",
        "Sensing-based Robustness Challenges in Agricultural Robotic Harvesting",
        "Dictionary-Learning-Based Data Pruning for System Identification",
        "Initial Guess Generation for Low-Thrust Trajectory Design with\n  Robustness to Missed-Thrust-Events",
        "Machine Learning for Health symposium 2024 -- Findings track",
        "Quantum Regular Black Holes and Complete Monotonicity",
        "Encrypted Qubits can be Cloned",
        "On Some Fundamental Problems for Multi-Agent Systems Over Multilayer\n  Networks",
        "Semantic Shift Estimation via Dual-Projection and Classifier\n  Reconstruction for Exemplar-Free Class-Incremental Learning",
        "FloPE: Flower Pose Estimation for Precision Pollination",
        "Ancient Greek Technology: An Immersive Learning Use Case Described Using\n  a Co-Intelligent Custom ChatGPT Assistant",
        "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
        "A search for close binary systems in the SALT survey of\n  hydrogen-deficient stars using {\\it TESS}",
        "Spectral properties of bottomonium at high temperature: a systematic\n  investigation",
        "Radii of light nuclei from the Jacobi No-Core Shell Model"
      ],
      "abstract":[
        "The Einstein Telescope (ET), a proposed next-generation gravitational wave\n(GW) observatory, will expand the reach of GW astronomy of stellar-mass compact\nobject binaries to unprecedented distances, enhancing opportunities for\nmulti-messenger observations. Here we investigate multi-messenger emission\nproperties of binary neutron star (NSNS) and black hole-neutron star (BHNS)\nmergers detectable by ET, providing projections to optimize observational\nstrategies and maximize scientific insights from these sources. Using a\nsynthetic population of compact binary mergers, we model each source's GW\nsignal-to-noise ratio, sky localization uncertainty, kilonova (KN) light curves\nin optical and near-infrared bands, fluence of the relativistic jet gamma-ray\nburst (GRB) prompt emission and afterglow light curves across radio, optical,\nX-ray and very high energy wavelengths. We analyze multi-messenger\ndetectability prospects for ET as a standalone observatory with two different\nconfigurations and within a network of next-generation GW detectors. ET will\ndetect over $10^4$ NSNS mergers annually, enabling potential observation of\ntens to hundreds of electromagnetic (EM) counterparts. BHNS mergers have more\nlimited multi-messenger prospects, but joint GW-EM rates will increase by an\norder of magnitude compared to current-generation instruments. We quantify\nuncertainties due to the NS equation of state (EoS) and mass distribution of\nNSNSs, as well as the NS EoS and BH spin for BHNSs. While a single ET will\nachieve an impressive GW detection rate, the fraction of well-localized events\nis orders of magnitude lower than in a network with additional detectors. This\nsignificantly limits efficient EM follow-up and science cases requiring\nwell-characterized counterparts or early observations. The challenge is even\ngreater for BHNS mergers due to their low EM rate.",
        "In this paper, a finite family of nonsmooth locally Lipschitz continuous\nfunctions that are invex with respect to the same function {\\eta} are\ncharacterized in terms of their scalarized counterparts.",
        "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https:\/\/hong-yu-zhang.github.io\/MagicComp-Page\/.",
        "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
        "This paper addresses the challenge of comprehending very long contexts in\nLarge Language Models (LLMs) by proposing a method that emulates Retrieval\nAugmented Generation (RAG) through specialized prompt engineering and\nchain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens\nin a single prompt, simply enlarging context windows has not guaranteed robust\nmulti-hop reasoning when key details are scattered across massive input. Our\napproach treats the model as both the retriever and the reasoner: it first tags\nrelevant segments within a long passage, then employs a stepwise CoT workflow\nto integrate these pieces of evidence. This single-pass method thereby reduces\nreliance on an external retriever, yet maintains focus on crucial segments. We\nevaluate our approach on selected tasks from BABILong, which interleaves\nstandard bAbI QA problems with large amounts of distractor text. Compared to\nbaseline (no retrieval) and naive RAG pipelines, our approach more accurately\nhandles multi-fact questions such as object location tracking, counting, and\nindefinite knowledge. Furthermore, we analyze how prompt structure, including\nthe order of question, relevant-text tags, and overall instructions,\nsignificantly affects performance. These findings underscore that optimized\nprompt engineering, combined with guided reasoning, can enhance LLMs'\nlong-context comprehension and serve as a lightweight alternative to\ntraditional retrieval pipelines.",
        "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.",
        "Risk-sensitive planning aims to identify policies maximizing some\ntail-focused metrics in Markov Decision Processes (MDPs). Such an optimization\ntask can be very costly for the most widely used and interpretable metrics such\nas threshold probabilities or (Conditional) Values at Risk. Indeed, previous\nwork showed that only Entropic Risk Measures (EntRM) can be efficiently\noptimized through dynamic programming, leaving a hard-to-interpret parameter to\nchoose. We show that the computation of the full set of optimal policies for\nEntRM across parameter values leads to tight approximations for the metrics of\ninterest. We prove that this optimality front can be computed effectively\nthanks to a novel structural analysis and smoothness properties of entropic\nrisks. Empirical results demonstrate that our approach achieves strong\nperformance in a variety of decision-making scenarios.",
        "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps:\/\/github.com\/ Aquasar11\/CER.",
        "We show that the spherical integral of the Circular Unitary Ensemble\nconverges in expectation to Euler's generating function for integer partitions,\nand that subleading corrections to this high-dimensional limit are quasimodular\nforms.",
        "In this paper, an improved clustering technique for large textual datasets by\nleveraging fine-tuned word embeddings is presented. WEClustering technique is\nused as the base model. WEClustering model is fur-ther improvements\nincorporating fine-tuning contextual embeddings, advanced dimensionality\nreduction methods, and optimization of clustering algorithms. Experimental\nresults on benchmark datasets demon-strate significant improvements in\nclustering metrics such as silhouette score, purity, and adjusted rand index\n(ARI). An increase of 45% and 67% of median silhouette score is reported for\nthe proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based\non Agglomerative models), respec-tively. The proposed technique will help to\nbridge the gap between semantic understanding and statistical robustness for\nlarge-scale text-mining tasks.",
        "We propose a novel approach for channel state information (CSI) compression\nin multiple-input multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) systems, where the frequency-domain channel matrix is treated as a\nhigh-dimensional complex-valued image. Our method leverages transformer-based\nnonlinear transform coding (NTC), an advanced deep-learning-driven image\ncompression technique that generates a highly compact binary representation of\nthe CSI. Unlike conventional autoencoder-based CSI compression, NTC optimizes a\nnonlinear mapping to produce a latent vector while simultaneously estimating\nits probability distribution for efficient entropy coding. By exploiting the\nstatistical independence of latent vector entries, we integrate a\ntransformer-based deep neural network with a scalar nested-lattice uniform\nquantization scheme, enabling low-complexity, multi-rate CSI feedback that\ndynamically adapts to varying feedback channel conditions. The proposed\nmulti-rate CSI compression scheme achieves state-of-the-art rate-distortion\nperformance, outperforming existing techniques with the same number of neural\nnetwork parameters. Simulation results further demonstrate that our approach\nprovides a superior rate-distortion trade-off, requiring only 6% of the neural\nnetwork parameters compared to existing methods, making it highly efficient for\npractical deployment.",
        "The ACF-monotonicity formula is a powerful tool in the study of two-phase\nfree boundary problems, which was introduced by Alt, Caffarelli, and\nFriedman[1]. In this paper, we extend it to RCD(0,N) metric measure cones. As\nan application, we give a rigidity result for RCD(0,N) metric measure cones.",
        "Generative AI is transforming image synthesis, enabling the creation of\nhigh-quality, diverse, and photorealistic visuals across industries like\ndesign, media, healthcare, and autonomous systems. Advances in techniques such\nas image-to-image translation, text-to-image generation, domain transfer, and\nmultimodal alignment have broadened the scope of automated visual content\ncreation, supporting a wide spectrum of applications. These advancements are\ndriven by models like Generative Adversarial Networks (GANs), conditional\nframeworks, and diffusion-based approaches such as Stable Diffusion. This work\npresents a structured classification of image generation techniques based on\nthe nature of the input, organizing methods by input modalities like noisy\nvectors, latent representations, and conditional inputs. We explore the\nprinciples behind these models, highlight key frameworks including DALL-E,\nControlNet, and DeepSeek Janus-Pro, and address challenges such as\ncomputational costs, data biases, and output alignment with user intent. By\noffering this input-centric perspective, this study bridges technical depth\nwith practical insights, providing researchers and practitioners with a\ncomprehensive resource to harness generative AI for real-world applications.",
        "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.",
        "We explore the dynamical evolution of spherically symmetric objects made of\nelectrically counterpoised dust in general relativity. It has been claimed that\nthese objects are in neutral equilibrium and, therefore, that black hole\nmimickers made of electrically counterpoised dust are feasible. Here we show\nthat if a velocity is imparted to the fluid elements, no matter how small, the\nevolution leads either to a black hole or to the dispersion of the fluid.\nFurthermore, in the case of collapse, the resulting object is necessarily an\nextremal Reissner-Nordstr\\\"om black hole.",
        "This paper presents the challenges agricultural robotic harvesters face in\ndetecting and localising fruits under various environmental disturbances. In\ncontrolled laboratory settings, both the traditional HSV (Hue Saturation Value)\ntransformation and the YOLOv8 (You Only Look Once) deep learning model were\nemployed. However, only YOLOv8 was utilised in outdoor experiments, as the HSV\ntransformation was not capable of accurately drawing fruit contours.\nExperiments include ten distinct fruit patterns with six apples and six\noranges. A grid structure for homography (perspective) transformation was\nemployed to convert detected midpoints into 3D world coordinates. The\nexperiments evaluated detection and localisation under varying lighting and\nbackground disturbances, revealing accurate performance indoors, but\nsignificant challenges outdoors. Our results show that indoor experiments using\nYOLOv8 achieved 100% detection accuracy, while outdoor conditions decreased\nperformance, with an average accuracy of 69.15% for YOLOv8 under direct\nsunlight. The study demonstrates that real-world applications reveal\nsignificant limitations due to changing lighting, background disturbances, and\ncolour and shape variability. These findings underscore the need for further\nrefinement of algorithms and sensors to enhance the robustness of robotic\nharvesters for agricultural use.",
        "System identification is normally involved in augmenting time series data by\ntime shifting and nonlinearisation (via polynomial basis), which introduce\nredundancy both feature-wise and sample-wise. Many research works focus on\nreducing redundancy feature-wise, while less attention is paid to sample-wise\nredundancy. This paper proposes a novel data pruning method, called\n(mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary\nlearning. Time series data is represented by some representative samples,\ncalled atoms, via dictionary learning. The useful samples are selected based on\ntheir correlation with the atoms. The method is tested on one simulated dataset\nand two benchmark datasets. The R-squared between the coefficients of models\ntrained on the full and the coefficients of models trained on pruned datasets\nis adopted to evaluate the performance of data pruning methods. It is found\nthat the proposed method significantly outperforms the random pruning method.",
        "The growing interest in cislunar space exploration in recent years has driven\nan increasing demand for efficient low-thrust missions to key cislunar orbits.\nThese missions, typically possessing long thrust arcs, are particularly\nsusceptible to operational uncertainties such as missed thrust events.\nAddressing these challenges requires efficient robust trajectory design\nframeworks during the preliminary mission design phase, where it is necessary\nto explore the solution space at a rapid cadence under evolving operational\nconstraints. However, existing methods for missed thrust design rely on solving\nhigh-dimensional nonlinear programs, where generating effective initial guesses\nbecomes challenging. To enhance computational efficiency, quality, and depth of\nrobustness of solutions from global search, we compare two initial guess\nstrategies: a baseline non-conditional global search, which samples from a\nstatic distribution with global support, and a conditional global search, which\ngenerates initial guesses conditioned on solutions to problems with less depth\nof robustness. The conditional search provides a sequential procedure for\nsolving increasingly robust problems. We validate the improvements in the\nconditional approach using a low-thrust case study for the Lunar Gateway Power\nand Propulsion Element, where our results demonstrate that it significantly\nimproves convergence rate and solution quality, highlighting its potential in\npreliminary robust trajectory design.",
        "A collection of the accepted Findings papers that were presented at the 4th\nMachine Learning for Health symposium (ML4H 2024), which was held on December\n15-16, 2024, in Vancouver, BC, Canada. ML4H 2024 invited high-quality\nsubmissions describing innovative research in a variety of health-related\ndisciplines including healthcare, biomedicine, and public health. Works could\nbe submitted to either the archival Proceedings track, or the non-archival\nFindings track. The Proceedings track targeted mature, cohesive works with\ntechnical sophistication and high-impact relevance to health. The Findings\ntrack promoted works that would spark new insights, collaborations, and\ndiscussions at ML4H. Both tracks were given the opportunity to share their work\nthrough the in-person poster session. All the manuscripts submitted to ML4H\nSymposium underwent a double-blind peer-review process.",
        "We examine the conjecture for the complete monotonicity of certain curvature\ninvariants for quantum black holes. In this note, we study a class of quantum\nregular black holes that are static, spherically symmetric, and characterized\nonly by their mass. Additionally, this class of black holes reduces to the\nSchwarzschild solution in the classical limit $\\hbar\\to 0$. We provide evidence\nsupporting the non-perturbativity conjecture that perturbative corrections\ncannot falsify complete monotonicity. We demonstrate that these quantum black\nholes cannot be generated by perturbative quantum corrections to the Einstein\nequations. We then investigate the thermodynamics of these black holes and\nderive a bound on their entropy, showing that the entropy is always greater\nthan the horizon area divided by 4G. We Also demonstrate that these black holes\nexhibit a bounded temperature, with a maximum temperature scaling as\n$T\\sim\\frac{1}{L_p}$ and a critical mass scale where the temperature vanishes",
        "We show that encrypted cloning of unknown quantum states is possible. Any\nnumber of encrypted clones of a qubit can be created through a unitary\ntransformation, and each of the encrypted clones can be decrypted through a\nunitary transformation. The decryption of an encrypted clone consumes the\ndecryption key, i.e., only one decryption is possible, in agreement with the\nno-cloning theorem. A possible application of encrypted cloning is to enable\nencrypted quantum multi-cloud storage. Beyond applications in cryptography,\nencrypted cloning could provide a form of redundancy, parallelism, fault\ntolerance or scalability where direct duplication is forbidden by the\nno-cloning theorem.",
        "Many researchers have considered multi-agent systems over single-layer\nnetworks as models for studying diffusion phenomena. Since real-world networks\ninvolve connections between agents with different semantics (e.g., family\nmember, friend, colleague), the study of multi-agent systems over multilayer\nnetworks has assumed importance. Our focus is on one class of multi-agent\nsystem models over multilayer networks, namely multilayer synchronous dynamical\nsystems (MSyDSs). We study several fundamental problems for this model. We\nestablish properties of the phase spaces of MSyDSs and bring out interesting\ndifferences between single-layer and multilayer dynamical systems. We show\nthat, in general, the problem of determining whether two given MSyDSs are\ninequivalent is NP-complete. This hardness result holds even when the only\ndifference between the two systems is the local function at just one node in\none layer. We also present efficient algorithms for the equivalence problem for\nrestricted versions of MSyDSs (e.g., systems where each local function is a\nbounded-threshold function, systems where the number of layers is fixed and\neach local function is symmetric). In addition, we investigate the expressive\npower of MSyDSs based on the number of layers. In particular, we examine\nconditions under which a system with k >= 2 layers has an equivalent system\nwith k-1 or fewer layers.",
        "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods.",
        "This study presents Flower Pose Estimation (FloPE), a real-time flower pose\nestimation framework for computationally constrained robotic pollination\nsystems. Robotic pollination has been proposed to supplement natural\npollination to ensure global food security due to the decreased population of\nnatural pollinators. However, flower pose estimation for pollination is\nchallenging due to natural variability, flower clusters, and high accuracy\ndemands due to the flowers' fragility when pollinating. This method leverages\n3D Gaussian Splatting to generate photorealistic synthetic datasets with\nprecise pose annotations, enabling effective knowledge distillation from a\nhigh-capacity teacher model to a lightweight student model for efficient\ninference. The approach was evaluated on both single and multi-arm robotic\nplatforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees\nwithin a low computational cost. Our experiments validate the effectiveness of\nFloPE, achieving up to 78.75% pollination success rate and outperforming prior\nrobotic pollination techniques.",
        "Achieving consistency in immersive learning case descriptions is essential\nbut challenging due to variations in research focus, methodology, and\nresearchers' background. We address these challenges by leveraging the\nImmersive Learning Case Sheet (ILCS), a methodological instrument to\nstandardize case descriptions, that we applied to an immersive learning case on\nancient Greek technology in VRChat. Research team members had differing levels\nof familiarity with the ILCS and the case content, so we developed a custom\nChatGPT assistant to facilitate consistent terminology and process alignment\nacross the team. This paper constitutes an example of how structured case\nreports can be a novel contribution to immersive learning literature. Our\nfindings demonstrate how the ILCS supports structured reflection and\ninterpretation of the case. Further we report that the use of a ChatGPT\nassistant significantly sup-ports the coherence and quality of the team members\ndevelopment of the final ILCS. This exposes the potential of employing\nAI-driven tools to enhance collaboration and standardization of research\npractices in qualitative educational research. However, we also discuss the\nlimitations and challenges, including reliance on AI for interpretive tasks and\nmanaging varied levels of expertise within the team. This study thus provides\ninsights into the practical application of AI in standardizing immersive\nlearning research processes.",
        "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
        "The {\\it TESS} periodograms of the SALT survey catalogue of\nhydrogen-deficient stars were searched for evidence of short-period\nvariability. Periodic light curve variations were identified in 16 stars out of\n153 catalogue objects, of which 10 were false positives. From the remaining 6\nidentified variables, Ton S 415 is a known close binary system and the sixth\nclose binary containing a hydrogen-deficient hot subdwarf. Radial velocity and\nSED analyses ruled out the remaining 5 as close binary systems; the causes of\ntheir variability remain uncertain. With one or more K-type companions, BPS CS\n22956-0094 may be a wide binary or triple. From this SALT+{\\it TESS} sample,\nthe fraction of close binaries stands at $ 1\/29 \\approx 3.5\\%$ for intermediate\nhelium hot subdwarfs and $0\/124 = 0\\%$ for extreme helium subdwarfs.",
        "We investigate spectral features of bottomonium at high temperature, in\nparticular the thermal mass shift and width of ground state S-wave and P-wave\nstate. We employ and compare a range of methods for determining these features\nfrom lattice NRQCD correlators, including direct correlator analyses\n(multi-exponential fits and moments of spectral functions), linear methods\n(Backus-Gilbert, Tikhonov and HLT methods), and Bayesian methods for spectral\nfunction reconstruction (MEM and BR). We comment on the reliability and\nlimitations of the various methods.",
        "Accurately determining the size of the atomic nucleus with realistic nuclear\nforces is a long outstanding issue of nuclear physics. The no-core shell model\n(NCSM), one of the powerful ab initio methods for nuclear structure, can\nachieve accurate energies of light nuclei. The extraction of converged radii is\nmore difficult. In this work, we present a novel method to effectively extract\nthe radius of light nuclei by restoring the long-range behavior of densities\nfrom NCSM calculations. The correct large distance asymptotic of two-body\nrelative densities are deduced based on the NCSM densities in limited basis\nsize. The resulting radii using the corrected densities show a nice\nconvergence. The root-mean-square matter and charge radii of $^{4,6,8}$He and\n$^{6,7,8}$Li can be accurately obtained based on Jacobi-NCSM calculations with\nthe high-precision chiral two-nucleon and three-nucleon forces combined with\nthis new method. Our method can be straightforwardly extended to other ab\ninitio calculations, potentially providing a better description of nuclear\nsizes with realistic nuclear forces."
      ]
    }
  },
  {
    "id":2411.19345,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"GAN\uff08Generative Adversarial Nets\uff09",
    "start_abstract":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: model G that captures the data distribution, and discriminative D estimates probability sample came from training rather than G. The procedure is to maximize of making mistake. This corresponds minimax two-player game. In space arbitrary functions D, unique solution exists, with recovering distribution equal \u00bd everywhere. case where are defined by multilayer perceptrons, entire system can be trained backpropagation. There no need any Markov chains or unrolled approximate inference networks during either generation samples. Experiments demonstrate potential through qualitative quantitative evaluation generated",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "On the use of preclinical imaging to explore the principles of brain function in rodent models and their relevance for illnesses of the human mind"
      ],
      "abstract":[
        "Dear Editor, We recently published in Translational Psychiatry an article that examine the strategies for evaluating brain function at the wholebrain level [1]. In this review, we covered several methods, from functional MRI to functional ultrasound to calcium imaging. For each technique, we wrote a brief history of its development, the physical notion, some key applications, its potentials, and its limitations. We concluded that methods for imaging the rodent brain at the network level are growing and will advance our understanding of brain function. A commentary by Zhuo and colleagues further enhances the complexity of addressing the issue of \u201ctranslation\u201d from animal models to patients for the discipline of psychiatry [2]. They propose that the approaches employed to develop an animal model for a psychiatric disease need to be thoroughly scrutinized and, perhaps, revised. For example, most rodent models of mental diseases are to-date established using a simple pharmacological infusion [3] and\/or psychosocial stimulation [4]. The key concern posed, however, is how these manipulations change the brain\u2019s structure and function, and whether these models genuinely reflect the pathophysiology of human mental illnesses. Especially since it is difficult to evaluate whether one can speak of inverse inference from rodents to humans. This is a true and acceptable statement. However, this is exactly what preclinical imaging aims to deliver. By mapping the dynamic responses of brain networks in animal models and compare them, if possible, with those reported in clinical studies, we can obtain quantitative data and parameters to establish whether our models are effectively translational [5]. If these metrics demonstrate temporal and spatial similarity in network-level modifications as those observed in humans, we can pursue further inquiry utilizing more intrusive and more specific methods for brain recordings in animal models. Otherwise, we must have the confidence and the correctness to move forward and attempt other solutions. Two recent examples. In 2019 we established a causal association between activity of the noradrenergic nucleus locus coeruleus (LC) and the engagement of numerous large-scale brain networks in mice, in particular of the salience and amygdala networks [6]. In addition, we could link network-changes with direct markers of norepinephrine (NE) turnover and with the distribution of NE receptors over the entire brain. The hypothesis that specific brain networks dynamics are related to LC activity and to NE receptor density derives from stress-research and pharmacological studies in humans [7, 8]. However, since it is impossible to selectively stimulate LC in people, it has remained a hypothesis for more than a decade. Our preclinical work helped confirm this causal relationship and this has direct implications for interpreting the results of clinical imaging studies on stress and anxiety behavior. More recently, the Gozzi lab described how chronic local neuronal suppression via overexpression of a potassium channel or acute silencing via chemogenetics result in a paradoxical hyperconnectivity [9]; an intriguing finding often reported in humans after stroke [10] and in early stages of Alzheimer\u2019s disease [11], but never truly understood. Using in vivo electrophysiology, they showed local inhibition improves low frequency (0.1\u20134 Hz) oscillatory power via suppression of neuronal activity not phaselocked to slow rhythms, resulting in increased slow and \u03b4 band coherence between areas that display fMRI overconnectivity. These data present causal evidence that cortical inactivation can counterintuitively augment fMRI connectivity via greater, lesslocalized slow oscillatory processes. Once again, this could be only achieved by combining functional MRI and electrophysiology with neuromodulation in animal models. These and other examples give a peek of what the future of preclinical imaging might look like: a field of research capable of delivering causal explanations to the hypotheses presented by human neuroscience, neurology and psychiatry. Lastly, I would argue against statements like \u201cthe computational complexity of human brains is billions of times that of mouse brain\u201d. While this may be true from a numerical standpoint of mere neuronal counts, preclinical neuroimaging\u2019s objective should not be per se to map every single neuron in real time but of identifying the general neural and cellular principles governing the assembly of brain networks and its breakdown in brain disorders. The field is relatively new but is moving fast and has already produced some important insights. The future is challenging and will require time, devotion and an optimal synergy between engineering, chemistry, biology, and computer science. If the community will be patient and supportive enough, there will be further important discoveries in the future."
      ],
      "categories":[
        "Psychiatry"
      ]
    },
    "list":{
      "title":[
        "Tight Bounds on the Binomial CDF, and the Minimum of i.i.d Binomials, in\n  terms of KL-Divergence",
        "Euclid Quick Data Release (Q1). Extending the quest for little red dots\n  to z<4",
        "Energy-variational structure in evolution equations",
        "Classification of Electron and Muon Neutrino Events for the ESS$\\nu$SB\n  Near Water Cherenkov Detector using Graph Neural Networks",
        "Modified scattering for small data solutions to the Vlasov-Maxwell\n  system: a short proof",
        "Prompt and Conventional High-Energy Muon Spectra from a full Monte Carlo\n  Simulation via $\\texttt{CORSIKA7}$",
        "Enhancing Diffusion Models Efficiency by Disentangling Total-Variance\n  and Signal-to-Noise Ratio",
        "Spin 1 Transverse Momentum Dependent Tensor Structure Functions in\n  CLAS12",
        "A lower bound on the Ulrich complexity of hypersurfaces",
        "Oscillations of Solitonic Galactic Cores in Ultralight Dark Matter",
        "First detection of variable radio emission originating from the infant\n  planetary system V1298 Tau",
        "Regularized Sparse Optimal Discriminant Clustering",
        "Number of spanning trees in a wheel graph with two identified vertices\n  via hitting times",
        "Benchmarking low-power flopping-mode spin qubit fidelities in Si\/SiGe\n  devices with alloy disorder",
        "Successive Interference Cancellation-aided Diffusion Models for Joint\n  Channel Estimation and Data Detection in Low Rank Channel Scenarios",
        "Coding methods for string reconstruction from erroneous prefix-suffix\n  compositions",
        "Linear enhanced dissipation for the 2D Taylor-Couette flow in the\n  exterior region: A supplementary example for Gearhart-Pr\\\"uss type lemma",
        "The SPHERE infrared survey for exoplanets (SHINE). V. Complete\n  observations, data reduction and analysis, detection performances, and final\n  results",
        "Bridging Voting and Deliberation with Algorithms: Field Insights from\n  vTaiwan and Kultur Komitee",
        "$\\textit{In situ}$ time-resolved X-ray absorption spectroscopy of\n  shock-loaded magnesiosiderite",
        "Incomplete Data Multi-Source Static Computed Tomography Reconstruction\n  with Diffusion Priors and Implicit Neural Representation",
        "From Policy to Practice. Upper Bound Cost Estimates of Europes Green\n  Hydrogen Ambitions",
        "Existence of dynamical fluctuation in AMPT generated data for Au+Au\n  collisions at 10 AGeV",
        "Sensing Rate Optimization for Multi-Band Cooperative ISAC Systems",
        "Extreme Ultraviolet High-Harmonic Interferometry of Excitation-Induced\n  Bandgap Dynamics in Solids",
        "Magnetic Bloch States at Integer Flux Quanta Induced by Super-moir\\'e\n  Potential in Graphene Aligned with Twisted Boron Nitride",
        "Below the Schwinger critical magnetic field value, quantum vacuum and\n  gamma-ray bursts delay",
        "Synchronization in the complexified Kuramoto model",
        "Relations amongst the distances between $C^{*}$-subalgebras and some\n  canonically associated operator algebras"
      ],
      "abstract":[
        "We provide finite sample upper and lower bounds on the Binomial tail\nprobability which are a direct application of Sanov's theorem. We then use\nthese to obtain high probability upper and lower bounds on the minimum of\ni.i.d. Binomial random variables. Both bounds are finite sample, asymptotically\ntight, and expressed in terms of the KL-divergence.",
        "Recent James Webb Space Telescope (JWST) observations have revealed a\npopulation of sources with a compact morphology and a `v-shaped' continuum,\nnamely blue at rest-frame $\\lambda<4000$A and red at longer wavelengths. The\nnature of these sources, called `little red dots' (LRDs), is still debated,\nsince it is unclear if they host active galactic nuclei (AGN) and their number\nseems to drastically drop at z<4. We utilise the 63 $deg^2$ covered by the\nquick Euclid Quick Data Release (Q1) to extend the search for LRDs to brighter\nmagnitudes and to lower z than what has been possible with JWST to have a\nbroader view of the evolution of this peculiar galaxy population. The selection\nis done by fitting the available photometric data (Euclid, Spitzer\/IRAC, and\nground-based griz data) with two power laws, to retrieve the rest-frame optical\nand UV slopes consistently over a large redshift range (i.e, z<7.6). We exclude\nextended objects and possible line emitters, and perform a visual inspection to\nremove imaging artefacts. The final selection includes 3341 LRD candidates from\nz=0.33 to z=3.6, with 29 detected in IRAC. Their rest-frame UV luminosity\nfunction, in contrast with previous JWST studies, shows that the number density\nof LRD candidates increases from high-z down to z=1.5-2.5 and decreases at even\nlower z. Less evolution is apparent focusing on the subsample of more robust\nLRD candidates having IRAC detections, which is affected by low statistics and\nlimited by the IRAC resolution. The comparison with previous quasar UV\nluminosity functions shows that LRDs are not the dominant AGN population at\nz<4. Follow-up studies of these LRD candidates are key to confirm their nature,\nprobe their physical properties and check for their compatibility with JWST\nsources, since the different spatial resolution and wavelength coverage of\nEuclid and JWST could select different samples of compact sources.",
        "We consider different measure-valued solvability concepts from the literature\nand show that they could be simplified by using the energy-variational\nstructure of the underlying system of partial differential equations. In the\nconsidered examples, we prove that a certain class of improved measure-valued\nsolutions can be equivalently expressed as an energy-variational solution. The\nfirst concept represents the solution as a high-dimensional Young measure,\nwhether for the second concept, only a scalar auxiliary variable is introduced\nand the formulation is relaxed to an energy-variational inequality. We\ninvestigate four examples: the two-phase Navier--Stokes equations, a\nquasilinear wave equation, a system stemming from polyconvex elasticity, and\nthe Ericksen--Leslie equations equipped with the Oseen--Frank energy. The wide\nrange of examples suggests that this is a recurrent feature in evolution\nequations in general.",
        "In the effort to obtain a precise measurement of leptonic CP-violation with\nthe ESS$\\nu$SB experiment, accurate and fast reconstruction of detector events\nplays a pivotal role. In this work, we examine the possibility of replacing the\ncurrently proposed likelihood-based reconstruction method with an approach\nbased on Graph Neural Networks (GNNs). As the likelihood-based reconstruction\nmethod is reasonably accurate but computationally expensive, one of the\nbenefits of a Machine Learning (ML) based method is enabling fast event\nreconstruction in the detector development phase, allowing for easier\ninvestigation of the effects of changes to the detector design. Focusing on\nclassification of flavour and interaction type in muon and electron events and\nmuon- and electron neutrino interaction events, we demonstrate that the GNN\nreconstructs events with greater accuracy than the likelihood method for events\nwith greater complexity, and with increased speed for all events. Additionally,\nwe investigate the key factors impacting reconstruction performance, and\ndemonstrate how separation of events by pion production using another GNN\nclassifier can benefit flavour classification.",
        "We prove that for any global solution to the Vlasov-Maxwell system arising\nfrom compactly supported data, and such that the electromagnetic field decays\nfast enough, the distribution function exhibits a modified scattering dynamic.\nIn particular, our result applies to every small data solution constructed by\nGlassey-Strauss.",
        "Extensive air showers produce high-energy muons that can be utilized to probe\nhadronic interaction models in cosmic ray interactions. Most muons originate\nfrom pion and kaon decays, called $\\textit{conventional}$ muons, while a\nsmaller fraction, referred to as $\\textit{prompt}$ muons, arises from the decay\nof heavier, short-lived hadrons. The $\\texttt{EHISTORY}$ option of the air\nshower simulation tool $\\texttt{CORSIKA7}$ is used in this work to investigate\nthe prompt and conventional muon flux in the energy range of 100 TeV to 100\nPeV, utilizing the newly developed open-source python software\n$\\texttt{PANAMA}$. Identifying the muon parent particles allows for scaling the\ncontribution of prompt particles, which can be leveraged by future experimental\nanalyses to measure the normalization of the prompt muon flux. Obtained prompt\nmuon spectra from $\\texttt{CORSIKA7}$ are compared to $\\texttt{MCEq}$ results.\nThe relevance to large-volume neutrino detectors, such as IceCube and KM3NeT,\nand the connection to hadronic interaction models is discussed.",
        "The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance\/signal-to-noise-ratio\ndisentangled (TV\/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that different existing schedules, where\nthe TV explodes exponentially, can be $\\textit{improved}$ by setting a constant\nTV schedule while preserving the same SNR schedule. Furthermore, generalizing\nthe SNR schedule of the optimal transport flow matching significantly improves\nthe performance in molecular structure generation, achieving few step\ngeneration of stable molecules. A similar tendency is observed in image\ngeneration, where our approach with a uniform diffusion time grid performs\ncomparably to the highly tailored EDM sampler.",
        "We propose to analyze CLAS12 RG-C data to study the tensor\ntransverse-momentum-dependent parton distribution functions (TMDs) on deuteron\ndata. The deuteron is the lightest nucleus with spin-1, in essence a weakly\nbound system of two spin-1\/2 nucleons. However, one of the most intriguing\ncharacteristics of the deuteron is that the tensor polarized structure provides\ndirect access to the quark and gluon distribution of light nuclear system,\nwhich cannot be naively constructed from the proton and neutron. We will study\nthe tensor polarized structure functions with the Semi-inclusive Deep Inelastic\nScattering (SIDIS) $eD \\arrow eP_{h}X$ and Inclusive processes in the available\ndata on deuterated ammonia (ND3) target. We will perform the first ever SIDIS\nanalysis extraction of the tensor structure functions, which can be interpreted\nin term of completely unexplored tensor polarized TMDs. Our analysis will focus\non the extraction of the tensor structure functions b1 from inclusive process,\nand $F_{U(LL),T}$ and $F^{cos 2\\phi_{h}}_{U(LL)}$ from SIDIS. These last two\nstructure functions carry information related to two tensor-polarized TMDs,\n$f_{1LL}$ and $h^{\\perp}_{1LL}$. These initial exploratory measurements of\ntensor-polarized structure functions will enable the first extraction of spin-1\nTMDs and motivate more precise future measurements.",
        "We give a lower bound on the Ulrich complexity of hypersurfaces of dimension\n$n \\ge 6$.",
        "A remarkable feature of dark matter consisting of ultralight bosonic\nparticles is the emergence of superfluid Bose-Einstein condensate structures on\ngalactic scales. We investigate the oscillations of the solitonic dark matter\nstructure in the central galactic region by numerically solving the\nBogoliubov-de Gennes problem, accounting for perturbations in the gravitational\npotential and local self-interactions. Our findings reveal that the central\nsolitonic core, formed by the balance of gravitational attraction, quantum\npressure, and repulsive interactions, exhibits significant oscillatory\nbehaviour. These oscillations, characterized by distinct eigenmodes, provide\ninsights into the dynamical properties of solitonic dark matter structures and\ntheir observational implications and contributions to galactic structure\nformation and evolution.",
        "V1298 Tau is a very young and magnetically active star which hosts a\nbenchmark multi-planetary system to study planet formation and evolutionary\nhistory at the earliest stages. We selected V1298 Tau for a first targeted\nfollow-up at radio frequencies with the Karl G. Jansky Very Large Telescope\n(JVLA), the upgraded Giant Metrewave Radio Telescope (uGMRT), and the Sardinia\nRadio Telescope (SRT), to search for emission in the overall frequency range\n0.55-7.2 GHz. Detecting radio emission from such a very active star is key to\ncharacterise its magnetosphere, allowing in principle to probe the strength of\nthe coronal magnetic field and plasma density. Observations were carried out\nbetween Oct 2023 and Sept 2024: three epochs with uGMRT band-4 (0.55-0.75 GHz),\n12 epochs with the JVLA using L (1-2 GHz) and C (4.5-6.5 GHz) bands, and three\nepochs with SRT using C-high band (6-7.2 GHz). We report the first detection of\nradio emission from V1298 Tau at different epochs using the JVLA. The emission\nhas maximum peak flux densities of 91$\\pm$10 and 177$\\pm$6 $\\mu$Jy\/beam in the\nL- and C-band, respectively. From a comparison with contemporary optical\nphotometry, we found that the detected emission with the highest fluxes are\nlocated around a phase of minimum of the photospheric light curve. Although the\nuGMRT and SRT observations could not detect the source, we measured 3$\\sigma$\nflux density upper limits in the range ~41-56 $\\mu$Jy\/beam using uGMRT, while\nwith SRT we reached upper limits down to 13 mJy. The lack of a significant\nfraction of circular polarisation indicates that the observed flux is not due\nto electron cyclotron maser emission from star-planet interaction, and it is\nlikely produced by gyrosynchroton\/cyclotron emission from the corona triggered\nby stellar magnetic activity, although we cannot exclude thermal emission due\nto a lack of constraints on the brightness temperature.",
        "We propose a new method based on sparse optimal discriminant clustering\n(SODC), by a penalty term to scoring matrix based on convex clustering. With\nthe addition of this penalty term, it is expected to improve the accuracy of\ncluster identification by attaching points from the same cluster closer\ntogether and points from different clusters further apart. Moreover, we develop\na novel algorithm to derive the updated formula of this scoring matrix using\nmajorizing function. It solves the difficulty to satisfy both constraint and\ncontaining the clustering structure to the scoring matrix. We have demonstrated\nthe numerical simulations and its an application to real data to assess the\nperformance of the proposed method.",
        "In this paper, we provide an exact formula for the average hitting times in a\nwheel graph $W_{N+1}$ using a combinatorial approach. For this wheel graph, the\naverage hitting times can be expressed using Fibonacci numbers when the number\nof surrounding vertices is odd and Lucas numbers when it is even. Furthermore,\ncombining the exact formula for the average hitting times with the general\nformula for the effective resistance of the graph allows determination of the\nnumber of spanning trees of the graph with two identified vertices.",
        "In the \"flopping-mode\" regime of electron spin resonance, a single electron\nconfined in a double quantum dot is electrically driven in the presence of a\nmagnetic field gradient. The increased dipole moment of the charge in the\nflopping mode significantly reduces the amount of power required to drive spin\nrotations. However, the susceptibility of flopping-mode spin qubits to charge\nnoise, and consequently their overall performance, has not been examined in\ndetail. In this work, we simulate single-qubit gate fidelities of electrically\ndriven spin rotations in an ensemble of devices configured to operate in both\nthe single-dot and flopping-mode regimes. Our model accounts for the valley\nphysics of conduction band electrons in silicon and realistic alloy disorder in\nthe SiGe barrier layers, allowing us to investigate device-to-device\nvariability. We include charge and magnetic noise, as well as spin relaxation\nprocesses arising from charge noise and electron-phonon coupling. We find that\nthe two operating modes exhibit significantly different susceptibilities to the\nvarious noise sources, with valley splitting and spin relaxation times also\nplaying a role in their relative performance. For realistic noise strengths, we\nfind that single-dot gate fidelities are limited by magnetic noise while\nflopping-mode fidelities are primarily limited by charge noise and spin\nrelaxation. For sufficiently long spin relaxation times, flopping-mode spin\noperation is feasible with orders-of-magnitude lower drive power and gate\nfidelities that are on par with conventional single-dot electric dipole spin\nresonance.",
        "This paper proposes a novel joint channel-estimation and source-detection\nalgorithm using successive interference cancellation (SIC)-aided generative\nscore-based diffusion models. Prior work in this area focuses on massive MIMO\nscenarios, which are typically characterized by full-rank channels, and fail in\nlow-rank channel scenarios. The proposed algorithm outperforms existing methods\nin joint source-channel estimation, especially in low-rank scenarios where the\nnumber of users exceeds the number of antennas at the access point (AP). The\nproposed score-based iterative diffusion process estimates the gradient of the\nprior distribution on partial channels, and recursively updates the estimated\nchannel parts as well as the source. Extensive simulation results show that the\nproposed method outperforms the baseline methods in terms of normalized mean\nsquared error (NMSE) and symbol error rate (SER) in both full-rank and low-rank\nchannel scenarios, while having a more dominant effect in the latter, at\nvarious signal-to-noise ratios (SNR).",
        "The number of zeros and the number of ones in a binary string are referred to\nas the composition of the string, and the prefix-suffix compositions of a\nstring are a multiset formed by the compositions of the prefixes and suffixes\nof all possible lengths of the string. In this work, we present binary codes of\nlength n in which every codeword can be efficiently reconstructed from its\nerroneous prefix-suffix compositions with at most t composition errors. All our\nconstructions have decoding complexity polynomial in n and the best of our\nconstructions has constant rate and can correct $t = \\Theta(n)$ errors. As a\ncomparison, no prior constructions can afford to efficiently correct $t =\n\\Theta(n)$ arbitrary composition errors.\n  Additionally, we propose a method of encoding h arbitrary strings of the same\nlength so that they can be reconstructed from the multiset union of their\nerror-free prefix-suffix compositions, at the expense of h-fold coding\noverhead. In contrast, existing methods can only recover h distinct strings,\nalbeit with code rate asymptotically equal to 1\/h. Building on the top of the\nproposed method, we also present a coding scheme that enables efficient\nrecovery of h strings from their erroneous prefix-suffix compositions with $t =\n\\Theta(n)$ errors.",
        "From the perspective of asymptotic stability at high Reynolds numbers,\nTaylor-Couette flow, as a typical rotating shear flow, exhibits rich decay\nbehaviors. Previously, for the extensively studied Couette flow or the\nTaylor-Couette flow in bounded annular domains, methods based on resolvent\nestimates could derive exponential decay asymptotic for the solutions of the\nlinearized system. However, unlike the Couette flow or the Taylor-Couette flow\nin bounded annular domains, the Taylor-Couette flow in exterior regions\nexhibits degeneration of derivatives of any order at infinity. In this paper,\nwe present in Theorem 1.1 that the linearized system of the 2D Taylor-Couette\nflow in the exterior region exhibits space-time coupled polynomial decay\nasymptotics. We also prove that the solution to this system, when it contains\ninhomogeneous terms, cannot be expected to exhibit space-time coupled\nexponential decay, as detailed in Theorem 1.2. The result of Theorem 1.2\nindicates that, even if we can obtain sharp resolvent estimates in different\nweighted spaces, the Gearhart-Pr\\\"uss type lemma no longer applies. This\nsuggests that resolvent estimates may not be very effective for handling\ndegenerate shear flows. Furthermore, Theorem 1.2 also implies that, for the\ntransition threshold problem of the 2D Taylor-Couette flow in exterior regions,\nwe can at most expect the solution to exhibit long-time behavior with\nspace-time coupled polynomial decay. Finally, we present a generalization of\nTheorem 1.2, as detailed in Theorem 1.3.",
        "During the past decade, state-of-the-art planet-finder instruments like\nSPHERE@VLT, coupling coronagraphic devices and extreme adaptive optics systems,\nunveiled, thanks to large surveys, around 20 planetary mass companions at\nsemi-major axis greater than 10 astronomical units. Direct imaging being the\nonly detection technique to be able to probe this outer region of planetary\nsystems, the SPHERE infrared survey for exoplanets (SHINE) was designed and\nconducted from 2015 to 2021 to study the demographics of such young gas giant\nplanets around 400 young nearby solar-type stars. In this paper, we present the\nobserving strategy, the data quality, and the point sources analysis of the\nfull SHINE statistical sample as well as snapSHINE. Both surveys used the\nSPHERE@VLT instrument with the IRDIS dual band imager in conjunction with the\nintegral field spectrograph IFS and the angular differential imaging observing\ntechnique. All SHINE data (650 datasets), corresponding to 400 stars, including\nthe targets of the F150 survey, are processed in a uniform manner with an\nadvanced post-processing algorithm called PACO ASDI. An emphasis is put on the\nclassification and identification of the most promising candidate companions.\nCompared to the previous early analysis SHINE F150, the use of advanced\npost-processing techniques significantly improved by one or 2 magnitudes\n(x3-x6) the contrast detection limits, which will allow us to put even tighter\nconstraints on the radial distribution of young gas giants. This increased\nsensitivity directly places SHINE as the largest and deepest direct imaging\nsurvey ever conducted. We detected and classified more than 3500 physical\nsources. One additional substellar companion has been confirmed during the\nsecond phase of the survey (HIP 74865 B), and several new promising candidate\ncompanions are awaiting second epoch confirmations.",
        "Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Radial Clustering for Preference\nBased Subgroups, which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives.\n  By introducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes.",
        "Carbonate minerals are important in Earth's system sciences and have been\nfound on Mars and in meteorites and asteroids, highlighting the importance of\nimpacts in planetary processes. While extensively studied under static\ncompression, the behavior of carbonates under shock compression remains\nunderexplored, with no $\\textit{in situ}$ X-ray investigations reported so far.\nHere we investigate natural magnesiosiderite (Fe$_{0.6}$Mg$_{0.4}$CO$_{3}$)\nunder nanosecond laser-driven shock compression at pressures up to 150 GPa,\ncoupled with $\\textit{in situ}$ ultrafast synchrotron X-ray absorption\nspectroscopy (XAS). The interpretation of the experimental spectra is\ncomplemented using first-principles absorption cross-section calculations\nperformed on crystalline phases at different pressures and on a dense liquid\nphase obtained using density functional theory-based molecular dynamics\n(DFT-MD) simulations. Under laser-driven shock compression, the\nmagnesiosiderite crystal phase remains unchanged up to the melt. Under shock\nreverberation, the absorption spectra show changes similar to those attributed\nto a high-spin to low-spin transition observed under static compression. At\nhigher pressures, the laser shock induces the formation of CO$_4$ tetrahedral\nunits in the melt. Upon unloading from the shocked state, only a few\nnanoseconds later, the original magnesiosiderite phase is recovered.",
        "The dose of X-ray radiation and the scanning time are crucial factors in\ncomputed tomography (CT) for clinical applications. In this work, we introduce\na multi-source static CT imaging system designed to rapidly acquire sparse view\nand limited angle data in CT imaging, addressing these critical factors. This\nlinear imaging inverse problem is solved by a conditional generation process\nwithin the denoising diffusion image reconstruction framework. The noisy volume\ndata sample generated by the reverse time diffusion process is projected onto\nthe affine set to ensure its consistency to the measured data. To enhance the\nquality of the reconstruction, the 3D phantom's orthogonal space projector is\nparameterized implicitly by a neural network. Then, a self-supervised learning\nalgorithm is adopted to optimize the implicit neural representation. Through\nthis multistage conditional generation process, we obtain a new approximate\nposterior sampling strategy for MSCT volume reconstruction. Numerical\nexperiments are implemented with various imaging settings to verify the\neffectiveness of our methods for incomplete data MSCT volume reconstruction.",
        "As the European countries strive to meet their ambitious climate goals,\nrenewable hydrogen has emerged to aid in decarbonizing energy-intensive sectors\nand support the overall energy transition. To ensure that hydrogen production\naligns with these goals, the European Commission has introduced criteria for\nadditionality, temporal correlation, and geographical correlation. These\ncriteria are designed to ensure that hydrogen production from renewable sources\nsupports the growth of renewable energy. This study assesses the impact of\nthese criteria on green hydrogen production, focusing on production costs and\ntechnology impacts. The European energy market is simulated up to 2048 using\nstochastic programming, applying these requirements exclusively to green\nhydrogen production without the phased-in compliance period outlined in the EU\nregulations. The findings show that meeting the criteria will increase expected\nsystem costs by 82 EUR billion from 2024 to 2048, largely due to the rapid\nshift from fossil fuels to renewable energy. The additionality requirement,\nwhich mandates the use of new renewable energy installations for electrolysis,\nproves to be the most expensive, but also the most effective in accelerating\nrenewable energy adoption.",
        "In this study, the intermittency behavior of emitted particles produced in\nheavy ion collisions has been studied using both modes (default $\\&$ string\nmelting) of A Multi Phase Transport (AMPT) model-generated data. We adopted one\nof the most conventional and successful techniques, the Scaled Factorial Moment\n(SFM) method, using Monte Carlo (MC) data for 10 AGeV Au+Au collisions in\nsearch of intermittency in the model-generated data. Our interest is to search\nfor intermittency behavior of particles that leads to multiplicity fluctuations\nand that would reveal a phase transition from hadronic matter to QGP. In this\narticle, the intermittency values for both modes of AMPT data are presented.\nThe results obtain some insight into the dynamics of heavy ion collisions and\nthe formation of QGP.",
        "Integrated sensing and communication (ISAC) has been recognized as one of the\nkey technologies for future wireless networks, which potentially need to\noperate in multiple frequency bands to satisfy ever-increasing demands for both\ncommunication and sensing services. Motivated by this, we consider the sum\nsensing rate (SR) optimization for a cooperative ISAC system with linear\nprecoding, where each base station (BS) works in a different frequency band.\nWith this aim, we propose an optimization algorithm based on the semi-definite\nrank relaxation that introduces covariance matrices as optimization variables,\nand we apply the inner approximation (IA) method to deal with the nonconvexity\nof the resulting problem. Simulation results show that the proposed algorithm\nincreases the SR by approximately 25 % and 40 % compared to the case of equal\npower distribution in a cooperative ISAC system with two and three BSs,\nrespectively. Additionally, the algorithm converges in only a few iterations,\nwhile its most optimal implementation scenario is in the low power regime.",
        "Interferometry is a fundamental technique in physics, enabling precise\nmeasurements through the interference of waves. High-harmonic generation (HHG)\nin solids has emerged as a powerful method for probing ultrafast electronic\ndynamics within crystalline structures.\n  In this study, we employed extreme ultraviolet (XUV) high-harmonic\ninterferometry with phase-locked XUV pulse pairs to investigate\nexcitation-induced bandgap dynamics in solids. Our experiments on amorphous\nSiO2 and crystalline MgO, complemented by analytical modeling and semiconductor\nBloch equation simulations, reveal a correlation between transient bandgap\nmodifications and variations in the phase of harmonic emission. These findings\nsuggest a potential pathway for sub-cycle, all-optical control of band\nstructure modifications, advancing prospects for petahertz-scale electronic\napplications and attosecond diagnostics of carrier dynamics.",
        "Two-dimensional electron systems in both magnetic fields and periodic\npotentials are described by Hofstadter butterfly, a fundamental problem of\nsolid-state physics. While moir\\'e systems provide a powerful method to realize\nthis spectrum, previous experiments, however, have been limited to fractional\nflux quanta regime due to the difficulty of building ~ 50 nm periodic\nmodulations. Here, we demonstrate a super-moir\\'e strategy to overcome this\nchallenge. By aligning monolayer graphene (G) with 1.0{\\deg} twisted hexagonal\nboron nitride (t-hBN), a 63.2 nm bichromatic G\/t-hBN super-moir\\'e is\nconstructed, made possible by exploiting the electrostatic nature of t-hBN\npotential. Under magnetic field B, magnetic Bloch states at integer flux quanta\n(1-9) are achieved and observed as integer Brown-Zak oscillations, expanding\nthe flux quanta from factions to integers. Theoretical analysis reproduces\nthese experimental findings. This work opens new avenues to study unexplored\nHofstadter butterfly, explore emergent topological order at integer flux quanta\nand engineer long-wavelength periodic modulations.",
        "A magnetic field above the Schwinger critical value $B_{\\rm crit} = 10^9$\nTesla is much higher than any magnetic field known by now in the interstellar\nbulk except in the vicinity of observed magnetars with magnetic fields between\n$10^9$ and $10^{11}~$Tesla. Above the critical magnetic field, calculated by\nSchwinger in the lowest order perturbation in quantum electrodynamics (QED),\none reaches the threshold for electron-positron pair creation, which has\ninteresting consequences. Therefore, finding out whether one could encounter\nsome consequences of interest also for the values of the magnetic field below\nthe Schwinger critical point, we invoke the next higher-order effect in QED,\nwhich is emerging from the Quantum Vacuum Effect. The latter is equivalent to\nthe use of the Euler-Heisenberg effective theory in nonlinear electrodynamics,\nwhere the Lagrangian has a term with a higher power, $B^4$. In this case, in\nthe region $B<B_{\\rm crit}$, we show that interesting effects appear, among\nthem the Cherenkov radiation and the reduction in the speed of light. The\nlatter effects appear because of the quantum vacuum mimicking a medium. We also\npresent quantitative arguments for such a close analogy. As a rough estimate,\nwe show that the time delay $\\tau$ of gamma-ray bursts (GRB) having traveled\nthrough the entire cosmological distances in an average strong magnetic field\nsuch as $10^6~$Tesla, reaches an experimentally considerable value of $\\tau =\n2.4$ hours. In the vicinity of magnetars, the magnetic field is much stronger,\nof the order of $10^9-10^{11}$ Tesla. However, in this case the linear scale of\nGRB trajectory through such regions would be much smaller. For the latter, we\ngive an estimate for the number of the magnetars along the trajectory and also\nfor the delay. Finally, we shall dwell on the recently raised issue in the\nliterature, namely the Lorentz invariance violation (LIV).",
        "In this paper, we consider an $N$-oscillators complexified Kuramoto model.\nWhen the coupling strength $\\lambda$ is strong, sufficient conditions for\nvarious types of synchronization are established for general $N \\geq 2$. On the\nother hand, we analyze the case when the coupling strength is weak. For $N=2$,\nwhen the coupling strength is below a critical coupling strength $\\lambda_c$,\nwe show that periodic orbits emerge near each equilibrium point, and hence full\nphase-locking state exists. This phenomenon significantly differentiates the\ncomplexified Kuramoto model from the real Kuramoto system, as synchronization\nnever occurs when $\\lambda<\\lambda_c$ in the latter. For $N=3$, we demonstrate\nthat if the natural frequencies are in arithmetic progression, non-trivial\nsynchronization states can be achieved for certain initial conditions even when\nthe coupling strength is weak. In particular, we characterize the critical\ncoupling strength ($\\lambda\/\\lambda_c = 0.85218915...$) such that a semistable\nequilibrium point in the real Kuramoto model bifurcates into a pair of stable\nand unstable equilibria, marking a new phenomenon in complexified Kuramoto\nmodels.",
        "We prove that the Christensen distance (resp., the Kadison-Kastler distance)\nbetween two $C^*$-subalgebras $\\mathcal{A}$ and $\\mathcal{B}$ of a\n$C^*$-algebra $\\mathcal{C}$ is equal to that between their enveloping von\nNeumann algebras $\\mathcal{A}^{**}$ and $\\mathcal{B}^{**}$ (resp., the tensor\nproduct algebras $\\mathcal{A} \\otimes^{\\min} \\mathcal{D}$ and $\\mathcal{B}\n\\otimes^{\\min} \\mathcal{D}$, for any unital commutative $C^*$-algebra\n$\\mathcal{D}$)."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Parameter-Free FISTA by Adaptive Restart and Backtracking",
    "start_abstract":"We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
      ],
      "abstract":[
        "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Decay rates of $\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar\\nu_\\ell$ using\n  helicity analysis and phase-moment parametrization",
        "Ultraviolet Renormalization of Spin Boson Models I. Normal and\n  2-Nilpotent Interactions",
        "Development of a high-rate capable DLC-RPC based on a current evacuation\n  pattern",
        "Non-Lorentzian model for strong exciton-plasmon coupling",
        "Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly\n  Detection in Videos",
        "PrivilegedDreamer: Explicit Imagination of Privileged Information for\n  Rapid Adaptation of Learned Policies",
        "Are you a DePIN? A Decision Tree to Classify Decentralized Physical\n  Infrastructure Networks",
        "Bridging HCI and AI Research for the Evaluation of Conversational SE\n  Assistants",
        "Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in\n  Ecommerce Web Components Generated by LLMs",
        "Blast waves and reverse shocks: from ultra-relativistic GRBs to\n  moderately relativistic X-ray binaries",
        "How Low Can You Go? Searching for the Intrinsic Dimensionality of\n  Complex Networks using Metric Node Embeddings",
        "Native antisite defects in h-BN",
        "Black Box Causal Inference: Effect Estimation via Meta Prediction",
        "Exploring the energy spectrum of a four-terminal Josephson junction:\n  Towards topological Andreev band structures",
        "Near-Field ISAC: Synergy of Dual-Purpose Codebooks and Space-Time\n  Adaptive Processing",
        "Designing Guidance for Multiple Valley-based Topological States Driven\n  by Magnetic Substrates: Potential Applications at High Temperatures",
        "Euclid Quick Data Release (Q1). Galaxy shapes and alignments in the\n  cosmic web",
        "Hierarchical Deep Reinforcement Learning for Adaptive Resource\n  Management in Integrated Terrestrial and Non-Terrestrial Networks",
        "Stochastic Method for Delayed Neutron Precursors Transport in Liquid\n  Fuel",
        "Nonlinear bubble behaviours of compressible Rayleigh-Taylor instability\n  with isothermal stratification in cylindrical geometry",
        "Efficient License Plate Recognition in Videos Using Visual Rhythm and\n  Accumulative Line Analysis",
        "Understanding User Preference -- Comparison between Linear and\n  Directional Top-K Query results",
        "Stellar Population and Metal Production in AGN Disks",
        "A comparison of data filtering techniques for English-Polish LLM-based\n  machine translation in the biomedical domain",
        "QAOA in Quantum Datacenters: Parallelization, Simulation, and\n  Orchestration",
        "Efficient cavity-mediated energy transfer between photosynthetic light\n  harvesting complexes from strong to weak coupling regime",
        "Convergence Rates of GMM Estimators with Nonsmooth Moments under\n  Misspecification",
        "Learning to Negotiate via Voluntary Commitment",
        "A Unified Knowledge-Distillation and Semi-Supervised Learning Framework\n  to Improve Industrial Ads Delivery Systems"
      ],
      "abstract":[
        "Based on the helicity method, formulae for the semileptonic transition of\n$\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar \\nu_\\ell$ including lepton mass\neffects are derived. In order to calculate the form factors of the $\\Lambda_b$\nbaryon transition matrix element, we employ the phase-moment parameterization\nand perform fits to the Lattice QCD data. With the help of the obtained form\nfactors, six helicity amplitudes and the differential decay widths are\nevaluated. Through appropriate angular integrations, we express the helicity\nflip, helicity nonflip integrated decay rates, and the lepton-side\nforward-backward asymmetry. We present a numerical analysis of these physical\nobservables. We obtain the mentioned physical quantities by performing fits to\nthe Lattice QCD data using the well-known Boyd-Grinstein-Lebed parametrization.\nComparisons with other experimental and theoretical data are also discussed.",
        "We study the ultraviolet problem for models of a finite-dimensional quantum\nmechanical system linearly coupled to a bosonic quantum field, such as the\n(many-)spin boson model or its rotating-wave approximation. If the state change\nof the system upon emission or absorption of a boson is either given by a\nnormal matrix or by a 2-nilpotent one, which is the case for the previously\nnamed examples, we prove an optimal renormalization result. We complement it,\nby proving the norm resolvent convergence of appropriately regularized models\nto the renormalized one. Our method consists of a dressing transformation\nargument in the normal case and an appropriate interior boundary condition for\nthe 2-nilpotent case.",
        "A Resistive Plate Chamber using Diamond-Like Carbon electrodes (DLC-RPC) has\nbeen developed as a background tagging detector in the MEG$~$II experiment. The\nDLC-RPC is planned to be installed in a high-intensity and low-momentum muon\nbeam. This detector is required to have a detection efficiency above 90 % with\nfour active gaps in the muon beam due to the limitation of the material budget.\nIn such an environment, the high current flowing through the resistive\nelectrodes causes a voltage drop, which reduces the performance of the DLC-RPC.\nThis voltage drop can be suppressed by implementing a current evacuation\npattern, though discharges are more likely to occur near the pattern. Therefore\nthe pattern must be covered by a protection cover made of an insulator. In this\nstudy, electrode samples with a current evacuation pattern and different widths\nof protection cover (0.2 mm and 0.8 mm) have been produced, and their\nperformance and stability were measured. The detection efficiency of a\nsingle-gap chamber for $\\beta$-rays from a $^{90}$Sr source was measured to be\nup to approximately 60 % in both electrode samples. The target efficiency can\nbe achieved even with a drop of 100 $-$ 150 V. On the other hand, after more\nthan a dozen hours of operation, discharges suddenly occurred and the detector\nwas prevented from further operation. These discharges created current paths on\nthe spacing pillars. This serious problem must be investigated and solved in\nthe future.",
        "We develop a non-Lorentzian approach for quantum emitters (QE) resonantly\ncoupled to localized surface plasmons (LSP) in metal-dielectric structures.\nUsing the exact LSP Green function, we derive non-Lorentzian version of\nMaxwell-Bloch equations which describe LSP in terms of metal complex dielectric\nfunction rather than via Lorentzian resonances. For a single QE coupled to the\nLSP, we obtain an explicit expression for the system effective optical\npolarizability which, in the Lorentzian approximation, recovers the classical\ncoupled oscillator (CO) model. We demonstrate that non-Lorentzian effects\noriginating from the temporal dispersion of metal dielectric function affect\ndramatically the optical spectra as the system transitions to the strong\ncoupling regime. Specifically, in contrast to Lorentzian models, the main\nspectral weight is shifted towards the lower energy polaritonic band,\nconsistent with the experiment.",
        "Anomaly detection in videos is a challenging task as anomalies in different\nvideos are of different kinds. Therefore, a promising way to approach video\nanomaly detection is by learning the non-anomalous nature of the video at hand.\nTo this end, we propose a one-class few-shot learning driven transformer based\napproach for anomaly detection in videos that is self-context aware. Features\nfrom the first few consecutive non-anomalous frames in a video are used to\ntrain the transformer in predicting the non-anomalous feature of the subsequent\nframe. This takes place under the attention of a self-context learned from the\ninput features themselves. After the learning, given a few previous frames, the\nvideo-specific transformer is used to infer if a frame is anomalous or not by\ncomparing the feature predicted by it with the actual. The effectiveness of the\nproposed method with respect to the state-of-the-art is demonstrated through\nqualitative and quantitative results on different standard datasets. We also\nstudy the positive effect of the self-context used in our approach.",
        "Numerous real-world control problems involve dynamics and objectives affected\nby unobservable hidden parameters, ranging from autonomous driving to robotic\nmanipulation, which cause performance degradation during sim-to-real transfer.\nTo represent these kinds of domains, we adopt hidden-parameter Markov decision\nprocesses (HIP-MDPs), which model sequential decision problems where hidden\nvariables parameterize transition and reward functions. Existing approaches,\nsuch as domain randomization, domain adaptation, and meta-learning, simply\ntreat the effect of hidden parameters as additional variance and often struggle\nto effectively handle HIP-MDP problems, especially when the rewards are\nparameterized by hidden variables. We introduce Privileged-Dreamer, a\nmodel-based reinforcement learning framework that extends the existing\nmodel-based approach by incorporating an explicit parameter estimation module.\nPrivilegedDreamer features its novel dual recurrent architecture that\nexplicitly estimates hidden parameters from limited historical data and enables\nus to condition the model, actor, and critic networks on these estimated\nparameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates\nthat PrivilegedDreamer outperforms state-of-the-art model-based, model-free,\nand domain adaptation learning algorithms. Additionally, we conduct ablation\nstudies to justify the inclusion of each component in the proposed\narchitecture.",
        "Decentralized physical infrastructure networks (DePINs) are an emerging\nvertical within \"Web3\" replacing the traditional method that physical\ninfrastructures are constructed. Yet, the boundaries between DePIN and\ntraditional method of building crowd-sourced infrastructures such as citizen\nscience initiatives or other Web3 verticals are not always so clear cut. In\nthis work, we systematically analyze the differences between DePIN and other\nWeb2 and Web3 verticals. For this, the study proposes a novel decision tree for\nclassifying systems as DePIN. This tree is informed by prior studies and\ndifferentiates DePIN from related concepts using criteria such as the presence\nof a three-sided market, token-based incentives for supply, and the requirement\nfor physical asset placement in those systems.\n  The paper demonstrates the application of the decision tree to various\nblockchain systems, including Helium and Bitcoin, showcasing its practical\nutility in differentiating DePIN systems.\n  This research offers significant contributions towards establishing a more\nobjective and systematic approach to identifying and categorizing DePIN\nsystems. It lays the groundwork for creating a comprehensive and unbiased\ndatabase of DePIN systems, which will inform future research and development\nwithin this emerging sector.",
        "As Large Language Models (LLMs) are increasingly adopted in software\nengineering, recently in the form of conversational assistants, ensuring these\ntechnologies align with developers' needs is essential. The limitations of\ntraditional human-centered methods for evaluating LLM-based tools at scale\nraise the need for automatic evaluation. In this paper, we advocate combining\ninsights from human-computer interaction (HCI) and artificial intelligence (AI)\nresearch to enable human-centered automatic evaluation of LLM-based\nconversational SE assistants. We identify requirements for such evaluation and\nchallenges down the road, working towards a framework that ensures these\nassistants are designed and deployed in line with user needs.",
        "Recent work has highlighted the risks of LLM-generated content for a wide\nrange of harmful behaviors, including incorrect and harmful code. In this work,\nwe extend this by studying whether LLM-generated web design contains dark\npatterns. This work evaluated designs of ecommerce web components generated by\nfour popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used\necommerce components (e.g., search, product reviews) and used them as prompts\nto generate a total of 312 components across all models. Over one-third of\ngenerated components contain at least one dark pattern. The majority of dark\npattern strategies involve hiding crucial information, limiting users' actions,\nand manipulating them into making decisions through a sense of urgency. Dark\npatterns are also more frequently produced in components that are related to\ncompany interests. These findings highlight the need for interventions to\nprevent dark patterns during front-end code generation with LLMs and emphasize\nthe importance of expanding ethical design education to a broader audience.",
        "Blast wave models are commonly used to model relativistic outflows from\nultra-relativistic gamma-ray bursts (GRBs), but are also applied to lower\nLorentz factor ejections from X-ray binaries (XRBs). Here we revisit the\nphysics of blast waves and reverse shocks in these systems and explore the\nsimilarities and differences between the ultra-relativistic ($\\Gamma \\gg 1$)\nand moderately relativistic ($\\Gamma \\sim$ a few) regimes. We first demonstrate\nthat the evolution of the blast wave radius as a function of the observer frame\ntime is recovered in the on-axis ultra-relativistic limit from a general energy\nand radius blast wave evolution, emphasizing that XRB ejections are off-axis,\nmoderately relativistic cousins of GRB afterglows. We show that, for fixed\nblast wave or ejecta energy, reverse shocks cross the ejecta much later\n(earlier) on in the evolution for less (more) relativistic systems, and find\nthat reverse shocks are much longer-lived in XRBs and off-axis GRBs compared to\non-axis GRBs. Reverse shock crossing should thus typically finish after\n$\\sim10-100$ days (in the observer frame) in XRB ejections. This\ncharacteristic, together with their moderate Lorentz factors and resolvable\ncore separations, makes XRB ejections unique laboratories for shock and\nparticle acceleration physics. We discuss the impact of geometry and lateral\nspreading on our results, explore how to distinguish between different shock\ncomponents, and comment on the implications for GRB and XRB environments.\nAdditionally, we argue that identification of reverse shock signatures in XRBs\ncould provide an independent constraint on the ejecta Lorentz factor.",
        "Low-dimensional embeddings are essential for machine learning tasks involving\ngraphs, such as node classification, link prediction, community detection,\nnetwork visualization, and network compression. Although recent studies have\nidentified exact low-dimensional embeddings, the limits of the required\nembedding dimensions remain unclear. We presently prove that lower dimensional\nembeddings are possible when using Euclidean metric embeddings as opposed to\nvector-based Logistic PCA (LPCA) embeddings. In particular, we provide an\nefficient logarithmic search procedure for identifying the exact embedding\ndimension and demonstrate how metric embeddings enable inference of the exact\nembedding dimensions of large-scale networks by exploiting that the metric\nproperties can be used to provide linearithmic scaling. Empirically, we show\nthat our approach extracts substantially lower dimensional representations of\nnetworks than previously reported for small-sized networks. For the first time,\nwe demonstrate that even large-scale networks can be effectively embedded in\nvery low-dimensional spaces, and provide examples of scalable, exact\nreconstruction for graphs with up to a million nodes. Our approach highlights\nthat the intrinsic dimensionality of networks is substantially lower than\npreviously reported and provides a computationally efficient assessment of the\nexact embedding dimension also of large-scale networks. The surprisingly low\ndimensional representations achieved demonstrate that networks in general can\nbe losslessly represented using very low dimensional feature spaces, which can\nbe used to guide existing network analysis tasks from community detection and\nnode classification to structure revealing exact network visualizations.",
        "Hexagonal boron nitride (hBN) is an excellent host for solid-state single\nphonon emitters. Experimental observed emission ranges from infrared to\nultraviolet. The emission centers are generally attributed to either intrinsic\nor extrinsic point defects embedded into hBN. Nevertheless, the microscopic\nstructure of most of these defect emitters is uncertain. Here, through\ndensity-functional theory calculations we studied the native antisite defects\nin hBN. We find that the neutral boron antisite might be a nonmagnetic single\nphoton source with zero-phonon-line (ZPL) at 1.58 eV and such a lineshape that\nis often observed in experiments. Furthermore, the positively charged nitrogen\nantisite might be associated with a dim color center recently observed as a\nblue emitter with ZPL at 2.63 eV. These simple single substitution defects\nindicate the existence of out-of-plane phonon mode which significantly affects\nthe optical properties. Our results could provide useful information for\nidentification of quantum emitters in hBN.",
        "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
        "Hybrid multiterminal Josephson junctions (JJs) are expected to harbor a novel\nclass of Andreev bound states (ABSs), including topologically nontrivial states\nin four-terminal devices. In these systems, topological phases emerge when ABSs\ndepend on at least three superconducting phase differences, resulting in a\nthree-dimensional (3D) energy spectrum characterized by Weyl nodes at zero\nenergy. Here, we realize a four-terminal JJ in a hybrid Al\/InAs\nheterostructure, where ABSs form a synthetic 3D band structure. We probe the\nenergy spectrum using tunneling spectroscopy and identify spectral features\nassociated with the formation of a tri-Andreev molecule, a bound state whose\nenergy depends on three superconducting phases and, therefore, is able to host\ntopological ABSs. The experimental observations are well described by a\nnumerical model. The calculations predict the appearance of four Weyl nodes at\nzero energy within a gap smaller than the experimental resolution. These\ntopological states are theoretically predicted to remain stable within an\nextended region of the parameter space, well accessible by our device. These\nfindings establish an experimental foundation to study high-dimensional\nsynthetic band structures in multiterminal JJs, and to realize topological\nAndreev bands.",
        "Integrated sensing and communication (ISAC) has emerged as a transformative\nparadigm, enabling situationally aware and perceptive next-generation wireless\nnetworks through the co-design of shared network resources. With the adoption\nof millimeter-wave (mmWave) and terahertz (THz) frequency bands, ultra-massive\nMIMO (UM-MIMO) systems and holographic surfaces unlock the potential of\nnear-field (NF) propagation, characterized by spherical wavefronts that\nfacilitate beam manipulation in both angular and range domains. This paper\npresents a unified approach to near-field beam-training and sensing,\nintroducing a dual-purpose codebook design that employs discrete Fourier\ntransform (DFT)-based codebooks for coarse estimation of sensing parameters and\npolar codebooks for parameter refinement. Leveraging these range and angle\nestimates, a customized low-complexity space-time adaptive processing (STAP)\ntechnique is proposed for NF-ISAC to detect slow-moving targets and efficiently\nmitigate clutter. The interplay between codebooks and NF-STAP framework offers\nthree key advantages: reduced communication beam training overhead, improved\nestimation accuracy, and minimal STAP computational complexity. Simulation\nresults show that the proposed framework can reduce STAP complexity by three\norders of magnitude, validating efficacy, and highlighting the potential of the\nproposed approach to seamlessly integrate NF communication and sensing\nfunctionalities in future wireless networks.",
        "Valley-based topological phases offer a wealth of exotic quantum states with\ntunable functionalities, driven by the valley degree of freedom. In this work,\nby constructing heterostructures of germanene (silicene, stanene) on various\nmagnetic substrates, we address key tuning factors such as the spin-orbit\ncoupling (SOC) strength of the substrate, magnetic orientations, and stacking\norders, all of which govern multiple valley-based topological features. We\npresent a comprehensive guiding principle for the efficient manipulation of\nthese features, achieved simply by designing and modulating the magnetic\nproperties of the underlying substrates. Specifically, increasing the SOC\nstrength of the magnetic substrate acilitates a range of topological phase\ntransitions characterized by different Chern numbers, with many systems\nexhibiting a transition from quantum valley Hall to quantum anomalous Hall\n(QAH) states. Additionally, rotating the in-plane magnetic orientation of the\nsubstrate enables tunability of the Chern number and chirality, within a\nmoderate range of SOC strength. Furthermore, the antiferromagnetic coupling of\nthe magnetic substrate can induce valley-based QAH states with substantial\nvalley gaps, leveraging its high Curie temperature (TC) to enable the\nrealization of multiple tunable magnetic topologies at elevated temperatures.\nOur findings provide a straightforward strategy for the design and manipulation\nof spintronic and valleytronic devices that can potentially operate under\nhigh-temperature conditions.",
        "Galaxy morphologies and shape orientations are expected to correlate with\ntheir large-scale environment, since they grow by accreting matter from the\ncosmic web and are subject to interactions with other galaxies. Cosmic\nfilaments are extracted in projection from the Euclid Quick Data Release 1\n(covering 63.1 $\\mathrm{deg}^2$) at $0.5<z<0.9$ in tomographic slices of 170\ncomoving $h^{-1}\\mathrm{Mpc}$ using photometric redshifts. Galaxy morphologies\nare accurately retrieved thanks to the excellent resolution of VIS data. The\ndistribution of massive galaxies ($M_* > 10^{10} M_\\odot$) in the projected\ncosmic web is analysed as a function of morphology measured from VIS data.\nSpecifically, the 2D alignment of galaxy shapes with large-scale filaments is\nquantified as a function of S\\'ersic indices and masses. We find the known\ntrend that more massive galaxies are closer to filament spines. At fixed\nstellar masses, morphologies correlate both with densities and distances to\nlarge-scale filaments. In addition, the large volume of this data set allows us\nto detect a signal indicating that there is a preferential alignment of the\nmajor axis of massive early-type galaxies along projected cosmic filaments.\nOverall, these results demonstrate our capabilities to carry out detailed\nstudies of galaxy environments with Euclid, which will be extended to higher\nredshift and lower stellar masses with the future Euclid Deep Survey.",
        "Efficient spectrum allocation has become crucial as the surge in\nwireless-connected devices demands seamless support for more users and\napplications, a trend expected to grow with 6G. Innovations in satellite\ntechnologies such as SpaceX's Starlink have enabled non-terrestrial networks\n(NTNs) to work alongside terrestrial networks (TNs) and allocate spectrum based\non regional demands. Existing spectrum sharing approaches in TNs use machine\nlearning for interference minimization through power allocation and spectrum\nsensing, but the unique characteristics of NTNs like varying orbital dynamics\nand coverage patterns require more sophisticated coordination mechanisms. The\nproposed work uses a hierarchical deep reinforcement learning (HDRL) approach\nfor efficient spectrum allocation across TN-NTN networks. DRL agents are\npresent at each TN-NTN hierarchy that dynamically learn and allocate spectrum\nbased on regional trends. This framework is 50x faster than the exhaustive\nsearch algorithm while achieving 95\\% of optimum spectral efficiency. Moreover,\nit is 3.75x faster than multi-agent DRL, which is commonly used for spectrum\nsharing, and has a 12\\% higher overall average throughput.",
        "This paper presents a novel stochastic method for modeling the transport of\nDelayed Neutron Precursors (DNPs) in liquid nuclear fuel. The method\nincorporates advection and diffusion effects into the Monte Carlo solution of\nthe neutron balance equation by leveraging the Green's function of the\nadvection-diffusion-reaction (ADR) equation. For a 1D system, we demonstrate\nthat the Green's function can be interpreted as the Probability Density\nFunction (PDF) of the position increment of a Brownian motion with drift. Using\nthis interpretation, the position of DNPs is sampled via a time-of-flight\nprocess combined with a drift and diffusion model. The method is validated on a\nmodified 1D rod problem, where results from the Monte Carlo implementation are\ncompared against those obtained using a deterministic approach. The comparison\nconfirms that the method accurately captures the impact of fuel velocity and\ndiffusion on neutron flux. As expected, the fuel velocity shifts the neutron\nflux. Reactivity decreases as a function of speed while diffusion can\ncounteract this decrease under certain conditions. While the current study is\nlimited to 1D systems, the approach could be extended to higher dimensions and\nmore complex geometries by replacing the Green's function with the Stochastic\nDifferential Equation (SDE) associated with the ADR equation.",
        "Nonlinear evolutions of two-dimensional single-mode compressible\nRayleigh--Taylor instability (RTI) with isothermal stratification are\ninvestigated in cylindrical geometry via direct numerical simulation for\ndifferent Atwood numbers ($A_T=0.1-0.9$) and Mach numbers ($Ma=0.1-0.9$). It is\nfound that the nonlinear bubble growth involves the effects of density\nstratification, vorticity accumulation and flow compressibility and shows\nconsiderable differences between convergent (acceleration acting radially\ninward) and divergent (acceleration acting radially outward) cases.\nSpecifically, the density stratification leads to non-acceleration at low $A_T$\nand high $Ma$. The accelerations in convergent cases are dominated by vorticity\naccumulation at low $A_T$ and low $Ma$ and by flow compressibility at high\n$A_T$ and high $Ma$ whereas the accelerations in divergent cases are purely\ninduced by flow compressibility at high $A_T$ and high $Ma$. Based on the\nnonlinear theory of incompressible cylindrical RTI with uniform-density\nbackground~(Zhao et al., J. Fluid Mech., vol. 900, 2020, A24), an improved\nmodel is proposed by taking the density variation, vorticity accumulation and\nflow compressibility into consideration. This model is verified by numerical\nresults and well reproduces the bubble evolution for different $A_T$ and $Ma$\nfrom linear to highly nonlinear regimes.",
        "Video-based Automatic License Plate Recognition (ALPR) involves extracting\nvehicle license plate text information from video captures. Traditional systems\ntypically rely heavily on high-end computing resources and utilize multiple\nframes to recognize license plates, leading to increased computational\noverhead. In this paper, we propose two methods capable of efficiently\nextracting exactly one frame per vehicle and recognizing its license plate\ncharacters from this single image, thus significantly reducing computational\ndemands. The first method uses Visual Rhythm (VR) to generate time-spatial\nimages from videos, while the second employs Accumulative Line Analysis (ALA),\na novel algorithm based on single-line video processing for real-time\noperation. Both methods leverage YOLO for license plate detection within the\nframe and a Convolutional Neural Network (CNN) for Optical Character\nRecognition (OCR) to extract textual information. Experiments on real videos\ndemonstrate that the proposed methods achieve results comparable to traditional\nframe-by-frame approaches, with processing speeds three times faster.",
        "This paper investigates user preferences for Linear Top-k Queries and\nDirectional Top-k Queries, two methods for ranking results in multidimensional\ndatasets. While Linear Queries prioritize weighted sums of attributes,\nDirectional Queries aim to deliver more balanced results by incorporating the\nspatial relationship between data points and a user-defined preference line.\nThe study explores how preferences for these methods vary across different\ncontexts by focusing on two real-world topics: used cars (e-commerce domain)\nand football players (personal interest domain). A user survey involving 106\nparticipants was conducted to evaluate preferences, with results visualized as\nscatter plots for comparison. The findings reveal a significant preference for\ndirectional queries in the used cars topic, where balanced results align better\nwith user goals. In contrast, preferences in the football players topic were\nmore evenly distributed, influenced by user expertise and familiarity with the\ndomain. Additionally, the study demonstrates that the two specific topics\nselected for this research exhibit significant differences in their impact on\nuser preferences. This research reveals authentic user preferences,\nhighlighting the practical utility of Directional Queries for lifestyle-related\napplications and the subjective nature of preferences in specialized domains.\nThese insights contribute to advancing personalized database technologies,\nguiding the development of more user-centric ranking systems.",
        "As gravitational wave detections increase the number of observed compact\nbinaries (consisting of neutron stars or blacks), we begin to probe the\ndifferent conditions producing these binaries. Most studies of compact remnant\nformation focus either on stellar collapse from the evolution of field binary\nstars in gas-free environments or the formation of stars in clusters where\ndynamical interactions capture the compact objects, forming binaries. But a\nthird scenario exists. In this paper, we study the fate of massive stars\nformed, accrete gas, and evolve in the dense disks surrounding supermassive\nblack holes. We calculate the explosions produced and compact objects formed by\nthe collapse of these massive stars. Nucleosynthetic yields may provide an\nideal, directly observable, diagnostic of the formation and fate of these stars\nin active galactic nuclei. We present a first study of the explosive yields\nfrom these stars, comparing these yields with the observed nucleosynthetic\nsignatures in the disks around supermassive stars with quasars. We show that,\neven though these stars tend to form black holes, their rapid rotation leads to\ndisks that can eject a considerable amount of iron during the collapse of the\nstar. The nucleosynthetic yields from these stars can produce constraints on\nthe number of systems formed in this manner, but further work is needed to\nexploit variations from the initial models presented in this paper.",
        "Large Language Models (LLMs) have become state-of-the-art in Machine\nTranslation (MT), often trained on massive bilingual parallel corpora scraped\nfrom the web, that contain low-quality entries and redundant information,\nleading to significant computational challenges. Various data filtering methods\nexist to reduce dataset sizes, but their effectiveness largely varies based on\nspecific language pairs and domains. This paper evaluates the impact of\ncommonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on\nEnglish-Polish translation within the biomedical domain. By filtering the UFAL\nMedical Corpus, we created varying dataset sizes to fine-tune the mBART50\nmodel, which was then evaluated using the SacreBLEU metric on the Khresmoi\ndataset, having the quality of translations assessed by bilingual speakers. Our\nresults show that both LASER and MUSE can significantly reduce dataset sizes\nwhile maintaining or even enhancing performance. We recommend the use of LASER,\nas it consistently outperforms the other methods and provides the most fluent\nand natural-sounding translations.",
        "Scaling quantum computing requires networked systems, leveraging HPC for\ndistributed simulation now and quantum networks in the future. Quantum\ndatacenters will be the primary access point for users, but current approaches\ndemand extensive manual decisions and hardware expertise. Tasks like algorithm\npartitioning, job batching, and resource allocation divert focus from quantum\nprogram development. We present a massively parallelized, automated QAOA\nworkflow that integrates problem decomposition, batch job generation, and\nhigh-performance simulation. Our framework automates simulator selection,\noptimizes execution across distributed, heterogeneous resources, and provides a\ncloud-based infrastructure, enhancing usability and accelerating quantum\nprogram development. We find that QAOA partitioning does not significantly\ndegrade optimization performance and often outperforms classical solvers. We\nintroduce our software components -- Divi, Maestro, and our cloud platform --\ndemonstrating ease of use and superior performance over existing methods.",
        "Excitation energy transfer between photosynthetic light-harvesting complexes\nis vital for highly efficient primary photosynthesis. Controlling this process\nis the key for advancing the emerging artificial photosynthetic systems. Here,\nwe experimentally demonstrate the enhanced excitation energy transfer between\nphotosynthetic light-harvesting 2 complexes (LH2) mediated through the\nFabry-Perot optical microcavity. Using intensity-dependent pump-probe\nspectroscopy, we analyse the exciton-exciton annihilation (EEA) due to\ninter-LH2 energy transfer. Comparing EEA in LH2 within cavity samples and the\nbare LH2 films, we observe enhanced EEA in cavities indicating improved\nexcitation energy transfer via coupling to a common cavity mode. Surprisingly,\nthe effect remains even in the weak coupling regime. The enhancement is\nattributed to the additional connectivity between LH2s introduced by the\nresonant optical microcavity. Our results suggest that optical microcavities\ncan be a strategic tool for modifying excitation energy transfer between\nmolecular complexes, offering a promising approach towards efficient artificial\nlight harvesting.",
        "The asymptotic behavior of GMM estimators depends critically on whether the\nunderlying moment condition model is correctly specified. Hong and Li (2023,\nEconometric Theory) showed that GMM estimators with nonsmooth\n(non-directionally differentiable) moment functions are at best\n$n^{1\/3}$-consistent under misspecification. Through simulations, we verify the\nslower convergence rate of GMM estimators in such cases. For the two-step GMM\nestimator with an estimated weight matrix, our results align with theory.\nHowever, for the one-step GMM estimator with the identity weight matrix, the\nconvergence rate remains $\\sqrt{n}$, even under severe misspecification.",
        "The partial alignment and conflict of autonomous agents lead to mixed-motive\nscenarios in many real-world applications. However, agents may fail to\ncooperate in practice even when cooperation yields a better outcome. One well\nknown reason for this failure comes from non-credible commitments. To\nfacilitate commitments among agents for better cooperation, we define Markov\nCommitment Games (MCGs), a variant of commitment games, where agents can\nvoluntarily commit to their proposed future plans. Based on MCGs, we propose a\nlearnable commitment protocol via policy gradients. We further propose\nincentive-compatible learning to accelerate convergence to equilibria with\nbetter social welfare. Experimental results in challenging mixed-motive tasks\ndemonstrate faster empirical convergence and higher returns for our method\ncompared with its counterparts. Our code is available at\nhttps:\/\/github.com\/shuhui-zhu\/DCL.",
        "Industrial ads ranking systems conventionally rely on labeled impression\ndata, which leads to challenges such as overfitting, slower incremental gain\nfrom model scaling, and biases due to discrepancies between training and\nserving data. To overcome these issues, we propose a Unified framework for\nKnowledge-Distillation and Semi-supervised Learning (UKDSL) for ads ranking,\nempowering the training of models on a significantly larger and more diverse\ndatasets, thereby reducing overfitting and mitigating training-serving data\ndiscrepancies. We provide detailed formal analysis and numerical simulations on\nthe inherent miscalibration and prediction bias of multi-stage ranking systems,\nand show empirical evidence of the proposed framework's capability to mitigate\nthose. Compared to prior work, UKDSL can enable models to learn from a much\nlarger set of unlabeled data, hence, improving the performance while being\ncomputationally efficient. Finally, we report the successful deployment of\nUKDSL in an industrial setting across various ranking models, serving users at\nmulti-billion scale, across various surfaces, geological locations, clients,\nand optimize for various events, which to the best of our knowledge is the\nfirst of its kind in terms of the scale and efficiency at which it operates."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Convex generalizations of total variation based on the structure tensor with applications to inverse problems",
    "start_abstract":"We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Parameter-Free FISTA by Adaptive Restart and Backtracking"
      ],
      "abstract":[
        "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Introduction of the G$_2$-Ricci Flow: Geometric Implications for\n  Spontaneous Symmetry Breaking and Gauge Boson Masses",
        "A bang-bang optimal control for a nonlinear system modeling the Gate\n  Control Theory of Pain",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "Major-Minor Mean Field Game of Stopping: An Entropy Regularization\n  Approach",
        "TOI-512: Super-Earth transiting a K-type star discovered by TESS and\n  ESPRESSO",
        "Chung-Graham and Zeckendorf representations",
        "The Layered Catalan Monoids: Structure and Determinants",
        "Multi-Channel Currency: A Secure Method Using Semi-Quantum Tokens",
        "High Resolution {\\it BOES} Spectroscopy of Raman-scattered\n  He~II$\\lambda$6545 in Young Planetary Nebulae",
        "Reducing Simulation Effort for RIS Optimization using an Efficient\n  Far-Field Approximation",
        "Learning in Markets with Heterogeneous Agents: Dynamics and Survival of\n  Bayesian vs. No-Regret Learners",
        "Large thermoelectric spin-valve effect with a superconductor",
        "High Harmonic Generation with Orbital Angular Momentum Beams:\n  Beyond-dipole Corrections",
        "Polynomial sequences with the same recurrence relation as Chebyshev\n  polynomials and the minimal polynomial of $2\\cos (2\\pi \/n)$",
        "Preference learning made easy: Everything should be understood through\n  win rate",
        "Classical Attack on Bell Inequalities",
        "Topological altermagnetic Josephson junctions",
        "On the Khovanov homology of 3-braids",
        "Bayesian estimation of Unit-Weibull distribution based on dual\n  generalized order statistics with application to the Cotton Production Data",
        "The feasibility of multi-graph alignment: a Bayesian approach",
        "Tate's question, Standard conjecture D, semisimplicity and Dynamical\n  degree comparison conjecture",
        "Multiplicity $=$ Volume formula and Newton non-degenerate ideals in\n  regular local rings",
        "Fast and Precise Spectral Analysis for Dark Matter Searches with LIGO",
        "Quantum effects in near-extremal charged black hole spacetimes",
        "Exact Stark analytical function for H{\\alpha} line based on the FFM\n  Model related with plasma parameters",
        "Well-Posedness of Contact Discontinuity Solutions and Vanishing Pressure\n  Limit for the Aw-Rascle Traffic Flow Model",
        "Slit-Slide-Sew bijections for planar bipartite maps with prescribed\n  degree",
        "Reinforced Galton--Watson processes III: Empirical offspring\n  distributions",
        "Integrated Computation and Communication with Fiber-optic Transmissions"
      ],
      "abstract":[
        "This work introduces the G$_2$-Ricci flow on seven-dimensional manifolds with\nnon-zero torsion and explores its physical implications. By extending the Ricci\nflow to manifolds with G$_2$ structures, we study the evolution of solitonic\nsolutions and their role in spontaneous symmetry breaking in gauge theories. In\nparticular, this model proposes that the masses of the W and Z bosons are\ndetermined not by an external scalar field, as in the Higgs mechanism, but by\nthe intrinsic geometric torsion of the manifold. Furthermore, a possible\nconnection between the geometry of extra dimensions and the curvature of our\nspacetime is explored, with implications for the experimentally observed\npositive cosmological constant. This approach provides an innovative\ninterpretation of fundamental interactions in theoretical physics, opening new\npossibilities for studying extra dimensions and the geometry of\nG$_2$-manifolds.",
        "We consider a nonlinear system of coupled ordinary differential equations\n(representing the excitatory, inhibitory, and T-cell potentials) based on the\nGate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in\n1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in\n1988.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "This paper studies a discrete-time major-minor mean field game of stopping\nwhere the major player can choose either an optimal control or stopping time.\nWe look for the relaxed equilibrium as a randomized stopping policy, which is\nformulated as a fixed point of a set-valued mapping, whose existence is\nchallenging by direct arguments. To overcome the difficulties caused by the\npresence of a major player, we propose to study an auxiliary problem by\nconsidering entropy regularization in the major player's problem while\nformulating the minor players' optimal stopping problems as linear programming\nover occupation measures. We first show the existence of regularized equilibria\nas fixed points of some simplified set-valued operator using the\nKakutani-Fan-Glicksberg fixed-point theorem. Next, we prove that the\nregularized equilibrium converges as the regularization parameter $\\lambda$\ntends to 0, and the limit corresponds to a fixed point of the original\noperator, thereby confirming the existence of a relaxed equilibrium in the\noriginal mean field game problem.",
        "One of the goals of the ESPRESSO guaranteed time observations (GTOs) at the\nESO 8.2m telescope is to follow up on candidate planets from transit surveys\nsuch as the TESS mission. High-precision radial velocities are required to\ncharacterize small exoplanets. Aims. We intend to confirm the existence of a\ntransiting super-Earth around the bright (V=9.74) K0-type star TOI-512 (TIC\n119292328) and provide a characterization. Combining photometric data from TESS\nand 37 high-resolution spectroscopic observations from ESPRESSO in a joint\nMarkov chain Monte Carlo analysis, we determined the planetary parameters of\nTOI-512b and characterized its internal structure. We find that TOI-512b is a\nsuper-Earth, with a radius of $1.54 \\pm 0.10$ R$_\\oplus$ and mass of\n$3.57_{-0.55}^{+0.53}$~M$_\\oplus$, on a $7.19_{-6.1\\cdot 10^{-5}}^{+7\\cdot\n10^{-5}}$ day orbit. This corresponds to a bulk density of\n$5.62_{-1.28}^{+1.59}$ g cm$^{-3}$. Our interior structure analysis presents a\nsmall inner core representing $0.13^{+0.13}_{-0.11}$ of the solid mass fraction\nfor the planet, surrounded by a mantle with a mass fraction of\n$0.69^{+0.20}_{-0.22}$, and an upper limit of the water layer of $0.16$. The\ngas mass below $10^{-8.93}$ indicates a very small amount of gas on the planet.\nWe find no evidence of the second candidate found by the TESS pipeline,\nTOI-512.02, neither in TESS photometry, nor in the ESPRESSO radial velocities.\nThe low stellar activity makes it an interesting transmission spectroscopy\ncandidate for future-generation instruments.",
        "We examine the relationship between the Chung-Graham and Zeckendorf\nrepresentations of an integer using the software package {\\tt Walnut}.",
        "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
        "Digital currencies primarily operate online, but there is growing interest in\nenabling offline transactions to improve digital inclusion. Existing offline\nmethods struggle with double-spending risks, often limiting transaction\namounts. In this work, we propose a quantum-state-based currency system that\nuses the non-cloning theorem to enable secure, multi-channel transactions\nwithout the risk of double spending. We demonstrate this system's\nimplementation with experimental results, including use cases for currency\ntransfers and swaps. To mitigate credit risks in swaps, we also integrate\nblockchain to show its wide applicability. Our approach paves the way for\nquantum-secure digital currencies and opens new possibilities for optimizing\nmulti-channel tokens.",
        "Young planetary nebulae (PNe) are characterized by their hot central stars\nand the presence of abundant neutral and molecular components, which result\nfrom significant mass loss during the asymptotic giant branch (AGB) phase of\nstellar evolution. Far-UV \\ion{He}{2}$\\lambda$1025 line photons produced near\nthe central star can undergo Raman scattering by hydrogen atoms, creating a\nbroad emission feature centered at $\\sim$ 6545~\\AA. We conducted\nhigh-resolution spectroscopy of 12 young PNe from April 2019 to March 2020\nusing the Bohyunsan Observatory Echelle Spectrograph ({\\it BOES}). Building on\nthe study by Choi and Lee, who identified Raman-scattered \\ion{He}{2} at\n6545~\\AA\\ in NGC~6881 and NGC~6886, we report new detections of this feature in\nNGC~6741 and NGC~6884. Profile fitting reveals that the velocity of the\n\\ion{H}{1} component relative to the \\ion{He}{2} emission region ranges from\n$26-33~{\\rm km~s^{-1}}$ in these PNe. Using photoionization modeling, we\nestimate the line flux of \\ion{He}{2}$\\lambda$1025 and derive Raman conversion\nefficiencies of 0.39, 0.21, 0.24, and 0.07 for NGC~6881, NGC~6741, NGC~6886,\nand NGC~6884, respectively. These results, combined with radiative transfer\nmodeling, suggest the presence of \\ion{H}{1} components with masses around\n$10^{-2}~M_\\odot$, moving outward from the central \\ion{He}{2} emission region\nat speeds characteristic of the slow stellar wind from a mass-losing giant\nstar.",
        "Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously\nintroduced method is effective, but time-consuming, because multiport impedance\nor scatter matrices are required for each transmitter and receiver position,\nwhich generally must be obtained through full-wave simulation. Herein, a simple\nand efficient far-field approximation is introduced, to extrapolate scatter\nmatrices for arbitrary receiver and transmitter positions from only a single\nsimulation while still maintaining high accuracy suitable for optimization\npurposes. This is demonstrated through comparisons of the optimized capacitance\nvalues and further supported by empirical measurements.",
        "We analyze the performance of heterogeneous learning agents in asset markets\nwith stochastic payoffs. Our agents aim to maximize the expected growth rate of\ntheir wealth but have different theories on how to learn this best. We focus on\ncomparing Bayesian and no-regret learners in market dynamics. Bayesian learners\nwith a prior over a finite set of models that assign positive prior probability\nto the correct model have posterior probabilities that converge exponentially\nto the correct model. Consequently, they survive even in the presence of agents\nwho invest according to the correct model of the stochastic process. Bayesians\nwith a continuum prior converge to the correct model at a rate of $O((\\log\nT)\/T)$. Online learning theory provides no-regret algorithms for maximizing the\nlog of wealth in this setting, achieving a worst-case regret bound of $O(\\log\nT)$ without assuming a steady underlying stochastic process but comparing to\nthe best fixed investment rule. This regret, as we observe, is of the same\norder of magnitude as that of a Bayesian learner with a continuum prior.\nHowever, we show that even such low regret may not be sufficient for survival\nin asset markets: an agent can have regret as low as $O(\\log T)$, but still\nvanish in market dynamics when competing against agents who invest according to\nthe correct model or even against a perfect Bayesian with a finite prior. On\nthe other hand, we show that Bayesian learning is fragile, while no-regret\nlearning requires less knowledge of the environment and is therefore more\nrobust. Any no-regret learner will drive out of the market an imperfect\nBayesian whose finite prior or update rule has even small errors. We formally\nestablish the relationship between notions of survival, vanishing, and market\ndomination studied in economics and the framework of regret minimization, thus\nbridging these theories.",
        "Recent studies have revealed magnetically controllable thermoelectric effects\nin superconductor\/ferromagnet (S\/F) structures. A tunable cryogenic\nthermoelectric generator needs not only a high conversion factor between\nelectricity and heat, but also a large change in the thermoelectric output when\nswitching the magnetic state of the device. Here, we experimentally measure and\nnumerically model thermoelectric effects in fully epitaxial F\/S\/F junctions\nbased on commercially available, easily grown materials, as well as their\ndependence on the magnetic configuration of the F electrodes. We observe\nsizeable Seebeck coefficients for the parallel alignment of the ferromagnetic\nelectrodes, reaching values of about $100$~$\\mu$V\/K. Importantly, we find a\ndecrease of the thermoelectric signal of more than an order of magnitude when\nswitching from a parallel to an antiparallel configuration, constituting a\nlarge thermoelectric spin-valve effect. Theoretical modeling based on a\nself-consistent non-equilibrium Keldysh-Usadel Green function theory, combined\nwith micromagnetic simulations, qualitatively reproduce the experimental\nfindings. These findings pave the way for the development of efficient and\nversatile cryogenic thermoelectric heat engines.",
        "We study the high harmonic generation with vortex beams beyond the dipole\napproximation. To do so we employ the full minimal coupling approach to account\nfor multipolar coupling without truncation and describe the full\nspatio-temporal properties of the electromagnetic field. This allows us to\ninvestigate the beyond-dipole deviations in electron trajectories and the\nemitted power, where the influence of the orbital angular momentum contains\nboth magnetic and quadrupolar effects. In contrast to the system driven by\nplane-wave light, we show that the non-linear dipole dynamics induced by the\nvortex beams are not confined to the polarization or propagation directions,\nbut also have a component in the orthogonal direction. We identify the effects\nof the resulting symmetry breaking via increased beyond dipole corrections\nwhich are particularly apparent in even harmonics.",
        "In this paper we consider the minimal polynomial $\\psi_n(x)$ of $2\\cos (2\\pi\n\/n)$. We introduce some polynomial sequences with the same recurrence relation\nas the rescaled Chebyshev polynomials $t_n(x)=2\\, T_n(x\/2)$ of the first kind,\nwhich turn out to be related to those of various kinds, all coming from those\nof the second kind. We see that $t_n(x)\\pm 2=2(T_n(x\/2)\\pm 1)$ are divisible by\nthe square of either of these polynomials. Then by appropriately removing\nunnecessary factors from these polynomials, we can easily calculate\n$\\psi_n(x)$, which improves Barnes' result in 1977. As an appendix, we give a\ncompact list of the minimal polynomials $\\psi_n(x)$ of $2\\cos (2\\pi \/n)$ for\n$n\\leqslant 120$.",
        "Preference learning, or the task of aligning generative models to preference\ncomparison data, has yet to reach the conceptual maturity of classification,\ndensity estimation, etc. To close this gap, this work presents a framework to\nunderstand preference learning starting from the sampling distribution of\npairwise preference data. First, we prove that the only evaluation of a\ngenerative model that respects both preferences and prevalences in the data\ndistribution is a form of win rate, justifying win rate as the focal point to\nunderstand preference learning. We then analyze preference learning methods as\nwin rate optimization (WRO) or non-WRO. We present novel instances of WRO\nbeyond existing examples (RLHF, NLHF) and identify two key theoretical benefits\nof all such methods. We prove that common non-WRO methods like DPO and SFT on\npreferred samples lack these properties and suggest ways to mitigate such\ntheoretical limitations. We also show that WRO underperforms in practice due\noptimization difficulties and that optimization success predicts performance\nbetter than choices which affect the objective's solution. Our analysis\nhighlights best practices for existing methods and provides recommendations for\nfuture research, guided by the principle that one should either align non-WRO\nmethods more closely with WRO or improve the optimization of WRO objectives.",
        "Representing multi-mode squeezed light with a Gaussian random vector, our\nlocally deterministic detection model challenges the CHSH game, achieving\nfidelities exceeding 96\\%. Squeezing strength, detector threshold, and\nefficiency influence the security of the quantum bound.",
        "Planar Josephson junctions are pivotal for engineering topological\nsuperconductivity, yet are severely hindered by orbital effects induced by\nin-plane magnetic fields. In this work, we introduce the generic topological\naltermagnetic Josephson junctions (TAJJs) by leveraging the intrinsic\nspin-polarized band splitting and zero net magnetization attributes of\naltermagnets. Our proposed TAJJs effectively mitigate the detrimental orbital\neffects while robustly hosting Majorana end modes (MEMs) at both ends of the\njunction. Specifically, we demonstrate that MEMs emerge in $d_{x^2-y^2}$-wave\nTAJJs but vanish in the $d_{xy}$-wave configuration, thereby establishing the\ncrystallographic orientation angle $\\theta$ of the altermagnet as a novel\ncontrol parameter of topology. The distinct spin-polarization of the MEMs\nprovides an unambiguous experimental signature for the spin-resolved\nmeasurement. Furthermore, by harnessing the synergy between the\n$d_{x^2-y^2}$-wave altermagnet and its superconducting counterpart, our\nproposal extends to high-$T_c$ platforms naturally. Overall, this work\nestablishes altermagnets as a versatile paradigm for realizing topological\nsuperconductivity, bridging conceptual innovations with scalable quantum\narchitectures devoid of orbital effects and stray fields.",
        "We prove the conjecture of Przytycki and Sazdanovic that the Khovanov\nhomology of the closure of a 3-stranded braid only contains torsion of order 2.\nThis conjecture has been known for six out of seven classes in the\nMurasugi-classification of 3-braids and we show it for the remaining class. Our\nproof also works for the other classes and relies on Bar-Natan's version of\nKhovanov homology for tangles as well as his delooping and cancellation\ntechniques, and the reduced integral Bar-Natan--Lee--Turner spectral sequence.\nWe also show that the Knight-move conjecture holds for 3-braids.",
        "The Unit Weibull distribution with parameters $\\alpha$ and $\\beta$ is\nconsidered to study in the context of dual generalized order statistics. For\nthe analysis purpose, Bayes estimators based on symmetric and asymmetric loss\nfunctions are obtained. The methods which are utilized for Bayesian estimation\nare approximation and simulation tools such as Lindley, Tierney-Kadane and\nMarkov chain Monte Carlo methods. The authors have considered squared error\nloss function as symmetric and LINEX and general entropy loss function as\nasymmetric loss functions. After presenting the mathematical results, a\nsimulation study is conducted to exhibit the performances of various derived\nestimators. As this study is considered for the dual generalized order\nstatistics that is unification of models based distinct ordered random variable\nsuch as order statistics, record values, etc. This provides flexibility in our\nresults and in continuation of this, the cotton production data of USA is\nanalyzed for both submodels of ordered random variables: order statistics and\nrecord values.",
        "We establish thresholds for the feasibility of random multi-graph alignment\nin two models. In the Gaussian model, we demonstrate an \"all-or-nothing\"\nphenomenon: above a critical threshold, exact alignment is achievable with high\nprobability, while below it, even partial alignment is statistically\nimpossible. In the sparse Erd\\H{o}s-R\\'enyi model, we rigorously identify a\nthreshold below which no meaningful partial alignment is possible and\nconjecture that above this threshold, partial alignment can be achieved. To\nprove these results, we develop a general Bayesian estimation framework over\nmetric spaces, which provides insight into a broader class of high-dimensional\nstatistical problems.",
        "Let $X$ be a smooth projective variety of dimension $n$ over the algebraic\nclosure of a finite field $\\mathbb{F}_p$.\n  Assuming the standard conjecture $D$, we prove\n  a weaker form of the Dynamical Degree Comparison conjecture;\n  equivalence of semisimplicity of Frobenius endomorphism and of any polarized\nendomorphism (a more general result, in terms of the biggest size of Jordan\nblocks, holds).\n  We illustrate these results through examples, including varieties dominated\nby rational maps from Abelian varieties and suitable products of $K3$ surfaces.\n  Using the same idea, we provide a new proof of the main result in a recent\npaper by the third author, including Tate's question\/Serre's conjecture that\nfor a polarized endomorphism $f:X\\rightarrow X$, all eigenvalues of the action\nof $f$ on $H^k(X)$ have the same absolute value.",
        "We develop the notions of Newton non-degenerate (NND) ideals and Newton\npolyhedra for regular local rings. These concepts were first defined in the\ncontext of complex analysis. We show that the characterization of NND ideals\nvia their integral closures known in the analytical setting extends to regular\nlocal rings. We use the limiting body $\\mathcal{C}(\\mathcal{I})$ associated to\na graded family $\\mathcal{I}$ of ideals to provide a new understanding of the\ncelebrated \"Multiplicity $=$ Volume\" formula. Particularly, we prove that, for\na Noetherian graded family $\\mathcal{I}$ of $\\mathfrak{m}$-primary ideals in a\nregular local ring $(R,\\mathfrak{m})$ of dimension $d$, the equality\n$$e(\\mathcal{I}) = d!\\text{co-vol}_d(\\mathcal{C}(\\mathcal{I}))$$ holds if and\nonly if $\\mathcal{I}$ contains certain subfamily of NND ideals.",
        "We introduce a novel logarithmic spectral estimation method for dark matter\nsearches using gravitational-wave detectors, integrating established dark\nmatter search techniques with insights from computer music analysis. By\nleveraging symmetries between the time and frequency domains, this method\nmatches the computational efficiency of FFT based algorithms without, unlike\nsuch algorithms, compromising precision. We apply this approach to data from\nLIGO's third observing run, directly comparing its performance with that of a\nprevious search. Our results show a consistent 15 percent improvement across\nnearly the entire frequency range, without additional computational costs. With\npotential for further refinements, this method already offers a solution\ncapable of maximizing the scientific potential of current and future\ngravitational-wave observatories.",
        "We compute the semiclassical current and stress-energy fluxes both at the\nevent and Cauchy horizon of a near-extremal Reissner-Nordstr\\\"om black hole. We\nconsider a minimally-coupled, massless, charged scalar field in the Unruh\nstate, describing an evaporating black hole. The near-extremal domain allows\nfor an analytical treatment of the scattering problem of the Boulware modes\nboth in the interior and exterior regions. We present this and explicit\nanalytical expressions for $\\langle j_v\\rangle $, $\\langle T_{vv}\\rangle$ at\nthe horizons, as well as estimates for $\\langle j_u\\rangle$ and $\\langle\nT_{uu}\\rangle $. We cross-check the analytical results numerically by bringing\nthe radial Klein-Gordon equation into the form of the confluent Heun equation.\nInserting these expectation values as sources to the Einstein-Maxwell\nequations, we find that at least in the near-extremal regime of small field\ncharge, quantum effects drive the black hole interior away from extremality.\nOur work generalizes the known results for the real scalar field [Phys. Rev. D\n104, 024066] and is in agreement with recent work on charged fields in\nexpanding Reissner-Nordstr\\\"om deSitter universes [Phys. Rev. Lett. 127,\n231301, Phys. Rev. D 104, 025009].",
        "Optical Emission Spectroscopy is a widely used technique for plasma\ndiagnosis, with particular interest in hydrogen atomic emission due to its\nprevalence in plasmas. However, accurately determining plasma parameters like\nelectron density, electron temperature, and gas temperature starting from the\nexperimental profiles remains a challenge. This paper introduces a\ncomprehensive model for Stark broadening of the H{\\alpha} line in a wide range\nof plasma conditions, addressing the limitations of existing analytical\nexpressions for line shapes. The proposed model encompasses the full splitting\nof the transition into fifteen Lorentzian profiles and electric micro-field\nfluctuations surrounding the emitting atoms due to collisions with charged\nparticles. Starting from accurate spectral data obtained from realistic\ncomputer simulations, fitting parameters of the model, have been obtained by\nusing an optimization method based on a genetic algorithm. The set of\nparameters of the model are reported for a wide range of plasma conditions. The\nbehavior of these parameters is analyzed to understand their dependence in\nterms of the electron density and temperature and gas density of the plasma.\nThe model parameters here obtained constitute a useful tool in plasma diagnosis\nto obtaining the values of the physical parameters of the plasma starting from\nthe experimental profiles.",
        "This paper investigates the well-posedness of contact discontinuity solutions\nand the vanishing pressure limit for the Aw-Rascle traffic flow model with\ngeneral pressure functions. The well-posedness problem is formulated as a free\nboundary problem, where initial discontinuities propagate along linearly\ndegenerate characteristics. By mollifying initial data in Lagrangian\ncoordinates, the problem is reduced to analyzing the limit of classical\nsolutions. To avoid equation degeneracy due to vacuum states, we establish a\nuniform lower bound for density by categorizing discontinuity types at singular\npoints. Using level sets for velocity derivatives, we derive uniform estimates\nfor density and velocity gradients. Through equivalence of coordinate\ntransformations, the well-posedness of contact discontinuity solutions in\noriginal coordinates is rigorously proven. Results show that compressive\ninitial data induce finite-time singularity formation, while rarefactive\ninitial data ensure global existence. Furthermore, we demonstrate that\nsolutions of the Aw-Rascle model converge to those of the pressureless model\nunder vanishing pressure, with matching convergence rates for velocity and\ncharacteristic triangles. By enhancing regularity in non-discontinuous regions,\nwe prove convergence of the Aw-Rascle model's blow-up time to the pressureless\nmodel's blow-up time. Finally, analogous results are extended to the Chaplygin\ngas pressure case.",
        "We present a bijective proof for the planar case of Louf's counting formula\non bipartite planar maps with prescribed face degree, that arises from the Toda\nhierarchy. We actually show that his formula hides two simpler formulas, both\nof which can be rewritten as equations on trees using duality and Schaeffer's\nbijection for eulerian maps. We prove them bijectively and show that the\nconstructions we provide for trees can also be interpreted as \"slit-slide-sew\"\noperations on maps. As far as we know, this is the first bijection for a\nformula arising from an integrable hierarchy with infinitely many parameters.",
        "Reinforced Galton--Watson processes describe the dynamics of a population\nwhere reproduction events are reinforced, in the sense that offspring numbers\nof forebears can be repeated randomly by descendants. More specifically, the\nevolution depends on the empirical offspring distribution of each individual\nalong its ancestral lineage. We are interested here in asymptotic properties of\nthe empirical distributions observed in the population, such as concentration,\nevanescence and persistence. For this, we incorporate tools from the theory of\nlarge deviations to our preceding analysis [arXiv:2306.02476,arXiv:2310.19030].",
        "Fiber-optic transmission systems are leveraged not only as high-speed\ncommunication channels but also as nonlinear kernel functions for machine\nlearning computations, enabling the seamless integration of computational\nintelligence and communication."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"On the Distribution of the Two-Sample Cramer-von Mises Criterion",
    "start_abstract":"The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",
    "start_categories":[
      "q-fin.GN"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Quant GANs: deep generation of financial time series"
      ],
      "abstract":[
        "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Performance of the ALICE Inner Tracking System 2",
        "Empowering the Future Workforce: Prioritizing Education for the\n  AI-Accelerated Job Market",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "Human Re-ID Meets LVLMs: What can we expect?",
        "Providing Machine Learning Potentials with High Quality Uncertainty\n  Estimates",
        "Causal AI-based Root Cause Identification: Research to Practice at Scale",
        "Non-local modular flows across deformed null-cuts",
        "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language\n  Models",
        "2D transmons with lifetimes and coherence times exceeding 1 millisecond",
        "Validation of the DESI DR2 Measurements of Baryon Acoustic Oscillations\n  from Galaxies and Quasars",
        "Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent\n  Hypersonic Flows on Arbitrary Grids",
        "On the Effectiveness of Random Weights in Graph Neural Networks",
        "3+1 neutrino mixings model with $A_4$ triplet Majorana neutrino",
        "Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism,\n  Genhumanism",
        "regMMD: a package for parametric estimation and regression with maximum\n  mean discrepancy",
        "A proof of the multi-component $q$-Baker--Forrester conjecture",
        "Post-Newtonian theory-inspired framework for characterizing eccentricity\n  in gravitational waveforms",
        "Programs Versus Finite Tree-Programs",
        "Uniform stability for the inverse Sturm-Liouville problem with\n  eigenparameter-dependent boundary conditions",
        "Evidence for a hot galactic halo around the Andromeda Galaxy using fast\n  radio bursts",
        "Abelian varieties analogs of two results about algebraic curves",
        "Existence of Constant Mean Curvature Surfaces in Asymptotically Flat and\n  Asymptotically Hyperbolic Manifolds",
        "Around the Merino--Welsh conjecture: improving Jackson's inequality",
        "Predictive Performance of Photonic SRAM-based In-Memory Computing for\n  Tensor Decomposition",
        "Training Data Provenance Verification: Did Your Model Use Synthetic Data\n  from My Generative Model for Training?",
        "Dephasing-tolerant quantum sensing for transverse magnetic fields with\n  spin qudits",
        "A three-family supersymmetric Pati-Salam model from rigid intersecting\n  D6-branes",
        "Harnessing Generative Pre-Trained Transformer for Datacenter Packet\n  Trace Generation",
        "Revisiting the $\\phi^6$ Theory in Three Dimensions at Large $N$"
      ],
      "abstract":[
        "The upgraded Inner Tracking System (ITS2) of the ALICE experiment at the CERN\nLarge Hadron Collider is based on Monolithic Active Pixel Sensors (MAPS). With\na sensitive area of about 10 $m^2$ and 12.5 billion pixels, ITS2 represents the\nlargest pixel detector in high-energy physics. The detector consists of seven\nconcentric layers equipped with ALPIDE pixel sensors manufactured in the\nTowerJazz 180 nm CMOS Imaging Sensor process. The high spatial resolution and\nlow material budget, in combination with small radial distance of the innermost\nlayer from the interaction point, make the detector well suited for secondary\nvertex reconstruction as well as for tracking at low transverse momentum.\n  This paper will present the detector performance during the LHC Run 3 and\ngive an overview on the calibration methods and running experience.",
        "AI's rapid integration into the workplace demands new approaches to workforce\neducation and training and broader AI literacy across disciplines. Coordinated\naction from government, industry, and educational institutions is necessary to\nensure workers can adapt to accelerating technological change.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "Large vision-language models (LVLMs) have been regarded as a breakthrough\nadvance in an astoundingly variety of tasks, from content generation to virtual\nassistants and multimodal search or retrieval. However, for many of these\napplications, the performance of these methods has been widely criticized,\nparticularly when compared with state-of-the-art methods and technologies in\neach specific domain. In this work, we compare the performance of the leading\nlarge vision-language models in the human re-identification task, using as\nbaseline the performance attained by state-of-the-art AI models specifically\ndesigned for this problem. We compare the results due to ChatGPT-4o,\nGemini-2.0-Flash, Claude 3.5 Sonnet, and Qwen-VL-Max to a baseline ReID\nPersonViT model, using the well-known Market1501 dataset. Our evaluation\npipeline includes the dataset curation, prompt engineering, and metric\nselection to assess the models' performance. Results are analyzed from many\ndifferent perspectives: similarity scores, classification accuracy, and\nclassification metrics, including precision, recall, F1 score, and area under\ncurve (AUC). Our results confirm the strengths of LVLMs, but also their severe\nlimitations that often lead to catastrophic answers and should be the scope of\nfurther research. As a concluding remark, we speculate about some further\nresearch that should fuse traditional and LVLMs to combine the strengths from\nboth families of techniques and achieve solid improvements in performance.",
        "Computational chemistry has come a long way over the course of several\ndecades, enabling subatomic level calculations particularly with the\ndevelopment of Density Functional Theory (DFT). Recently, machine-learned\npotentials (MLP) have provided a way to overcome the prevalent time and length\nscale constraints in such calculations. Unfortunately, these models utilise\ncomplex and high dimensional representations, making it challenging for users\nto intuit performance from chemical structure, which has motivated the\ndevelopment of methods for uncertainty quantification. One of the most common\nmethods is to introduce an ensemble of models and employ an averaging approach\nto determine the uncertainty. In this work, we introduced Bayesian Neural\nNetworks (BNNs) for uncertainty aware energy evaluation as a more principled\nand resource efficient method to achieve this goal. The richness of our\nuncertainty quantification enables a new type of hybrid workflow where\ncalculations can be offloaded to a MLP in a principled manner.",
        "Modern applications are built as large, distributed systems spanning numerous\nmodules, teams, and data centers. Despite robust engineering and recovery\nstrategies, failures and performance issues remain inevitable, risking\nsignificant disruptions and affecting end users. Rapid and accurate root cause\nidentification is therefore vital to ensure system reliability and maintain key\nservice metrics.\n  We have developed a novel causality-based Root Cause Identification (RCI)\nalgorithm that emphasizes causation over correlation. This algorithm has been\nintegrated into IBM Instana-bridging research to practice at scale-and is now\nin production use by enterprise customers. By leveraging \"causal AI,\" Instana\nstands apart from typical Application Performance Management (APM) tools,\npinpointing issues in near real-time. This paper highlights Instana's advanced\nfailure diagnosis capabilities, discussing both the theoretical underpinnings\nand practical implementations of the RCI algorithm. Real-world examples\nillustrate how our causality-based approach enhances reliability and\nperformance in today's complex system landscapes.",
        "Modular flows probe important aspects of the entanglement structures,\nespecially those of QFTs, in a dynamical framework. Despite the expected\nnon-local nature in the general cases, the majority of explicitly understood\nexamples feature local space-time trajectories under modular flows. In this\nwork, we study a particular class of non-local modular flows. They are\nassociated with the relativistic vacuum state and sub-regions whose boundaries\nlie on a planar null-surface. They satisfy a remarkable algebraic property\nknown as the half-sided modular inclusion, and as a result the modular\nHamiltonians are exactly known in terms of the stress tensor operators. To be\nexplicit, we focus on the simplest QFT of a massive or massless free scalar in\n$2+1$ dimensions. We obtain explicit expressions for the generators. They can\nbe separated into a sum of local and non-local terms showing certain universal\npattern. The preservation of von-Neumann algebra under modular flow works in a\nsubtle way for the non-local terms. We derive a differential-integral equation\nfor the finite modular flow, which can be analyzed in perturbation theory of\nsmall distance deviating from the entanglement boundary, and re-summation can\nbe performed in appropriate limits. Comparison with the general expectation of\nmodular flows in such limits are discussed.",
        "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.",
        "Materials improvements are a powerful approach to reducing loss and\ndecoherence in superconducting qubits because such improvements can be readily\ntranslated to large scale processors. Recent work improved transmon coherence\nby utilizing tantalum (Ta) as a base layer and sapphire as a substrate. The\nlosses in these devices are dominated by two-level systems (TLSs) with\ncomparable contributions from both the surface and bulk dielectrics, indicating\nthat both must be tackled to achieve major improvements in the state of the\nart. Here we show that replacing the substrate with high-resistivity silicon\n(Si) dramatically decreases the bulk substrate loss, enabling 2D transmons with\ntime-averaged quality factors (Q) exceeding 1.5 x 10^7, reaching a maximum Q of\n2.5 x 10^7, corresponding to a lifetime (T_1) of up to 1.68 ms. This low loss\nallows us to observe decoherence effects related to the Josephson junction, and\nwe use improved, low-contamination junction deposition to achieve Hahn echo\ncoherence times (T_2E) exceeding T_1. We achieve these material improvements\nwithout any modifications to the qubit architecture, allowing us to readily\nincorporate standard quantum control gates. We demonstrate single qubit gates\nwith 99.994% fidelity. The Ta-on-Si platform comprises a simple material stack\nthat can potentially be fabricated at wafer scale, and therefore can be readily\ntranslated to large-scale quantum processors.",
        "The Dark Energy Spectroscopic Instrument (DESI) data release 2 (DR2) galaxy\nand quasar clustering data represents a significant expansion of data from DR1,\nproviding improved statistical precision in BAO constraints across multiple\ntracers, including bright galaxies (BGS), luminous red galaxies (LRGs),\nemission line galaxies (ELGs), and quasars (QSOs). In this paper, we validate\nthe BAO analysis of DR2. We present the results of robustness tests on the\nblinded DR2 data and, after unblinding, consistency checks on the unblinded DR2\ndata. All results are compared to those obtained from a suite of mock catalogs\nthat replicate the selection and clustering properties of the DR2 sample. We\nconfirm the consistency of DR2 BAO measurements with DR1 while achieving a\nreduction in statistical uncertainties due to the increased survey volume and\ncompleteness. We assess the impact of analysis choices, including different\ndata vectors (correlation function vs. power spectrum), modeling approaches and\nsystematics treatments, and an assumption of the Gaussian likelihood, finding\nthat our BAO constraints are stable across these variations and assumptions\nwith a few minor refinements to the baseline setup of the DR1 BAO analysis. We\nsummarize a series of pre-unblinding tests that confirmed the readiness of our\nanalysis pipeline, the final systematic errors, and the DR2 BAO analysis\nbaseline. The successful completion of these tests led to the unblinding of the\nDR2 BAO measurements, ultimately leading to the DESI DR2 cosmological analysis,\nwith their implications for the expansion history of the Universe and the\nnature of dark energy presented in the DESI key paper.",
        "Designing re-entry vehicles requires accurate predictions of hypersonic flow\naround their geometry. Rapid prediction of such flows can revolutionize vehicle\ndesign, particularly for morphing geometries. We evaluate advanced neural\noperator models such as Deep Operator Networks (DeepONet),\nparameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet,\nwith the objective of addressing the challenge of learning geometry-dependent\nhypersonic flow fields with limited data. Specifically, we compare the\nperformance of these models for two grid types: uniform Cartesian and irregular\ngrids. To train these models, we use 36 unique elliptic geometries for\ngenerating high-fidelity simulations with a high-order entropy-stable DGSEM\nsolver, emphasizing the challenge of working with a scarce dataset. We evaluate\nand compare the four operator-based models for their efficacy in predicting\nhypersonic flow field around the elliptic body. Moreover, we develop a novel\nframework, called Fusion DeepONet, which leverages neural field concepts and\ngeneralizes effectively across varying geometries. Despite the scarcity of\ntraining data, Fusion DeepONet achieves performance comparable to\nparameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNet\nand vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requires\nsignificantly fewer trainable parameters as compared to U-Net, MeshGraphNet,\nand FNO, making it computationally efficient. We also analyze the basis\nfunctions of the Fusion DeepONet model using Singular Value Decomposition. This\nanalysis reveals that Fusion DeepONet generalizes effectively to unseen\nsolutions and adapts to varying geometries and grid points, demonstrating its\nrobustness in scenarios with limited training data.",
        "Graph Neural Networks (GNNs) have achieved remarkable success across diverse\ntasks on graph-structured data, primarily through the use of learned weights in\nmessage passing layers. In this paper, we demonstrate that random weights can\nbe surprisingly effective, achieving performance comparable to end-to-end\ntraining counterparts, across various tasks and datasets. Specifically, we show\nthat by replacing learnable weights with random weights, GNNs can retain strong\npredictive power, while significantly reducing training time by up to 6$\\times$\nand memory usage by up to 3$\\times$. Moreover, the random weights combined with\nour construction yield random graph propagation operators, which we show to\nreduce the problem of feature rank collapse in GNNs. These understandings and\nempirical results highlight random weights as a lightweight and efficient\nalternative, offering a compelling perspective on the design and training of\nGNN architectures.",
        "We study a 3+1 active-sterile neutrino mixings model using an $A_4$ triplet\nright-handed neutrino $\\nu_R$ and a singlet eV-scale sterile neutrino under\n$A_4\\times Z_3 \\times Z_2$ discrete symmetry. Four scalar flavons are\nconsidered to reproduce neutrino oscillation parameters within the experimental\n3$\\sigma$ range. The model also studies the effective mass parameter in\nneutrinoless double beta decay experiments. Deviation from $\\mu-\\tau$ symmetry\nin the active neutrino mass matrix is generated through an antisymmetric\ninteraction of $\\nu_R$. This model successfully explains active-sterile\nneutrino mixings consistent with the cosmological upper bound on the sum of\nactive neutrino mass $\\sum m_i < 0.113$ eV (0.145 eV) in NH(IH).",
        "Three directions for the AI avant-garde are sketched against the background\nof time. Posthumanism changes what we are, and belongs to the radical future.\nTranshumanism changes how we are, and corresponds with the radical past.\nGenhumanism changes who we are, and exists in the radical present. While\ndeveloping the concepts, this essay intersects in two ways with theoretical\ndebates about humanism in the face of technological advance. First, it\ndescribes how temporal divisions may cleanly differentiate post- and\ntranshumanism. Second, the essay introduces generative humanism, which\ncontributes to discussions about AI and society by delineating a novel\nhumanistic response to contemporary technology. Finally, grounds are provided\nfor a practical project, one where philosophers work with AI engineers in the\narea of genhumanism. Contemporary AI research into serendipity in\nrecommendation engines provides natural support for the shared research.",
        "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided.",
        "The Selberg integral, an $n$-dimensional generalization of the Euler beta\nintegral, plays a central role in random matrix theory, Calogero--Sutherland\nquantum many body systems, Knizhnik--Zamolodchikov equations, and multivariable\northogonal polynomial theory. The Selberg integral is known to be equivalent to\nthe Morris constant term identity. In 1998, Baker and Forrester conjectured a\n$(p+1)$-component generalization of the $q$-Morris identity. It in turn yields\na generalization of the Selberg integral. The $p=1$ case of Baker and\nForrester's conjecture was proved by K\\'{a}rolyi, Nagy, Petrov and Volkov in\n2015. In this paper, we give a proof of the $(p+1)$-component\n$q$-Baker--Forrester conjecture, thereby settling this 26-year-old conjecture.",
        "Characterizing eccentricity in gravitational waveforms in a consistent manner\nis crucial to facilitate parameter estimation, astrophysical population\nstudies, as well as searches for these rare systems. We present a framework to\ncharacterize eccentricity directly from gravitational waveforms for\nnon-precessing eccentric binary black hole (BBH) mergers using common\nmodulations that eccentricity induces in all spherical harmonic modes of the\nsignals. Our framework is in the spirit of existing methods that use frequency\nmodulations in the waveforms, but we refine the approach by connecting it to\nstate-of-the-art post-Newtonian calculations of the time evolution of the\neccentricity. Using 39 numerical relativity (NR) simulations from the SXS and\nRIT catalogs, as well as waveforms obtained from the post-Newtonian\napproximation and effective-one-body (EOB) formalism, we show that our\nframework provides eccentricity estimates that connect smoothly into the\nrelativistic regime (even up to $\\sim 2M$ before merger). We also find that it\nis necessary to carry existing post-Newtonian calculations to an extra $0.5$PN\norder to adequately characterize existing NR simulations, and provide fits to\nthe extra coefficient for existing simulations. We make the framework publicly\navailable through the Python-based \\texttt{gwModels} package.",
        "In this paper, we study classes of structures and individual structures for\nwhich programs implementing functions defined everywhere are equivalent to\nfinite tree-programs. The programs under consideration may have cycles and at\nmost countably many nodes. We start with programs in which arbitrary terms of a\ngiven signature may be used in function nodes and arbitrary formulas of this\nsignature may be used in predicate nodes. We then extend our results to\nprograms that are close in nature to computation trees: if such a program is a\nfinite tree-program, then it is an ordinary computation tree.",
        "We consider a class of self-adjoint Sturm-Liouville problems with rational\nfunctions of the spectral parameter in the boundary conditions. The uniform\nstability for direct and inverse spectral problems is proved for the first time\nfor Sturm-Liouville operator pencils with boundary conditions depending on the\neigenparameter. Furthermore, we obtain stability estimates for finite data\napproximations, which are important from the practical viewpoint. Our method is\nbased on Darboux-type transforms and proving of their Lipschitz continuity.",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients that serve\nas unique probes of extragalactic matter. We report on the discovery and\nlocalization of two FRBs piercing the Andromeda Galaxy (M31) by the realfast\nfast transient detection system at the Very Large Array. Their unique\nsightlines allow constraints on M31's electron density distribution. We\nlocalized FRB 20230903A to its host galaxy at a redshift $z=0.09$ and FRB\n20230506C to a host galaxy at a redshift $z=0.39$. After accounting for the\ndispersion contribution from the Milky Way, the host galaxy and the\nintergalactic medium along the line of sight of the FRBs, we estimate that M31\nalone will likely contribute between 21-217 $\\mathrm{pc~cm^{-3}}$ along FRB\n20230903A and between 43-338 $\\mathrm{pc~cm^{-3}}$ along FRB 20230506C with a\n90% confidence. We also modeled the M31 disk's contribution to the DM to\ndetermine the halo contribution. We find that the halo of M31 will contribute\nbetween 9-145 $\\mathrm{pc~cm^{-3}}$ along FRB 20230903A and between 28-286\n$\\mathrm{pc~cm^{-3}}$ along FRB 20230506C with 90% confidence. The measured\nvalues of $\\rm DM_{M31,halo}$ are consistent with the predictions from the\nmodified Navarro-Frenk-White profile of M31's halo for a given impact\nparameter. The ions of the cool halo alone cannot account for the calculated\n$\\rm DM_{M31,halo}$ and therefore this measurement presents indirect evidence\nof the hot halo of M31. We also suggest a possible intersection of the line of\nsight of FRB 20230506C with a hot baryon bridge between M31 and the Milky Way",
        "We characterize decomposable principally polarized abelian varieties of the\nform $E\\times B$, with $E$ an elliptic curve, in two different ways. The first\none is by the non-surjectivity of a certain multiplication map of global\nsections, i.e. by the non-vanishing of a certain $0$-th Koszul cohomology\ngroup. The second one is by the non-surjectivity of a certain second order\ngaussian map. Both results are analogous to well known characterizations of\nhyperelliptic curves among all smooth curves of given genus. We also show that,\naccording to previous work of the first author, the second characterization can\nbe seen as an effective version of a theorem of Nakamaye characterizing the\nabove decomposable abelian varieties as those of minimal Seshadri constant.",
        "We prove the existence of compact surfaces with prescribed constant mean\ncurvature in asymptotically flat and asymptotically hyperbolic manifolds. More\nprecisely, let $(M^3,g)$ be an asymptotically flat manifold with scalar\ncurvature $R\\ge 0$. Then, for each constant $c>0$, there exists a compact,\nalmost-embedded, free boundary constant mean curvature surface $\\Sigma \\subset\nM$ with mean curvature $c$. Likewise, let $(M^3,g)$ be an asymptotically\nhyperbolic manifold with scalar curvature $R\\ge -6$. Then, for each constant\n$c>2$, there exists a compact, almost-embedded, free boundary constant mean\ncurvature surface $\\Sigma \\subset M$ with mean curvature $c$. The proof\ncombines min-max theory with the following fact about inverse mean curvature\nflow which is of independent interest: for any $T$ the inverse mean curvature\nflow emerging out of a point $p$ far enough out in an asymptotically flat (or\nasymptotically hyperbolic) end will remain smooth for all times $t\\in\n(-\\infty,T]$.",
        "The Merino-Welsh conjecture states that for a graph $G$ without loops and\nbridges we have $$\\max(T_G(2,0),T_G(0,2))\\geq T_G(1,1).$$ Later Jackson proved\nthat for any matroid $M$ without loop and coloop we have $$T_M(3,0)T_M(0,3)\\geq\nT_M(1,1)^2.$$ The value $3$ in this statement was improved to $2.9242$ by Beke,\nCs\\'aji, Csikv\\'ari and Pituk. In this paper, we further improve on this result\nby showing that $$T_M(2.355,0)T_M(0,2.355)\\geq T_M(1,1)^2.$$ We also prove that\nthe Merino--Welsh conjecture is true for matroids $M$, where all circuits of\n$M$ and its dual $M^*$ have length between $\\ell$ and $(\\ell-2)^4$ for some\n$\\ell\\geq 6$.",
        "Photonics-based in-memory computing systems have demonstrated a significant\nspeedup over traditional transistor-based systems because of their ultra-fast\noperating frequencies and high data bandwidths. Photonic static random access\nmemory (pSRAM) is a crucial component for achieving the objective of ultra-fast\nphotonic in-memory computing systems. In this work, we model and evaluate the\nperformance of a novel photonic SRAM array architecture in development.\nAdditionally, we examine hyperspectral operation through wavelength division\nmultiplexing (WDM) to enhance the throughput of the pSRAM array. We map\nMatricized Tensor Times Khatri-Rao Product (MTTKRP), a computational kernel\ncommonly used in tensor decomposition, to the proposed pSRAM array\narchitecture. We also develop a predictive performance model to estimate the\nsustained performance of different configurations of the pSRAM array. Using the\npredictive performance model, we demonstrate that the pSRAM array achieves 17\nPetaOps while performing MTTKRP in a practical hardware configuration.",
        "High-quality open-source text-to-image models have lowered the threshold for\nobtaining photorealistic images significantly, but also face potential risks of\nmisuse. Specifically, suspects may use synthetic data generated by these\ngenerative models to train models for specific tasks without permission, when\nlacking real data resources especially. Protecting these generative models is\ncrucial for the well-being of their owners. In this work, we propose the first\nmethod to this important yet unresolved issue, called Training data Provenance\nVerification (TrainProVe). The rationale behind TrainProVe is grounded in the\nprinciple of generalization error bound, which suggests that, for two models\nwith the same task, if the distance between their training data distributions\nis smaller, their generalization ability will be closer. We validate the\nefficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4,\nlatent consistency model, PixArt-$\\alpha$, and Stable Cascade). The results\nshow that TrainProVe achieves a verification accuracy of over 99\\% in\ndetermining the provenance of suspicious model training data, surpassing all\nprevious methods. Code is available at https:\/\/github.com\/xieyc99\/TrainProVe.",
        "We propose a dephasing-tolerant protocol for quantum sensing of transverse\nmagnetic fields which exploits spin qudit sensors with embedded fault-tolerant\n(FT) quantum error correction. By exploiting longitudinal drives, the\ntransverse field induces logical Rabi oscillations between encoded states,\nwhose frequency is linear in the transverse field to be probed. Numerical\nsimulations show that the present FT protocol enables the detection of very\nsmall fields, orders of magnitudes below the limit imposed by the coherence\ntime.",
        "We for the first time construct a three-family ${\\cal N}=1$ supersymmetric\nPati-Salam model from rigid intersecting D6-branes on a factorizable\n$\\mathbb{T}^6\/(\\mathbb{Z}_2\\times \\mathbb{Z}_2')$ orientifold with discrete\ntorsion. We can break the Pati-Salam gauge symmetry down to the Standard Model\n(SM) gauge symmetry via the supersymmetry preserving Higgs mechanism, generate\nthe SM fermion masses and mixings, and break the supersymmetry via gaugino\ncondensations in the hidden sector.",
        "Today, the rapid growth of applications reliant on datacenters calls for new\nadvancements to meet the increasing traffic and computational demands. Traffic\ntraces from datacenters are essential for further development and optimization\nof future datacenters. However, traces are rarely released to the public.\nResearchers often use simplified mathematical models that lack the depth needed\nto recreate intricate traffic patterns and, thus, miss optimization\nopportunities found in realistic traffic. In this preliminary work, we\nintroduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on\nthe generative pre-trained transformer (GPT) architecture used by many\nstate-of-the-art large language models. We train our model on a small set of\navailable traffic traces from different domains and offer a simple methodology\nto evaluate the fidelity of the generated traces to their original\ncounterparts. We show that DTG-GPT can synthesize novel traces that mimic the\nspatiotemporal patterns found in real traffic traces. We further demonstrate\nthat DTG-GPT can generate traces for networks of different scales while\nmaintaining fidelity. Our findings indicate the potential that, in the future,\nsimilar models to DTG-GPT will allow datacenter operators to release traffic\ninformation to the research community via trained GPT models.",
        "We investigate the $O(N)$--symmetric $\\phi^6$ theory in three spacetime\ndimensions using dimensional regularisation and minimal subtraction. The\npredictions of other methods are scrutinised in a large-$N$ expansion. We show\nhow the tricritical line of fixed point emerges in a strict $N\\to\\infty$ limit\nbut argue that it is not a physical manifestation. For the first time in this\nexplicit manner, we compute the effective potential at next-to-leading order in\nthe $1\/N$-expansion and discuss its stability. The Bardeen-Moshe-Bander\nphenomenon is also analysed at next-to-leading order, and we demonstrate that\nit disappears without breaking the scale invariance spontaneously. Our findings\nindicate that the UV fixed point found by Pisarski persists at large $N$."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Quant GANs: deep generation of financial time series",
    "start_abstract":"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
      ],
      "abstract":[
        "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
      ],
      "categories":[
        "q-fin.GN"
      ]
    },
    "list":{
      "title":[
        "Centre-of-momentum Variables in $\\nu_\\mu$CC1p1$\\pi$",
        "Is fixed-node diffusion quantum Monte Carlo reproducible?",
        "Optical signatures of noncentrosymmetric structural distortion in\n  altermagnetic MnTe",
        "Fast-response low power atomic oven for integration into an ion\n  microchip",
        "Limits on WIMP dark matter with NaI(Tl) crystals in three years of\n  COSINE-100 data",
        "Building a Software Stack for Quantum-HPC Integration",
        "Eightfold Degenerate Dirac Nodal Line in Collinear Antiferromagnet\n  Mn$_5$Si$_3$",
        "A Full AGN Feedback Prescription for Numerical Models: Negative,\n  Positive and Hot Gas-Ejection Mode",
        "All Order Classical Electromagnetic Soft Theorems",
        "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation",
        "Channel Estimation for Pinching-Antenna Systems (PASS)",
        "Rainbow Tur\\'an numbers for short brooms",
        "Influence of departures from LTE on determinations of the sulfur\n  abundances in A-K type stars",
        "Heat transport model for the transition between scaling regimes in\n  quasistatic and full magnetoconvection",
        "Structure and Skewness of the Effective Inspiral Spin Distribution of\n  Binary Black Hole Mergers",
        "Critical String Theory in a $D=4$ Robertson-Walker Background and the\n  Large Scale Structure of Spacetime",
        "Feldman-Cousins' ML Cousin: Sterile Neutrino Global Fits using\n  Simulation-Based Inference",
        "Complements of the point schemes of noncommutative projective lines",
        "Heat kernel estimates for Schr\\\"odinger operators with supercritical\n  killing potentials",
        "Depth of extensions of valuations",
        "Data-Driven Non-Parametric Model Learning and Adaptive Control of MDPs\n  with Borel spaces: Identifiability and Near Optimal Design",
        "Fast M{\\o}lmer-S{\\o}rensen gates in trapped-ion quantum processors with\n  compensated carrier transition",
        "The basic locus of ramified unitary Rapoport-Zink space at maximal\n  vertex level",
        "A fourth-order cut-cell method for solving the two-dimensional\n  advection-diffusion equation with moving boundaries",
        "Spin glass behavior in amorphous CrSiTe3 alloy",
        "Transport in integrable and perturbed easy-axis Heisenberg chain:\n  Thouless approach",
        "Constraining the Nuclear Equation of State of neutron star via\n  high-frequency quasi-periodic oscillation in short gamma-ray bursts",
        "Neutral particle analyzer for plasma diagnostics on tokamak ST40",
        "Aspects of the dilute Glasma"
      ],
      "abstract":[
        "This study introduces a novel set of variables, namely the centre-of-momentum\nvariables, $\\theta_{\\textrm{COM}}$ and $E_{\\textrm{COM}}$, designed to isolate\nfinal-state interactions (FSI) from other aspects of neutrino-nucleus\ninteractions. Through detailed simulation studies, this work demonstrates the\nability of these variables to distinguish FSI contributions with minimal\ndependence on the nuclear initial state and, practically, on the neutrino flux,\nhighlighting their potential for advancing FSI modelling. With high-purity\nneutrino-hydrogen interaction selections, $\\theta_{\\textrm{COM}}$ offers the\nfirst opportunity for a direct cross-comparison among different neutrino\ncross-section experiments.",
        "Fixed-node diffusion quantum Monte Carlo (FN-DMC) is a widely-trusted\nmany-body method for solving the Schr\\\"{o}dinger equation, known for its\nreliable predictions of material and molecular properties. Furthermore, its\nexcellent scalability with system complexity and near-perfect utilization of\ncomputational power makes FN-DMC ideally positioned to leverage new advances in\ncomputing to address increasingly complex scientific problems. Even though the\nmethod is widely used as a computational gold standard, reproducibility across\nthe numerous FN-DMC code implementations has yet to be demonstrated. This\ndifficulty stems from the diverse array of DMC algorithms and trial wave\nfunctions, compounded by the method's inherent stochastic nature. This study\nrepresents a community-wide effort to address the titular question, affirming\nthat: Yes, FN-DMC is reproducible (when handled with care). Using the\nwater-methane dimer as the canonical test case, we compare results from eleven\ndifferent FN-DMC codes and show that the approximations to treat the\nnon-locality of pseudopotentials are the primary source of the discrepancies\nbetween them. In particular, we demonstrate that, for the same choice of\ndeterminantal component in the trial wave function, reliable and reproducible\npredictions can be achieved by employing the T-move (TM), the determinant\nlocality approximation (DLA), or the determinant T-move (DTM) schemes, while\nthe older locality approximation (LA) leads to considerable variability in\nresults. This work lays the foundation to establish accurate and reproducible\nFN-DMC estimates for all future studies across applications in materials\nscience, physics, chemistry, and biology.",
        "The hexagonal MnTe is a prime material candidate for altermagnets, an\nemerging class of magnetic compounds characterized by the nontrivial interplay\nof antiparallel spin arrangements with their underlying crystal structures.\nRecognizing precise knowledge of crystal symmetry as the cornerstone of the\nspin-group classification scheme, we report here a native\ninversion-symmetry-breaking structural distortion in this compound that has\npreviously been overlooked. Through optical polarimetry experiments and\nfirst-principle calculations, we show that MnTe belongs to the\nnoncentrosymmetric $D_{3h}$ group, effectively resolving key inconsistencies in\nthe earlier interpretations of Raman spectroscopy data. Our finding impacts the\nsymmetry analysis of MnTe within the altermagnetic class and sheds light on the\nmechanism of its magneto-controllable N\\'eel order.",
        "We present a novel microfabricated neutral atom source for quantum\ntechnologies that can be easily integrated onto microchip devices using\nwell-established MEMS fabrication techniques, and contrast this to conventional\noff-chip ion loading mechanisms. The heating filament of the device is shown to\nbe as small as 90$\\times$90 $\\mu$m$^2$. Testing of the $^{171}$Yb fluorescence\nresponse is found to be in the low tens of milliseconds, two orders of\nmagnitude faster compared to previous literature at a power of milliwatts\nmaking it desirable for low-power device packages. We demonstrate how the\nevaporation material can be capped in vacuum to work with materials such as Ba\nthat oxidise easily in air, which can avoid the need for ablation lasers in the\nloading process. We calculate oven lifetimes to be over 10 years of continuous\nuse for commonly used ion species in quantum technology.",
        "We report limits on WIMP dark matter derived from three years of data\ncollected by the COSINE-100 experiment with NaI(Tl) crystals, achieving an\nimproved energy threshold of 0.7 keV. This lowered threshold enhances\nsensitivity in the sub-GeV mass range, extending the reach for direct detection\nof low-mass dark matter. Although no excess of WIMP-like events was observed,\nthe increased sensitivity enabled a model-independent comparison between the\nexpected WIMP signal rate-based on mass limits from our data-and DAMA's\nreported modulation amplitude. Our findings strongly disfavor the DAMA signal\nas originating from WIMP interactions, fully excluding DAMA\/LIBRA 3$\\sigma$\nallowed regions and providing enhanced WIMP mass limits by an order of\nmagnitude in the spin-independent model compared to previous results. In the\nspin-dependent model, cross-section upper limits were obtained in the mass\nrange [0.1-5.0] GeV\/c$^2$, with additional sensitivity to sub-GeV WIMPs through\nthe inclusion of the Migdal effect. These results represent substantial\nprogress in low-mass dark matter exploration and reinforce constraints on the\nlongstanding DAMA claim.",
        "This paper presents a comprehensive software stack architecture for\nintegrating quantum computing (QC) capabilities with High-Performance Computing\n(HPC) environments. While quantum computers show promise as specialized\naccelerators for scientific computing, their effective integration with\nclassical HPC systems presents significant technical challenges. We propose a\nhardware-agnostic software framework that supports both current noisy\nintermediate-scale quantum devices and future fault-tolerant quantum computers,\nwhile maintaining compatibility with existing HPC workflows. The architecture\nincludes a quantum gateway interface, standardized APIs for resource\nmanagement, and robust scheduling mechanisms to handle both simultaneous and\ninterleaved quantum-classical workloads. Key innovations include: (1) a unified\nresource management system that efficiently coordinates quantum and classical\nresources, (2) a flexible quantum programming interface that abstracts\nhardware-specific details, (3) A Quantum Platform Manager API that simplifies\nthe integration of various quantum hardware systems, and (4) a comprehensive\ntool chain for quantum circuit optimization and execution. We demonstrate our\narchitecture through implementation of quantum-classical algorithms, including\nthe variational quantum linear solver, showcasing the framework's ability to\nhandle complex hybrid workflows while maximizing resource utilization. This\nwork provides a foundational blueprint for integrating QC capabilities into\nexisting HPC infrastructures, addressing critical challenges in resource\nmanagement, job scheduling, and efficient data movement between classical and\nquantum resources.",
        "We study the electronic, magnetic, and spin transport properties of the\northorhombic Mn$_{5}$Si$_{3}$ compound in the $AF2$ phase using symmetry\nanalysis and ab-initio calculations. Our ground state energy calculations align\nwith experimental observations, demonstrating that the collinear\nantiferromagnetic (AFM) order, with N\\'{e}el vector in the [010] direction, is\nthe most stable magnetic configuration both with and without spin-orbit\ncoupling (SOC) in a bulk lattice geometry. We identified an unconventional\neight-fold degenerate Dirac nodal line (DNL) close to the Fermi level,\ncharacterized by negligible SOC. This DNL is robustly protected by a unique\ncombination of a pure-spin symmetry and a lattice symmetry together with\nmagnetic space group symmetries. Upon introducing SOC, this degeneracy is\nreduced to two four-fold DNLs, being protected by the combination of\ntime-reversal, partial translation and nonsymmorphic symmetries within the\nmagnetic space group. We predict also a large intrinsic spin Hall conductivity\n(SHC) which correlates with the presence of SOC-induced splitting of these\neight-fold degenerate DNLs near the Fermi level. These intriguing\ncharacteristics position collinear antiferromagnet Mn$_{5}$Si$_{3}$ as a\ncompelling candidate for spintronic applications, particularly in the\ngeneration and detection of spin currents, while remaining compatible with\nmodern silicon technology.",
        "We build upon the state-of-the-art semi-analytic model \\texttt{FEGA24}\n(Formation and Evolution of GAlaxies, \\citealt{contini2024d}), which integrates\nthe latest prescriptions relevant to galaxy formation and evolution, alongside\na comprehensive AGN feedback model. This model incorporates three modes of\nfeedback: negative (preventing excessive cooling), positive (enhancing star\nformation), and hot gas ejection (expelling gas beyond the virial radius of\nhalos). These modes operate in a coordinated manner: the negative mode\nregulates the cooling process, the positive mode promotes bursts of star\nformation, and the hot gas ejection mode expels gas beyond the virial radius\nwhen the AGN is sufficiently powerful. Our updated semi-analytic model,\n\\texttt{FEGA25}, retains the qualitative and quantitative consistency of the\nanalyses presented in \\cite{contini2024d}, while delivering more robust\nresults. Notably, \\texttt{FEGA25} provides a more detailed characterization of\nthe fraction of red galaxies as a function of stellar mass, predicts a main\nsequence of star-forming galaxies more consistent with observations, and\nestimates the fraction of hot gas in halos closer to observed values. These\nfindings underscore the importance of a physical mechanism capable of ejecting\nhot gas beyond the virialized region of dark matter halos without significantly\naltering the stellar and cold gas components. Such a mechanism is crucial to\nensure the proper functioning of other processes, such as cooling and star\nformation. Since supernova feedback is already modeled at its maximum\nefficiency, AGN feedback emerges as the natural candidate for this role.",
        "If a set of charged objects collide in space and the fragments disperse, then\nthis process will emit electromagnetic waves. Classical soft photon theorem\ndetermines the constant term and the leading power law fall-off of the\nwave-form at late and early times in terms of only the momenta and charges of\nthe incoming and outgoing objects. In this paper we determine an infinite set\nof subleading terms in the late and early time expansion of the wave-form,\nwhich also depend only on the momenta and charges of the incoming and outgoing\nparticles. For two-particle scattering, we derive a resummed low-frequency\nelectromagnetic wave-form, as well as the resummed wave-form at early and late\ntimes. In this analysis we ignore the effect of long range gravitational\ninteraction, but our result is unaffected by any other short range interactions\namong the objects.",
        "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and\nparameter estimation.",
        "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
        "A graph $G$ is rainbow-$F$-free if it admits a proper edge-coloring without a\nrainbow copy of $F$. The rainbow Tur\\'an number of $F$, denoted\n$\\mathrm{ex^*}(n,F)$, is the maximum number of edges in a rainbow-$F$-free\ngraph on $n$ vertices. We determine bounds on the rainbow Tur\\'an numbers of\nstars with a single edge subdivided twice; we call such a tree with $t$ total\nedges a $t$-edge \\textit{broom} with length-$3$ handle, denoted by $B_{t,3}$.\nWe improve the best known upper bounds on $\\mathrm{ex^*}(n,B_{t,3})$ in all\ncases where $t \\neq 2^s - 2$. Moreover, in the case where $t$ is odd and in a\nfew cases when $t \\equiv 0 \\mod 4$, we provide constructions asymptotically\nachieving these upper bounds. Our results also demonstrate a dependence of\n$\\mathrm{ex^*}(n,B_{t,3})$ on divisibility properties of $t$.",
        "The influence of departures from local thermodynamic equilibrium (LTE) on\nneutral sulfur lines is considered. A grid of corrections is proposed to take\ninto account the influence of departures from LTE for neutral sulfur lines in\nthe visible and infrared spectral regions, including the H-band. The grid is\ncalculated using the atomic model of sulfur incorporating the most up-to-date\ncollision rates with electrons and hydrogen. The inclusion of levels and\ntransitions of ionized sulfur in the atomic model made it possible to expand\nthe range of effective temperatures of stellar photospheres in the grid up to\n10000 K. The atomic model was tested in determining the sulfur abundance of 13\nstars and showed its adequacy in a wide range of fundamental stellar\nparameters. In the spectra of all test stars, the sulfur lines are fitted with\nsimilar abundances of the element, regardless of the degree of influence of the\neffects of deviation from LTE on a particular spectral line. For lines of\nseveral multiplets, the wavelengths and oscillator strengths were refined. A\nlist of S I lines recommended for determining sulfur abundance has been\ncreated.",
        "In magnetoconvection, the flow is governed by the interplay between\ngravitational buoyancy and the Lorentz force, with one of these forces\ndominating in different regimes. In this paper, we develop a model with a\nsingle adjustable parameter that accurately captures the smooth transition from\na buoyancy-dominated regime to one dominated by the Lorentz force. A\nperturbative extension of the model accounts for distinct transition features\nthat occur at high Prandtl numbers. We validate the model for magnetoconvection\nin both the quasistatic regime and at finite magnetic Reynolds numbers using\ndata from direct numerical simulations and existing experimental data sets. The\nmodel contains a natural extension to rotating convection and offers a\npotential generalisation to rotating magnetoconvection.",
        "The detection of gravitational waves has brought to light a population of\nbinary black holes that merge within a Hubble time. Multiple formation channels\ncan contribute to this population, making it difficult to definitively\nassociate particular population features with underlying stellar physics. Black\nhole spins are considered an important discriminator between various channels,\nbut they are less well-measured than masses, making conclusive astrophysical\nstatements using spins difficult thus far. In this paper, we consider the\ndistribution of the effective inspiral spin $\\chi_{\\rm eff}$ -- a quantity much\nbetter measured than individual component spins. We show that non-Gaussian\nfeatures like skewness, asymmetry about zero, and multimodality can naturally\narise in the $\\chi_{\\rm eff}$ distribution when multiple channels contribute to\nthe population. Searching for such features, we find signs of skewness and\nasymmetry already in the current catalogs, but no statistically significant\nsigns of bimodality. These features provide robust evidence for the presence of\na subpopulation with spins preferentially aligned to the binary's orbital\nangular momentum; and we conservatively estimate the fraction of this\nsubpopulation to be at least $12 \\% - 17\\%$ (at $90\\%$ credibility). Our models\ndo not find an excess of non-spinning systems and instead find that at least\n$\\sim 20 \\%$ of the binaries have some degree of negative $\\chi_{\\rm eff}$. The\ndata also suggest that, if preferentially aligned mergers form a significant\nfraction of the population, they must have small spins.",
        "We show that $4$-dimensional Robertson-Walker spacetimes can be constructed\nfor which all of the beta functions vanish to leading order, yielding\nconsistent string theory without extra dimensions. We find that there is a\nunique static solution, which we refer to as an anti-Einstein static universe.\nThe associated stress energy tensor can be interpreted as a perfect fluid with\na negative energy density. Interestingly, a fluid with a negative energy\ndensity was proposed in [2] as an ad hoc hypothesis to serve as a possible\nexplanation for dark energy and dark matter. Here, such a fluid spontaneously\nappears by trying to fit string theory into only $4$ dimensions. We can look at\nperturbations away from the anti-Einstein static universe. This has to be done\nnumerically and we only do it for a few choices of initial conditions. We find\nthat these solutions are very sensitive to the initial conditions and yield a\nvariety of behaviors. We hope that this behavior is rich enough to match\ncosmological observations by appropriately choosing the initial conditions. One\nof these solutions that we found is particularly interesting. It has a hubble\nparameter which is negative in the past (meaning a contracting universe), then\na point at which the hubble parameter changes sign, so the universe starts\nexpanding which could be identified with a big bang, after which the hubble\nparameter continues growing. Throughout all of this time, the acceleration is\npositive since the Hubble parameter is increasing. Finally, we comment that our\nresult does not contradict [1] as it seems that the authors overlooked the\npossibility of a purely complex axion field which allows for the\nRobertson-Walker metrics to make a contribution $c_{RW}\\geq 4$ to the central\ncharge. Thus, we can interpret dark matter and dark energy as being parts of a\nmechanism needed to keep $4$-dimensional string theory consistent.",
        "For many small-signal particle physics analyses, Wilks' theorem, a\nsimplifying assumption that presumes log-likelihood asymptotic normality, does\nnot hold. The most common alternative approach applied in particle physics is a\nhighly computationally expensive procedure put forward by Feldman and Cousins.\nWhen many experiments are combined for a global fit to data, deviations from\nWilks' theorem are exacerbated, and Feldman-Cousins becomes computationally\nintractable. We present a novel, machine learning-based procedure that can\napproximate a full-fledged Bayesian analysis 200 times faster than the\nFeldman-Cousins method. We demonstrate the utility of this novel method by\nperforming a joint analysis of electron neutrino\/antineutrino disappearance\ndata within a single sterile neutrino oscillation framework. Although we\npresent a prototypical simulation-based inference method for a sterile neutrino\nglobal fit, we anticipate that similar procedures will be useful for global\nfits of all kinds, especially those in which Feldman-Cousins is too\ncomputationally expensive to use.",
        "Recently, Chan and Nyman constructed noncommutative projective lines via a\nnoncommutative symmetric algebra for a bimodule $V$ over a pair of fields.\nThese noncommutative projective lines of contain a canonical closed subscheme\n(the point scheme) determined by a normal family of elements in the\nnoncommutative symmetric algebra. We study the complement of this subscheme\nwhen $V$ is simple, the coordinate ring of which is obtained by inverting said\nnormal family. We show that this localised ring is a noncommutative Dedekind\ndomain of Gelfand-Kirillov dimension 1. Furthermore, the question of simplicity\nof these Dedekind domains is answered by a similar dichotomy to an analogous\nopen subscheme of the noncommutative quadrics of Artin, Tate and Van den Bergh.",
        "In this paper, we study the Schr\\\"odinger operator $\\Delta-V$, where $V$ is a\nsupercritical non-negative potential belonging to a large class of functions\ncontaining functions of the form $b|x|^{-(2+2\\beta)}$, $b, \\beta>0$. We obtain\ntwo-sided estimates on the heat kernel $p(t, x, y)$ of $\\Delta-V$, along with\nestimates for the corresponding Green function. Unlike the case of the\nfractional Schr\\\"odinger operator $-(-\\Delta)^{\\alpha\/2}-V$, $\\alpha\\in (0,\n2)$, with supercritical killing potential dealt with in [11], in the present\ncase, the heat kernel $p(t, x, y)$ decays to 0 exponentially as $x$ or $y$\ntends to the origin.",
        "In this paper we develop the theory of the depth of a simple algebraic\nextension of valued fields $(L\/K,v)$. This is defined as the minimal number of\naugmentations appearing in some Mac Lane-Vaqui\\'e chain for the valuation on\n$K[x]$ determined by the choice of some generator of the extension. In the\ndefectless and unibranched case, this concept leads to a generalization of a\nclassical result of Ore about the existence of $p$-regular generators for\nnumber fields. Also, we find what valuation-theoretic conditions characterize\nthe extensions having depth one.",
        "We consider an average cost stochastic control problem with standard Borel\nspaces and an unknown transition kernel. We do not assume a parametric\nstructure on the unknown kernel. We present topologies on kernels which lead to\ntheir identifiability and ensures near optimality through robustness to\nidentifiability errors. Following this, we present two data-driven\nidentifiability results; the first one being Bayesian (with finite time\nconvergence guarantees) and the second one empirical (with asymptotic\nconvergence guarantees). The identifiability results are, then, used to design\nnear-optimal adaptive control policies which alternate between periods of\nexploration, where the agent attempts to learn the true kernel, and periods of\nexploitation where the knowledge accrued throughout the exploration phase is\nused to minimize costs. We will establish that such policies are near optimal.\nMoreover, we will show that near optimality can also be achieved through\npolicies that simultaneously explore and exploit the environment. Thus, our\nprimary contributions are to present very general conditions ensuring\nidentifiability, as well as adaptive learning and control which leads to\noptimality. Key tools facilitating our analysis are a general measurability\ntheorem, robustness to model learning, and continuity of expected average cost\nin stationary policies under Young topology.",
        "Carrier transition is one of the major factors hindering the high-speed\nimplementation of M{\\o}lmer-S{\\o}rensen gates in trapped-ion quantum\nprocessors. We present an approach to design laser pulse shapes for\nM{\\o}lmer-S{\\o}rensen gate in ion chains which accounts for the effect of\ncarrier transition on qubit-phonon dynamics. We show that the fast-oscillating\ncarrier term effectively modifies the spin-dependent forces acting on ions, and\nthis can be compensated by a simple nonlinear transformation of a laser pulse.\nUsing numerical simulations for short ion chains and perturbation theory for\nlonger chains, we demonstrate that our approach allows to reach the infidelity\nbelow $10^{-4}$ while keeping the gate duration below 100 $\\mu$s.",
        "We construct the Bruhat-Tits stratification of the ramified unitary\nRapoport-Zink space, with the level being the stabilizer of a vertex lattice.\nWe develop the local model theory for Bruhat-Tits strata, proving their\nnormality and Cohen-Macaulayness, and provide precise dimension formulas.\nAdditionally, we establish an explicit isomorphism between Bruhat-Tits strata\nand Deligne-Lusztig varieties, revealing new phenomena beyond the previously\nstudied Coxeter-type cases.",
        "We propose a fourth-order cut-cell method for solving the two-dimensional\nadvection-diffusion equation with moving boundaries on a Cartesian grid. We\nemploy the ARMS technique to give an explicit and accurate representation of\nmoving boundaries, and introduce a cell-merging technique to overcome\ndiscontinuities caused by topological changes in cut cells and the small cell\nproblem. We use a polynomial interpolation technique base on poised lattice\ngeneration to achieve fourth-order spatial discretization, and use a\nfourth-order implicit-explicit Runge-Kutta scheme for time integration.\nNumerical tests are performed on various moving regions, with advection\nvelocity both matching and differing from boundary velocity, which demonstrate\nthe fourth-order accuracy of the proposed method.",
        "Owing to the intrinsically high crystallization temperatures, layered\nphase-change materials, such as CrGeTe3 and InGeTe3, are attracting attention\nfor embedded memory applications, In addition to the electrical contrast, a\nmajor change in magnetic properties is observed in CrGeTe3 upon switching from\nthe crystalline to the amorphous state. In this work, we report a combined ab\ninitio modeling and magnetic characterization study on the isostructural\nsilicon parent compound of CrGeTe3, namely, CrSiTe3. Amorphous CrSiTe3 has\nsimilar structural properties to amorphous CrGeTe3; however, it shows a smaller\nenergy difference between the ferromagnetic configuration and the random\nmagnetic configuration, indicating a high probability of spin glass formation.\nIndeed, direct-current and alternating-current magnetic measurements show that\nthe coercive force of amorphous CrSiTe3 is higher than that of amorphous\nCrGeTe3. Therefore, the pinning effect of spins is enhanced in amorphous\nCrSiTe3, leading to a more robust spin glass state with a higher freezing\ntemperature. The large magnetic contrast between the amorphous and crystalline\nphase could make CrSiTe3 a potential candidate for phase-change magnetic\nswitching applications.",
        "We study transport in the spin chains by employing the Thouless approach\nbased on the level sensitivity to the boundary conditions, $R$. Although spin\ntransport in the integrable easy-axis XXZ model is diffusive, corresponding $R$\nis much closer to ballistic chains than to chaotic diffusive systems. In the\ncase of the grand canonical ensemble this observation can be rigorously\njustified, while in the case of the canonical ensemble it can be demonstrated\nby numerical calculations. Integrability breaking perturbation (IBP) strongly\nreduces $R$ which reveals a pronounced minimum at the crossover from anomalous\ndiffusive to normal dissipative transport. This minimum coincides with the\nonset of the universality of the random matrix theory. Results for various IBP\nsuggest a discontinuous jump of the spin conductivity in the thermodynamic\nlimit, and moreover that its value universally decreases when the strength of\nIBP decreases.",
        "The determination of the equation of state (EOS) of a neutron star (NS) and\nits maximum mass is very important for understanding the formation and\nproperties of NSs under extreme conditions, but they remain open questions.\nShort-duration gamma-ray bursts (GRBs) are believed to originate from the\nmerger of binary NSs or giant flares (GFs) of soft gamma repeaters (SGRs).\nRecently, the high-frequency quasi-periodic oscillations (QPOs) have been\nclaimed to be identified from two short GRBs (GRB 931101B and GRB 910711). In\nthis paper, we propose that the observed high-frequency QPOs in these two short\nGRBs result from torsional oscillations in the GFs of SGRs associated with cold\nNSs, or from radial oscillations of hypermassive NSs as the hot remnants of\nbinary NS mergers, and then to constrain the EOS of NSs. For torsional\noscillations, the six selected EOSs (TM1, NL3, APR, SLy4, DDME2, and GM1) of\nNSs suitable for the zero-temperature condition exhibit significant overlap in\nmass ranges, suggesting that we cannot constrain the EOS of NSs. For radial\noscillations, the six selected EOSs (IUF, TM1, TMA, FSG, BHBLp, and NL3) of NSs\nsuitable for the high-temperature condition cannot be ruled out when redshift\nis considered. However, it is found that the EOS can only be constrained if the\nredshift and temperature of the remnant can be measured.",
        "The paper presents a detailed description of a neutral particle analyzer\ndesigned and produced for plasma diagnostics on the spherical tokamak ST40. The\naim of the diagnostic is to measure both the bulk ion temperature in the range\nfrom 0.5 to 10 keV and the energy distribution of fast ions with energies up to\n40 keV, which appear in the plasma through neutral beam injection. A feature of\nthe analyzer is its ability to separate hydrogen isotopes (protium and\ndeuterium) and measure the ion distribution function of the selected isotope.",
        "The Glasma is a semiclassical nonequilibrium state describing the earliest\nstage in relativistic heavy-ion collisions predicted by the Color Glass\nCondensate effective theory. It is characterized by strong color fields, which\nare sourced by color currents pertaining to hard partons in the colliding\nnuclei. We introduce the (3+1)D dilute Glasma framework, which incorporates the\nlongitudinal and transverse structure of colliding particles and describes the\nrapidity-dependence of observables like the energy-momentum tensor. This is in\nstark contrast to the canonical picture of boost-invariance, where nuclei are\ninfinitesimally thin in longitudinal direction, and the rapidity-dependence of\nobservables is lost. We discuss the derivation of the (3+1)D dilute Glasma\nfield-strength tensor, which relies on linearizing the Yang-Mills equations in\nthe dilute approximation, i.e., assuming weak sources. The dilute Glasma\nenergy-momentum tensor can efficiently be evaluated numerically on a lattice.\nEmploying a generalized 3D McLerran-Venugopalan model, we discuss numerical\nresults for the collisions of heavy ions at energies corresponding to\nexperiments at RHIC and the LHC. We discover longitudinal flow that differs\nsignificantly from Bjorken flow and argue that this is a consequence of taking\ninto account the longitudinal extension of nuclei. Furthermore, we find\nlimiting fragmentation as a universal feature of the dilute Glasma analytically\nand numerically. Finally, we study the applicability of the dilute Glasma to\nproton-proton collisions and show the necessary modifications to reproduce\nexperimental multiplicity distributions."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Quantifying Variance in Evaluation Benchmarks",
    "start_abstract":"Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The Llama 3 Herd of Models"
      ],
      "abstract":[
        "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Generalized Optimal AMG Convergence Theory for Stokes Equations Using\n  Smooth Aggregation and Vanka Relaxation Strategies",
        "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "Production, Characteristics and Biological effects of Protonated Small\n  Water Clusters",
        "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai\n  Legal Question Answering",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Accelerated Preference Elicitation with LLM-Based Proxies",
        "A car-following model with behavioural adaptation to road geometry",
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and\n  Future Directions",
        "Two simple photon gauges in inflation",
        "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion\n  Prior and Differentiable Physics",
        "Split Adaptation for Pre-trained Vision Transformers",
        "A Detailed Analysis of Close Binary OCs",
        "An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the\n  SUPERIOT Framework",
        "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
        "Nuclear modification of $B_c$ mesons in relativistic heavy-ion\n  collisions based on a linear Boltzmann transport model",
        "Resilient UAV Trajectory Planning via Few-Shot Meta-Offline\n  Reinforcement Learning",
        "Metis: A Foundation Speech Generation Model with Masked Generative\n  Pre-training",
        "Contextual Speech Extraction: Leveraging Textual History as an Implicit\n  Cue for Target Speech Extraction",
        "Observational Constraints on Dark Energy Models with $\\Lambda$ as an\n  Equilibrium Point",
        "SMT-Boosted Security Types for Low-Level MPC",
        "Sub-Power Law Decay of the Wave Packet Maximum in Disordered Anharmonic\n  Chains",
        "Secure On-Device Video OOD Detection Without Backpropagation",
        "Topological Operations Around Exceptional Points via Shortcuts to\n  Adiabaticity",
        "Bulk superconductivity in pressurized trilayer nickelate Pr4Ni3O10\n  single crystals",
        "Anomalous Reynolds stress and dynamic mechanisms in two-dimensional\n  elasto-inertial turbulence of viscoelastic channel flow",
        "Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision\n  Tree and Forest: A Comprehensive Cross-Datasets Evaluation"
      ],
      "abstract":[
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "This paper discusses our recent generalized optimal algebraic multigrid (AMG)\nconvergence theory applied to the steady-state Stokes equations discretized\nusing Taylor-Hood elements ($\\pmb{ \\mathbb{P}}_2\/\\mathbb{P}_{1}$). The\ngeneralized theory is founded on matrix-induced orthogonality of the left and\nright eigenvectors of a generalized eigenvalue problem involving the system\nmatrix and relaxation operator. This framework establishes a rigorous lower\nbound on the spectral radius of the two-grid error-propagation operator,\nenabling precise predictions of the convergence rate for symmetric indefinite\nproblems, such as those arising from saddle-point systems. We apply this theory\nto the recently developed monolithic smooth aggregation AMG (SA-AMG) solver for\nStokes, constructed using evolution-based strength of connection, standard\naggregation, and smoothed prolongation. The performance of these solvers is\nevaluated using additive and multiplicative Vanka relaxation strategies.\nAdditive Vanka relaxation constructs patches algebraically on each level,\nresulting in a nonsymmetric relaxation operator due to the partition of unity\nbeing applied on one side of the block-diagonal matrix. Although symmetry can\nbe restored by eliminating the partition of unity, this compromises\nconvergence. Alternatively, multiplicative Vanka relaxation updates velocity\nand pressure sequentially within each patch, propagating updates\nmultiplicatively across the domain and effectively addressing velocity-pressure\ncoupling, ensuring a symmetric relaxation. We demonstrate that the generalized\noptimal AMG theory consistently provides accurate lower bounds on the\nconvergence rate for SA-AMG applied to Stokes equations. These findings suggest\npotential avenues for further enhancement in AMG solver design for saddle-point\nsystems.",
        "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5.",
        "The production and characteristics of protonated small water clusters (PSWCs)\nwere reported in this work, where in electrospray ionization (ESI) of pure\nwater, the species obtained were singly charged molecular ions consisting of 2,\n3, 4 or 5 water molecules attached to a hydrogen ion, [(H2O)n+H]+, where n = 2,\n3, 4 or 5. We proposed a new type of PSWCs structure: 2, 3, 4, 5 water\nmolecules wrapped around a hydrogen ion which is located at the electrical and\ngeometric center, forming a very stable molecular structure. Furthermore,\nbiological tests of the PSWCs on mitochondrial function of intestinal\nepithelial cells and liver cells in mice showed the better therapeutic effect\non inflammatory bowel diseases compared to that of the biologic agent\nInfliximab.",
        "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms.",
        "Understanding the effect of road geometry on human driving behaviour is\nessential for both road safety studies and traffic microsimulation. Research on\nthis topic is still limited, mainly focusing on free-flow traffic and not\nadequately considering the influence of curvature on car-following dynamics.\nThis work attempts to investigate this issue and model the adaptation of\ncar-following behaviour to horizontal curvature. For this purpose, the maximum\ndesired speed - which mainly determines the free-flow dynamics - is expressed\nas a parsimonious function of the curvature. A spatial anticipation mechanism\nis also included in order to realistically describe the driving behaviour when\napproaching or exiting from curves. The accuracy of the augmented model is\nevaluated using the Modified Intelligent Driver Model (M-IDM) and trajectory\ndata from free-flow and car-following traffic (Naples data and Zen Traffic\nData). The results show that a significant improvement is achieved in free-flow\ndynamics. In car-following situations, improvements are mainly observed at high\nspeed and are dependent on the observed driver. Overall, the analysis\nhighlights the lack of sufficiently spatially extended trajectory data to\ncalibrate and evaluate such driving behaviours.",
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "As the potential for autonomous vehicles to be integrated on a large scale\ninto modern traffic systems continues to grow, ensuring safe navigation in\ndynamic environments is crucial for smooth integration. To guarantee safety and\nprevent collisions, autonomous vehicles must be capable of accurately\npredicting the trajectories of surrounding traffic agents. Over the past\ndecade, significant efforts from both academia and industry have been dedicated\nto designing solutions for precise trajectory forecasting. These efforts have\nproduced a diverse range of approaches, raising questions about the differences\nbetween these methods and whether trajectory prediction challenges have been\nfully addressed. This paper reviews a substantial portion of recent trajectory\nprediction methods and devises a taxonomy to classify existing solutions. A\ngeneral overview of the prediction pipeline is also provided, covering input\nand output modalities, modeling features, and prediction paradigms discussed in\nthe literature. In addition, the paper discusses active research areas within\ntrajectory prediction, addresses the posed research questions, and highlights\nthe remaining research gaps and challenges.",
        "Photon propagators for power-law inflation are constructed in two\none-parameter families of noncovariant gauges, in an arbitrary number of\nspacetime dimensions. In both gauges photon propagators take relatively simple\nforms expressed in terms of scalar propagators and their derivatives. These are\nconsiderably simpler compared to their general covariant gauge counterpart.\nThis makes feasible performing dimensionally regulated loop computations\ninvolving massless vector fields in inflation.",
        "Recent advances in large models have significantly advanced image-to-3D\nreconstruction. However, the generated models are often fused into a single\npiece, limiting their applicability in downstream tasks. This paper focuses on\n3D garment generation, a key area for applications like virtual try-on with\ndynamic garment animations, which require garments to be separable and\nsimulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs\nphysics-plausible, simulation-ready separated garments with sewing patterns and\nhumans from an in-the-wild image. Starting with the image, our approach\ncombines a pre-trained image-to-sewing pattern generation model for creating\ncoarse sewing patterns with a pre-trained multi-view diffusion model to produce\nmulti-view images. The sewing pattern is further refined using a differentiable\ngarment simulator based on the generated multi-view images. Versatile\nexperiments demonstrate that our optimization approach substantially enhances\nthe geometric alignment of the reconstructed 3D garments and humans with the\ninput image. Furthermore, by integrating a texture generation module and a\nhuman motion generation module, we produce customized physics-plausible and\nrealistic dynamic garment demonstrations. Project page:\nhttps:\/\/dress-1-to-3.github.io\/",
        "Vision Transformers (ViTs), extensively pre-trained on large-scale datasets,\nhave become essential to foundation models, allowing excellent performance on\ndiverse downstream tasks with minimal adaptation. Consequently, there is\ngrowing interest in adapting pre-trained ViTs across various fields, including\nprivacy-sensitive domains where clients are often reluctant to share their\ndata. Existing adaptation methods typically require direct data access,\nrendering them infeasible under these constraints. A straightforward solution\nmay be sending the pre-trained ViT to clients for local adaptation, which poses\nissues of model intellectual property protection and incurs heavy client\ncomputation overhead. To address these issues, we propose a novel split\nadaptation (SA) method that enables effective downstream adaptation while\nprotecting data and models. SA, inspired by split learning (SL), segments the\npre-trained ViT into a frontend and a backend, with only the frontend shared\nwith the client for data representation extraction. But unlike regular SL, SA\nreplaces frontend parameters with low-bit quantized values, preventing direct\nexposure of the model. SA allows the client to add bi-level noise to the\nfrontend and the extracted data representations, ensuring data protection.\nAccordingly, SA incorporates data-level and model-level out-of-distribution\nenhancements to mitigate noise injection's impact on adaptation performance.\nOur SA focuses on the challenging few-shot adaptation and adopts patch\nretrieval augmentation for overfitting alleviation. Extensive experiments on\nmultiple datasets validate SA's superiority over state-of-the-art methods and\ndemonstrate its defense against advanced data reconstruction attacks while\npreventing model leakage with minimal computation cost on the client side. The\nsource codes can be found at https:\/\/github.com\/conditionWang\/Split_Adaptation.",
        "In this study, we analyzed the close binary open clusters CWNU 2666 and HSC\n224, which are in close spatial proximity, using photometric and astrometric\ndata from the {\\it Gaia} DR3 catalog. Likely member stars were identified based\non a membership probability threshold ($P \\geq 0.5$), resulting in 106 and 146\nmembers for CWNU 2666 and HSC 224, respectively. The mean proper motion\ncomponents ($\\mu_{\\alpha}\\cos\\delta$, $\\mu_{\\delta}$) were determined to be\n(0.646$\\pm$0.155, -0.769$\\pm$0.124) mas yr$^{-1}$ for CWNU 2666, and\n(0.665$\\pm$0.131, -0.728$\\pm$0.107) mas yr$^{-1}$ for HSC 224. The isochrone\ndistances ($d_{\\rm iso}$) were estimated as 1885$\\pm$44 pc for CWNU 2666 and\n1866$\\pm$29 pc for HSC 224. The corresponding cluster ages ($t$) were derived\nas 160$\\pm$15 Myr and 140$\\pm$15 Myr, respectively. The astrometric and\nfundamental astrophysical parameters derived in this study demonstrate that the\ntwo open clusters are a close pair of open clusters.",
        "This paper presents a comprehensive analysis of the energy consumption\ncharacteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node\ndeveloped in the initial phase of the SUPERIOT project, focusing on key\noperating states, including Bluetooth Low Energy (BLE) communication,\nNarrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.\nExtensive measurements were conducted to establish a detailed energy profile,\nwhich serves as a benchmark for evaluating the effectiveness of subsequent\noptimizations and future node iterations. To minimize the energy consumption,\nmultiple optimizations were implemented at both the software and hardware\nlevels, achieving a reduction of over 60% in total energy usage through\nsoftware modifications alone. Further improvements were realized by optimizing\nthe E-ink display driving waveform and implementing a very low-power mode for\nnon-communication activities. Based on the measured data, three\nmeasurement-based energy consumption models were developed to characterize the\nenergy behavior of the node under: (i) normal, unoptimized operation, (ii)\nlow-power, software-optimized operation, and (iii) very low-power,\nhardware-optimized operation. These models, validated with new measurement\ndata, achieved an accuracy exceeding 97%, confirming their reliability for\npredicting energy consumption in diverse configurations.",
        "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
        "The nuclear modification factor ($R_\\mathrm{AA}$) of $B_c$ mesons in\nhigh-energy nuclear collisions provides a novel probe of heavy quark\ninteractions with the quark-gluon plasma (QGP). Based on a linear Boltzmann\ntransport model that incorporates both Yukawa and string types of interactions\nbetween heavy quarks and the QGP, we study the production and evolution of\nheavy quarks and $B_c$ mesons within the same framework. A $B_c$ bound state\ndissociates while one of its constituent heavy quarks scatters with the QGP\nwith momentum transfer greater than its binding energy. The medium-modified\ncharm and bottom quarks can recombine into $B_c$ mesons, and the\nmedium-modified bottom quarks can also fragment to $B_c$ mesons. We find that\nmost primordial $B_c$ mesons generated from the initial hard collisions\ndissociate inside the QGP. The production of $B_c$ mesons is primarily driven\nby the recombination mechanism at low transverse momentum and fragmentation at\nhigh transverse momentum. The string interaction dominates over the Yukawa\ninteraction in the nuclear modification of $B_c$ mesons. The participant number\ndependence of the $B_c$ meson $R_\\mathrm{AA}$ is determined by the complicated\ninterplay between the heavy quark yield, energy loss, and the QGP volume. We\nobtain a reasonable description of the $R_\\mathrm{AA}$ of $B_c$ mesons in Pb+Pb\ncollisions at $\\sqrt{s_\\mathrm{NN}}=5.02$ TeV, and provide predictions for\nAu+Au collisions at $\\sqrt{s_\\mathrm{NN}}=200$ GeV.",
        "Reinforcement learning (RL) has been a promising essence in future 5G-beyond\nand 6G systems. Its main advantage lies in its robust model-free\ndecision-making in complex and large-dimension wireless environments. However,\nmost existing RL frameworks rely on online interaction with the environment,\nwhich might not be feasible due to safety and cost concerns. Another problem\nwith online RL is the lack of scalability of the designed algorithm with\ndynamic or new environments. This work proposes a novel, resilient, few-shot\nmeta-offline RL algorithm combining offline RL using conservative Q-learning\n(CQL) and meta-learning using model-agnostic meta-learning (MAML). The proposed\nalgorithm can train RL models using static offline datasets without any online\ninteraction with the environments. In addition, with the aid of MAML, the\nproposed model can be scaled up to new unseen environments. We showcase the\nproposed algorithm for optimizing an unmanned aerial vehicle (UAV) 's\ntrajectory and scheduling policy to minimize the age-of-information (AoI) and\ntransmission power of limited-power devices. Numerical results show that the\nproposed few-shot meta-offline RL algorithm converges faster than baseline\nschemes, such as deep Q-networks and CQL. In addition, it is the only algorithm\nthat can achieve optimal joint AoI and transmission power using an offline\ndataset with few shots of data points and is resilient to network failures due\nto unprecedented environmental changes.",
        "We introduce Metis, a foundation model for unified speech generation. Unlike\nprevious task-specific or multi-task models, Metis follows a pre-training and\nfine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data\nusing masked generative modeling and then fine-tuned to adapt to diverse speech\ngeneration tasks. Specifically, 1) Metis utilizes two discrete speech\nrepresentations: SSL tokens derived from speech self-supervised learning (SSL)\nfeatures, and acoustic tokens directly quantized from waveforms. 2) Metis\nperforms masked generative pre-training on SSL tokens, utilizing 300K hours of\ndiverse speech data, without any additional condition. 3) Through fine-tuning\nwith task-specific conditions, Metis achieves efficient adaptation to various\nspeech generation tasks while supporting multimodal input, even when using\nlimited data and trainable parameters. Experiments demonstrate that Metis can\nserve as a foundation model for unified speech generation: Metis outperforms\nstate-of-the-art task-specific or multi-task systems across five speech\ngeneration tasks, including zero-shot text-to-speech, voice conversion, target\nspeaker extraction, speech enhancement, and lip-to-speech, even with fewer than\n20M trainable parameters or 300 times less training data. Audio samples are are\navailable at https:\/\/metis-demo.github.io\/.",
        "In this paper, we investigate a novel approach for Target Speech Extraction\n(TSE), which relies solely on textual context to extract the target speech. We\nrefer to this task as Contextual Speech Extraction (CSE). Unlike traditional\nTSE methods that rely on pre-recorded enrollment utterances, video of the\ntarget speaker's face, spatial information, or other explicit cues to identify\nthe target stream, our proposed method requires only a few turns of previous\ndialogue (or monologue) history. This approach is naturally feasible in mobile\nmessaging environments where voice recordings are typically preceded by textual\ndialogue that can be leveraged implicitly. We present three CSE models and\nanalyze their performances on three datasets. Through our experiments, we\ndemonstrate that even when the model relies purely on dialogue history, it can\nachieve over 90 % accuracy in identifying the correct target stream with only\ntwo previous dialogue turns. Furthermore, we show that by leveraging both\ntextual context and enrollment utterances as cues during training, we further\nenhance our model's flexibility and effectiveness, allowing us to use either\ncue during inference, or combine both for improved performance. Samples and\ncode available on https:\/\/miraodasilva.github.io\/cse-project-page .",
        "We investigate a dynamical reconstruction of the dark energy equation of\nstate parameter by assuming that it satisfies a law of motion described by an\nautonomous second-order differential equation, with the limit of the\ncosmological constant as an equilibrium point. We determine the asymptotic\nsolutions of this equation and use them to construct two families of parametric\ndark energy models, employing both linear and logarithmic parametrizations with\nrespect to the scale factor. We perform observational constraints by using the\nSupernova, the Cosmic Chronometers and the Baryon Acoustic Oscillations. The\nconstraint parameters are directly related with the initial value problem for\nthe law of motion and its algebraic properties. The analysis shows that most of\nthe models fit the observational data well with a preference to the models of\nthe logarithmic parametrization. Furthermore, we introduce a new class of\nmodels as generalizations of the CPL model, for which the equilibrium point is\na constant value rather than the cosmological constant. These models fit the\ndata in a similar or better way to the CPL and the $\\Lambda$CDM cosmological\nmodels.",
        "Secure Multi-Party Computation (MPC) is an important enabling technology for\ndata privacy in modern distributed applications. We develop a new type theory\nto automatically enforce correctness,confidentiality, and integrity properties\nof protocols written in the \\emph{Prelude\/Overture} language framework.\nJudgements in the type theory are predicated on SMT verifications in a theory\nof finite fields, which supports precise and efficient analysis. Our approach\nis automated, compositional, scalable, and generalizes to arbitrary prime\nfields for data and key sizes.",
        "We show that the peak of an initially localized wave packet in\none-dimensional nonlinear disordered chains decays more slowly than any power\nlaw of time. The systems under investigation are Klein-Gordon and nonlinear\ndisordered Schr\\\"odinger-type chains, characterized by a harmonic onsite\ndisordered potential and quartic nearest-neighbor coupling. Our results apply\nin the long-time limit, hold almost surely, and are valid for arbitrary finite\nenergy values.",
        "Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https:\/\/github.com\/Dystopians\/SecDOOD.",
        "The existence of singularities in the spectrum of non-Hermitian Hamiltonians\nleads to a non-trivial spectral topology which can be exploited to generate\ntopological operations. However, their implementation has remained elusive due\nto the difficulty of generating a true adiabatic evolution. Here, we develop\nfast, robust control protocols that generate a desired topological operation.\nOur strategy relies on shortcuts to adiabaticity, but is not a trivial\nextension. The presence of spectral singularities renders the strategy\ndeveloped for Hermitian Hamiltonians impractical as it will lead to faulty\ncontrol protocols. Moreover, due to the dynamics sensitivity to parameter\nuncertainties, not all shortcuts to adiabaticity can be used in a realistic\nsetting. We illustrate our method in the context of a two-mode non-Hermitian\nHamiltonian and discuss why in general celebrated shortcuts to adiabaticiy like\ntransitionless driving and superadiabatic transitionless driving are not\nappropriate control protocols for non-Hermitian systems.",
        "The discovery of superconductivity in pressurized bilayer and trilayer\nnickelates has generated significant interest. However, their superconducting\nproperties are often dependent on sample quality and pressure conditions,\ncomplicating the interpretation of the underlying physics. Finding new systems\nwith optimized bulk superconducting properties is therefore important for\nadvancing our understanding of these materials. Unlike cupates, where trilayer\ncompounds typically exhibit the highest transition temperature (Tc), the\nbilayer nickelate La3Ni2O7 has thus far outperformed the trilayer La4Ni3O10 in\nreported Tc. Whether the trilayer nickelates have achieved the optimal Tc\nremains unclear, with various scenarios suggesting different possibilities.\nHere, we report the discovery of bulk superconductivity in pressurized\nPr4Ni3O10 single crystals, achieving a maximum onset Tc of 40.5 K at 80.1 GPa,\nsignificantly exceeding the 30 K observed in La4Ni3O10. The bulk nature of\nsuperconductivity is confirmed by zero resistance and a strong diamagnetic\nresponse below Tc with a superconducting volume fraction exceeding 80%. These\nfindings establish trilayer nickelates as genuine bulk high-temperature\nsuperconductors, provide new insights into the mechanisms driving\nsuperconductivity, and point to a promising route toward further enhancing\nsuperconducting properties in nickelates.",
        "Elasto-inertial turbulence (EIT) has been demonstrated to be able to sustain\nin two-dimensional (2D) channel flow; however the systematic investigations on\n2D EIT remain scare. This study addresses this gap by examining the statistical\ncharacteristics and dynamic mechanisms of 2D EIT, while exploring its\nsimilarities to and differences from three-dimensional (3D) EIT. We demonstrate\nthat the influence of elasticity on the statistical properties of 2D EIT\nfollows distinct trends compared to those observed in 3D EIT and drag-reducing\nturbulence (DRT). These differences can be attributed to variations in the\nunderlying dynamical processes. As nonlinear elasticity increases, the dominant\ndynamic evolution in 3D flows involves the gradual suppression of inertial\nturbulence (IT). In contrast, 2D flows exhibit a progressive enhancement of\nEIT. More strikingly, we identify an anomalous Reynolds stress in 2D EIT that\ncontributes negatively to flow resistance, a behavior opposite to that of IT.\nQuadrant analysis of velocity fluctuations reveals the predominance of motions\nin the first and third quadrants. These motions are closely associated with\npolymer sheet-like extension structures, which are inclined from the near-wall\nregion toward the channel center along the streamwise direction. Finally, we\npresent the dynamical budget of 2D EIT, which shows significant similarities to\nthat of 3D EIT, thereby providing compelling evidence for the objective\nexistence of the 2D nature of EIT.",
        "This research presents a robust approach to classifying COVID-19 cough sounds\nusing cutting-edge machine-learning techniques. Leveraging deep neural decision\ntrees and deep neural decision forests, our methodology demonstrates consistent\nperformance across diverse cough sound datasets. We begin with a comprehensive\nextraction of features to capture a wide range of audio features from\nindividuals, whether COVID-19 positive or negative. To determine the most\nimportant features, we use recursive feature elimination along with\ncross-validation. Bayesian optimization fine-tunes hyper-parameters of deep\nneural decision tree and deep neural decision forest models. Additionally, we\nintegrate the SMOTE during training to ensure a balanced representation of\npositive and negative data. Model performance refinement is achieved through\nthreshold optimization, maximizing the ROC-AUC score. Our approach undergoes a\ncomprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID,\nVirufy, and the combined Virufy with the NoCoCoDa dataset. Consistently\noutperforming state-of-the-art methods, our proposed approach yields notable\nAUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective\ndatasets. Merging all datasets into a combined dataset, our method, using a\ndeep neural decision forest classifier, achieves an AUC of 0.97. Also, our\nstudy includes a comprehensive cross-datasets analysis, revealing demographic\nand geographic differences in the cough sounds associated with COVID-19. These\ndifferences highlight the challenges in transferring learned features across\ndiverse datasets and underscore the potential benefits of dataset integration,\nimproving generalizability and enhancing COVID-19 detection from audio signals."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The Llama 3 Herd of Models",
    "start_abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Quantifying Variance in Evaluation Benchmarks"
      ],
      "abstract":[
        "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "De facto Openness to Immigration",
        "Extended $s$-wave pairing from an emergent Feshbach resonanc in bilayer\n  nickelate superconductors",
        "Optical control of the spin-Hall effect in a two-dimensional hole gas",
        "Quantum-enhanced neural networks for quantum many-body simulations",
        "A Link Between White Dwarf Pulsars and Polars: Multiwavelength\n  Observations of the 9.36-Minute Period Variable Gaia22ayj",
        "Decoding lithium's subtle phase stability with a machine learning force\n  field",
        "Decay of large solutions around shocks to multi-D viscous conservation\n  law with strictly convex flux",
        "Scalable architecture for dark photon searches: Superconducting-qubit\n  proof of principle",
        "On zero-sum Ramsey numbers modulo 3",
        "Isogenies of minimal Cantor systems: from Sturmian to Denjoy and\n  interval exchanges",
        "Modeling reflection and refraction of freeform surfaces",
        "Anisotropy can make a moving active fluid membrane rough or crumpled",
        "Tomographic identification of all molecular orbitals in a wide binding\n  energy range",
        "Star formation in interacting galaxy systems: UVIT imaging of NGC 7252\n  and NGC 5291",
        "Using Lagrangian descriptors to reveal the phase space structure of\n  dynamical systems described by fractional differential equations: Application\n  to the Duffing oscillator",
        "Evidence of Replica Symmetry Breaking under the Nishimori conditions in\n  epidemic inference on graphs",
        "Jointly Assigning Processes to Machines and Generating Plans for\n  Autonomous Mobile Robots in a Smart Factory",
        "ouladFormat R package: Preparing the Open University Learning Analytics\n  Dataset for analysis",
        "Large $\\theta$ angle in two-dimensional large $N$ $\\mathbb{CP}^{N-1}$\n  model",
        "On the learning power of Friedman-Stanley jumps",
        "Homoclinic and Heteroclinic Trajectories of Differential Equations with\n  Piecewise Constant Arguments of Generalized Type",
        "A deep BSDE approach for the simultaneous pricing and delta-gamma\n  hedging of large portfolios consisting of high-dimensional multi-asset\n  Bermudan options",
        "Impact of Data Patterns on Biotype identification Using Machine Learning",
        "Bandgap-Dependent Doping of Semiconducting Carbon Nanotube Networks by\n  Proton-Coupled Electron Transfer for Stable Thermoelectrics",
        "Evidence of the P_ccbars(4459)0 in Upsilon(1S, 2S) inclusive decays at\n  Belle",
        "Decoding FRB energetics and frequency features hidden by observational\n  incompleteness",
        "Search for the radiative leptonic decay $D^+\\to\\gamma e^+\\nu_e$ with\n  Deep Learning",
        "Observation of Giant Orbital Hall Effect in Si",
        "Best Approximations on Quasi-Cone Metric Spaces"
      ],
      "abstract":[
        "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
        "Since the discovery of unconventional superconductivity in cuprates,\nunraveling the pairing mechanism of charge carriers in doped antiferromagnets\nhas been a long-standing challenge. Motivated by the discovery of high-T$_c$\nsuperconductivity in nickelate bilayer La$_3$Ni$_2$O$_7$ (LNO), we study a\nminimal mixed dimensional (MixD) $t-J$ model supplemented with a repulsive\nCoulomb interaction $V$. When hole-doped, previous numerical simulations\nrevealed that the system exhibits strong binding energies, with a phenomenology\nresembling a BCS-to-BEC crossover accompanied by a Feshbach resonance between\ntwo distinct types of charge carriers. Here, we perform a mean-field analysis\nthat enables a direct observation of the BCS-to-BEC crossover as well as\nmicroscopic insights into the crossover region and the pairing symmetry for\ntwo-dimensional bilayers. We benchmark our mean-field description by comparing\nit to density-matrix renormalization group (DMRG) simulations in quasi-one\ndimensional settings and find remarkably good agreement. For the\ntwo-dimensional system relevant to LNO our mean-field calculations predict a\nBCS pairing gap with an extended $s$-wave symmetry, directly resulting from the\npairing mechanism's Feshbach-origin. Our analysis hence gives insights into\npairing in unconventional superconductors and, further, can be tested in\ncurrently available ultracold atom experiments.",
        "Relativistic effects influence the motion of charged particles in solids by\nintertwining spin and momentum. The resulting phenomena exhibit rich and\nintriguing properties that can unveil radically new quantum devices. In this\ncontext, the two-dimensional hole gas formed in group IV heterostructures is a\nparticularly promising platform, owning to a notable spin-orbit coupling.\nHowever, the exploitation of spin-momentum locking and precise manipulation of\nspin currents has remained elusive thus far. Here we use the modulation-doping\ntechnique to break inversion symmetry at novel Ge1-xSnx\/Ge interfaces and\nexplore spin-orbit phenomena in the emergent Rashba-coupled hole gases.\nMagneto-optical investigations demonstrate the unusual establishment of a\nstaggered band alignment with carrier lifetime in the ns range. Optical spin\norientation is then leveraged to directly inject spin-polarized currents in the\nRashba-split 2D gas. Spin-to-charge conversion is shown to genuinely occur at\nthe staggered gap through the inverse spin-Hall effect. This provides\nunprecedented access to low-order contributions of the spin-orbit Hamiltonian.\nMoreover, it leads to the startling demonstration that the spin Hall angle can\nbe optically controlled by modifying the Rashba coupling through the\nphotoexcitation density. Ge1-xSnx quantum wells thus offer innovative solutions\nand functionalities stemming from their unique spin-dependent properties and\nintriguing quantum phenomena at the crossroad between transport and photonic\nrealms.",
        "Neural quantum states (NQS) have gained prominence in variational quantum\nMonte Carlo methods in approximating ground-state wavefunctions. Despite their\nsuccess, they face limitations in optimization, scalability, and expressivity\nin addressing certain problems. In this work, we propose a quantum-neural\nhybrid framework that combines parameterized quantum circuits with neural\nnetworks to model quantum many-body wavefunctions. This approach combines the\nefficient sampling and optimization capabilities of autoregressive neural\nnetworks with the enhanced expressivity provided by quantum circuits. Numerical\nsimulations demonstrate the scalability and accuracy of the hybrid ansatz in\nspin systems and quantum chemistry problems. Our results reveal that the hybrid\nmethod achieves notably lower relative energy compared to standalone NQS. These\nfindings underscore the potential of quantum-neural hybrid methods for tackling\nchallenging problems in quantum many-body simulations.",
        "White dwarfs (WDs) are the most abundant compact objects, and recent surveys\nhave suggested that over a third of WDs in accreting binaries host a strong (B\n$\\gtrsim$ 1 MG) magnetic field. However, the origin and evolution of WD\nmagnetism remain under debate. Two WD pulsars, AR Sco and J191213.72-441045.1\n(J1912), have been found, which are non-accreting binaries hosting rapidly\nspinning (1.97-min and 5.30-min, respectively) magnetic WDs. The WD in AR Sco\nis slowing down on a $P\/\\dot{P}\\approx 5.6\\times 10^6$ yr timescale. It is\nbelieved they will eventually become polars, accreting systems in which a\nmagnetic WD (B $\\approx 10-240$ MG) accretes from a Roche lobe-filling donor\nspinning in sync with the orbit ($\\gtrsim 78$ min). Here, we present\nmultiwavelength data and analysis of Gaia22ayj, which outbursted in March 2022.\nWe find that Gaia22ayj is a magnetic accreting WD that is rapidly spinning down\n($P\/\\dot{P} = 6.1^{+0.3}_{-0.2}\\times 10^6$ yr) like WD pulsars, but shows\nclear evidence of accretion, like polars. Strong linear polarization (40%) is\ndetected in Gaia22ayj; such high levels have only been seen in the WD pulsar AR\nSco and demonstrate the WD is magnetic. High speed photometry reveals a\n9.36-min period accompanying a high amplitude ($\\sim 2$ mag) modulation. We\nassociate this with a WD spin or spin-orbit beat period, not an orbital period\nas was previously suggested. Fast (60-s) optical spectroscopy reveals a broad\n``hump'', reminiscent of cyclotron emission in polars, between 4000-8000\nAngstrom. We find an X-ray luminosity of $L_X = 2.7_{-0.8}^{+6.2}\\times10^{32}\n\\textrm{ erg s}^{-1}$ in the 0.3-8 keV energy range, while two VLA radio\ncampaigns resulted in a non-detection with a $F_r < 15.8\\mu\\textrm{Jy}$ 3$\n\\sigma$ upper limit. The shared properties of both WD pulsars and polars\nsuggest that Gaia22ayj is a missing link between the two classes of magnetic WD\nbinaries.",
        "Understanding the phase stability of elemental lithium (Li) is crucial for\noptimizing its performance in lithium-metal battery anodes, yet this seemingly\nsimple metal exhibits complex polymorphism that requires proper accounting for\nquantum and anharmonic effects to capture the subtleties in its flat energy\nlandscape. Here we address this challenge by developing an accurate graph\nneural network-based machine learning force field and performing efficient\nself-consistent phonon calculations for bcc-, fcc-, and 9R-Li under\nnear-ambient conditions, incorporating quantum, phonon renormalization and\nthermal expansion effects. Our results reveal the important role of\nanharmonicity in determining Li's thermodynamic properties. The free energy\ndifferences between these phases, particularly fcc- and 9R-Li are found to be\nonly a few meV\/atom, explaining the experimental challenges in obtaining\nphase-pure samples and suggesting a propensity for stacking faults and related\ndefect formation. fcc-Li is confirmed as the ground state at zero temperature\nand pressure, and the predicted bcc-fcc phase boundary qualitatively matches\nexperimental phase transition lines, despite overestimation of the transition\ntemperature and pressure slope. These findings provide crucial insights into\nLi's complex polymorphism and establish an effective computational approach for\nlarge-scale atomistic simulations of Li in more realistic settings for\npractical energy storage applications.",
        "We consider a planar viscous shock for a scalar viscous conservation law with\na strictly convex flux in multi-dimensional setting, where the transversal\ndirection is periodic. We first show the contraction property for any solutions\nevolving from a large bounded initial perturbation in $L^2$ of the viscous\nshock. The contraction holds up to a dynamical shift, and it is measured by a\nweighted relative entropy. This result for the contraction extends the existing\nresult in 1D \\cite{Kang19} to the multi-dimensional case. As a consequence, if\nthe large bounded initial $L^2$-perturbation is also in $L^1$, then the large\nperturbation decays of rate $t^{-1\/4}$ in $L^2$, up to a dynamical shift that\nis uniformly bounded in time. This is the first result for the quantitative\nestimate converging to a planar shock under large perturbations.",
        "The dark photon is a well-motivated candidate of dark matter due to its\npotential to open the window of new physics beyond the Standard Model. A\nfundamental mass-range-sensitivity dilemma is always haunting the dark photon\nsearching experiments: The resonant haloscopes have excellent sensitivity but\nare narrowband, and vice versa for the non-resonant ones. A scalable\narchitecture integrating numerous resonant haloscopes will be a desirable\nsolution to this dilemma. However, even the concept of scalable searching\nremains rarely explored, due to the size limitation of conventional haloscopes\nimposed by the dark photon wavelength. Here we propose and demonstrate a novel\narchitecture using superconducting qubits as sub-wavelength haloscope units. By\nvirtue of the scalability of superconducting qubits, it is possible to\nintegrate multiple qubits with different frequencies on a chip-scale device.\nFurthermore, the frequencies of the qubits can be tuned to extend the searching\nmass range. Thus, our architectures allow for searching for dark photons in a\nbroad mass range with high sensitivity. As a proof-of-principle experiment, we\ndesigned and fabricated a three-qubit chip and successfully demonstrated a\nscalable dark-photon searching. Our work established constraints on dark\nphotons in the mass range of 15.632 $\\mu$eV$\\sim$15.638 $\\mu$eV, 15.838\n$\\mu$eV$\\sim$15.845 $\\mu$eV, and 16.463 $\\mu$eV$\\sim$16.468 $\\mu$eV,\nsimultaneously, and the constraints are much more stringent than the cosmology\nconstraints. Our work can be scaled up in the future to boost the scrutiny of\nnew physics and extended to search for more dark matter candidates, including\ndark photons, axions and axion-like particles.",
        "We start with a systematic study of the zero-sum Ramsey numbers. For a graph\n$G$ with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, the zero-sum Ramsey number is defined as\nthe smallest positive integer $R(G, \\mathbb{Z}_3)$ such that for every $n \\geq\nR(G, \\mathbb{Z}_3)$ and every edge-colouring $f$ of $K_n$ using $\\mathbb{Z}_3$,\nthere is a zero-sum copy of $G$ in $K_n$ coloured by $f$, that is: $\\sum_{e \\in\nE(G)} f(e) \\equiv 0 \\ (\\!\\!\\!\\!\\mod 3)$.\n  Only sporadic results are known for these Ramsey numbers, and we discover\nmany new ones. In particular we prove that for every forest $F$ on $n$ vertices\nand with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, $R(F, \\mathbb{Z}_3) \\leq n+2$, and this\nbound is tight if all the vertices of $F$ have degrees $1 \\ (\\!\\!\\!\\!\\mod 3)$.\nWe also determine exact values of $R(T, \\mathbb{Z}_3)$ for infinite families of\ntrees.",
        "This work is motivated by the study of continued fraction expansions of real\nnumbers: we describe in dynamical terms their orbits under the action of\n$\\mathrm{PGL}_2(\\mathbb{Q})$. A real number gives rise to a Sturmian system\nencoding a rotation of the circle. It is well known that\n$\\mathrm{PGL}_2(\\mathbb{Z})$-equivalence of real numbers, characterized by the\ntails of their continued fraction expansions, amounts to flow equivalence of\nSturmian systems. We show that the multiplicative action of $m\\in \\mathbb{Z}$\non a real number corresponds to taking the $m$th-power followed by what we call\nan infinitesimal 2-asymptotic factor of its Sturmian system.\n  This leads us to introduce the notion of isogeny between zero-dimensional\nsystems: it combines virtual flow equivalences and infinitesimal asymptotic\nequivalences. We develop tools for classifying systems up to isogeny involving\ncohomological invariants and states. We then use this to give a complete\ndescription of $\\mathrm{PSL}_2(\\mathbb{Q})$-equivalence of real numbers in\nterms of Sturmian systems. We classify Denjoy systems up to isogenies within\nthis class via the action of $\\mathrm{PGL}_{2}(\\mathbb{Q})$ on their\ninvariants.\n  We also investigate eventual flow equivalence of Sturmian systems: we show\nthat for non-quadratic parameters it amounts to topological conjugacy and for\nquadratic parameters it implies total flow equivalence and other arithmetic\nconstraints.\n  In another direction, we consider interval exchanges satisfying Keane's\ncondition. We characterize flow equivalence in terms of interval-induced\nsubsystems (or the tails of their paths in the bilateral Rauzy induction\ndiagram). Finally we find rational invariants for isogeny involving the length\nmodules and SAF invariants of the associated ergodic measures. This leads to a\nconjecture for their classification up to isogeny, which we prove in the\ntotally ergodic case.",
        "In this work, we present a detailed procedure of computer implementation of\nthe laws of refraction and reflection on an arbitrary surface with rotational\nsymmetry with respect to the propagation axis. The goal is to facilitate the\nunderstanding and application of these physical principles in a computational\ncontext. This enables students and instructors alike to develop simulations and\ninteractive applications that faithfully replicate the behavior of light and\nsound propagating in a diversity of media separated by arbitrary surfaces. In\nparticular it can help to explore freeform optics. Additionally, we include a\npractical example demonstrating these implementations using either Matlab or\nopen-source Octave programming language.",
        "We present a hydrodynamic theory of anisotropic and inversion-asymmetric\nmoving active permeable fluid membranes. These are described by an anisotropic\nKardar-Parisi-Zhang equation. Depending upon the anisotropy parameters, the\nmembrane can be large-scale anisotropic and logarithmically rough with\ntranslational quasi long range order and orientational long range order,\ntogether with the relaxational dynamics being logarithmically faster than\nordinary diffusion. For other choices of the anisotropy parameters, the\nmembrane is either effectively isotropic and algebraically rough with\ntranslational short, but orientational long range order, or crumpled.",
        "In the past decade, photoemission orbital tomography (POT) has evolved into a\npowerful tool to investigate the electronic structure of organic molecules\nadsorbed on surfaces. Here we show that POT allows for the comprehensive\nexperimental identification of all molecular orbitals in a substantial binding\nenergy range, in the present case more than 10 eV. Making use of the angular\ndistribution of photoelectrons as a function of binding energy, we exemplify\nthis by extracting orbital-resolved partial densities of states (pDOS) for 15\n$\\pi$ and 23 $\\sigma$ orbitals from the experimental photoemission intensities\nof the prototypical organic molecule bisanthene (C$_{28}$H$_{14}$) on a Cu(110)\nsurface. In their entirety, these experimentally measured orbital-resolved pDOS\nfor an essentially complete set of orbitals serve as a stringent benchmark for\nelectronic structure methods, which we illustrate by performing density\nfunctional theory (DFT) calculations employing four frequently-used\nexchange-correlation functionals. By computing the respective\nmolecular-orbital-projected densities of states of the bisanthene\/Cu(110)\ninterface, a one-to-one comparison with experimental data for an unprecedented\nnumber of 38 orbital energies becomes possible. The quantitative analysis of\nour data reveals that the range-separated hybrid functional HSE performs best\nfor the investigated organic\/metal interface. At a more fundamental level, the\nremarkable agreement between the experimental and the Kohn-Sham orbital\nenergies over a binding energy range larger than 10\\,eV suggests that --\nperhaps unexpectedly -- Kohn-Sham orbitals approximate Dyson orbitals, which\nwould rigorously account for the electron extraction process in photoemission\nspectroscopy but are notoriously difficult to compute, in a much better way\nthan previously thought.",
        "Interactions play a significant role in the formation and evolution of\ngalaxies in the Universe. The galaxy systems, NGC 7252 and NGC 5291 are two\nnearby interacting systems that are hosting Tidal Dwarf Galaxies (TDGs) and\nstar-forming knots. The present work aims (a) To determine the\nattenuation-corrected star formation rate (SFR) of the interacting system NGC\n7252 (b) To compare the star formation in the NGC 7252 system with that of the\nNGC 5291 system (c) To explore the relation between surface densities of gas\nand SFR in these two systems. The study utilises high-resolution FUV and NUV\nimaging data from the Ultraviolet Imaging Telescope (UVIT) on board AstroSat.\nSix star-forming regions, including the merger remnant, were identified in the\nNGC 7252 system. The SFR corrected for attenuation of the knots in the NGC 7252\nsystem is determined using the continuum slope (\\beta) calculated from the\nFUV-NUV colour. It has been observed that the attenuation-corrected SFR values\nof the knots in this system fall within the range of SFR values determined for\nthe NGC 5291 knots. The TDGs in both systems adhere to the same\nKennicutt-Schmidt (KS) relation as regular spiral galaxies.",
        "We showcase the utility of the Lagrangian descriptors method in qualitatively\nunderstanding the underlying dynamical behavior of dynamical systems governed\nby fractional-order differential equations. In particular, we use the\nLagrangian descriptors method to study the phase space structure of the\nunforced and undamped Duffing oscillator when its time evolution is governed by\nfractional-order differential equations. In our study, we implement two types\nof fractional derivatives, namely the standard Gr\\\"unwald-Letnikov method,\nwhich is a finite difference approximation of the Riemann-Liouville fractional\nderivative, and a Gr\\\"unwald-Letnikov method with a correction term that\napproximates the Caputo fractional derivative. While there is no issue with\nforward-time integrations needed for the evaluation of Lagrangian descriptors,\nwe discuss in detail ways to perform the non-trivial task of backward-time\nintegrations and implement two methods for this purpose: a `nonlocal implicit\ninverse' technique and a `time-reverse inverse' approach. We analyze the\ndifferences in the Lagrangian descriptors results due to the two backward-time\nintegration approaches, discuss the physical significance of these differences,\nand eventually argue that the nonlocal implicit inverse implementation of the\nGr\\\"unwald-Letnikov fractional derivative manages to reveal the phase space\nstructure of fractional-order dynamical systems correctly.",
        "In Bayesian inference, computing the posterior distribution from the data is\ntypically a non-trivial problem, which usually requires approximations such as\nmean-field approaches or numerical methods, like the Monte Carlo Markov Chain.\nBeing a high-dimensional distribution over a set of correlated variables, the\nposterior distribution can undergo the notorious replica symmetry breaking\ntransition. When it happens, several mean-field methods and virtually every\nMonte Carlo scheme can not provide a reasonable approximation to the posterior\nand its marginals. Replica symmetry is believed to be guaranteed whenever the\ndata is generated with known prior and likelihood distributions, namely under\nthe so-called Nishimori conditions. In this paper, we break this belief, by\nproviding a counter-example showing that, under the Nishimori conditions,\nreplica symmetry breaking arises. Introducing a simple, geometrical model that\ncan be thought of as a patient zero retrieval problem in a highly infectious\nregime of the epidemic Susceptible-Infectious model, we show that under the\nNishimori conditions, there is evidence of replica symmetry breaking. We\nachieve this result by computing the instability of the replica symmetric\ncavity method toward the one step replica symmetry broken phase. The origin of\nthis phenomenon -- replica symmetry breaking under the Nishimori conditions --\nis likely due to the correlated disorder appearing in the epidemic models.",
        "A modern smart factory runs a manufacturing procedure using a collection of\nprogrammable machines. Typically, materials are ferried between these machines\nusing a team of mobile robots. To embed a manufacturing procedure in a smart\nfactory, a factory operator must a) assign its processes to the smart factory's\nmachines and b) determine how agents should carry materials between machines. A\ngood embedding maximizes the smart factory's throughput; the rate at which it\noutputs products. Existing smart factory management systems solve the\naforementioned problems sequentially, limiting the throughput that they can\nachieve. In this paper we introduce ACES, the Anytime Cyclic Embedding Solver,\nthe first solver which jointly optimizes the assignment of processes to\nmachines and the assignment of paths to agents. We evaluate ACES and show that\nit can scale to real industrial scenarios.",
        "Analysing educational data sets is fundamental to many fields of research\nfocusing on improving student learning. However, large educational data sets\nare complex and can involve intensive preprocessing. These obstacles can be\novercome through the development of educational tools which simplifies the\npreprocessing stages of analysis. The Open University Learning Analytics\nDataset (OULAD), available online, contains data from 32,593 students across 22\nmodule presentations at the Open University. This paper introduces the R\nsoftware package ouladFormat; which loads and formats the OULAD for data\nanalysis. The paper summarizes the ouladFormat R package and explains the\ndifferent functions within the package. In addition, two case studies are\nprovided which discuss how the OULAD and ouladFormat R package could be used\nwhen preparing for an educational study, and in the early identification of\nat-risk students. The package increases the accessibility of the OULAD for\nresearchers, practitioners, and educators, and supports reproducibility and\ncomparability of educational studies.",
        "In confining large $N$ theories with a $\\theta$ angle such as\nfour-dimensional $\\rm{SU}(N)$ pure Yang-Mills theory, there are multiple\nmetastable vacua and it makes sense to consider the parameter region of ``large\n$\\theta$ of order $N$'' despite the fact that $\\theta$ is a $2\\pi$-periodic\nparameter. We investigate this parameter region in the two-dimensional\n$\\mathbb{CP}^{N-1}$ model by computing the partition function on $T^2$. When\n$\\theta\/N$ is of order $ \\mathcal{O}(0.1) $ or less, we get perfectly sensible\nresults for the vacuum energies and decay rates of metastable vacua. However,\nwhen $\\theta\/N$ is of order $\\mathcal{O}(1) $, we encounter a problem about\nsaddle points that would give larger contributions to the partition function\nthan the true vacuum. We discuss why it might not be straightforward to resolve\nthis problem.",
        "Recently, a surprising connection between algorithmic learning of algebraic\nstructures and descriptive set theory has emerged. Following this line of\nresearch, we define the learning power of an equivalence relation $E$ on a\ntopological space as the class of isomorphism relations with countably many\nequivalence classes that are continuously reducible to $E$. In this paper, we\ndescribe the learning power of the finite Friedman-Stanley jumps of\n$=_{\\mathbb{N}}$ and $=_{\\mathbb{N}^\\mathbb{N}}$, proving that these\nequivalence relations learn the families of countable structures that are\npairwise distinguished by suitable infinitary sentences. Our proof techniques\nintroduce new ideas for assessing the continuous complexity of Borel\nequivalence relations.",
        "Quasilinear systems with piecewise constant arguments of generalized type are\nunder investigation from the asymptotic point of view. The systems have\ndiscontinuous right-hand sides which are identified via a discrete-time map. It\nis rigorously proved that homoclinic and heteroclinic solutions are generated,\nand they are taken into account in the functional sense. The Banach fixed point\ntheorem is used for the verification. The hyperbolic set of solutions is also\ndiscussed, and an example supporting the theoretical findings is provided.",
        "A deep BSDE approach is presented for the pricing and delta-gamma hedging of\nhigh-dimensional Bermudan options, with applications in portfolio risk\nmanagement. Large portfolios of a mixture of multi-asset European and Bermudan\nderivatives are cast into the framework of discretely reflected BSDEs. This\nsystem is discretized by the One Step Malliavin scheme (Negyesi et al. [2024,\n2025]) of discretely reflected Markovian BSDEs, which involves a $\\Gamma$\nprocess, corresponding to second-order sensitivities of the associated option\nprices. The discretized system is solved by a neural network regression Monte\nCarlo method, efficiently for a large number of underlyings. The resulting\noption Deltas and Gammas are used to discretely rebalance the corresponding\nreplicating strategies. Numerical experiments are presented on both\nhigh-dimensional basket options and large portfolios consisting of multiple\noptions with varying early exercise rights, moneyness and volatility. These\nexamples demonstrate the robustness and accuracy of the method up to $100$ risk\nfactors. The resulting hedging strategies significantly outperform benchmark\nmethods both in the case of standard delta- and delta-gamma hedging.",
        "Background: Patient stratification in brain disorders remains a significant\nchallenge, despite advances in machine learning and multimodal neuroimaging.\nAutomated machine learning algorithms have been widely applied for identifying\npatient subtypes (biotypes), but results have been inconsistent across studies.\nThese inconsistencies are often attributed to algorithmic limitations, yet an\noverlooked factor may be the statistical properties of the input data. This\nstudy investigates the contribution of data patterns on algorithm performance\nby leveraging synthetic brain morphometry data as an exemplar.\n  Methods: Four widely used algorithms-SuStaIn, HYDRA, SmileGAN, and SurrealGAN\nwere evaluated using multiple synthetic pseudo-patient datasets designed to\ninclude varying numbers and sizes of clusters and degrees of complexity of\nmorphometric changes. Ground truth, representing predefined clusters, allowed\nfor the evaluation of performance accuracy across algorithms and datasets.\n  Results: SuStaIn failed to process datasets with more than 17 variables,\nhighlighting computational inefficiencies. HYDRA was able to perform\nindividual-level classification in multiple datasets with no clear pattern\nexplaining failures. SmileGAN and SurrealGAN outperformed other algorithms in\nidentifying variable-based disease patterns, but these patterns were not able\nto provide individual-level classification.\n  Conclusions: Dataset characteristics significantly influence algorithm\nperformance, often more than algorithmic design. The findings emphasize the\nneed for rigorous validation using synthetic data before real-world application\nand highlight the limitations of current clustering approaches in capturing the\nheterogeneity of brain disorders. These insights extend beyond neuroimaging and\nhave implications for machine learning applications in biomedical research.",
        "Networks of semiconducting single-walled carbon nanotubes (SWNTs) are a\npromising material for thermoelectric energy harvesting due to their mechanical\nflexibility, solution processability, high Seebeck coefficients and high\nelectrical conductivities after chemical p- or n-doping. Here, we demonstrate\nthat proton-coupled electron transfer (PCET) with benzoquinone (BQ) as the\noxidant and lithium bis(trifluoromethylsulfonyl)imide (Li[TFSI]) for\nelectrolyte counterions is a promising method for p-doping of polymer-sorted\nsemiconducting SWNT networks. The achieved doping levels, as determined from\nabsorption bleaching, depend directly on both the pH of the aqueous doping\nsolutions and the bandgap (i.e., diameter) of the nanotubes within the network.\nFast screening of different nanotube networks under various doping conditions\nwas enabled by a high-throughput setup for thermoelectric measurements of five\nsamples in parallel. For small-bandgap SWNTs, PCET-doping is sufficient to\nreach the maximum thermoelectric power factors, which are equal to those\nobtained by conventional methods. In contrast to other doping methods, the\nelectrical conductivity of PCET-doped SWNTs remains stable over at least 5 days\nin air. These results confirm PCET to be a suitable approach for more\nenvironmentally friendly and stable doping of semiconducting SWNTs as promising\nthermoelectric materials.",
        "Using data samples of 102 million Upsilon(1S) events and 158 million\nUpsilon(2S) events collected by the Belle detector at the KEKB\nasymmetric-energy $e^+e^-$ collider, we search for [udsccbar] pentaquark states\ndecaying to Jpsi Lambda. Using the first observations of Upsilon(1S, 2S)\ninclusive decays to Jpsi Lambda, we find evidence of the P_ccbars(4459)0 state\nwith a significance of 3.3 standard deviations, including statistical and\nsystematic uncertainties. We measure the mass and width of the Pccbars(4459)0\nto be (4471.7 +- 4.8 +- 0.6) MeV\/c2 and (21.9 +- 13.1 +- 2.7) MeV,\nrespectively. The branching fractions for P_ccbars(4459)0 production are\nmeasured to be B[Upsilon(1S) -> P_ccbars(4459)0\/ Pbar_ccbars(4459)0 + anything]\n= (3.5 +- 2.0 +- 0.2)*10-6 and B[Upsilin(2S) -> P_ccbars(4459)0\/\nPbar_ccbars(4459)0 +anything] = (2.9 +- 1.7 +- 0.4)*10-6. The inclusive\nbranching fractions of Upsilon(1S, 2S) -> Jpsi Lambda\/Lambdabar are measured to\nbe B[Upsilin(1S) -> Jpsi Lambda\/Lambdabar + anything] = (36.9 +- 5.3 +-\n2.4)*10-6 and B[Upsilon(2S) -> Jpsi Lambda\/Lambdabar + anything] = (22.3 +- 5.7\n+- 3.1)*10-6. We measure the visible cross section $\\sigma(e^+e^- \\to J\/psi\n\\Lambda\/\\bar\\Lambda$ + anything) = (90 +- 14 +- 6) fb for the continuum\nproduction at $\\sqrt{s} = 10.52$ GeV. In all cases, the first uncertainties are\nstatistical and the second are systematic.",
        "Fast radio bursts (FRBs) are fierce radio flashes lasting for a few\nmilliseconds from the sky. Although their connection to strongly magnetized\nneutron stars has been strongly indicated, the exact triggering process and\nradiation mechanism are still unknown and highly debated. Due to their\nextremely short duration, the observation of FRBs has long been a difficult\ntask even for large radio telescopes. The difficulty results from the fact that\nthe information obtained in observations is always incomplete, since the\ntelescope always has a limited flux sensitivity and finite operating frequency\nband. A pressing challenge is to decode the intrinsic features of FRBs from the\nincomplete observations. Here we establish an efficient methodology to overcome\nthis problem, aiming at effectively correcting for the fluence and frequency\ncutoffs. Using this method, inverse modeling is performed on a large number of\nrepeating bursts from FRB 20121102A to recover their intrinsic features. It is\nfound that strong bursts intrinsically tend to concentrate their energy in a\nnarrow band, while the spectral range of weak bursts can be either narrow or\nwide. However, when a weak burst has a broad spectrum, the wing of the spectrum\ncan easily go undetected, resulting in a very narrow spectrum being observed.\nThe narrow spectrum features observed in repeating FRBs are thus an\nobservational selection effect. Underestimation of the burst energy caused by\nobservational cutoffs is also corrected for, and the intrinsic burst energy\ndistribution is re-constructed. It is also found that the bandwidth increases\nwith the increasing central frequency in the Arecibo sample (1.15-1.73 GHz),\nbut such a correlation is not observed in the FAST (1-1.5 GHz) and GBT (4-8\nGHz) sample. It indicates the emission pattern of the FRB source might vary\nacross different active periods and frequency bands.",
        "Using 20.3$~\\rm fb^{-1}$ of $e^+e^-$ annihilation data collected at a\ncenter-of-mass energy of 3.773$~\\rm GeV$ with the BESIII detector, we report an\nimproved search for the radiative leptonic decay $D^+\\to\\gamma e^+\\nu_e$. An\nupper limit on its partial branching fraction for photon energies\n$E_\\gamma>10~\\rm MeV$ is determined to be $1.2\\times10^{-5}$ at 90\\% confidence\nlevel, which excludes most current theoretical predictions. A sophisticated\ndeep learning approach with thorough validation, based on the Transformer\narchitecture, is implemented to efficiently distinguish the signal from massive\nbackgrounds.",
        "Controlling\/storing information carriers, such as electron charge and spin,\nis key for modern information society, and significant efforts have been paid\nmade to establish novel technologies at the nanoscale. The rise of Si-based\nsemiconductor technology and magnetism-based technology has been motivated by\nthe aforementioned demands. However, both technologies have been individually\ndeveloped, with little effort in fusing them. Hence, establishing a technology\nto bridge semiconductor and magnetism-based technologies that would allow\nrealization of a novel information device is strongly awaited. In line with\nthis research strategy, the creation of a magnetic device using semiconductors\nwould enable fundamental innovation. Here, we show that a mother material for\nmodern electronics, Si, gives rise to a giant room-temperature orbital Hall\neffect (OHE), enabling the creation of novel energy-efficient magnetic memory\nvia efficient torque generation. The orbital torque efficiency largely exceeds\nthat of the archetypal metallic materials used in the OHE. Our achievement\novertures the conventional understanding that nonmagnetic semiconductors cannot\nplay a pivotal role in magnetic devices and paves a new avenue for creating\nnovel information devices through the fusion of semiconductor and\nmagnetism-based technologies.",
        "This paper focuses on the best approximation in quasi-cone metric spaces, a\ncombination of quasi-metrics and cone metrics, which generalizes the notion of\ndistance by allowing it to take values in an ordered Banach space. We explore\nthe fundamental properties of best approximations in this setting, such as the\nbest approximation sets and the Chebyshev sets."
      ]
    }
  },
  {
    "id":2411.18784,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"nnu-net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Multimodal Breast Parenchymal Patterns Correlation Using a Patient-Specific Biomechanical Model"
      ],
      "abstract":[
        "In this paper, we aim to produce a realistic 2-D projection of the breast parenchymal distribution from a 3-D breast magnetic resonance image (MRI). To evaluate the accuracy of our simulation, we compare our results with the local breast density (i.e., density map) obtained from the complementary full-field digital mammogram. To achieve this goal, we have developed a fully automatic framework, which registers MRI volumes to X-ray mammograms using a subject-specific biomechanical model of the breast. The optimization step modifies the position, orientation, and elastic parameters of the breast model to perform the alignment between the images. When the model reaches an optimal solution, the MRI glandular tissue is projected and compared with the one obtained from the corresponding mammograms. To reduce the loss of information during the ray-casting, we introduce a new approach that avoids resampling the MRI volume. In the results, we focus our efforts on evaluating the agreement of the distributions of glandular tissue, the degree of structural similarity, and the correlation between the real and synthetic density maps. Our approach obtained a high-structural agreement regardless the glandularity of the breast, whilst the similarity of the glandular tissue distributions and correlation between both images increase in denser breasts. Furthermore, the synthetic images show continuity with respect to large structures in the density maps."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "Mantra: Rewriting Quantum Programs to Minimize Trap-Movements for Zoned\n  Rydberg Atom Arrays",
        "Investigating quantum criticality through charged scalar fields near the\n  BTZ black hole horizon",
        "Discovery of High-Temperature Superconducting Ternary Hydrides via Deep\n  Learning",
        "Cove-edged Chiral Graphene Nanoribbons with Chirality-Dependent Bandgap\n  and Carrier Mobility",
        "Nonlinear Einstein-Power-Yang-Mills AdS Black Holes: From Quantum\n  Tunneling to Aschenbach Effect",
        "Multi-set variational quantum dynamics algorithm for simulating\n  nonadiabatic dynamics on quantum computers",
        "Cooperative Decay of N Atoms in a Ring Configuration",
        "NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed\n  Machine Learning",
        "On a geometric extremum problem for convex cones",
        "Streamlining Compliance And Risk Management with Regtech Solutions",
        "Rationalizability and Monotonocity in Games with Incomplete Information",
        "On representations of the crystallization of the quantized function\n  algebra C(SUq(n + 1))",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Remote State Estimation over a Wearing Channel: Information Freshness\n  vs. Channel Aging",
        "Polynomial maps with constants on split octonion algebras",
        "Exponentially Tilted Thermodynamic Maps (expTM): Predicting Phase\n  Transitions Across Temperature, Pressure, and Chemical Potential",
        "Photon-by-photon quantum light state engineering",
        "Storing quantum coherence in a quantum dot nuclear spin ensemble for\n  over 100 milliseconds",
        "Uhlmann's theorem for relative entropies",
        "Arithmetic properties for Overpartitions where nonoverlined parts are\n  $\\ell$-regular",
        "On the possibility of chiral symmetry breaking in liquid hydrogen\n  peroxide",
        "Multi-channel pattern reconstruction through $L$-directional associative\n  memories",
        "New Bounds for Sparse Variational Gaussian Processes",
        "Automated Microsolvation for Minimum Energy Path Construction in\n  Solution",
        "Quantitative observability for the Schr\\\"{o}dinger equation with an\n  anharmonic oscillator",
        "Higher Eckmann-Hilton Arguments in Type Theory",
        "Leveraging Pre-Trained Visual Transformers for Multi-Band Photometric\n  Light Curve Classification",
        "Probabilistic adaptation of language comprehension for individual\n  speakers: Evidence from neural oscillations",
        "An Air-Gap Element for the Isogeometric Space-Time-Simulation of\n  Electric Machines"
      ],
      "abstract":[
        "A zoned neutral atom architecture achieves exceptional fidelity by\nsegregating the execution spaces of 1- and 2-qubit gates, being a promising\ncandidate for high-accuracy quantum systems. Unfortunately, naively applying\nprograms designed for static qubit topologies to zoned architectures may result\nin most execution time being consumed by inter-zone travels of atoms. To\naddress this, we introduce Mantra (Minimizing trAp movemeNts for aTom aRray\nArchitectures), which rewrites quantum programs to reduce the interleaving of\nsingle- and two-qubit gates. Mantra incorporates three strategies: (i) a\nfountain-shaped controlled-Z (CZ) chain, (ii) ZZ-interaction protocol without a\n1-qubit gate, and (iii) preemptive gate scheduling. Mantra reduces inter-zone\nmovements by 68%, physical gate counts by 35%, and improves circuit fidelities\nby 17% compared to the standard executions.",
        "We examine a charged scalar field with a position-dependent mass \\( m(\\rho) =\nm_0 + \\mathcal{S}(\\rho) \\), where \\(\\mathcal{S}(\\rho)\\) represents a Lorentz\nscalar potential, near a BTZ black hole in the presence of an external magnetic\nfield. By deriving the Klein-Gordon equation for this setup, we explore two\nscenarios: (i) a mass-modified scalar field with \\(m(\\rho) = m_0 - a\/\\rho\\) (an\nexactly solvable case), and (ii) a scenario involving both mass modification\nand an external magnetic field (conditionally exactly solvable). We identify\nquantum critical points (QCPs) associated with the coupling constant \\(a\\). In\nthe first scenario, for massless charged scalar fields, critical points occur\nat \\(a = n + 1\/2\\) for all radial quantum numbers \\(n \\geq 0\\) and magnetic\nquantum numbers \\(|m| \\geq 0\\). In the second scenario, these critical points\nshift to \\(a = n + 3\/2\\) for \\(n \\geq 0\\) and \\(|m| > 0\\), with the case \\(m =\n0\\) excluded. For massive scalar fields, QCPs emerge at \\(a = (n + 1\/2)\/2\\),\nleading to non-propagating fields at zero frequency. At these QCPs, the field\nfrequencies drop to zero, marking transitions from stable oscillatory modes to\nnon-propagating states. Below the critical points, the system exhibits\ninstability, characterized by negative imaginary frequencies that suggest rapid\ndecay and high dissipation. Above the critical points, the modes stabilize and\npropagate, indicating a transition to a superconducting-like phase, where\ndissipation vanishes and stable excitations dominate.",
        "The discovery of novel high-temperature superconductor materials holds\ntransformative potential for a wide array of technological applications.\nHowever, the combinatorially vast chemical and configurational search space\nposes a significant bottleneck for both experimental and theoretical\ninvestigations. In this study, we employ the design of high-temperature ternary\nsuperhydride superconductors as a representative case to demonstrate how this\nchallenge can be well addressed through a deep-learning-driven theoretical\nframework. This framework integrates high-throughput crystal structure\nexploration, physics-informed screening, and accurate prediction of\nsuperconducting critical temperatures. Our approach enabled the exploration of\napproximately 36 million ternary hydride structures across a chemical space of\n29 elements, leading to the identification of 144 potential high-Tc\nsuperconductors with predicted Tc > 200 K and superior thermodynamic stability\nat 200 GPa. Among these, 129 compounds spanning 27 novel structural prototypes\nare reported for the first time, representing a significant expansion of the\nknown structural landscape for hydride superconductors. This work not only\ngreatly expands the known repertoire of high-Tc hydride superconductors but\nalso establishes a scalable and efficient methodology for navigating the\ncomplex landscape of multinary hydrides.",
        "Graphene nanoribbons (GNRs) have garnered significant interest due to their\nhighly customizable physicochemical properties and potential utility in\nnanoelectronics. Besides controlling widths and edge structures, the inclusion\nof chirality in GNRs brings another dimension for fine-tuning their\noptoelectronic properties, but related studies remain elusive owing to the\nabsence of feasible synthetic strategies. Here, we demonstrate a novel class of\ncove-edged chiral GNRs (CcGNRs) with a tunable chiral vector (n,m). Notably,\nthe bandgap and effective mass of (n,2)- CcGNR show a distinct positive\ncorrelation with the increasing value of n, as indicated by theory. Within this\nGNR family, two representative members, namely, (4,2)- CcGNR and (6,2)-CcGNR,\nare successfully synthesized. Both CcGNRs exhibit prominently curved geometries\narising from the incorporated [4]helicene motifs along their peripheries, as\nalso evidenced by the single-crystal structures of the two respective model\ncompounds (1 and 2). The chemical identities and optoelectronic properties of\n(4,2)- and (6,2)-CcGNRs are comprehensively investigated via a combination of\nIR, Raman, solid-state NMR, UV-vis, and THz spectroscopies as well as\ntheoretical calculations. In line with theoretical expectation, the obtained\n(6,2)-CcGNR possesses a low optical bandgap of 1.37 eV along with charge\ncarrier mobility of 8 cm2\/Vs, whereas (4,2)-CcGNR exhibits a narrower bandgap\nof 1.26 eV with increased mobility of 14 cm2\/Vs. This work opens up a new\navenue to precisely engineer the bandgap and carrier mobility of GNRs by\nmanipulating their chiral vector.",
        "This study investigates the thermodynamic and quantum properties of\nEinstein-Power-Yang-Mills (EPYM) black holes in an Anti-de Sitter background,\nfocusing on the effects of the nonlinear Yang-Mills charge parameter $\\gamma$.\nWe derive the metric function, analyze Hawking radiation through boson\ntunneling, and calculate thermodynamic properties including temperature and\nphase transitions. The quantum tunneling of $W^+$ bosons is examined using the\nWKB approximation and Hamilton-Jacobi formalism, revealing how nonlinearity\nmodifies the radiation spectrum. We compute the effective potential governing\nphoton orbits and null geodesics, demonstrating significant alterations in\nlight behavior in strong gravitational fields. Additionally, we explore the\nAschenbach effect, showing that this phenomenon, which is typically associated\nwith rotating black holes, can emerge in spherically symmetric EPYM spacetimes\nbecause of non-linear field interactions. Our results may yield observational\nmarkers that can be identified with instruments such as the Event Horizon\nTelescope and upcoming gravitational wave detectors.",
        "Accelerating quantum dynamical simulations with quantum computing has\nreceived considerable attention but remains a significant challenge. In\nvariational quantum algorithms for quantum dynamics, designing an expressive\nand shallow-depth parameterized quantum circuit (PQC) is a key difficulty.\nHere, we proposed a multi-set variational quantum dynamics algorithm (MS-VQD)\ntailored for nonadiabatic dynamics involving multiple electronic states. MS-VQD\nemploys multiple PQCs to represent the electronic-nuclear coupled wavefunction,\nwith each circuit adapting to the motion of nuclear wavepacket on a specific\npotential energy surface. By simulating excitation energy transfer dynamics in\nmolecular aggregates described by the Frenkel-Holstein model, we demonstrated\nthat MS-VQD achieves the same accuracy as traditional VQD while requiring\nsignificantly shallower PQCs. Notably, its advantage increases with the number\nof electronic states, making it suitable for simulating nonadiabatic quantum\ndynamics in complex molecular systems.",
        "We provide an analytic expression of the spectrum of the cooperative decay\nrate of N two-level atoms regularly distributed on a ring in the\nsingle-excitation configuration. The results are obtained first for the scalar\nmodel and then extended to the vectorial light model, assuming all the dipoles\nare aligned.",
        "NeuralFoil is an open-source Python-based tool for rapid aerodynamics\nanalysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to\n1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.\nNeuralFoil computes both global and local quantities (lift, drag, velocity\ndistribution, etc.) over a broad input space, including: an 18-dimensional\nspace of airfoil shapes, possibly including control deflections; a 360 degree\nrange of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic\nflows up to the transonic drag rise; and with varying turbulence parameters.\nResults match those of XFoil closely: the mean relative error of drag is 0.37%\non simple cases, and remains as low as 2.0% on a test dataset with numerous\npost-stall and transitional cases. NeuralFoil facilitates gradient-based design\noptimization, due to its $C^\\infty$-continuous solutions,\nautomatic-differentiation-compatibility, and bounded computational cost without\nnon-convergence issues.\n  NeuralFoil is a hybrid of physics-informed machine learning techniques and\nanalytical models. Here, physics information includes symmetries that are\nstructurally embedded into the model architecture, feature engineering using\ndomain knowledge, and guaranteed extrapolation to known limit cases. This work\nalso introduces a new approach for surrogate model uncertainty quantification\nthat enables robust design optimization.\n  This work discusses the methodology and performance of NeuralFoil with\nseveral case studies, including a practical airfoil design optimization study\nincluding both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil\noptimization is able to produce airfoils nearly identical in performance and\nshape to expert-designed airfoils within seconds; these\ncomputationally-optimized airfoils provide a useful starting point for further\nexpert refinement.",
        "We discuss the optimization problem for minimizing the $(n-1)$-volume of the\nintersection of a convex cone in $\\Bbb R^n$ with a hyperplane through a given\npoint.",
        "RegTech is a rapidly rising financial services sector focused on using\ncutting-edge technology to improve the process of regulatory compliance.\nRegTech solutions are characterized by numerous features and benefits that can\nconsiderably contribute to helping organizations operate effectively in the\nincreasingly regulated environment, when it comes to compliance and risk\nmanagement. This paper sheds light on why RegTech will be one of the most\npromising markets, driven by the rising cost of compliance and the growing\nreliance on technology in crisis management. Moreover, this paper will examine\nthe advantages of using such solutions to strike a balance between compliance\nand operational efficiencies. This paper will deepen the understanding of\nregulatory compliance, introduce RegTech, and examine the benefits of using\nthese solutions to achieve compliance.",
        "This paper examines games with strategic complements or substitutes and\nincomplete information, where players are uncertain about the opponents'\nparameters. We assume that the players' beliefs about the opponent's parameters\nare selected from some given set of beliefs. One extreme is the case where\nthese sets only contain a single belief, representing a scenario where the\nplayers' actual beliefs about the parameters are commonly known among the\nplayers. Another extreme is the situation where these sets contain all possible\nbeliefs, representing a scenario where the players have no information about\nthe opponents' beliefs about parameters. But we also allow for intermediate\ncases, where these sets contain some, but not all, possible beliefs about the\nparameters. We introduce an assumption of weakly increasing differences that\ntakes both the choice belief and parameter belief of a player into account.\nUnder this assumption, we demonstrate that greater choice-parameter beliefs\nleads to greater optimal choices. Moreover, we show that the greatest and least\npoint rationalizable choice of a player is increasing in their parameter, and\nthese can be determined through an iterative procedure. In each round of the\niterative procedure, the lowest surviving choice is optimal for the lowest\nchoice-parameter belief, while the greatest surviving choice is optimal for the\nhighest choice-parameter belief.",
        "The crystallization C(SU0(n + 1)) of the q-family of C*-algebras C(SUq(n +\n1)) was defined by Giri & Pal for all n \\geq 2 and they observed that any\nirreducible representation at the q = 0 level is the norm limit of irreducible\nrepresentations of C(SUq(n + 1)) as q\\to 0+. In this article, we will\ngeneralize this to any non-degenerate representation (respectively faithful) of\nC(SU0(n+1)). As a consequence, one can alternatively realize C(SU0(n+1)) as the\nC*-algebra generated by the limit operators of faithful representations of\nC(SUq(n+1)).",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "We study the remote estimation of a linear Gaussian system over a\nnonstationary channel that wears out over time and with every use. The sensor\ncan either transmit a fresh measurement in the current time slot, restore the\nchannel quality at the cost of downtime, or remain silent. More frequent\ntransmissions yield accurate estimates but incur significant wear on the\nchannel. Renewing the channel too often improves channel conditions but results\nin poor estimation quality. What is the optimal timing to transmit measurements\nand restore the channel? We formulate the problem as a Markov decision process\n(MDP) and show the monotonicity properties of an optimal policy. A structured\npolicy iteration algorithm is proposed to find the optimal policy.",
        "Let $\\mathbf{O}(\\mathbb{F})$ be the split octonion algebra over an\nalgebraically closed field $\\mathbb{F}$. For positive integers $k_1, k_2\\geq\n2$, we study surjectivity of the map $A_1(x^{k_1}) + A_2(y^{k_2}) \\in\n\\mathbf{O}(\\mathbb{F})\\langle x, y\\rangle$ on $\\mathbf{O}(\\mathbb{F})$. For\nthis, we use the orbit representatives of the ${G}_2(\\mathbb{F})$-action on\n$\\mathbf{O}(\\mathbb{F}) \\times \\mathbf{O}(\\mathbb{F}) $ for the tuple $(A_1,\nA_2)$, and characterize the ones which give a surjective map.",
        "Predicting and characterizing phase transitions is crucial for understanding\ngeneric physical phenomena such as crystallization, protein folding and others.\nHowever, directly observing phase transitions is not always easy, and often one\nhas limited observations far from the phase boundary and measured under some\nspecific thermodynamic conditions. In this study, we propose a statistical\nphysics and Generative AI driven framework that can take such limited\ninformation to generate samples of different phases under arbitrary\nthermodynamic conditions, which we name Exponentially Tilted Thermodynamic Maps\n(expTM). The central idea is to map collected data into a tractable simple\nprior expressed as an exponentially tilted Gaussian. We demonstrate how the\nvariance and mean of the prior can be correlated with pairs of thermodynamic\ncontrol variables, including temperature, pressure, and chemical potential.\nThis gives us the ability to generate thermodynamically correct samples under\nany values of the control variables. To demonstrate the practical applicability\nof this approach, we use expTM to sample the lattice gas models with the Grand\nCanonical ensemble, capturing phase transitions under varying chemical\npotentials and temperatures. We further demonstrate how expTM can model the\nisothermal-isobaric ensemble, with which we predict different phases of CO2\nunder varying pressure conditions. Both examples are trained on very limited\ndata far from the phase boundary. These results establish expTM as a robust\ntool for understanding phase transitions across diverse thermodynamic\nconditions requiring only a small number of observations.",
        "The ability to manipulate light at the level of single photons, its\nelementary excitation quanta, has recently made it possible to produce a rich\nvariety of tailor-made quantum states and arbitrary quantum operations, of high\ninterest for fundamental science and applications. Here we present a concise\nreview of the progress made over the last few decades in the engineering of\nquantum light states. Although far from exhaustive, this review aims at\nproviding a sufficiently wide and updated introduction that may serve as the\nentry point to such a fascinating and rapidly evolving field.",
        "States with long coherence are a crucial requirement for qubits and quantum\nmemories. Nuclear spins in epitaxial quantum dots are a great candidate,\noffering excellent isolation from external environments and on-demand coupling\nto optical flying qubits. However, coherence times are limited to $\\lesssim1$\nms by the dipole-dipole interactions between the nuclei and their quadrupolar\ncoupling to inhomogeneous crystal strain. Here, we combine strain engineering\nof the nuclear spin ensemble and tailored dynamical decoupling sequences to\nachieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible\ntransfer of quantum information into nuclear spin ensembles has been\ndemonstrated in quantum dots. Our results provide a path to develop this\nconcept into a functioning solid-state quantum memory suitable for quantum\nrepeaters in optical quantum communication networks.",
        "Uhlmann's theorem states that, for any two quantum states $\\rho_{AB}$ and\n$\\sigma_A$, there exists an extension $\\sigma_{AB}$ of $\\sigma_A$ such that the\nfidelity between $\\rho_{AB}$ and $\\sigma_{AB}$ equals the fidelity between\ntheir reduced states $\\rho_A$ and $\\sigma_A$. In this work, we generalize\nUhlmann's theorem to $\\alpha$-R\\'enyi relative entropies for $\\alpha \\in\n[\\frac{1}{2},\\infty]$, a family of divergences that encompasses fidelity,\nrelative entropy, and max-relative entropy corresponding to\n$\\alpha=\\frac{1}{2}$, $\\alpha=1$, and $\\alpha=\\infty$, respectively.",
        "In this paper, we study the partition function $\\overline{R_\\ell^\\ast}(n)$,\nwhich counts the number of overpartitions of $n$ where the non-overlined parts\nare $\\ell$-regular. Using the theory of modular forms, we establish various\ncongruences and infinite families for this function when $\\ell = 6,8$.\nAdditionally, we present some intriguing conjectures.",
        "Molecular chirality is a key concept in chemistry with implications for the\norigin of life and the manufacturing of pharmaceuticals. Previous simulations\nof a chiral molecular model with an energetic bias towards homochiral\ninteractions show a spontaneous symmetry-breaking transition from a\nsupercritical racemic liquid into a subcritical liquid enriched in one of the\ntwo enantiomers. Here, we employ molecular dynamics simulations in order to\ntest the possible existence of this phenomenon in hydrogen peroxide, the\nsmallest chiral molecule. For this purpose, we study the fluid phase of this\nsubstance between 100 K and 1500 K, and from $10^{-4}$ GPa to 1 GPa. We find a\nglass transition and we suggest that hydrogen bonds play a central role in such\nbehavior. We also test the possibility of observing chiral symmetry breaking by\nperforming both constant temperature and cooling simulations at multiple\npressures, and we do not observe the phenomenon. An analysis of the structure\nof the liquid shows negligible differences between homochiral and heterochiral\ninteractions, supporting the difficulty in observing chiral symmetry breaking.\nIf hydrogen peroxide manifests spontaneous chiral symmetry breaking, it likely\ntakes place significantly below room temperature and is hidden by other\nphenomena, such as the glass transition or crystallization. More broadly, our\nresults, and recent experimental observations, suggest that greater molecular\ncomplexity is needed for spontaneous chiral symmetry breaking in the liquid\nphase to occur.",
        "We consider $L$-directional associative memories, composed of $L$ Hopfield\nnetworks, displaying imitative Hebbian intra-network interactions and\nanti-imitative Hebbian inter-network interactions, where couplings are built\nover a set of hidden binary patterns. We evaluate the model's performance in\nreconstructing the whole set of hidden binary patterns when provided with\nmixtures of noisy versions of these patterns. Our numerical results demonstrate\nthe model's high effectiveness in the reconstruction task for structureless and\nstructured datasets.",
        "Sparse variational Gaussian processes (GPs) construct tractable posterior\napproximations to GP models. At the core of these methods is the assumption\nthat the true posterior distribution over training function values ${\\bf f}$\nand inducing variables ${\\bf u}$ is approximated by a variational distribution\nthat incorporates the conditional GP prior $p({\\bf f} | {\\bf u})$ in its\nfactorization. While this assumption is considered as fundamental, we show that\nfor model training we can relax it through the use of a more general\nvariational distribution $q({\\bf f} | {\\bf u})$ that depends on $N$ extra\nparameters, where $N$ is the number of training examples. In GP regression, we\ncan analytically optimize the evidence lower bound over the extra parameters\nand express a tractable collapsed bound that is tighter than the previous\nbound. The new bound is also amenable to stochastic optimization and its\nimplementation requires minor modifications to existing sparse GP code.\nFurther, we also describe extensions to non-Gaussian likelihoods. On several\ndatasets we demonstrate that our method can reduce bias when learning the\nhyperpaparameters and can lead to better predictive performance.",
        "Describing chemical reactions in solution on a molecular level is a\nchallenging task due to the high mobility of weakly interacting solvent\nmolecules which requires configurational sampling. For instance, polar and\nprotic solvents can interact strongly with solutes and may interfere in\nreactions. However, to define and identify representative arrangements of\nsolvent molecules modulating a transition state is a non-trivial task. Here, we\npropose to monitor their active participation in the decaying normal mode at a\ntransition state, which defines active solvent molecules. Moreover, it is\ndesirable to prepare a low-dimensional microsolvation model in a well-defined,\nfully automated, high-throughput, and easy-to-deploy fashion, which we propose\nto derive in a stepwise protocol. First, transition state structures are\noptimized in a sufficiently solvated quantum-classical hybrid model, which are\nthen subjected to a re-definition of a then reduced quantum region. From the\nreduced model, minimally microsolvated structures are extracted that contain\nonly active solvent molecules. Modeling the remaining solvation effects is\ndeferred to a continuum model. To establish an easy-to-use free-energy model,\nwe combine the standard thermochemical gas-phase model with a correction for\nthe cavity entropy in solution. We assess our microsolvation and free-energy\nmodels for methanediol formation from formaldehyde, for the hydration of carbon\ndioxide (which we consider in a solvent mixture to demonstrate the versatility\nof our approach), and, finally, for the chlorination of phenol with\nhypochlorous acid.",
        "This paper studies the observability inequalities for the Schr\\\"{o}dinger\nequation associated with an anharmonic oscillator $H=-\\frac{\\d^2}{\\d x^2}+|x|$.\nWe build up the observability inequality over an arbitrarily short time\ninterval $(0,T)$, with an explicit expression for the observation constant\n$C_{obs}$ in terms of $T$, for some observable set that has a different\ngeometric structure compared to those discussed in \\cite{HWW}. We obtain the\nsufficient conditions and the necessary conditions for observable sets,\nrespectively. We also present counterexamples to demonstrate that half-lines\nare not observable sets, highlighting a major difference in the geometric\nproperties of observable sets compared to those of Schr\\\"{o}dinger operators\n$H=-\\frac{\\d^2}{\\d x^2}+|x|^{2m}$ with $m\\ge 1$.\n  Our approach is based on the following ingredients: First, the use of an\nIngham-type spectral inequality constructed in this paper; second, the\nadaptation of a quantitative unique compactness argument, inspired by the work\nof Bourgain-Burq-Zworski \\cite{Bour13}; third, the application of the\nSzeg\\\"{o}'s limit theorem from the theory of Toeplitz matrices, which provides\na new mathematical tool for proving counterexamples of observability\ninequalities.",
        "We use a type theory for omega-categories to produce higher-dimensional\ngeneralisations of the Eckmann-Hilton argument. The heart of our construction\nis a family of padding and repadding techniques, which give a notion of\ncongruence between cells of different types. This gives explicit witnesses in\nall dimensions that, for cells with degenerate boundary, all composition\noperations are congruent and commutative. Our work has been implemented,\nallowing us to explicitly compute these witnesses, and we show these grow\nrapidly in complexity as the parameters are varied. Our results can also be\nexported as elements of identity types in Martin-Lof type theory, and hence are\nof relevance in homotopy type theory.",
        "This study investigates the potential of a pre-trained visual transformer\n(VT) model, specifically the Swin Transformer V2 (SwinV2), to classify\nphotometric light curves without the need for feature extraction or multi-band\npreprocessing. The goal is to assess whether this image-based approach can\naccurately differentiate astronomical phenomena and serve as a viable option\nfor working with multi-band photometric light curves. We transformed each\nmulti-band light curve into an image. These images serve as input to the SwinV2\nmodel, which is pre-trained on ImageNet-21K. The datasets employed include the\npublic Catalog of Variable Stars from the Massive Compact Halo Object (MACHO)\nsurvey, using both one and two bands, and the first round of the recent\nExtended LSST Astronomical Time-Series Classification Challenge (ELAsTiCC),\nwhich includes six bands. The performance of the model was evaluated on six\nclasses for the MACHO dataset and 20 distinct classes of variable stars and\ntransient events for the ELAsTiCC dataset. The fine-tuned SwinV2 achieved\nbetter performance than models specifically designed for light curves, such as\nAstromer and the Astronomical Transformer for Time Series and Tabular Data\n(ATAT). When trained on the full MACHO dataset, it attained a macro F1-score of\n80.2 and outperformed Astromer in single-band experiments. Incorporating a\nsecond band further improved performance, increasing the F1-score to 84.1. In\nthe ELAsTiCC dataset, SwinV2 achieved a macro F1-score of 65.5, slightly\nsurpassing ATAT by 1.3.",
        "Listeners adapt language comprehension based on their mental representations\nof speakers, but how these representations are dynamically updated remains\nunclear. We investigated whether listeners probabilistically adapt their\ncomprehension based on the likelihood of speakers producing\nstereotype-incongruent utterances. Our findings reveal two potential\nmechanisms: a speaker-general mechanism that adjusts overall expectations about\nspeaker-content relationships, and a speaker-specific mechanism that updates\nindividual speaker models. In two EEG experiments, participants heard speakers\nmake stereotype-congruent or incongruent utterances, with incongruency base\nrate manipulated between blocks. In Experiment 1, speaker incongruency\nmodulated both high-beta (21-30 Hz) and theta (4-6 Hz) oscillations:\nincongruent utterances decreased oscillatory power in low base rate condition\nbut increased it in high base rate condition. The theta effect varied with\nlisteners' openness trait: less open participants showed theta increases to\nspeaker-incongruencies, suggesting maintenance of speaker-specific information,\nwhile more open participants showed theta decreases, indicating flexible model\nupdating. In Experiment 2, we dissociated base rate from the target speaker by\nmanipulating the overall base rate using an alternative non-target speaker.\nOnly the high-beta effect persisted, showing power decrease for\nspeaker-incongruencies in low base rate condition but no effect in high base\nrate condition. The high-beta oscillations might reflect the speaker-general\nadjustment, while theta oscillations may index the speaker-specific model\nupdating. These findings provide evidence for how language processing is shaped\nby social cognition in real time.",
        "Space-time methods promise more efficient time-domain simulations, in\nparticular of electrical machines. However, most approaches require the motion\nto be known in advance so that it can be included in the space-time mesh. To\novercome this problem, this paper proposes to use the well-known air-gap\nelement for the rotor-stator coupling of an isogeometric machine model. First,\nwe derive the solution in the air-gap region and then employ it to couple the\nrotor and stator. This coupling is angle dependent and we show how to\nefficiently update the coupling matrices to a different angle, avoiding\nexpensive quadrature. Finally, the resulting time-dependent problem is solved\nin a space-time setting. The spatial discretization using isogeometric analysis\nis particularly suitable for coupling via the air-gap element, as NURBS can\nexactly represent the geometry of the air-gap. Furthermore, the model including\nthe air-gap element can be seamlessly transferred to the space-time setting.\nHowever, the air-gap element is well known in the literature. The originality\nof this work is the application to isogeometric analysis and space-time."
      ]
    }
  },
  {
    "id":2411.18784,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Multimodal Breast Parenchymal Patterns Correlation Using a Patient-Specific Biomechanical Model",
    "start_abstract":"In this paper, we aim to produce a realistic 2-D projection of the breast parenchymal distribution from a 3-D breast magnetic resonance image (MRI). To evaluate the accuracy of our simulation, we compare our results with the local breast density (i.e., density map) obtained from the complementary full-field digital mammogram. To achieve this goal, we have developed a fully automatic framework, which registers MRI volumes to X-ray mammograms using a subject-specific biomechanical model of the breast. The optimization step modifies the position, orientation, and elastic parameters of the breast model to perform the alignment between the images. When the model reaches an optimal solution, the MRI glandular tissue is projected and compared with the one obtained from the corresponding mammograms. To reduce the loss of information during the ray-casting, we introduce a new approach that avoids resampling the MRI volume. In the results, we focus our efforts on evaluating the agreement of the distributions of glandular tissue, the degree of structural similarity, and the correlation between the real and synthetic density maps. Our approach obtained a high-structural agreement regardless the glandularity of the breast, whilst the similarity of the glandular tissue distributions and correlation between both images increase in denser breasts. Furthermore, the synthetic images show continuity with respect to large structures in the density maps.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Signs of Non-Monotonic Finite-Volume Corrections to $g_A$",
        "Taming Knowledge Conflicts in Language Models",
        "Dual-Polarized Intelligent Omni-Surfaces for Independent\n  Reflective-Refractive Transmission",
        "Turbulent fragmentation as the primary driver of core formation in\n  Polaris Flare and Lupus I",
        "An Unconventional Ultra-Sub-Wavelength Receiving Nano-Antenna Activated\n  by ac Spin Pumping and the ac Inverse Spin Hall Effect",
        "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation\n  approach",
        "CoCoI: Distributed Coded Inference System for Straggler Mitigation",
        "When is Cat(Q) cartesian closed?",
        "Text Data Analysis of Maternal Narratives: Albanian Women in Italy",
        "Going deeper into the dark with COSMOS-Web: JWST unveils the total\n  contribution of Radio-Selected NIRfaint galaxies to the cosmic Star Formation\n  Rate Density",
        "Fusion systems related to polynomial representations of\n  $\\mathrm{SL}_2(q)$",
        "Glacier data assimilation on an Arctic glacier: Learning from large\n  ensemble twin experiments",
        "NL2SQL-BUGs: A Benchmark for Detecting Semantic Errors in NL2SQL\n  Translation",
        "Asymmetric results about graph homomorphisms",
        "Bridging Information Gaps with Comprehensive Answers: Improving the\n  Diversity and Informativeness of Follow-Up Questions",
        "Iterative Feature Space Optimization through Incremental Adaptive\n  Evaluation",
        "Large Language Models for Healthcare Text Classification: A Systematic\n  Review",
        "On Qualitative Preference in Alternating-time Temporal Logic with\n  Strategy Contexts",
        "On the local analyticity for the Euler equations",
        "BYOS: Knowledge-driven Large Language Models Bring Your Own Operating\n  System More Excellent",
        "Complexity and Algorithm for the Matching vertex-cutset Problem",
        "Bigger Isn't Always Better: Towards a General Prior for Medical Image\n  Reconstruction",
        "Higher Weil-Petersson volumes of the moduli space of super Riemann\n  surfaces",
        "Gravitational Waves from Resonant Transitions of Tidally Perturbed\n  Gravitational Atoms",
        "Semi-classical limit of the massive Klein-Gordon-Maxwell system toward\n  the relativistic Euler-Maxwell system via an adapted modulated energy method",
        "Distilling heterogeneous treatment effects: Stable subgroup estimation\n  in causal inference",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "Back to business: SLX 1746--331 after 13 years of silence",
        "MGSR: 2D\/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface\n  Reconstruction under Various Light Conditions"
      ],
      "abstract":[
        "We study finite-volume (FV) corrections to determinations of $g_A$ via\nlattice quantum chromodynamics (QCD) using analytic results and numerical\nanalysis. We observe that $SU(2)$ Heavy Baryon Chiral Perturbation Theory does\nnot provide an unambiguous prediction for the sign of the FV correction, which\nis not surprising when one also considers large-$N_c$ constraints on the axial\ncouplings. We further show that non-monotonic FV corrections are naturally\nallowed when one considers either including explicit $\\Delta$-resonance degrees\nof freedom or one works to higher orders in the chiral expansion. We\ninvestigate the potential impact of these FV corrections with a precision study\nof $g_A$ using models of FV corrections that are monotonic and non-monotonic.\nUsing lattice QCD data that is approximately at the 1% level of precision, we\ndo not see significant evidence of non-monotonic corrections. Looking forward\nto the next phase of lattice QCD calculations, we estimate that calculations\nthat are between the 0.1%-1%-level of precision may be sensitive to these FV\nartifacts. Finally, we present an update of the CalLat prediction of $g_A$ in\nthe isospin limit with sub-percent precision, $g_A^{\\rm QCD} = 1.2674(96)$.",
        "Language Models (LMs) often encounter knowledge conflicts when parametric\nmemory contradicts contextual knowledge. Previous works attribute this conflict\nto the interplay between \"memory heads\" and \"context heads\", attention heads\nassumed to promote either memory or context exclusively. In this study, we go\nbeyond this fundamental assumption by uncovering a critical phenomenon we term\nthe \"superposition of contextual information and parametric memory\", where\nhighly influential attention heads could simultaneously contribute to both\nmemory and context. Building upon this insight, we propose Just Run Twice\n(JUICE), a test-time attention intervention method that steers LMs toward\neither parametric beliefs or contextual knowledge without requiring\nfine-tuning. JUICE identifies a set of reliable attention heads and leverages a\ndual-run approach to mitigate the superposition effects. Extensive experiments\nacross 11 datasets and 6 model architectures demonstrate that JUICE sets the\nnew state-of-the-art performance and robust generalization, achieving\nsignificant and consistent improvement across different domains under various\nconflict types. Finally, we theoretically analyze knowledge conflict and the\nsuperposition of contextual information and parametric memory in attention\nheads, which further elucidates the effectiveness of JUICE in these settings.",
        "Intelligent omni-surface (IOS), which are capable of providing service\ncoverage to mobile users (MUs) in a reflective and a refractive manner, has\nrecently attracted widespread attention. However, the performance of\ntraditionally IOS-aid systems is limited by the intimate coupling between the\nrefraction and reflection behavior of IOS elements. In this letter, to overcome\nthis challenge, we introduce the concept of dual-polarized IOS-assisted\ncommunication. More precisely, by employing the polarization domain in the\ndesign of IOS, full independent refraction and reflection modes can be\ndelivered. We consider a downlink dual-polarized IOS-aided system, while also\naccounting for the leakage between different polarizations. To maximize the sum\nrate, we formulate a joint IOS phase shift and BS beamforming problem and\nproposed an iterative algorithm to solve the non-convex program. Simulation\nresults validate that dual-polarized IOS significantly enhances the performance\nthan the traditional one.",
        "Stars form from dense cores in turbulent molecular clouds. According to the\nstandard scenario of star formation, dense cores are created by cloud\nfragmentation. However, the physical mechanisms driving this process are still\nnot fully understood from an observational standpoint. However, the physical\nmechanisms driving this process are still not fully understood from an\nobservational standpoint. Our goal is to investigate the process of cloud\nfragmentation using observational data from nearby clouds. Specifically, we aim\nto examine the role of self-gravity and turbulence, both of which are key to\nthe dynamical evolution of clouds. We applied astrodendro to the Herschel H2\ncolumn density maps to identify dense cores and determine their mass and\nseparation in two nearby low-mass clouds: the Polaris Flare and Lupus I clouds.\nWe then compared the observed core masses and separations with predictions from\nmodels of gravitational and turbulent fragmentation. For turbulent\nfragmentation, the key scales are the cloud sonic scale and its corresponding\nmass. The average core masses are estimated to be 0.242 Msun for Lupus I and\n0.276 Msun for the Polaris Flare. The core separations peak at 0.1 - 0.2 pc in\nboth clouds. These separations are significantly smaller than the Jeans length\nbut agree well with the cloud sonic scale. Additionally, the density\nprobability distribution functions of the dense cores follow log-normal\ndistributions, which is consistent with the predictions of turbulent\nfragmentation. These findings suggest that the primary process driving core\nformation in the observed low-mass star-forming regions is not gravitational\nfragmentation but rather turbulent fragmentation. We found no evidence that\nfilament fragmentation plays a significant role in the formation of dense\ncores.",
        "We report an extreme sub-wavelength unconventional receiving antenna. It\nconsists of an array of nanomagnets connected to heavy metal nanostrips.\nIncident electromagnetic (EM) radiation generates intrinsic and extrinsic spin\nwaves in the nanomagnets, which pump spin into the heavy metal nanostrips at\ntheir own frequencies giving rise to a polychromatic alternating voltage across\nthe latter owing to the ac inverse spin Hall effect. This implements a\nreceiving nano-antenna. We demonstrate its operation at two different EM wave\nfrequencies of 1.5 GHz and 2.4 GHz - the latter being the Bluetooth and Wi-Fi\nfrequency. We measure the receiving gain at 2.4 GHz to be approximately -9 db.\nThe free space radiated wavelength \"lambda\" at 2.4 GHz is 12.5 cm while the\nantenna area A is merely 160 micron^2, making the ratio A\/lambda^2 =\n0.97x10^-8. This antenna's receiving gain should be very poor because of the\ntiny size. Yet the measured gain is more than 4000 times larger than the\ntheoretical limit for a conventional antenna of this size at this wavelength\nbecause of the unconventional operating principle.",
        "A common characteristic in integer linear programs (ILPs) is symmetry,\nallowing variables to be permuted without altering the underlying problem\nstructure. Recently, GNNs have emerged as a promising approach for solving\nILPs. However, a significant challenge arises when applying GNNs to ILPs with\nsymmetry: classic GNN architectures struggle to differentiate between symmetric\nvariables, which limits their predictive accuracy. In this work, we investigate\nthe properties of permutation equivariance and invariance in GNNs, particularly\nin relation to the inherent symmetry of ILP formulations. We reveal that the\ninteraction between these two factors contributes to the difficulty of\ndistinguishing between symmetric variables. To address this challenge, we\nexplore the potential of feature augmentation and propose several guiding\nprinciples for constructing augmented features. Building on these principles,\nwe develop an orbit-based augmentation scheme that first groups symmetric\nvariables and then samples augmented features for each group from a discrete\nuniform distribution. Empirical results demonstrate that our proposed approach\nsignificantly enhances both training efficiency and predictive performance.",
        "Convolutional neural networks (CNNs) are widely applied in real-time\napplications on resource-constrained devices. To accelerate CNN inference,\nprior works proposed to distribute the inference workload across multiple\ndevices. However, they did not address stragglers and device failures in\ndistributed inference, which is challenging due to the devices' time-varying\nand possibly unknown computation\/communication capacities. To address this, we\npropose a distributed coded inference system, called CoCoI. It splits the\nconvolutional layers of CNN, considering the data dependency of\nhigh-dimensional inputs and outputs, and then adapts coding schemes to generate\ntask redundancy. With CoCoI, the inference results can be determined once a\nsubset of devices complete their subtasks, improving robustness against\nstragglers and failures. To theoretically analyze the tradeoff between\nredundancy and subtask workload, we formulate an optimal splitting problem to\nminimize the expected inference latency. Despite its non-convexity, we\ndetermine an approximate strategy with minor errors, and prove that CoCoI\noutperforms uncoded benchmarks. For performance evaluation, we build a testbed\nwith Raspberry Pi 4Bs. The experimental results show that the approximate\nstrategy closely matches the optimal solution. When compared with uncoded\nbenchmarks, CoCoI reduces inference latency by up to 34.2% in the presence of\nstragglers and device failures.",
        "An example in the paper \"Exponentiable functors between quantaloid-enriched\ncategories\" is mistaken: it is not true that the category of categories and\nfunctors enriched in any free quantaloid is cartesian closed. To correct this,\nwe give here an elementary characterization of those quantaloids Q for which\nCat(Q) is cartesian closed. With this characterization, we unify several known\ncases (previously proven using ad hoc methods) and we give some new examples.",
        "Despite growing interest in migration studies, research on motherhood among\nmigrant women in Italy remains limited. This study contributes to the\nliterature by examining the family trajectories of Albanian women in Italy,\nexploring how their migration patterns and experiences have shaped these life\naspects. We conducted a comprehensive textual analysis to find the main topics\nof 30 semi-structured interviews with Albanian mothers living in Milan, Rome,\nand Bari. After pre-processing the text, we performed an exploratory analysis\nto identify key features and explore word relationships. The predominant\ndimensions that emerged relate to family management, work paths and schedules,\nand strategies and concerns arising from the trade-off between work and\nchildcare. Subsequently, we stratified the sample by entry channel into Italy\n(study and work, reunification, and irregular channel) and applied Latent\nDirichlet Allocation to model each sub-corpus as a mixture of topics. Our\nresults resonate with existing literature [1] on the key role of female\nmigratory patterns in shaping post-migration fertility. Interviewees who\nentered Italy through various migratory channels not only differ in their\ncharacteristics and migration experiences but also exhibit dissimilar fertility\ndesires and behaviors, motherhood trajectories, and conceptions of their role\nas mothers and family ideals. These differences influence their priorities and\nlevel of commitment to family and work obligations.",
        "We present the first follow-up with JWST of radio-selected NIRfaint galaxies\nas part of the COSMOS-Web survey. By selecting galaxies detected at radio\nfrequencies ($S_{\\rm 3 GHz}>11.5$ $\\mu$Jy; i.e. S\/N$>5$) and with faint\ncounterparts at NIR wavelengths (F150W$>26.1$ mag), we collect a sample of 127\nlikely dusty star-forming galaxies (DSFGs). We estimate their physical\nproperties through SED fitting, compute the first radio luminosity function for\nthese types of sources, and their contribution to the total cosmic star\nformation rate density. Our analysis confirms that these sources represent a\npopulation of highly dust-obscured ($\\langle A_{\\rm v} \\rangle \\sim3.5$ mag),\nmassive ($\\langle M_\\star \\rangle \\sim10^{10.8}$ M$_\\odot$) and star-forming\ngalaxies ($\\langle {\\rm SFR} \\rangle\\sim300$ M$_\\odot$ yr$^{-1}$) located at\n$\\langle z \\rangle\\sim3.6$, representing the high-redshift tail of the full\ndistribution of radio sources. Our results also indicate that these galaxies\ncould dominate the bright end of the radio luminosity function and reach a\ntotal contribution to the cosmic star formation rate density equal to that\nestimated only considering NIR-bright sources at $z\\sim4.5$. Finally, our\nanalysis further confirms that the radio selection can be employed to collect\nstatistically significant samples of DSFGs, representing a complementary\nalternative to the other selections based on JWST colors or detection at\nFIR\/(sub)mm wavelengths.",
        "Let $q$ be a power of a fixed prime $p$. We classify up to isomorphism all\nsimple saturated fusion systems on a certain class of $p$-groups constructed\nfrom the polynomial representations of $\\mathrm{SL}_2(q)$, which includes the\nSylow $p$-subgroups of $\\mathrm{GL}_3(q)$ and $\\mathrm{Sp}_4(q)$ as special\ncases. The resulting list includes all Clelland--Parker fusion systems, a\nsimple exotic fusion system discovered by Henke--Shpectorov, and a new infinite\nfamily of exotic examples.",
        "Glacier modeling is crucial for quantifying the evolution of cryospheric\nprocesses. At the same time, uncertainties hamper process understanding and\npredictive accuracy. Here, we suggest improving glacier mass balance\nsimulations for the Kongsvegen glacier in Svalbard through the application of\nBayesian data assimilation techniques in a set of large ensemble twin\nexperiments. Noisy synthetic observations of albedo and snow depth, generated\nusing the multilayer CryoGrid community model with a full energy balance, are\nassimilated using two ensemble-based data assimilation schemes: the particle\nbatch smoother and the ensemble smoother. A comprehensive evaluation exercise\ndemonstrates that the joint assimilation of albedo and snow depth improves the\nsimulation skill by up to 86% relative to the prior in specific glacier\nregions. The particle batch smoother excels in representing albedo dynamics,\nwhile the ensemble smoother is particularly effective for snow depth under low\nsnowfall conditions. By combining the strengths of both observations, the joint\nassimilation achieves improved mass balance simulations across different\nglacier zones using either assimilation scheme. This work underscores the\npotential of ensemble-based data assimilation methods for refining glacier\nmodels by offering a robust framework to enhance predictive accuracy and reduce\nuncertainties in cryospheric simulations. Further advances in glacier data\nassimilation will be critical to better understanding the fate and role of\nArctic glaciers in a changing climate.",
        "Natural Language to SQL (i.e., NL2SQL) translation is crucial for\ndemocratizing database access, but even state-of-the-art models frequently\ngenerate semantically incorrect SQL queries, hindering the widespread adoption\nof these techniques by database vendors. While existing NL2SQL benchmarks\nprimarily focus on correct query translation, we argue that a benchmark\ndedicated to identifying common errors in NL2SQL translations is equally\nimportant, as accurately detecting these errors is a prerequisite for any\nsubsequent correction-whether performed by humans or models. To address this\ngap, we propose NL2SQL-BUGs, the first benchmark dedicated to detecting and\ncategorizing semantic errors in NL2SQL translation. NL2SQL-BUGs adopts a\ntwo-level taxonomy to systematically classify semantic errors, covering 9 main\ncategories and 31 subcategories. The benchmark consists of 2018\nexpert-annotated instances, each containing a natural language query, database\nschema, and SQL query, with detailed error annotations for semantically\nincorrect queries. Through comprehensive experiments, we demonstrate that\ncurrent large language models exhibit significant limitations in semantic error\ndetection, achieving an average detection accuracy of only 75.16%. Despite\nthis, the models were able to successfully detect 106 errors (accounting for\n6.91%) in the widely-used NL2SQL dataset, BIRD, which were previously\nannotation errors in the benchmark. This highlights the importance of semantic\nerror detection in NL2SQL systems.",
        "Many important results in extremal graph theory can be roughly summarised as\n\"if a triangle-free graph $G$ has certain properties, then it has a\nhomomorphism to a triangle-free graph $\\Gamma$ of bounded size\". For example,\nbounds on homomorphism thresholds give such a statement if $G$ has sufficiently\nhigh minimum degree, and the approximate homomorphism theorem gives such a\nstatement for all $G$, if one weakens the notion of homomorphism appropriately.\n  In this paper, we study asymmetric versions of these results, where the\nassumptions on $G$ and $\\Gamma$ need not match. For example, we prove that if\n$G$ is a graph with odd girth at least $9$ and minimum degree at least $\\delta\n|G|$, then $G$ is homomorphic to a triangle-free graph whose size depends only\non $\\delta$. Moreover, the odd girth assumption can be weakened to odd girth at\nleast $7$ if $G$ has bounded VC dimension or bounded domination number. This\ngives a new and improved proof of a result of Huang et al.\n  We also prove that in the asymmetric approximate homomorphism theorem, the\nbounds exhibit a rather surprising ``double phase transition'': the bounds are\nsuper-exponential if $G$ is only assumed to be triangle-free, they become\nexponential if $G$ is assumed to have odd girth $7$ or $9$, and become linear\nif $G$ has odd girth at least $11$.\n  Our proofs use a wide variety of techniques, including entropy arguments, the\nFrieze--Kannan weak regularity lemma, properties of the generalised Mycielskian\nconstruction, and recent work on abundance and the asymmetric removal lemma.",
        "Effective conversational systems are expected to dynamically generate\ncontextual follow-up questions to elicit new information while maintaining the\nconversation flow. While humans excel at asking diverse and informative\nquestions by intuitively assessing both obtained and missing information,\nexisting models often fall short of human performance on this task. To mitigate\nthis, we propose a method that generates diverse and informative questions\nbased on targeting unanswered information using a hypothetical LLM-generated\n\"comprehensive answer\". Our method is applied to augment an existing follow-up\nquestions dataset. The experimental results demonstrate that language models\nfine-tuned on the augmented datasets produce follow-up questions of\nsignificantly higher quality and diversity. This promising approach could be\neffectively adopted to future work to augment information-seeking dialogues for\nreducing ambiguities and improving the accuracy of LLM answers.",
        "Iterative feature space optimization involves systematically evaluating and\nadjusting the feature space to improve downstream task performance. However,\nexisting works suffer from three key limitations:1) overlooking differences\namong data samples leads to evaluation bias; 2) tailoring feature spaces to\nspecific machine learning models results in overfitting and poor\ngeneralization; 3) requiring the evaluator to be retrained from scratch during\neach optimization iteration significantly reduces the overall efficiency of the\noptimization process. To bridge these gaps, we propose a gEneralized Adaptive\nfeature Space Evaluator (EASE) to efficiently produce optimal and generalized\nfeature spaces. This framework consists of two key components: Feature-Sample\nSubspace Generator and Contextual Attention Evaluator. The first component aims\nto decouple the information distribution within the feature space to mitigate\nevaluation bias. To achieve this, we first identify features most relevant to\nprediction tasks and samples most challenging for evaluation based on feedback\nfrom the subsequent evaluator. This decoupling strategy makes the evaluator\nconsistently target the most challenging aspects of the feature space. The\nsecond component intends to incrementally capture evolving patterns of the\nfeature space for efficient evaluation. We propose a weighted-sharing\nmulti-head attention mechanism to encode key characteristics of the feature\nspace into an embedding vector for evaluation. Moreover, the evaluator is\nupdated incrementally, retaining prior evaluation knowledge while incorporating\nnew insights, as consecutive feature spaces during the optimization process\nshare partial information. Extensive experiments on fourteen real-world\ndatasets demonstrate the effectiveness of the proposed framework. Our code and\ndata are publicly available.",
        "Large Language Models (LLMs) have fundamentally transformed approaches to\nNatural Language Processing (NLP) tasks across diverse domains. In healthcare,\naccurate and cost-efficient text classification is crucial, whether for\nclinical notes analysis, diagnosis coding, or any other task, and LLMs present\npromising potential. Text classification has always faced multiple challenges,\nincluding manual annotation for training, handling imbalanced data, and\ndeveloping scalable approaches. With healthcare, additional challenges are\nadded, particularly the critical need to preserve patients' data privacy and\nthe complexity of the medical terminology. Numerous studies have been conducted\nto leverage LLMs for automated healthcare text classification and contrast the\nresults with existing machine learning-based methods where embedding,\nannotation, and training are traditionally required. Existing systematic\nreviews about LLMs either do not specialize in text classification or do not\nfocus on the healthcare domain. This research synthesizes and critically\nevaluates the current evidence found in the literature regarding the use of\nLLMs for text classification in a healthcare setting. Major databases (e.g.,\nGoogle Scholar, Scopus, PubMed, Science Direct) and other resources were\nqueried, which focused on the papers published between 2018 and 2024 within the\nframework of PRISMA guidelines, which resulted in 65 eligible research\narticles. These were categorized by text classification type (e.g., binary\nclassification, multi-label classification), application (e.g., clinical\ndecision support, public health and opinion analysis), methodology, type of\nhealthcare text, and metrics used for evaluation and validation. This review\nreveals the existing gaps in the literature and suggests future research lines\nthat can be investigated and explored.",
        "We show how to add and eliminate binary preference on plays in\nAlternating-time Temporal Logic (ATL) with strategy contexts on Concurrent Game\nModels (CGMs) by means of a translation which preserves satisfaction in models\nwhere preference-indiscernibility between plays is an equivalence relation of\nfinite index. The elimination technique also works for a companion second-order\npath quantifier, which makes quantified path variables range over sets of plays\nthat are closed under preference-indiscernibility. We argue that the preference\noperator and the specialized quantifier facilitate formulating interesting\nsolution concepts such as Nash equilibrium and secure equilibrium in a\nstraightforward way. We also present a novel translation from ATL with strategy\ncontexts to Quantified Computation Tree Logic (QCTL). Together with the\ntranslation which eliminates preference and the specialized form of\nquantification, this translation allows reasoning about infinite multiplayer\nsynchronous games on CGMs to be translated from the proposed extension of ATL\nwith strategy contexts into QCTL. The setting is related to that of ordered\nobjectives in the works of Bouyer, Brenguier, Markey and Ummels, except that\nour focus is on the use of the temporal logic languages mentioned above, and we\nrely on translations into QCTL for the algorithmic solutions.",
        "In this paper, we study the existence and uniqueness of solutions to the\nEuler equations with initial conditions that exhibit analytic regularity near\nthe boundary and Sobolev regularity away from it. A key contribution of this\nwork is the introduction of the diamond-analyticity framework, which captures\nthe spatial decay of the analyticity radius in a structured manner, improving\nupon uniform analyticity approaches. We employ the Leray projection and a\nnonstandard mollification technique to demonstrate that the quotient between\nthe imaginary and real parts of the analyticity radius remains unrestricted,\nthus extending the analyticity persistence results beyond traditional\nconstraints. Our methodology combines analytic-Sobolev estimates with an\niterative scheme which is nonstandard in the Cauchy-Kowalevskaya framework,\nensuring rigorous control over the evolution of the solution. These results\ncontribute to a deeper understanding of the interplay between analyticity and\nboundary effects in fluid equations. They might have implications for the study\nof the inviscid limit of the Navier-Stokes equations and the role of complex\nsingularities in fluid dynamics.",
        "Kernel configurations play an important role in the performance of Operating\nSystem (OS). However, with the rapid iteration of OS, finding the proper\nconfigurations that meet specific requirements can be challenging, which can be\nprimarily attributed to the default kernel provided by vendors does not take\nthe requirements of specific workloads into account, and the heavyweight tuning\nprocess cannot catch up with the rapid evolving pace of the kernel. To address\nthese challenges, we propose BYOS, a novel framework powered by Large Language\nModels (LLMs) to customize kernel configurations for diverse user requirements.\nBy integrating OS-oriented Dual-layer Knowledge Graph (OD-KG) and corresponding\nreasoning strategy, BYOS enhanced the LLM's understanding of the\ncharacteristics and capabilities of OS, thus enabling customized,\ncost-effective, and convenient generation of kernel configurations. Experiments\nshow that the kernels configured by BYOS outperform the default\nvendor-configured kernels by 7.1% to 155.4%, demonstrating the effectiveness\nand efficiency of BYOS in customizing kernel configurations. Our code is\navailable at https:\/\/github.com\/LHY-24\/BYOS.",
        "In 1985, Chv\\'{a}tal introduced the concept of star cutsets as a means to\ninvestigate the properties of perfect graphs, which inspired many researchers\nto study cutsets with some specific structures, for example, star cutsets,\nclique cutsets, stable cutsets. In recent years, approximation algorithms have\ndeveloped rapidly, the computational complexity associated with determining the\nminimum vertex cut possessing a particular structural property have attracted\nconsiderable academic attention.\n  In this paper, we demonstrate that determining whether there is a matching\nvertex-cutset in $H$ with size at most $k$, is $\\mathbf{NP}$-complete, where\n$k$ is a given positive integer and $H$ is a connected graph. Furthermore, we\ndemonstrate that for a connected graph $H$, there exists a $2$-approximation\nalgorithm in $O(nm^2)$ for us to find a minimum matching vertex-cutset.\nFinally, we show that every plane graph $H$ satisfying $H\\not\\in\\{K_2, K_4\\}$\ncontains a matching vertex-cutset with size at most three, and this bound is\ntight.",
        "Diffusion model have been successfully applied to many inverse problems,\nincluding MRI and CT reconstruction. Researchers typically re-purpose models\noriginally designed for unconditional sampling without modifications. Using two\ndifferent posterior sampling algorithms, we show empirically that such large\nnetworks are not necessary. Our smallest model, effectively a ResNet, performs\nalmost as good as an attention U-Net on in-distribution reconstruction, while\nbeing significantly more robust towards distribution shifts. Furthermore, we\nintroduce models trained on natural images and demonstrate that they can be\nused in both MRI and CT reconstruction, out-performing model trained on medical\nimages in out-of-distribution cases. As a result of our findings, we strongly\ncaution against simply re-using very large networks and encourage researchers\nto adapt the model complexity to the respective task. Moreover, we argue that a\nkey step towards a general diffusion-based prior is training on natural images.",
        "Inspired by the theory of JT supergravity, Stanford-Witten derived a\nremarkable recursion formula of Weil-Petersson volumes of moduli space of super\nRiemann surfaces. It is the super version of the celebrated Mirzakhani's\nrecursion formula. In this paper, we generalize Stanford-Witten's formula to\ninclude high degree kappa classes.",
        "Light bosons can form gravitational atoms (GA) around spinning black holes\nthrough the superradiance process. Considering the black hole to be part of a\nbinary system,the tidal potential of the companion periodically perturbs the GA\nsuch that an atomic transition occurs between two of its energy eigenstates.\nThe resonant transition is modeled by the Landau-Zener system,where the orbital\nfrequency of the companion determines the relevant transition. In this work, we\nstudy a novel gravitational wave signal originating directly from the atomic\ntransition of the GA in a binary system. We derive the analytical formulae of\nboth the strain waveform and frequency spectrum of the signal. We further\npresent the GA-binary systems that can have a large signal-to-noise ratio in\nLISA's frequency band. Finally, we discuss the implications of detection of the\nsignal:inferring model parameters,including the boson mass and black hole\nspin,and computing the phase shift and Doppler shift of the gravitational wave\nsignal for equal mass binaries.",
        "We show that the momentum, the density, and the electromagnetic field\nassociated with the massive KleinGordon-Maxwell equations converge in the\nsemi-classical limit towards their respective equivalents associated with the\nrelativistic Euler-Maxwell equations. The proof relies on a modulated\nstress-energy method and a compactness argument. We also give a proof of the\nwell-posedness of the relativistic Euler-Maxwell equations and show how this\nsystem, and so the semi-classical limit of Klein-Gordon-Maxwell, is related to\nthe relativistic massive Vlasov-Maxwell equations.",
        "Recent methodological developments have introduced new black-box approaches\nto better estimate heterogeneous treatment effects; however, these methods fall\nshort of providing interpretable characterizations of the underlying\nindividuals who may be most at risk or benefit most from receiving the\ntreatment, thereby limiting their practical utility. In this work, we introduce\ncausal distillation trees (CDT) to estimate interpretable subgroups. CDT allows\nresearchers to fit any machine learning model to estimate the individual-level\ntreatment effect, and then leverages a simple, second-stage tree-based model to\n\"distill\" the estimated treatment effect into meaningful subgroups. As a\nresult, CDT inherits the improvements in predictive performance from black-box\nmachine learning models while preserving the interpretability of a simple\ndecision tree. We derive theoretical guarantees for the consistency of the\nestimated subgroups using CDT, and introduce stability-driven diagnostics for\nresearchers to evaluate the quality of the estimated subgroups. We illustrate\nour proposed method on a randomized controlled trial of antiretroviral\ntreatment for HIV from the AIDS Clinical Trials Group Study 175 and show that\nCDT out-performs state-of-the-art approaches in constructing stable, clinically\nrelevant subgroups.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "The black hole candidate system SLX 1746--331 was back to business in 2023,\nafter a long silence of roughly 13 years. An outburst was observed thoroughly\nby \\textit{Insight}-HXMT and \\textit{NICER}. The outburst is characterized by\nspectral dominance of the soft state, where the joint \\textit{Insight}-HXMT and\n\\textit{NICER} spectral analysis shows the temperature dependence of the disk\nflux follows $T_{\\rm in}^{3.98}$, and thus suggests that the inner disk reaches\nto ISCO during almost the entire outburst. By assuming 0.3 $L_{\\rm Edd}$ for\nthe peak flux and an inclination angle of zero degrees, the lower limit of the\ncompact object hosted in this system is estimated as 3.28$\\pm 2.14 M_\\odot$. We\nalso look into the relation of the disk temperature and disk flux for a sample\nof black hole systems, and by taking the disk temperature derived in the\noutburst of SLX 1746--331, such a relation results in a mass estimation of $5.2\n\\pm 4.5M_\\odot$. Finally, the spin of the compact object is constrained to\nlarger than 0.8 with a spectral model of kerrbb.",
        "Novel view synthesis (NVS) and surface reconstruction (SR) are essential\ntasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks\nare often addressed independently, with GS-based rendering methods struggling\nunder diverse light conditions and failing to produce accurate surfaces, while\nGS-based reconstruction methods frequently compromise rendering quality. This\nraises a central question: must rendering and reconstruction always involve a\ntrade-off? To address this, we propose MGSR, a 2D\/3D Mutual-boosted Gaussian\nsplatting for Surface Reconstruction that enhances both rendering quality and\n3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS\nand the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,\nproviding precise geometry information to the 3D-GS branch. Leveraging this\ngeometry, the 3D-GS branch employs a geometry-guided illumination decomposition\nmodule that captures reflected and transmitted components, enabling realistic\nrendering under varied light conditions. Using the transmitted component as\nsupervision, the 2D-GS branch also achieves high-fidelity surface\nreconstruction. Throughout the optimization process, the 2D-GS and 3D-GS\nbranches undergo alternating optimization, providing mutual supervision. Prior\nto this, each branch completes an independent warm-up phase, with an early\nstopping strategy implemented to reduce computational costs. We evaluate MGSR\non a diverse set of synthetic and real-world datasets, at both object and scene\nlevels, demonstrating strong performance in rendering and surface\nreconstruction."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Deep Learning for Automatic Gross Tumor Volumes Contouring in Esophageal Cancer Based on Contrast-Enhanced Computed Tomography Images: A Multi-Institutional Study",
    "start_abstract":"Purpose To develop and externally validate an automatic artificial intelligence (AI) tool for delineating gross tumor volume (GTV) in patients with esophageal squamous cell carcinoma (ESCC), which can assist in neo-adjuvant or radical radiation therapy treatment planning. Methods and Materials In this multi-institutional study, contrast-enhanced CT images from 580 eligible ESCC patients were retrospectively collected. The GTV contours delineated by 2 experts via consensus were used as ground truth. A 3-dimensional deep learning model was developed for GTV contouring in the training cohort and internally and externally validated in 3 validation cohorts. The AI tool was compared against 12 board-certified experts in 25 patients randomly selected from the external validation cohort to evaluate its assistance in improving contouring performance and reducing variation. Contouring performance was measured using dice similarity coefficient (DSC) and average surface distance. Additionally, our previously established radiomics model for predicting pathologic complete response was used to compare AI-generated and ground truth contours, to assess the potential of the AI contouring tool in radiomics analysis. Results The AI tool demonstrated good GTV contouring performance in multicenter validation cohorts, with median DSC values of 0.865, 0.876, and 0.866 and median average surface distance values of 0.939, 0.789, and 0.875 mm, respectively. Furthermore, the AI tool significantly improved contouring performance for half of 12 board-certified experts (DSC values, 0.794-0.835 vs 0.856-0.881, P = .003-0.048), reduced the intra- and interobserver variations by 37.4% and 55.2%, respectively, and saved contouring time by 77.6%. In the radiomics analysis, 88.7% of radiomic features from ground truth and AI-generated contours demonstrated stable reproducibility, and similar pathologic complete response prediction performance for these contours (P = .430) was observed. Conclusions Our AI contouring tool can improve GTV contouring performance and facilitate radiomics analysis in ESCC patients, which indicates its potential for GTV contouring during radiation therapy treatment planning and radiomics studies.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "MRI-Guided Adaptive Radiation Therapy"
      ],
      "abstract":[
        "Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "A type-theoretic approach to semistrict higher categories",
        "Theory of spin magnetization driven by chiral phonons",
        "Well-posedness of McKean-Vlasov equations under the weak H\\\"ormander\n  condition",
        "A filtered Hochschild-Kostant-Rosenberg theorem for real Hochschild\n  homology",
        "Spectral Analysis and Stability of Wave Equations with Dispersive\n  Nonlinearity",
        "Dynamic Programming in Ordered Vector Space",
        "Characterization of Markarian 421 during its most violent year:\n  Multiwavelength variability and correlations",
        "Hyperbolic Monopoles, (Semi-)Holomorphic Chern-Simons Theories, and\n  Generalized Chiral Potts Models",
        "FEDS: Feature and Entropy-Based Distillation Strategy for Efficient\n  Learned Image Compression",
        "Selection from Hierarchical Data with Conformal e-values",
        "Generating logical magic states with the aid of non-Abelian topological\n  order",
        "Minimizing Human-Induced Variability in Quantitative Angiography for\n  Robust and Explainable AI-Based Occlusion Prediction",
        "Investigating the shadows of new regular black holes with a Minkowski\n  core: Effects of spherical accretion and core type differences",
        "Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing",
        "A quantitative sampling method for elastic and electromagnetic sources",
        "Turbulence-Induced Fluctuating Interfaces in Heterogeneously-Active\n  Suspensions",
        "On the learning power of Friedman-Stanley jumps",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "A new cutoff criterion for non-negatively curved chains",
        "SynLlama: Generating Synthesizable Molecules and Their Analogs with\n  Large Language Models",
        "Observation of the dimer-singlet phase in the one-dimensional S = 1\/2\n  Heisenberg antiferromagnet Cu(Ampy)ClBr (Ampy= C6H8N2 =\n  2-(Aminomethyl)pyridine)",
        "Interpretable and Equation-Free Response Theory for Complex Systems",
        "A Web-Based Application Leveraging Geospatial Information to Automate\n  On-Farm Trial Design",
        "Genesis of the James Webb Space Telescope Architecture: The Designers'\n  Story",
        "Hypoelliptic Regularization in the Obstacle Problem for the Kolmogorov\n  Operator",
        "Exploring the Reform and Development Pathways of AIIB's Climate\n  Accountability Mechanism in the Context of Global Climate Governance",
        "An integral transformation approach to differential games: a climate\n  model application",
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis"
      ],
      "abstract":[
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Weak $\\infty$-categories are known to be more expressive than their strict\ncounterparts, but are more difficult to work with, as constructions in such a\ncategory involve the manipulation of explicit coherence data. This motivates\nthe search for definitions of semistrict $\\infty$-categories, where some, but\nnot all, of the operations have been strictified.\n  We introduce a general framework for adding definitional equality to the type\ntheory $\\mathsf{Catt}$, a type theory whose models correspond to globular weak\n$\\infty$-categories, which was introduced by Finster and Mimram. Adding\nequality to this theory causes the models to exhibit semistrict behaviour,\ntrivialising some operations while leaving others weak. The framework consists\nof a generalisation of $\\mathsf{Catt}$ extended with an equality relation\ngenerated by an arbitrary set of equality rules $\\mathcal{R}$, which we name\n$\\mathsf{Catt}_{\\mathcal{R}}$. We study this framework in detail, formalising\nmuch of its metatheory in the proof assistant Agda, and studying how certain\noperations of $\\mathsf{Catt}$ behave in the presence of definitional equality.\n  We use this framework to introduce two type theories,\n$\\mathsf{Catt}_{\\mathsf{su}}$ and $\\mathsf{Catt}_{\\mathsf{sua}}$, which are\ninstances of this general framework. Further, we provide terminating and\nconfluent reduction systems that generate the equality of both systems. We\ntherefore prove that the equality, and hence typechecking, of both theories is\ndecidable. This is used to give an implementation of these type theories, which\nuses an approach inspired by normalisation by evaluation to efficiently find\nnormal forms for terms. We further introduce a bidirectional typechecking\nalgorithm used by the implementation which allows for terms to be defined in a\nconvenient syntax where many arguments can be left implicit.",
        "We construct a general theory of spin magnetization driven by chiral phonons\nunder an adiabatic process, in which atoms rotate around their equilibrium\npositions with a low phonon frequency. Here the spin magnetization originates\nfrom the modulated electronic states with spin-orbital coupling by atomic\nrotations. Under the adiabatic approximation, the time-dependent spin\nmagnetization can be calculated by a Berry-phase method. In this paper, we\nfocus on its time average, which is evaluated by assuming that the phonon\ndisplacement is small. As a result, the time average of the spin magnetization\nis concisely formulated in the form of the Berry curvature defined in the\nphonon-displacement space as an intrinsic property of atomic rotations. Our\nformula for spin magnetization reflects the chiral nature of phonons, and is\nconvenient for $ab$ $initio$ calculations.",
        "We consider the McKean-Vlasov equation $dX_t = b(t, X_t, [X_t])dt + \\sigma(t,\nX_t, [X_t])dW_t$ where $[X_t]$ is the law of $X_t$. We focus specifically on\nthe \"kinetic\" case, where the equation is degenerate in the sense that the\ndimension of the Brownian motion $W$ can be strictly less than the dimension of\nthe solution $X$. Assuming H\\\"older continuous coefficients and a\nhypoellipticity condition, we prove the well-posedness of the equation. This\nresult complements the existing literature by addressing the case where the\ndiffusion coefficient $\\sigma$ depends on the law $[X_t]$, which have been\npreviously unexplored. Our proof employs a novel technique that offers a\nsimplified and direct argument, eliminating the need for PDEs involving\nderivatives with respect to the measure argument and exploiting the\nsub-Riemannian metric structure induced by the corresponding Fokker-Planck\noperator.",
        "In this paper, we introduce a notion of derived involutive algebras in $ C_2\n$-Mackey functors which simultaneously generalize commutative rings with\ninvolution and the (non-equivariant) derived algebras of Bhatt--Mathew and\nRaksit. We show that the $ \\infty $-category of derived involutive algebras\nadmits involutive enhancements of the cotangent complexes, de Rham complex, and\nde Rham cohomology functors; furthermore, their real Hochschild homology is\ndefined. We identify a filtration on the real Hochschild homology of these\nderived involutive algebras via a universal property and show that its\nassociated graded may be identified with the involutive de Rham complex. Using\n$ C_2 $-$ \\infty $-categories of Barwick--Dotto--Glasman--Nardin--Shah, we show\nthat our filtered real Hochschild homology specializes to the HKR-filtered\nHochschild homology considered by Raksit.",
        "This study employs spectral methods to capture the behaviour of wave equation\nwith dispersive-nonlinearity. We describe the evolution of hump initial data\nand track the conservation of the mass and energy functionals. The\ndispersive-nonlinearity results to solution in an extended Schwartz space via\nanalytic approach. We construct numerical schemes based on spectral methods to\nsimulate soliton interactions under Schwartzian initial data. The computational\nanalysis includes validation of energy and mass conservation to ensure\nnumerical accuracy. Results show that initial data from the Schwartz space\ndecompose into smaller wave-packets due to the weaker dispersive-nonlinearity\nbut leads to wave collapse as a result of stronger dispersive-nonlinearity. We\nconjecture that the hyperbolic equation with a positive nonlinearity and\nexponent greater or equal 2 admits global solutions, while lower exponents lead\nto localized solutions. A stability analysis of solitonic solutions of the\nequation is provided via the perturbation approach.",
        "Recent approaches to the theory of dynamic programming view dynamic programs\nas families of policy operators acting on partially ordered sets. In this\npaper, we extend these ideas by shifting from arbitrary partially ordered sets\nto ordered vector space. The advantage of working in this setting is that\nordered vector spaces have well integrated algebric and order structure, which\nleads to sharper fixed point results. These fixed point results can then be\nexploited to obtain strong optimality properties. We illustrate our results\nthrough a range of applications, including new findings for several useful\nmodels.",
        "Mrk 421 was in its most active state around early 2010, which led to the\nhighest TeV gamma-ray flux ever recorded from any active galactic nuclei. We\naim to characterize the multiwavelength behavior during this exceptional year\nfor Mrk 421, and evaluate whether it is consistent with the picture derived\nwith data from other less exceptional years. We investigated the period from\nNovember 5, 2009, (MJD 55140) until July 3, 2010, (MJD 55380) with extensive\ncoverage from very-high-energy (VHE; E$\\,>\\,$100$\\,$GeV) gamma rays to radio\nwith MAGIC, VERITAS, Fermi-LAT, RXTE, Swift, GASP-WEBT, VLBA, and a variety of\nadditional optical and radio telescopes. We investigated the variability and\ncorrelation behavior among different energy bands in great detail. We find the\nstrongest variability in X-rays and VHE gamma rays, and PSDs compatible with\npower-law functions. We observe strong correlations between X-rays and VHE\ngamma rays. We also report a marginally significant positive correlation\nbetween high-energy (HE; E$\\,>\\,$100$\\,$MeV) gamma rays and the ultraviolet\nband. We detected marginally significant correlations between the HE and VHE\ngamma rays, and between HE gamma rays and the X-ray, that disappear when the\nlarge flare in February 2010 is excluded from the correlation study. The\nactivity of Mrk 421 also yielded the first ejection of features in the VLBA\nimages of the jet of Mrk 421. Yet the large uncertainties in the ejection times\nof these radio features prevent us from firmly associating them to the specific\nflares recorded during the campaign. We also show that the collected\nmulti-instrument data are consistent with a scenario where the emission is\ndominated by two regions, a compact and extended zone, which could be\nconsidered as a simplified implementation of an energy-stratified jet as\nsuggested by recent IXPE observations.",
        "We study the relation between spectral data of magnetic monopoles in\nhyperbolic space and the curve of the spectral parameter of generalized chiral\nPotts models (gCPM) through the lens of (semi-)holomorphic field theories. We\nrealize the identification of the data on the two sides, which we call the\nhyperbolic monopole\/gCPM correspondence. For the group $\\text{SU}(2)$, this\ncorrespondence had been observed by Atiyah and Murray in the 80s. Here, we\nrevisit and generalize this correspondence and establish its origin. By\ninvoking the work of Murray and Singer on hyperbolic monopoles, we first\ngeneralize the observation of Atiyah and Murray to the group $\\text{SU}(n)$. We\nthen propose a technology to engineer gCPM within the 4d Chern-Simons (CS)\ntheory, which explains various features of the model, including the lack of\nrapidity-difference property of its R-matrix and its peculiarity of having a\ngenus$\\,\\ge 2$ curve of the spectral parameter. Finally, we investigate the\norigin of the correspondence. We first clarify how the two sides of the\ncorrespondence can be realized from the 6d holomorphic CS theory on\n$\\mathbb{P}S(M)$, the projective spinor bundle of the Minkowski space\n$M=\\mathbb{R}^{1,3}$, for hyperbolic $\\text{SU}(n)$-monopoles, and the\nEuclidean space $M=\\mathbb{R}^4$, for the gCPM. We then establish that\n$\\mathbb{P}S(M)$ can be holomorphically embedded into\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$, the projective spinor bundle of\n$\\mathbb{C}^{1,3}$, of complex dimension five with a fixed complex structure.\nWe finally explain how the 6d CS theory on $\\mathbb{P}S(M)$ can be realized as\nthe dimensional reduction of the 10d holomorphic CS theory on\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$. As the latter theory is only sensitive to the\ncomplex structure of $\\mathbb{P}S(\\mathbb{C}^{1,3})$, which has been fixed, we\nrealize the correspondence as two incarnations of the same physics in ten\ndimensions.",
        "Learned image compression (LIC) methods have recently outperformed\ntraditional codecs such as VVC in rate-distortion performance. However, their\nlarge models and high computational costs have limited their practical\nadoption. In this paper, we first construct a high-capacity teacher model by\nintegrating Swin-Transformer V2-based attention modules, additional residual\nblocks, and expanded latent channels, thus achieving enhanced compression\nperformance. Building on this foundation, we propose a \\underline{F}eature and\n\\underline{E}ntropy-based \\underline{D}istillation \\underline{S}trategy\n(\\textbf{FEDS}) that transfers key knowledge from the teacher to a lightweight\nstudent model. Specifically, we align intermediate feature representations and\nemphasize the most informative latent channels through an entropy-based loss. A\nstaged training scheme refines this transfer in three phases: feature\nalignment, channel-level distillation, and final fine-tuning. Our student model\nnearly matches the teacher across Kodak (1.24\\% BD-Rate increase), Tecnick\n(1.17\\%), and CLIC (0.55\\%) while cutting parameters by about 63\\% and\naccelerating encoding\/decoding by around 73\\%. Moreover, ablation studies\nindicate that FEDS generalizes effectively to transformer-based networks. The\nexperimental results demonstrate our approach strikes a compelling balance\namong compression performance, speed, and model parameters, making it\nwell-suited for real-time or resource-limited scenarios.",
        "Distribution-free predictive inference beyond the construction of prediction\nsets has gained a lot of interest in recent applications. One such application\nis the selection task, where the objective is to design a reliable selection\nrule to pick out individuals with desired unobserved outcomes while controlling\nthe error rate. In this work, we address the selection problem in the context\nof hierarchical data, where groups of observations may exhibit distinct\nwithin-group distributions. This generalizes existing techniques beyond the\nstandard i.i.d.\/exchangeable data settings. As a correction, For hierarchical\ndata, we introduce methods to construct valid conformal e-values, enabling\ncontrol of the false discovery rate (FDR) through the e-BH procedure. In\nparticular, we introduce and compare two approaches -- subsampling conformal\ne-values and hierarchical conformal e-values. Empirical results demonstrate\nthat both approaches achieve valid FDR control while highlighting a tradeoff\nbetween stability and power. The subsampling-based method, though random,\ntypically offers higher power, whereas the hierarchical approach, being\ndeterministic, tends to be slightly less powerful. The effectiveness of the\nproposed methods is illustrated in two real-world applications.",
        "In fault-tolerant quantum computing with the surface code, non-Clifford gates\nare crucial for universal computation. However, implementing these gates using\nmethods like magic state distillation and code switching requires significant\nresources. In this work, we propose a new protocol that combines magic state\npreparation and code switching to realize logical non-Clifford operations with\nthe potential for fault tolerance. Our approach begins with a special logical\nstate in the $\\mathbb{Z}_4$ surface code. By applying a sequence of\ntransformations, the system goes through different topological codes, including\nthe non-Abelian $D_4$ quantum double model. This process ultimately produces a\nmagic state in a condensed $\\mathbb{Z}_2$ surface code, which enables the\nimplementation of a logical $T$ gate in the standard $\\mathbb{Z}_2$ surface\ncode. In our analysis, we employ a framework where the topological codes are\nrepresented by their topological orders and all the transformations are\nconsidered as topological manipulations such as gauging symmetries and\ncondensing anyons. This perspective is particularly useful for understanding\ncode switching between topological codes.",
        "Bias from contrast injection variability is a significant obstacle to\naccurate intracranial aneurysm occlusion prediction using quantitative\nangiography and deep neural networks . This study explores bias removal and\nexplainable AI for outcome prediction. This study used angiograms from 458\npatients with flow diverters treated IAs with six month follow up defining\nocclusion status. We minimized injection variability by deconvolving the parent\nartery input to isolate the impulse response of aneurysms, then reconvolving it\nwith a standardized injection curve. A deep neural network trained on these QA\nderived biomarkers predicted six month occlusion. Local Interpretable Model\nAgnostic Explanations identified the key imaging features influencing the\nmodel, ensuring transparency and clinical relevance.",
        "We investigate the shadows and optical appearances of a new type of regular\nblack holes (BHs) with a Minkowski core under different spherical accretion.\nThese BHs are constructed by modifying the Newtonian potential based on the\nminimum observable length in the Generalized Uncertainty Principle (GUP). They\ncorrespond one-to-one with the traditional regular BHs with a de-Sitter (dS)\ncore (such as Bardeen\/Hayward BHs), characterized by quantum gravity effect\nparameter ($\\alpha_0$) and spacetime deformation factor ($n$). We find that the\ncharacteristic parameters give rise to some novel observable features. For\nthese new BHs, the shadow radius, photon sphere radius, and total observed\nintensity increase with the increase of $\\alpha_0$ but decrease with the\nincrease of $n$. Under different spherical accretion, the shadow and photon\nsphere radius are identical, but the total observed intensity under the static\nspherical accretion is greater than that under the infalling spherical\naccretion. In addition, we find that these regular BHs with different cores\nshow differences in shadows and optical appearances, especially under the\nstatic spherical accretion. Compared with Bardeen BH, the new BH has a smaller\ntotal observed intensity, dimmer maximum luminosity, and smaller shadow and\nphoton sphere radius. The larger $\\alpha_0$ leads to more significant\ndifferences, and a similar trend is also seen in the comparison with Hayward\nBH. Under the infalling spherical accretion, these regular BHs with different\ncores only have slight differences in total observed intensity, shadow and\nphoton sphere radius, which become more obvious when $\\alpha_0$ is relatively\nlarge. It suggests that the unique spacetime features of these regular BHs with\ndifferent cores can be distinguished through astronomical observations.",
        "Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in such tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortcoming, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which extends Hummos' fast-and-slow\nlearning to image-computable tasks demanding flexible rule-based behavior. WiNN\nemploys a pretrained convolutional neural network for vision, coupled with an\nadjustable \"context state\" that guides attention to relevant features. If WiNN\nproduces an incorrect response, it first iteratively updates its context state\nto refocus attention on task-relevant cues, then performs minimal parameter\nupdates to attention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory and attention networks, reducing catastrophic\nforgetting. We evaluate WiNN on an image-based extension of the Wisconsin Card\nSorting Task, revealing several markers of cognitive flexibility: (i) WiNN\nautonomously infers underlying rules, (ii) requires fewer examples to do so\nthan control models reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state updates alone.\nBy blending fast context inference with targeted attentional guidance, WiNN\nachieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks.",
        "This work is dedicated to a novel sampling method for accurately\nreconstructing elastic and electromagnetic sources from the far field patterns.\nWe show that the proposed indicators in the form of integrals with full far\nfield patterns are exactly the source functions. These facts not only give\nconstructive uniqueness proofs of the inverse source problems, but also\nestablish the theoretical basis of the proposed sampling methods. Furthermore,\nwe derive the stability estimates for the corresponding discrete indicators\nusing the far field patterns with finitely many observations and frequencies.\nWe have also proposed the indicators with partial far field patterns and proved\ntheir validity for providing the derivative information of the unknown sources.\nNumerical examples are presented to verify the accuracy and stability of the\nproposed quantitative sampling method.",
        "We investigate the effects of heterogeneous (spatially varying) activity in a\nhydrodynamical model for dense bacterial suspensions, confining ourselves to\nexperimentally realizable, simple, quenched, activity patterns. We show that\nthe evolution of the bacterial velocity field under such activity patterning\nleads to the emergence of hydrodynamic interfaces separating spatially\nlocalized turbulence from jammed frictional surroundings. We characterise the\nintermittent and multiscale fluctuations of this interface and also investigate\nhow heterogeneity influences mixing via the residence times of Lagrangian\ntracers. This work reveals how naturally occurring heterogeneities could\ndecisively steer active flows into more complex configurations than those\ntypically studied, opening up parallels to droplet dynamics, front propagation\nand turbulent mixing layers.",
        "Recently, a surprising connection between algorithmic learning of algebraic\nstructures and descriptive set theory has emerged. Following this line of\nresearch, we define the learning power of an equivalence relation $E$ on a\ntopological space as the class of isomorphism relations with countably many\nequivalence classes that are continuously reducible to $E$. In this paper, we\ndescribe the learning power of the finite Friedman-Stanley jumps of\n$=_{\\mathbb{N}}$ and $=_{\\mathbb{N}^\\mathbb{N}}$, proving that these\nequivalence relations learn the families of countable structures that are\npairwise distinguished by suitable infinitary sentences. Our proof techniques\nintroduce new ideas for assessing the continuous complexity of Borel\nequivalence relations.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "The cutoff phenomenon was recently shown to systematically follow from\nnon-negative curvature and the product condition, for all Markov diffusions.\nThe proof crucially relied on a classical \\emph{chain rule} satisfied by the\ncarr\\'e du champ operator, which is specific to differential generators and\nhence fails on discrete spaces. In the present paper, we show that an\napproximate version of this chain rule in fact always holds, with an extra cost\nthat depends on the log-Lipschitz regularity of the considered observable. As a\nconsequence, we derive a new cutoff criterion for non-negatively curved chains\non finite spaces. The latter allows us to recover, in a simple and unified way,\na number of historical instances of cutoff that had been established through\nmodel-specific arguments. Emblematic examples include random walk on the\nhypercube, random transpositions, random walk on the multislice, or MCMC\nsamplers for popular spin systems such as the Ising and Hard-core models on\nbounded-degree graphs.",
        "Generative machine learning models for small molecule drug discovery have\nshown immense promise, but many molecules generated by this approach are too\ndifficult to synthesize to be worth further investigation or further\ndevelopment. We present a novel approach by fine-tuning Meta's Llama3 large\nlanguage models (LLMs) to create SynLlama, which generates full synthetic\npathways made of commonly accessible Enamine building blocks and robust organic\nreaction templates. SynLlama explores a large synthesizable space using\nsignificantly less data compared to other state-of-the-art methods, and offers\nstrong performance in bottom-up synthesis, synthesizable analog generation, and\nhit expansion, offering medicinal chemists a valuable tool for drug discovery\ndevelopments. We find that SynLlama can effectively generalize to unseen yet\npurchasable building blocks, meaning that its reconstruction capabilities\nextend to a broader synthesizable chemical space than the training data.",
        "Spin-1\/2 Heisenberg antiferromagnetic frustrated spin chain systems display\nexotic ground states with unconventional excitations and distinct quantum phase\ntransitions as the ratio of next-nearest-neighbor to nearest-neighbor coupling\nis tuned. We present a comprehensive investigation of the structural, magnetic,\nand thermodynamics properties of the spin-1\/2 compound, Cu(Ampy)ClBr (Ampy=\nC6H8N2 = 2-(Aminomethyl)pyridine) via x-ray diffraction, magnetization,\nspecific heat, 1H nuclear magnetic resonance (NMR), electron spin resonance\n(ESR), and muon spin relaxation (muSR) techniques. The crystal structure\nfeatures an anisotropic triangular chain lattice of magnetic Cu2+ ions. Our\nbulk and local probe experiments detect neither long-range magnetic ordering\nnor spin freezing down to 0.06 K despite the presence of moderate\nantiferromagnetic interaction between Cu2+ spins as reflected by a Curie-Weiss\ntemperature of about -9 K from the bulk susceptibility data. A broad maximum is\nobserved at about 9 K in magnetic susceptibility and specific heat data,\nindicating the onset of short-range spin correlations. At low temperatures, the\nzero-field magnetic specific heat and the 1H NMR spin-lattice relaxation rate\nfollow an exponential temperature dependence, indicating the presence of gapped\nmagnetic excitations. Furthermore, persistent spin dynamics down to 0.088 K\nobserved by zero-field muSR evidences lack of any static magnetism. We\nattribute these experimental results to the stabilization of a dimer-singlet\nphase in the presence of a next-near neighbor interaction and of a randomness\nin the exchange coupling driven by Cl\/Br mixing.",
        "Response theory provides a pathway for understanding the sensitivity of a\nsystem and, more in general, to predict how its statistical properties change\nas a possibly time-dependent perturbation is applied. Recently discovered\ngeneral forms of the celebrated Fluctuation-Dissipation Theorem allow for\nexpressing response operators as correlation functions of suitably defined\nobservables in the unperturbed state, also when such a state is far from\nequilibrium. In the case of complex and multiscale systems, to achieved\nenhanced practical applicability, response theory must be interpretable,\ncapable of focusing of relevant timescales, and amenable to implemented by\ndata-driven approaches that are potentially equation-agnostic. Complex systems\ntypically exhibit a hierarchy of temporal behaviors, and unresolved or\nundesired timescales can obscure the dominant mechanisms driving macroscopic\nresponses. As an element of this desired framework, in the spirit of Markov\nstate modelling, we propose here a comprehensive analysis of the linear and\nnonlinear response of Markov chains to general time-dependent perturbations. We\nobtain simple and easily implementable formulas that can be used to predict the\nresponse of observables as well as higher-order correlations of the system. The\nmethodology proposed here can be implemented in a purely data-driven setting\nand even if we do not know the underlying evolution equations. The use of\nalgebraic expansions inspired by Koopmanism allow to elucidate the role of\ndifferent time scales and find explicit and interpretable expressions for the\nGreen's functions at all orders. This is a major advantage of the framework\nproposed here. We illustrate our methodology in a very simple yet instructive\nmetastable system. Finally, our results provide a dynamical foundation for the\nProny method, which is commonly used for the statistical analysis of discrete\ntime signals.",
        "On-farm sensor data have allowed farmers to implement field management\ntechniques and intensively track the corresponding responses. These data\ncombined with historical records open the door for real-time field management\nimprovements with the help of current advancements in computing power. However,\ndespite these advances, the statistical design of experiments is rarely used to\nevaluate the performance of field management techniques accurately.\nTraditionally, randomized block design is prevalent in statistical designs of\nfield trials, but in practice it is limited in dealing with large variations in\nsoil classes, management practices, and crop varieties. More specifically,\nalthough this experimental design is suited for most trial types, it is not the\noptimal choice when multiple factors are tested over multifarious natural\nvariations in farms, due to the economic constraints caused by the sheer number\nof variables involved. Experimental refinement is required to better estimate\nthe effects of the primary factor in the presence of auxiliary factors. In this\nway, farmers can better understand the characteristics and limitations of the\nprimary factor. This work presents a framework for automating the analysis of\nlocal field variations by fusing soil classification data and lidar topography\ndata with historical yield. This framework will be leveraged to automate the\ndesigning of field experiments based on multiple topographic features",
        "The James Webb Space Telescope, launched in 2021, is an infrared observatory\nof novel design: deployable, with active optics, fully open to space for\nradiative cooling and orbiting the Lagrange point no. 2. This article explains\nthe rationale leading to this specific design and describes the various other\narchitectures that were considered along the way: from a monolithic 10-meter\ntelescope in geosynchronous orbit to a 6-meter one in High Earth Orbit, then a\n16-meter observatory on the Moon, a 4- or 6-meter one in an elliptical\nheliocentric orbit, and a segmented 8-meter one passively cooled to 50 K at L2,\nwhich was finally descoped to 6.6 meters. It also addresses the optimization\nfor scientific performance, the challenge of dealing with such an ultra-low\noperating temperature, cost issues, supporting technology, modifications made\nduring final design and, finally, how the architecture performs on orbit.",
        "We study the obstacle problem associated with the Kolmogorov operator\n$\\Delta_v - \\partial_t - v\\cdot\\nabla_x$, which arises from the theory of\noptimal control in Asian-American options pricing models.\n  Our first main contribution is to improve the known regularity of solutions,\nfrom $C^{0,1}_t \\cap C^{0,2\/3}_x \\cap C^{1,1}_v$ to $C^{0,1}_{t,x} \\cap\nC^{1,1}_v$. The previous result in the literature, which has been called\noptimal, corresponds to $C^{1,1}$ regularity with respect to the Kolmogorov\ndistance. This is the expected regularity for solutions to obstacle problems.\nOur unexpected improvement of regularity in the $x$ variable is obtained using\nBernstein's technique and an approach drawing on ideas from Evans-Krylov\ntheory.\n  We then use this improvement in regularity of the solution to prove the first\nknown free boundary regularity result. We show that under a standard thickness\ncondition, the free boundary is a $C^{0,1\/2}_{t,x} \\cap C^{0,1}_v$ regular\nsurface. This result constitutes the first step in the program of free boundary\nregularity. Critically, our arguments rely on a new monotonicity formula and a\ncommutator estimate that are only made possible by the solution's enhanced\nregularity in $x$.",
        "After the Asian Infrastructure Investment Bank (AIIB) revised its\nEnvironmental and Social Framework, it has committed to certain climate-related\nobjectives, yet an independent climate accountability mechanism has not been\nestablished. The absence of clear evaluation principles and procedural rules\npresents challenges in effectively addressing environmental investment\ndisputes. This article reviews both domestic and international literature\nrelated to AIIB's climate accountability mechanism, identifying that the\ncurrent Environmental and Social Framework's principled content and reliance on\ntraditional approaches have resulted in issues of enforceability and the\nabsence of an independent accountability institution. To enhance the\neffectiveness of AIIB's climate accountability mechanism, a reassessment of its\ndevelopment path is necessary. Future developments should transition from an\nadditive path to a substitutive path, focusing on the application of\ninternational environmental and social standards, increasing stakeholder\nrecognition and participation, promoting comprehensive reforms within AIIB, and\nestablishing a coordinated independent accountability system. These measures\naim to support the robust development of AIIB's climate accountability\nmechanism.",
        "We develop an Integral Transformation Method (ITM) for the study of suitable\noptimal control and differential game models. This allows for a solution to\nsuch dynamic problems to be found through solving a family of optimization\nproblems parametrized by time. The method is quite flexible, and it can be used\nin several economic applications where the state equation and the objective\nfunctional are linear in a state variable. We illustrate the ITM in the context\nof a two-country integrated assessment climate model. We characterize\nemissions, consumption, transfers, and welfare by computing the Nash equilibria\nof the associated dynamic game. We then compare them to efficiency benchmarks.\nFurther, we apply the ITM in a robust control setup, where we investigate how\n(deep) uncertainty affects climate outcomes.",
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax",
    "start_abstract":"Radiology narrative reports often describe characteristics of a patient's disease, including its location, size, and shape. Motivated by the recent success multimodal learning, we hypothesized that this descriptive text could guide medical image analysis algorithms. We proposed novel vision-language model, ConTEXTual Net, for task pneumothorax segmentation on chest radiographs. Net extracts language features from physician-generated free-form radiology using pre-trained model. then introduced cross-attention between intermediate embeddings an encoder-decoder convolutional neural network to enable guidance analysis. was trained CANDID-PTX dataset consisting 3196 positive cases with annotations 6 different physicians as well clinical reports. Using cross-validation, achieved Dice score 0.716\u00b10.016, which similar degree inter-reader variability (0.712\u00b10.044) computed subset data. It outperformed vision-only models (Swin UNETR: 0.670\u00b10.015, ResNet50 U-Net: 0.677\u00b10.015, GLoRIA: 0.686\u00b10.014, nnUNet 0.694\u00b10.016) competing model (LAVT: 0.706\u00b10.009). Ablation studies confirmed it information led performance gains. Additionally, show certain augmentation methods degraded Net's breaking image-text concordance. also evaluated effects activation functions in module, highlighting efficacy our chosen architectural design.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "MRI-Guided Adaptive Radiation Therapy"
      ],
      "abstract":[
        "Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "A type-theoretic approach to semistrict higher categories",
        "Theory of spin magnetization driven by chiral phonons",
        "Well-posedness of McKean-Vlasov equations under the weak H\\\"ormander\n  condition",
        "A filtered Hochschild-Kostant-Rosenberg theorem for real Hochschild\n  homology",
        "Spectral Analysis and Stability of Wave Equations with Dispersive\n  Nonlinearity",
        "Dynamic Programming in Ordered Vector Space",
        "Characterization of Markarian 421 during its most violent year:\n  Multiwavelength variability and correlations",
        "Hyperbolic Monopoles, (Semi-)Holomorphic Chern-Simons Theories, and\n  Generalized Chiral Potts Models",
        "FEDS: Feature and Entropy-Based Distillation Strategy for Efficient\n  Learned Image Compression",
        "Selection from Hierarchical Data with Conformal e-values",
        "Generating logical magic states with the aid of non-Abelian topological\n  order",
        "Minimizing Human-Induced Variability in Quantitative Angiography for\n  Robust and Explainable AI-Based Occlusion Prediction",
        "Investigating the shadows of new regular black holes with a Minkowski\n  core: Effects of spherical accretion and core type differences",
        "Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing",
        "A quantitative sampling method for elastic and electromagnetic sources",
        "Turbulence-Induced Fluctuating Interfaces in Heterogeneously-Active\n  Suspensions",
        "On the learning power of Friedman-Stanley jumps",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "A new cutoff criterion for non-negatively curved chains",
        "SynLlama: Generating Synthesizable Molecules and Their Analogs with\n  Large Language Models",
        "Observation of the dimer-singlet phase in the one-dimensional S = 1\/2\n  Heisenberg antiferromagnet Cu(Ampy)ClBr (Ampy= C6H8N2 =\n  2-(Aminomethyl)pyridine)",
        "Interpretable and Equation-Free Response Theory for Complex Systems",
        "A Web-Based Application Leveraging Geospatial Information to Automate\n  On-Farm Trial Design",
        "Genesis of the James Webb Space Telescope Architecture: The Designers'\n  Story",
        "Hypoelliptic Regularization in the Obstacle Problem for the Kolmogorov\n  Operator",
        "Exploring the Reform and Development Pathways of AIIB's Climate\n  Accountability Mechanism in the Context of Global Climate Governance",
        "An integral transformation approach to differential games: a climate\n  model application",
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis"
      ],
      "abstract":[
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Weak $\\infty$-categories are known to be more expressive than their strict\ncounterparts, but are more difficult to work with, as constructions in such a\ncategory involve the manipulation of explicit coherence data. This motivates\nthe search for definitions of semistrict $\\infty$-categories, where some, but\nnot all, of the operations have been strictified.\n  We introduce a general framework for adding definitional equality to the type\ntheory $\\mathsf{Catt}$, a type theory whose models correspond to globular weak\n$\\infty$-categories, which was introduced by Finster and Mimram. Adding\nequality to this theory causes the models to exhibit semistrict behaviour,\ntrivialising some operations while leaving others weak. The framework consists\nof a generalisation of $\\mathsf{Catt}$ extended with an equality relation\ngenerated by an arbitrary set of equality rules $\\mathcal{R}$, which we name\n$\\mathsf{Catt}_{\\mathcal{R}}$. We study this framework in detail, formalising\nmuch of its metatheory in the proof assistant Agda, and studying how certain\noperations of $\\mathsf{Catt}$ behave in the presence of definitional equality.\n  We use this framework to introduce two type theories,\n$\\mathsf{Catt}_{\\mathsf{su}}$ and $\\mathsf{Catt}_{\\mathsf{sua}}$, which are\ninstances of this general framework. Further, we provide terminating and\nconfluent reduction systems that generate the equality of both systems. We\ntherefore prove that the equality, and hence typechecking, of both theories is\ndecidable. This is used to give an implementation of these type theories, which\nuses an approach inspired by normalisation by evaluation to efficiently find\nnormal forms for terms. We further introduce a bidirectional typechecking\nalgorithm used by the implementation which allows for terms to be defined in a\nconvenient syntax where many arguments can be left implicit.",
        "We construct a general theory of spin magnetization driven by chiral phonons\nunder an adiabatic process, in which atoms rotate around their equilibrium\npositions with a low phonon frequency. Here the spin magnetization originates\nfrom the modulated electronic states with spin-orbital coupling by atomic\nrotations. Under the adiabatic approximation, the time-dependent spin\nmagnetization can be calculated by a Berry-phase method. In this paper, we\nfocus on its time average, which is evaluated by assuming that the phonon\ndisplacement is small. As a result, the time average of the spin magnetization\nis concisely formulated in the form of the Berry curvature defined in the\nphonon-displacement space as an intrinsic property of atomic rotations. Our\nformula for spin magnetization reflects the chiral nature of phonons, and is\nconvenient for $ab$ $initio$ calculations.",
        "We consider the McKean-Vlasov equation $dX_t = b(t, X_t, [X_t])dt + \\sigma(t,\nX_t, [X_t])dW_t$ where $[X_t]$ is the law of $X_t$. We focus specifically on\nthe \"kinetic\" case, where the equation is degenerate in the sense that the\ndimension of the Brownian motion $W$ can be strictly less than the dimension of\nthe solution $X$. Assuming H\\\"older continuous coefficients and a\nhypoellipticity condition, we prove the well-posedness of the equation. This\nresult complements the existing literature by addressing the case where the\ndiffusion coefficient $\\sigma$ depends on the law $[X_t]$, which have been\npreviously unexplored. Our proof employs a novel technique that offers a\nsimplified and direct argument, eliminating the need for PDEs involving\nderivatives with respect to the measure argument and exploiting the\nsub-Riemannian metric structure induced by the corresponding Fokker-Planck\noperator.",
        "In this paper, we introduce a notion of derived involutive algebras in $ C_2\n$-Mackey functors which simultaneously generalize commutative rings with\ninvolution and the (non-equivariant) derived algebras of Bhatt--Mathew and\nRaksit. We show that the $ \\infty $-category of derived involutive algebras\nadmits involutive enhancements of the cotangent complexes, de Rham complex, and\nde Rham cohomology functors; furthermore, their real Hochschild homology is\ndefined. We identify a filtration on the real Hochschild homology of these\nderived involutive algebras via a universal property and show that its\nassociated graded may be identified with the involutive de Rham complex. Using\n$ C_2 $-$ \\infty $-categories of Barwick--Dotto--Glasman--Nardin--Shah, we show\nthat our filtered real Hochschild homology specializes to the HKR-filtered\nHochschild homology considered by Raksit.",
        "This study employs spectral methods to capture the behaviour of wave equation\nwith dispersive-nonlinearity. We describe the evolution of hump initial data\nand track the conservation of the mass and energy functionals. The\ndispersive-nonlinearity results to solution in an extended Schwartz space via\nanalytic approach. We construct numerical schemes based on spectral methods to\nsimulate soliton interactions under Schwartzian initial data. The computational\nanalysis includes validation of energy and mass conservation to ensure\nnumerical accuracy. Results show that initial data from the Schwartz space\ndecompose into smaller wave-packets due to the weaker dispersive-nonlinearity\nbut leads to wave collapse as a result of stronger dispersive-nonlinearity. We\nconjecture that the hyperbolic equation with a positive nonlinearity and\nexponent greater or equal 2 admits global solutions, while lower exponents lead\nto localized solutions. A stability analysis of solitonic solutions of the\nequation is provided via the perturbation approach.",
        "Recent approaches to the theory of dynamic programming view dynamic programs\nas families of policy operators acting on partially ordered sets. In this\npaper, we extend these ideas by shifting from arbitrary partially ordered sets\nto ordered vector space. The advantage of working in this setting is that\nordered vector spaces have well integrated algebric and order structure, which\nleads to sharper fixed point results. These fixed point results can then be\nexploited to obtain strong optimality properties. We illustrate our results\nthrough a range of applications, including new findings for several useful\nmodels.",
        "Mrk 421 was in its most active state around early 2010, which led to the\nhighest TeV gamma-ray flux ever recorded from any active galactic nuclei. We\naim to characterize the multiwavelength behavior during this exceptional year\nfor Mrk 421, and evaluate whether it is consistent with the picture derived\nwith data from other less exceptional years. We investigated the period from\nNovember 5, 2009, (MJD 55140) until July 3, 2010, (MJD 55380) with extensive\ncoverage from very-high-energy (VHE; E$\\,>\\,$100$\\,$GeV) gamma rays to radio\nwith MAGIC, VERITAS, Fermi-LAT, RXTE, Swift, GASP-WEBT, VLBA, and a variety of\nadditional optical and radio telescopes. We investigated the variability and\ncorrelation behavior among different energy bands in great detail. We find the\nstrongest variability in X-rays and VHE gamma rays, and PSDs compatible with\npower-law functions. We observe strong correlations between X-rays and VHE\ngamma rays. We also report a marginally significant positive correlation\nbetween high-energy (HE; E$\\,>\\,$100$\\,$MeV) gamma rays and the ultraviolet\nband. We detected marginally significant correlations between the HE and VHE\ngamma rays, and between HE gamma rays and the X-ray, that disappear when the\nlarge flare in February 2010 is excluded from the correlation study. The\nactivity of Mrk 421 also yielded the first ejection of features in the VLBA\nimages of the jet of Mrk 421. Yet the large uncertainties in the ejection times\nof these radio features prevent us from firmly associating them to the specific\nflares recorded during the campaign. We also show that the collected\nmulti-instrument data are consistent with a scenario where the emission is\ndominated by two regions, a compact and extended zone, which could be\nconsidered as a simplified implementation of an energy-stratified jet as\nsuggested by recent IXPE observations.",
        "We study the relation between spectral data of magnetic monopoles in\nhyperbolic space and the curve of the spectral parameter of generalized chiral\nPotts models (gCPM) through the lens of (semi-)holomorphic field theories. We\nrealize the identification of the data on the two sides, which we call the\nhyperbolic monopole\/gCPM correspondence. For the group $\\text{SU}(2)$, this\ncorrespondence had been observed by Atiyah and Murray in the 80s. Here, we\nrevisit and generalize this correspondence and establish its origin. By\ninvoking the work of Murray and Singer on hyperbolic monopoles, we first\ngeneralize the observation of Atiyah and Murray to the group $\\text{SU}(n)$. We\nthen propose a technology to engineer gCPM within the 4d Chern-Simons (CS)\ntheory, which explains various features of the model, including the lack of\nrapidity-difference property of its R-matrix and its peculiarity of having a\ngenus$\\,\\ge 2$ curve of the spectral parameter. Finally, we investigate the\norigin of the correspondence. We first clarify how the two sides of the\ncorrespondence can be realized from the 6d holomorphic CS theory on\n$\\mathbb{P}S(M)$, the projective spinor bundle of the Minkowski space\n$M=\\mathbb{R}^{1,3}$, for hyperbolic $\\text{SU}(n)$-monopoles, and the\nEuclidean space $M=\\mathbb{R}^4$, for the gCPM. We then establish that\n$\\mathbb{P}S(M)$ can be holomorphically embedded into\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$, the projective spinor bundle of\n$\\mathbb{C}^{1,3}$, of complex dimension five with a fixed complex structure.\nWe finally explain how the 6d CS theory on $\\mathbb{P}S(M)$ can be realized as\nthe dimensional reduction of the 10d holomorphic CS theory on\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$. As the latter theory is only sensitive to the\ncomplex structure of $\\mathbb{P}S(\\mathbb{C}^{1,3})$, which has been fixed, we\nrealize the correspondence as two incarnations of the same physics in ten\ndimensions.",
        "Learned image compression (LIC) methods have recently outperformed\ntraditional codecs such as VVC in rate-distortion performance. However, their\nlarge models and high computational costs have limited their practical\nadoption. In this paper, we first construct a high-capacity teacher model by\nintegrating Swin-Transformer V2-based attention modules, additional residual\nblocks, and expanded latent channels, thus achieving enhanced compression\nperformance. Building on this foundation, we propose a \\underline{F}eature and\n\\underline{E}ntropy-based \\underline{D}istillation \\underline{S}trategy\n(\\textbf{FEDS}) that transfers key knowledge from the teacher to a lightweight\nstudent model. Specifically, we align intermediate feature representations and\nemphasize the most informative latent channels through an entropy-based loss. A\nstaged training scheme refines this transfer in three phases: feature\nalignment, channel-level distillation, and final fine-tuning. Our student model\nnearly matches the teacher across Kodak (1.24\\% BD-Rate increase), Tecnick\n(1.17\\%), and CLIC (0.55\\%) while cutting parameters by about 63\\% and\naccelerating encoding\/decoding by around 73\\%. Moreover, ablation studies\nindicate that FEDS generalizes effectively to transformer-based networks. The\nexperimental results demonstrate our approach strikes a compelling balance\namong compression performance, speed, and model parameters, making it\nwell-suited for real-time or resource-limited scenarios.",
        "Distribution-free predictive inference beyond the construction of prediction\nsets has gained a lot of interest in recent applications. One such application\nis the selection task, where the objective is to design a reliable selection\nrule to pick out individuals with desired unobserved outcomes while controlling\nthe error rate. In this work, we address the selection problem in the context\nof hierarchical data, where groups of observations may exhibit distinct\nwithin-group distributions. This generalizes existing techniques beyond the\nstandard i.i.d.\/exchangeable data settings. As a correction, For hierarchical\ndata, we introduce methods to construct valid conformal e-values, enabling\ncontrol of the false discovery rate (FDR) through the e-BH procedure. In\nparticular, we introduce and compare two approaches -- subsampling conformal\ne-values and hierarchical conformal e-values. Empirical results demonstrate\nthat both approaches achieve valid FDR control while highlighting a tradeoff\nbetween stability and power. The subsampling-based method, though random,\ntypically offers higher power, whereas the hierarchical approach, being\ndeterministic, tends to be slightly less powerful. The effectiveness of the\nproposed methods is illustrated in two real-world applications.",
        "In fault-tolerant quantum computing with the surface code, non-Clifford gates\nare crucial for universal computation. However, implementing these gates using\nmethods like magic state distillation and code switching requires significant\nresources. In this work, we propose a new protocol that combines magic state\npreparation and code switching to realize logical non-Clifford operations with\nthe potential for fault tolerance. Our approach begins with a special logical\nstate in the $\\mathbb{Z}_4$ surface code. By applying a sequence of\ntransformations, the system goes through different topological codes, including\nthe non-Abelian $D_4$ quantum double model. This process ultimately produces a\nmagic state in a condensed $\\mathbb{Z}_2$ surface code, which enables the\nimplementation of a logical $T$ gate in the standard $\\mathbb{Z}_2$ surface\ncode. In our analysis, we employ a framework where the topological codes are\nrepresented by their topological orders and all the transformations are\nconsidered as topological manipulations such as gauging symmetries and\ncondensing anyons. This perspective is particularly useful for understanding\ncode switching between topological codes.",
        "Bias from contrast injection variability is a significant obstacle to\naccurate intracranial aneurysm occlusion prediction using quantitative\nangiography and deep neural networks . This study explores bias removal and\nexplainable AI for outcome prediction. This study used angiograms from 458\npatients with flow diverters treated IAs with six month follow up defining\nocclusion status. We minimized injection variability by deconvolving the parent\nartery input to isolate the impulse response of aneurysms, then reconvolving it\nwith a standardized injection curve. A deep neural network trained on these QA\nderived biomarkers predicted six month occlusion. Local Interpretable Model\nAgnostic Explanations identified the key imaging features influencing the\nmodel, ensuring transparency and clinical relevance.",
        "We investigate the shadows and optical appearances of a new type of regular\nblack holes (BHs) with a Minkowski core under different spherical accretion.\nThese BHs are constructed by modifying the Newtonian potential based on the\nminimum observable length in the Generalized Uncertainty Principle (GUP). They\ncorrespond one-to-one with the traditional regular BHs with a de-Sitter (dS)\ncore (such as Bardeen\/Hayward BHs), characterized by quantum gravity effect\nparameter ($\\alpha_0$) and spacetime deformation factor ($n$). We find that the\ncharacteristic parameters give rise to some novel observable features. For\nthese new BHs, the shadow radius, photon sphere radius, and total observed\nintensity increase with the increase of $\\alpha_0$ but decrease with the\nincrease of $n$. Under different spherical accretion, the shadow and photon\nsphere radius are identical, but the total observed intensity under the static\nspherical accretion is greater than that under the infalling spherical\naccretion. In addition, we find that these regular BHs with different cores\nshow differences in shadows and optical appearances, especially under the\nstatic spherical accretion. Compared with Bardeen BH, the new BH has a smaller\ntotal observed intensity, dimmer maximum luminosity, and smaller shadow and\nphoton sphere radius. The larger $\\alpha_0$ leads to more significant\ndifferences, and a similar trend is also seen in the comparison with Hayward\nBH. Under the infalling spherical accretion, these regular BHs with different\ncores only have slight differences in total observed intensity, shadow and\nphoton sphere radius, which become more obvious when $\\alpha_0$ is relatively\nlarge. It suggests that the unique spacetime features of these regular BHs with\ndifferent cores can be distinguished through astronomical observations.",
        "Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in such tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortcoming, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which extends Hummos' fast-and-slow\nlearning to image-computable tasks demanding flexible rule-based behavior. WiNN\nemploys a pretrained convolutional neural network for vision, coupled with an\nadjustable \"context state\" that guides attention to relevant features. If WiNN\nproduces an incorrect response, it first iteratively updates its context state\nto refocus attention on task-relevant cues, then performs minimal parameter\nupdates to attention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory and attention networks, reducing catastrophic\nforgetting. We evaluate WiNN on an image-based extension of the Wisconsin Card\nSorting Task, revealing several markers of cognitive flexibility: (i) WiNN\nautonomously infers underlying rules, (ii) requires fewer examples to do so\nthan control models reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state updates alone.\nBy blending fast context inference with targeted attentional guidance, WiNN\nachieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks.",
        "This work is dedicated to a novel sampling method for accurately\nreconstructing elastic and electromagnetic sources from the far field patterns.\nWe show that the proposed indicators in the form of integrals with full far\nfield patterns are exactly the source functions. These facts not only give\nconstructive uniqueness proofs of the inverse source problems, but also\nestablish the theoretical basis of the proposed sampling methods. Furthermore,\nwe derive the stability estimates for the corresponding discrete indicators\nusing the far field patterns with finitely many observations and frequencies.\nWe have also proposed the indicators with partial far field patterns and proved\ntheir validity for providing the derivative information of the unknown sources.\nNumerical examples are presented to verify the accuracy and stability of the\nproposed quantitative sampling method.",
        "We investigate the effects of heterogeneous (spatially varying) activity in a\nhydrodynamical model for dense bacterial suspensions, confining ourselves to\nexperimentally realizable, simple, quenched, activity patterns. We show that\nthe evolution of the bacterial velocity field under such activity patterning\nleads to the emergence of hydrodynamic interfaces separating spatially\nlocalized turbulence from jammed frictional surroundings. We characterise the\nintermittent and multiscale fluctuations of this interface and also investigate\nhow heterogeneity influences mixing via the residence times of Lagrangian\ntracers. This work reveals how naturally occurring heterogeneities could\ndecisively steer active flows into more complex configurations than those\ntypically studied, opening up parallels to droplet dynamics, front propagation\nand turbulent mixing layers.",
        "Recently, a surprising connection between algorithmic learning of algebraic\nstructures and descriptive set theory has emerged. Following this line of\nresearch, we define the learning power of an equivalence relation $E$ on a\ntopological space as the class of isomorphism relations with countably many\nequivalence classes that are continuously reducible to $E$. In this paper, we\ndescribe the learning power of the finite Friedman-Stanley jumps of\n$=_{\\mathbb{N}}$ and $=_{\\mathbb{N}^\\mathbb{N}}$, proving that these\nequivalence relations learn the families of countable structures that are\npairwise distinguished by suitable infinitary sentences. Our proof techniques\nintroduce new ideas for assessing the continuous complexity of Borel\nequivalence relations.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "The cutoff phenomenon was recently shown to systematically follow from\nnon-negative curvature and the product condition, for all Markov diffusions.\nThe proof crucially relied on a classical \\emph{chain rule} satisfied by the\ncarr\\'e du champ operator, which is specific to differential generators and\nhence fails on discrete spaces. In the present paper, we show that an\napproximate version of this chain rule in fact always holds, with an extra cost\nthat depends on the log-Lipschitz regularity of the considered observable. As a\nconsequence, we derive a new cutoff criterion for non-negatively curved chains\non finite spaces. The latter allows us to recover, in a simple and unified way,\na number of historical instances of cutoff that had been established through\nmodel-specific arguments. Emblematic examples include random walk on the\nhypercube, random transpositions, random walk on the multislice, or MCMC\nsamplers for popular spin systems such as the Ising and Hard-core models on\nbounded-degree graphs.",
        "Generative machine learning models for small molecule drug discovery have\nshown immense promise, but many molecules generated by this approach are too\ndifficult to synthesize to be worth further investigation or further\ndevelopment. We present a novel approach by fine-tuning Meta's Llama3 large\nlanguage models (LLMs) to create SynLlama, which generates full synthetic\npathways made of commonly accessible Enamine building blocks and robust organic\nreaction templates. SynLlama explores a large synthesizable space using\nsignificantly less data compared to other state-of-the-art methods, and offers\nstrong performance in bottom-up synthesis, synthesizable analog generation, and\nhit expansion, offering medicinal chemists a valuable tool for drug discovery\ndevelopments. We find that SynLlama can effectively generalize to unseen yet\npurchasable building blocks, meaning that its reconstruction capabilities\nextend to a broader synthesizable chemical space than the training data.",
        "Spin-1\/2 Heisenberg antiferromagnetic frustrated spin chain systems display\nexotic ground states with unconventional excitations and distinct quantum phase\ntransitions as the ratio of next-nearest-neighbor to nearest-neighbor coupling\nis tuned. We present a comprehensive investigation of the structural, magnetic,\nand thermodynamics properties of the spin-1\/2 compound, Cu(Ampy)ClBr (Ampy=\nC6H8N2 = 2-(Aminomethyl)pyridine) via x-ray diffraction, magnetization,\nspecific heat, 1H nuclear magnetic resonance (NMR), electron spin resonance\n(ESR), and muon spin relaxation (muSR) techniques. The crystal structure\nfeatures an anisotropic triangular chain lattice of magnetic Cu2+ ions. Our\nbulk and local probe experiments detect neither long-range magnetic ordering\nnor spin freezing down to 0.06 K despite the presence of moderate\nantiferromagnetic interaction between Cu2+ spins as reflected by a Curie-Weiss\ntemperature of about -9 K from the bulk susceptibility data. A broad maximum is\nobserved at about 9 K in magnetic susceptibility and specific heat data,\nindicating the onset of short-range spin correlations. At low temperatures, the\nzero-field magnetic specific heat and the 1H NMR spin-lattice relaxation rate\nfollow an exponential temperature dependence, indicating the presence of gapped\nmagnetic excitations. Furthermore, persistent spin dynamics down to 0.088 K\nobserved by zero-field muSR evidences lack of any static magnetism. We\nattribute these experimental results to the stabilization of a dimer-singlet\nphase in the presence of a next-near neighbor interaction and of a randomness\nin the exchange coupling driven by Cl\/Br mixing.",
        "Response theory provides a pathway for understanding the sensitivity of a\nsystem and, more in general, to predict how its statistical properties change\nas a possibly time-dependent perturbation is applied. Recently discovered\ngeneral forms of the celebrated Fluctuation-Dissipation Theorem allow for\nexpressing response operators as correlation functions of suitably defined\nobservables in the unperturbed state, also when such a state is far from\nequilibrium. In the case of complex and multiscale systems, to achieved\nenhanced practical applicability, response theory must be interpretable,\ncapable of focusing of relevant timescales, and amenable to implemented by\ndata-driven approaches that are potentially equation-agnostic. Complex systems\ntypically exhibit a hierarchy of temporal behaviors, and unresolved or\nundesired timescales can obscure the dominant mechanisms driving macroscopic\nresponses. As an element of this desired framework, in the spirit of Markov\nstate modelling, we propose here a comprehensive analysis of the linear and\nnonlinear response of Markov chains to general time-dependent perturbations. We\nobtain simple and easily implementable formulas that can be used to predict the\nresponse of observables as well as higher-order correlations of the system. The\nmethodology proposed here can be implemented in a purely data-driven setting\nand even if we do not know the underlying evolution equations. The use of\nalgebraic expansions inspired by Koopmanism allow to elucidate the role of\ndifferent time scales and find explicit and interpretable expressions for the\nGreen's functions at all orders. This is a major advantage of the framework\nproposed here. We illustrate our methodology in a very simple yet instructive\nmetastable system. Finally, our results provide a dynamical foundation for the\nProny method, which is commonly used for the statistical analysis of discrete\ntime signals.",
        "On-farm sensor data have allowed farmers to implement field management\ntechniques and intensively track the corresponding responses. These data\ncombined with historical records open the door for real-time field management\nimprovements with the help of current advancements in computing power. However,\ndespite these advances, the statistical design of experiments is rarely used to\nevaluate the performance of field management techniques accurately.\nTraditionally, randomized block design is prevalent in statistical designs of\nfield trials, but in practice it is limited in dealing with large variations in\nsoil classes, management practices, and crop varieties. More specifically,\nalthough this experimental design is suited for most trial types, it is not the\noptimal choice when multiple factors are tested over multifarious natural\nvariations in farms, due to the economic constraints caused by the sheer number\nof variables involved. Experimental refinement is required to better estimate\nthe effects of the primary factor in the presence of auxiliary factors. In this\nway, farmers can better understand the characteristics and limitations of the\nprimary factor. This work presents a framework for automating the analysis of\nlocal field variations by fusing soil classification data and lidar topography\ndata with historical yield. This framework will be leveraged to automate the\ndesigning of field experiments based on multiple topographic features",
        "The James Webb Space Telescope, launched in 2021, is an infrared observatory\nof novel design: deployable, with active optics, fully open to space for\nradiative cooling and orbiting the Lagrange point no. 2. This article explains\nthe rationale leading to this specific design and describes the various other\narchitectures that were considered along the way: from a monolithic 10-meter\ntelescope in geosynchronous orbit to a 6-meter one in High Earth Orbit, then a\n16-meter observatory on the Moon, a 4- or 6-meter one in an elliptical\nheliocentric orbit, and a segmented 8-meter one passively cooled to 50 K at L2,\nwhich was finally descoped to 6.6 meters. It also addresses the optimization\nfor scientific performance, the challenge of dealing with such an ultra-low\noperating temperature, cost issues, supporting technology, modifications made\nduring final design and, finally, how the architecture performs on orbit.",
        "We study the obstacle problem associated with the Kolmogorov operator\n$\\Delta_v - \\partial_t - v\\cdot\\nabla_x$, which arises from the theory of\noptimal control in Asian-American options pricing models.\n  Our first main contribution is to improve the known regularity of solutions,\nfrom $C^{0,1}_t \\cap C^{0,2\/3}_x \\cap C^{1,1}_v$ to $C^{0,1}_{t,x} \\cap\nC^{1,1}_v$. The previous result in the literature, which has been called\noptimal, corresponds to $C^{1,1}$ regularity with respect to the Kolmogorov\ndistance. This is the expected regularity for solutions to obstacle problems.\nOur unexpected improvement of regularity in the $x$ variable is obtained using\nBernstein's technique and an approach drawing on ideas from Evans-Krylov\ntheory.\n  We then use this improvement in regularity of the solution to prove the first\nknown free boundary regularity result. We show that under a standard thickness\ncondition, the free boundary is a $C^{0,1\/2}_{t,x} \\cap C^{0,1}_v$ regular\nsurface. This result constitutes the first step in the program of free boundary\nregularity. Critically, our arguments rely on a new monotonicity formula and a\ncommutator estimate that are only made possible by the solution's enhanced\nregularity in $x$.",
        "After the Asian Infrastructure Investment Bank (AIIB) revised its\nEnvironmental and Social Framework, it has committed to certain climate-related\nobjectives, yet an independent climate accountability mechanism has not been\nestablished. The absence of clear evaluation principles and procedural rules\npresents challenges in effectively addressing environmental investment\ndisputes. This article reviews both domestic and international literature\nrelated to AIIB's climate accountability mechanism, identifying that the\ncurrent Environmental and Social Framework's principled content and reliance on\ntraditional approaches have resulted in issues of enforceability and the\nabsence of an independent accountability institution. To enhance the\neffectiveness of AIIB's climate accountability mechanism, a reassessment of its\ndevelopment path is necessary. Future developments should transition from an\nadditive path to a substitutive path, focusing on the application of\ninternational environmental and social standards, increasing stakeholder\nrecognition and participation, promoting comprehensive reforms within AIIB, and\nestablishing a coordinated independent accountability system. These measures\naim to support the robust development of AIIB's climate accountability\nmechanism.",
        "We develop an Integral Transformation Method (ITM) for the study of suitable\noptimal control and differential game models. This allows for a solution to\nsuch dynamic problems to be found through solving a family of optimization\nproblems parametrized by time. The method is quite flexible, and it can be used\nin several economic applications where the state equation and the objective\nfunctional are linear in a state variable. We illustrate the ITM in the context\nof a two-country integrated assessment climate model. We characterize\nemissions, consumption, transfers, and welfare by computing the Nash equilibria\nof the associated dynamic game. We then compare them to efficiency benchmarks.\nFurther, we apply the ITM in a robust control setup, where we investigate how\n(deep) uncertainty affects climate outcomes.",
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MRI-Guided Adaptive Radiation Therapy",
    "start_abstract":"Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training.",
    "start_categories":[
      "Oncology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b4",
        "b2"
      ],
      "title":[
        "Deep Learning for Automatic Gross Tumor Volumes Contouring in Esophageal Cancer Based on Contrast-Enhanced Computed Tomography Images: A Multi-Institutional Study",
        "ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax"
      ],
      "abstract":[
        "Purpose To develop and externally validate an automatic artificial intelligence (AI) tool for delineating gross tumor volume (GTV) in patients with esophageal squamous cell carcinoma (ESCC), which can assist in neo-adjuvant or radical radiation therapy treatment planning. Methods and Materials In this multi-institutional study, contrast-enhanced CT images from 580 eligible ESCC patients were retrospectively collected. The GTV contours delineated by 2 experts via consensus were used as ground truth. A 3-dimensional deep learning model was developed for GTV contouring in the training cohort and internally and externally validated in 3 validation cohorts. The AI tool was compared against 12 board-certified experts in 25 patients randomly selected from the external validation cohort to evaluate its assistance in improving contouring performance and reducing variation. Contouring performance was measured using dice similarity coefficient (DSC) and average surface distance. Additionally, our previously established radiomics model for predicting pathologic complete response was used to compare AI-generated and ground truth contours, to assess the potential of the AI contouring tool in radiomics analysis. Results The AI tool demonstrated good GTV contouring performance in multicenter validation cohorts, with median DSC values of 0.865, 0.876, and 0.866 and median average surface distance values of 0.939, 0.789, and 0.875 mm, respectively. Furthermore, the AI tool significantly improved contouring performance for half of 12 board-certified experts (DSC values, 0.794-0.835 vs 0.856-0.881, P = .003-0.048), reduced the intra- and interobserver variations by 37.4% and 55.2%, respectively, and saved contouring time by 77.6%. In the radiomics analysis, 88.7% of radiomic features from ground truth and AI-generated contours demonstrated stable reproducibility, and similar pathologic complete response prediction performance for these contours (P = .430) was observed. Conclusions Our AI contouring tool can improve GTV contouring performance and facilitate radiomics analysis in ESCC patients, which indicates its potential for GTV contouring during radiation therapy treatment planning and radiomics studies.",
        "Radiology narrative reports often describe characteristics of a patient's disease, including its location, size, and shape. Motivated by the recent success multimodal learning, we hypothesized that this descriptive text could guide medical image analysis algorithms. We proposed novel vision-language model, ConTEXTual Net, for task pneumothorax segmentation on chest radiographs. Net extracts language features from physician-generated free-form radiology using pre-trained model. then introduced cross-attention between intermediate embeddings an encoder-decoder convolutional neural network to enable guidance analysis. was trained CANDID-PTX dataset consisting 3196 positive cases with annotations 6 different physicians as well clinical reports. Using cross-validation, achieved Dice score 0.716\u00b10.016, which similar degree inter-reader variability (0.712\u00b10.044) computed subset data. It outperformed vision-only models (Swin UNETR: 0.670\u00b10.015, ResNet50 U-Net: 0.677\u00b10.015, GLoRIA: 0.686\u00b10.014, nnUNet 0.694\u00b10.016) competing model (LAVT: 0.706\u00b10.009). Ablation studies confirmed it information led performance gains. Additionally, show certain augmentation methods degraded Net's breaking image-text concordance. also evaluated effects activation functions in module, highlighting efficacy our chosen architectural design."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Reducing T Gates with Unitary Synthesis",
        "Thermodynamics of strongly magnetized dense quark matter from hard dense\n  loop perturbation theory",
        "ChatIoT: Large Language Model-based Security Assistant for Internet of\n  Things with Retrieval-Augmented Generation",
        "Generalized symmetries and the dimensional reduction of 6d so SCFTs",
        "Searches for light Dark Matter with Spherical Proportional Counters",
        "PDE-DKL: PDE-constrained deep kernel learning in high dimensionality",
        "Sken and cluster algebras of punctured surfaces",
        "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
        "UAV Swarm-enabled Collaborative Post-disaster Communications in Low\n  Altitude Economy via a Two-stage Optimization Approach",
        "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining",
        "Impact of trace amounts of water on the stability of Micro-Pattern\n  Gaseous Detectors",
        "Universal law for the dispersal of motile microorganisms in porous media",
        "Unveiling the Oxidation Mechanisms of Octa-Penta Graphene: A\n  Multidimensional Exploration from First-Principles to Machine Learning",
        "Strong conciseness and equationally Noetherian groups",
        "Evaluating Prediction-based Interventions with Human Decision Makers In\n  Mind",
        "Derived derivations govern contraderived deformations of dg algebras\n  over dg (pr)operads",
        "Large Language Models as Common-Sense Heuristics",
        "On the kernel learning problem",
        "Impilict Runge-Kutta based sparse identification of governing equations\n  in biologically motivated systems",
        "The quantum enigma of teleportation near black holes",
        "Assessing workflow impact and clinical utility of AI-assisted brain\n  aneurysm detection: a multi-reader study",
        "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
        "Evaluating Hallucination in Large Vision-Language Models based on\n  Context-Aware Object Similarities",
        "DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance\n  Analysis",
        "Improving the stellar age determination through joint modeling of\n  binarity and asteroseismology -- Grid modeling of the seismic red-giant\n  binary KIC 9163796",
        "Protein Structure Tokenization: Benchmarking and New Recipe",
        "Accurate, transferable, and verifiable machine-learned interatomic\n  potentials for layered materials",
        "Observational evidence for a correlation between the magnetic field of\n  jets and star formation rate in host galaxies"
      ],
      "abstract":[
        "Quantum error correction is essential for achieving practical quantum\ncomputing but has a significant computational overhead. Among fault-tolerant\n(FT) gate operations, non-Clifford gates, such as $T$, are particularly\nexpensive due to their reliance on magic state distillation. These costly $T$\ngates appear frequently in FT circuits as many quantum algorithms require\narbitrary single-qubit rotations, such as $R_x$ and $R_z$ gates, which must be\ndecomposed into a sequence of $T$ and Clifford gates. In many quantum circuits,\n$R_x$ and $R_z$ gates can be fused to form a single $U3$ unitary. However,\nexisting synthesis methods, such as gridsynth, rely on indirect decompositions,\nrequiring separate $R_z$ decompositions that result in a threefold increase in\n$T$ count.\n  This work presents a novel FT synthesis algorithm that directly synthesizes\narbitrary single-qubit unitaries, avoiding the overhead of separate $R_z$\ndecompositions. By leveraging tensor network-based search, our approach enables\nnative $U3$ synthesis, reducing the $T$ count, Clifford gate count, and\napproximation error. Compared to gridsynth-based circuit synthesis, for 187\nrepresentative benchmarks, our design reduces the $T$ count by up to\n$3.5\\times$, and Clifford gates by $7\\times$, resulting in up to $4\\times$\nimprovement in overall circuit infidelity.",
        "We discuss the hard dense loop perturbation theory approach for studying the\nthermodynamics of strongly magnetized dense quark matter. The free energy of\nquarks and gluons have been calculated for one-loop quark and gluon\nself-energies, respectively. The longitudinal and transverse components of\npressure, magnetization, second-order quark number susceptibility, and speed of\nsound have been computed, and their behavior with chemical potential and\nmagnetic field has been analyzed. Our numerical results show that the\nlongitudinal pressure increases with chemical potential and magnetic field,\nwhile for the transverse component, it is diminished. We also analyze the\nlongitudinal component of the speed of sound at high chemical potentials, which\napproaches the speed of light in the asymptotic limit. The obtained results may\nbe helpful in studying magnetized quark matter in the core of neutron stars and\nmagnetars.",
        "Internet of Things (IoT) has gained widespread popularity, revolutionizing\nindustries and daily life. However, it has also emerged as a prime target for\nattacks. Numerous efforts have been made to improve IoT security, and\nsubstantial IoT security and threat information, such as datasets and reports,\nhave been developed. However, existing research often falls short in leveraging\nthese insights to assist or guide users in harnessing IoT security practices in\na clear and actionable way. In this paper, we propose ChatIoT, a large language\nmodel (LLM)-based IoT security assistant designed to disseminate IoT security\nand threat intelligence. By leveraging the versatile property of\nretrieval-augmented generation (RAG), ChatIoT successfully integrates the\nadvanced language understanding and reasoning capabilities of LLM with\nfast-evolving IoT security information. Moreover, we develop an end-to-end data\nprocessing toolkit to handle heterogeneous datasets. This toolkit converts\ndatasets of various formats into retrievable documents and optimizes chunking\nstrategies for efficient retrieval. Additionally, we define a set of common use\ncase specifications to guide the LLM in generating answers aligned with users'\nspecific needs and expertise levels. Finally, we implement a prototype of\nChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3,\nLLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can\ngenerate more reliable, relevant, and technical in-depth answers for most use\ncases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above\nmetrics by over 10% on average, particularly in relevance and technicality,\ncompared to using LLMs alone.",
        "We consider the dimensional reduction on a torus of the family of 6d $(1,0)$\nSCFTs UV completing an $so(N)$ gauge theory with $N-8$ vector hypermultiplets.\nThese SCFTs are known to possess a rich structure of discrete symmetries,\nnotably 0-form and 1-form symmetries, which often merge to form a higher group\nstructure, both split and non-split. We investigate what happens to this\nsymmetry structure once the theory is reduced on a circle to 5d and on a torus\nto 4d, especially when a non-trivial Stiefel-Whitney class for the flavor\nsymmetry is turned on. Unlike in Lagrangian theories, here the 1-form\nsymmetries of the 6d theory reduce to non-trivially acting 1-form and 0-form\nsymmetries, and the original higher group structure leads to an extension of\nthe 0-form symmetries.",
        "Elucidating the nature of dark matter is a key priority that would involve\ndiscovering new fundamental physics and is essential for understanding the\nstructure and evolution of the universe. Despite the decades-long\never-more-sensitive searches, the particle content of dark matter remains\nelusive. Direct searches for dark matter candidates, to-date, focused mainly on\ncandidates in the 10 GeV to 1 TeV, however, more recently lighter candidates\nwith sub-GeV mass have been brought to the spotlight. This is an experimentally\nchallenging mass region, which remains largely uncharted. The spherical\nproportional counter is a new type of gaseous detector which exhibits several\nfeatures that make it ideally suited for the exploration of this mass range. In\nthis article the invention and development of the spherical proportional\ncounter are presented, its applications in the search for particle dark matter\nand beyond are reviewed, and possible future directions are discussed.",
        "Many physics-informed machine learning methods for PDE-based problems rely on\nGaussian processes (GPs) or neural networks (NNs). However, both face\nlimitations when data are scarce and the dimensionality is high. Although GPs\nare known for their robust uncertainty quantification in low-dimensional\nsettings, their computational complexity becomes prohibitive as the\ndimensionality increases. In contrast, while conventional NNs can accommodate\nhigh-dimensional input, they often require extensive training data and do not\noffer uncertainty quantification. To address these challenges, we propose a\nPDE-constrained Deep Kernel Learning (PDE-DKL) framework that combines DL and\nGPs under explicit PDE constraints. Specifically, NNs learn a low-dimensional\nlatent representation of the high-dimensional PDE problem, reducing the\ncomplexity of the problem. GPs then perform kernel regression subject to the\ngoverning PDEs, ensuring accurate solutions and principled uncertainty\nquantification, even when available data are limited. This synergy unifies the\nstrengths of both NNs and GPs, yielding high accuracy, robust uncertainty\nestimates, and computational efficiency for high-dimensional PDEs. Numerical\nexperiments demonstrate that PDE-DKL achieves high accuracy with reduced data\nrequirements. They highlight its potential as a practical, reliable, and\nscalable solver for complex PDE-based applications in science and engineering.",
        "We prove the full Fock-Goncharov conjecture for\n$\\mathcal{A}_{SL_2,\\Sigma_{g,p}}$--the $\\mathcal{A}$-cluster variety associated\nto representation of $SL_2$ local systems on most punctured surfaces with at\nleast 2 punctures in the classical $q\\to 1$ setting, that is, the tagged skein\nalgebra coincides with the upper cluster algebra (namely $Sk^{ta}=U(\\Sigma)$ or\n$mid(\\mathcal{A})=up(\\mathcal{A})$), with methods being potentially useful to\ntackle the quantum case. We deduce similar results for the Roger Yang skein\nalgebra via a birational geometric description, obtaining\n$Sk^{RY}=U(\\Sigma)[v_i^{\\pm1}]$ as conjectured by Shen, Sun and Weng, proving\nimportant algebraic properties of $Sk^{RY}$ including normality and\nCohen-Macaulayness. Our result is complementary to what and Mandel and Qin have\nshown in arXiv:2301.11101 for surface with marked points, based on\narXiv:1411.1394 . The once-punctured case where cluster structures are\nsignificantly different is also discussed in the paper, and relevant\nconjectures are proposed (and proved in the once-punctured torus case).\n  By contrast, we define the ordinary cluster algebra with potentials added\n$A(\\Sigma)[v_i^{\\pm1}]$, introduced by Shen, Sun and Weng, which is shown to be\nusually smaller than $Sk^{RY}$ and $U(\\Sigma)[v_i^{\\pm1}]$. This strengthen the\nresult of arXiv:2201.08833 that the classical $A=U$ fails for $\\Sigma_{g,p}$\nwith $g\\geq 1, p\\geq 1$.",
        "Since the introduction of Vision Transformer (ViT), patchification has long\nbeen regarded as a de facto image tokenization approach for plain visual\narchitectures. By compressing the spatial size of images, this approach can\neffectively shorten the token sequence and reduce the computational cost of\nViT-like plain architectures. In this work, we aim to thoroughly examine the\ninformation loss caused by this patchification-based compressive encoding\nparadigm and how it affects visual understanding. We conduct extensive patch\nsize scaling experiments and excitedly observe an intriguing scaling law in\npatchification: the models can consistently benefit from decreased patch sizes\nand attain improved predictive performance, until it reaches the minimum patch\nsize of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable\nacross different vision tasks, various input scales, and diverse architectures\nsuch as ViT and the recent Mamba models. Moreover, as a by-product, we discover\nthat with smaller patches, task-specific decoder heads become less critical for\ndense prediction. In the experiments, we successfully scale up the visual\nsequence to an exceptional length of 50,176 tokens, achieving a competitive\ntest accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We\nhope this study can provide insights and theoretical foundations for future\nworks of building non-compressive vision models. Code is available at\nhttps:\/\/github.com\/wangf3014\/Patch_Scaling.",
        "The low-altitude economy (LAE) plays an indispensable role in cargo\ntransportation, healthcare, infrastructure inspection, and especially\npost-disaster communication. Specifically, unmanned aerial vehicles (UAVs), as\none of the core technologies of the LAE, can be deployed to provide\ncommunication coverage, facilitate data collection, and relay data for trapped\nusers, thereby significantly enhancing the efficiency of post-disaster response\nefforts. In this paper, we design an efficient and robust UAV-swarm enabled\ncollaborative self-organizing network to facilitate post-disaster\ncommunications. Specifically, a ground device transmits data to UAV swarms,\nwhich then use collaborative beamforming (CB) technique to form virtual antenna\narrays and relay the data to a remote access point (AP) efficiently. Then, we\nformulate a rescue-oriented post-disaster transmission rate maximization\noptimization problem (RPTRMOP). Then, we propose a two-stage optimization\napproach to address it. In the first stage, the optimal traffic routing and the\ntheoretical upper bound on the transmission rate of the network are derived. In\nthe second stage, we transform the formulated RPTRMOP into a variant named\nV-RPTRMOP, and a diffusion model-enabled particle swarm optimization (DM-PSO)\nalgorithm is proposed to deal with the V-RPTRMOP. Simulation results show the\neffectiveness of the proposed two-stage optimization approach in improving the\ntransmission rate of the constructed network, which demonstrates the great\npotential for post-disaster communications. Moreover, the robustness of the\nconstructed network is also validated via evaluating the impact of two\nunexpected situations on the system transmission rate.",
        "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
        "In this study, we investigate the influence of humidity on the performance of\nvarious non-resistive Micro Pattern Gaseous Detectors, such as GEM, Thick-GEM,\nand Micromegas, operated with Ar-CO$_2$ (90-10) gas mixture. The water content\nis introduced in a range of $0-5000~ppm_{\\mathrm{V}}$. It is observed that the\npresence of increased humidity does not significantly degrade any of the\nstudied performance criteria. On the contrary, our measurements suggest an\nimprovement in discharge stability with increasing humidity levels at the\nhighest gains and fields. No significant difference is observed at the lower\ngains, indicating that humidity helps to reduce the rate of spurious discharges\nrelated to electrode defects or charging-up of the insulating layers. We\nconclude that adding a small amount of water to the gas mixture may be\nbeneficial for the stable operation of an MPGD.",
        "Dispersal is essential to the plethora of motile microorganisms living in\nporous environments, yet how it relates to movement patterns and pore space\nstructure remains largely unknown. Here we investigate numerically the\nlong-time dispersal of a run-and-tumble microorganism that remains trapped at\nsolid surfaces and escapes from them by tumbling. We find that dispersal and\nmean run time are connected by a universal relation, that applies for a variety\nof porous microstructures and swimming strategies. We explain how this generic\ndependence originates in the invariance of the mean free path with respect to\nthe movement pattern, and we discuss the optimal strategy that maximizes\ndispersal. Finally, we extend our approach to microorganisms moving along the\nsurface. Our results provide a general framework to quantify dispersal that\nworks across the vast diversity of movement patterns and porous media.",
        "Octa-penta graphene (OPG), a novel carbon allotrope characterized by its\ndistinctive arrangement of pentagonal and octagonal rings, has garnered\nconsiderable attention due to its exceptional structure and functional\nproperties. This study systematically investigates the oxidation mechanisms of\nOPG and elucidates the oxygen migration patterns on the OPG monolayer through\nfirst-principles calculations and machine-learning-based molecular dynamics\n(MLMD) simulations. Specifically, the oxidation processes on OPG-L and OPG-Z\ninvolve exothermic chemisorption, where oxygen molecules dissociate at the\nsurfaces, forming stable epoxy groups. Furthermore, the integrated-crystal\norbital Hamilton population (ICOHP) and Bader charge analyses provide insights\ninto the physical mechanisms of oxygen atom adsorption. Importantly, we found\nthat oxidation also impact the electronic properties of OPG, with OPG-L\nretaining its metallic characteristics post-oxygen adsorption, whereas OPG-Z\nundergoes a transformation from a metallic to a semiconducting state due to the\nintroduction of oxygen. Oxygen migration on OPG monolayer involves breaking and\nreforming of C-O bonds, with varying stability across adsorption sites and\nlimited migration along the basal plane. MLMD simulations corroborate these\nmigration patterns, offering detailed migration trajectories consistent with\ntheoretical predictions. These findings enhance the understanding of oxygen\nmigration dynamics on OPG, facilitate its experimental validations, and\nhighlight its potential as a novel 2D material for applications in batteries,\nheat-resistant materials, and oxidation-resistant coatings.",
        "A word $w$ is said to be concise in a class of groups if, for every $G$ in\nthat class such that the set of $w$-values $w\\{G\\}$ is finite, the verbal\nsubgroup $w(G)$ is also finite. In the context of profinite groups, the notion\nof strong conciseness imposes a more demanding condition on $w$, requiring that\n$w(G)$ is finite whenever $|w\\{G\\}|< 2^{\\aleph_0}$. We investigate the relation\nbetween these two properties and the notion of equationally Noetherian groups,\nby proving that in a profinite group $G$ with a dense equationally Noetherian\nsubgroup, $w\\{G\\}$ is finite whenever $|w\\{G\\}|< 2^{\\aleph_0}$. Consequently,\nwe conclude that every word is strongly concise in the classes of profinite\nlinear groups, pro-$\\mathcal{C}$ completions of residually $\\mathcal{C}$ linear\ngroups and pro-$\\mathcal{C}$ completions of virtually abelian-by-polycyclic\ngroups, thereby extending well-known conciseness properties of these classes of\ngroups.",
        "Automated decision systems (ADS) are broadly deployed to inform and support\nhuman decision-making across a wide range of consequential settings. However,\nvarious context-specific details complicate the goal of establishing meaningful\nexperimental evaluations for prediction-based interventions. Notably, current\nexperiment designs rely on simplifying assumptions about human decision making\nin order to derive causal estimates. In reality, specific experimental design\ndecisions may induce cognitive biases in human decision makers, which could\nthen significantly alter the observed effect sizes of the prediction\nintervention. In this paper, we formalize and investigate various models of\nhuman decision-making in the presence of a predictive model aid. We show that\neach of these behavioural models produces dependencies across decision subjects\nand results in the violation of existing assumptions, with consequences for\ntreatment effect estimation. This work aims to further advance the scientific\nvalidity of intervention-based evaluation schemes for the assessment of ADS\ndeployments.",
        "We show that Hinich's simplicial nerve of the differential graded Lie algebra\n(DGLA) of derived derivations of a dg algebra $A$ over a dg properad\n$\\mathcal{P}$ is equivalent to the space of deformations of $A$ as a\n$\\mathcal{P}_{\\infty}$-algebra in Positselski's contraderived dg category. This\nresolves Hinich's counterexamples to the general existence of derived\ndeformations. It also generalises his results when $A$ is homologically bounded\nbelow, since contraderived deformations are then precisely derived\ndeformations.",
        "While systems designed for solving planning tasks vastly outperform Large\nLanguage Models (LLMs) in this domain, they usually discard the rich semantic\ninformation embedded within task descriptions. In contrast, LLMs possess\nparametrised knowledge across a wide range of topics, enabling them to leverage\nthe natural language descriptions of planning tasks in their solutions.\nHowever, current research in this direction faces challenges in generating\ncorrect and executable plans. Furthermore, these approaches depend on the LLM\nto output solutions in an intermediate language, which must be translated into\nthe representation language of the planning task. We introduce a novel planning\nmethod, which leverages the parametrised knowledge of LLMs by using their\noutput as a heuristic for Hill-Climbing Search. This approach is further\nenhanced by prompting the LLM to generate a solution estimate to guide the\nsearch. Our method outperforms the task success rate of similar systems within\na common household environment by 22 percentage points, with consistently\nexecutable plans. All actions are encoded in their original representation,\ndemonstrating that strong results can be achieved without an intermediate\nlanguage, thus eliminating the need for a translation step.",
        "The classical kernel ridge regression problem aims to find the best fit for\nthe output $Y$ as a function of the input data $X\\in \\mathbb{R}^d$, with a\nfixed choice of regularization term imposed by a given choice of a reproducing\nkernel Hilbert space, such as a Sobolev space. Here we consider a\ngeneralization of the kernel ridge regression problem, by introducing an extra\nmatrix parameter $U$, which aims to detect the scale parameters and the feature\nvariables in the data, and thereby improve the efficiency of kernel ridge\nregression. This naturally leads to a nonlinear variational problem to optimize\nthe choice of $U$. We study various foundational mathematical aspects of this\nvariational problem, and in particular how this behaves in the presence of\nmultiscale structures in the data.",
        "Identifying governing equations in physical and biological systems from\ndatasets remains a long-standing challenge across various scientific\ndisciplines, providing mechanistic insights into complex system evolution.\nCommon methods like sparse identification of nonlinear dynamics (SINDy) often\nrely on precise derivative estimations, making them vulnerable to data scarcity\nand noise. This study presents a novel data-driven framework by integrating\nhigh order implicit Runge-Kutta methods (IRKs) with the sparse identification,\ntermed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity\nand noise by leveraging the lower stepsize constraint of IRKs. Two methods for\nincorporating IRKs into sparse regression are introduced: one employs iterative\nschemes for numerically solving nonlinear algebraic system of equations, while\nthe other utilizes deep neural networks to predict stage values of IRKs. The\nperformance of IRK-SINDy is demonstrated through numerical experiments on\nbenchmark problems with varied dynamical behaviors, including linear and\nnonlinear oscillators, the Lorenz system, and biologically relevant models like\npredator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results\nindicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy\nframework, particularly under conditions of extreme data scarcity and noise,\nyielding interpretable and generalizable models.",
        "The enigma surrounding the existence of black holes has recently been\nsubstantiated through the groundbreaking work of experimental physicists\n\\cite{genzel2024}. Exploring quantum systems under the gravitational influence\nof black holes has emerged as a pivotal area of research. Among the frontier\nworks in quantum information processing is the utilization of quantum states as\nquantum channels. A fundamental quantum information protocol is teleportation,\nin which two parties, Alice and Bob, share entangled states. In this protocol,\nthe sender, Alice, who holds an unknown qubit, utilizes local operations and\nclassical communication (LOCC) to recreate the qubit at the recipient's (Bob's)\nend. Notably, during the execution of this protocol, Alice loses the unknown\nqubit on her side. The teleportation protocol, originally proposed by Bennett\net al. \\cite{bennett1993}, has been extensively studied with various states and\nunder different physical setups. Researchers have explored both modifications\nto the protocol itself and the viability of various quantum states as\nteleportation channels. In this paper, we investigate whether bipartite mixed\nstates derived from two inequivalent classes of tripartite pure states,\nsubjected to the gravitational influence of two different black hole models,\ncan still serve as efficient quantum channels for teleportation. We emphasize\nthe teleportation fidelity of these states, a critical factor for determining\ntheir efficacy as quantum channels. Specifically, the fidelity must exceed the\nclassical limit of $\\frac{2}{3}$ to be considered effective \\cite{pop1994}. We\nconjecture that, even under the gravitational influence of black holes, the\nquantum characteristics of the given states are preserved, enabling them to\nfunction effectively as quantum channels for teleportation.",
        "Despite the plethora of AI-based algorithms developed for anomaly detection\nin radiology, subsequent integration into clinical setting is rarely evaluated.\nIn this work, we assess the applicability and utility of an AI-based model for\nbrain aneurysm detection comparing the performance of two readers with\ndifferent levels of experience (2 and 13 years). We aim to answer the following\nquestions: 1) Do the readers improve their performance when assisted by the AI\nalgorithm? 2) How much does the AI algorithm impact routine clinical workflow?\nWe reuse and enlarge our open-access, Time-Of-Flight Magnetic Resonance\nAngiography dataset (N=460). We use 360 subjects for training\/validating our\nalgorithm and 100 as unseen test set for the reading session. Even though our\nmodel reaches state-of-the-art results on the test set (sensitivity=74%, false\npositive rate=1.6), we show that neither the junior nor the senior reader\nsignificantly increase their sensitivity (p=0.59, p=1, respectively). In\naddition, we find that reading time for both readers is significantly higher in\nthe \"AI-assisted\" setting than in the \"Unassisted\" (+15 seconds, on average;\np=3x10^(-4) junior, p=3x10^(-5) senior). The confidence reported by the readers\nis unchanged across the two settings, indicating that the AI assistance does\nnot influence the certainty of the diagnosis. Our findings highlight the\nimportance of clinical validation of AI algorithms in a clinical setting\ninvolving radiologists. This study should serve as a reminder to the community\nto always examine the real-word effectiveness and workflow impact of proposed\nalgorithms.",
        "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B\/AdvBench, using less than a third of the generation time).",
        "Despite their impressive performance on multi-modal tasks, large\nvision-language models (LVLMs) tend to suffer from hallucinations. An important\ntype is object hallucination, where LVLMs generate objects that are\ninconsistent with the images shown to the model. Existing works typically\nattempt to quantify object hallucinations by detecting and measuring the\nfraction of hallucinated objects in generated captions. Additionally, more\nrecent work also measures object hallucinations by directly querying the LVLM\nwith binary questions about the presence of likely hallucinated objects based\non object statistics like top-k frequent objects and top-k co-occurring\nobjects. In this paper, we present Context-Aware Object Similarities (CAOS), a\nnovel approach for evaluating object hallucination in LVLMs using object\nstatistics as well as the generated captions. CAOS uniquely integrates object\nstatistics with semantic relationships between objects in captions and\nground-truth data. Moreover, existing approaches usually only detect and\nmeasure hallucinations belonging to a predetermined set of in-domain objects\n(typically the set of all ground-truth objects for the training dataset) and\nignore generated objects that are not part of this set, leading to\nunder-evaluation. To address this, we further employ language model--based\nobject recognition to detect potentially out-of-domain hallucinated objects and\nuse an ensemble of LVLMs for verifying the presence of such objects in the\nquery image. CAOS also examines the sequential dynamics of object generation,\nshedding light on how the order of object appearance influences hallucinations,\nand employs word embedding models to analyze the semantic reasons behind\nhallucinations. CAOS aims to offer a nuanced understanding of the hallucination\ntendencies of LVLMs by providing a systematic framework to identify and\ninterpret object hallucinations.",
        "The rapid development of deep neural networks (DNNs) is inherently\naccompanied by the problem of high computational costs. To tackle this\nchallenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising\ntechnology for balancing the latency and energy consumption of DNN inference by\nadjusting the computing frequency of processors. However, most existing models\nof DNN inference time are based on the CPU-DVFS technique, and directly\napplying the CPU-DVFS model to DNN inference on GPUs will lead to significant\nerrors in optimizing latency and energy consumption. In this paper, we propose\na DVFS-aware latency model to precisely characterize DNN inference time on\nGPUs. We first formulate the DNN inference time based on extensive experiment\nresults for different devices and analyze the impact of fitting parameters.\nThen by dividing DNNs into multiple blocks and obtaining the actual inference\ntime, the proposed model is further verified. Finally, we compare our proposed\nmodel with the CPU-DVFS model in two specific cases. Evaluation results\ndemonstrate that local inference optimization with our proposed model achieves\na reduction of no less than 66% and 69% in inference time and energy\nconsumption respectively. In addition, cooperative inference with our proposed\nmodel can improve the partition policy and reduce the energy consumption\ncompared to the CPU-DVFS model.",
        "Context. Typical uncertainties of ages determined for single star giants from\nisochrone fitting using single-epoch spectroscopy and photometry without any\nadditional constraints are 30-50 %. Binary systems, particularly double-lined\nspectroscopic (SB2) binaries, provide an opportunity to study the intricacies\nof internal stellar physics and better determine stellar parameters,\nparticularly the stellar age. Aims. By using the constraints from binarity and\nasteroseismology, we aim to obtain precise age and stellar parameters for the\nred giant-subgiant binary system KIC 9163796, a system with a mass ratio of\n1.015 but distinctly different positions in the Hertzsprung-Russell diagram\n(HRD). Methods. We compute a multidimensional model grid of individual stellar\nmodels. From different combinations of figures of merit, we use the constraints\ndrawn from binarity, spectroscopy, and asteroseismology to determine the\nstellar mass, chemical composition, and age of KIC 9163796. Results. Our\ncombined-modeling approach leads to an age estimation of the binary system KIC\n9163796 of 2.44$^{+0.25}_{-0.20}$ Gyr, which corresponds to a relative error in\nthe age of 9 %. Furthermore, we found both components exhibiting equal initial\nhelium abundance of 0.27 to 0.30, significantly higher than the primordial\nhelium abundance, and an initial heavy metal abundance below the spectroscopic\nvalue. The masses of our models are in agreement with masses derived from the\nasteroseismic scaling relations. Conclusions. By exploiting the unique,\ndistinct positions of KIC 9163796, we successfully demonstrated that combining\nasteroseismic and binary constraints leads to a significant improvement of\nprecision in age estimation, that have a relative error below 10% for a giant\nstar.",
        "Recent years have witnessed a surge in the development of protein structural\ntokenization methods, which chunk protein 3D structures into discrete or\ncontinuous representations. Structure tokenization enables the direct\napplication of powerful techniques like language modeling for protein\nstructures, and large multimodal models to integrate structures with protein\nsequences and functional texts. Despite the progress, the capabilities and\nlimitations of these methods remain poorly understood due to the lack of a\nunified evaluation framework. We first introduce StructTokenBench, a framework\nthat comprehensively evaluates the quality and efficiency of structure\ntokenizers, focusing on fine-grained local substructures rather than global\nstructures, as typical in existing benchmarks. Our evaluations reveal that no\nsingle model dominates all benchmarking perspectives. Observations of codebook\nunder-utilization led us to develop AminoAseed, a simple yet effective strategy\nthat enhances codebook gradient updates and optimally balances codebook size\nand dimension for improved tokenizer utilization and quality. Compared to the\nleading model ESM3, our method achieves an average of 6.31% performance\nimprovement across 24 supervised tasks, with sensitivity and utilization rates\nincreased by 12.83% and 124.03%, respectively.",
        "Twisted layered van-der-Waals materials often exhibit unique electronic and\noptical properties absent in their non-twisted counterparts. Unfortunately,\npredicting such properties is hindered by the difficulty in determining the\natomic structure in materials displaying large moir\\'e domains. Here, we\nintroduce a split machine-learned interatomic potential and dataset curation\napproach that separates intralayer and interlayer interactions and\nsignificantly improves model accuracy -- with a tenfold increase in energy and\nforce prediction accuracy relative to conventional models. We further\ndemonstrate that traditional MLIP validation metrics -- force and energy errors\n-- are inadequate for moir\\'e structures and develop a more holistic,\nphysically-motivated metric based on the distribution of stacking\nconfigurations. This metric effectively compares the entirety of large-scale\nmoir\\'e domains between two structures instead of relying on conventional\nmeasures evaluated on smaller commensurate cells. Finally, we establish that\none-dimensional instead of two-dimensional moir\\'e structures can serve as\nefficient surrogate systems for validating MLIPs, allowing for a practical\nmodel validation protocol against explicit DFT calculations. Applying our\nframework to HfS2\/GaS bilayers reveals that accurate structural predictions\ndirectly translate into reliable electronic properties. Our model-agnostic\napproach integrates seamlessly with various intralayer and interlayer\ninteraction models, enabling computationally tractable relaxation of moir\\'e\nmaterials, from bilayer to complex multilayers, with rigorously validated\naccuracy.",
        "Accretion supermassive black holes in the center of active galaxies usually\nproduce ``jets''-collimated bipolar outflows of relativistic particles.\nMagnetic fields near the black hole event horizon may play a crucial role in\nthe formation of jets\/outflows. Both theory and observation indicate that\njets\/outflows driven by centrally active supermassive black holes (SMBHs) have\na feedback effect on the overall properties of the host galaxies. Therefore,\nthe magnetic field is a key ingredient for the formation and evolution of\ngalaxies. Here we report a clear correlation between the magnetic field of jets\nand star formation rate (SFR) for a large sample of 96 galaxies hosting\nsupermassive black holes, which suggests that the star formation of active\ngalactic nuclei (AGN) host galaxies may be powered by the jets."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Rsna pneumonia detection challenge",
    "start_abstract":"In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b14",
        "b9"
      ],
      "title":[
        "Adding Conditional Control to Text-to-Image Diffusion Models",
        "Highresolution image synthesis with latent diffusion models"
      ],
      "abstract":[
        "We present ControlNet, a neural network architecture to add spatial conditioning controls large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large models, and reuses their deep robust encoding layers with billions of images as strong backbone learn diverse set conditional controls. The is connected \"zero convolutions\" (zero-initialized convolution layers) that progressively grow parameters from zero ensure no harmful noise could affect finetuning. test various controls, e.g., edges, depth, segmentation, human pose, etc., Stable Diffusion, using single or multiple conditions, without prompts. show training ControlNets small (<50k) (>1m) datasets. Extensive results may facilitate wider applications control image",
        "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Certified algebraic curve projections by path tracking",
        "Galactic-scale emission-line outflow from the radio-loud quasar 3C 191",
        "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
        "Representation Engineering for Large-Language Models: Survey and\n  Research Challenges",
        "Strong Lensing analysis of SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440,\n  two Powerful Cosmic Telescopes ($R_E > 40''$) from the SPT Clusters Sample",
        "Turing-Completeness and Undecidability in Coupled Nonlinear Optical\n  Resonators",
        "Cyclicity of Cowen-Douglas tuples",
        "Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds",
        "An end to end gluing construction for metrics of constant Q-curvature",
        "Percolation in a three-dimensional non-symmetric multi-color loop model",
        "Projected Spread Models",
        "The long-term solar variability, as reconstructed from historical\n  sources: Several case studies in the 17th -- 18th centuries",
        "IDEA: Image Description Enhanced CLIP-Adapter",
        "Least-Squares Problem Over Probability Measure Space",
        "Fair Vertex Problems Parameterized by Cluster Vertex Deletion",
        "An Improved Deep Learning Model for Word Embeddings Based Clustering for\n  Large Text Datasets",
        "Attacker Control and Bug Prioritization",
        "One-Class Domain Adaptation via Meta-Learning",
        "Time-Varying Coronary Artery Deformation: A Dynamic Skinning Framework\n  for Surgical Training",
        "CAMEL: Continuous Action Masking Enabled by Large Language Models for\n  Reinforcement Learning",
        "ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse\n  Points",
        "Advances in Set Function Learning: A Survey of Techniques and\n  Applications",
        "Constructing Merger Trees of Density Peaks Using Phase-Space Watershed\n  Segmentation Algorithm",
        "Two-Dimensional Deep ReLU CNN Approximation for Korobov Functions: A\n  Constructive Approach",
        "Magnetoelasticity in MnPt $L1_{0}$ system",
        "Recoil nuclear size corrections in hydrogenic systems",
        "Learning Humanoid Standing-up Control across Diverse Postures",
        "Anomalous Chern-Simons orbital magnetoelectric coupling of\n  three-dimensional Chern insulators: gauge-discontinuity formalism and\n  adiabatic pumping"
      ],
      "abstract":[
        "We present a certified algorithm that takes a smooth algebraic curve in\n$\\mathbb{R}^n$ and computes an isotopic approximation for a generic projection\nof the curve into $\\mathbb{R}^2$. Our algorithm is designed for curves given\nimplicitly by the zeros of $n-1$ polynomials, but it can be partially extended\nto parametrically defined curves. The main challenge in correctly computing the\nprojection is to guarantee the topological correctness of crossings in the\nprojection. Our approach combines certified path tracking and interval\narithmetic in a two-step procedure: first, we construct an approximation to the\ncurve in $\\mathbb{R}^n$, and, second, we refine the approximation until the\ntopological correctness of the projection can be guaranteed. We provide a\nproof-of-concept implementation illustrating the algorithm.",
        "Quasar feedback is routinely invoked as an indispensable ingredient in galaxy\nformation models. Galactic outflows are a crucial agent of quasar feedback that\nfrequently manifest themselves in absorption and emission lines. Measuring the\nsize and energetics of outflows based on absorption lines remains a challenge,\nand integral-field spectroscopy (IFS) mapping in emission lines is\ncomplementary. We present a VLT\/SINFONI IFS mapping of quasar 3C 191 at $z \\sim\n2$, in which the outflow has been analyzed in absorption line spectroscopy.\nThree components are found based on the morphology and kinetics of\n[OIII]-emitting gas: a unshifted component which consistent with the systemic\nredshift and the location of the nucleus, a blueshifted in the north, and a\nredshifted in the south. The latter two components have velocities $\\sim$ 600\nkm s$^{-1}$ and projected extents of 5 and 11 kpc, respectively, suggesting a\nbiconical outflow structure. The blueshifted component's velocity is consistent\nwith that derived from absorption lines. Using the electron density measured by\nthe absorption lines and the luminosity and velocity of [OIII] outflow, we\nderive the mass outflow rate to be $\\dot{M} \\sim $ 9.5-13.4 M$_\\odot$ yr$^{-1}$\nand kinetic luminosity $\\dot{E}_{\\rm kin}$ ~ 2.5-3.7 $\\times 10^{42}$ erg\ns$^{-1}$, consistent with absorption line analyses with VLT\/Xshooter spectrum.\nThe kinetic luminosity is only 0.01% of the bolometric luminosity, rendering a\nrelatively weak outflow compared to typical expectation for effective feedback.",
        "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
        "Large-language models are capable of completing a variety of tasks, but\nremain unpredictable and intractable. Representation engineering seeks to\nresolve this problem through a new approach utilizing samples of contrasting\ninputs to detect and edit high-level representations of concepts such as\nhonesty, harmfulness or power-seeking. We formalize the goals and methods of\nrepresentation engineering to present a cohesive picture of work in this\nemerging discipline. We compare it with alternative approaches, such as\nmechanistic interpretability, prompt-engineering and fine-tuning. We outline\nrisks such as performance decrease, compute time increases and steerability\nissues. We present a clear agenda for future research to build predictable,\ndynamic, safe and personalizable LLMs.",
        "We report the results from a study of two massive ($M_{500c} > 6.0 \\times\n10^{14} M_{\\odot}$) strong lensing clusters selected from the South Pole\nTelescope cluster survey for their high Einstein radius ($R_E > 40''$),\nSPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440. Ground-based and shallow HST imaging\nindicated extensive strong lensing evidence in these fields, with giant arcs\nspanning 18\\arcsec\\ and 31\\arcsec, respectively, motivating further space-based\nimaging followup. Here, we present multiband HST imaging and ground-based\nMagellan spectroscopy of the fields, from which we compile detailed strong\nlensing models. The lens models of SPT-CL\\,J2325$-$4111 and\nSPT-CL\\,J0049$-$2440 were optimized using 9, and 8 secure multiple-imaged\nsystems with a final image-plane rms of 0\\farcs63 and 0\\farcs73, respectively.\nFrom the lensing analysis, we measure the projected mass density within 500~kpc\nof $M(<500 ~{\\rm kpc}) = 7.30\\pm0.07 \\times 10^{14}$$M_{\\odot}$, and $M(<500\n~{\\rm kpc})=7.12^{+0.16}_{-0.19}\\times 10^{14}$ $M_{\\odot}$ for these two\nclusters, and a sub-halos mass ratio of $0.12\\pm{0.01}$ and\n$0.21^{+0.07}_{-0.05}$, respectively. Both clusters produce a large area with\nhigh magnification ($\\mu\\geq 3$) for a source at $z=9$, $A^{lens}_{| \\mu | \\geq\n3 }=4.93^{+0.03}_{-0.04} arcmin^2$, and $A^{lens}_{| \\mu | \\geq 3\n}=3.64^{+0.14}_{-0.10} arcmin^2$ respectively, placing them in the top tier of\nstrong lensing clusters. We conclude that these clusters are spectacular\nsightlines for further observations that will reduce the systematic\nuncertainties due to cosmic variance. This paper provides the community with\ntwo additional well-calibrated cosmic telescopes, as strong as the Frontier\nFields, suitable for studies of the highly magnified background Universe.",
        "Networks of coupled nonlinear optical resonators have emerged as an important\nclass of systems in ultrafast optical science, enabling richer and more complex\nnonlinear dynamics compared to their single-resonator or travelling-wave\ncounterparts. In recent years, these coupled nonlinear optical resonators have\nbeen applied as application-specific hardware accelerators for computing\napplications including combinatorial optimization and artificial intelligence.\nIn this work, we rigorously prove a fundamental result showing that coupled\nnonlinear optical resonators are Turing-complete computers, which endows them\nwith much greater computational power than previously thought. Furthermore, we\nshow that the minimum threshold of hardware complexity needed for\nTuring-completeness is surprisingly low, which has profound physical\nconsequences. In particular, we show that several problems of interest in the\nstudy of coupled nonlinear optical resonators are formally undecidable. These\ntheoretical findings can serve as the foundation for better understanding the\npromise of next-generation, ultrafast all-optical computers.",
        "The study of Cowen-Douglas operators involves not only operator-theoretic\ntools but also complex geometry on holomorphic vector bundles. By leveraging\nthe properties of holomorphic vector bundles, this paper investigates the\ncyclicity of Cowen-Douglas tuples and demonstrates conclusively that every such\ntuple is cyclic.",
        "We introduce the Riemannian Proximal Sampler, a method for sampling from\ndensities defined on Riemannian manifolds. The performance of this sampler\ncritically depends on two key oracles: the Manifold Brownian Increments (MBI)\noracle and the Riemannian Heat-kernel (RHK) oracle. We establish high-accuracy\nsampling guarantees for the Riemannian Proximal Sampler, showing that\ngenerating samples with $\\varepsilon$-accuracy requires\n$O(\\log(1\/\\varepsilon))$ iterations in Kullback-Leibler divergence assuming\naccess to exact oracles and $O(\\log^2(1\/\\varepsilon))$ iterations in the total\nvariation metric assuming access to sufficiently accurate inexact oracles.\nFurthermore, we present practical implementations of these oracles by\nleveraging heat-kernel truncation and Varadhan's asymptotics. In the latter\ncase, we interpret the Riemannian Proximal Sampler as a discretization of the\nentropy-regularized Riemannian Proximal Point Method on the associated\nWasserstein space. We provide preliminary numerical results that illustrate the\neffectiveness of the proposed methodology.",
        "We produce many new complete, constant Q-curvature metrics on finitely\npunctured spheres by gluing together known examples. In our construction we\ntruncate one end of each summand and glue the two summands together\n\"end-to-end,\" where we've truncated them. We use this construction to show that\nthe unmarked moduli space of solutions with a fixed number of punctures is\ntopologically nontrivial provided the number of punctures is at least four.",
        "We conducted Monte Carlo simulations to analyze the percolation transition of\na non-symmetric loop model on a regular three-dimensional lattice. We\ncalculated the critical exponents for the percolation transition of this model.\nThe percolation transition occurs at a temperature that is close to, but not\nexactly the thermal critical temperature. Our finite-size study on this model\nyielded a correlation length exponent that agrees with that of the\nthree-dimensional XY model with an error margin of six per cent.",
        "We present a disease transmission model that considers both explicit and\nnon-explicit factors. This approach is crucial for accurate prediction and\ncontrol of infectious disease spread. In this paper, we extend the spread model\nfrom our previous works \\cite{ban2021mathematical,ban2023randomspread,\nban2023mathematical, ban2023spread} to a projected spread model that considers\nboth hidden and explicit types. Additionally, we provide the spread rate for\nthe projected spread model corresponding to the topological and random models.\nFurthermore, examples and numerical results are provided to illustrate the\ntheory.",
        "On a centennial timescale, solar activity was quantified based on records of\ninstrumental sunspot observations. This article briefly discusses several\naspects of the recent archival investigations of historical sunspot records in\nthe 17th to 18th centuries. This article also reviews the recent updates for\nthe active day fraction and positions of the reported sunspot groups of the\nMaunder Minimum to show their significance within the observational history.\nThese archival investigations serve as base datasets for reconstructing solar\nactivity.",
        "CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https:\/\/github.com\/FourierAI\/IDEA.",
        "In this work, we investigate the variational problem $$\\rho_x^\\ast =\n\\text{argmin}_{\\rho_x} D(G\\#\\rho_x, \\rho_y)\\,, $$ where $D$ quantifies the\ndifference between two probability measures, and ${G}$ is a forward operator\nthat maps a variable $x$ to $y=G(x)$. This problem can be regarded as an\nanalogue of its counterpart in linear spaces (e.g., Euclidean spaces),\n$\\text{argmin}_x \\|G(x) - y\\|^2$. Similar to how the choice of norm $\\|\\cdot\\|$\ninfluences the optimizer in $\\mathbb R^d$ or other linear spaces, the minimizer\nin the probabilistic variational problem also depends on the choice of $D$. Our\nfindings reveal that using a $\\phi$-divergence for $D$ leads to the recovery of\na conditional distribution of $\\rho_y$, while employing the Wasserstein\ndistance results in the recovery of a marginal distribution.",
        "We study fair vertex problem metatheorems on graphs within the scope of\nstructural parameterization in parameterized complexity. Unlike typical graph\nproblems that seek the smallest set of vertices satisfying certain properties,\nthe goal here is to find such a set that does not contain too many vertices in\nany neighborhood of any vertex. Formally, the task is to find a set $X\\subseteq\nV(G)$ of fair cost $k$, i.e., such that for all $v\\in V(G)$ $|X\\cap N(v)|\\le\nk$. Recently, Knop, Masa\\v{r}\\'ik, and Toufar [MFCS 2019] showed that all fair\nMSO$_1$ definable problems can be solved in FPT time parameterized by the twin\ncover of a graph. They asked whether such a statement would be achievable for a\nmore general parameterization of cluster vertex deletion, i.e., the smallest\nnumber of vertices required to be removed from the graph such that what remains\nis a collection of cliques.\n  In this paper, we prove that in full generality this is not possible by\ndemonstrating a W[1]-hardness. On the other hand, we give a sufficient property\nunder which a fair MSO$_1$ definable problem admits an FPT algorithm\nparameterized by the cluster vertex deletion number. Our algorithmic\nformulation is very general as it captures the fair variant of many natural\nvertex problems such as the Fair Feedback Vertex Set, the Fair Vertex Cover,\nthe Fair Dominating Set, the Fair Odd Cycle Transversal, as well as a connected\nvariant of thereof. Moreover, we solve the Fair $[\\sigma,\\rho]$-Domination\nproblem for $\\sigma$ finite, or $\\sigma=\\mathbb{N}$ and $\\rho$ cofinite.\nSpecifically, given finite or cofinite $\\rho\\subseteq \\mathbb{N}$ and finite\n$\\sigma$, or $\\rho\\subseteq \\mathbb{N}$ cofinite and $\\sigma=\\mathbb{N}$, the\ntask is to find set of vertices $X\\subseteq V(G)$ of fair cost at most $k$ such\nthat for all $v\\in X$, $|N(v)\\cap X|\\in\\sigma$ and for all $v\\in V(G)\\setminus\nX$, $|N(v)\\cap X|\\in\\rho$.",
        "In this paper, an improved clustering technique for large textual datasets by\nleveraging fine-tuned word embeddings is presented. WEClustering technique is\nused as the base model. WEClustering model is fur-ther improvements\nincorporating fine-tuning contextual embeddings, advanced dimensionality\nreduction methods, and optimization of clustering algorithms. Experimental\nresults on benchmark datasets demon-strate significant improvements in\nclustering metrics such as silhouette score, purity, and adjusted rand index\n(ARI). An increase of 45% and 67% of median silhouette score is reported for\nthe proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based\non Agglomerative models), respec-tively. The proposed technique will help to\nbridge the gap between semantic understanding and statistical robustness for\nlarge-scale text-mining tasks.",
        "As bug-finding methods improve, bug-fixing capabilities are exceeded,\nresulting in an accumulation of potential vulnerabilities. There is thus a need\nfor efficient and precise bug prioritization based on exploitability. In this\nwork, we explore the notion of control of an attacker over a vulnerability's\nparameters, which is an often overlooked factor of exploitability. We show that\ntaint as well as straightforward qualitative and quantitative notions of\ncontrol are not enough to effectively differentiate vulnerabilities. Instead,\nwe propose to focus analysis on feasible value sets, which we call domains of\ncontrol, in order to better take into account threat models and expert insight.\nOur new Shrink and Split algorithm efficiently extracts domains of control from\npath constraints obtained with symbolic execution and renders them in an easily\nprocessed, human-readable form. This in turn allows to automatically compute\nmore complex control metrics, such as weighted Quantitative Control, which\nfactors in the varying threat levels of different values. Experiments show that\nour method is both efficient and precise. In particular, it is the only one\nable to distinguish between vulnerabilities such as cve-2019-14192 and\ncve-2022-30552, while revealing a mistake in the human evaluation of\ncve-2022-30790. The high degree of automation of our tool also brings us closer\nto a fully-automated evaluation pipeline.",
        "The deployment of IoT (Internet of Things) sensor-based machine learning\nmodels in industrial systems for anomaly classification tasks poses significant\nchallenges due to distribution shifts, as the training data acquired in\ncontrolled laboratory settings may significantly differ from real-time data in\nproduction environments. Furthermore, many real-world applications cannot\nprovide a substantial number of labeled examples for each anomalous class in\nevery new environment. It is therefore crucial to develop adaptable machine\nlearning models that can be effectively transferred from one environment to\nanother, enabling rapid adaptation using normal operational data. We extended\nthis problem setting to an arbitrary classification task and formulated the\none-class domain adaptation (OC-DA) problem setting. We took a meta-learning\napproach to tackle the challenge of OC-DA, and proposed a task sampling\nstrategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified\nthe well-established model-agnostic meta-learning (MAML) algorithm and\nintroduced the OC-DA MAML algorithm. We provided a theoretical analysis showing\nthat OC-DA MAML optimizes for meta-parameters that enable rapid one-class\nadaptation across domains. The OC-DA MAML algorithm is evaluated on the\nRainbow-MNIST meta-learning benchmark and on a real-world dataset of\nvibration-based sensor readings. The results show that OC-DA MAML significantly\nimproves the performance on the target domains and outperforms MAML using the\nstandard task sampling strategy.",
        "Purpose: This study proposes a novel anatomically-driven dynamic modeling\nframework for coronary arteries using skeletal skinning weights computation,\naiming to achieve precise control over vessel deformation while maintaining\nreal-time performance for surgical simulation applications. Methods: We\ndeveloped a computational framework based on biharmonic energy minimization for\nskinning weight calculation, incorporating volumetric discretization through\ntetrahedral mesh generation. The method implements temporal sampling and\ninterpolation for continuous vessel deformation throughout the cardiac cycle,\nwith mechanical constraints and volume conservation enforcement. The framework\nwas validated using clinical datasets from 5 patients, comparing interpolated\ndeformation results against ground truth data obtained from frame-by-frame\nsegmentation across cardiac phases. Results: The proposed framework effectively\nhandled interactive vessel manipulation. Geometric accuracy evaluation showed\nmean Hausdorff distance of 4.96 +- 1.78 mm and mean surface distance of 1.78 +-\n0.75 mm between interpolated meshes and ground truth models. The Branch\nCompleteness Ratio achieved 1.82 +- 0.46, while Branch Continuity Score\nmaintained 0.84 +- 0.06 (scale 0-1) across all datasets. The system\ndemonstrated capability in supporting real-time guidewire-vessel collision\ndetection and contrast medium flow simulation throughout the complete coronary\ntree structure. Conclusion: Our skinning weight-based methodology enhances\nmodel interactivity and applicability while maintaining geometric accuracy. The\nframework provides a more flexible technical foundation for virtual surgical\ntraining systems, demonstrating promising potential for both clinical practice\nand medical education applications. The code is available at\nhttps:\/\/github.com\/ipoirot\/DynamicArtery.",
        "Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.",
        "We introduce ArcPro, a novel learning framework built on architectural\nprograms to recover structured 3D abstractions from highly sparse and\nlow-quality point clouds. Specifically, we design a domain-specific language\n(DSL) to hierarchically represent building structures as a program, which can\nbe efficiently converted into a mesh. We bridge feedforward and inverse\nprocedural modeling by using a feedforward process for training data synthesis,\nallowing the network to make reverse predictions. We train an encoder-decoder\non the points-program pairs to establish a mapping from unstructured point\nclouds to architectural programs, where a 3D convolutional encoder extracts\npoint cloud features and a transformer decoder autoregressively predicts the\nprograms in a tokenized form. Inference by our method is highly efficient and\nproduces plausible and faithful 3D abstractions. Comprehensive experiments\ndemonstrate that ArcPro outperforms both traditional architectural proxy\nreconstruction and learning-based abstraction methods. We further explore its\npotential to work with multi-view image and natural language inputs.",
        "Set function learning has emerged as a crucial area in machine learning,\naddressing the challenge of modeling functions that take sets as inputs. Unlike\ntraditional machine learning that involves fixed-size input vectors where the\norder of features matters, set function learning demands methods that are\ninvariant to permutations of the input set, presenting a unique and complex\nproblem. This survey provides a comprehensive overview of the current\ndevelopment in set function learning, covering foundational theories, key\nmethodologies, and diverse applications. We categorize and discuss existing\napproaches, focusing on deep learning approaches, such as DeepSets and Set\nTransformer based methods, as well as other notable alternative methods beyond\ndeep learning, offering a complete view of current models. We also introduce\nvarious applications and relevant datasets, such as point cloud processing and\nmulti-label classification, highlighting the significant progress achieved by\nset function learning methods in these domains. Finally, we conclude by\nsummarizing the current state of set function learning approaches and\nidentifying promising future research directions, aiming to guide and inspire\nfurther advancements in this promising field.",
        "Structure identification in cosmological simulations plays an important role\nin analysing simulation outputs. The definition of these structures directly\nimpacts the inferred properties derived from these simulations. This paper\nproposes a more straightforward definition and model of structure by focusing\non density peaks rather than halos and clumps. It introduces a new watershed\nalgorithm that uses phase-space analysis to identify structures, especially in\ncomplex environments where traditional methods may struggle due to spatially\noverlapping structures. Additionally, a merger tree code is introduced to track\ndensity peaks across timesteps, making use of the boosted potential for\nidentifying the most bound particles for each peak.",
        "This paper investigates approximation capabilities of two-dimensional (2D)\ndeep convolutional neural networks (CNNs), with Korobov functions serving as a\nbenchmark. We focus on 2D CNNs, comprising multi-channel convolutional layers\nwith zero-padding and ReLU activations, followed by a fully connected layer. We\npropose a fully constructive approach for building 2D CNNs to approximate\nKorobov functions and provide rigorous analysis of the complexity of the\nconstructed networks. Our results demonstrate that 2D CNNs achieve near-optimal\napproximation rates under the continuous weight selection model, significantly\nalleviating the curse of dimensionality. This work provides a solid theoretical\nfoundation for 2D CNNs and illustrates their potential for broader applications\nin function approximation.",
        "Magnetic materials play an important role in the industry. Except for common\nmaterial parameters such as magnetocrystalline anisotropy, magnetoelastic\nbehavior can be significant for application serving in various devices, e.g. in\nacoustic actuators, transducers, or sensors providing desirable fast response\nand high efficiency. Magnetoelasticit properties have been studied for the\ncubic system. However, their magnetoelastic behavior is weak. Therefore, we\nfocus on Pt-based tetragonal binary compounds bearing engaging magnetic\nbehavior. We evaluate MnPt magnetoelastic behavior using the finite\ndisplacement method, discussing the influence of the magnetic structure, and\nfocusing on the origin. While, the theoretically obtained results are compared\nto our performed experiments.",
        "Formulas for the combined nuclear-recoil and finite-nuclear-size effects of\norder $(Z\\,\\alpha)^5$ and $(Z\\,\\alpha)^6$ are derived without any expansion in\nthe nuclear charge radius $r_C$, making them applicable to both electronic and\nmuonic atoms. The obtained results are particularly relevant for high-precision\ndeterminations of root-mean-square charge radii from muonic atom spectroscopy.\nWe demonstrate that calculations of the atomic isotope shift based on the\nwidely used Breit approximation give rise to an unphysical nuclear-size\ncontribution that is linear in the nuclear charge radius $r_C$ at order\n$(Z\\,\\alpha)^5$. This spurious term vanishes in a full QED treatment, leaving\nthe correct contribution quadratic in $r_C$. For electronic atoms, this\nquadratic term is significantly smaller than the spurious linear contribution.",
        "Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps:\/\/taohuang13.github.io\/humanoid-standingup.github.io\/.",
        "Chern-Simons orbital magnetoelectric (OME) coupling is usually the hallmark\nof nontrivial band topology in three-dimensional (3D) crystalline insulators.\nHowever, if a 3D insulator exhibits nonzero Chern number within any\ntwo-dimensional plane of the Brillouin zone, then traditionally the\nChern-Simons coupling becomes ill defined for such 3D Chern insulators due to\ntopological obstructions. In this work, by employing a ``gauge-discontinuity\"\nformalism, we resolve this long-standing issue and rigorously derive a\nquantized layer-resolved OME response in 3D Chern insulators. We demonstrate\nthat the difference of the layer-resolved OME coupling between adjacent layers\nis universally quantized in unit of $-C e^2\/h$, where $C$ is the Chern number.\nThis quantization arises from an anomalous contribution to the Chern-Simons OME\ncoupling, which is closely associated with the Berry curvature of the occupied\nbands and the hybrid Wannier centers along the direction of the Chern vector\n$(0,0, C)$. Furthermore, we demonstrate that the anomalous Chern-Simons\ncoupling can be transported by an exact integer quantum from one unit cell to\nits neighboring cell through an adiabatic cyclic pumping process, accompanied\nby a quantized displacement of Wannier center along the direction of the Chern\nvector. Our work provides a rigorous theoretical framework for understanding\nmagnetoelectric response in 3D Chern insulators and opens avenues for designing\ntopological quantum phenomena in layered systems."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Adding Conditional Control to Text-to-Image Diffusion Models",
    "start_abstract":"We present ControlNet, a neural network architecture to add spatial conditioning controls large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large models, and reuses their deep robust encoding layers with billions of images as strong backbone learn diverse set conditional controls. The is connected \"zero convolutions\" (zero-initialized convolution layers) that progressively grow parameters from zero ensure no harmful noise could affect finetuning. test various controls, e.g., edges, depth, segmentation, human pose, etc., Stable Diffusion, using single or multiple conditions, without prompts. show training ControlNets small (<50k) (>1m) datasets. Extensive results may facilitate wider applications control image",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Rsna pneumonia detection challenge"
      ],
      "abstract":[
        "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "The relationship between galaxy size and halo properties: Insights from\n  the IllustrisTNG simulations and differential clustering",
        "A Geometric Interpretation of Virtual Knotoids",
        "A simple model for entangled photon generation in resonant structures",
        "Validating uncertainty propagation approaches for two-stage Bayesian\n  spatial models using simulation-based calibration",
        "A versatile experimental method to measure the traction forces at\n  interfaces",
        "Spin-dependent dark matter scattering in quasi-two-dimensional magnets",
        "New Construction of Locally q-ary Sequential Recoverable Codes:\n  Parity-check Matrix Approach",
        "A higher algebraic approach to liftings of modules over derived\n  quotients",
        "Second-Order $\\Gamma$-Limit for the Cahn-Hilliard Functional with\n  Dirichlet Boundary Conditions, I",
        "Transmission Probability in Double Quantum Well with Triple Barrier",
        "Dynamic Circuits for the Quantum Lattice-Boltzmann Method",
        "Unveiling individual and collective temporal patterns in the tanker\n  shipping network",
        "Tracking time-varying signals with quantum-enhanced atomic magnetometers",
        "Duality viewpoint of noninvertible symmetry protected topological phases",
        "Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov\n  Decision Processes",
        "Evolution of the pseudogap band structure in a system of\n  electron-correlated lattice polarons",
        "Semiclassical trace formula for the Bochner-Schr\\\"odinger operator",
        "Geometry-Driven Moir\\'e Engineering in Twisted Bilayers of\n  High-Pseudospin Fermions",
        "Detecting LHC Neutrinos at Surface Level",
        "Critical quasilinear equations on Riemannian manifolds",
        "Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance",
        "5G Channel Models for Railway Use Cases at mmWave Band and the Path\n  Towards Terahertz",
        "Soliquidy: a descriptor for atomic geometrical confusion",
        "The Venusian Chronicles",
        "Improved amplitude amplification strategies for the quantum simulation\n  of classical transport problems",
        "Optimal regulation in a periodic environment: insights from a simple\n  model",
        "Quantum state exclusion for group-generated ensembles of pure states",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Search for the Cabibbo-suppressed decays\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{0}$ and\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$"
      ],
      "abstract":[
        "The physical origin of the radial sizes of galaxies and how galaxy sizes are\ncorrelated with the properties of their host dark matter halos is an open\nquestion in galaxy formation. In observations, the large-scale clustering of\ngalaxies selected by stellar mass is significantly different for large and\nsmall galaxies, and Behroozi et al. (2022) showed that these results are in\ntension with some of the correlations between galaxy size and halo properties\nin the literature. We analyze the IllustrisTNG suite of large volume\ncosmological hydrodynamic simulations along with dark matter only simulations\nwith matched initial conditions. We investigate correlations between the ratio\nof galaxy size to halo virial radius ($r_{\\rm gal}\/R_{\\rm vir}$) and halo spin,\nconcentration, and formation time at redshift 0-3. We find a significant\ncorrelation between $r_{\\rm gal}\/R_{\\rm vir}$ and concentration, but only above\na critical value $c \\simeq 16$, and we also find a correlation between $r_{\\rm\ngal}\/R_{\\rm vir}$ and halo formation time. We suggest that galaxy formation\nhistory and environment, in addition to halo properties at a given output time,\nplay an important role in shaping galaxy size. In addition, we directly measure\nsize-based differential clustering in the TNG300 simulation and compare\ndirectly with the observational results. We find significant scale-dependent\nsize-based differential clustering in TNG, in qualitative agreement with\nobservations. However, correlations between $r_{\\rm gal}\/R_{\\rm vir}$ and\nsecondary halo properties are not the drivers of the differential clustering in\nthe simulations; instead, we find that most of this signal in TNG arises from\nsatellite galaxies.",
        "In this paper, we give a three-dimensional geometric interpretation of\nvirtual knotoids. Then we show that virtual knotoid theory is a generalization\nof classical knotoid theory by proving a conjecture of Kauffman and the first\nauthor given in arXiv:1602.03579.",
        "The ability to engineer pairs of entangled photons is essential to quantum\ninformation science, and generating these states using spontaneous parametric\ndown-conversion (SPDC) in nano- and micrometer-scale materials offers numerous\nadvantages. To properly engineer such sources, a reliable model describing\nnano- and micrometer-scale SPDC is necessary; however, such a theoretical\ndescription remains a challenge. Here, we propose and derive a simplified model\nto describe SPDC in resonant structures, which considers the generation of\nphoton pairs and the resonant enhancement of spectral bands to be separate\nprocesses, even though they actually occur simultaneously. We compare our\nsimplified model to both the rigorous theory of SPDC in an etalon - a simple\nexample of a resonant structure - and our experiments on SPDC in etalons and\nfind agreement for low-gain SPDC. By simplifying the calculations required to\ngenerate photon pairs, our model promises to make designing complex resonant\nstructures easier, and it promises to hasten the iteration of designs across\nthe field of quantum state engineering.",
        "This work tackles the problem of uncertainty propagation in two-stage\nBayesian models, with a focus on spatial applications. A two-stage modeling\nframework has the advantage of being more computationally efficient than a\nfully Bayesian approach when the first-stage model is already complex in\nitself, and avoids the potential problem of unwanted feedback effects. Two ways\nof doing two-stage modeling are the crude plug-in method and the posterior\nsampling method. The former ignores the uncertainty in the first-stage model,\nwhile the latter can be computationally expensive. This paper validates the two\naforementioned approaches and proposes a new approach to do uncertainty\npropagation, which we call the $\\mathbf{Q}$ uncertainty method, implemented\nusing the Integrated Nested Laplace Approximation (INLA). We validate the\ndifferent approaches using the simulation-based calibration method, which tests\nthe self-consistency property of Bayesian models. Results show that the crude\nplug-in method underestimates the true posterior uncertainty in the\nsecond-stage model parameters, while the resampling approach and the proposed\nmethod are correct. We illustrate the approaches in a real life data\napplication which aims to link relative humidity and Dengue cases in the\nPhilippines for August 2018.",
        "Measurement of surface forces, including cohesive forces and contact forces,\nis critical for understanding and controlling interactions at interfaces to\noptimize the interfacial performance of applications. The objective of this\npaper is to introduce a general in-situ method that enables the measurement of\n3D micron-scale displacements and corresponding force distribution at\ninterfaces in dry or wet environment. Stereo digital image correlation was used\nto measure the 3D-displacement of a soft and deformable substrate. The\nefficiency and accuracy of the technique were evaluated by applying compression\nto the substrate using a steel ball, with the measured 3D displacements\naligning closely with finite element analysis simulations. To further assess\nthe method's applicability, the wet adhesion between mussel plaques and\nsubstrate was tested under aqueous conditions. The interfacial displacements\nand forces at different stages during the test were measured. The application\nof the technique can be extended for varied circumstances regarding force range\nand substrate materials based on Winkler Spring model.",
        "We study the prospects of detecting dark matter coupled to the spin of the\nelectron, such that it may scatter and excite magnons - collective excitations\nof electronic spins. We show that materials exhibiting long-range magnetic\norder where the spins are coupled only along a plane may act as directional\ndark matter detectors. These quasi-2D materials possess anisotropic dispersion\nrelations and structure functions which induce a sidereal modulation in the\nexcitation rate. We calculate the expected signal rate for some candidate\n(anti)ferromagnets, demonstrating a possible route to the direct detection of\nspin-dependent dark matter in the keV to MeV mass range.",
        "This paper develops a new family of locally recoverable codes for distributed\nstorage systems, Sequential Locally Recoverable Codes (SLRCs) constructed to\nhandle multiple erasures in a sequential recovery approach. We propose a new\nconnection between parallel and sequential recovery, which leads to a general\nconstruction of q-ary linear codes with information $(r, t_i,\n\\delta)$-sequential-locality where each of the $i$-th information symbols is\ncontained in $t_i$ punctured subcodes with length $(r+\\delta-1)$ and minimum\ndistance $\\delta$. We prove that such codes are $(r, t)_q$-SLRC ($t \\geq \\delta\nt_i+1$), which implies that they permit sequential recovery for up to $t$\nerasures each one by $r$ other code symbols.",
        "We show a certain existence of a lifting of modules under the\nself-$\\mathrm{Ext}^2$-vanishing condition over the \"derived quotient\" by using\nthe notion of higher algebra. This refines a work of Auslander--Ding--Solberg's\nsolution of the Auslander--Reiten conjecture for complete interesctions.\nTogether with Auslander's zero-divisor theorem, we show that the existence of\nsuch $\\mathrm{Ext}$-vanishing module over derived quotients is equivalent to\nbeing local complete intersections.",
        "This paper addresses the asymptotic development of order 2 by the $\\Gamma$\n-convergence of the Cahn-Hilliard functional with Dirichlet boundary\nconditions. The Dirichlet data are assumed to be well separated from one of the\ntwo wells. In the case where there are no interfaces, it is shown that there is\na transition layer near the boundary of the domain.",
        "Quantum well of AlGaAs\/GaAs is very important to study transport properties\nof electrons due to its wider application in electronic devices. Hence, the\ndouble well of AlGaAs\/GaAs with triple barrier is taken to study transmission\nprobability. Transmission probability is found to decrease with the increase in\nthe height and width of the barrier. Transmission probability with energy of\nelectron shows two peaks while taking all three barrier of the same height.\nWhereas a single and higher value of peak is found when the height of the\ncentral barrier is slightly reduced.",
        "We propose a quantum algorithm for the linear advection-diffusion equation\n(ADE) Lattice-Boltzmann method (LBM) that leverages dynamic circuits. Dynamic\nquantum circuits allow for an optimized collision-operator quantum algorithm,\nintroducing partial measurements as an integral step. Efficient adaptation of\nthe quantum circuit during execution based on digital information obtained\nthrough mid-circuit measurements is achieved. The proposed new collision\nalgorithm is implemented as a fully unitary operator, which facilitates the\ncomputation of multiple time steps without state reinitialization. Unlike\nprevious quantum collision operators that rely on linear combinations of\nunitaries, the proposed algorithm does not exhibit a probabilistic failure\nrate. Moreover, additional qubits no longer depend on the chosen velocity set,\nwhich reduces both qubit overhead and circuit complexity. Validation of the\nquantum collision algorithm is performed by comparing results with digital LBM\nin one and two dimensions, demonstrating excellent agreement. Performance\nanalysis for multiple time steps highlights advantages compared to previous\nmethods. As an additional variant, a hybrid quantum-digital approach is\nproposed, which reduces the number of mid-circuit measurements, therefore\nimproving the efficiency of the quantum collision algorithm.",
        "The global shipping network, which moves over 80% of the world's goods, is\nnot only a vital backbone of the global economy but also one of the most\npolluting industries. Studying how this network operates is crucial for\nimproving its efficiency and sustainability. While the transport of solid goods\nlike packaged products and raw materials has been extensively researched, far\nless is known about the competitive trade of crude oil and petroleum, despite\nthese commodities accounting for nearly 30% of the market. Using 4 years of\nhigh-resolution data on oil tanker movements, we employ sequential motif mining\nand dynamic mode decomposition to uncover global spatio-temporal patterns in\nthe movement of individual ships. Across all ship classes, we demonstrate that\nmaximizing the proportion of time ships spend carrying cargo -- a metric of\nefficiency -- is achieved through strategic diversification of routes and the\neffective use of intra-regional ports for trips without cargo. Moreover, we\nuncover a globally stable travel structure in the fleet, with pronounced\nseasonal variations linked to annual and semi-annual regional climate patterns\nand economic cycles. Our findings highlight the importance of integrating\nhigh-resolution data with innovative analysis methods not only to improve our\nunderstanding of the underlying dynamics of shipping patterns, but to design\nand evaluate strategies aimed at reducing their environmental impact.",
        "Quantum entanglement, in the form of spin squeezing, is known to improve the\nsensitivity of atomic instruments to static or slowly-varying quantities.\nSensing transient events presents a distinct challenge, requires different\nanalysis methods, and has not been shown to benefit from entanglement in\npractically-important scenarios such as spin-precession magnetometry (SPM).\nHere we adapt estimation control techniques introduced in\narXiv:2403.14764(2024) to the experimental setting of SPM and analogous\ntechniques. We demonstrate that real-time tracking of fluctuating fields\nbenefits from measurement-induced spin-squeezing and that quantum limits\ndictated by decoherence are within reach of today's experiments. We illustrate\nthis quantum advantage by single-shot tracking, within the coherence time of a\nspin-precession magnetometer, of a magnetocardiography signal overlain with\nbroadband noise.",
        "Recent advancements in generalized symmetries have drawn significant\nattention to gapped phases of matter exhibiting novel symmetries, such as\nnoninvertible symmetries. By leveraging the duality transformations, the\nclassification and construction of gapped phases with noninvertible symmetry\ncan be mapped to those involving conventional group symmetries. We demonstrate\nthis approach by classifying symmetry-protected topological phases with a broad\nclass of noninvertible symmetries in arbitrary spacetime dimensions. Our\nresults reveal new classifications that extend beyond those based on group\nsymmetries. Additionally, we construct lattice models in $(1+1)d$ and $(2+1)d$\nthat realize these new phases and explore their anomalous interfaces.",
        "The impact of communication on decision-making systems has been extensively\nstudied under the assumption of dedicated communication channels. We instead\nconsider communicating through actions, where the message is embedded into the\nactions of an agent which interacts with the environment in a Markov decision\nprocess (MDP) framework. We conceptualize the MDP environment as a finite-state\nchannel (FSC), where the actions of the agent serve as the channel input, while\nthe states of the MDP observed by another agent (i.e., receiver) serve as the\nchannel output. Here, we treat the environment as a communication channel over\nwhich the agent communicates through its actions, while at the same time,\ntrying to maximize its reward. We first characterize the optimal information\ntheoretic trade-off between the average reward and the rate of reliable\ncommunication in the infinite-horizon regime. Then, we propose a novel\nframework to design a joint control\/coding policy, termed \\textit{Act2Comm},\nwhich seamlessly embeds messages into actions. From a communication\nperspective, \\textit{Act2Comm} functions as a learning-based channel coding\nscheme for non-differentiable FSCs under input-output constraints. From a\ncontrol standpoint, \\textit{Act2Comm} learns an MDP policy that incorporates\ncommunication capabilities, though at the cost of some control performance.\nOverall, \\textit{Act2Comm} effectively balances the dual objectives of control\nand communication in this environment. Experimental results validate\n\\textit{Act2Comm}'s capability to enable reliable communication while\nmaintaining a certain level of control performance.",
        "The evolution of the role of lattice vibrations in the formation of the\npseudogap state in strongly correlated electron systems has been investigated\nconcerning changes in the electron-phonon coupling parameters and the\nconcentration of doped charge carriers. We apply the polaronic version of the\ngeneralized tight-binding method to analyze the band structure of a realistic\nmultiband two-dimensional model that incorporates the electron-lattice\ncontributions of both Holstein and Peierls types. It has been demonstrated that\nthe emergence of polaronic effects begins with the modulation of spectral\nfunction intensity. However, within a specific region of the phase diagram, a\nsignificant transformation of the electron band structure and pseudogap state\noccurs. It results from coherent polaron excitations that create a partially\nflat band near the Fermi level. This process leads to a change in the topology\nof the Fermi surface and the emergence of corresponding features in the density\nof states.",
        "We study the semiclassical Bochner-Schr\\\"odinger operator\n$H_{p}=\\frac{1}{p^2}\\Delta^{L^p\\otimes E}+V$ on tensor powers $L^p$ of a\nHermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a\nRiemannian manifold of bounded geometry. For any function $\\varphi\\in\nC^\\infty_c(\\mathbb R)$, we consider the bounded linear operator $\\varphi(H_p)$\nin $L^2(X,L^p\\otimes E)$ defined by the spectral theorem. We prove that its\nsmooth Schwartz kernel on the diagonal admits a complete asymptotic expansion\nin powers of $p^{-1}$ in the semiclassical limit $p\\to \\infty$. In particular,\nwhen the manifold is compact, we get a complete asymptotic expansion for the\ntrace of $\\varphi(H_p)$.",
        "Moir\\'e engineering offers new pathways for manipulating emergent states in\ntwisted layered materials and lattice-mismatched heterostructures. With the key\nrole of the geometry of the underlying lattice in mind, here we introduce the\nwatermill lattice, a two-dimensional structure with low-energy states\ncharacterized by massless pseudospin-3\/2 fermions with high winding numbers.\nIts twisted bilayer is shown to exhibit magic angles, where four isolated flat\nbands emerge around the Fermi level, featuring elevated Wilson-loop windings\nand enhanced quantum geometric effects, such as an increase in the ratio of the\nBerezinskii-Kosterlitz-Thouless (BKT) transition temperature to the mean-field\ncritical temperature under a weak Bardeen-Cooper-Schrieffer (BCS) pairing. We\ndiscuss how the watermill lattice could be realized in the MXene and group-IV\nmaterials. Our study highlights the potential of exploiting lattice geometry in\nmoir\\'e engineering to uncover novel quantum phenomena and tailor emergent\nelectronic properties in materials.",
        "The first direct detection of neutrinos at the LHC not only marks the\nbeginning of a novel collider neutrino program at CERN but also motivates\nconsidering additional neutrino detectors to fully exploit the associated\nphysics potential. We investigate the feasibility and physics potential of\nneutrino experiments located at the surface-level. A topographic desk study was\nperformed to identify all points at which the LHC's neutrino beams exit the\nearth. The closest location lies about 9 km east of the CMS interaction point,\nat the bottom of Lake Geneva. Several detectors to be placed at this location\nare considered, including a water Cherenkov detector and an emulsion detector.\nThe detector concepts are introduced, and projections for their contribution to\nthe LHC forward neutrino program and searches for dark sector particles are\npresented. However, the dilution of the neutrino flux over distance reduces the\nneutrino yield significantly, limiting the physics potential of surface-level\ndetectors compared to ones closer to the interaction point, including the\nproposed FPF.",
        "In this paper, we investigate critical quasilinear elliptic partial\ndifferential equations on a complete Riemannian manifold with nonnegative Ricci\ncurvature. By exploiting a new and sharp nonlinear Kato inequality and\nestablishing some Cheng-Yau type gradient estimates for positive solutions, we\nclassify positive solutions to the critical $p$-Laplace equation and show\nrigidity concerning the ambient manifold. Our results extend and improve some\nprevious conclusions in the literature. Similar results are obtained for\nsolutions to the quasilinear Liouville equation involving the $n$-Laplace\noperator, where $n$ corresponds to the dimension of the ambient manifold.",
        "Neural network-based decoding methods have shown promise in enhancing error\ncorrection performance, but traditional approaches struggle with the challenges\nposed by punctured codes. In particular, these methods fail to address the\ncomplexities of variable code rates and the need for protocol compatibility.\nThis paper presents a unified Long Short-Term Memory (LSTM)-based decoding\narchitecture specifically designed to overcome these challenges. The proposed\nmethod unifies punctured convolutional and Turbo codes. A puncture embedding\nmechanism integrates puncturing patterns directly into the network, enabling\nseamless adaptation to varying code rates, while balanced bit error rate\ntraining ensures robustness across different code lengths, rates, and channels,\nmaintaining protocol flexibility. Extensive simulations in Additive White\nGaussian Noise and Rayleigh fading channels demonstrate that the proposed\napproach outperforms conventional decoding techniques, providing significant\nimprovements in decoding accuracy and robustness. These results underscore the\npotential of LSTM-based decoding as a promising solution for next-generation\nartificial intelligence powered communication systems.",
        "High-speed trains are one of the most relevant scenarios for the\nfifth-generation (5G) mobile communications and the \"smart rail mobility\"\nvision, where a high-data-rate wireless connectivity with up to several GHz\nbandwidths will be required. This is a strong motivation for the exploration of\nmillimeter wave (mmWave) band. In this article, we identify the main challenges\nand make progress towards realistic 5G mmWave channel models for railway use\ncases. In order to cope with the challenge of including the railway features in\nthe channel models, we define reference scenarios to help the parameterization\nof channel models for railway use at mmWave band. Simulations and the\nsubsequent measurements used to validate the model reflect the detailed\ninfluence of railway objects and the accuracy of the simulations. Finally, we\npoint out the future directions towards the full version of the smart rail\nmobility which will be powered by terahertz (THz) communications.",
        "Tailoring material properties often requires understanding the solidification\nprocess. Herein, we introduce the geometric descriptor Soliquidy, which\nnumerically captures the Euclidean transport cost between the translationally\ndisordered versus ordered states of a materials. As a testbed, we apply\nSoliquidy to the classification of glass-forming metal alloys. By extending and\ncombining an experimental library of metallic thin-films (glass\/no-glass) with\nthe aflow.org computational database (geometrical and energetic information of\nmixtures) we found that the combination of Soliquity and formation enthalpies\ngenerates an effective classifier for glass formation. Such classifier is then\nused to tackle a public dataset of metallic glasses showing that the\nglass-agnostic assumptions of Soliquity can be useful for understanding\nkinetically-controlled phase transitions.",
        "Venus' atmosphere -- specifically its clouds buoyed up 40 to 60 km above the\nsurface -- has long been suspected to encompass a biosphere where Earth-like\nliving organisms could grow and flourish. This idea has been recently rekindled\nby the observation (signal-to-noise ratio of about 15$\\sigma$) of a phosphine\n(PH$_3$) absorption-line profile against the thermal background from deeper,\nhotter layers of the atmosphere. There is a chance that this observation could\nbe a sign of life, because the PH$_3$ gas observed on Earth originates mostly\nin decaying organic material. Furthermore, it has been shown that there is no\nother natural process on Venus that could otherwise produce the observed PH$_3$\nabsorption line. On the other hand, cosmic rays and the particle cascades they\nproduce in the Earth's atmosphere are hazardous to living organisms, because\nthe ionizing radiation produced in air showers can blow apart chemical\nmolecules and disrupt biochemical processes, causing cells to die or undergo\ndangerous mutations. Compared to Earth, the hypothesized biosphere of Venus\ncould be exposed to substantially more ionizing radiation. This is because\nVenus has no protective intrinsic magnetic field, orbits closer to the Sun, and\nthe entire eventual habitable region lies in the clouds high in the atmosphere.\nThereby, if the clouds were sterilized there would be no reservoir of deeper\nlife that can recolonize afterwards. In this communication we study the effects\nof particle cascades in the venusian atmosphere using the AIRES simulation\npackage properly configured. We show that the effects of cosmic radiation in\nthe habitable zone would be comparable to those on the Earth's surface and so\nwould not have any hazardous effect on possible venusian microorganisms.",
        "The quantum simulation of classical fluids often involves the use of\nprobabilistic algorithms that encode the result of the dynamics in the form of\nthe amplitude of the selected quantum state. In most cases, however, the\namplitude probability is too low to allow an efficient use of these algorithms,\nthereby hindering the practical viability of the quantum simulation. The\noblivious amplitude amplification algorithm is often presented as a solution to\nthis problem, but to no avail for most classical problems, since its\napplicability is limited to unitary dynamics. In this paper, we show\nanalytically that oblivious amplitude amplification when applied to non-unitary\ndynamics leads to a distortion of the quantum state and to an accompanying\nerror in the quantum update. We provide an analytical upper bound of such error\nas a function of the degree of non-unitarity of the dynamics and we test it\nagainst a quantum simulation of an advection-diffusion-reaction equation, a\ntransport problem of major relevance in science and engineering. Finally, we\nalso propose an amplification strategy that helps mitigate the distortion\nerror, while still securing an enhanced success probability.",
        "We perform a detailed study of a simple mathematical model addressing the\nproblem of optimally regulating a process subject to periodic external forcing,\nwhich is interesting both in view of its direct applications and as a prototype\nfor more general problems. In this model one must determine an optimal\ntime-periodic `effort' profile, and the natural setting for the problem is in a\nspace of periodic non-negative measures. We prove that there exists a unique\nsolution for the problem in the space of measures, and then turn to\ncharacterizing this solution. Under some regularity conditions on the problem's\ndata, we prove that its solution is an absolutely continuous measure, and\nprovide an explicit formula for the measure's density. On the other hand, when\nthe problem's data is discontinuous, the solution measure can also include\natomic components. Complementing our analytical results, we carry out numerical\ncomputations to obtain solutions of the problem in various instances, which\nenable us to examine the interesting ways in which the solution's structure\nvaries as the problem's data is varied.",
        "Quantum state exclusion is the task of determining which states from a given\nset a system was not prepared in. We provide a complete solution to optimal\nquantum state exclusion for arbitrary sets of pure states generated by finite\ngroups, establishing necessary and sufficient conditions for perfect\n(zero-error conclusive) exclusion. When perfect exclusion is impossible, we\nintroduce two natural extensions: minimum-error and unambiguous exclusion. For\nboth, we derive the optimal protocols and present analytical expressions for\nthe corresponding failure probabilities and measurements, offering a new\nperspective on how quantum states encode information.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Utilizing 4.5 $fb^-$ of $e^+e^-$ annihilation data collected at\ncenter-of-mass energies ranging from 4599.53 MeV to 4698.82 MeV by the BESIII\ndetector at the BEPCII collider, we search for the singly Cabibbo-suppressed\nhadronic decays $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$\\Lambda_{c}^{+}\\to\\Sigma^{0}K^{+}\\pi^+\\pi^-$ with a single-tag method. No\nsignificant signals are observed for both decays. The upper limits on the\nbranching fractions at the $90\\%$ confidence level are determined to be\n$5.0\\times 10^{-4}$ for $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$6.5\\times 10^{-4}$ for $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Highresolution image synthesis with latent diffusion models",
    "start_abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Rsna pneumonia detection challenge"
      ],
      "abstract":[
        "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "The relationship between galaxy size and halo properties: Insights from\n  the IllustrisTNG simulations and differential clustering",
        "A Geometric Interpretation of Virtual Knotoids",
        "A simple model for entangled photon generation in resonant structures",
        "Validating uncertainty propagation approaches for two-stage Bayesian\n  spatial models using simulation-based calibration",
        "A versatile experimental method to measure the traction forces at\n  interfaces",
        "Spin-dependent dark matter scattering in quasi-two-dimensional magnets",
        "New Construction of Locally q-ary Sequential Recoverable Codes:\n  Parity-check Matrix Approach",
        "A higher algebraic approach to liftings of modules over derived\n  quotients",
        "Second-Order $\\Gamma$-Limit for the Cahn-Hilliard Functional with\n  Dirichlet Boundary Conditions, I",
        "Transmission Probability in Double Quantum Well with Triple Barrier",
        "Dynamic Circuits for the Quantum Lattice-Boltzmann Method",
        "Unveiling individual and collective temporal patterns in the tanker\n  shipping network",
        "Tracking time-varying signals with quantum-enhanced atomic magnetometers",
        "Duality viewpoint of noninvertible symmetry protected topological phases",
        "Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov\n  Decision Processes",
        "Evolution of the pseudogap band structure in a system of\n  electron-correlated lattice polarons",
        "Semiclassical trace formula for the Bochner-Schr\\\"odinger operator",
        "Geometry-Driven Moir\\'e Engineering in Twisted Bilayers of\n  High-Pseudospin Fermions",
        "Detecting LHC Neutrinos at Surface Level",
        "Critical quasilinear equations on Riemannian manifolds",
        "Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance",
        "5G Channel Models for Railway Use Cases at mmWave Band and the Path\n  Towards Terahertz",
        "Soliquidy: a descriptor for atomic geometrical confusion",
        "The Venusian Chronicles",
        "Improved amplitude amplification strategies for the quantum simulation\n  of classical transport problems",
        "Optimal regulation in a periodic environment: insights from a simple\n  model",
        "Quantum state exclusion for group-generated ensembles of pure states",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Search for the Cabibbo-suppressed decays\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{0}$ and\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$"
      ],
      "abstract":[
        "The physical origin of the radial sizes of galaxies and how galaxy sizes are\ncorrelated with the properties of their host dark matter halos is an open\nquestion in galaxy formation. In observations, the large-scale clustering of\ngalaxies selected by stellar mass is significantly different for large and\nsmall galaxies, and Behroozi et al. (2022) showed that these results are in\ntension with some of the correlations between galaxy size and halo properties\nin the literature. We analyze the IllustrisTNG suite of large volume\ncosmological hydrodynamic simulations along with dark matter only simulations\nwith matched initial conditions. We investigate correlations between the ratio\nof galaxy size to halo virial radius ($r_{\\rm gal}\/R_{\\rm vir}$) and halo spin,\nconcentration, and formation time at redshift 0-3. We find a significant\ncorrelation between $r_{\\rm gal}\/R_{\\rm vir}$ and concentration, but only above\na critical value $c \\simeq 16$, and we also find a correlation between $r_{\\rm\ngal}\/R_{\\rm vir}$ and halo formation time. We suggest that galaxy formation\nhistory and environment, in addition to halo properties at a given output time,\nplay an important role in shaping galaxy size. In addition, we directly measure\nsize-based differential clustering in the TNG300 simulation and compare\ndirectly with the observational results. We find significant scale-dependent\nsize-based differential clustering in TNG, in qualitative agreement with\nobservations. However, correlations between $r_{\\rm gal}\/R_{\\rm vir}$ and\nsecondary halo properties are not the drivers of the differential clustering in\nthe simulations; instead, we find that most of this signal in TNG arises from\nsatellite galaxies.",
        "In this paper, we give a three-dimensional geometric interpretation of\nvirtual knotoids. Then we show that virtual knotoid theory is a generalization\nof classical knotoid theory by proving a conjecture of Kauffman and the first\nauthor given in arXiv:1602.03579.",
        "The ability to engineer pairs of entangled photons is essential to quantum\ninformation science, and generating these states using spontaneous parametric\ndown-conversion (SPDC) in nano- and micrometer-scale materials offers numerous\nadvantages. To properly engineer such sources, a reliable model describing\nnano- and micrometer-scale SPDC is necessary; however, such a theoretical\ndescription remains a challenge. Here, we propose and derive a simplified model\nto describe SPDC in resonant structures, which considers the generation of\nphoton pairs and the resonant enhancement of spectral bands to be separate\nprocesses, even though they actually occur simultaneously. We compare our\nsimplified model to both the rigorous theory of SPDC in an etalon - a simple\nexample of a resonant structure - and our experiments on SPDC in etalons and\nfind agreement for low-gain SPDC. By simplifying the calculations required to\ngenerate photon pairs, our model promises to make designing complex resonant\nstructures easier, and it promises to hasten the iteration of designs across\nthe field of quantum state engineering.",
        "This work tackles the problem of uncertainty propagation in two-stage\nBayesian models, with a focus on spatial applications. A two-stage modeling\nframework has the advantage of being more computationally efficient than a\nfully Bayesian approach when the first-stage model is already complex in\nitself, and avoids the potential problem of unwanted feedback effects. Two ways\nof doing two-stage modeling are the crude plug-in method and the posterior\nsampling method. The former ignores the uncertainty in the first-stage model,\nwhile the latter can be computationally expensive. This paper validates the two\naforementioned approaches and proposes a new approach to do uncertainty\npropagation, which we call the $\\mathbf{Q}$ uncertainty method, implemented\nusing the Integrated Nested Laplace Approximation (INLA). We validate the\ndifferent approaches using the simulation-based calibration method, which tests\nthe self-consistency property of Bayesian models. Results show that the crude\nplug-in method underestimates the true posterior uncertainty in the\nsecond-stage model parameters, while the resampling approach and the proposed\nmethod are correct. We illustrate the approaches in a real life data\napplication which aims to link relative humidity and Dengue cases in the\nPhilippines for August 2018.",
        "Measurement of surface forces, including cohesive forces and contact forces,\nis critical for understanding and controlling interactions at interfaces to\noptimize the interfacial performance of applications. The objective of this\npaper is to introduce a general in-situ method that enables the measurement of\n3D micron-scale displacements and corresponding force distribution at\ninterfaces in dry or wet environment. Stereo digital image correlation was used\nto measure the 3D-displacement of a soft and deformable substrate. The\nefficiency and accuracy of the technique were evaluated by applying compression\nto the substrate using a steel ball, with the measured 3D displacements\naligning closely with finite element analysis simulations. To further assess\nthe method's applicability, the wet adhesion between mussel plaques and\nsubstrate was tested under aqueous conditions. The interfacial displacements\nand forces at different stages during the test were measured. The application\nof the technique can be extended for varied circumstances regarding force range\nand substrate materials based on Winkler Spring model.",
        "We study the prospects of detecting dark matter coupled to the spin of the\nelectron, such that it may scatter and excite magnons - collective excitations\nof electronic spins. We show that materials exhibiting long-range magnetic\norder where the spins are coupled only along a plane may act as directional\ndark matter detectors. These quasi-2D materials possess anisotropic dispersion\nrelations and structure functions which induce a sidereal modulation in the\nexcitation rate. We calculate the expected signal rate for some candidate\n(anti)ferromagnets, demonstrating a possible route to the direct detection of\nspin-dependent dark matter in the keV to MeV mass range.",
        "This paper develops a new family of locally recoverable codes for distributed\nstorage systems, Sequential Locally Recoverable Codes (SLRCs) constructed to\nhandle multiple erasures in a sequential recovery approach. We propose a new\nconnection between parallel and sequential recovery, which leads to a general\nconstruction of q-ary linear codes with information $(r, t_i,\n\\delta)$-sequential-locality where each of the $i$-th information symbols is\ncontained in $t_i$ punctured subcodes with length $(r+\\delta-1)$ and minimum\ndistance $\\delta$. We prove that such codes are $(r, t)_q$-SLRC ($t \\geq \\delta\nt_i+1$), which implies that they permit sequential recovery for up to $t$\nerasures each one by $r$ other code symbols.",
        "We show a certain existence of a lifting of modules under the\nself-$\\mathrm{Ext}^2$-vanishing condition over the \"derived quotient\" by using\nthe notion of higher algebra. This refines a work of Auslander--Ding--Solberg's\nsolution of the Auslander--Reiten conjecture for complete interesctions.\nTogether with Auslander's zero-divisor theorem, we show that the existence of\nsuch $\\mathrm{Ext}$-vanishing module over derived quotients is equivalent to\nbeing local complete intersections.",
        "This paper addresses the asymptotic development of order 2 by the $\\Gamma$\n-convergence of the Cahn-Hilliard functional with Dirichlet boundary\nconditions. The Dirichlet data are assumed to be well separated from one of the\ntwo wells. In the case where there are no interfaces, it is shown that there is\na transition layer near the boundary of the domain.",
        "Quantum well of AlGaAs\/GaAs is very important to study transport properties\nof electrons due to its wider application in electronic devices. Hence, the\ndouble well of AlGaAs\/GaAs with triple barrier is taken to study transmission\nprobability. Transmission probability is found to decrease with the increase in\nthe height and width of the barrier. Transmission probability with energy of\nelectron shows two peaks while taking all three barrier of the same height.\nWhereas a single and higher value of peak is found when the height of the\ncentral barrier is slightly reduced.",
        "We propose a quantum algorithm for the linear advection-diffusion equation\n(ADE) Lattice-Boltzmann method (LBM) that leverages dynamic circuits. Dynamic\nquantum circuits allow for an optimized collision-operator quantum algorithm,\nintroducing partial measurements as an integral step. Efficient adaptation of\nthe quantum circuit during execution based on digital information obtained\nthrough mid-circuit measurements is achieved. The proposed new collision\nalgorithm is implemented as a fully unitary operator, which facilitates the\ncomputation of multiple time steps without state reinitialization. Unlike\nprevious quantum collision operators that rely on linear combinations of\nunitaries, the proposed algorithm does not exhibit a probabilistic failure\nrate. Moreover, additional qubits no longer depend on the chosen velocity set,\nwhich reduces both qubit overhead and circuit complexity. Validation of the\nquantum collision algorithm is performed by comparing results with digital LBM\nin one and two dimensions, demonstrating excellent agreement. Performance\nanalysis for multiple time steps highlights advantages compared to previous\nmethods. As an additional variant, a hybrid quantum-digital approach is\nproposed, which reduces the number of mid-circuit measurements, therefore\nimproving the efficiency of the quantum collision algorithm.",
        "The global shipping network, which moves over 80% of the world's goods, is\nnot only a vital backbone of the global economy but also one of the most\npolluting industries. Studying how this network operates is crucial for\nimproving its efficiency and sustainability. While the transport of solid goods\nlike packaged products and raw materials has been extensively researched, far\nless is known about the competitive trade of crude oil and petroleum, despite\nthese commodities accounting for nearly 30% of the market. Using 4 years of\nhigh-resolution data on oil tanker movements, we employ sequential motif mining\nand dynamic mode decomposition to uncover global spatio-temporal patterns in\nthe movement of individual ships. Across all ship classes, we demonstrate that\nmaximizing the proportion of time ships spend carrying cargo -- a metric of\nefficiency -- is achieved through strategic diversification of routes and the\neffective use of intra-regional ports for trips without cargo. Moreover, we\nuncover a globally stable travel structure in the fleet, with pronounced\nseasonal variations linked to annual and semi-annual regional climate patterns\nand economic cycles. Our findings highlight the importance of integrating\nhigh-resolution data with innovative analysis methods not only to improve our\nunderstanding of the underlying dynamics of shipping patterns, but to design\nand evaluate strategies aimed at reducing their environmental impact.",
        "Quantum entanglement, in the form of spin squeezing, is known to improve the\nsensitivity of atomic instruments to static or slowly-varying quantities.\nSensing transient events presents a distinct challenge, requires different\nanalysis methods, and has not been shown to benefit from entanglement in\npractically-important scenarios such as spin-precession magnetometry (SPM).\nHere we adapt estimation control techniques introduced in\narXiv:2403.14764(2024) to the experimental setting of SPM and analogous\ntechniques. We demonstrate that real-time tracking of fluctuating fields\nbenefits from measurement-induced spin-squeezing and that quantum limits\ndictated by decoherence are within reach of today's experiments. We illustrate\nthis quantum advantage by single-shot tracking, within the coherence time of a\nspin-precession magnetometer, of a magnetocardiography signal overlain with\nbroadband noise.",
        "Recent advancements in generalized symmetries have drawn significant\nattention to gapped phases of matter exhibiting novel symmetries, such as\nnoninvertible symmetries. By leveraging the duality transformations, the\nclassification and construction of gapped phases with noninvertible symmetry\ncan be mapped to those involving conventional group symmetries. We demonstrate\nthis approach by classifying symmetry-protected topological phases with a broad\nclass of noninvertible symmetries in arbitrary spacetime dimensions. Our\nresults reveal new classifications that extend beyond those based on group\nsymmetries. Additionally, we construct lattice models in $(1+1)d$ and $(2+1)d$\nthat realize these new phases and explore their anomalous interfaces.",
        "The impact of communication on decision-making systems has been extensively\nstudied under the assumption of dedicated communication channels. We instead\nconsider communicating through actions, where the message is embedded into the\nactions of an agent which interacts with the environment in a Markov decision\nprocess (MDP) framework. We conceptualize the MDP environment as a finite-state\nchannel (FSC), where the actions of the agent serve as the channel input, while\nthe states of the MDP observed by another agent (i.e., receiver) serve as the\nchannel output. Here, we treat the environment as a communication channel over\nwhich the agent communicates through its actions, while at the same time,\ntrying to maximize its reward. We first characterize the optimal information\ntheoretic trade-off between the average reward and the rate of reliable\ncommunication in the infinite-horizon regime. Then, we propose a novel\nframework to design a joint control\/coding policy, termed \\textit{Act2Comm},\nwhich seamlessly embeds messages into actions. From a communication\nperspective, \\textit{Act2Comm} functions as a learning-based channel coding\nscheme for non-differentiable FSCs under input-output constraints. From a\ncontrol standpoint, \\textit{Act2Comm} learns an MDP policy that incorporates\ncommunication capabilities, though at the cost of some control performance.\nOverall, \\textit{Act2Comm} effectively balances the dual objectives of control\nand communication in this environment. Experimental results validate\n\\textit{Act2Comm}'s capability to enable reliable communication while\nmaintaining a certain level of control performance.",
        "The evolution of the role of lattice vibrations in the formation of the\npseudogap state in strongly correlated electron systems has been investigated\nconcerning changes in the electron-phonon coupling parameters and the\nconcentration of doped charge carriers. We apply the polaronic version of the\ngeneralized tight-binding method to analyze the band structure of a realistic\nmultiband two-dimensional model that incorporates the electron-lattice\ncontributions of both Holstein and Peierls types. It has been demonstrated that\nthe emergence of polaronic effects begins with the modulation of spectral\nfunction intensity. However, within a specific region of the phase diagram, a\nsignificant transformation of the electron band structure and pseudogap state\noccurs. It results from coherent polaron excitations that create a partially\nflat band near the Fermi level. This process leads to a change in the topology\nof the Fermi surface and the emergence of corresponding features in the density\nof states.",
        "We study the semiclassical Bochner-Schr\\\"odinger operator\n$H_{p}=\\frac{1}{p^2}\\Delta^{L^p\\otimes E}+V$ on tensor powers $L^p$ of a\nHermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a\nRiemannian manifold of bounded geometry. For any function $\\varphi\\in\nC^\\infty_c(\\mathbb R)$, we consider the bounded linear operator $\\varphi(H_p)$\nin $L^2(X,L^p\\otimes E)$ defined by the spectral theorem. We prove that its\nsmooth Schwartz kernel on the diagonal admits a complete asymptotic expansion\nin powers of $p^{-1}$ in the semiclassical limit $p\\to \\infty$. In particular,\nwhen the manifold is compact, we get a complete asymptotic expansion for the\ntrace of $\\varphi(H_p)$.",
        "Moir\\'e engineering offers new pathways for manipulating emergent states in\ntwisted layered materials and lattice-mismatched heterostructures. With the key\nrole of the geometry of the underlying lattice in mind, here we introduce the\nwatermill lattice, a two-dimensional structure with low-energy states\ncharacterized by massless pseudospin-3\/2 fermions with high winding numbers.\nIts twisted bilayer is shown to exhibit magic angles, where four isolated flat\nbands emerge around the Fermi level, featuring elevated Wilson-loop windings\nand enhanced quantum geometric effects, such as an increase in the ratio of the\nBerezinskii-Kosterlitz-Thouless (BKT) transition temperature to the mean-field\ncritical temperature under a weak Bardeen-Cooper-Schrieffer (BCS) pairing. We\ndiscuss how the watermill lattice could be realized in the MXene and group-IV\nmaterials. Our study highlights the potential of exploiting lattice geometry in\nmoir\\'e engineering to uncover novel quantum phenomena and tailor emergent\nelectronic properties in materials.",
        "The first direct detection of neutrinos at the LHC not only marks the\nbeginning of a novel collider neutrino program at CERN but also motivates\nconsidering additional neutrino detectors to fully exploit the associated\nphysics potential. We investigate the feasibility and physics potential of\nneutrino experiments located at the surface-level. A topographic desk study was\nperformed to identify all points at which the LHC's neutrino beams exit the\nearth. The closest location lies about 9 km east of the CMS interaction point,\nat the bottom of Lake Geneva. Several detectors to be placed at this location\nare considered, including a water Cherenkov detector and an emulsion detector.\nThe detector concepts are introduced, and projections for their contribution to\nthe LHC forward neutrino program and searches for dark sector particles are\npresented. However, the dilution of the neutrino flux over distance reduces the\nneutrino yield significantly, limiting the physics potential of surface-level\ndetectors compared to ones closer to the interaction point, including the\nproposed FPF.",
        "In this paper, we investigate critical quasilinear elliptic partial\ndifferential equations on a complete Riemannian manifold with nonnegative Ricci\ncurvature. By exploiting a new and sharp nonlinear Kato inequality and\nestablishing some Cheng-Yau type gradient estimates for positive solutions, we\nclassify positive solutions to the critical $p$-Laplace equation and show\nrigidity concerning the ambient manifold. Our results extend and improve some\nprevious conclusions in the literature. Similar results are obtained for\nsolutions to the quasilinear Liouville equation involving the $n$-Laplace\noperator, where $n$ corresponds to the dimension of the ambient manifold.",
        "Neural network-based decoding methods have shown promise in enhancing error\ncorrection performance, but traditional approaches struggle with the challenges\nposed by punctured codes. In particular, these methods fail to address the\ncomplexities of variable code rates and the need for protocol compatibility.\nThis paper presents a unified Long Short-Term Memory (LSTM)-based decoding\narchitecture specifically designed to overcome these challenges. The proposed\nmethod unifies punctured convolutional and Turbo codes. A puncture embedding\nmechanism integrates puncturing patterns directly into the network, enabling\nseamless adaptation to varying code rates, while balanced bit error rate\ntraining ensures robustness across different code lengths, rates, and channels,\nmaintaining protocol flexibility. Extensive simulations in Additive White\nGaussian Noise and Rayleigh fading channels demonstrate that the proposed\napproach outperforms conventional decoding techniques, providing significant\nimprovements in decoding accuracy and robustness. These results underscore the\npotential of LSTM-based decoding as a promising solution for next-generation\nartificial intelligence powered communication systems.",
        "High-speed trains are one of the most relevant scenarios for the\nfifth-generation (5G) mobile communications and the \"smart rail mobility\"\nvision, where a high-data-rate wireless connectivity with up to several GHz\nbandwidths will be required. This is a strong motivation for the exploration of\nmillimeter wave (mmWave) band. In this article, we identify the main challenges\nand make progress towards realistic 5G mmWave channel models for railway use\ncases. In order to cope with the challenge of including the railway features in\nthe channel models, we define reference scenarios to help the parameterization\nof channel models for railway use at mmWave band. Simulations and the\nsubsequent measurements used to validate the model reflect the detailed\ninfluence of railway objects and the accuracy of the simulations. Finally, we\npoint out the future directions towards the full version of the smart rail\nmobility which will be powered by terahertz (THz) communications.",
        "Tailoring material properties often requires understanding the solidification\nprocess. Herein, we introduce the geometric descriptor Soliquidy, which\nnumerically captures the Euclidean transport cost between the translationally\ndisordered versus ordered states of a materials. As a testbed, we apply\nSoliquidy to the classification of glass-forming metal alloys. By extending and\ncombining an experimental library of metallic thin-films (glass\/no-glass) with\nthe aflow.org computational database (geometrical and energetic information of\nmixtures) we found that the combination of Soliquity and formation enthalpies\ngenerates an effective classifier for glass formation. Such classifier is then\nused to tackle a public dataset of metallic glasses showing that the\nglass-agnostic assumptions of Soliquity can be useful for understanding\nkinetically-controlled phase transitions.",
        "Venus' atmosphere -- specifically its clouds buoyed up 40 to 60 km above the\nsurface -- has long been suspected to encompass a biosphere where Earth-like\nliving organisms could grow and flourish. This idea has been recently rekindled\nby the observation (signal-to-noise ratio of about 15$\\sigma$) of a phosphine\n(PH$_3$) absorption-line profile against the thermal background from deeper,\nhotter layers of the atmosphere. There is a chance that this observation could\nbe a sign of life, because the PH$_3$ gas observed on Earth originates mostly\nin decaying organic material. Furthermore, it has been shown that there is no\nother natural process on Venus that could otherwise produce the observed PH$_3$\nabsorption line. On the other hand, cosmic rays and the particle cascades they\nproduce in the Earth's atmosphere are hazardous to living organisms, because\nthe ionizing radiation produced in air showers can blow apart chemical\nmolecules and disrupt biochemical processes, causing cells to die or undergo\ndangerous mutations. Compared to Earth, the hypothesized biosphere of Venus\ncould be exposed to substantially more ionizing radiation. This is because\nVenus has no protective intrinsic magnetic field, orbits closer to the Sun, and\nthe entire eventual habitable region lies in the clouds high in the atmosphere.\nThereby, if the clouds were sterilized there would be no reservoir of deeper\nlife that can recolonize afterwards. In this communication we study the effects\nof particle cascades in the venusian atmosphere using the AIRES simulation\npackage properly configured. We show that the effects of cosmic radiation in\nthe habitable zone would be comparable to those on the Earth's surface and so\nwould not have any hazardous effect on possible venusian microorganisms.",
        "The quantum simulation of classical fluids often involves the use of\nprobabilistic algorithms that encode the result of the dynamics in the form of\nthe amplitude of the selected quantum state. In most cases, however, the\namplitude probability is too low to allow an efficient use of these algorithms,\nthereby hindering the practical viability of the quantum simulation. The\noblivious amplitude amplification algorithm is often presented as a solution to\nthis problem, but to no avail for most classical problems, since its\napplicability is limited to unitary dynamics. In this paper, we show\nanalytically that oblivious amplitude amplification when applied to non-unitary\ndynamics leads to a distortion of the quantum state and to an accompanying\nerror in the quantum update. We provide an analytical upper bound of such error\nas a function of the degree of non-unitarity of the dynamics and we test it\nagainst a quantum simulation of an advection-diffusion-reaction equation, a\ntransport problem of major relevance in science and engineering. Finally, we\nalso propose an amplification strategy that helps mitigate the distortion\nerror, while still securing an enhanced success probability.",
        "We perform a detailed study of a simple mathematical model addressing the\nproblem of optimally regulating a process subject to periodic external forcing,\nwhich is interesting both in view of its direct applications and as a prototype\nfor more general problems. In this model one must determine an optimal\ntime-periodic `effort' profile, and the natural setting for the problem is in a\nspace of periodic non-negative measures. We prove that there exists a unique\nsolution for the problem in the space of measures, and then turn to\ncharacterizing this solution. Under some regularity conditions on the problem's\ndata, we prove that its solution is an absolutely continuous measure, and\nprovide an explicit formula for the measure's density. On the other hand, when\nthe problem's data is discontinuous, the solution measure can also include\natomic components. Complementing our analytical results, we carry out numerical\ncomputations to obtain solutions of the problem in various instances, which\nenable us to examine the interesting ways in which the solution's structure\nvaries as the problem's data is varied.",
        "Quantum state exclusion is the task of determining which states from a given\nset a system was not prepared in. We provide a complete solution to optimal\nquantum state exclusion for arbitrary sets of pure states generated by finite\ngroups, establishing necessary and sufficient conditions for perfect\n(zero-error conclusive) exclusion. When perfect exclusion is impossible, we\nintroduce two natural extensions: minimum-error and unambiguous exclusion. For\nboth, we derive the optimal protocols and present analytical expressions for\nthe corresponding failure probabilities and measurements, offering a new\nperspective on how quantum states encode information.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Utilizing 4.5 $fb^-$ of $e^+e^-$ annihilation data collected at\ncenter-of-mass energies ranging from 4599.53 MeV to 4698.82 MeV by the BESIII\ndetector at the BEPCII collider, we search for the singly Cabibbo-suppressed\nhadronic decays $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$\\Lambda_{c}^{+}\\to\\Sigma^{0}K^{+}\\pi^+\\pi^-$ with a single-tag method. No\nsignificant signals are observed for both decays. The upper limits on the\nbranching fractions at the $90\\%$ confidence level are determined to be\n$5.0\\times 10^{-4}$ for $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$6.5\\times 10^{-4}$ for $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$."
      ]
    }
  },
  {
    "id":2411.18902,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Biomechanics and motor control of human movement",
    "start_abstract":"Preface to the Fourth Edition. 1 Biomechanics as an Interdiscipline. 1.0 Introduction. 1.1 Measurement, Description, Analysis, and Assessment. 1.2 its Relationship with Physiology Anatomy. 1.3 Scope of Textbook. 1.4 References. 2 Signal Processing. 2.0 2.1 Auto- Cross-Correlation Analyses. 2.2 Frequency Analysis. 2.3 Ensemble Averaging Repetitive Waveforms. 2.4 3 Kinematics. 3.0 Historical Development Complexity Problem. 3.1 Kinematic Conventions. 3.2 Direct Measurement Techniques. 3.3 Imaging 3.4 Processing Raw Data. 3.5 Calculation Other Variables. 3.6 Problems Based on 3.7 4 Anthropometry. 4.0 Anthropometry in Movement Biomechanics. 4.1 Density, Mass, Inertial Properties. 4.2 Experimental Measures. 4.3 Muscle 4.4 Anthropometric 4.5 5 Kinetics: Forces Moments Force. 5.0 Biomechanical Models. 5.1 Basic Link-Segment Equations-the Free-Body Diagram. 5.2 Force Transducers Plates. 5.3 Bone-on-Bone During Dynamic Conditions. 5.4 Kinetic 5.5 6 Mechanical Work, Energy, Power. 6.0 6.1 Efficiency. 6.2 Forms Energy Storage. 6.3 Internal External Work. 6.4 Power Balances at Joints Within Segments. 6.5 6.6 7 Three-Dimensional Kinematics Kinetics. 7.0 7.1 Axes Systems. 7.2 Marker Anatomical 7.3 Determination Segment Angular Velocities Accelerations. 7.4 Analysis Reaction Moments. 7.5 Suggested Further Reading. 7.6 8 Synthesis Human Movement-Forward Solutions. 8.0 8.1 Review Forward Solution 8.2 Mathematical Formulation. 8.3 System Energy. 8.4 Torques. 8.5 Designation Joints. 8.6 Illustrative Example. 8.7 Conclusions. 8.8 9 Mechanics. 9.0 9.1 Force-Length Characteristics Muscles. 9.2 Force-Velocity Characteristics. 9.3 Modeling. 9.4 10 Kinesiological Electromyography. 10.0 10.1 Electrophysiology Contraction. 10.2 Recording Electromyogram. 10.3 Electromyogram,. 10.4 between Electromyogram 10.5 11 Synergies. 11.0 11.1 The Support Moment Synergy. 11.2 Medial\/Lateral Anterior\/Posterior Balance Standing. 11.3 during Walking. 11.4 APPENDICES. A. Kinematic, Kinetic, Figure A.1 Walking Trial-Marker Locations Mass Frame Rate Information. Table Coordinate Data (cm). A.2( a ) Filtered Kinematics-Rib Cage Greater Trochanter (Hip). b Kinematics-Femoral Lateral Epicondyle (Knee) Head Fibula. c Kinematics-Lateral Malleolus (Ankle) Heel. d Kinematics-Fifth Metatarsal Toe. A.3( Linear Kinematics-Foot. Kinematics-Leg. Kinematics-Thigh. Kinematics-1\/2 HAT. A.4 Relative Joint Kinematics-Ankle, Knee, Hip. A.5( Force-Ankle Knee. Force-Hip. A.6 Potential, Total Energies-Foot, Leg, Thigh, and1\/2 A.7 Generation\/Absorption Transfer-Ankle, B. Units Definitions Related Electromyographical Measurements. B.1 Base SI Units. B.2 Derived Index.",
    "start_categories":[
      "Biomechanics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      ],
      "abstract":[
        "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution recurrent structured state space models (SSMs) have been developed to address Transformers' computational inefficiency long sequences, but they not performed well important modalities language. We identify that a key weakness is their inability perform content-based reasoning, make several improvements. First, simply letting SSM parameters be functions input addresses with discrete modalities, allowing model selectively propagate or forget information along sequence length dimension depending current token. Second, even though this change prevents use efficient convolutions, we design hardware-aware parallel algorithm mode. integrate these selective SSMs into simplified end-to-end neural network without MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) scaling length, performance improves real data up million-length sequences. As general backbone, achieves state-of-the-art across language, audio, genomics. On language modeling, our Mamba-3B outperforms Transformers same size matches twice size, both pretraining downstream evaluation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "An Unsupervised C-Uniform Trajectory Sampler with Applications to Model\n  Predictive Path Integral Control",
        "Integrated Multiphysics Modeling of a Piezoelectric Micropump",
        "The Cosmic Microwave Background -- Secondary Anisotropies",
        "A neural network approach for line detection in complex atomic emission\n  spectra measured by high-resolution Fourier transform spectroscopy",
        "CLoCKDistill: Consistent Location-and-Context-aware Knowledge\n  Distillation for DETRs",
        "INDIGO+: A Unified INN-Guided Probabilistic Diffusion Algorithm for\n  Blind and Non-Blind Image Restoration",
        "SeqSee: A schema-based approach to spectral sequence visualization",
        "The Halo Occupation Distribution Modeling of the X-ray-selected AGNs at\n  0.6 < z < 2.6 in the COSMOS field",
        "SeWA: Selective Weight Average via Probabilistic Masking",
        "On the extremal eigenvalues of Jacobi ensembles at zero temperature",
        "Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton",
        "AI Drawing Partner: Co-Creative Drawing Agent and Research Platform to\n  Model Co-Creation",
        "Web Execution Bundles: Reproducible, Accurate, and Archivable Web\n  Measurements",
        "4bit-Quantization in Vector-Embedding for RAG",
        "Frame-dependent Random Utility",
        "Complexity of Jelly-No and Hanano games with various constraints",
        "StarCast: A Secure and Spectrum-Efficient Group Communication Scheme for\n  LEO Satellite Networks",
        "Bifurcation of global energy minimizers for a diffusion-aggregation\n  model on sphere",
        "Isolating Unisolated Upsilons with Anomaly Detection in CMS Open Data",
        "Learning the Integral Quadratic Constraints on Plant-Model Mismatch",
        "A Laplace duality for integration",
        "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning",
        "QUOKKA-based understanding of outflows (QED) -- III. Outflow loading and\n  phase structure as a function of galactic environment",
        "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "TacticExpert: Spatial-Temporal Graph Language Model for Basketball\n  Tactics",
        "Automatic Linear Resource Bound Analysis for Rust via Prophecy\n  Potentials",
        "PythonPal: Enhancing Online Programming Education through Chatbot-Driven\n  Personalized Feedback",
        "Power-Efficient Deceptive Wireless Beamforming Against Eavesdroppers",
        "Compression in 3D Gaussian Splatting: A Survey of Methods, Trends, and\n  Future Directions"
      ],
      "abstract":[
        "Sampling-based model predictive controllers generate trajectories by sampling\ncontrol inputs from a fixed, simple distribution such as the normal or uniform\ndistributions. This sampling method yields trajectory samples that are tightly\nclustered around a mean trajectory. This clustering behavior in turn, limits\nthe exploration capability of the controller and reduces the likelihood of\nfinding feasible solutions in complex environments. Recent work has attempted\nto address this problem by either reshaping the resulting trajectory\ndistribution or increasing the sample entropy to enhance diversity and promote\nexploration. In our recent work, we introduced the concept of C-Uniform\ntrajectory generation [1] which allows the computation of control input\nprobabilities to generate trajectories that sample the configuration space\nuniformly. In this work, we first address the main limitation of this method:\nlack of scalability due to computational complexity. We introduce Neural\nC-Uniform, an unsupervised C-Uniform trajectory sampler that mitigates\nscalability issues by computing control input probabilities without relying on\na discretized configuration space. Experiments show that Neural C-Uniform\nachieves a similar uniformity ratio to the original C-Uniform approach and\ngenerates trajectories over a longer time horizon while preserving uniformity.\nNext, we present CU-MPPI, which integrates Neural C-Uniform sampling into\nexisting MPPI variants. We analyze the performance of CU-MPPI in simulation and\nreal-world experiments. Our results indicate that in settings where the optimal\nsolution has high curvature, CU-MPPI leads to drastic improvements in\nperformance.",
        "This paper presents an integrated multiphysics simulation approach of\npiezoelectric micropumps. Micropumps and micro blowers are essential devices in\nvarious cutting-edge industries like laboratory equipment, medical devices, and\nfuel cells. A piezoelectric micropump involves complex physics including\nmicrofluidics, flow-structure interaction, electricity, and piezoelectric\nmaterial. Hence, a comprehensive analysis of the interactions between different\nphysical phenomena, would be essential for the effective design and\noptimization of these micropumps. Prior studies on piezoelectric micropump were\nmainly focused on isolated physical aspects of these pumps, such as\npiezoelectric mechanics, fluid dynamics, electrical properties, and also\nfluid-structure Interactions. The present paper fills this gap by integrating\nthese aspects into a holistic simulation and design approach, introducing a new\nmethodology for micropump analysis. Advanced simulation and design tools like\nCOMSOL and SolidWorks were employed in accordance. A brief review of\npiezoelectric materials, and an exploration of different types of micropumps\nand their operating principles is discussed. Also, a comparison of various\npiezoelectric materials, including their properties and applications is\ninvestigated. Further, the paper discusses the simulation process of the\nmicropumps, using COMSOL software, and presents an in-depth analysis of the\nsimulation results. This structured approach provides a comprehensive\nunderstanding of piezoelectric micropumps, from theoretical underpinnings to\npractical design considerations. ..",
        "The cosmic microwave background (CMB), the relic radiation from the early\nUniverse, offers a unique window into both primordial conditions and the\nintervening large-scale structure (LSS) it traverses. Interactions between CMB\nphotons and the evolving Universe imprint secondary anisotropies --\nmodifications to the CMB's intensity and polarization caused by gravitational\neffects and scattering processes. These anisotropies serve as a powerful probe\nof fundamental physics while also revealing astrophysical processes governing\nthe thermodynamics and distribution of baryonic matter. In this chapter, we\nprovide a comprehensive review of the physical mechanisms underlying secondary\nanisotropies, their observational status, and their potential to advance\nprecision cosmology. With the increasing sensitivity of CMB experiments and\nsynergy with LSS surveys, secondary anisotropies are poised to unveil\nunprecedented insights into the Universe's composition, evolution, and\ndynamics.",
        "The atomic spectra and structure of the open d- and f-shell elements are\nextremely complex, where tens of thousands of transitions between fine\nstructure energy levels can be observed as spectral lines across the infrared\nand UV per species. Energy level quantum properties and transition wavenumbers\nof these elements underpins almost all spectroscopic plasma diagnostic\ninvestigations, with prominent demands from astronomy and fusion research.\nDespite their importance, these fundamental data are incomplete for many\nspecies. A major limitation for the analyses of emission spectra of the open d-\nand f-shell elements is the amount of time and human resource required to\nextract transition wavenumbers and intensities from the spectra. Here, the\nspectral line detection problem is approached by encoding the spectrum\npoint-wise using bidirectional Long Short-Term Memory networks, where\ntransition wavenumber positions are decoded by a fully connected neural\nnetwork. The model was trained using simulated atomic spectra and evaluated\nagainst experimental Fourier transform spectra of Ni ($Z=28$) covering\n1800-70,000 cm$^{-1}$ (5555-143 nm) and Nd ($Z=60$) covering 25,369-32,485\ncm$^{-1}$ (394-308 nm), measured under a variety of experimental set-ups.\nImprovements over conventional methods in line detection were evident,\nparticularly for spectral lines that are noisy, blended, and\/or distorted by\ninstrumental spectral resolution-limited ringing. In evaluating model\nperformance, a brief energy level analysis of Ni II using lines newly detected\nby the neural networks has led to the confident identification of two Ni II\nlevels, $3\\text{d}^8$$(^3\\text{F}_4)6\\text{f} [2]_{3\/2}$ at 134,261.8946 $\\pm$\n0.0081 cm$^{-1}$ and $3\\text{d}^8$$(^3\\text{F}_4)6\\text{f} [1]_{3\/2}$ at\n134,249.5264 $\\pm$ 0.0054 cm$^{-1}$, previously concluded to be unidentifiable\nusing previously analysed Ni spectra.",
        "Object detection has advanced significantly with Detection Transformers\n(DETRs). However, these models are computationally demanding, posing challenges\nfor deployment in resource-constrained environments (e.g., self-driving cars).\nKnowledge distillation (KD) is an effective compression method widely applied\nto CNN detectors, but its application to DETR models has been limited. Most KD\nmethods for DETRs fail to distill transformer-specific global context. Also,\nthey blindly believe in the teacher model, which can sometimes be misleading.\nTo bridge the gaps, this paper proposes Consistent Location-and-Context-aware\nKnowledge Distillation (CLoCKDistill) for DETR detectors, which includes both\nfeature distillation and logit distillation components. For feature\ndistillation, instead of distilling backbone features like existing KD methods,\nwe distill the transformer encoder output (i.e., memory) that contains valuable\nglobal context and long-range dependencies. Also, we enrich this memory with\nobject location details during feature distillation so that the student model\ncan prioritize relevant regions while effectively capturing the global context.\nTo facilitate logit distillation, we create target-aware queries based on the\nground truth, allowing both the student and teacher decoders to attend to\nconsistent and accurate parts of encoder memory. Experiments on the KITTI and\nCOCO datasets show our CLoCKDistill method's efficacy across various DETRs,\ne.g., single-scale DAB-DETR, multi-scale deformable DETR, and denoising-based\nDINO. Our method boosts student detector performance by 2.2% to 6.4%.",
        "Generative diffusion models are becoming one of the most popular prior in\nimage restoration (IR) tasks due to their remarkable ability to generate\nrealistic natural images. Despite achieving satisfactory results, IR methods\nbased on diffusion models present several limitations. First of all, most\nnon-blind approaches require an analytical expression of the degradation model\nto guide the sampling process. Secondly, most existing blind approaches rely on\nfamilies of pre-defined degradation models for training their deep networks.\nThe above issues limit the flexibility of these approaches and so their ability\nto handle real-world degradation tasks. In this paper, we propose a novel\nINN-guided probabilistic diffusion algorithm for non-blind and blind image\nrestoration, namely INDIGO and BlindINDIGO, which combines the merits of the\nperfect reconstruction property of invertible neural networks (INN) with the\nstrong generative capabilities of pre-trained diffusion models. Specifically,\nwe train the forward process of the INN to simulate an arbitrary degradation\nprocess and use the inverse to obtain an intermediate image that we use to\nguide the reverse diffusion sampling process through a gradient step. We also\nintroduce an initialization strategy, to further improve the performance and\ninference speed of our algorithm. Experiments demonstrate that our algorithm\nobtains competitive results compared with recently leading methods both\nquantitatively and visually on synthetic and real-world low-quality images.",
        "We present SeqSee, a software system that addresses spectral sequence\nvisualization through a schema-based approach. By introducing a standardized\nJSON schema as an intermediate representation, SeqSee decouples the\nmathematical computations of spectral sequences from their visualizations. We\ndemonstrate the system through a case study of the classical and C-motivic\nAdams spectral sequences.",
        "We conducted precise measurements of Active Galactic Nuclei (AGNs) clustering\nat $z\\sim1$ and $z\\sim2$ by measuring the two-point cross-correlation function\n(CCF) between galaxies and X-ray-selected AGNs, and the two-point\nauto-correlation function (ACF) of galaxies in the COSMOS field to interpret\nthe CCF results. The galaxy sample was selected from the COSMOS2015 catalog,\nwhile the AGN sample was chosen from the {\\sl Chandra} COSMOS-Legacy survey\ncatalog. For the AGN samples at $z\\sim1$ and $z\\sim2$, we calculated AGN bias\nvalues of $b=1.16\\ (1.16;1.31)$ and $b=2.95\\ (2.30;3.55)$, respectively. These\nvalues correspond to typical host dark matter halo (DMH) masses of log$(M_{\\rm\ntyp}\/M_{\\odot})=11.82\\ (11.82;12.12)$ and log$(M_{\\rm typ}\/M_{\\odot})=12.80\\\n(12.38;13.06)$, respectively. Subsequently, we performed Halo Occupation\nDistribution (HOD) modeling of X-ray-selected AGNs using the CCF and ACF of\ngalaxies. We have found a significant satellite AGN population at $z\\sim 1$ all\nover the DMH mass ($M_{\\rm DMH}$) range occupied by AGNs. While $z\\sim 2$ AGNs\nin our sample are associated with higher mass DMHs and smaller satellite\nfractions. The HOD analysis suggests a marginal tendency of increasing\nsatellite slope with redshift, but larger samples are needed to confirm this\nwith sufficient statistical significance. We find that the best-fit values of\nsatellite slope in both redshift bins are greater than 0, suggesting tendencies\nof increasing satellite AGN number with $M_{\\rm DMH}$.",
        "Weight averaging has become a standard technique for enhancing model\nperformance. However, methods such as Stochastic Weight Averaging (SWA) and\nLatest Weight Averaging (LAWA) often require manually designed procedures to\nsample from the training trajectory, and the results depend heavily on\nhyperparameter tuning. To minimize human effort, this paper proposes a simple\nyet efficient algorithm called Selective Weight Averaging (SeWA), which\nadaptively selects checkpoints during the final stages of training for\naveraging. Based on SeWA, we show that only a few points are needed to achieve\nbetter generalization and faster convergence. Theoretically, solving the\ndiscrete subset selection problem is inherently challenging. To address this,\nwe transform it into a continuous probabilistic optimization framework and\nemploy the Gumbel-Softmax estimator to learn the non-differentiable mask for\neach checkpoint. Further, we theoretically derive the SeWA's stability-based\ngeneralization bounds, which are sharper than that of SGD under both convex and\nnon-convex assumptions. Finally, solid extended experiments in various domains,\nincluding behavior cloning, image classification, and text classification,\nfurther validate the effectiveness of our approach.",
        "For the $\\beta$-Hermite, Laguerre, and Jacobi ensembles of dimension $N$\nthere exist central limit theorems for the freezing case $\\beta\\to\\infty$ such\nthat the associated means and covariances can be expressed in terms of the\nassociated Hermite, Laguerre, and Jacobi polynomials of order $N$ respectively\nas well as via the associated dual polynomials in the sense of de Boor and\nSaff. In this paper we derive limits for $N\\to\\infty$ for the covariances of\nthe $r\\in\\mathbb N$ largest (and smallest) eigenvalues for these frozen Jacobi\nensembles in terms of Bessel functions. These results correspond to the hard\nedge analysis in the frozen Laguerre cases by Andraus and Lerner-Brecher and to\nknown results for finite $\\beta$.",
        "Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.",
        "This paper describes the AI Drawing Partner, which is a co-creative drawing\nagent that also serves as a research platform to model co-creation. The AI\nDrawing Partner is an early example of a quantified co-creative AI system that\nautomatically models the co-creation that happens on the system. The method the\nsystem uses to capture this data is based on a new cognitive science framework\ncalled co-creative sense-making (CCSM). The CCSM is based on the cognitive\ntheory of enaction, which describes how meaning emerges through interaction\nwith the environment and other people in that environment in a process of\nsense-making. The CCSM quantifies elements of interaction dynamics to identify\nsense-making patterns and interaction trends. This paper describes a new\ntechnique for modeling the interaction and collaboration dynamics of\nco-creative AI systems with the co-creative sense-making (CCSM) framework. A\ncase study is conducted of ten co-creative drawing sessions between a human\nuser and the co-creative agent. The analysis includes showing the artworks\nproduced, the quantified data from the AI Drawing Partner, the curves\ndescribing interaction dynamics, and a visualization of interaction trend\nsequences. The primary contribution of this paper is presenting the AI Drawing\nPartner, which is a unique co-creative AI system and research platform that\ncollaborates with the user in addition to quantifying, modeling, and\nvisualizing the co-creative process using the CCSM framework.",
        "Recently, reproducibility has become a cornerstone in the security and\nprivacy research community, including artifact evaluations and even a new\nsymposium topic. However, Web measurements lack tools that can be reused across\nmany measurement tasks without modification, while being robust to\ncircumvention, and accurate across the wide range of behaviors in the Web. As a\nresult, most measurement studies use custom tools and varied archival formats,\neach of unknown correctness and significant limitations, systematically\naffecting the research's accuracy and reproducibility.\n  To address these limitations, we present WebREC, a Web measurement tool that\nis, compared against the current state-of-the-art, accurate (i.e., correctly\nmeasures and attributes events not possible with existing tools), general\n(i.e., reusable without modification for a broad range of measurement tasks),\nand comprehensive (i.e., handling events from all relevant browser behaviors).\nWe also present .web, an archival format for the accurate and reproducible\nmeasurement of a wide range of website behaviors. We empirically evaluate\nWebREC's accuracy by replicating well-known Web measurement studies and showing\nthat WebREC's results more accurately match our baseline. We then assess if\nWebREC and .web succeed as general-purpose tools, which could be used to\naccomplish many Web measurement tasks without modification. We find that this\nis so: 70% of papers discussed in a 2024 web crawling SoK paper could be\nconducted using WebREC as is, and a larger number (48%) could be leveraged\nagainst .web archives without requiring any new crawling.",
        "Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps:\/\/github.com\/taeheej\/4bit-Quantization-in-Vector-Embedding-for-RAG",
        "We explore the influence of framing on decision-making, where some products\nare framed (e.g., displayed, recommended, endorsed, or labeled). We introduce a\nnovel choice function that captures observed variations in framed alternatives.\nBuilding on this, we conduct a comprehensive revealed preference analysis,\nemploying the concept of frame-dependent utility using both deterministic and\nprobabilistic data. We demonstrate that simple and intuitive behavioral\nprinciples characterize our frame-dependent random utility model (FRUM), which\noffers testable conditions even with limited data. Finally, we introduce a\nparametric model to increase the tractability of FRUM. We also discuss how to\nrecover the choice types in our framework.",
        "This work shows new results on the complexity of games Jelly-No and Hanano\nwith various constraints on the size of the board and number of colours. Hanano\nand Jelly-No are one-player, 2D side-view puzzle games with a dynamic board\nconsisting of coloured, movable blocks disposed on platforms. These blocks can\nbe moved by the player and are subject to gravity. Both games somehow vary in\ntheir gameplay, but the goal is always to move the coloured blocks in order to\nreach a specific configuration and make them interact with each other or with\nother elements of the game. In Jelly-No the goal is to merge all coloured\nblocks of a same colour, which also happens when they make contact. In Hanano\nthe goal is to make all the coloured blocks bloom by making contact with\nflowers of the same colour. Jelly-No was proven by Chao Yang to be NP-Complete\nunder the restriction that all movable blocks are the same colour and NP-Hard\nfor more colours. Hanano was proven by Michael C. Chavrimootoo to be\nPSPACE-Complete under the restriction that all movable blocks are the same\ncolour. However, the question whether Jelly-No for more than one colours is\nalso PSPACE-complete or if it too stays in NP was left open. In this paper, we\nsettle this question, proving that Jelly-No is PSPACE-Complete with an\nunbounded number of colours. We further show that, if we allow black jellies\n(that is, jellies that do not need to be merged), the game is PSPACE-complete\neven for one colour. We further show that one-colour Jelly-No and Hanano remain\nNP-Hard even if the width or the height of the board are small constants.",
        "Low Earth Orbit (LEO) satellite networks serve as a cornerstone\ninfrastructure for providing ubiquitous connectivity in areas where terrestrial\ninfrastructure is unavailable. With the emergence of Direct-to-Cell (DTC)\nsatellites, these networks can provide direct access to mobile phones and IoT\ndevices without relying on terrestrial base stations, leading to a surge in\nmassive connectivity demands for the serving satellite. To address this issue,\ngroup communication is an effective paradigm that enables simultaneous content\ndelivery to multiple users and thus optimizes bandwidth reuse. Although\nextensive research has been conducted to improve group communication\nperformance, securing this communication without compromising its inherent\nspectrum efficiency remains a critical challenge. To address this, we introduce\nStarCast, a secure group encryption scheme for LEO satellite networks. Our\nsolution leverages ciphertext-policy attribute-based encryption (CP-ABE) to\nimplement fine-grained access control by embedding access policies directly\nwithin the ciphertext. Unlike standard secure communication approaches that\nrequire dedicated per-user channels and significantly deplete limited satellite\nspectrum resources, StarCast maintains efficient spectrum reuse within user\ngroups while ensuring that only authorized users can access transmitted data.\nAdditionally, it significantly reduces the costly key management overhead\nassociated with conventional encryption schemes.",
        "We consider a free energy functional defined on probability densities on the\nunit sphere $\\mathbb{S}^d$, and investigate its global minimizers. The energy\nconsists of two components: an entropy and a nonlocal interaction energy, which\nfavour spreading and aggregation behaviour, respectively. We find a threshold\nvalue for the size of the attractive interactions, and establish the global\nenergy minimizers in each case. The bifurcation at this threshold value is\ninvestigated. We also generalize the results to spaces consisting of an\narbitrary number of spheres (e.g., the flat torus $\\mathbb{S}^1 \\times\n\\mathbb{S}^1$).",
        "We present the first study of anti-isolated Upsilon decays to two muons\n($\\Upsilon \\to \\mu^+ \\mu^-$) in proton-proton collisions at the Large Hadron\nCollider. Using a machine learning (ML)-based anomaly detection strategy, we\n\"rediscover\" the $\\Upsilon$ in 13 TeV CMS Open Data from 2016, despite\noverwhelming anti-isolated backgrounds. We elevate the signal significance to\n$6.4 \\sigma$ using these methods, starting from $1.6 \\sigma$ using the dimuon\nmass spectrum alone. Moreover, we demonstrate improved sensitivity from using\nan ML-based estimate of the multi-feature likelihood compared to traditional\n\"cut-and-count\" methods. Our work demonstrates that it is possible and\npractical to find real signals in experimental collider data using ML-based\nanomaly detection, and we distill a readily-accessible benchmark dataset from\nthe CMS Open Data to facilitate future anomaly detection developments.",
        "While a characterization of plant-model mismatch is necessary for robust\ncontrol, the mismatch usually can not be described accurately due to the lack\nof knowledge about the plant model or the complexity of nonlinear plants.\nHence, this paper considers this problem in a data-driven way, where the\nmismatch is captured by parametric forms of integral quadratic constraints\n(IQCs) and the parameters contained in the IQC equalities are learned from\nsampled trajectories from the plant. To this end, a one-class support vector\nmachine (OC-SVM) formulation is proposed, and its generalization performance is\nanalyzed based on the statistical learning theory. The proposed approach is\ndemonstrated by a single-input-single-output time delay mismatch and a\nnonlinear two-phase reactor with a linear nominal model, showing accurate\nrecovery of frequency-domain uncertainties.",
        "We consider the integral v(y) = Ky f (x)dx on a domain Ky = {x $\\in$ R d :\ng(x) $\\le$ y}, where g is nonnegative and Ky is compact for all y $\\in$ [0,\n+$\\infty$). Under some assumptions, we show that for every y $\\in$ (0,\n$\\infty$) there exists a distinguished scalar $\\lambda$y $\\in$ (0, +$\\infty$)\nsuch that which is the counterpart analogue for integration of Lagrangian\nduality for optimization. A crucial ingredient is the Laplace transform, the\nanalogue for integration of Legendre-Fenchel transform in optimization. In\nparticular, if both f and g are positively homogeneous then $\\lambda$y is a\nsimple explicitly rational function of y. In addition if g is quadratic form\nthen computing v(y) reduces to computing the integral of f with respect to a\nspecific Gaussian measure for which exact and approximate numerical methods\n(e.g. cubatures) are available.",
        "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.",
        "We present results from a suite of 3D high-resolution hydrodynamic\nsimulations of supernova-driven outflows from galactic disc regions with a\nrange of gas surface density, metallicity, and supernova scale height. We use\nthis suite to quantify how outflow properties -- particularly the loading\nfactors for mass, metallicity, and energy -- vary with these parameters. We\nfind that the winds fall into three broad categories: steady and hot,\nmultiphase and moderately bursty, and cool and highly bursty. The first of\nthese is characterised by efficient metal and energy loading but weak mass\nloading, the second by moderate loading of mass, metals, and energy, and the\nthird by negligible metal and energy loading but substantial mass loading. The\nmost important factor in determining the kind of wind a galaxy will produce is\nthe ratio of supernova to gas gas scale heights, with the latter set by a\ncombination of supernova rate, metallicity-dependent cooling rate, and the\ngravitational potential. These often combine in counterintuitive ways -- for\nexample increased cooling causes cold clouds to sink into the galactic midplane\nmore rapidly, lowering the volume-filling factor of dense gas and making the\nenvironment more favourable for strong winds. Our findings suggest that the\nnature of galactic winds is likely highly sensitive to phenomena such as\nrunaway stars occuring at a large height and dense gas and are poorly captured\nin most simulations, and that metal loading factors for type Ia supernovae may\nbe substantially larger than those for type II, with important implications for\ngalactic chemical evolution.",
        "In a globalized and interconnected world, interoperability has become a key\nconcept for advancing tactical scenarios. Federated Coalition Networks (FCN)\nenable cooperation between entities from multiple nations while allowing each\nto maintain control over their systems. However, this interoperability\nnecessitates the sharing of increasing amounts of information between different\ntactical assets, raising the need for higher security measures. Emerging\ntechnologies like blockchain drive a revolution in secure communications,\npaving the way for new tactical scenarios. In this work, we propose a\nblockchain-based framework to enhance the resilience and security of the\nmanagement of these networks. We offer a guide to FCN design to help a broad\naudience understand the military networks in international missions by a use\ncase and key functions applied to a proposed architecture. We evaluate its\neffectiveness and performance in information encryption to validate this\nframework.",
        "The core challenge in basketball tactic modeling lies in efficiently\nextracting complex spatial-temporal dependencies from historical data and\naccurately predicting various in-game events. Existing state-of-the-art (SOTA)\nmodels, primarily based on graph neural networks (GNNs), encounter difficulties\nin capturing long-term, long-distance, and fine-grained interactions among\nheterogeneous player nodes, as well as in recognizing interaction patterns.\nAdditionally, they exhibit limited generalization to untrained downstream tasks\nand zero-shot scenarios. In this work, we propose a Spatial-Temporal\nPropagation Symmetry-Aware Graph Transformer for fine-grained game modeling.\nThis architecture explicitly captures delay effects in the spatial space to\nenhance player node representations across discrete-time slices, employing\nsymmetry-invariant priors to guide the attention mechanism. We also introduce\nan efficient contrastive learning strategy to train a Mixture of Tactics\nExperts module, facilitating differentiated modeling of offensive tactics. By\nintegrating dense training with sparse inference, we achieve a 2.4x improvement\nin model efficiency. Moreover, the incorporation of Lightweight Graph Grounding\nfor Large Language Models enables robust performance in open-ended downstream\ntasks and zero-shot scenarios, including novel teams or players. The proposed\nmodel, TacticExpert, delineates a vertically integrated large model framework\nfor basketball, unifying pretraining across multiple datasets and downstream\nprediction tasks. Fine-grained modeling modules significantly enhance\nspatial-temporal representations, and visualization analyzes confirm the strong\ninterpretability of the model.",
        "Rust has become a popular system programming language that strikes a balance\nbetween memory safety and performance. Rust's type system ensures the safety of\nlow-level memory controls; however, a well-typed Rust program is not guaranteed\nto enjoy high performance. This article studies static analysis for resource\nconsumption of Rust programs, aiming at understanding the performance of Rust\nprograms. Although there have been tons of studies on static resource analysis,\nexploiting Rust's memory safety -- especially the borrow mechanisms and their\nproperties -- to aid resource-bound analysis, remains unexplored. This article\npresents RaRust, a type-based linear resource-bound analysis for well-typed\nRust programs. RaRust follows the methodology of automatic amortized resource\nanalysis (AARA) to build a resource-aware type system. To support Rust's borrow\nmechanisms, including shared and mutable borrows, RaRust introduces shared and\nnovel prophecy potentials to reason about borrows compositionally. To prove the\nsoundness of RaRust, this article proposes Resource-Aware Borrow Calculus\n(RABC) as a variant of recently proposed Low-Level Borrow Calculus (LLBC). The\nexperimental evaluation of a prototype implementation of RaRust demonstrates\nthat RaRust is capable of inferring symbolic linear resource bounds for Rust\nprograms featuring shared and mutable borrows, reborrows, heap-allocated data\nstructures, loops, and recursion.",
        "The rise of online programming education has necessitated more effective,\npersonalized interactions, a gap that PythonPal aims to fill through its\ninnovative learning system integrated with a chatbot. This research delves into\nPythonPal's potential to enhance the online learning experience, especially in\ncontexts with high student-to-teacher ratios where there is a need for\npersonalized feedback. PythonPal's design, featuring modules for conversation,\ntutorials, and exercises, was evaluated through student interactions and\nfeedback. Key findings reveal PythonPal's proficiency in syntax error\nrecognition and user query comprehension, with its intent classification model\nshowing high accuracy. The system's performance in error feedback, though\nvaried, demonstrates both strengths and areas for enhancement. Student feedback\nindicated satisfactory query understanding and feedback accuracy but also\npointed out the need for faster responses and improved interaction quality.\nPythonPal's deployment promises to significantly enhance online programming\neducation by providing immediate, personalized feedback and interactive\nlearning experiences, fostering a deeper understanding of programming concepts\namong students. These benefits mark a step forward in addressing the challenges\nof distance learning, making programming education more accessible and\neffective.",
        "Eavesdroppers of wireless signals want to infer as much as possible regarding\nthe transmitter (Tx). Popular methods to minimize information leakage to the\neavesdropper include covert communication, directional modulation, and\nbeamforming with nulling. In this paper we do not attempt to prevent\ninformation leakage to the eavesdropper like the previous methods. Instead we\npropose to beamform the wireless signal at the Tx in such a way that it\nincorporates deceptive information. The beamformed orthogonal frequency\ndivision multiplexing (OFDM) signal includes a deceptive value for the Doppler\n(velocity) and range of the Tx. To design the optimal baseband waveform with\nthese characteristics, we define and solve an optimization problem for\npower-efficient deceptive wireless beamforming (DWB). The relaxed convex\nQuadratic Program (QP) is solved using a heuristic algorithm. Our simulation\nresults indicate that our DWB scheme can successfully inject deceptive\ninformation with low power consumption, while preserving the shape of the\ncreated beam.",
        "3D Gaussian Splatting (3DGS) has recently emerged as a pioneering approach in\nexplicit scene rendering and computer graphics. Unlike traditional neural\nradiance field (NeRF) methods, which typically rely on implicit,\ncoordinate-based models to map spatial coordinates to pixel values, 3DGS\nutilizes millions of learnable 3D Gaussians. Its differentiable rendering\ntechnique and inherent capability for explicit scene representation and\nmanipulation positions 3DGS as a potential game-changer for the next generation\nof 3D reconstruction and representation technologies. This enables 3DGS to\ndeliver real-time rendering speeds while offering unparalleled editability\nlevels. However, despite its advantages, 3DGS suffers from substantial memory\nand storage requirements, posing challenges for deployment on\nresource-constrained devices. In this survey, we provide a comprehensive\noverview focusing on the scalability and compression of 3DGS. We begin with a\ndetailed background overview of 3DGS, followed by a structured taxonomy of\nexisting compression methods. Additionally, we analyze and compare current\nmethods from the topological perspective, evaluating their strengths and\nlimitations in terms of fidelity, compression ratios, and computational\nefficiency. Furthermore, we explore how advancements in efficient NeRF\nrepresentations can inspire future developments in 3DGS optimization. Finally,\nwe conclude with current research challenges and highlight key directions for\nfuture exploration."
      ]
    }
  },
  {
    "id":2411.18902,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "start_abstract":"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution recurrent structured state space models (SSMs) have been developed to address Transformers' computational inefficiency long sequences, but they not performed well important modalities language. We identify that a key weakness is their inability perform content-based reasoning, make several improvements. First, simply letting SSM parameters be functions input addresses with discrete modalities, allowing model selectively propagate or forget information along sequence length dimension depending current token. Second, even though this change prevents use efficient convolutions, we design hardware-aware parallel algorithm mode. integrate these selective SSMs into simplified end-to-end neural network without MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) scaling length, performance improves real data up million-length sequences. As general backbone, achieves state-of-the-art across language, audio, genomics. On language modeling, our Mamba-3B outperforms Transformers same size matches twice size, both pretraining downstream evaluation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Biomechanics and motor control of human movement"
      ],
      "abstract":[
        "Preface to the Fourth Edition. 1 Biomechanics as an Interdiscipline. 1.0 Introduction. 1.1 Measurement, Description, Analysis, and Assessment. 1.2 its Relationship with Physiology Anatomy. 1.3 Scope of Textbook. 1.4 References. 2 Signal Processing. 2.0 2.1 Auto- Cross-Correlation Analyses. 2.2 Frequency Analysis. 2.3 Ensemble Averaging Repetitive Waveforms. 2.4 3 Kinematics. 3.0 Historical Development Complexity Problem. 3.1 Kinematic Conventions. 3.2 Direct Measurement Techniques. 3.3 Imaging 3.4 Processing Raw Data. 3.5 Calculation Other Variables. 3.6 Problems Based on 3.7 4 Anthropometry. 4.0 Anthropometry in Movement Biomechanics. 4.1 Density, Mass, Inertial Properties. 4.2 Experimental Measures. 4.3 Muscle 4.4 Anthropometric 4.5 5 Kinetics: Forces Moments Force. 5.0 Biomechanical Models. 5.1 Basic Link-Segment Equations-the Free-Body Diagram. 5.2 Force Transducers Plates. 5.3 Bone-on-Bone During Dynamic Conditions. 5.4 Kinetic 5.5 6 Mechanical Work, Energy, Power. 6.0 6.1 Efficiency. 6.2 Forms Energy Storage. 6.3 Internal External Work. 6.4 Power Balances at Joints Within Segments. 6.5 6.6 7 Three-Dimensional Kinematics Kinetics. 7.0 7.1 Axes Systems. 7.2 Marker Anatomical 7.3 Determination Segment Angular Velocities Accelerations. 7.4 Analysis Reaction Moments. 7.5 Suggested Further Reading. 7.6 8 Synthesis Human Movement-Forward Solutions. 8.0 8.1 Review Forward Solution 8.2 Mathematical Formulation. 8.3 System Energy. 8.4 Torques. 8.5 Designation Joints. 8.6 Illustrative Example. 8.7 Conclusions. 8.8 9 Mechanics. 9.0 9.1 Force-Length Characteristics Muscles. 9.2 Force-Velocity Characteristics. 9.3 Modeling. 9.4 10 Kinesiological Electromyography. 10.0 10.1 Electrophysiology Contraction. 10.2 Recording Electromyogram. 10.3 Electromyogram,. 10.4 between Electromyogram 10.5 11 Synergies. 11.0 11.1 The Support Moment Synergy. 11.2 Medial\/Lateral Anterior\/Posterior Balance Standing. 11.3 during Walking. 11.4 APPENDICES. A. Kinematic, Kinetic, Figure A.1 Walking Trial-Marker Locations Mass Frame Rate Information. Table Coordinate Data (cm). A.2( a ) Filtered Kinematics-Rib Cage Greater Trochanter (Hip). b Kinematics-Femoral Lateral Epicondyle (Knee) Head Fibula. c Kinematics-Lateral Malleolus (Ankle) Heel. d Kinematics-Fifth Metatarsal Toe. A.3( Linear Kinematics-Foot. Kinematics-Leg. Kinematics-Thigh. Kinematics-1\/2 HAT. A.4 Relative Joint Kinematics-Ankle, Knee, Hip. A.5( Force-Ankle Knee. Force-Hip. A.6 Potential, Total Energies-Foot, Leg, Thigh, and1\/2 A.7 Generation\/Absorption Transfer-Ankle, B. Units Definitions Related Electromyographical Measurements. B.1 Base SI Units. B.2 Derived Index."
      ],
      "categories":[
        "Biomechanics"
      ]
    },
    "list":{
      "title":[
        "Counterexample to Winkler's conjecture on Venn diagrams",
        "On the sampling entropy of permutons",
        "A classical proof of quantum knowledge for multi-prover interactive\n  proof systems",
        "Impact of UC2, UC, UBC and UB2 target compositions on the release of\n  fission products",
        "Dual-Lagrange Encoding for Storage and Download in Elastic Computing for\n  Resilience",
        "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome\n  Prediction",
        "Outer space and finiteness properties for symmetric automorphisms of\n  RAAGs, and generalisations",
        "Non-Stationary Gradient Descent for Optimal Auto-Scaling in Serverless\n  Platforms",
        "A single-loop SPIDER-type stochastic subgradient method for\n  expectation-constrained nonconvex nonsmooth optimization",
        "Finite groups admitting a regular tournament $m$-semiregular\n  representation",
        "Elastic displacements and viscous hydrodynamic flows in wedge-shaped\n  geometries featuring a straight edge: Green's functions for forces oriented\n  parallel to the edge",
        "An Extremely Hot Pulsating Pre-White Dwarf from OGLE",
        "A Max-Min problem on spectral radius and connectedness of graphs",
        "New constraints on the galactic ionizing efficiency and escape fraction\n  at 2.5 < z < 6 based on quasar absorption spectra",
        "An Efficient Dual ADMM for Huber Regression with Fused Lasso Penalty",
        "A Bayesian Proportional Mean Model Using Panel Binary Data-An\n  Application to Health and Retirement Study",
        "MoSi$_2$N$_4$-like crystals -- the new family of two-dimensional\n  materials",
        "Simulation of Random LR Fuzzy Intervals",
        "Optimal control in phase space applied to minimal-time transfer of\n  thermal atoms in optical traps",
        "Biasing with an independent increment: Gaussian approximations and\n  proximity of Poisson mixtures",
        "Mid-infrared dual comb spectroscopy via continuous-wave optical\n  parametric oscillation",
        "Analysis and Mitigation of Cascading Failures Using a Stochastic\n  Interaction Graph with Eigen-analysis",
        "A spectroscopic map of the Galactic centre -- Observations and resolved\n  stars",
        "Duality breaking, mobility edges, and the connection between topological\n  Aubry-Andr\\'e and quantum Hall insulators in atomic wires with fermions",
        "Thermodynamics-Like Formalism for Immiscible and Incompressible\n  Two-Phase Flow in Porous Media",
        "Borel fractional perfect matchings in quasi-transitive amenable graphs",
        "Decentralized Federated Dataset Dictionary Learning for Multi-Source\n  Domain Adaptation",
        "Extensions of a theorem of P. Hall on indexes of maximal subgroups",
        "Scalar-Tensor Gravity and DESI 2024 BAO data"
      ],
      "abstract":[
        "In 1984, Peter Winkler conjectured that every simple Venn diagram with $n$\ncurves can be extended to a simple Venn diagram with $n+1$ curves. We present a\ncounterexample to his conjecture for $n=7$, which is obtained by combining\ntheoretical ideas with computer assistance from state-of-the-art SAT solvers.",
        "For a permuton $\\mu$ let $H_n(\\mu)$ denote the Shannon entropy of the\nsampling distribution of $\\mu$ on $n$ points. We investigate the asymptotic\ngrowth of $H_n(\\mu)$ for a wide class of permutons.\n  We prove that if $\\mu$ has a non-vanishing absolutely continuous part, then\n$H_n(\\mu)$ has a growth rate $\\Theta(n \\log n)$. We show that if $\\mu$ is the\ngraph of a piecewise continuously differentiable, measure-preserving function\n$f$, then $H_n(\\mu)\/n$ tends to the Kolmogorov--Sinai entropy of $f$. Using\ngenericity arguments, we also prove the existence of function permutons for\nwhich $H_n(\\mu)$ does not converge either after normalizing by $n$ or by $n\\log\nn$.\n  We study the sampling entropy of a natural family of random fractal-like\npermutons determined by a sequence of i.i.d. choices. It turns out that for\nevery $n$, $H_n(\\mu)\/n$ is heavily concentrated. We prove that the sequence\n$H_n(\\mu)\/n$ either converges or has deterministic log-periodic oscillations\nalmost surely, and argue towards the conjecture that in nondegenerate case,\noscillation holds. On the other hand, for a straightforward random perturbation\nof the model $\\tilde{\\mu}$ of $\\mu$, we prove the almost sure convergence of\n$H_n(\\tilde{\\mu})\/n$.",
        "In a proof of knowledge (PoK), a verifier becomes convinced that a prover\npossesses privileged information. In combination with zero-knowledge proof\nsystems, PoKs are an important part of secure protocols such as digital\nsignature schemes and authentication schemes as they enable a prover to\ndemonstrate possession of a certain piece of information (such as a private key\nor a credential), without revealing it. Formally, A PoK is defined via the\nexistence of an extractor, which is capable of reconstructing the key\ninformation that makes a verifier accept, given oracle access to the prover. We\nextend the concept of a PoK in the setting of a single classical verifier and\ntwo quantum provers, and exhibit the PoK property for a non-local game for the\nlocal Hamiltonian problem. More specifically, we construct an extractor which,\ngiven oracle access to a provers' strategy that leads to high acceptance\nprobability, is able to reconstruct the ground state of a local Hamiltonian.\nOur result can be seen as a new form of self-testing, where, in addition to\ncertifying a pre-shared entangled state and the prover's strategy, the verifier\nalso certifies a local quantum state. This technique thus provides a method to\nascertain that a prover has access to a quantum system, in particular, a ground\nstate, thus indicating a new level of verification for a proof of quantumness.",
        "The release properties of 4 targets (UC2, UC, UBC, UB2) were measured for 11\nelements (Kr, Sr, Ru, Sn, Sb, Te, I, Cs, Ba, La, and Ce) using an off-line\ntechnique. The crystal packing fraction and the size of the studied element\nplay a key role in the release process. However, physicochemical properties are\nalso involved, notably melting and boiling points in vacuum and the minimal\noxidation state. Principal component analysis was used to investigate the\ninterrelationships between the physicochemical properties of fission products\n(from Fe to Dy) and the observed releases, thereby enabling predictions to be\nmade about the release properties of the four crystallic configurations for\nelements that are inaccessible in off-line experiments.",
        "Coded elastic computing enables virtual machines to be preempted for\nhigh-priority tasks while allowing new virtual machines to join ongoing\ncomputation seamlessly. This paper addresses coded elastic computing for\nmatrix-matrix multiplications with straggler tolerance by encoding both storage\nand download using Lagrange codes. In 2018, Yang et al. introduced the first\ncoded elastic computing scheme for matrix-matrix multiplications, achieving a\nlower computational load requirement. However, this scheme lacks straggler\ntolerance and suffers from high upload cost. Zhong et al. (2023) later tackled\nthese shortcomings by employing uncoded storage and Lagrange-coded download.\nHowever, their approach requires each machine to store the entire dataset. This\npaper introduces a new class of elastic computing schemes that utilize Lagrange\ncodes to encode both storage and download, achieving a reduced storage size.\nThe proposed schemes efficiently mitigate both elasticity and straggler\neffects, with a storage size reduced to a fraction $\\frac{1}{L}$ of Zhong et\nal.'s approach, at the expense of doubling the download cost. Moreover, we\nevaluate the proposed schemes on AWS EC2 by measuring computation time under\ntwo different tasks allocations: heterogeneous and cyclic assignments. Both\nassignments minimize computation redundancy of the system while distributing\nvarying computation loads across machines.",
        "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and\/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
        "We define the symmetric (outer) automorphism group of a right-angled Artin\ngroup and construct for it a (spine of) Outer space. This `symmetric spine' is\na contractible cube complex upon which the symmetric outer automorphism group\nacts properly and cocompactly. One artefact of our technique is a strengthening\nof the proof of contractibility of the untwisted spine, mimicking the original\nproof that Culler--Vogtmann Outer space is contractible, which may be of\nindependent interest. We apply our results to derive finiteness properties for\ncertain subgroups of outer automorphisms. In particular, we prove that the\nsubgroup consisting of those outer automorphisms which permute any given finite\nset of conjugacy classes of a right-angled Artin group is of type \\emph{VF},\nand we show that the virtual cohomological dimension of the symmetric outer\nautomorphism group is equal to both the dimension of the symmetric spine and\nthe rank of a free abelian subgroup.",
        "To efficiently manage serverless computing platforms, a key aspect is the\nauto-scaling of services, i.e., the set of computational resources allocated to\na service adapts over time as a function of the traffic demand. The objective\nis to find a compromise between user-perceived performance and energy\nconsumption. In this paper, we consider the \\emph{scale-per-request}\nauto-scaling pattern and investigate how many function instances (or servers)\nshould be spawned each time an \\emph{unfortunate} job arrives, i.e., a job that\nfinds all servers busy upon its arrival. We address this problem by following a\nstochastic optimization approach: we develop a stochastic gradient descent\nscheme of the Kiefer--Wolfowitz type that applies \\emph{over a single run of\nthe state evolution}. At each iteration, the proposed scheme computes an\nestimate of the number of servers to spawn each time an unfortunate job arrives\nto minimize some cost function. Under natural assumptions, we show that the\nsequence of estimates produced by our scheme is asymptotically optimal almost\nsurely. In addition, we prove that its convergence rate is $O(n^{-2\/3})$ where\n$n$ is the number of iterations.\n  From a mathematical point of view, the stochastic optimization framework\ninduced by auto-scaling exhibits non-standard aspects that we approach from a\ngeneral point of view. We consider the setting where a controller can only get\nsamples of the \\emph{transient} -- rather than stationary -- behavior of the\nunderlying stochastic system. To handle this difficulty, we develop arguments\nthat exploit properties of the mixing time of the underlying Markov chain. By\nmeans of numerical simulations, we validate the proposed approach and quantify\nits gain with respect to common existing scale-up rules.",
        "Many real-world problems, such as those with fairness constraints, involve\ncomplex expectation constraints and large datasets, necessitating the design of\nefficient stochastic methods to solve them. Most existing research focuses on\ncases with no {constraint} or easy-to-project constraints or deterministic\nconstraints. In this paper, we consider nonconvex nonsmooth stochastic\noptimization problems with expectation constraints, for which we build a novel\nexact penalty model. We first show the relationship between the penalty model\nand the original problem. Then on solving the penalty problem, we present a\nsingle-loop SPIDER-type stochastic subgradient method, which utilizes the\nsubgradients of both the objective and constraint functions, as well as the\nconstraint function value at each iteration. Under certain regularity\nconditions (weaker than Slater-type constraint qualification or strong\nfeasibility assumed in existing works), we establish an iteration complexity\nresult of $O(\\epsilon^{-4})$ to reach a near-$\\epsilon$ stationary point of the\npenalized problem in expectation, matching the lower bound for such tasks.\nBuilding on the exact penalization, an $(\\epsilon,\\epsilon)$-KKT point of the\noriginal problem is obtained. For a few scenarios, our complexity of either the\n{objective} sample subgradient or the constraint sample function values can be\nlower than the state-of-the-art results by a factor of $\\epsilon^{-2}$.\nMoreover, on solving two fairness-constrained problems, our method is\nsignificantly (up to 466 times) faster than the state-of-the-art algorithms,\nincluding switching subgradient method and inexact proximal point methods.",
        "For a positive integer $m$, a finite group $G$ is said to admit a tournament\n$m$-semiregular representation (TmSR for short) if there exists a tournament\n$\\Gamma$ such that the automorphism group of $\\Gamma$ is isomorphic to $G$ and\nacts semiregularly on the vertex set of $\\Gamma$ with $m$ orbits. Clearly,\nevery finite group of even order does not admit a TmSR for any positive integer\n$m$, and T1SR is the well-known tournament regular representation (TRR for\nshort). In 1986, Godsil \\cite{god} proved, by a probabilistic approach, that\nthe only finite groups of odd order without a TRR are $\\mathbb{Z}_3^2$ and\n$\\mathbb{Z}_3^3$ .\n  More recently, Du \\cite{du} proved that every finite group of odd order has a\nTmSR for every $m \\geq 2$. The author of \\cite{du} observed that a finite group\nof odd order has no regular TmSR when $m$ is an even integer, a group of order\n$1$ has no regular T3SR, and $\\mathbb{Z}_3^2$ admits a regular T3SR. At the end\nof \\cite{du}, Du proposed the following problem.\n  \\noindent{\\sf\\it Problem.} \\ \\ {\\it For every odd integer $m\\geq 3$, classify\nfinite groups of odd order which have a regular TmSR.}\n  The motivation of this paper is to give an answer for the above problem. We\nproved that if $G$ is a finite group with odd order $n>1$, then $G$ admits a\nregular TmSR for any odd integer $m\\geq 3$.",
        "For homogeneous and isotropic linearly elastic solids and for incompressible\nfluids under low-Reynolds-number conditions the fundamental solutions of the\nassociated continuum equations were derived a long time ago for bulk systems.\nThat is, the corresponding Green's functions are available in infinitely\nextended systems, where boundaries do not play any role. However, introducing\nboundaries renders the situation significantly more complex. Here, we derive\nthe corresponding Green's functions for a linearly elastic homogeneous and\nisotropic material in a wedge-shaped geometry. Two flat boundaries confine the\nmaterial and meet at a straight edge. No-slip and free-slip conditions are\nconsidered. The force is oriented in a direction parallel to the straight edge\nof the wedge. Assuming incompressibility, our expressions also apply to the\nsituation of low-Reynolds-number hydrodynamic viscous fluid flows. Thus, they\nmay be used, for instance, to describe the motion of self-propelled objects\nguided by an edge or the distortion of soft elastic actuators in wedge-shaped\nenvironments of operation.",
        "We show that the blue 18.3-minute variable object discovered in the Galactic\ndisk by the OGLE-III survey and named OGLE-GD-WD-0001 is a pulsating pre-white\ndwarf of PG 1159 spectral type. With an effective temperature of about 160,000\nK it is among the hottest known pulsators being located close to the blue edge\nof the GW Virginis instability strip. The long-term OGLE observations indicate\nthat the object has a positive period change rate of about $5 \\times 10^{-10}$\n$s~s^{-1}$ and thus already contracts. There are no traces of a planetary\nnebula around this star.",
        "In the past decades, many scholars concerned which edge-extremal problems\nhave spectral analogues? Recently, Wang, Kang and Xue showed an interesting\nresult on $F$-free graphs [J. Combin. Theory Ser. B 159 (2023) 20--41]. In this\npaper, we study the above problem on critical graphs.Let $P$ be a property\ndefined on a family $\\mathbb{G}$ of graphs. A graph $G$ in $\\mathbb{G}$ is said\nto be $P$-critical,if it has the property $P$ but $G-e$ no longer has for any\nedge $e\\in E(G)$. Especially, a graph is minimally $k$-(edge)-connected,if it\nis $k$-connected (respectively, $k$-edge connected) and deleting an arbitrary\nedge always leaves a graph which is not $k$-connected (respectively,\n$k$-edge-connected). An interesting Max-Min problem asks what is the maximal\nspectral radius of an $n$-vertex minimally $k$-(edge)-connected graphs? In\n2019, Chen and Guo [Discrete Math. 342 (2019) 2092--2099] gave the answer for\n$k=2$. In 2021, Fan, Goryainov and Lin [Discrete Appl. Math. 305 (2021)\n154--163] determined the extremal spectral radius for minimally $3$-connected\ngraphs. We obtain some structural properties of minimally $k$-(edge)-connected\ngraphs. Furthermore, we solve the above Max-Min problem for $k\\geq3$, which\nimplies that every minimally $k$-(edge)-connected graph with maximal spectral\nradius also has maximal number of edges. Finally, a general problem is posed\nfor further research.",
        "Measurements of the ionization state of the intergalactic medium (IGM) can\nprobe the sources of the extragalactic ionizing background. We provide new\nmeasurements of the ionizing emissivity of galaxies using measurements of the\nionizing background and ionizing photon mean free path from high-redshift\nquasar spectra at $2.5 < z < 6$. Unlike most prior works, we account for\nradiative-transfer effects and possible neutral islands from the tail of\nreionization at $z > 5$. We combine our results with measurements of the UV\nluminosity function to constrain the average escaping ionizing efficiency of\ngalaxies, $\\langle f_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}$. Assuming\ngalaxies with $M_{\\rm UV} < -11$ emit ionizing photons, we find $\\log (\\langle\nf_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}\/{\\rm erg^{-1}Hz}) =\n24.47_{-0.17}^{+0.09}$ and $24.75_{-0.28}^{+0.15}$ at $z=5$ and $6$, and\n$1\\sigma$ upper limits of $24.48$ and $24.31$ at $z = 2.5$ and $4$,\nrespectively. We also estimate the population-averaged $f_{\\rm esc}$ using\nmeasurements of intrinsic ionizing efficiency from JWST. We find $\\langle\nf_{\\rm esc} \\rangle = 0.126_{-0.041}^{+0.034}$ and $0.224_{-0.108}^{+0.098}$ at\n$z=5$ and $6$, and $1\\sigma$ upper limits of $f_{\\rm esc}< 0.138$ and $0.096$\nat $z=2.5$ and $4$, respectively, for $M_{\\rm UV} < -11$. Our findings are\nconsistent with prior measurements of $f_{\\rm esc} \\lesssim 10\\%$ at $z \\leq\n4$, but indicate a factor of several increase between $z = 4$ and $6$. The\nsteepness of this evolution is sensitive to the highly uncertain mean free path\nand ionizing background intensity at $z>5$. Lastly, we find\n$1.10^{+0.21}_{-0.39}$ photons per H atom are emitted into the IGM between\n$z=6$ and $=5.3$. This is $\\approx 4\\times$ more than needed to complete the\nlast $20\\%$ of reionization absent recombinations, suggesting that\nreionization's end was likely absorption-dominated.",
        "The ordinary least squares estimate in linear regression is sensitive to the\ninfluence of errors with large variance, which reduces its robustness,\nespecially when dealing with heavy-tailed errors or outliers frequently\nencountered in real-world scenarios. To address this issue and accommodate the\nsparsity of coefficients along with their sequential disparities, we combine\nthe adaptive robust Huber loss function with a fused lasso penalty. This\ncombination yields a robust estimator capable of simultaneously achieving\nestimation and variable selection. Furthermore, we utilize an efficient\nalternating direction method of multipliers to solve this regression model from\na dual perspective. The effectiveness and efficiency of our proposed approach\nis demonstrated through numerical experiments carried out on both simulated and\nreal datasets.",
        "In recurrent event studies, panel binary data arise when subjects are\nobserved at discrete time points and only the recurrent event status within\neach observation window is recorded. Such data frequently occur in longitudinal\nstudies due to recall difficulties or participants' privacy concerns during\nfollow-ups, necessitating rigorous statistical analysis. While frequentist\nmethods exist for handling such data, Bayesian approaches remain largely\nunexplored. This article proposes an efficient Bayesian proportional mean model\nfor analysing recurrent events using panel binary data. In addition to the\nestimation procedure, the article introduces techniques for model validation,\nselection, and Bayesian influence diagnostics. Simulation studies demonstrate\nthe method's effectiveness and robustness in different practical scenarios. The\nproposed approach is then applied to analyse the latest version of the Health\nand Retirement Study dataset, identifying key risk factors influencing doctor\nvisits among the elderly. The analysis is therefore capable of providing\nvaluable insights into healthcare utilisation patterns in ageing populations.",
        "Recently-synthesised MoSi$_2$N$_4$ is the first septuple layer\ntwo-dimensional material, which doesn't naturally occurs as a layered crystal,\nand has been obtained with CVD growth. It can be considered as MoN$_2$ crystal\n(with a crystal structure of MoS2) intercalating Si2N2 two-dimensional layer\n(with the structure similar to InSe). Such classification gave rise to the\nunderstanding of the electronic properties of the material, but also to the\nprediction of other members of the family (many dozens of them) as well as to\nthe way to classify those. Whereas the originally-synthesised MoSi$_2$N$_4$ is\na semiconductor, some of the members of the family are also metallic and some\neven demonstrate magnetic properties. Interestingly, the room-temperature\nmobility predicted for such crystals can be as high few thousands cm$^2$\/V*s\n(hole mobility typically higher than electron) with some record cases as high\nas $5\\times 10^4$ cm$^2$\/V*s, making these materials strong contenders for\nfuture electronic applications. The major interest towards these materials is\ncoming from the septuple layer structure, which allows multiple crystal phases,\nbut also complex compositions, in particular those with broken\nmirror-reflection symmetry against the layer of metal atoms.",
        "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
        "We present an optimal control procedure for the non-adiabatic transport of\nultracold neutral thermal atoms in optical tweezers arranged in a\none-dimensional array, with focus on reaching minimal transfer time. The\nparticle dynamics are modeled first using a classical approach through the\nLiouville equation and second through the quantum Wigner equation to include\nquantum effects. Both methods account for typical experimental noise described\nas stochastic effects through Fokker-Planck terms. The optimal control process\nis initialized with a trajectory computed for a single classical particle and\ndetermines the phase-space path that minimizes transport time and ensures high\ntransport fidelity to the target trap. This approach provides the fastest and\nmost efficient method for relocating atoms from an initial configuration to a\ndesired target arrangement, minimizing time and energy costs while ensuring\nhigh fidelity. Such an approach may be highly valuable to initialize large atom\narrays for quantum simulation or computation experiments.",
        "We establish three sets of approximation results: (a) bounds on the proximity\nof Poisson mixtures with infinitely divisible mixing distributions, (b) central\nlimit theorems with explicit error bounds for sums of associated or negatively\nassociated random variables which do not require boundedness of the underlying\ndistributions, and (c) a Gaussian approximation theorem under a vanishing third\nmoment condition. Each of these make use of the observation that size-biasing\nor zero-biasing an infinitely divisible random variable may be achieved by\nadding an independent increment, combined with tools from Stein's method for\ncompound Poisson and Gaussian approximations. Applications include a\nDickman-type limit theorem and simple random sampling.",
        "Dual-comb spectroscopy has demonstrated remarkable capabilities for rapid and\nsensitive measurements; however, significant challenges still exist in\ngenerating high-power, mutually coherent mid-infrared combs. Here we\ndemonstrate that a pair of near-infrared femtosecond frequency combs can be\nspectrally translated via a continuous-wave optical parametric oscillator. The\npair of spectrally translated combs demonstrated high mutual coherence, power\nper comb tooth in excess of hundreds of microwatts, and were tunable between 4\num and 5 um. Unlike previous approaches which relied upon synchronous optical\nparametric oscillation, the present approach avoids challenges associated with\ncomb stabilization, low power per comb tooth, and complex cavity designs.\nFurther it is readily amenable to high repetition rates (gigahertz-level and\nbeyond). The flexible and facile nature of this approach provides a robust path\nfor the spectral translation of mode-locked combs, achieving spectral\nbandwidths limited only by the phase matching bandwidth of the optical\nparametric oscillator. This approach holds significant promise for applications\nin chemical kinetics, remote sensing, combustion science, and precision\nspectroscopy, where the combination of high powers, broad bandwidths, and high\nmeasurement rates are transformative.",
        "In studies on complex network systems using graph theory, eigen-analysis is\ntypically performed on an undirected graph model of the network. However, when\nanalyzing cascading failures in a power system, the interactions among failures\nsuggest the need for a directed graph beyond the topology of the power system\nto model directions of failure propagation. To accurately quantify failure\ninteractions for effective mitigation strategies, this paper proposes a\nstochastic interaction graph model and associated eigen-analysis. Different\ntypes of modes on failure propagations are defined and characterized by the\neigenvalues of a stochastic interaction matrix, whose absolute values are\nunity, zero, or in between. Finding and interpreting these modes helps identify\nthe probable patterns of failure propagation, either local or widespread, and\nthe participating components based on eigenvectors. Then, by lowering the\nfailure probabilities of critical components highly participating in a mode of\nwidespread failures, cascading can be mitigated. The validity of the proposed\nstochastic interaction graph model, eigen-analysis and the resulting mitigation\nstrategies is demonstrated using simulated cascading failure data on an NPCC\n140-bus system.",
        "The Galactic Centre region contains a dense accumulation of stars, which can\nbe separated into two components: A flattened and dense nuclear star cluster\n(NSC), and a surrounding, more extended and more flattened, nuclear stellar\ndisc (NSD). Previous studies have collected a few thousand spectra of the inner\nNSC, and also the outer NSD, and measured line-of-sight velocities and\nmetallicities. Until now, such measurements exist only for a few 100 stars in\nthe region where the stellar surface density transitions from being dominated\nby the NSC into being dominated by the NSD. We want to study the stellar\npopulation from the centre of the NSC out to well beyond its effective radius,\nwhere the NSD dominates. We investigate whether and how the mean properties and\nkinematics of the stars change systematically. We conducted spectroscopic\nobservations with Flamingos-2 in the K-band via a continuous slit-scan. The\ndata extend from the central NSC into the inner NSD, out to 32 pc from Sgr A*\nalong Galactic longitude l. Based on their CO equivalent width, we classify the\nstars as hot or cool stars. The former are massive, young stars, while almost\nall of the latter are older than one to a few Gyr. We measure the overall\nmetallicity [M\/H] and line-of-sight velocity for >2,500 cool stars, and present\nthe first continuous spatial maps and profiles of the mean value of various\nstellar and kinematic parameters. We identify hot, young stars across the field\nof view. Some stars appear to be isolated, while others accumulate near the\nQuintuplet cluster or the central parsec cluster. The position-velocity curve\nof the cool stars shows no dependence on [M\/H], but it depends on the colour of\nthe stars. The colour may be a tracer of the line-of-sight distance and thus\ndistinguish stars located in the NSC from those in the NSD. [abridged]",
        "It is well known that the Aubry-Andr{\\'e} model lacks mobility edges due to\nits energy-independent self-duality but may exhibit edge states. When duality\nis broken, we show that mobility regions arise and non-trivial topological\nphases emerge. By varying the degree of duality breaking, we identify mobility\nregions and establish a connection between Aubry-Andr{\\'e} atomic wires with\nfermions and quantum Hall systems for a family of Hamiltonians that depends on\nthe relative phase of laser fields, viewed as a synthetic dimension. Depending\non the filling factor and the degree of duality breaking, we find three classes\nof non-trivial phases: conventional topological insulator, conventional\ntopological Aubry-Andr{\\'e} insulator, and unconventional (hybrid) topological\nAubry-Andr{\\'e} insulator. Finally, we discuss appropriate Chern numbers that\nillustrate the classification of topological phases of localized fermions in\natomic wires.",
        "It is possible to formulate immiscible and incompressible two-phase flow in\nporous media in a mathematical framework resembling thermodynamics based on the\nJaynes generalization of statistical mechanics. We review this approach and\ndiscuss the meaning of the emergent variables that appear, agiture, flow\nderivative and flow pressure, which are conjugate to the configurational\nentropy, the saturation and the porosity respectively. We conjecture that the\nagiture, the temperature-like variable, is directly related to the pressure\ngradient. This has as a consequence that the configurational entropy, a measure\nof how the fluids are distributed within the porous media and the accompanying\nvelocity field, and the differential mobility of the fluids are related. We\nalso develop elements of another version of the thermodynamics-like formalism\nwhere fractional flow rather than saturation is the control variable, since\nthis is typically the natural control variable in experiments.",
        "We show that if a locally finite Borel graph with quasitransitive amenable\ncomponents admits a fractional perfect matching, it will admit a Borel\nfractional perfect matching. In particular, if a countable amenable\nquasitransitive graph admits a fractional perfect matching then its Bernoulli\ngraph admits a Borel fractional perfect matching.",
        "Decentralized Multi-Source Domain Adaptation (DMSDA) is a challenging task\nthat aims to transfer knowledge from multiple related and heterogeneous source\ndomains to an unlabeled target domain within a decentralized framework. Our\nwork tackles DMSDA through a fully decentralized federated approach. In\nparticular, we extend the Federated Dataset Dictionary Learning (FedDaDiL)\nframework by eliminating the necessity for a central server. FedDaDiL leverages\nWasserstein barycenters to model the distributional shift across multiple\nclients, enabling effective adaptation while preserving data privacy. By\ndecentralizing this framework, we enhance its robustness, scalability, and\nprivacy, removing the risk of a single point of failure. We compare our method\nto its federated counterpart and other benchmark algorithms, showing that our\napproach effectively adapts source domains to an unlabeled target domain in a\nfully decentralized manner.",
        "We extend a classical theorem of P. Hall that claims that if the index of\nevery maximal subgroup of a finite group $G$ is a prime or the square of a\nprime, then $G$ is solvable. Precisely, we prove that if one allows, in\naddition, the possibility that every maximal subgroup of $G$ is nilpotent\ninstead of having prime or squared-prime index, then $G$ continues to be\nsolvable. Likewise, we obtain the solvability of $G$ when we assume that every\nproper non-maximal subgroup of $G$ lies in some subgroup of index prime or\nsquared prime.",
        "We discuss the implications of the DESI 2024 BAO data on scalar-tensor models\nof gravity. We consider four representative models: induced gravity (IG,\nequivalent to Jordan-Brans-Dicke), where we either fix today's value of the\neffective gravitational constant on cosmological scales to the Newton's\nconstant or allow them to differ, Jordan-Brans-Dicke supplemented with a\nGalileon term (BDG), and early modified gravity (EMG) with a conformal\ncoupling. In this way it is possible to investigate how different modified\ngravity models compare with each other when confronted with DESI 2024 BAO data.\nCompared to previous analyses, for all of these models, the combination of\nPlanck and DESI data favors a larger value of the key parameter of the theory,\nsuch as the nonminimal coupling to gravity or the Galileon term, leading also\nto a larger value of $H_0$, due to the known degeneracy between these\nparameters. These new results are mainly driven by the first two redshift bins\nof DESI. In BDG, in which we find the largest value for $H_0$ among the models\nconsidered, the combination of Planck and DESI is consistent with CCHP results\nand reduces the $H_0$ tension with the SH0ES measurement to $1.2\\sigma$\n(compared to $4.5\\sigma$ of $\\Lambda$CDM in our Planck + DESI analysis)."
      ]
    }
  }
]