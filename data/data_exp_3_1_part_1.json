[
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms",
    "start_abstract":"Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b1"
      ],
      "title":[
        "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
        "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
      ],
      "abstract":[
        "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
        "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
      ],
      "categories":[
        "q-bio.CB",
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Ultra-high-energy $\\gamma$-ray emission associated with the tail of a\n  bow-shock pulsar wind nebula",
        "A new algorithm for detecting X-ray shots in Cyg X-1",
        "Feedforward Cancellation of High-Frequency Phase Noise in\n  Frequency-Doubled Lasers",
        "Exact Fluctuating Hydrodynamics of the Scaled Light-Heavy Model",
        "Uniqueness of solutions to elliptic and parabolic equations on metric\n  graphs",
        "A sharper Lyapunov-Katz central limit error bound for i.i.d. summands\n  Zolotarev-close to normal",
        "Unitary Friedberg-Jacquet periods and their twists: Fundamental lemmas",
        "Quantum One-Time Memories from Stateless Hardware, Random Access Codes,\n  and Simple Nonconvex Optimization",
        "A monotonicity-based globalization of the level-set method for inclusion\n  detection",
        "Attention-Based Functional-Group Coarse-Graining: A Deep Learning\n  Framework for Molecular Prediction and Design",
        "Separation control applied to the turbulent flow around a NACA4412 wing\n  section",
        "A Cheeger-type inequality for the drift Laplacian with Wentzell-type\n  boundary condition",
        "Complete intersection algebras with binomial Macaulay dual generator",
        "Time derivative estimates for parabolic $p$-Laplace equations and\n  applications to optimal regularity",
        "A family of convolution operators, part two",
        "$\\Lambda$CDM model against redshift-binned data: A mock analysis based\n  on SNIa and Cosmic Chronometers",
        "Two characterizations of Sheffer-Dunkl sequences",
        "Search for resonance-enhanced $CP$ and angular asymmetries in the\n  $\\Lambda^+_{c}\\to p\\mu^+\\mu^-$ decay at LHCb",
        "Revealing Local Structures through Machine-Learning- Fused Multimodal\n  Spectroscopy",
        "The Effect of Foreground Galaxies on the Estimation of the Hubble\n  Constant from Type Ia Supernovae",
        "Remark to a Theorem of van Geemen",
        "Towards a Generalized SA Model: Symbolic Regression-Based Correction for\n  Separated Flows",
        "BCS-like formula for $T_c$ does not necessarily imply BCS pairing\n  mechanism",
        "Stochastic quantization of $\\lambda \\phi_2^4$- theory in 2-d Moyal space",
        "Quantum chaos at finite temperature in local spin Hamiltonians",
        "Viscosity, entanglement and acceleration",
        "Fast Jet Finding in Julia",
        "Quantitative Magnetohydrodynamic Modelling of Flux Pumping in ASDEX\n  Upgrade"
      ],
      "abstract":[
        "In this study, we present a comprehensive analysis of an unidentified\npoint-like ultra-high-energy (UHE) $\\gamma$-ray source, designated as 1LHAASO\nJ1740+0948u, situated in the vicinity of the middle-aged pulsar PSR J1740+1000.\nThe detection significance reached 17.1$\\sigma$ (9.4$\\sigma$) above 25$\\,$TeV\n(100$\\,$TeV). The source energy spectrum extended up to 300$\\,$TeV, which was\nwell fitted by a log-parabola function with $N0 = (1.93\\pm0.23) \\times 10^{-16}\n\\rm{TeV^{-1}\\,cm^{-2}\\,s^{-2}}$, $\\alpha = 2.14\\pm0.27$, and $\\beta =\n1.20\\pm0.41$ at E0 = 30$\\,$TeV. The associated pulsar, PSR J1740+1000, resides\nat a high galactic latitude and powers a bow-shock pulsar wind nebula (BSPWN)\nwith an extended X-ray tail. The best-fit position of the gamma-ray source\nappeared to be shifted by $0.2^{\\circ}$ with respect to the pulsar position. As\nthe (i) currently identified pulsar halos do not demonstrate such offsets, and\n(ii) centroid of the gamma-ray emission is approximately located at the\nextension of the X-ray tail, we speculate that the UHE $\\gamma$-ray emission\nmay originate from re-accelerated electron\/positron pairs that are advected\naway in the bow-shock tail.",
        "The short-term X-ray variability of Cyg X-1 can be interpreted as random\noccurrence of mini-flares known as the shots, whose physical nature is still\nunclear. We propose a new algorithm for shot identification in the X-ray light\ncurve, based on baseline detection and template fitting. Compared with previous\ntechniques, our algorithm allows us to detect shots with lower amplitudes and\nshorter time separations. With NICER observations, we find that, after\ncorrection for detection sensitivity, both the shot amplitude and recurrence\nrate are positively scaled with the mean count rate, while the recurrence rate\nhas a much higher dependence on the count rate. These suggest that a higher\nmass accretion rate will drive more and slightly larger shots. We also find\nthat the abrupt hardening near the shot peak found in previous studies is\nattributed to different shot profiles in different energy bands; there is no\nneed to involve a rapid physical process to suddenly harden the emitting\nspectrum.",
        "The cancellation of high-frequency laser phase noise using feedforward\ntechniques, as opposed to feedback methods, has achieved significant\nadvancements in recent years. However, directly applying existing feedforward\ntechniques to laser systems based on nonlinear conversion still faces\nsubstantial challenges. Here, we propose and demonstrate a feedforward scheme\nthat suppresses phase noise in frequency-doubled light by utilizing phase noise\ninformation of its fundamental pump. This scheme is enabled by the fact that\nthe phase jitter of the frequency-doubled light is simply twice that of the\npump, except for a first-order low-pass filtering effect introduced by the SHG\nenhancement cavity. Testing this method on a 420-nm frequency-doubled laser\nsystem, we realize a 25-dB suppression of the servo noise bump near 1 MHz on\nthe 420-nm light, and an average suppression of 30 dB for strong injected noise\nranging from 100 kHz to 20 MHz. This scheme shows promising potential for\napplications requiring blue or ultraviolet light with minimal high-frequency\nphase noise, such as precision control of atoms and molecules.",
        "We study the exact fluctuating hydrodynamics of the scaled Light-Heavy model\n(sLH), in which two species of particles (light and heavy) interact with a\nfluctuating surface. This model is similar in definition to the unscaled\nLight-Heavy model (uLH), except it uses rates scaled with the system size. The\nconsequence, it turns out, is a phase diagram that differs from that of the\nunscaled model. We derive the fluctuating hydrodynamics for this model using an\naction formalism involving the construction of path integrals for the\nprobability of different states that give the complete macroscopic picture\nstarting from the microscopic one. This is then used to obtain the two-point\nsteady-state (static) correlation functions between fluctuations in the two\ndensity fields in the homogeneous phase. We show that these theoretical results\nmatch well with microscopic simulations away from the critical line. We derive\nan exponentially decaying form for the two-point steady-state correlation\nfunction with a correlation length that diverges as the critical line is\napproached. Finally, we also compute the dynamic correlations in the\nhomogeneous phase and use them to determine the relaxation dynamics as well as\nthe dynamic exponents of the system.",
        "We investigate uniqueness of solutions to certain classes of elliptic and\nparabolic equations posed on metric graphs. In particular, we address the\nlinear Schr\\\"odinger equation with a potential, and the heat equation with a\nvariable density. We assume suitable growth conditions on the solutions, which\nare related to the behaviour at infinity of the potential or of the density.",
        "We prove a central limit error bound for convolution powers of laws with\nfinite moments of order $r \\in \\mathopen]2,3\\mathclose]$, taking a closeness of\nthe laws to normality into account. Up to a universal constant, this\ngeneralises the case of $r=3$ of the sharpening of the Berry (1941) - Esseen\n(1942) theorem obtained by Mattner (2024), namely by sharpening here the Katz\n(1963) error bound for the i.i.d. case of Lyapunov's (1901) theorem. Our proof\nuses a partial generalisation of the theorem of Senatov and Zolotarev (1986)\nused for the earlier special case. A result more general than our main one\ncould be obtained by using instead another theorem of Senatov (1980), but\nunfortunately an auxiliary inequality used in the latter's proof is wrong.",
        "We formulate a global conjecture for the automorphic period integral\nassociated to the symmetric pairs defined by unitary groups over number fields,\ngeneralizing a theorem of Waldspurger's toric period for $\\mathrm{GL}(2)$. We\nintroduce a new relative trace formula to prove our global conjecture under\nsome local hypotheses. A new feature is the presence of the relative endoscopy.\nIn this paper we prove the main local theorem: a new relative fundamental lemma\ncomparing certain orbital integrals of functions matched in terms of Hironaka\nand Satake transforms.",
        "We present a construction of one-time memories (OTMs) using\nclassical-accessible stateless hardware, building upon the work of Broadbent et\nal. and Behera et al.. Unlike the aforementioned work, our approach leverages\nquantum random access codes (QRACs) to encode two classical bits, $b_0$ and\n$b_1$, into a single qubit state $\\mathcal{E}(b_0 b_1)$ where the receiver can\nretrieve one of the bits with a certain probability of error. To prove\nsoundness, we define a nonconvex optimization problem over POVMs on\n$\\mathbb{C}^2$. This optimization gives an upper bound on the probability of\ndistinguishing bit $b_{1-\\alpha}$ given that the probability that the receiver\nrecovers bit $b_\\alpha$ is high. Assuming the optimization is sufficiently\naccurate, we then prove soundness against a polynomial number of classical\nqueries to the hardware.",
        "We focus on a geometrical inverse problem that involves recovering\ndiscontinuities in electrical conductivity based on boundary measurements. This\nproblem serves as a model to introduce a shape recovery technique that merges\nthe monotonicity method with the level-set method. The level-set method,\ncommonly used in shape optimization, often relies heavily on the accuracy of\nthe initial guess. To overcome this challenge, we utilize the monotonicity\nmethod to generate a more precise initial guess, which is then used to\ninitialize the level-set method. We provide numerical results to illustrate the\neffectiveness of this combined approach.",
        "Machine learning (ML) offers considerable promise for the design of new\nmolecules and materials. In real-world applications, the design problem is\noften domain-specific, and suffers from insufficient data, particularly labeled\ndata, for ML training. In this study, we report a data-efficient, deep-learning\nframework for molecular discovery that integrates a coarse-grained\nfunctional-group representation with a self-attention mechanism to capture\nintricate chemical interactions. Our approach exploits group-contribution\ntheory to create a graph-based intermediate representation of molecules,\nserving as a low-dimensional embedding that substantially reduces the data\ndemands typically required for training. By leveraging the self-attention\nmechanism to learn subtle chemical context, our method consistently outperforms\nconventional methods in predicting multiple thermophysical properties. In a\ncase study focused on adhesive polymer monomers, we train on a limited dataset\ncomprising just 6,000 unlabeled and 600 labeled monomers. The resulting\nchemistry prediction model achieves over 92% accuracy in forecasting properties\ndirectly from SMILES strings, exceeding the performance of current\nstate-of-the-art techniques. Furthermore, the latent molecular embedding is\ninvertible, allowing the design pipeline to incorporate a decoder that can\nautomatically generate new monomers from the learned chemical subspace. We\nillustrate this functionality by targeting high and low glass transition\ntemperatures ($T_g$), successfully identifying novel candidates whose $T_g$\nextends beyond the range observed in the training data. The ease with which our\ncoarse-grained, attention-based framework navigates both chemical diversity and\ndata scarcity offers a compelling route to accelerate and broaden the search\nfor functional materials.",
        "We carried out high-resolution large-eddy simulations (LESs) to investigate\nthe effects of several separation-control approaches on a NACA4412 wing section\nwith spanwise width of $L_z = 0.6$ at an angle of attack of $AoA=11^{\\circ}$ at\na Reynolds number of $Re_c = 200,000$ based on chord length $c$ and free-stream\nvelocity $U_{\\infty}$. Two control strategies were considered: (1) steady\nuniform blowing and\/or suction on the suction and\/or pressure sides, and (2)\nperiodic control on the suction side. A wide range of control configurations\nwere evaluated in terms of aerodynamic efficiency (i.e., lift-to-drag ratio)\nand separation delay. Uniform blowing and\/or suction effectively delayed flow\nseparation, leading to a lift increase of up to $11\\%$, but yielded only\nmarginal improvements in aerodynamic efficiency. In contrast, periodic control\nneither enhanced separation delay nor improved efficiency. A detailed analysis\nof the interaction between uniform blowing and\/or suction and turbulent\nboundary layers (TBLs) over the wing was performed, including assessments of\n(1) integral boundary-layer quantities, (2) turbulence statistics, and (3)\npower-spectral densities. Significant modifications in Reynolds stresses and\nspectral characteristics were observed. To the authors' best knowledge, this is\nthe first numerical study utilizing high-resolution LESs to provide\ncomprehensive assessments on separation control.",
        "We prove lower bounds for the first non-trivial eigenvalue of the drift\nLaplacian on manifolds with Wentzell-type boundary condition in terms of some\nCheeger-type constants for bulk-boundary interactions. Our results are in the\nspirit of Cheeger's classical inequality.",
        "In this paper, we characterize all Artinian complete intersection\n$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In\naddition, we prove that such\n  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong\nLefschetz property.",
        "We establish the boundedness of time derivatives of solutions to parabolic\n$p$-Laplace equations. Our approach relies on the Bernstein technique combined\nwith a suitable approximation method. As a consequence, we obtain an optimal\nregularity result with a connection to the well-known $C^{p'}$-conjecture in\nthe elliptic setting. Finally, we extend our method to treat global regularity\nresults for both fully nonlinear and general quasilinear degenerate parabolic\nproblems.",
        "We study a family of convolution operators. Their regarding Fourier\nmultipliers are defined in terms of distributions having singularity on the\nlight-cone in $\\mathbb{R}^{n+1}$. As a result, we give a new approach to the\nBochner-Riesz summability.",
        "Despite the broad successes of the flat $\\Lambda$CDM model and its fitness to\nthe various cosmological observations, it confronts challenges stemming from\nanomalies in the measurements of the Hubble constant ($H_0$) and the amplitude\nof matter fluctuations ($\\sigma_8$). These inconsistencies have necessitated a\nreassessment of the model parameters, with a particular focus on their\npotential dependence on redshift. This study pioneers a new investigation to\nprobe this redshift dependency by generating mock data simulated from\nobservational data of Type Ia supernovae (SNIa) and cosmic chronometers (CC),\nthereby increasing the data density in this field. By sorting the data into\nhigh-redshift and low-redshift bins, we aim to refine the cosmological\nconstraints on the parameters of the $\\Lambda$CDM model and determine whether\nthe noted dependence on redshift is due to a lack of high-redshift\nobservational data or if they signify intrinsic issues within the model itself.\nOur approach employs the Markov Chain Monte Carlo (MCMC) algorithm to minimize\nthe $\\chi^2$ function, thus tightening the cosmological constraints. Our\nfindings within the mock analysis reveal discrepancies between the values of\n$\\Omega_{m0}$ and $H_0$ derived from the mock data bins with high redshift and\nlow redshift, indicating the potential deviation of the standard $\\Lambda$ CDM\ncosmology from the high-redshift SNIa and CC data. If this deviation proposes a\nnew physics beyond the standard model, then with better quality future data\ntracking the new physics, these discrepancies will be statistically\nsignificant.",
        "Sheffer polynomials can be characterized using different Stieltjes integrals.\nThese families of polynomials have been recently extended to the Dunkl context.\nIn this way some classical operators as the derivative operator or the\ndifference operator are replaced as analogous operators in the Dunkl universe.\nIn this paper we establish two Stieltjes integrals that help us to characterize\nthe Sheffer-Dunkl polynomials.",
        "The first measurement of the $CP$ asymmetry of the decay rate ($A_{CP}$) and\nthe $CP$ average ($\\Sigma A_{\\text{FB}}$) and $CP$ asymmetry ($\\Delta\nA_{\\text{FB}}$) of the forward-backward asymmetry in the muon system of\n$\\mathit{\\Lambda}^+_c\\to p\\mu^+\\mu^-$ decays is reported. The measurement is\nperformed using a data sample of proton-proton collisions, recorded by the LHCb\nexperiment from 2016 to 2018 at a center-of-mass energy of 13$\\text{ TeV}$,\nwhich corresponds to an integrated luminosity of 5.4$\\text{ fb}^{-1}$. The\nasymmetries are measured in two regions of dimuon mass near the $\\phi$-meson\nmass peak. The dimuon-mass integrated results are \\begin{align*} A_{CP} &=\n(-1.1 \\pm 4.0 \\pm 0.5)\\%,\\\\ \\Sigma A_{\\text{FB}} &= (\\phantom{-}3.9 \\pm 4.0 \\pm\n0.6)\\%,\\\\ \\Delta A_{\\text{FB}} &= (\\phantom{-}3.1 \\pm 4.0 \\pm 0.4)\\%,\n\\end{align*} where the first uncertainty is statistical and the second\nsystematic. The results are consistent with the conservation of $CP$ symmetry\nand the Standard Model expectations.",
        "Atomistic structures of materials offer valuable insights into their\nfunctionality. Determining these structures remains a fundamental challenge in\nmaterials science, especially for systems with defects. While both experimental\nand computational methods exist, each has limitations in resolving nanoscale\nstructures. Core-level spectroscopies, such as x-ray absorption (XAS) or\nelectron energy-loss spectroscopies (EELS), have been used to determine the\nlocal bonding environment and structure of materials. Recently, machine\nlearning (ML) methods have been applied to extract structural and bonding\ninformation from XAS\/EELS, but most of these frameworks rely on a single data\nstream, which is often insufficient. In this work, we address this challenge by\nintegrating multimodal ab initio simulations, experimental data acquisition,\nand ML techniques for structure characterization. Our goal is to determine\nlocal structures and properties using EELS and XAS data from multiple elements\nand edges. To showcase our approach, we use various lithium nickel manganese\ncobalt (NMC) oxide compounds which are used for lithium ion batteries,\nincluding those with oxygen vacancies and antisite defects, as the sample\nmaterial system. We successfully inferred local element content, ranging from\nlithium to transition metals, with quantitative agreement with experimental\ndata. Beyond improving prediction accuracy, we find that ML model based on\nmultimodal spectroscopic data is able to determine whether local defects such\nas oxygen vacancy and antisites are present, a task which is impossible for\nsingle mode spectra or other experimental techniques. Furthermore, our\nframework is able to provide physical interpretability, bridging spectroscopy\nwith the local atomic and electronic structures.",
        "Type Ia supernovae are the established `standard candle' in the construction\nof the Hubble diagram out to high luminosity distances. Since the Hubble\nconstant that best fits observations of these supernovae often turns out to be\nhigh compared to fits to other data, they are currently being investigated for\npossible systematic effects, with many studies focusing on the calibration of\nthe distance ladder in the local Universe. Here we present a simulation-based\nassessment of another type of systematic effect, related to the chance that the\nline of sight to a distant supernova passes close to a foreground galaxy. We\nconsider two cases separately: First, the foreground galaxy may block the line\nof sight so that the supernova is not observed. Since foreground galaxies are\ncorrelated with overdensities that typically magnify the flux of background\nsources, this effect leads to a systematic removal of lensed supernovae from\nthe sample, biasing the high-redshift Hubble diagram towards demagnified\n(fainter) supernovae. Second, if the supernova can be observed, its proximity\nto the foreground galaxy can lead to an incorrect host assignment, especially\nif the true host has a low surface brightness. Since foreground galaxies are\ntypically found at lower redshifts, this effect introduces another systematic\nbias. The probability of line-of-sight alignments with foreground galaxies\nincreases with redshift and therefore affects distant supernovae more strongly.\nWe find that both effects are small, but the effect of host misidentification\nshould be included in the systematic error budget at current levels of\nmeasurement precision.",
        "In [ Ge], Bert van Geemen computed the dimension of the space of the fourth\npower of the theta nullwerte.\n  In [SM2], it has been observe that all linear relations between the\n$\\theta_m^4$ are consequences of the quartic Riemann relations.\n  In this note, we want to give a new proof of these result and extend them.",
        "This study focuses on the numerical simulation of high Reynolds number\nseparated flows and proposes a data-driven approach to improve the predictive\ncapability of the SA turbulence model. First, data assimilation was performed\non two typical airfoils with high angle-of-attack separated flows to obtain a\nhigh-fidelity flow field dataset. Based on this dataset, a white-box model was\ndeveloped using symbolic regression to modify the production term of the SA\nmodel. To validate the effectiveness of the modified model, multiple\nrepresentative airfoils and wings, such as the SC1095 airfoil, DU91-W2-250\nairfoil, and ONERA-M6 wing, were selected as test cases. A wide range of flow\nconditions was considered, including subsonic to transonic regimes, Reynolds\nnumbers ranging from hundreds of thousands to tens of millions, and angles of\nattack varying from small to large. The results indicate that the modified\nmodel significantly improves the prediction accuracy of separated flows while\nmaintaining the predictive capability for attached flows. It notably enhances\nthe reproduction of separated vortex structures and flow separation locations,\nreducing the mean relative error in lift prediction at stall angles by 69.2%\nand improving computational accuracy by more than three times. Furthermore,\nvalidation using a zero-pressure-gradient flat plate case confirms the modified\nmodel's ability to accurately predict the turbulent boundary layer velocity\nprofile and skin friction coefficient distribution. The findings of this study\nprovide new insights and methodologies for the numerical simulation of high\nReynolds number separated flows, contributing to more accurate modeling of\ncomplex flow phenomena in engineering applications.",
        "In the BCS theory of superconductivity, an instability towards pairing\ndevelops at arbitrary weak dimensionless coupling $\\lambda$ due to a divergence\nof logarithmic perturbative series for the pairing susceptibility (Cooper\nlogarithms) at $T_c \\sim \\omega_0 e^{-1\/\\lambda}$, where $\\omega_0$ is an\nenergy cutoff. On the contrary, in many models of superconductivity out of a\nnon-Fermi liquid, Cooper logarithm is absent and superconductivity emerges only\nwhen $\\lambda$ exceeds a certain threshold. We argue that there are situations\nwhen there is no threshold and at weak coupling the formula for $T_c$ is\nBCS-like, yet the origin of the pairing instability is fundamentally different\nfrom that in the BCS scenario. As an example, we revisit superconductivity in a\nmetal at the onset of $(\\pi,\\pi)$ spin-density-wave order. Earlier studies of\nthis problem found no threshold and a BCS-like expression for $T_c$ at weak\ncoupling. We argue that, despite this, the pairing is not caused by the Cooper\nlogarithm and in many respects is qualitatively similar to that in non-Fermi\nliquids.",
        "There is strong evidence for the conjecture that the $\\lambda \\phi^4$ QFT-\nmodel on 4-dimensional non-commutative Moyal space can be non-perturbatively\nconstructed. As preparation, in this paper we construct the 2-dimensional case\nwith the method of stochastic quantization. We show the local well-posedness\nand global well-posedness of the stochastic quantization equation, leading to a\nconstruction of the Moyal $\\lambda \\phi^4_2$ measure for any non-negative\ncoupling constant $\\lambda$.",
        "Understanding the emergence of chaos in many-body quantum systems away from\nsemi-classical limits, particularly in spatially local interacting spin\nHamiltonians, has been a long-standing problem. In these intrinsically quantum\nregimes, quantum chaos has been primarily understood through the correspondence\nbetween the eigensystem statistics of midspectrum eigenstates and the universal\nstatistics described by random matrix theory (RMT). However, this\ncorrespondence no longer holds for finite-temperature eigenstates. Here we show\nthat the statistical properties of finite-temperature eigenstates of quantum\nchaotic Hamiltonians can be accurately described by pure random states\nconstrained by a local charge, with the average charge density of the\nconstrained random state ensemble playing the same role as the average energy\ndensity of the eigenstates. By properly normalizing the energy density using a\nsingle Hamiltonian-dependent parameter that quantifies the typical energy per\ndegree of freedom, we find excellent agreement between the entanglement entropy\nstatistics of eigenstates and that of constrained random states. Interestingly,\nin small pockets of Hamiltonian parameter phase space which we previously\nidentified as `maximally chaotic' [PRX 14, 031014 (2024)], we find excellent\nagreement not only at the level of the first moment, including O(1)\ncorrections, but also at the level of statistical fluctuations. These results\nshow that notions of maximal chaos -- in terms of how much randomness\neigenstates contain -- can still be defined at finite temperature in physical\nHamiltonian models away from semi-classical and large-$N$ limits.",
        "The Minkowski vacuum in an accelerated frame behaves like a fluid that has\nnot only a finite temperature due to the Unruh effect, but also a finite shear\nviscosity. Moreover, the ratio of this viscosity to the entropy density exactly\nsatisfies the Kovtun-Son-Starinets (KSS) bound, inspired by the string theory $\n\\eta\/s=1\/4\\pi $. The origin of this viscosity is purely kinematical and is\nbelieved to be related to entanglement introduced by the Rindler horizon. We\ndirectly calculate the viscosity, entropy density, and their ratio for massless\nfields with spins 1\/2 and 1. We show that locally the ratio of viscosity to\nentropy density can be below the limiting value $ 1\/4\\pi $ at distances of the\norder of the thickness of the membrane corresponding to the stretched horizon,\nand is described by the universal function for different spins. In particular,\non the membrane surface $ \\eta\/s=1\/8\\pi $.",
        "Jet reconstruction remains a critical task in the analysis of data from HEP\ncolliders. We describe in this paper a new, highly performant, Julia package\nfor jet reconstruction, JetReconstruction.jl, which integrates into the growing\necosystem of Julia packages for HEP. With this package users can run sequential\nreconstruction algorithms for jets. In particular, for LHC events, the\nAnti-${k}_\\text{T}$, Cambridge\/Aachen and Inclusive-${k}_\\text{T}$ algorithms\ncan be used. For FCCee studies the use of alternative algorithms such as the\nGeneralised ${k}_\\text{T}$ for $e^+e^-$ and Durham are also supported.\n  The performance of the core algorithms is better than Fastjet's C++\nimplementation, for typical LHC and FCCee events, thanks to the Julia\ncompiler's exploitation of single-instruction-multiple-data (SIMD), as well as\nergonomic compact data layouts.\n  The full reconstruction history is made available, allowing inclusive and\nexclusive jets to be retrieved. The package also provides the means to\nvisualise the reconstruction. Substructure algorithms have been added that\nallow advanced analysis techniques to be employed. The package can read event\ndata from EDM4hep files and reconstruct jets from these directly, opening the\ndoor to FCCee and other future collider studies in Julia.",
        "The sawtooth-free hybrid scenario has been achieved recently in ASDEX Upgrade\n(AUG) with applied non-inductive current sources and auxiliary heating [A.\nBurckhart et al 2023 Nucl. Fusion 63 126056]. Control experiments in AUG\nsuggest that the self-regulating magnetic flux pumping mechanism, characterized\nby anomalous current redistribution, is responsible for clamping the central\nsafety factor (q_0) close to unity, thereby preventing the sawtooth onset. This\nwork presents a numerical and theoretical investigation of flux pumping in the\nAUG hybrid scenario based on the two-temperature, visco-resistive, full\nmagnetohydrodynamic (MHD) model with the JOREK code. To quantitatively model\nthe flux pumping, we choose realistic parameters, plasma configurations, and\nsource terms based on AUG experiments. During the initial saturation stage of\nthe unstable 1\/1 quasi-interchange mode (on millisecond timescales), q_0\nexhibits fast under-damped oscillation and reaches a value closer to unity,\nwhich is attributed to the self-regulation of core plasma and the fast dynamo\neffect on the order of V\/m. On the longer resistive diffusion timescale of\nseconds, the slow negative dynamo effect on the order of mV\/m induced by the\n1\/1 MHD instability plays an effective role in flux pumping, which provides\nquantitative agreement with experimental observations for the first time. The\nfinal saturated 1\/1 MHD instability exhibits features of the quasi-interchange\nmode and tearing mode, and the associated convective plasma flow velocity is a\nfew m\/s. The toroidal negative electric field from the slow dynamo dominantly\noffsets the positive current drive and continuously redistributes the current\ndensity and pressure. As a result, q_0 is maintained close to unity due to the\nlow-shear profiles of current density and pressure in the plasma core, and the\nsystem enters a sawtooth-free and quasi-stationary helical state."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Screening for lung cancer: 2023 guideline update from the American Cancer Society",
    "start_abstract":"Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies",
        "The Electric Location-Routing Problem: Improved Formulations and Effects\n  of Nonlinear Charging",
        "DiffVSR: Revealing an Effective Recipe for Taming Robust Video\n  Super-Resolution Against Complex Degradations",
        "Entanglement dynamics of many-body quantum states with evolving system\n  conditions",
        "An Accurate Computational Approach for Partial Likelihood Using\n  Poisson-Binomial Distributions",
        "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel\n  Tool Invocation",
        "Two-sided Remotely Almost Periodic Solutions of Ordinary Differential\n  Equations in Banach Spaces",
        "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "Comparison of Bending-Energy Discretization Methods for Anisotropic\n  Meshes in Morphogenetic Simulations",
        "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
        "Counting for rigidity under projective transformations in the plane",
        "Green points in the reals",
        "Fault-tolerance of [[6, 1, 3]] non-CSS code family generated using\n  measurements on graph states",
        "Design of resilient structures by randomization and bistability",
        "Getting SMARTER for Motion Planning in Autonomous Driving Systems"
      ],
      "abstract":[
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones.",
        "Electric Location-Routing models (ELRP) can contribute to the effective\nplanning of electric vehicles (EVs) fleets and charging infrastructure within\nEV logistic networks because it simultaneously combines routing and location\ndecisions to find optimal solutions to the network design. This study\nintroduces ELRP models that incorporate nonlinear charging process, multiple\ncharging station types and develop new improved formulations to the problem.\nExisting ELRP models commonly assume a linear charging process and employ a\nnode-based formulation for tracking EV energy and time consumption. In\ncontrast, we propose novel formulations offering alternative approaches for\nmodeling EV energy, time consumption, and nonlinear charging. Through extensive\ncomputational experiments, our analysis demonstrates the effectiveness of the\nnew formulations, reducing the average gap from 29.1% to 11.9%, yielding\nimproved solutions for 28 out of 74 instances compared to the node-based\nformulation. Moreover, our findings provide valuable insights into the\nstrategic implications of nonlinear charging in ELRP decision-making, offering\nnew perspectives for planning charging infrastructure in EV logistic networks.",
        "Diffusion models have demonstrated exceptional capabilities in image\nrestoration, yet their application to video super-resolution (VSR) faces\nsignificant challenges in balancing fidelity with temporal consistency. Our\nevaluation reveals a critical gap: existing approaches consistently fail on\nseverely degraded videos--precisely where diffusion models' generative\ncapabilities are most needed. We identify that existing diffusion-based VSR\nmethods struggle primarily because they face an overwhelming learning burden:\nsimultaneously modeling complex degradation distributions, content\nrepresentations, and temporal relationships with limited high-quality training\ndata. To address this fundamental challenge, we present DiffVSR, featuring a\nProgressive Learning Strategy (PLS) that systematically decomposes this\nlearning burden through staged training, enabling superior performance on\ncomplex degradations. Our framework additionally incorporates an Interweaved\nLatent Transition (ILT) technique that maintains competitive temporal\nconsistency without additional training overhead. Experiments demonstrate that\nour approach excels in scenarios where competing methods struggle, particularly\non severely degraded videos. Our work reveals that addressing the learning\nstrategy, rather than focusing solely on architectural complexity, is the\ncritical path toward robust real-world video super-resolution with diffusion\nmodels.",
        "The entanglement analysis of a pure state of a many-body quantum system\nrequires a prior information about its density matrix\/ state matrix, obtained\nin principle by solving the Hamiltonian matrix. The missing information due to\ncomplexity of the many-body interactions however renders it necessary to\nconsider an ensemble of Hamiltonians and thereby an ensemble of pure states.\nThis in turn leaves a statistical description of the entanglement measures as\nthe only option.\n  We consider physical Hamiltonians that can be modelled by a multiparametric\nGaussian ensembles, theoretically derive the state ensembles for its\neigenstates and analyze the effect of varying system conditions on its\nentanglement statistics. Our approach leads to a single parametric based common\nmathematical formulation for the evolution of the statistics of different state\nensembles. The parameter turns out to be a single functional of the system\nparameters and thereby reveals a deep web of connection underlying different\nquantum states.",
        "In a Cox model, the partial likelihood, as the product of a series of\nconditional probabilities, is used to estimate the regression coefficients. In\npractice, those conditional probabilities are approximated by risk score ratios\nbased on a continuous time model, and thus result in parameter estimates from\nonly an approximate partial likelihood. Through a revisit to the original\npartial likelihood idea, an accurate partial likelihood computing method for\nthe Cox model is proposed, which calculates the exact conditional probability\nusing the Poisson-binomial distribution. New estimating and inference\nprocedures are developed, and theoretical results are established for the\nproposed computational procedure. Although ties are common in real studies,\ncurrent theories for the Cox model mostly do not consider cases for tied data.\nIn contrast, the new approach includes the theory for grouped data, which\nallows ties, and also includes the theory for continuous data without ties,\nproviding a unified framework for computing partial likelihood for data with or\nwithout ties. Numerical results show that the proposed method outperforms\ncurrent methods in reducing bias and mean squared error, while achieving\nimproved confidence interval coverage rates, especially when there are many\nties or when the variability in risk scores is large. Comparisons between\nmethods in real applications have been made.",
        "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT\/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess\/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https:\/\/corn0205.github.io\/",
        "The aim of this paper is studying the two-sided remotely almost periodic\nsolutions of ordinary differential equations in Banach spaces of the form\n$x'=A(t)x+f(t)+F(t,x)$ with two-sided remotely almost periodic coefficients if\nthe linear equation $x'=A(t)x$ satisfies the condition of exponential\ntrichotomy and nonlinearity $F$ is \"small\".",
        "Recent advances in diffusion models have greatly improved text-driven video\ngeneration. However, training models for long video generation demands\nsignificant computational power and extensive data, leading most video\ndiffusion models to be limited to a small number of frames. Existing\ntraining-free methods that attempt to generate long videos using pre-trained\nshort video diffusion models often struggle with issues such as insufficient\nmotion dynamics and degraded video fidelity. In this paper, we present\nBrick-Diffusion, a novel, training-free approach capable of generating long\nvideos of arbitrary length. Our method introduces a brick-to-wall denoising\nstrategy, where the latent is denoised in segments, with a stride applied in\nsubsequent iterations. This process mimics the construction of a staggered\nbrick wall, where each brick represents a denoised segment, enabling\ncommunication between frames and improving overall video quality. Through\nquantitative and qualitative evaluations, we demonstrate that Brick-Diffusion\noutperforms existing baseline methods in generating high-fidelity videos.",
        "Accurately modeling bending energy in morphogenetic simulations is crucial,\nespecially when dealing with anisotropic meshes where remeshing is infeasible\ndue to the biologically meaningful entities of vertex positions (e.g., cells).\nThis study addresses the underexplored question of which bending-energy\ndiscretization methods are most accurate and suitable for such simulations.\n  The evaluation consists of two stages: First, the accuracy of each method is\ntested by comparing predicted bending energy and force against theoretical\nvalues for two benchmark cases--a wrinkled planar sheet and a smooth spherical\nsheet. Second, we simulate the formation of wrinkles in a planar sheet caused\nby anisotropic cell division, analyzing the resulting wavenumber patterns for\ntwo division orientations: uniaxial and random.\n  The results highlight that the choice of the optimal discretization method\ndepends on the application. For simulations requiring precise quantitative\npredictions, the Hamann model demonstrates superior accuracy. Conversely, for\nsimulations where qualitative trends in morphology are of primary interest, the\nJ\\\"ulicher model provides a computationally efficient alternative. These\nfindings provide guidance for selecting appropriate bending-energy\ndiscretization methods in morphogenetic simulations, ultimately leading to more\naccurate and efficient modeling of complex biological forms.",
        "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
        "Let $P$ be a set of points and $L$ a set of lines in the (extended) Euclidean\nplane, and $I \\subseteq P\\times L$, where $i =(p,l) \\in I$ means that point $p$\nand line $l$ are incident. The incidences can be interpreted as quadratic\nconstraints on the homogeneous coordinates of the points and lines. We study\nthe space of incidence preserving motions of the given incidence structure by\nlinearizing the system of quadratic equations. The Jacobian of the quadratic\nsystem, our projective rigidity matrix, leads to the notion of\nindependence\/dependence of incidences. Column dependencies correspond to\ninfinitesimal motions. Row dependencies or self-stresses allow for new\ninterpretations of classical geometric incidence theorems. We show that\nself-stresses are characterized by a 3-fold balance. As expected, infinitesimal\n(first order) projective rigidity as well as second order projective rigidity\nimply projective rigidity but not conversely. Several open problems and\npossible generalizations are indicated.",
        "We construct an expansion of a real closed field by a multiplicative subgroup\nadapting Poizat's theory of green points. Its theory is strongly dependent, and\nevery open set definable in a model of this theory is semialgebraic. We prove\nthat the real field with a dense family of logarithmic spirals, proposed by\nZilber, satisfies our theory.",
        "We construct and analyze the fault tolerance of $[[6,1,3]]$ non-CSS quantum\nerror correcting code under the anisotropic and depolarizing noise models. This\nrate-optimized code achieves fault-tolerance using a single ancilla qubit for\nsyndrome measurement under anisotropic noise conditions. This method was called\nfault-tolerance using bare ancilla by Brown \\emph{et al.} We give explicit\nconstruction of the code using measurements on non-planar graph states. We also\nargue that using our approach, we can construct a family of such fault-tolerant\ncodes. This method fills a notable gap in constructing fault-tolerant non-CSS\ncode families.",
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com\/huawei-noah\/SMARTS."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects",
    "start_abstract":"Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies",
        "The Electric Location-Routing Problem: Improved Formulations and Effects\n  of Nonlinear Charging",
        "DiffVSR: Revealing an Effective Recipe for Taming Robust Video\n  Super-Resolution Against Complex Degradations",
        "Entanglement dynamics of many-body quantum states with evolving system\n  conditions",
        "An Accurate Computational Approach for Partial Likelihood Using\n  Poisson-Binomial Distributions",
        "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel\n  Tool Invocation",
        "Two-sided Remotely Almost Periodic Solutions of Ordinary Differential\n  Equations in Banach Spaces",
        "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "Comparison of Bending-Energy Discretization Methods for Anisotropic\n  Meshes in Morphogenetic Simulations",
        "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
        "Counting for rigidity under projective transformations in the plane",
        "Green points in the reals",
        "Fault-tolerance of [[6, 1, 3]] non-CSS code family generated using\n  measurements on graph states",
        "Design of resilient structures by randomization and bistability",
        "Getting SMARTER for Motion Planning in Autonomous Driving Systems"
      ],
      "abstract":[
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones.",
        "Electric Location-Routing models (ELRP) can contribute to the effective\nplanning of electric vehicles (EVs) fleets and charging infrastructure within\nEV logistic networks because it simultaneously combines routing and location\ndecisions to find optimal solutions to the network design. This study\nintroduces ELRP models that incorporate nonlinear charging process, multiple\ncharging station types and develop new improved formulations to the problem.\nExisting ELRP models commonly assume a linear charging process and employ a\nnode-based formulation for tracking EV energy and time consumption. In\ncontrast, we propose novel formulations offering alternative approaches for\nmodeling EV energy, time consumption, and nonlinear charging. Through extensive\ncomputational experiments, our analysis demonstrates the effectiveness of the\nnew formulations, reducing the average gap from 29.1% to 11.9%, yielding\nimproved solutions for 28 out of 74 instances compared to the node-based\nformulation. Moreover, our findings provide valuable insights into the\nstrategic implications of nonlinear charging in ELRP decision-making, offering\nnew perspectives for planning charging infrastructure in EV logistic networks.",
        "Diffusion models have demonstrated exceptional capabilities in image\nrestoration, yet their application to video super-resolution (VSR) faces\nsignificant challenges in balancing fidelity with temporal consistency. Our\nevaluation reveals a critical gap: existing approaches consistently fail on\nseverely degraded videos--precisely where diffusion models' generative\ncapabilities are most needed. We identify that existing diffusion-based VSR\nmethods struggle primarily because they face an overwhelming learning burden:\nsimultaneously modeling complex degradation distributions, content\nrepresentations, and temporal relationships with limited high-quality training\ndata. To address this fundamental challenge, we present DiffVSR, featuring a\nProgressive Learning Strategy (PLS) that systematically decomposes this\nlearning burden through staged training, enabling superior performance on\ncomplex degradations. Our framework additionally incorporates an Interweaved\nLatent Transition (ILT) technique that maintains competitive temporal\nconsistency without additional training overhead. Experiments demonstrate that\nour approach excels in scenarios where competing methods struggle, particularly\non severely degraded videos. Our work reveals that addressing the learning\nstrategy, rather than focusing solely on architectural complexity, is the\ncritical path toward robust real-world video super-resolution with diffusion\nmodels.",
        "The entanglement analysis of a pure state of a many-body quantum system\nrequires a prior information about its density matrix\/ state matrix, obtained\nin principle by solving the Hamiltonian matrix. The missing information due to\ncomplexity of the many-body interactions however renders it necessary to\nconsider an ensemble of Hamiltonians and thereby an ensemble of pure states.\nThis in turn leaves a statistical description of the entanglement measures as\nthe only option.\n  We consider physical Hamiltonians that can be modelled by a multiparametric\nGaussian ensembles, theoretically derive the state ensembles for its\neigenstates and analyze the effect of varying system conditions on its\nentanglement statistics. Our approach leads to a single parametric based common\nmathematical formulation for the evolution of the statistics of different state\nensembles. The parameter turns out to be a single functional of the system\nparameters and thereby reveals a deep web of connection underlying different\nquantum states.",
        "In a Cox model, the partial likelihood, as the product of a series of\nconditional probabilities, is used to estimate the regression coefficients. In\npractice, those conditional probabilities are approximated by risk score ratios\nbased on a continuous time model, and thus result in parameter estimates from\nonly an approximate partial likelihood. Through a revisit to the original\npartial likelihood idea, an accurate partial likelihood computing method for\nthe Cox model is proposed, which calculates the exact conditional probability\nusing the Poisson-binomial distribution. New estimating and inference\nprocedures are developed, and theoretical results are established for the\nproposed computational procedure. Although ties are common in real studies,\ncurrent theories for the Cox model mostly do not consider cases for tied data.\nIn contrast, the new approach includes the theory for grouped data, which\nallows ties, and also includes the theory for continuous data without ties,\nproviding a unified framework for computing partial likelihood for data with or\nwithout ties. Numerical results show that the proposed method outperforms\ncurrent methods in reducing bias and mean squared error, while achieving\nimproved confidence interval coverage rates, especially when there are many\nties or when the variability in risk scores is large. Comparisons between\nmethods in real applications have been made.",
        "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT\/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess\/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https:\/\/corn0205.github.io\/",
        "The aim of this paper is studying the two-sided remotely almost periodic\nsolutions of ordinary differential equations in Banach spaces of the form\n$x'=A(t)x+f(t)+F(t,x)$ with two-sided remotely almost periodic coefficients if\nthe linear equation $x'=A(t)x$ satisfies the condition of exponential\ntrichotomy and nonlinearity $F$ is \"small\".",
        "Recent advances in diffusion models have greatly improved text-driven video\ngeneration. However, training models for long video generation demands\nsignificant computational power and extensive data, leading most video\ndiffusion models to be limited to a small number of frames. Existing\ntraining-free methods that attempt to generate long videos using pre-trained\nshort video diffusion models often struggle with issues such as insufficient\nmotion dynamics and degraded video fidelity. In this paper, we present\nBrick-Diffusion, a novel, training-free approach capable of generating long\nvideos of arbitrary length. Our method introduces a brick-to-wall denoising\nstrategy, where the latent is denoised in segments, with a stride applied in\nsubsequent iterations. This process mimics the construction of a staggered\nbrick wall, where each brick represents a denoised segment, enabling\ncommunication between frames and improving overall video quality. Through\nquantitative and qualitative evaluations, we demonstrate that Brick-Diffusion\noutperforms existing baseline methods in generating high-fidelity videos.",
        "Accurately modeling bending energy in morphogenetic simulations is crucial,\nespecially when dealing with anisotropic meshes where remeshing is infeasible\ndue to the biologically meaningful entities of vertex positions (e.g., cells).\nThis study addresses the underexplored question of which bending-energy\ndiscretization methods are most accurate and suitable for such simulations.\n  The evaluation consists of two stages: First, the accuracy of each method is\ntested by comparing predicted bending energy and force against theoretical\nvalues for two benchmark cases--a wrinkled planar sheet and a smooth spherical\nsheet. Second, we simulate the formation of wrinkles in a planar sheet caused\nby anisotropic cell division, analyzing the resulting wavenumber patterns for\ntwo division orientations: uniaxial and random.\n  The results highlight that the choice of the optimal discretization method\ndepends on the application. For simulations requiring precise quantitative\npredictions, the Hamann model demonstrates superior accuracy. Conversely, for\nsimulations where qualitative trends in morphology are of primary interest, the\nJ\\\"ulicher model provides a computationally efficient alternative. These\nfindings provide guidance for selecting appropriate bending-energy\ndiscretization methods in morphogenetic simulations, ultimately leading to more\naccurate and efficient modeling of complex biological forms.",
        "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
        "Let $P$ be a set of points and $L$ a set of lines in the (extended) Euclidean\nplane, and $I \\subseteq P\\times L$, where $i =(p,l) \\in I$ means that point $p$\nand line $l$ are incident. The incidences can be interpreted as quadratic\nconstraints on the homogeneous coordinates of the points and lines. We study\nthe space of incidence preserving motions of the given incidence structure by\nlinearizing the system of quadratic equations. The Jacobian of the quadratic\nsystem, our projective rigidity matrix, leads to the notion of\nindependence\/dependence of incidences. Column dependencies correspond to\ninfinitesimal motions. Row dependencies or self-stresses allow for new\ninterpretations of classical geometric incidence theorems. We show that\nself-stresses are characterized by a 3-fold balance. As expected, infinitesimal\n(first order) projective rigidity as well as second order projective rigidity\nimply projective rigidity but not conversely. Several open problems and\npossible generalizations are indicated.",
        "We construct an expansion of a real closed field by a multiplicative subgroup\nadapting Poizat's theory of green points. Its theory is strongly dependent, and\nevery open set definable in a model of this theory is semialgebraic. We prove\nthat the real field with a dense family of logarithmic spirals, proposed by\nZilber, satisfies our theory.",
        "We construct and analyze the fault tolerance of $[[6,1,3]]$ non-CSS quantum\nerror correcting code under the anisotropic and depolarizing noise models. This\nrate-optimized code achieves fault-tolerance using a single ancilla qubit for\nsyndrome measurement under anisotropic noise conditions. This method was called\nfault-tolerance using bare ancilla by Brown \\emph{et al.} We give explicit\nconstruction of the code using measurements on non-planar graph states. We also\nargue that using our approach, we can construct a family of such fault-tolerant\ncodes. This method fills a notable gap in constructing fault-tolerant non-CSS\ncode families.",
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com\/huawei-noah\/SMARTS."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Biological identifications through DNA barcodes",
    "start_abstract":"Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "BarcodeBERT: Transformers for Biodiversity Analysis"
      ],
      "abstract":[
        "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Complexity of Finite Semigroups: History and Decidability",
        "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
        "Two-component nonlinear wave solutions of the sixth-order generalised\n  Boussinesq-type equations",
        "Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC\n  Codes",
        "A central limit theorem for the giant in a stochastic block model",
        "Faster Machine Translation Ensembling with Reinforcement Learning and\n  Competitive Correction",
        "Unified Native Spaces in Kernel Methods",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
        "Pressure-induced structural and superconducting transitions in black\n  arsenic",
        "A healthier stochastic semiclassical gravity: world without\n  Schr\\\"{o}dinger cats",
        "From Target Tracking to Targeting Track -- Part II: Regularized\n  Polynomial Trajectory Optimization",
        "Open-Ended and Knowledge-Intensive Video Question Answering",
        "General Relativity and Geodesy",
        "Evaluating Interpretable Reinforcement Learning by Distilling Policies\n  into Programs",
        "An inherently parallel H2-ULV factorization for solving dense linear\n  systems on GPUs",
        "Current-induced magnetoresistance hysteresis in the kagome\n  superconductor CsV$_3$Sb$_5$",
        "Comment on \"Comment on Attosecond electron microscopy and diffraction\"",
        "The Ophiuchus DIsk Survey Employing ALMA (ODISEA): Complete Size\n  Distributions for the 100 Brightest Disks Across Multiplicity and SED Classes",
        "Comparison of Offset and Ratio Weighted Regressions in Tweedie Models\n  with Application to Mid-Term Cancellations",
        "High-Throughput Computational Screening and Interpretable Machine\n  Learning of Metal-organic Frameworks for Iodine Capture",
        "A Comprehensive Survey on Cross-Domain Recommendation: Taxonomy,\n  Progress, and Prospects",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Vibrational properties of photochromic yttrium oxyhydride and\n  oxydeuteride thin films",
        "Empirical Calibration and Metric Differential Privacy in Language Models",
        "Argument-Based Comparative Question Answering Evaluation Benchmark",
        "Bifurcation and multiplicity results for critical problems involving the\n  $p$-Grushin operator",
        "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\n  Large Language Models",
        "Teleportation scheme for the complete state of light at the example of\n  coherent states"
      ],
      "abstract":[
        "In recent papers, Margolis, Rhodes and Schilling proved that the complexity\nof a finite semigroup is computable. This solved a problem that had been open\nfor more than 50 years. The purpose of this paper is to survey the basic\nresults of Krohn-Rhodes complexity of finite semigroups and to outline the\nproof of its computability.",
        "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
        "Two different versions of cubic sixth-order generalised Boussinesq-type wave\nequations are considered in this study. A generalised perturbation reduction\nmethod is used to solve these equations, which allows the reduction of\nconsidered equations to coupled nonlinear Schrodinger equations. Two-component\nnonlinear wave solutions are obtained. The profiles and parameters of these\nsolutions for both nonlinear equations are presented and compared. These\nsolutions coincide with the vector 0 \\pi pulse of self-induced transparency,\nwhich was previously studied in several known nonlinear wave equations.",
        "Quantum low-density parity-check (QLDPC) codes present a promising route to\nlow-overhead fault-tolerant quantum computation, yet systematic strategies for\ntheir exploration remain underdeveloped. In this work, we establish a\ntopological framework for studying the bivariate-bicycle codes, a prominent\nclass of QLDPC codes tailored for real-world quantum hardware. Our framework\nenables the investigation of these codes through universal properties of\ntopological orders. Besides providing efficient characterizations for\ndemonstrations using Gr\\\"obner bases, we also introduce a novel\nalgebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko\ntheorem, allowing us to analytically determine how the topological order varies\nwith the generic choice of bivariate-bicycle codes under toric layouts. Novel\nphenomena are unveiled, including topological frustration, where ground-state\ndegeneracy on a torus deviates from the total anyon number, and quasi-fractonic\nmobility, where anyon movement violates energy conservation. We demonstrate\ntheir inherent link to symmetry-enriched topological orders and offer an\nefficient method for searching for finite-size codes. Furthermore, we extend\nthe connection between anyons and logical operators using Koszul complex\ntheory. Our work provides a rigorous theoretical basis for exploring the fault\ntolerance of QLDPC codes and deepens the interplay among topological order,\nquantum error correction, and advanced mathematical structures.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Ensembling neural machine translation (NMT) models to produce higher-quality\ntranslations than the $L$ individual models has been extensively studied.\nRecent methods typically employ a candidate selection block (CSB) and an\nencoder-decoder fusion block (FB), requiring inference across \\textit{all}\ncandidate models, leading to significant computational overhead, generally\n$\\Omega(L)$. This paper introduces \\textbf{SmartGen}, a reinforcement learning\n(RL)-based strategy that improves the CSB by selecting a small, fixed number of\ncandidates and identifying optimal groups to pass to the fusion block for each\ninput sentence. Furthermore, previously, the CSB and FB were trained\nindependently, leading to suboptimal NMT performance. Our DQN-based\n\\textbf{SmartGen} addresses this by using feedback from the FB block as a\nreward during training. We also resolve a key issue in earlier methods, where\ncandidates were passed to the FB without modification, by introducing a\nCompetitive Correction Block (CCB). Finally, we validate our approach with\nextensive experiments on English-Hindi translation tasks in both directions.",
        "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
        "We report high-pressure Raman spectra and resistance measurements of black\narsenic (b-As) up to 58 GPa, along with phonon density of states (DOS) and\nenthalpy calculations for four reported arsenic phases up to 50 GPa. It is\nfound that metastable b-As transforms into gray arsenic (g-As) phase at a\ncritical pressure of 1.51 GPa, followed by subsequent transitions to simple\ncubic arsenic (c-As) and incommensurate host-guest arsenic (hg-As) phases at\n25.9 and 44.8 GPa, respectively. Superconductivity emerges above 25 GPa in the\nc-As phase, with the superconducting transition temperature ($T$$\\rm_c$)\nremaining nearly a constant of 3 K. Upon further compression, $T$$\\rm_c$\nsteeply increases to a higher value around 4.5 K in the incommensurate hg-As\nphase above 43 GPa. We use our results to update the structural and\nsuperconducting phase diagrams under pressure for the novel semiconductor,\nblack arsenic.",
        "Semiclassical gravity couples classical gravity to the quantized matter in\nmeanfield approximation. The meanfield coupling is problematic for two reasons.\nFirst, it ignores the quantum fluctuation of matter distribution. Second, it\nviolates the linearity of the quantum dynamics. The first problem can be\nmitigated by allowing stochastic fluctuations of the geometry but the second\nproblem lies deep in quantum foundations. Restoration of quantum linearity\nrequires a conceptual approach to hybrid classical-quantum coupling. Studies of\nthe measurement problem and the quantum-classical transition point to the\nsolution. It is based on a postulated mechanism of spontaneous quantum\nmonitoring plus feedback. This approach eliminates Schr\\\"{o}dinger cat states,\ntakes quantum fluctuations into account, and restores the linearity of quantum\ndynamics. Such a captivating conceptionally `healthier' semiclassical theory\nexists in the Newtonian limit, but its relativistic covariance hits a wall.\nHere we will briefly recapitulate the concept and its realization in the\nnonrelativistic limit. We emphasize that the long-known obstacles to the\nrelativistic extension lie in quantum foundations.",
        "Target tracking entails the estimation of the evolution of the target state\nover time, namely the target trajectory. Different from the classical state\nspace model, our series of studies, including this paper, model the collection\nof the target state as a stochastic process (SP) that is further decomposed\ninto a deterministic part which represents the trend of the trajectory and a\nresidual SP representing the residual fitting error. Subsequently, the tracking\nproblem is formulated as a learning problem regarding the trajectory SP for\nwhich a key part is to estimate a trajectory FoT (T-FoT) best fitting the\nmeasurements in time series. For this purpose, we consider the polynomial T-FoT\nand address the regularized polynomial T-FoT optimization employing two\ndistinct regularization strategies seeking trade-off between the accuracy and\nsimplicity. One limits the order of the polynomial and then the best choice is\ndetermined by grid searching in a narrow, bounded range while the other adopts\n$\\ell_0$ norm regularization for which the hybrid Newton solver is employed.\nSimulation results obtained in both single and multiple maneuvering target\nscenarios demonstrate the effectiveness of our approaches.",
        "Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.",
        "Mass redistribution on Earth due to dynamic processes such as ice melting and\nsea level rise leads to a changing gravitational field, observable by geodetic\ntechniques. Monitoring this change over time allows us to learn more about our\nplanet and its dynamic evolution. In this paper, we highlight the impact of\nGeneral Relativity (GR) on geodesy: it provides corrections essential for the\ninterpretation of high-precision measurements and enables a completely novel\nmeasurement approach using chronometry, i.e., clock-based observations.\nFocusing on the latter, we review the construction of the relativistic gravity\npotential and the corresponding geoid definition as an isochronometric surface\nto elucidate the comparison to the conventional Newtonian geoid. Furthermore,\nwe comment on additional potentials due to the non-Newtonian degrees of freedom\nof the relativistic gravitational field, and assess the feasibility of\nclock-based measurements for Gravity Field Recovery (GFR) from space. Although\nclock observations in space demonstrate technical promise for GFR, achieving\nthe necessary precision for practical applications remains challenging.",
        "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
        "Hierarchical low-rank approximation of dense matrices can reduce the\ncomplexity of their factorization from O(N^3) to O(N). However, the complex\nstructure of such hierarchical matrices makes them difficult to parallelize.\nThe block size and ranks can vary between the sub-blocks, which creates load\nimbalance. The dependency between the sub-blocks during factorization results\nin serialization. Since many sub-blocks are low-rank, their small computational\nload exposes the overhead of runtime systems. The combination of these factors\nmakes it challenging to implement these methods on GPUs. In this work, we show\nthat dense matrices can be factorized with linear complexity, while extracting\nthe potential parallelism of GPUs. This is made possible through the H2-ULV\nfactorization, which removes the dependency on trailing sub-matrices.",
        "We report the observation of current-modulated magnetoresistance hysteresis\nbelow the superconducting transition temperature in the kagome superconductor\nCsV$_3$Sb$_5$. This highly tunable hysteresis behavior is confined to the\nsuperconducting state and vanishes when superconductivity is fully suppressed,\ndirectly linking magnetoresistance hysteresis to the superconducting order in\nCsV$_3$Sb$_5$. Additionally, the superconducting diode effect driven by a small\nmagnetic field is observed, indicating the enhanced electronic magnetochiral\nanisotropy by the chiral domain-wall scattering. Our findings position\nCsV$_3$Sb$_5$ as a promising platform for exploring nontrivial physical\nphenomena, including unconventional pairing mechanisms and topological\nsuperconductivity.",
        "Over the past few decades, following the first demonstration of ultrafast\nelectron microscopy, numerous research groups have focused on achieving\nattosecond temporal resolution in electron microscopy with the goal of imaging\nelectron and atomic motion. Recently, several studies have claimed to achieve\nattosecond temporal resolution in imaging(1-3). These claims are based on the\ngeneration of attosecond electron pulse trains. However, in typical\ntime-resolved measurements used to capture dynamic processes in real-time, the\ntemporal resolution is determined by the envelope of the pulse train. The\nreliance of using attosecond electron pulse trains fails to account for the\ndistinct temporal resolution advantages enabled by our attosecond optical\ngating, which are absent in the case of using a continuous-wave or long laser\npulse. These oversights highlight the limitations of this methodology (1-3) in\nstudying ultrafast phenomena of matter. It is crucial to clarify this\ndistinction to avoid confusion, misinterpretation, and potential miscitations\nwithin the community regarding attosecond temporal resolution in electron\nmicroscopy and the attosecond imaging of matter dynamics. In contrast, Hui et\nal. (4) present the first realistic demonstration of attosecond imaging\nresolution in electron microscopy, enabling the diffraction imaging of electron\nmotion dynamics in graphene. In a commentary by Peter Baum and Claus Ropers,\nthe authors conjecture that the graphene dynamics observed in our time-resolved\ndiffraction experiment (Fig. 5, Hui et al. 2024) (4) is an optical interference\nartifact or light modulation of electrons effects, similar to what was reported\npreviously (1-3), in addition to raising other technical concerns. In this\nreply, we are pleased to address these allegations and provide clarifications\nto resolve the raised technical questions.",
        "The size of a protoplanetary disk is a fundamental property, yet most remain\nunresolved, even in nearby star-forming regions (d $\\sim$ 140-200 pc). We\npresent the complete continuum size distribution for the $105$ brightest\nprotoplanetary disks (M$_{\\text{dust}}$ $\\gtrsim$ 2 M$_{\\oplus}$) in the\nOphiuchus cloud, obtained from ALMA Band 8 (410 GHz) observations at\n0.05$^{\\prime\\prime}$ (7 au) to 0.15$^{\\prime\\prime}$ (21 au) resolution. This\nsample includes 54 Class II and 51 Class I and Flat Spectrum sources, providing\na comprehensive distribution across evolutionary stages. We measure the Half\nWidth at Half Maximum (HWHM) and the radius encircling $68\\%$ of the flux\n($R_{68\\%}$) for most non-binary disks, yielding the largest flux-limited\nsample of resolved disks in any star-forming region. The distribution is\nlog-normal with a median value of $\\sim$14 au and a logarithmic standard\ndeviation $\\sigma_{\\log} = 0.46$ (factor of 2.9 in linear scale). Disks in\nclose binary systems ($<$ 200 au separation) have smaller radii, with median\nvalue of $\\sim$5 au, indicating efficient radial drift as predicted by dust\nevolution models. The size distribution for young embedded objects (SED Class I\nand Flat Spectrum, age $\\lesssim$ 1 Myr) is similar to that of Class II objects\n(age $\\sim$ a few Myr), implying that pressure bumps must be common at early\ndisk stages to prevent mm-sized particle migration at au scales.",
        "In property and casualty insurance, particularly in automobile insurance,\nrisk exposure is traditionally associated with the coverage duration. However,\nfactors such as early contract cancellations demand more precise modelling to\nensure accurate premium pricing. This study introduces and compares two\napproaches for modelling total claims (or loss costs) in insurance portfolios\nwith a high proportion of policies that have partial year exposure: the offset\nand ratio methods. We demonstrate that both approaches can be viewed as\nweighted regressions under the Tweedie distribution framework. Through an\nanalysis based on the financial balance property, we find that the ratio\napproach outperforms the offset method. This comparison is illustrated using an\nautomobile insurance portfolio, where a significant share of policyholders\nterminate their contracts before the coverage period concludes.",
        "The removal of leaked radioactive iodine isotopes in humid environments holds\nsignificant importance in nuclear waste management and nuclear accident\nmitigation. In this study, high-throughput computational screening and machine\nlearning were combined to reveal the iodine capture performance of 1816\nmetal-organic framework (MOF) materials under humid air conditions. Firstly,\nthe relationship between the structural characteristics of MOFs and their\nadsorption properties was explored, with the aim of identifying the optimal\nstructural parameters for iodine capture. Subsequently, two machine learning\nregression algorithms - Random Forest and CatBoost, were employed to predict\nthe iodine adsorption capabilities of MOFs. In addition to 6 structural\nfeatures, 25 molecular features and 8 chemical features were incorporated to\nenhance the prediction accuracy of the machine learning algorithms. Feature\nimportance was assessed to determine the relative influence of various features\non iodine adsorption performance, in which the Henry's coefficient and heat of\nadsorption to iodine were found the two most crucial chemical factors.\nFurthermore, four types of molecular fingerprints were introduced for providing\ncomprehensive and detailed structural information of MOF materials. The top 20\nmost significant MACCS molecular fingerprints were picked out, revealing that\nthe presence of six-membered ring structures and nitrogen atoms in the MOFs\nwere the key structural factors that enhanced iodine adsorption, followed by\nthe existence of oxygen atoms. This work combined high-throughput computation,\nmachine learning, and molecular fingerprints to comprehensively elucidate the\nmultifaceted factors influencing the iodine adsorption performance of MOFs,\noffering profound insightful guidelines for screening and structural design of\nadvanced MOF materials.",
        "Recommender systems (RS) have become crucial tools for information filtering\nin various real world scenarios. And cross domain recommendation (CDR) has been\nwidely explored in recent years in order to provide better recommendation\nresults in the target domain with the help of other domains. The CDR technology\nhas developed rapidly, yet there is a lack of a comprehensive survey\nsummarizing recent works. Therefore, in this paper, we will summarize the\nprogress and prospects based on the main procedure of CDR, including Cross\nDomain Relevance, Cross Domain Interaction, Cross Domain Representation\nEnhancement and Model Optimization. To help researchers better understand and\nengage in this field, we also organize the applications and resources, and\nhighlight several current important challenges and future directions of CDR.\nMore details of the survey articles are available at\nhttps:\/\/github.com\/USTCAGI\/Awesome-Cross-Domain\nRecommendation-Papers-and-Resources.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "A comprehensive study of the vibrational properties of photochromic yttrium\noxyhydride (YHO) and oxydeuteride (YDO) thin films is presented. These films\nare deposited using reactive magnetron sputtering, followed by post-oxidation.\nOur investigation employs vibrational Fourier-transform infrared (FTIR)\nspectroscopy, in conjunction with first-principles Density Functional Theory\n(DFT) calculations. The FTIR spectra of the films reveal broad vibrational\nbands, primarily attributed to the disordered structure containing small\ncrystallites (<10 nm), as confirmed by solid-state nuclear magnetic resonance\nand X-ray diffraction measurements. An isotopic shift from approximately 900 to\n745 cm-1 is observed in the hydrogen\/deuterium-related vibration band, while\nthe lower frequency bands (< 600 cm-1) remain unaffected upon replacement of\nhydrogen with deuterium. These experimental observations are consistent with\nthe DFT theoretical calculations for various stable YHO lattices reported in\nthe literature. Illumination of the films with ultraviolet light at 3.3 eV\nleads to additional absorption not only in the visible light range but also up\nto approximately 2000 cm-1 in the mid-infrared region. However, no phase\ntransformation change or formation of hydroxyl (OH) groups are observed\nfollowing illumination. Our findings provide valuable insight into the\nvibrational and photochromic properties of YH(D)O thin films.",
        "NLP models trained with differential privacy (DP) usually adopt the DP-SGD\nframework, and privacy guarantees are often reported in terms of the privacy\nbudget $\\epsilon$. However, $\\epsilon$ does not have any intrinsic meaning, and\nit is generally not possible to compare across variants of the framework. Work\nin image processing has therefore explored how to empirically calibrate noise\nacross frameworks using Membership Inference Attacks (MIAs). However, this kind\nof calibration has not been established for NLP. In this paper, we show that\nMIAs offer little help in calibrating privacy, whereas reconstruction attacks\nare more useful. As a use case, we define a novel kind of directional privacy\nbased on the von Mises-Fisher (VMF) distribution, a metric DP mechanism that\nperturbs angular distance rather than adding (isotropic) Gaussian noise, and\napply this to NLP architectures. We show that, even though formal guarantees\nare incomparable, empirical privacy calibration reveals that each mechanism has\ndifferent areas of strength with respect to utility-privacy trade-offs.",
        "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https:\/\/anonymous.4open.science\/r\/cqa-evaluation-benchmark-4561\/README.md}}.",
        "In this article we prove a bifurcation and multiplicity result for a critical\nproblem involving a degenerate nonlinear operator $\\Delta_\\gamma^p$. We extend\nto a generic $p>1$ a result which was proved only when $p=2$. When $p\\neq 2$,\nthe nonlinear operator $-\\Delta_\\gamma^p$ has no linear eigenspaces, so our\nextension is nontrivial and requires an abstract critical theorem which is not\nbased on linear subspaces. We also prove a new abstract result based on a\npseudo-index related to the $\\mathbf{Z}_2$-cohomological index that is\napplicable here. We provide a version of the Lions' Concentration-Compactness\nPrinciple for our operator.",
        "The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification.",
        "We present a scheme to teleport both the spatial and number-of-photons\ndegrees of freedom of light. This is achieved by teleportation of each pixel of\na target image. We take coherent states as the input and demonstrate the scheme\nfor the cases with ideal and realistic entanglement resources."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"BarcodeBERT: Transformers for Biodiversity Analysis",
    "start_abstract":"Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Biological identifications through DNA barcodes"
      ],
      "abstract":[
        "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "A note on improved bounds for hypergraph rainbow matching problems",
        "$\\mathcal{Z}$-stability of twisted group C*-algebras of nilpotent groups",
        "Coupled Oscillators and Dielectric Function",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Contrastive Language-Structure Pre-training Driven by Materials Science\n  Literature",
        "A confederacy of anomalies",
        "A reduced-order mean-field synchronization model for thermoacoustic\n  systems",
        "Excluding a rectangular grid",
        "Long-time asymptotics for the $N_{\\infty}$-soliton solution to the KdV\n  equation with two types of generalized reflection coefficients",
        "Maximal green sequences for $\\mathcal{Q}^N$ quivers",
        "Radio observations of the ultra-long GRB 220627A reveal a hot cocoon\n  supporting the blue supergiant progenitor scenario",
        "SN 2024ggi: another year, another striking Type II supernova",
        "Multimode fiber based high-dimensional light analyzer",
        "Efficient Over-parameterized Matrix Sensing from Noisy Measurements via\n  Alternating Preconditioned Gradient Descent",
        "Curvature Perturbations from First-Order Phase Transitions: Implications\n  to Black Holes and Gravitational Waves",
        "A Framework for Supervised and Unsupervised Segmentation and\n  Classification of Materials Microstructure Images",
        "Non-equilibrium distribution function in ultra-fast processes",
        "Electroweak Symmetry Breaking",
        "Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced\n  by High-Repetition-Rate Femtosecond Laser Filamentation",
        "Hydrodynamic Interactions in Particle Suspensions: A Perspective on\n  Stokesian Dynamics",
        "Spectral properties of a disordered insulating lattice under nonlinear\n  electric field",
        "A Near-optimal Algorithm for Learning Margin Halfspaces with Massart\n  Noise",
        "Evidence for a volcanic atmosphere on the sub-Earth L98-59b",
        "Complete intersection algebras with binomial Macaulay dual generator",
        "Vanishing coefficient results in four families of infinite q-products",
        "Quantum ensemble learning with a programmable superconducting processor",
        "New examples of geometrically special varieties: K3 surfaces, Enriques\n  surfaces, and algebraic groups",
        "The $\\alpha$-representation for the Tait coloring and for the\n  characteristic polynomial of matroid",
        "$L^p\\to L^q$ estimates for Stein's spherical maximal operators"
      ],
      "abstract":[
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "We prove that the twisted group C*-algebra of a finitely generated nilpotent\ngroup is $\\mathcal{Z}$-stable if and only if it is nowhere scattered, a\ncondition that we characterize in terms of the given group and 2-cocycle. As a\nmain application, we prove new converses to the Balian-Low Theorem for\nprojective, square-integrable representations of nilpotent Lie groups.",
        "A generalized Sellmeier model, also referred to as the Lorentz-Dirac model,\nhas been used for the description of the dielectric function of a number of\ntechnologically important materials in the literature. This model represents\nthe frequency-dependent dielectric function as a sum over Green functions of\nclassical damped harmonic oscillators, much in analogy with the functional form\nused for the dynamic polarizability of an atom, but with one important\naddition, namely, a complex-valued oscillator strength in the numerator. Here,\nwe show that this generalized functional form can be justified based on the\nresponse function of coupled damped oscillators. The encountered analogies\nsuggest an explanation for the generally observed success of the Lorentz--Dirac\nmodel in describing the dielectric function of crystals of consummate\ntechnological significance.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "Understanding structure-property relationships is an essential yet\nchallenging aspect of materials discovery and development. To facilitate this\nprocess, recent studies in materials informatics have sought latent embedding\nspaces of crystal structures to capture their similarities based on properties\nand functionalities. However, abstract feature-based embedding spaces are\nhuman-unfriendly and prevent intuitive and efficient exploration of the vast\nmaterials space. Here we introduce Contrastive Language--Structure Pre-training\n(CLaSP), a learning paradigm for constructing crossmodal embedding spaces\nbetween crystal structures and texts. CLaSP aims to achieve material embeddings\nthat 1) capture property- and functionality-related similarities between\ncrystal structures and 2) allow intuitive retrieval of materials via\nuser-provided description texts as queries. To compensate for the lack of\nsufficient datasets linking crystal structures with textual descriptions, CLaSP\nleverages a dataset of over 400,000 published crystal structures and\ncorresponding publication records, including paper titles and abstracts, for\ntraining. We demonstrate the effectiveness of CLaSP through text-based crystal\nstructure screening and embedding space visualization.",
        "A personal recollection of early years in lattice gauge theory with a bias\ntowards chiral symmetry and lattice fermions.",
        "The synchronization phenomena in thermoacoustic systems leading to\noscillatory instability can effectively be modeled using Kuramoto oscillators.\nSuch models consider the nonlinear response of flame as an ensemble of Kuramoto\nphase oscillators constrained to collectively evolve at the rhythm of acoustic\nfluctuations. However, these high-dimensional models are analytically\nintractable and computationally expensive. In this study, we reduce the\ndimensionality of such a high-dimensional model and present a low-order,\nanalytically tractable model for predicting transitions to thermoacoustic\ninstability. We reduce the dimensionality of the phase oscillator model coupled\nto the acoustic field using the Ott-Antonsen ansatz. Using the reduced-order\nequations, we estimate the transitions to thermoacoustic instability and\ncompare these transitions with the experiment. We validate the model for two\ncombustor configurations, viz., the bluff-body stabilized dump combustor and\nthe swirl-stabilized annular combustor. The low-order model accurately captures\nthe continuous and abrupt secondary transitions observed experimentally in\nthese distinct combustors.",
        "For every positive integer $k$, we define the $k$-treedepth as the largest\ngraph parameter $\\mathrm{td}_k$ satisfying (i) $\\mathrm{td}_k(\\emptyset)=0$;\n(ii) $\\mathrm{td}_k(G) \\leq 1+ \\mathrm{td}_k(G-u)$ for every graph $G$ and\nevery vertex $u \\in V(G)$; and (iii) if $G$ is a $(<k)$-clique-sum of $G_1$ and\n$G_2$, then $\\mathrm{td}_k(G) \\leq \\max\n\\{\\mathrm{td}_k(G_1),\\mathrm{td}_k(G_2)\\}$, for all graphs $G_1,G_2$. This\nparameter coincides with treedepth if $k=1$, and with treewidth plus $1$ if $k\n\\geq |V(G)|$. We prove that for every positive integer $k$, a class of graphs\n$\\mathcal{C}$ has bounded $k$-treedepth if and only if there is a positive\ninteger $\\ell$ such that for every tree $T$ on $k$ vertices, no graph in\n$\\mathcal{C}$ contains $T \\square P_\\ell$ as a minor. This implies for $k=1$\nthat a minor-closed class of graphs has bounded treedepth if and only if it\nexcludes a path, for $k=2$ that a minor-closed class of graphs has bounded\n$2$-treedepth if and only if it excludes as a minor a ladder (Huynh, Joret,\nMicek, Seweryn, and Wollan; Combinatorica, 2021), and for large values of $k$\nthat a minor-closed class of graphs has bounded treewidth if and only if it\nexcludes a grid (Grid-Minor Theorem, Robertson and Seymour; JCTB, 1986). As a\ncorollary, we obtain the following qualitative strengthening of the Grid-Minor\nTheorem in the case of bounded-height grids. For all positive integers $k,\n\\ell$, every graph that does not contain the $k \\times \\ell$ grid as a minor\nhas $(2k-1)$-treedepth at most a function of $(k, \\ell)$.",
        "We systematically investigate the long-time asymptotics for the\n$N_{\\infty}$-soliton solution to the KdV equation in the different regions with\nthe aid of the Riemann-Hilbert (RH) problems with two types of generalized\nreflection coefficients on the interval $\\left[\\eta_1, \\eta_2\\right]\\in\n\\mathbb{R}^+$: $r_0(\\lambda,\\eta_0; \\beta_0,\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\left|\\lambda-\\eta_0\\right|^{\\beta_0}\\gamma\\left(\\lambda\\right)$,\n$r_c(\\lambda,\\eta_0;\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\chi_c\\left(\\lambda,\n\\eta_0\\right)\\gamma \\left(\\lambda\\right)$, where the singularity $\\eta_0\\in\n(\\eta_1, \\eta_2)$ and $\\beta_j>-1$ ($j=0, 1, 2$), $\\gamma: \\left[\\eta_1,\n\\eta_2\\right] \\to\\mathbb{R}^+$ is continuous and positive on $\\left[\\eta_1,\n\\eta_2\\right]$, with an analytic extension to a neighborhood of this interval,\nand the step-like function $\\chi_c$ is defined as\n$\\chi_c\\left(\\lambda,\\eta_0\\right)=1$ for $\\lambda\\in\\left[\\eta_1,\n\\eta_0\\right)$ and $\\chi_c\\left(\\lambda,\\eta_0\\right)=c^2$ for\n$\\lambda\\in\\left(\\eta_0, \\eta_2\\right]$ with $c>0, \\, c\\ne1$. A critical step\nin the analysis of RH problems via the Deift-Zhou steepest descent technique is\nhow to construct local parametrices around the endpoints $\\eta_j$'s and the\nsingularity $\\eta_0$. Specifically, the modified Bessel functions of indexes\n$\\beta_j$'s are utilized for the endpoints $\\eta_j$'s, and the modified Bessel\nfunctions of index $\\left(\\beta_0\\pm 1\\right)\\left\/\\right.2$ and confluent\nhypergeometric functions are employed around the singularity $\\eta_0$ if the\nreflection coefficients are $r_0$ and $r_c$, respectively. This comprehensive\nstudy extends the understanding of generalized reflection coefficients and\nprovides valuable insights into the asymptotics of soliton gases.",
        "We introduce $\\mathcal{Q}^N$ quivers and construct maximal green sequences\nfor these quivers. We prove that any finite connected full subquiver of the\nquivers defined by Hernandez and Leclerc, arising in monoidal categorifications\nof cluster algebras, is a special case of $\\mathcal{Q}^N$ quivers. Moreover, we\nprove that the trees of oriented cycles introduced by Garver and Musiker are\nspecial cases of $\\mathcal{Q}^N$ quivers. This result resolves an open problem\nproposed by Garver and Musiker, providing a construction of maximal green\nsequences for quivers that are trees of oriented cycles. Furthermore, we prove\nthat quivers that are mutation equivalent to an orientation of a type AD Dynkin\ndiagram can also be recognized as special cases of $\\mathcal{Q}^N$ quivers.",
        "We present the discovery of the radio afterglow of the most distant\nultra-long gamma-ray burst (GRB) detected to date, GRB~220627A at redshift\n$z=3.084$. Its prompt gamma-ray light curve shows a double-pulse profile, with\nthe pulses separated by a period of quiescence lasting ${\\sim} 15\\,$min,\nleading to early speculation it could be a strongly gravitationally lensed GRB.\nHowever, our analysis of the $\\textit{Fermi}$\/GBM spectra taken during the time\nintervals of both pulses show clear differences in their spectral energy\ndistributions, disfavouring the lensing scenario. We observed the radio\nafterglow from $7$ to $456\\,$d post-burst: an initial, steep decay ($F_{\\nu}\n\\propto t^{-2}$) is followed by a shallower decline ($F_{\\nu} \\propto\nt^{-1\/2}$) after ${\\sim} 20\\,$d. Our afterglow modelling shows that these radio\nproperties can be explained by the presence of a slow, wide ejecta component in\naddition to a fast, narrow ejecta component, consistent with the picture of a\nhighly-collimated jet and its thermal cocoon decelerating into the ambient\nmedium. The properties of the cocoon point toward a progenitor with a large\nstellar radius, supporting the blue supergiant scenario proposed for ultra-long\nGRBs. We also conducted an independent test of the lensing hypothesis via Very\nLong Baseline Interferometry (VLBI) observations at ${\\sim} 12\\,$d post-burst\nby searching, for the first time, for multiple images of the candidate lensed\nGRB afterglow. Our experiment highlighted the growing need for developments in\nreal-time correlation capabilities for time-critical VLBI experiments,\nparticularly as we advance towards the SKA and ngVLA era of radio astronomy.",
        "SN 2024ggi is a Type II supernova that exploded in the nearby galaxy NGC 3621\nat a distance of approximately 7 Mpc, making it one of the closest supernovae\nof the decade. This SN shows clear signs of interaction with a dense\ncircumstellar material, and several studies have investigated the properties of\nits possible progenitor star using pre-explosion data. In this work we aim to\nconstrain the progenitor properties of SN 2024ggi by performing hydrodynamical\nmodeling of its bolometric light curve and expansion velocities. We present\nphotometric and spectroscopic observations of SN 2024ggi obtained in the\nComplejo Astron\\'omico El Leoncito, in Las Campanas Observatory, and in Las\nCumbres Observatory Global Telescope Network, spanning from 2 to 106 days after\nexplosion. We constructed its bolometric light curve and we characterize it by\ncalculating its morphological parameters. Then, we computed a grid of one\ndimensional explosion models for evolved stars with varying masses and\nestimated the properties of the progenitor star of SN 2024ggi by comparing the\nmodels to the observations. The observed bolometric luminosity and expansion\nvelocities are well-matched by a model involving the explosion of a star in the\npresence of a close circumstellar material (CSM), with a zero-age main sequence\nmass of $\\mathrm{M_{ZAMS}}$ = 15 $M_{\\odot}$, a pre-SN mass and radius of 14.1\n$M_{\\odot}$ and 517 $R_{\\odot}$, respectively, an explosion energy of\n$1.3\\times10^{51}$ erg, and a nickel mass below 0.035 $M_{\\odot}$. Our analysis\nsuggests that the progenitor suffered a mass-loss rate of $4 \\times 10^{-3}$\n$M_{\\odot}$yr$^{-1}$, confined to a distance of 3000 $R_{\\odot}$. The CSM\ndistribution is likely a two-component structure that consists of a compact\ncore and an extended tail. This analysis represents the first hydrodynamical\nmodel of SN 2024ggi with a complete coverage of the plateau phase.",
        "The wavelength and state of polarization (SOP) are fundamental properties of\nan optical field which are essential for applications in optical\ncommunications, imaging and other fields. However, it is challenging for\nexisting spectrometers and polarimeters to measure these parameters\nsimultaneously, resulting in reduced spatial and temporal efficiency. To\novercome this limitation, we propose and demonstrate a compact multimode fiber\n(MMF)-based high-dimensional light analyzer capable of simultaneously\nperforming high-precision measurements of both wavelength and SOP. Core-offset\nlaunching is introduced in the MMF to reshuffle the mode coupling. A neural\nnetwork named WP-Net has been designed dedicated to wavelength and SOP\nsynchronization measurements. Physics-informed loss function based on optical\nprior knowledge is used to optimize the learning process. These advancements\nhave enhanced the sensitivity, achieving a wavelength resolution of 0.045 pm\nand an SOP resolution of 0.0088.",
        "We consider the noisy matrix sensing problem in the over-parameterization\nsetting, where the estimated rank $r$ is larger than the true rank $r_\\star$.\nSpecifically, our main objective is to recover a matrix $ X_\\star \\in\n\\mathbb{R}^{n_1 \\times n_2} $ with rank $ r_\\star $ from noisy measurements\nusing an over-parameterized factorized form $ LR^\\top $, where $ L \\in\n\\mathbb{R}^{n_1 \\times r}, \\, R \\in \\mathbb{R}^{n_2 \\times r} $ and $\n\\min\\{n_1, n_2\\} \\ge r > r_\\star $, with the true rank $ r_\\star $ being\nunknown. Recently, preconditioning methods have been proposed to accelerate the\nconvergence of matrix sensing problem compared to vanilla gradient descent,\nincorporating preconditioning terms $ (L^\\top L + \\lambda I)^{-1} $ and $\n(R^\\top R + \\lambda I)^{-1} $ into the original gradient. However, these\nmethods require careful tuning of the damping parameter $\\lambda$ and are\nsensitive to initial points and step size. To address these limitations, we\npropose the alternating preconditioned gradient descent (APGD) algorithm, which\nalternately updates the two factor matrices, eliminating the need for the\ndamping parameter and enabling faster convergence with larger step sizes. We\ntheoretically prove that APGD achieves near-optimal error convergence at a\nlinear rate, starting from arbitrary random initializations. Through extensive\nexperiments, we validate our theoretical results and demonstrate that APGD\noutperforms other methods, achieving the fastest convergence rate. Notably,\nboth our theoretical analysis and experimental results illustrate that APGD\ndoes not rely on the initialization procedure, making it more practical and\nversatile.",
        "We employ a covariant formalism to study the evolution of cosmological\nperturbations during a first-order phase transition, addressing in particular\ntheir gauge dependence that have been overlooked so far. Our results reveal\nthat non-covariant treatments employed in previous studies can substantially\noverestimate the production of primordial black holes and scalar-induced\ngravitational waves. Once gauge dependencies are properly accounted for, we\nfind that both effects occur at significantly lower levels than previously\nestimated.",
        "Microstructure of materials is often characterized through image analysis to\nunderstand processing-structure-properties linkages. We propose a largely\nautomated framework that integrates unsupervised and supervised learning\nmethods to classify micrographs according to microstructure phase\/class and,\nfor multiphase microstructures, segments them into different homogeneous\nregions. With the advance of manufacturing and imaging techniques, the\nultra-high resolution of imaging that reveals the complexity of microstructures\nand the rapidly increasing quantity of images (i.e., micrographs) enables and\nnecessitates a more powerful and automated framework to extract materials\ncharacteristics and knowledge. The framework we propose can be used to\ngradually build a database of microstructure classes relevant to a particular\nprocess or group of materials, which can help in analyzing and\ndiscovering\/identifying new materials. The framework has three steps: (1)\nsegmentation of multiphase micrographs through a recently developed score-based\nmethod so that different microstructure homogeneous regions can be identified\nin an unsupervised manner; (2) {identification and classification of}\nhomogeneous regions of micrographs through an uncertainty-aware supervised\nclassification network trained using the segmented micrographs from Step $1$\nwith their identified labels verified via the built-in uncertainty\nquantification and minimal human inspection; (3) supervised segmentation (more\npowerful than the segmentation in Step $1$) of multiphase microstructures\nthrough a segmentation network trained with micrographs and the results from\nSteps $1$-$2$ using a form of data augmentation. This framework can iteratively\ncharacterize\/segment new homogeneous or multiphase materials while expanding\nthe database to enhance performance. The framework is demonstrated on various\nsets of materials and texture images.",
        "A simple expression for the non-equilibrium distribution function in\nultra-fast transient processes is proposed. Postulating its dependence on\ntemporal derivatives of the equilibrium integrals of motion, non-equilibrium\nanalogues of the thermodynamic relationships are derived and the conditions\nthat maximize the non-equilibrium entropy are identified. A rigorous threshold\nbetween ``slow\" and ``fast\" processes is suggested, identifying the range of\napplicability of classical quasi-equilibrium description. The proposed theory\nis validated by deriving the known law of inertial heat conduction, which\naccounts for finite speed of thermal propagation. Finally, a new expression for\nthe non-equilibrium work is derived, revealing two kinds of pressure that\nemerge in fast non-equilibrium.",
        "This paper examines the Higgs particle self-coupling and its implications for\nelectroweak symmetry breaking in the Standard Model. We review the current\nexperimental constraints on the Higgs trilinear coupling and discuss the\nchallenges in measuring it precisely. The potential consequences of deviations\nfrom the Standard Model prediction are explored, including the possibility of\nnew physics. We then consider an alternative ultraviolet complete electroweak\ntheory based on finite quantum field theory, which does not require spontaneous\nsymmetry breaking or a non-zero vacuum expectation value. The predictions of\nthis model for particle masses and the stability of the electroweak vacuum are\ncompared to the standard Higgs mechanism. Finally, we review the SM derivation\nof the W-boson mass, its dependence on radiative corrections and its\nexperimentally determined value.",
        "The advent of high-average-power, ultrafast ytterbium-based lasers allows us\nto generate laser filaments at repetition rates ranging from 10s of kHz up to\n100s of kHz. At such high repetition rates, the inter-pulse time lies below the\ntime required for the total diffusion of the deposited heat by each laser\npulse, leading to cumulative hydrodynamic effects that have so far been rarely\nstudied. Here, we present, to the best of our knowledge, the first experimental\ntime-resolved measurements of these dynamics in air for laser repetition rates\nbetween 1 kHz and 100 kHz. We measure the change in the air refractive index\ncaused by the localized heat deposition and the length of the\nfilament-generated plasma channel, with which we can infer the corresponding\nchange in air density. We observe that at repetition rates above 10 kHz,\nstationary density depletions with vanishing dynamics emerge. Our findings are\nof wide relevance for the fields of high-repetition-rate laser filamentation\nand its applications, as well as THz generation from laser-induced plasma\nsources.",
        "Stokesian Dynamics (SD) is a numerical framework used for simulating\nhydrodynamic interactions in particle suspensions at low Reynolds number. It\ncombines far-field approximations with near-field lubrication corrections,\noffering a balance between accuracy and efficiency. This work reviews SD and\nprovides a perspective on future directions for this approach. We outline the\nmathematical foundations, the method's strengths and weaknesses, and the\ncomputational challenges that need to be overcome to work with SD effectively.\nWe also discuss recent advancements that improve the algorithm's efficiency,\nincluding the use of iterative solvers and matrix-free approaches. In addition,\nwe highlight the limitations of making stronger, albeit more cost-effective\napproximations to studying hydrodynamic interactions in dense suspensions than\nmade in SD, such as the two-body Rotne-Prager-Yamakawa (RPY) approximation. To\novercome these issues, we propose a hybrid framework that replaces SD's full\nmany-body computations with a neural network trained on SD data. That is, we\ncorrect the RPY approximation, while avoiding costly matrix inversions. We\ndemonstrate the potential of this method on a simple system, where we find a\nclose match to SD data while algorithmically outperforming RPY. Our work\nprovides an outlook on the way in which large-scale simulations of particle\nsuspensions can be performed in the foreseeable future.",
        "Quenched disorder in a solid state system can result in Anderson localization\nwhere electrons are exponentially localized and the system behaves like an\ninsulator. In this study, we investigate the effect of a DC electric field on\nAnderson localization. The study highlights the case of a one-dimensional\ninsulator chain with on-site disorder when a DC electric field is applied\nthroughout the chain. We study spectral properties of an Anderson localized\nsystem in equilibrium and out-of-equilibrium using a full lattice\nnonequilibrium Green's function method in the steady-state limit. Tuning the\ndisorder and the electric field strength results in the creation of exponential\nLifshitz tails near the band edge by strongly localized levels. These Lifshtiz\ntails create effects like insulator-to-metal transitions and contribute to\nnon-local hopping. The electric field causes gradual delocalization of the\nsystem and Anderson localization crossing over to Wannier Stark ladders at very\nstrong fields. Our study makes a comparison with the coherent potential\napproximation (CPA) highlighting some major differences and similarities in the\nphysics of disorder.",
        "We study the problem of PAC learning $\\gamma$-margin halfspaces in the\npresence of Massart noise. Without computational considerations, the sample\ncomplexity of this learning problem is known to be\n$\\widetilde{\\Theta}(1\/(\\gamma^2 \\epsilon))$. Prior computationally efficient\nalgorithms for the problem incur sample complexity $\\tilde{O}(1\/(\\gamma^4\n\\epsilon^3))$ and achieve 0-1 error of $\\eta+\\epsilon$, where $\\eta<1\/2$ is the\nupper bound on the noise rate. Recent work gave evidence of an\ninformation-computation tradeoff, suggesting that a quadratic dependence on\n$1\/\\epsilon$ is required for computationally efficient algorithms. Our main\nresult is a computationally efficient learner with sample complexity\n$\\widetilde{\\Theta}(1\/(\\gamma^2 \\epsilon^2))$, nearly matching this lower\nbound. In addition, our algorithm is simple and practical, relying on online\nSGD on a carefully selected sequence of convex losses.",
        "Assessing the prevalence of atmospheres on rocky planets around M-dwarf stars\nis a top priority of exoplanet science. High-energy activity from M-dwarfs can\ndestroy the atmospheres of these planets, which could explain the lack of\natmosphere detections to date. Volcanic outgassing has been proposed as a\nmechanism to replenish the atmospheres of tidally-heated rocky planets. L 98-59\nb, a sub-Earth transiting a nearby M dwarf, was recently identified as the most\npromising exoplanet to detect a volcanic atmosphere. We present the\ntransmission spectrum of L 98-59 b from four transits observed with JWST\nNIRSpec G395H. Although the airless model provides an adequate fit to the data\nbased on its $\\chi^2$, an SO$_2$ atmosphere is preferred by 3.6$\\sigma$ over a\nflat line in terms of the Bayesian evidence. Such an atmosphere would likely be\nin a steady state where volcanism balances escape. If so, L 98-59 b must\nexperience at least eight times as much volcanism and tidal heating per unit\nmass as Io. If volcanism is driven by runaway melting of the mantle, we predict\nthe existence of a subsurface magma ocean in L 98-59 b extending up to $R_p\\sim\n60-90\\%$. An SO$_2$-rich volcanic atmosphere on L 98-59 b would be indicative\nof an oxidized mantle with an oxygen fugacity of $f\\rm{O}_2>IW+2.7$, and it\nwould imply that L 98-59 b must have retained some of its volatile endowment\ndespite its proximity to its star. Our findings suggest that volcanism may\nrevive secondary atmospheres on tidally heated rocky planets around M-dwarfs.",
        "In this paper, we characterize all Artinian complete intersection\n$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In\naddition, we prove that such\n  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong\nLefschetz property.",
        "In the recent past, the work in the area of vanishing coefficients of\ninfinite $q$-products has been taken to the forefront. Weaving the same thread\nas Ramanujan, Richmond, Szekeres, Andrews, Alladi, Gordon, Mc Laughlin, Baruah,\nKaur, Tang, we further prove vanishing coefficients in arithmetic progressions\nmoduli 5, 7, 11, 13, 19, 21, 23 and 29 of the following four families of\ninfinite products, where $\\{X_{a,b,sm,km,u,v}(n)\\}_{n\\geq n_0}$,\n$\\{Y_{a,b,sm,km,u,v}(n)\\}_{n\\geq n_0}$, $\\{Z_{a,b,sm,km,u,v}(n)\\}_{n\\geq n_0}$\nand $\\{W_{a,b,sm,km,u,v}(n) \\}_{n\\geq n_0}$ are defined by \\begin{align*}\n\\sum_{n\\geq\nn_0}^{\\infty}X_{a,b,sm,km,u,v}(n)q^n:=&(q^{a},q^{sm-a};q^{sm})_{infty}^u(q^{b},q^{km-b};q^{km})_{infty}^v,\n\\\\ \\sum_{n\\geq\nn_0}^{\\infty}Y_{a,b,sm,km,u,v}(n)q^n:=&(q^{a},q^{sm-a};q^{sm})_{infty}^u(-q^{b},-q^{km-b};q^{km})_{infty}^v,\n\\\\ \\sum_{n\\geq\nn_0}^{\\infty}Z_{a,b,sm,km,u,v}(n)q^n:=&(-q^{a},-q^{sm-a};q^{sm})_{infty}^u(q^{b},q^{km-b};q^{km})_{infty}^v,\\\\\n\\sum_{n\\geq\nn_0}^{\\infty}W_{a,b,sm,km,u,v}(n)q^n:=&(-q^{a},-q^{sm-a};q^{sm})_{infty}^u(-q^{b},-q^{km-b};q^{km})_{infty}^v,\n\\end{align*} here $a, b, s, k, u$ and $v$ are chosen in such a way that the\ninfinite products in the right-hand side of the above are convergent and $n_0$\nis an integer (possibly negative or zero) depending on $a, b, s, k, u$ and $v$.\nThe proof uses the Jacobi triple product identity and the properties of\nRamanujan general theta function.",
        "Quantum machine learning is among the most exciting potential applications of\nquantum computing. However, the vulnerability of quantum information to\nenvironmental noises and the consequent high cost for realizing fault tolerance\nhas impeded the quantum models from learning complex datasets. Here, we\nintroduce AdaBoost.Q, a quantum adaptation of the classical adaptive boosting\n(AdaBoost) algorithm designed to enhance learning capabilities of quantum\nclassifiers. Based on the probabilistic nature of quantum measurement, the\nalgorithm improves the prediction accuracy by refining the attention mechanism\nduring the adaptive training and combination of quantum classifiers. We\nexperimentally demonstrate the versatility of our approach on a programmable\nsuperconducting processor, where we observe notable performance enhancements\nacross various quantum machine learning models, including quantum neural\nnetworks and quantum convolutional neural networks. With AdaBoost.Q, we achieve\nan accuracy above 86% for a ten-class classification task over 10,000 test\nsamples, and an accuracy of 100% for a quantum feature recognition task over\n1,564 test samples. Our results demonstrate a foundational tool for advancing\nquantum machine learning towards practical applications, which has broad\napplicability to both the current noisy and the future fault-tolerant quantum\ndevices.",
        "We verify that elliptic K3 surfaces and algebraic groups have many rational\npoints over function fields, i.e., they are geometrically special in the sense\nof Javanpeykar-Rousseau. We also show that under additional assumptions, this\ngeometric specialness persists under removal of closed subsets of codimension\nat least two.",
        "Consider a finite field $\\mathbb F_q$, $q=p^d$, where $p$ is an odd number.\nLet $M=(E,r)$ be a regular matroid; denote by ${\\mathcal B}$ the family of its\nbases, $\\bar s(M;\\alpha)=\\sum_{B\\in {\\mathcal B}}\\prod_{e\\not\\in B} \\alpha_e$,\nwhere ${\\alpha_e\\in \\mathbb F_q}$, $\\alpha_e\\neq 0$. Let a subset $A\\equiv\nA(\\alpha)$ in $E$ have the maximal cardinality and satisfy the condition $\\bar\ns(M|A;\\alpha)\\neq 0$, while ${r^*}(\\alpha)=|A|-r(E)$. Let us represent the\nvalue of the characteristic polynomial of the matroid $M$ at the point $q$ as\nthe linear combination of Legendre symbols with respect to $\\bar\ns(M|A;\\alpha)$, whose coefficients are modulo equal to $1\/q^{r^*(\\alpha)\/2}$.\nThis representation generalizes the formula for a flow polynomial of a graph\nwhich was obtained by us earlier. The latter formula is an analog of the\nso-called $\\alpha$-representation of vacuum Feynman amplitudes in the case of a\nfinite field, which has inspired the Kontsevich conjecture (1997). The\n$\\alpha$-representation technique is also applicable for expressing the number\nof Tait colorings for a cubic biconnected planar graph in terms of principal\nminors of the matrix of faces of this graph.",
        "In this article we consider a modification of the Stein's spherical maximal\noperator of complex order $\\alpha$ on ${\\mathbb R^n}$: $$ {\\mathfrak\nM}^\\alpha_{[1,2]} f(x) =\\sup\\limits_{t\\in [1,2]} \\big| {1\\over \\Gamma(\\alpha) }\n\\int_{|y|\\leq 1} \\left(1-|y|^2 \\right)^{\\alpha -1} f(x-ty) dy\\big|. $$ We show\nthat when $n\\geq 2$, suppose $\\|{\\mathfrak M}^{\\alpha}_{[1,2]} f\n\\|_{L^q({\\mathbb R^n})} \\leq C\\|f \\|_{L^p({\\mathbb R^n})}$ holds for some\n$\\alpha\\in \\mathbb{C}$, $p,q\\geq1$, then we must have that $q\\geq p$ and $${\\rm\nRe}\\,\\alpha\\geq \\sigma_n(p,q):=\\max\\left\\{\\frac{1}{p}-\\frac{n}{q},\\\n\\frac{n+1}{2p}-\\frac{n-1}{2}\\left(\\frac{1}{q}+1\\right),\\frac{n}{p}-n+1\\right\\}.$$\n  Conversely, we show that ${\\mathfrak M}^\\alpha_{[1,2]}$ is bounded from\n$L^p({\\mathbb R^n})$ to $L^q({\\mathbb R^n})$ provided that $q\\geq p$ and ${\\rm\nRe}\\,\\alpha>\\sigma_2(p,q)$ for $n=2$; and ${\\rm\nRe}\\,\\alpha>\\max\\left\\{\\sigma_n(p,q), 1\/(2p)- (n-2)\/(2q) -(n-1)\/4\\right\\}$ for\n$n>2$. The range of $\\alpha,p$ and $q$ is almost optimal in the case either\n$n=2$, or $\\alpha=0$, or $(p,q)$ lies in some regions for $n>2$."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Pediatric low-grade glioma: State-of-the-art and ongoing challenges",
    "start_abstract":"Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
      ],
      "abstract":[
        "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "The Complex Magnetic Field of the Extreme Galactic Center: PRIMA Science\n  Potential",
        "FoundationStereo: Zero-Shot Stereo Matching",
        "Self-Evaluation for Job-Shop Scheduling",
        "Quantifying the Speed-Up from Non-Reversibility in MCMC Tempering\n  Algorithms",
        "When is dataset cartography ineffective? Using training dynamics does\n  not improve robustness against Adversarial SQuAD",
        "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models",
        "Generation of Frequency-Tunable Shaped Single Microwave Photons Using a\n  Fixed-Frequency Superconducting Qubit",
        "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
        "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models",
        "$\\beta$-delayed neutron spectroscopy of $^{85, 86}$As with MONSTER at\n  IGISOL",
        "Auxiliary-field quantum Monte Carlo method with quantum selected\n  configuration interaction",
        "Exotic spherical flexible octahedra and counterexamples to the Modified\n  Bellows Conjecture",
        "Is Relevance Propagated from Retriever to Generator in RAG?",
        "Understanding colors of Dufaycolor: Can we recover them using historical\n  colorimetric and spectral data?",
        "Light-by-Light scattering in ultraperipheral heavy ion collisions:\n  Estimating inelastic contributions",
        "Toward Copyright Integrity and Verifiability via Multi-Bit Watermarking\n  for Intelligent Transportation Systems",
        "Mimicking How Humans Interpret Out-of-Context Sentences Through\n  Controlled Toxicity Decoding",
        "The Forestry of Adversarial Totient Iterations",
        "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
        "3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic\n  Segmentation in Autonomous Driving",
        "SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack\n  Segmentation in Structures",
        "Partition Tree Weighting for Non-Stationary Stochastic Bandits",
        "A time-dependent inverse source problem for a semilinear\n  pseudo-parabolic equation with Neumann boundary condition",
        "Path-Adaptive Matting for Efficient Inference Under Various\n  Computational Cost Constraints",
        "Non-adiabatic linear response in open quantum systems",
        "Flexible Exoskeleton Control Based on Binding Alignment Strategy and\n  Full-arm Coordination Mechanism",
        "Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for\n  Perspective-Specific Summarization of Clinical Q&A Forums",
        "Social inequality and cultural factors impact the awareness and reaction\n  during the cryptic transmission period of pandemic",
        "A Globally Convergent Method for Computing B-stationary Points of\n  Mathematical Programs with Equilibrium Constraints"
      ],
      "abstract":[
        "The Central Molecular Zone (CMZ) of the Galactic Center (GC) region of the\nMilky Way contains a substantial fraction of the molecular mass of the Galaxy\n>10e7 solar masses yet exhibits an order of magnitude lower star formation\nefficiency (SFE) than expected given the high densities found in this region.\nThere are multiple possible explanations for the depressed SFE in the CMZ, like\nfeedback, strong turbulence, longer free-fall timescales, and high magnetic\nfield strengths. It is currently unclear which of these mechanisms is the\ndominant inhibitor of star formation in the CMZ. It is important to understand\nthe star formation process in the extreme environment of the CMZ because it is\nthe only Galactic nuclear region we are able to study at high spatial\nresolutions with current observatories. One way to determine the relative\nimportance of the different SFE inhibiting mechanisms is through multi-spatial\nand multi-frequency polarimetric observations of the CMZ. Such observations\nwill provide insight into the behavior of the magnetic field in this unique\nenvironment. These observations will complement radio observations of\nnon-thermal structures revealing the magnetic field morphology and\npolarization. The PRobe far--Infrared Mission for Astrophysics (PRIMA) will be\nuniquely capable of contributing to such explorations by providing unique\nresolutions and frequencies for polarimetric observations. The PRIMAger\ninstrument will yield polarimetric observations covering the wavelength range\n80 -- 261 um with beam sizes ranging from 11 -- 28'', capabilities that\ncomplement existing and upcoming observatories.",
        "Tremendous progress has been made in deep stereo matching to excel on\nbenchmark datasets through per-domain fine-tuning. However, achieving strong\nzero-shot generalization - a hallmark of foundation models in other computer\nvision tasks - remains challenging for stereo matching. We introduce\nFoundationStereo, a foundation model for stereo depth estimation designed to\nachieve strong zero-shot generalization. To this end, we first construct a\nlarge-scale (1M stereo pairs) synthetic training dataset featuring large\ndiversity and high photorealism, followed by an automatic self-curation\npipeline to remove ambiguous samples. We then design a number of network\narchitecture components to enhance scalability, including a side-tuning feature\nbackbone that adapts rich monocular priors from vision foundation models to\nmitigate the sim-to-real gap, and long-range context reasoning for effective\ncost volume filtering. Together, these components lead to strong robustness and\naccuracy across domains, establishing a new standard in zero-shot stereo depth\nestimation. Project page: https:\/\/nvlabs.github.io\/FoundationStereo\/",
        "Combinatorial optimization problems, such as scheduling and route planning,\nare crucial in various industries but are computationally intractable due to\ntheir NP-hard nature. Neural Combinatorial Optimization methods leverage\nmachine learning to address these challenges but often depend on sequential\ndecision-making, which is prone to error accumulation as small mistakes\npropagate throughout the process. Inspired by self-evaluation techniques in\nLarge Language Models, we propose a novel framework that generates and\nevaluates subsets of assignments, moving beyond traditional stepwise\napproaches. Applied to the Job-Shop Scheduling Problem, our method integrates a\nheterogeneous graph neural network with a Transformer to build a policy model\nand a self-evaluation function. Experimental validation on challenging,\nwell-known benchmarks demonstrates the effectiveness of our approach,\nsurpassing state-of-the-art methods.",
        "We investigate the increase in efficiency of simulated and parallel tempering\nMCMC algorithms when using non-reversible updates to give them \"momentum\". By\nmaking a connection to a certain simple discrete Markov chain, we show that,\nunder appropriate assumptions, the non-reversible algorithms still exhibit\ndiffusive behaviour, just on a different time scale. We use this to argue that\nthe optimally scaled versions of the non-reversible algorithms are indeed more\nefficient than the optimally scaled versions of their traditional reversible\ncounterparts, but only by a modest speed-up factor of about 42%.",
        "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.",
        "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
        "Scaling up a superconducting quantum computer will likely require quantum\ncommunication between remote chips, which can be implemented using an itinerant\nmicrowave photon in a transmission line. To realize high-fidelity\ncommunication, it is essential to control the frequency and temporal shape of\nthe microwave photon. In this work, we demonstrate the generation of\nfrequency-tunable shaped microwave photons without resorting to any\nfrequency-tunable circuit element. We develop a framework which treats a\nmicrowave resonator as a band-pass filter mediating the interaction between a\nsuperconducting qubit and the modes in the transmission line. This\ninterpretation allows us to stimulate the photon emission by an off-resonant\ndrive signal. We characterize how the frequency and temporal shape of the\ngenerated photon depends on the frequency and amplitude of the drive signal. By\nmodulating the drive amplitude and frequency, we achieve a frequency tunability\nof 40 MHz while maintaining the photon mode shape time-symmetric.Through\nmeasurements of the quadrature amplitudes of the emitted photons, we\ndemonstrate consistently high state and process fidelities around 95\\% across\nthe tunable frequency range. Our hardware-efficient approach eliminates the\nneed for additional biasing lines typically required for frequency tuning,\noffering a simplified architecture for scalable quantum communication.",
        "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
        "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.",
        "The $\\beta$-delayed neutron emission in the $^{85, 86}$As $\\beta$-decays has\nbeen measured at the Ion Guide Isotope Separator On Line facility of the\nAccelerator Laboratory of the University of Jyv\\\"askyl\\\"a. The complete\n$\\beta$-decays have been studied with a complex setup that consists of a\nplastic scintillator for $\\beta$-particles, MONSTER -- the MOdular Neutron\ntime-of-flight SpectromeTER -- for neutrons, and a high-purity germanium and\nfour LaBr$_3$ crystals for $\\gamma$-rays. The $\\beta$-delayed neutron energy\ndistributions have been determined by unfolding the time-of-flight spectra with\nan innovative methodology based on the iterative Bayesian unfolding method and\naccurate Monte Carlo simulations. The results obtained for $^{85}$As are in\nexcellent agreement with the existing evaluated data, validating the proposed\nmethodology. In the case of $^{86}$As, a stronger neutron intensity at higher\nenergies than previously predicted is discovered.",
        "We propose using the wave function generated by the quantum selected\nconfiguration interaction (QSCI) method as the trial wave function in phaseless\nauxiliary-field quantum Monte Carlo (ph-AFQMC). In the QSCI framework,\nelectronic configurations are sampled from the quantum state realized on a\nquantum computer. These configurations serve as basis states for constructing\nan effective Hamiltonian, which is then diagonalized to obtain the\ncorresponding eigenstate. Using this wave function, ph-AFQMC is performed to\nrecover the dynamical electron correlation across the whole orbital space. The\nuse of the QSCI trial wave function is expected to improve the feasibility of\nthe quantum-classical (QC) hybrid quantum Monte Carlo approach [Nature, 603,\n416 (2022)]. We call this integrated approach QC-QSCI-AFQMC, or QSCI-AFQMC for\nshort. This method is validated across several molecular systems. For H2O and a\nlinear H4 chain, we achieved chemical accuracy in most investigations relative\nto full configuration interaction while utilizing superconducting quantum\ncomputers at Osaka University and RIKEN. Additionally, the application of\nQSCI-AFQMC to the O-H bond dissociation in an organic molecule highlights the\ncomplementary synergy between capturing static correlation on quantum hardware\nand incorporating dynamical correlation via classical post-processing. For the\nN2, when QSCI-AFQMC is executed with a noiseless simulator, it ranks among the\nmost accurate methods compared to various multireference electronic structure\ntheories. Although the proposed method is demonstrated using small active\nspaces on current quantum devices, the concept is not limited to few-qubit\nproblems. The QSCI-AFQMC can compete with state-of-the-art classical\ncomputational techniques, particularly in larger active spaces, displaying\nconsiderable potential for resolving classically intractable problems in\nquantum chemistry.",
        "In 2014 the author showed that in the three-dimensional spherical space,\nalongside with three classical types of flexible octahedra constructed by\nBricard, there exists a new type of flexible octahedra, which was called\nexotic. In the present paper we give a geometric construction for exotic\nflexible octahedra, describe their configuration spaces, and calculate their\nvolumes. We show that the volume of an exotic flexible octahedron is\nnonconstant during the flexion, and moreover the volume remains nonconstant if\nwe replace any set of vertices of the octahedron with their antipodes. So\nexotic flexible octahedra are counterexamples to the Modified Bellows\nConjecture proposed by the author in 2015.",
        "Retrieval Augmented Generation (RAG) is a framework for incorporating\nexternal knowledge, usually in the form of a set of documents retrieved from a\ncollection, as a part of a prompt to a large language model (LLM) to\npotentially improve the performance of a downstream task, such as question\nanswering. Different from a standard retrieval task's objective of maximising\nthe relevance of a set of top-ranked documents, a RAG system's objective is\nrather to maximise their total utility, where the utility of a document\nindicates whether including it as a part of the additional contextual\ninformation in an LLM prompt improves a downstream task. Existing studies\ninvestigate the role of the relevance of a RAG context for knowledge-intensive\nlanguage tasks (KILT), where relevance essentially takes the form of answer\ncontainment. In contrast, in our work, relevance corresponds to that of topical\noverlap between a query and a document for an information seeking task.\nSpecifically, we make use of an IR test collection to empirically investigate\nwhether a RAG context comprised of topically relevant documents leads to\nimproved downstream performance. Our experiments lead to the following\nfindings: (a) there is a small positive correlation between relevance and\nutility; (b) this correlation decreases with increasing context sizes (higher\nvalues of k in k-shot); and (c) a more effective retrieval model generally\nleads to better downstream RAG performance.",
        "Dufaycolor, an additive color photography process produced from 1935 to the\nlate 1950s, represents one of the most advanced iterations of this technique.\nThis paper presents ongoing research and development of an open-source\nColor-Screen tool designed to reconstruct the original colors of additive color\nphotographs. We discuss the incorporation of historical measurements of dyes\nused in the production of the color-screen filter (r\\'eseau) to achieve\naccurate color recovery.",
        "The current state-of-the-art theoretical estimations lead to cross-sections\nfor $AA \\to \\gamma \\gamma AA$ which are somewhat smaller than the measured ones\nby the ATLAS and CMS Collaborations, which motivates the searching and\ncalculation of subleading corrections disregarded in these previous studies. In\nthis paper, we estimate the contribution of inelastic channels to the Light -\nby - Light (LbL) scattering in ultraperipheral collisions of heavy ions\n(UPHICs), in which one or both of the incident nuclei dissociate ($A A \\to\n\\gamma \\gamma X Y$ where $X, Y = A, A'$) due to the photon emission. These new\nmechanisms are related to extra emissions that are rather difficult to identify\nat the LHC and can be mistakenly interpreted as enhanced $\\gamma \\gamma \\to\n\\gamma \\gamma$ scattering compared to the Standard Model result. We include\nprocesses of coupling of photons to individual nucleons (protons and neutrons)\nin addition to coherent coupling to the whole nuclei (called standard approach\nhere). Both elastic (nucleon in the ground state) and inelastic (nucleon in an\nexcited state) in the couplings of photons to nucleons are taken into account.\nThe inelastic nucleon fluxes are calculated using CT18qed photon in nucleon\nPDFs. The inelastic photon fluxes are shown and compared to standard photon\nfluxes in the nucleus. In addition, we show the ratio of the inelastic\ncorrections to the standard contribution as a function of diphoton invariant\nmass and photon rapidity difference. We find the maximal effect of the\ninelastic corrections at $M_{\\gamma \\gamma} \\sim$ 14 GeV for the ATLAS rapidity\nand transverse momentum acceptance. Furthermore, the inelastic contribution\nincreases gradually with photon rapidity difference. Our results indicate that\nthe inelastic contributions can increase locally by 10-15 \\% the traditional\n(no nuclear excitation) predictions for the LbL scattering in UPCs.",
        "Intelligent transportation systems (ITS) use advanced technologies such as\nartificial intelligence to significantly improve traffic flow management\nefficiency, and promote the intelligent development of the transportation\nindustry. However, if the data in ITS is attacked, such as tampering or\nforgery, it will endanger public safety and cause social losses. Therefore,\nthis paper proposes a watermarking that can verify the integrity of copyright\nin response to the needs of ITS, termed ITSmark. ITSmark focuses on functions\nsuch as extracting watermarks, verifying permission, and tracing tampered\nlocations. The scheme uses the copyright information to build the multi-bit\nspace and divides this space into multiple segments. These segments will be\nassigned to tokens. Thus, the next token is determined by its segment which\ncontains the copyright. In this way, the obtained data contains the custom\nwatermark. To ensure the authorization, key parameters are encrypted during\ncopyright embedding to obtain cipher data. Only by possessing the correct\ncipher data and private key, can the user entirely extract the watermark.\nExperiments show that ITSmark surpasses baseline performances in data quality,\nextraction accuracy, and unforgeability. It also shows unique capabilities of\npermission verification and tampered location tracing, which ensures the\nsecurity of extraction and the reliability of copyright verification.\nFurthermore, ITSmark can also customize the watermark embedding position and\nproportion according to user needs, making embedding more flexible.",
        "Interpretations of a single sentence can vary, particularly when its context\nis lost. This paper aims to simulate how readers perceive content with varying\ntoxicity levels by generating diverse interpretations of out-of-context\nsentences. By modeling toxicity, we can anticipate misunderstandings and reveal\nhidden toxic meanings. Our proposed decoding strategy explicitly controls\ntoxicity in the set of generated interpretations by (i) aligning interpretation\ntoxicity with the input, (ii) relaxing toxicity constraints for more toxic\ninput sentences, and (iii) promoting diversity in toxicity levels within the\nset of generated interpretations. Experimental results show that our method\nimproves alignment with human-written interpretations in both syntax and\nsemantics while reducing model prediction uncertainty.",
        "We give a closed-form expression for\n$\\varphi(1+\\varphi(2+\\varphi(3+...+\\varphi(n)$, where $\\varphi$ is Euler's\ntotient function. More generally, for an integer sequence $A=\\{a_j\\}$ we study\nthe value of\n$A^\\varphi(n)=\\varphi(a_1+\\varphi(a_2+\\varphi(a_3+...+\\varphi(a_n)$ when $A$ is\nthe perfect squares or the perfect cubes. We show $A^\\varphi(n)$ is bounded for\nall sequences considered. We also present the Arboreal Algorithm which can\nsometimes determine a closed form of $A^\\varphi(n)$ using tree-like structures.",
        "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https:\/\/github.com\/zsq259\/Plan-over-Graph.",
        "Domain generalization aims to find ways for deep learning models to maintain\ntheir performance despite significant domain shifts between training and\ninference datasets. This is particularly important for models that need to be\nrobust or are costly to train. LiDAR perception in autonomous driving is\nimpacted by both of these concerns, leading to the emergence of various\napproaches. This work addresses the challenge by proposing a geometry-based\napproach, leveraging the sequential structure of LiDAR sensors, which sets it\napart from the learning-based methods commonly found in the literature. The\nproposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic\nSegmentation (LSS). Through extensive experimentation on seven datasets, it is\ndemonstrated to be a state-of-the-art approach, outperforming both naive and\nother domain generalization methods.",
        "Pixel-level segmentation of structural cracks across various scenarios\nremains a considerable challenge. Current methods encounter challenges in\neffectively modeling crack morphology and texture, facing challenges in\nbalancing segmentation quality with low computational resource usage. To\novercome these limitations, we propose a lightweight Structure-Aware Vision\nMamba Network (SCSegamba), capable of generating high-quality pixel-level\nsegmentation maps by leveraging both the morphological information and texture\ncues of crack pixels with minimal computational cost. Specifically, we\ndeveloped a Structure-Aware Visual State Space module (SAVSS), which\nincorporates a lightweight Gated Bottleneck Convolution (GBC) and a\nStructure-Aware Scanning Strategy (SASS). The key insight of GBC lies in its\neffectiveness in modeling the morphological information of cracks, while the\nSASS enhances the perception of crack topology and texture by strengthening the\ncontinuity of semantic information between crack pixels. Experiments on crack\nbenchmark datasets demonstrate that our method outperforms other\nstate-of-the-art (SOTA) methods, achieving the highest performance with only\n2.8M parameters. On the multi-scenario dataset, our method reached 0.8390 in F1\nscore and 0.8479 in mIoU. The code is available at\nhttps:\/\/github.com\/Karl1109\/SCSegamba.",
        "This paper considers a generalisation of universal source coding for\ninteraction data, namely data streams that have actions interleaved with\nobservations. Our goal will be to construct a coding distribution that is both\nuniversal \\emph{and} can be used as a control policy. Allowing for action\ngeneration needs careful treatment, as naive approaches which do not\ndistinguish between actions and observations run into the self-delusion problem\nin universal settings. We showcase our perspective in the context of the\nchallenging non-stationary stochastic Bernoulli bandit problem. Our main\ncontribution is an efficient and high performing algorithm for this problem\nthat generalises the Partition Tree Weighting universal source coding technique\nfor passive prediction to the control setting.",
        "In this paper, we study the inverse problem for determining an unknown\ntime-dependent source coefficient in a semilinear pseudo-parabolic equation\nwith variable coefficients and Neumann boundary condition. This unknown source\nterm is recovered from the integral measurement over the domain $\\Omega$. Based\non Rothe's method, the existence and uniqueness of a weak solution, under\nsuitable assumptions on the data, is established. A numerical time-discrete\nscheme for the unique weak solution and the unknown source coefficient is\ndesigned, and the convergence of the approximations is proved. Some numerical\nexperiments are presented to support the obtained theoretical results.",
        "In this paper, we explore a novel image matting task aimed at achieving\nefficient inference under various computational cost constraints, specifically\nFLOP limitations, using a single matting network. Existing matting methods\nwhich have not explored scalable architectures or path-learning strategies,\nfail to tackle this challenge. To overcome these limitations, we introduce\nPath-Adaptive Matting (PAM), a framework that dynamically adjusts network paths\nbased on image contexts and computational cost constraints. We formulate the\ntraining of the computational cost-constrained matting network as a bilevel\noptimization problem, jointly optimizing the matting network and the path\nestimator. Building on this formalization, we design a path-adaptive matting\narchitecture by incorporating path selection layers and learnable connect\nlayers to estimate optimal paths and perform efficient inference within a\nunified network. Furthermore, we propose a performance-aware path-learning\nstrategy to generate path labels online by evaluating a few paths sampled from\nthe prior distribution of optimal paths and network estimations, enabling\nrobust and efficient online path learning. Experiments on five image matting\ndatasets demonstrate that the proposed PAM framework achieves competitive\nperformance across a range of computational cost constraints.",
        "Adiabatic theorem and non-adiabatic corrections have been widely applied in\nmodern quantum technology. Recently, non-adiabatic linear response theory has\nbeen developed to probe the many-body correlations in closed systems. In this\nwork, we generalize the non-adiabatic linear response theory to open quantum\nmany-body systems. We find that the linear deviation from steady states is\nmemory-less, similar to the closed system. The linear response of observables\nwhile ramping the Hamiltonian is still related to the derivative of the\nretarded Green's function. However, as one ramps the dissipation, the\ncorresponding coefficient is determined by a new high-order correlation\nfunction in the steady state. Our work gives a neat result and generalizes the\ntool of ramping dynamics to study the many-body correlations in open quantum\nsystems.",
        "In rehabilitation, powered, and teleoperation exoskeletons, connecting the\nhuman body to the exoskeleton through binding attachments is a common\nconfiguration. However, the uncertainty of the tightness and the donning\ndeviation of the binding attachments will affect the flexibility and comfort of\nthe exoskeletons, especially during high-speed movement. To address this\nchallenge, this paper presents a flexible exoskeleton control approach with\nbinding alignment and full-arm coordination. Firstly, the sources of the force\ninteraction caused by donning offsets are analyzed, based on which the\ninteractive force data is classified into the major, assistant, coordination,\nand redundant component categories. Then, a binding alignment strategy (BAS) is\nproposed to reduce the donning disturbances by combining different force data.\nFurthermore, we propose a full-arm coordination mechanism (FCM) that focuses on\ntwo modes of arm movement intent, joint-oriented and target-oriented, to\nimprove the flexible performance of the whole exoskeleton control during\nhigh-speed motion. In this method, we propose an algorithm to distinguish the\ntwo intentions to resolve the conflict issue of the force component. Finally, a\nseries of experiments covering various aspects of exoskeleton performance\n(flexibility, adaptability, accuracy, speed, and fatigue) were conducted to\ndemonstrate the benefits of our control framework in our full-arm exoskeleton.",
        "The PerAnsSumm 2025 challenge focuses on perspective-aware healthcare answer\nsummarization (Agarwal et al., 2025). This work proposes a few-shot learning\nframework using a Snorkel-BART-SVM pipeline for classifying and summarizing\nopen-ended healthcare community question-answering (CQA). An SVM model is\ntrained with weak supervision via Snorkel, enhancing zero-shot learning.\nExtractive classification identifies perspective-relevant sentences, which are\nthen summarized using a pretrained BART-CNN model. The approach achieved 12th\nplace among 100 teams in the shared task, demonstrating computational\nefficiency and contextual accuracy. By leveraging pretrained summarization\nmodels, this work advances medical CQA research and contributes to clinical\ndecision support systems.",
        "The World Health Organization (WHO) declared the COVID-19 outbreak a Public\nHealth Emergency of International Concern (PHEIC) on January 31, 2020. However,\nrumors of a \"mysterious virus\" had already been circulating in China in\nDecember 2019, possibly preceding the first confirmed COVID-19 case.\nUnderstanding how awareness about an emerging pandemic spreads through society\nis vital not only for enhancing disease surveillance, but also for mitigating\ndemand shocks and social inequities, such as shortages of personal protective\nequipment (PPE) and essential supplies. Here we leverage a massive e-commerce\ndataset comprising 150 billion online queries and purchase records from 94\nmillion people to detect the traces of early awareness and public response\nduring the cryptic transmission period of COVID-19. Our analysis focuses on\nidentifying information gaps across different demographic cohorts, revealing\nsignificant social inequities and the role of cultural factors in shaping\nawareness diffusion and response behaviors. By modeling awareness diffusion in\nheterogeneous social networks and analyzing online shopping behavior, we\nuncover the evolving characteristics of vulnerable populations. Our findings\nexpand the theoretical understanding of awareness spread and social inequality\nin the early stages of a pandemic, highlighting the critical importance of\ne-commerce data and social network data in effectively and timely addressing\nfuture pandemic challenges. We also provide actionable recommendations to\nbetter manage and mitigate dynamic social inequalities in public health crises.",
        "This paper introduces a method that globally converges to B-stationary points\nof mathematical programs with equilibrium constraints (MPECs) in a finite\nnumber of iterations. B-stationarity is necessary for optimality and means that\nno feasible first-order direction improves the objective. Given a feasible\npoint of an MPEC, B-stationarity can be certified by solving a linear program\nwith equilibrium constraints (LPEC) constructed at this point. The proposed\nmethod solves a sequence of LPECs, which either certify B-stationarity or\nprovide an active-set estimate for the complementarity constraints, and\nnonlinear programs (NLPs) - referred to as branch NLPs (BNLPs) - obtained by\nfixing the active set in the MPEC. A BNLP is more regular than the MPEC, easier\nto solve, and with the correct active set, its solution coincides with the\nsolution of the MPEC. The method has two phases: the first phase identifies a\nfeasible BNLP or certifies local infeasibility, and the second phase solves a\nfinite sequence of BNLPs until a B-stationary point of the MPEC is found. The\npaper provides a detailed convergence analysis and discusses implementation\ndetails. In addition, extensive numerical experiments and an open-source\nsoftware implementation are provided. The experiments demonstrate that the\nproposed method is more robust and faster than relaxation-based methods, while\nalso providing a certificate of B-stationarity without requiring the usual\ntoo-restrictive assumptions."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks",
    "start_abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
      ],
      "abstract":[
        "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Automated Microsolvation for Minimum Energy Path Construction in\n  Solution",
        "Giant Uncompensated Magnon Spin Currents in X-type Magnets",
        "Hedging with Sparse Reward Reinforcement Learning",
        "Construction A Lattice Design Based on the Truncated Union Bound",
        "Data-Aided Regularization of Direct-Estimate Combiner in Distributed\n  MIMO Systems",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Coverage errors for Student's t confidence intervals comparable to those\n  in Hall (1988)",
        "Evidence of Athermal Metastable Phase in a Halide Perovskite: Optically\n  Tracked Thermal-Breach Memory",
        "Do We Need to Verify Step by Step? Rethinking Process Supervision from a\n  Theoretical Perspective",
        "The bound and resonant states of $D^{(*)}D^{(*)}$ and\n  $D^{(*)}\\bar{D}^{(*)}$ with the complex scaling method",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "Fastest mixing reversible Markov chain on friendship graph: Trade-off\n  between transition probabilities among friends and convergence rate",
        "From Paramagnet to Dipolar Topological Order via Duality and Dipolar SPT",
        "Engagement Zones for a Turn Constrained Pursuer",
        "Benefits of Early Stopping in Gradient Descent for Overparameterized\n  Logistic Regression",
        "Time-Varying Causal Survival Learning",
        "Design and Analysis of a Concatenated Code for Intersymbol Interference\n  Wiretap Channels",
        "Mixed-state learnability transitions in monitored noisy quantum dynamics",
        "Limit Theorems for One-Dimensional Homogenized Diffusion Processes",
        "Quantifying and Visualizing the Microscopic Degrees of Freedom of Grain\n  Boundaries in the Wigner-Seitz Cell of the Displacement-Shift-Complete\n  Lattice",
        "Titan's Fluvial and Lacustrine Landscapes",
        "Claw-free cubic graphs are (1, 1, 1, 3)-packing edge-colorable",
        "Dynamics of test particles and scalar perturbations around\n  Ay\\'{o}n-Beato-Garc\\'{i}a black hole coupled with cloud of strings",
        "The complex Liouville string: the gravitational path integral",
        "Spin density matrix for neutral $\\rho$ mesons in a pion gas in linear\n  response theory",
        "Single pion resonant production in BSM scenarios: cross sections and\n  amplitudes",
        "Auxiliary dynamical mean-field approach for Anderson-Hubbard model with\n  off-diagonal disorder",
        "Model Categories and the Higher Riemann-Hilbert Correspondence",
        "Taylor-Couette flow with split endcaps: preparatory hydrodynamic study\n  for upcoming DRESDYN-MRI experiment"
      ],
      "abstract":[
        "Describing chemical reactions in solution on a molecular level is a\nchallenging task due to the high mobility of weakly interacting solvent\nmolecules which requires configurational sampling. For instance, polar and\nprotic solvents can interact strongly with solutes and may interfere in\nreactions. However, to define and identify representative arrangements of\nsolvent molecules modulating a transition state is a non-trivial task. Here, we\npropose to monitor their active participation in the decaying normal mode at a\ntransition state, which defines active solvent molecules. Moreover, it is\ndesirable to prepare a low-dimensional microsolvation model in a well-defined,\nfully automated, high-throughput, and easy-to-deploy fashion, which we propose\nto derive in a stepwise protocol. First, transition state structures are\noptimized in a sufficiently solvated quantum-classical hybrid model, which are\nthen subjected to a re-definition of a then reduced quantum region. From the\nreduced model, minimally microsolvated structures are extracted that contain\nonly active solvent molecules. Modeling the remaining solvation effects is\ndeferred to a continuum model. To establish an easy-to-use free-energy model,\nwe combine the standard thermochemical gas-phase model with a correction for\nthe cavity entropy in solution. We assess our microsolvation and free-energy\nmodels for methanediol formation from formaldehyde, for the hydration of carbon\ndioxide (which we consider in a solvent mixture to demonstrate the versatility\nof our approach), and, finally, for the chlorination of phenol with\nhypochlorous acid.",
        "Magnon spin currents in insulating magnets are useful for low-power\nspintronics. However, in magnets stacked by antiferromagnetic (AFM) exchange\ncoupling, which have recently aroused significant interest for potential\napplications in spintronics, these currents are largely counteracted by\nopposite magnetic sublattices, thus suppressing their net effect. Contrary to\nthis common observation, here, we show that magnets with X-type AFM stacking,\nwhere opposite magnetic sublattices form orthogonal intersecting chains,\nsupport giant magnon spin currents with minimal compensation. Our model\nHamiltonian calculations predict magnetic chain locking of magnon spin currents\nin these X-type magnets, significantly reducing their compensation ratio. In\naddition, the one-dimensional nature of the chain-like magnetic sublattices\nenhances magnon spin conductivities surpassing those of two-dimensional\nferromagnets and canonical altermagnets. Notably, uncompensated X-type magnets,\nsuch as odd-layer antiferromagnets and ferrimagnets, can exhibit magnon spin\ncurrents polarized opposite to those expected by their net magnetization. These\nunprecedented properties of X-type magnets, combined with their inherent\nadvantages resulting from AFM coupling, offer a promising new path for\nlow-power high-performance spintronics.",
        "Derivatives, as a critical class of financial instruments, isolate and trade\nthe price attributes of risk assets such as stocks, commodities, and indices,\naiding risk management and enhancing market efficiency. However, traditional\nhedging models, constrained by assumptions such as continuous trading and zero\ntransaction costs, fail to satisfy risk control requirements in complex and\nuncertain real-world markets.\n  With advances in computing technology and deep learning, data-driven trading\nstrategies are becoming increasingly prevalent. This thesis proposes a\nderivatives hedging framework integrating deep learning and reinforcement\nlearning. The framework comprises a probabilistic forecasting model and a\nhedging agent, enabling market probability prediction, derivative pricing, and\nhedging.\n  Specifically, we design a spatiotemporal attention-based probabilistic\nfinancial time series forecasting Transformer to address the scarcity of\nderivatives hedging data. A low-rank attention mechanism compresses\nhigh-dimensional assets into a low-dimensional latent space, capturing\nnonlinear asset relationships. The Transformer models sequential dependencies\nwithin this latent space, improving market probability forecasts and\nconstructing an online training environment for downstream hedging tasks.\n  Additionally, we incorporate generalized geometric Brownian motion to develop\na risk-neutral pricing approach for derivatives. We model derivatives hedging\nas a reinforcement learning problem with sparse rewards and propose a behavior\ncloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This\npretraining-finetuning framework significantly enhances the hedging agent's\nperformance. Numerical experiments in the U.S. and Chinese financial markets\ndemonstrate our method's superiority over traditional approaches.",
        "This paper considers $n= 128$ dimensional construction A lattice design,\nusing binary codes with known minimum Hamming distance and codeword\nmultiplicity, the number of minimum weight codeword. A truncated theta series\nof the lattice is explicitly given to obtain the truncated union bound to\nestimate the word error rate under maximum likelihood decoding. The best\ncomponent code is selected by minimizing the required volume-to-noise ratio\n(VNR) for a target word error rate $P_e$. The estimate becomes accurate for\n$P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH\ncodes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is\nachieved compared to that by the classic balanced distance rule and the equal\nerror probability rule. The $(128, 106, 8)$ EBCH code gives the best-known\n$n=128$ construction A lattice at $P_e= 10^{-5}$.",
        "This paper explores the data-aided regularization of the direct-estimate\ncombiner in the uplink of a distributed multiple-input multiple-output system.\nThe network-wide combiner can be computed directly from the pilot signal\nreceived at each access point, eliminating the need for explicit channel\nestimation. However, the sample covariance matrix of the received pilot signal\nthat is used in its computation may significantly deviate from the actual\ncovariance matrix when the number of pilot symbols is limited. To address this,\nwe apply a regularization to the sample covariance matrix using a shrinkage\ncoefficient based on the received data signal. Initially, the shrinkage\ncoefficient is determined by minimizing the difference between the sample\ncovariance matrices obtained from the received pilot and data signals. Given\nthe limitations of this approach in interference-limited scenarios, the\nshrinkage coefficient is iteratively optimized using the sample mean squared\nerror of the hard-decision symbols, which is more closely related to the actual\nsystem's performance, e.g., the symbol error rate (SER). Numerical results\ndemonstrate that the proposed regularization of the direct-estimate combiner\nsignificantly enhances the SER, particularly when the number of pilot symbols\nis limited.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "Table 1 of Hall (1988) contains asymptotic coverage error formulas for some\nnonparametric approximate 95% confidence intervals for the mean based on $n$\nIID samples. The table includes an entry for an interval based on the central\nlimit theorem using Gaussian quantiles and the Gaussian maximum likelihood\nvariance estimate. It is missing an entry for the very widely used Student $t$\nconfidence intervals. This note makes a mild numerical correction for the\nGaussian entry and provides an entry for the Student $t$ intervals. For\nskewness $\\gamma$ and kurtosis $\\kappa$, the corrected Gaussian formula is\n$0.14\\kappa -2.16\\gamma^2-3.42$ and the formula for the $t$ intervals is\n$0.14\\kappa -2.16\\gamma^2$. The impetus to revisit this estimate arose from the\nsurprisingly robust performance of Student's t statistic in randomized\nquasi-Monte Carlo sampling.",
        "Halide perovskite materials have been extensively studied in the last decade\nbecause of their impressive optoelectronic properties. However, their one\ncharacteristic that is uncommon for semiconductors is that many undergo\nthermally induced structural phase transitions. The transition is hysteretic,\nwith the hysteresis window marking the boundary of the metastable phase. We\nhave discovered that in methylammonium lead iodide, this hysteretic metastable\nphase is athermal, meaning it shows almost no temporal phase evolution under\nisothermal conditions. We also show that a large number of distinguishable\nmetastable states can be prepared following different thermal pathways.\nFurthermore, under a reversible thermal perturbation, the states in the\nmetastable phase either show return-point memory or undergo a systematic\nnonrecoverable phase evolution, depending on the thermal history and the sign\nof the temperature perturbation. Since the phase fraction can be probed with\nextreme sensitivity via luminescence, we have an optically retrievable memory\nthat reliably records any breach in temperature stability. Such thermal-breach\nmemory in athermal martensites, of which there are numerous examples, may be\nuseful for tagging packages requiring strict temperature control during\ntransportation or preservation.",
        "As large language models have evolved, it has become crucial to distinguish\nbetween process supervision and outcome supervision -- two key reinforcement\nlearning approaches to complex reasoning tasks. While process supervision\noffers intuitive advantages for long-term credit assignment, the precise\nrelationship between these paradigms has remained an open question.\nConventional wisdom suggests that outcome supervision is fundamentally more\nchallenging due to the trajectory-level coverage problem, leading to\nsignificant investment in collecting fine-grained process supervision data.\n  In this paper, we take steps towards resolving this debate. Our main theorem\nshows that, under standard data coverage assumptions, reinforcement learning\nthrough outcome supervision is no more statistically difficult than through\nprocess supervision, up to polynomial factors in horizon. At the core of this\nresult lies the novel Change of Trajectory Measure Lemma -- a technical tool\nthat bridges return-based trajectory measure and step-level distribution shift.\nFurthermore, for settings with access to a verifier or a rollout capability, we\nprove that any policy's advantage function can serve as an optimal process\nreward model, providing a direct connection between outcome and process\nsupervision. These findings suggest that the empirically observed performance\ngap -- if any -- between outcome and process supervision likely stems from\nalgorithmic limitations rather than inherent statistical difficulties,\npotentially transforming how we approach data collection and algorithm design\nfor reinforcement learning.",
        "We perform a systematic study of the possible molecular states composed of a\npair of heavy mesons such as $D^{(*)}D^{(*)}$, $D^{(*)}\\bar{D}^{(*)}$ in the\nframework of the one-boson-exchange model. The exchanged bosons include the\npseudoscalar, scalar and vector mesons($\\pi$, $\\sigma$, $\\rho$, $\\omega$). We\nuse the Bonn approximation to get the interaction potential of\none-boson-exchange model, then apply the complex scaling method to calculate\nthe bound and resonant states. The results indicate that the $D^{(*)}D^{(*)}$\nand $D^{(*)}\\bar{D}^{(*)}$ system can not only form several bound states, but\nalso a P-wave resonant state. The hadron molecular state model can explain the\nstructure of $T_{cc}^+$ as a bound state $DD^{*}$ with quantum number $I(J^P) =\n0(1^+)$. In addition, we also discovered other bound and resonant states, which\nhave the potential to be observed experimentally.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "A long-standing goal of social network research has been to alter the\nproperties of network to achieve the desired outcome. In doing so, DeGroot's\nconsensus model has served as the popular choice for modeling the information\ndiffusion and opinion formation in social networks. Achieving a trade-off\nbetween the cost associated with modifications made to the network and the\nspeed of convergence to the desired state has shown to be a critical factor.\nThis has been treated as the Fastest Mixing Markov Chain (FMMC) problem over a\ngraph with given transition probabilities over a subset of edges. Addressing\nthis multi-objective optimization problem over the friendship graph, this paper\nhas provided the corresponding Pareto optimal points or the Pareto frontier. In\nthe case of friendship graph with at least three blades, it is shown that the\nPareto frontier is reduced to a global minimum point which is same as the\noptimal point corresponding to the minimum spanning tree of the friendship\ngraph, i.e., the star topology. Furthermore, a lower limit for transition\nprobabilities among friends has been provided, where values higher than this\nlimit do not have any impact on the convergence rate.",
        "A scheme for the adaptive preparation of a topological state with dipole\nsymmetry, dubbed the dipolar topological state (dTS), which serves as an\nexample of translation symmetry-enriched topological phase, is proposed. The\nmidcircuit state emerging during the preparation process is identified as a\ntwo-dimensional symmetry-protected topological (SPT) state protected by dipole\nbundle symmetry alongside charge and 1-form symmetries. The non-trivial\nboundary modes of the dipolar SPT state exhibiting the spontaneous breaking of\ncharge and dipole bundle symmetries are analyzed. The duality map between the\nparamagnetic state and the dipolar topological state is established in the\nframework of the {\\it simultaneous gauging} of two charge symmetries and one\ndipole symmetry that cannot be reduced as sequential gauging of the individual\nsymmetry. Leveraging this duality, we work out the phase diagram of the dipolar\ntopological state under perturbations by various transverse fields.",
        "This work derives two basic engagement zone models, describing regions of\npotential risk or capture for a mobile vehicle by a pursuer. The pursuer is\nmodeled as having turn-constraints rather than simple motion. Turn-only\n(C-Paths) and turn-straight (CS-Paths) paths are considered for the pursuer of\nlimited range. Following the derivation, a simulation of a vehicle avoiding the\npursuer's engagement zone is provided.",
        "In overparameterized logistic regression, gradient descent (GD) iterates\ndiverge in norm while converging in direction to the maximum $\\ell_2$-margin\nsolution -- a phenomenon known as the implicit bias of GD. This work\ninvestigates additional regularization effects induced by early stopping in\nwell-specified high-dimensional logistic regression. We first demonstrate that\nthe excess logistic risk vanishes for early-stopped GD but diverges to infinity\nfor GD iterates at convergence. This suggests that early-stopped GD is\nwell-calibrated, whereas asymptotic GD is statistically inconsistent. Second,\nwe show that to attain a small excess zero-one risk, polynomially many samples\nare sufficient for early-stopped GD, while exponentially many samples are\nnecessary for any interpolating estimator, including asymptotic GD. This\nseparation underscores the statistical benefits of early stopping in the\noverparameterized regime. Finally, we establish nonasymptotic bounds on the\nnorm and angular differences between early-stopped GD and $\\ell_2$-regularized\nempirical risk minimizer, thereby connecting the implicit regularization of GD\nwith explicit $\\ell_2$-regularization.",
        "This work bridges the gap between staggered adoption designs and survival\nanalysis to estimate causal effects in settings with time-varying treatments,\naddressing a fundamental challenge in medical research exemplified by the\nStanford Heart Transplant study. In medical interventions, particularly organ\ntransplantation, the timing of treatment varies significantly across patients\ndue to factors such as donor availability and patient readiness, introducing\npotential bias in treatment effect estimation if not properly accounted for. We\nidentify conditions under which staggered adoption assumptions can justify the\nuse of survival analysis techniques for causal inference with time-varying\ntreatments. By establishing this connection, we enable the use of existing\nsurvival analysis methods while maintaining causal interpretability.\nFurthermore, we enhance estimation performance by incorporating double machine\nlearning methods, improving efficiency when handling complex relationships\nbetween patient characteristics and survival outcomes. Through both simulation\nstudies and application to heart transplant data, our approach demonstrates\nsuperior performance compared to traditional methods, reducing bias and\noffering theoretical guarantees for improved efficiency in survival analysis\nsettings.",
        "We propose a two-stage concatenated coding scheme for reliable and\ninformation-theoretically secure communication over intersymbol interference\nwiretap channels. Motivated by the theoretical coding strategies that achieve\nthe secrecy capacity, our scheme integrates low-density parity-check (LDPC)\ncodes in the outer stage, forming a nested structure of wiretap codes, with\ntrellis codes in the inner stage to improve achievable secure rates. The\ntrellis code is specifically designed to transform the uniformly distributed\ncodewords produced by the LDPC code stage into a Markov process, achieving\ntight lower bounds on the secrecy capacity. We further estimate the information\nleakage rate of the proposed coding scheme using an upper bound. To meet the\nweak secrecy criterion, we optimize degree distributions of the irregular LDPC\ncodes at the outer stage, essentially driving the estimated upper bound on the\ninformation leakage rate to zero.",
        "We consider learnability transitions in monitored quantum systems that\nundergo noisy evolution, subject to a global strong symmetry -- i.e., in\naddition to the measuring apparatus, the system can interact with an unobserved\nenvironment, but does not exchange charge with it. As in the pure-state\nsetting, we find two information-theoretic phases -- a sharp (fuzzy) phase in\nwhich an eavesdropper can rapidly (slowly) learn the symmetry charge. However,\nbecause the dynamics is noisy, both phases can be simulated efficiently using\ntensor networks. Indeed, even when the true dynamics is unitary, introducing\nnoise by hand allows an eavesdropper to efficiently learn the symmetry charge\nfrom local measurements, as we demonstrate. We identify the fuzzy phase in this\nsetting as a mixed-state phase that exhibits spontaneous strong-to-weak\nsymmetry breaking.",
        "We present two limit theorems, a mean ergodic and a central limit theorem,\nfor a specific class of one-dimensional diffusion processes that depend on a\nsmall-scale parameter $\\varepsilon$ and converge weakly to a homogenized\ndiffusion process in the limit $\\varepsilon \\rightarrow 0$. In these results,\nwe allow for the time horizon to blow up such that $T_\\varepsilon \\rightarrow\n\\infty$ as $\\varepsilon \\rightarrow 0$. The novelty of the results arises from\nthe circumstance that many quantities are unbounded for $\\varepsilon\n\\rightarrow 0$, so that formerly established theory is not directly applicable\nhere and a careful investigation of all relevant $\\varepsilon$-dependent terms\nis required. As a mathematical application, we then use these limit theorems to\nprove asymptotic properties of a minimum distance estimator for parameters in a\nhomogenized diffusion equation.",
        "We introduce a grain boundary (GB) translation vector, $\\textbf{t}^{WS}$, to\ndescribe and quantify the domain of the microscopic degrees of freedom of GBs.\nIt has long been recognized that for fixed macroscopic degrees of freedom of a\nGB there exists a large multiplicity of states characterized by different\nrelative grain translations. More recently another degree of freedom, $[n]$,\nthe number of GB atoms, has emerged and is now recognized as an equally\nimportant component of GB structural multiplicity. In this work, we show that\nall GB microstates can be uniquely characterized by their value of\n$\\textbf{t}^{WS}$, which is located within the Wigner-Seitz (WS) cell of the\nDisplacement-Shift-Complete lattice (DSCL) of the GB. The GB translation vector\ncaptures information about both the translation state and the number of GB\natoms. We show that the density of GB microstates inside the cell of the DSCL\nis not uniform and can form clusters that correspond to different GB phases.\nThe vectors connecting the centers of the clusters correspond to the Burgers\nvectors of GB phase junctions, which can be predicted without building the\njunctions. Using $\\textbf{t}^{WS}$, we quantify GB excess shear and argue that\nit is defined up to a DSCL vector, which has implications for thermodynamic\nequilibrium conditions. Additionally, this work generalizes the definition of\nthe number of GB atoms $[n]$ to asymmetric boundaries.",
        "In this chapter we begin with a review of Titan's fluvial and lacustrine\nlandscapes as observed with Cassini remote sensing data, and what the many\ndiscoveries have revealed about Titan's surface materials and climate. Yet\nCassini remote sensing data are coarse, topographic data are largely lacking,\nand the absence of in situ field measurements means we have little\nunderstanding of what the surface is composed of. At present, our knowledge of\nTitan's hydrology is comparable to that of Mars in the 1970's during the Viking\nera. Fortunately, the coming decades promise many new and exciting discoveries\nthat can be achieved through Earth-based experiments, numerical modeling, and a\ncontinued commitment to the exploration of Titan by future missions, including\nboth Dragonfly and orbiting assets. We therefore close the chapter with a\ndiscussion about what can be done with the current Cassini data and how new\ndata, from both Dragonfly and a potential future orbiter, would allow us to\nleverage Titan to help solve some of the largest problems both here on Earth\nand on hydrologic planets and exoplanets more generally.",
        "For a non-decreasing positive integer sequence $S = (s_{1}, \\dots, s_{k})$,\nan $S$-packing edge-coloring of a graph $G$ is a partition of the edge set of\n$G$ into subsets $E_{1}, \\dots, E_{k}$ such that for each $1 \\leq i \\leq k$,\nthe distance between any two distinct edges $e_{1}, e_{2} \\in E_{i}$ is at\nleast $s_{i} + 1$. Hocquard et al. conjectured that cubic graphs, except for\nthe Petersen and Tietze graphs, admit $(1, 1, 1, 3)$-packing edge-colorings. In\nthis paper, we prove that every claw-free cubic graph admits such a coloring.",
        "In this paper, we investigate the geodesic motion and scalar perturbations of\nthe Ay\\'on-Beato-Garc\\'ia (ABG) black hole (BH) coupled with a cloud of strings\n(CS). By employing the effective potential approach, we analyze the\ntrajectories of massless and massive test particles around this regular BH\nsolution. The interplay between nonlinear electrodynamics (NLED) and CS\nsignificantly modifies the spacetime geometry, resulting in distinct dynamical\nproperties for test particles. The behavior of null and timelike geodesics,\nincluding their stability and escape conditions, is thoroughly examined. In\naddition to the geodesic analysis, we study scalar perturbations by deriving\nthe Klein-Gordon equation for massless scalar fields in this spacetime. The\nresulting Schr\\\"odinger-like wave equation reveals an effective potential that\ndepends intricately on the NLED and CS parameters. Furthermore, we compute the\ngreybody factors (GFs) to explore the energy transmission and absorption\nproperties of theBH. Our results demonstrate that the NLED parameter $g$ and\nthe CS parameter $\\alpha$ play pivotal roles in modulating the GFs, influencing\nthe energy spectrum of scalar radiation. These results provide significant\nunderstanding of the observational characteristics of the ABG NLED-CS BH in\nastrophysical contexts.",
        "We give a rigorous definition of sine dilaton gravity in terms of the\nworldsheet theory of the complex Liouville string arXiv:2409.17246. The latter\nhas a known exact solution that we leverage to explore the gravitational path\nintegral of sine dilaton gravity - a quantum deformation of dS JT gravity that\nadmits both AdS$_2$ and dS$_2$ vacua. We uncover that the gravitational path\nintegral receives contributions from new saddles describing transitions between\nvacua in a third-quantized picture. We also discuss the sphere and disk\npartition function in this context and contrast our findings with other recent\nwork on this theory.",
        "We calculate the spin density matrix for neutral $\\rho$ mesons from the\nspectral function and thermal shear tensor by Kubo formula in the linear\nresponse theory, which contributes to the $\\gamma$ correlator for the CME\nsearch. We derive the spectral function of neutral $\\rho$ mesons with\n$\\rho\\pi\\pi$ and $\\rho\\rho\\pi\\pi$ interactions using the Dyson-Schwinger\nequation. The thermal shear tensor contribution is obtained from the Kubo\nformula in the linear response theory. We numerically calculate $\\rho_{00}-1\/3$\nand $\\mathrm{Re}\\rho_{-1,1}$ using the simulation results for the thermal shear\ntensor by the hydrodynamical model, which are of the order\n$10^{-3}\\sim10^{-2}$.",
        "We present a comprehensive theoretical framework describing single pion\nresonant production through inelastic dark fermion-nucleon interactions\nmediated by resonances in the GeV-scale regime. Building upon the Rein-Sehgal\napproach, we derive differential cross sections for processes in which an\nincoming dark fermion scatters off a nucleon, exciting a resonance that\nsubsequently decays into a nucleon and a pion. Our formulation accommodates\nvarious mediator types-namely, dark photons with vector and axial couplings, as\nwell as scalar and pseudoscalar mediators-thereby extending the conventional\napproach that Rein, Sehgal and Berger performed for neutrino interactions.\nTransition amplitudes for the nucleon-to-resonance conversion are computed\nusing the relativistic harmonic-oscillator quark model from Feynman, Kislinger\nand Ravndal, while a Breit-Wigner prescription is employed to incorporate\nfinite resonance widths. This framework offers a robust tool for interpreting\nexperimental data in dark sector and matter searches and represents a\ncontribution to elucidate the role of resonances in GeV-scale phenomenology.",
        "This work reports a theoretical framework that combines the auxiliary\ncoherent potential approximation (ACPA-DMFT) with dynamical mean-field theory\nto study strongly correlated and disordered electronic systems with both\ndiagonal and off-diagonal disorders. In this method, by introducing an\nauxiliary coupling space with extended local degree of freedom,the diagonal and\noff-diagonal disorders are treated in a unified and self-consistent framework\nof coherent potential approximation, within which the dynamical mean-field\ntheory is naturally combined to handle the strongly correlated Anderson-Hubbard\nmodel. By using this approach, we compute matsubara Green's functions for a\nsimple cubic lattice at finite temperatures and derive impurity spectral\nfunctions through the maximum entropy method. Our results reveal the critical\ninfluence of off-diagonal disorder on Mott-type metal-insulator transitions.\nSpecifically, a reentrant phenomenon is identified, where the system\ntransitions between insulating and metallic states under varying interaction\nstrengths. The ACPA-DMFT method provides an efficient and robust computational\nmethod for exploring the intricate interplay of disorder and strong\ncorrelations.",
        "We construct a new model structure on the category of dg presheaves over a\ntopological space $X$, obtained through the right Bousfield localization of the\nlocal projective model structure. The motivation for this construction arises\nfrom the study of the homotopy theory underlying higher Riemann-Hilbert\ncorrespondence theorems, as developed by Chuang, Holstein, and Lazarev.\n  Let $X$ be a smooth manifold. We prove the existence of a zig-zag of Quillen\nequivalences between the category of dg modules over the de Rham algebra and\nthe category of dg presheaves of vector spaces over $X$. In the case where $X$\nis a complex manifold, we obtain an analogous result, where the de Rham algebra\nis replaced by the Dolbeault algebra. In both settings, we equip the categories\nof modules with model structures of the second kind, whose homotopy categories\nare, in general, finer invariants than those given by quasi-isomorphisms.\n  Finally, we introduce a singular analogue of this equivalence, stating it as\na zig-zag of Quillen equivalences between the category of dg contramodules over\nthe singular cochain algebra $C^{*}(X)$ and dg presheaves. At the level of\nhomotopy categories, this establishes an equivalence between the contraderived\ncategory of $C^{*}(X)$-contramodules and the homotopy category of dg\npresheaves.",
        "Magnetorotational instability (MRI) is of great importance in astrophysical\ndisks, driving angular momentum transport and accretion of matter onto a\ncentral object. A Taylor-Couette (TC) flow between two coaxial cylinders\nsubject to an axial magnetic field is a preferred setup for MRI-experiments. A\nmain challenge in those experiments has been to minimize the effects of axial\nboundaries, or endcaps, which substantially alter the flow structure compared\nto the axially unbounded idealized case. Understanding the influence of endcaps\non the flow stability is crucial for the unambiguous experimental\nidentification of MRI. In this paper, we examine the hydrodynamic evolution of\na TC flow in the presence of split endcap rims up to Reynolds number $Re =$\n$2\\times 10^5$. At this $Re$, the flow deviates from the ideal TC flow profile,\nresulting in about $15\\%$ deviation in angular velocity at the mid-height of\nthe cylinders. Aside from turbulent fluctuations caused by shearing instability\nat the endcaps, the bulk flow remains axially independent and exhibits Rayleigh\nstability. We characterize the scaling of the Ekman and Stewartson boundary\nlayer thickness with respect to $Re$. We also study the effect of changing the\nrotation ratio of the cylinders $\\mu$ on the flow at large $Re$ and show that\nTC experiments can be conducted for larger $\\mu \\sim 0.5$ to safely ensure the\nhydrodynamic stability of the flow in the upcoming DRESDYN-MRI experiment. In\nall configurations considered, the modification of the flow profile by the\nendcaps further decreases the required critical threshold for the onset of MRI\nthat can facilitate its detection in future experiments."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema",
    "start_abstract":"This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Residual connections provably mitigate oversmoothing in graph neural\n  networks",
        "Artificial intelligence for objective assessment of acrobatic movements:\n  How to apply machine learning for identifying tumbling elements in cheer\n  sports",
        "Cost-Efficient Continual Learning with Sufficient Exemplar Memory",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Dynamic Basis Function Generation for Network Revenue Management",
        "Sink-free orientations: a local sampler with applications",
        "Exploring coronal abundances of M dwarfs at moderate activity levels",
        "Counterdiabatic Driving with Performance Guarantees",
        "AI Explainability for Power Electronics: From a Lipschitz Continuity\n  Perspective",
        "Nuclear Spin Induced Transparency",
        "RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on\n  Deep Learning and Big Data Technology",
        "The Role of Planetary-Scale Waves on the Stratospheric Superrotation in\n  Titan's Atmosphere",
        "An examination of the extended Hong-Ou-Mandel effect and considerations\n  for experimental detection",
        "Supercritical phase transition on the Toeplitz algebra of $\\mathbb\n  N^\\times \\ltimes \\mathbb Z$",
        "Experimental Test of Nonlocality Limits from Relativistic Independence",
        "Golden Ratio Weighting Prevents Model Collapse",
        "Comment on \"Quantum coherence between mass eigenstates of a neutrino\n  cannot be destroyed by its mass-momentum entanglement\"",
        "Quantum interference with time-frequency modes and multiple-photons\n  generated by a silicon nitride microresonator",
        "UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance",
        "Investigation of physical and electrical properties of a suboxide layer\n  at Si\/Si-hexafluoride interface",
        "SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized\n  Pipeline for Automated Extraction in the Higher Education Science Domain",
        "Revisiting Robust RAG: Do We Still Need Complex Robust Training in the\n  Era of Powerful LLMs?",
        "Neutron relative effectiveness factors in Boron Neutron Capture Therapy:\n  estimation of their values from the secondary charged particles and\n  evaluation of weighted kerma factors for a standard tissue",
        "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages",
        "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
        "SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning",
        "Two-Timescale Channel Estimation for RIS-Aided Near-Field Communications",
        "Improving Value-based Process Verifier via Structural Prior Injection",
        "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation,\n  Negative Demonstration, and Adaptive Sampling"
      ],
      "abstract":[
        "Graph neural networks (GNNs) have achieved remarkable empirical success in\nprocessing and representing graph-structured data across various domains.\nHowever, a significant challenge known as \"oversmoothing\" persists, where\nvertex features become nearly indistinguishable in deep GNNs, severely\nrestricting their expressive power and practical utility. In this work, we\nanalyze the asymptotic oversmoothing rates of deep GNNs with and without\nresidual connections by deriving explicit convergence rates for a normalized\nvertex similarity measure. Our analytical framework is grounded in the\nmultiplicative ergodic theorem. Furthermore, we demonstrate that adding\nresidual connections effectively mitigates or prevents oversmoothing across\nseveral broad families of parameter distributions. The theoretical findings are\nstrongly supported by numerical experiments.",
        "Over the past four decades, cheerleading has evolved from a sideline activity\nat major sporting events into a professional, competitive sport with growing\nglobal popularity. Evaluating tumbling elements in cheerleading relies on both\nobjective measures and subjective judgments, such as difficulty and execution\nquality. However, the complexity of tumbling - encompassing team synchronicity,\nground interactions, choreography, and artistic expression - makes objective\nassessment challenging. Artificial intelligence (AI) has revolutionized various\nscientific fields and industries through precise data-driven analyses, yet\ntheir application in acrobatic sports remains limited despite significant\npotential for enhancing performance evaluation and coaching. This study\ninvestigates the feasibility of using an AI-based approach with data from a\nsingle inertial measurement unit to accurately identify and objectively assess\ntumbling elements in standard cheerleading routines. A sample of 16\nparticipants (13 females, 3 males) from a Division I collegiate cheerleading\nteam wore a single inertial measurement unit at the dorsal pelvis. Over a\n4-week seasonal preparation period, 1102 tumbling elements were recorded during\nregular practice sessions. Using triaxial accelerations and rotational speeds,\nvarious ML algorithms were employed to classify and evaluate the execution of\ntumbling manoeuvres. Results indicate that certain machine learning models can\neffectively identify different tumbling elements despite inter-individual\nvariability and data noise, achieving high accuracy. These findings demonstrate\nthe significant potential for integrating AI-driven assessments into\ncheerleading and other acrobatic sports, providing objective metrics that\ncomplement traditional judging methods.",
        "Continual learning (CL) research typically assumes highly constrained\nexemplar memory resources. However, in many real-world scenarios-especially in\nthe era of large foundation models-memory is abundant, while GPU computational\ncosts are the primary bottleneck. In this work, we investigate CL in a novel\nsetting where exemplar memory is ample (i.e., sufficient exemplar memory).\nUnlike prior methods designed for strict exemplar memory constraints, we\npropose a simple yet effective approach that directly operates in the model's\nweight space through a combination of weight resetting and averaging\ntechniques. Our method achieves state-of-the-art performance while reducing the\ncomputational cost to a quarter or third of existing methods. These findings\nchallenge conventional CL assumptions and provide a practical baseline for\ncomputationally efficient CL applications.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "This paper introduces an algorithm that dynamically generates basis functions\nto approximate the value function in Network Revenue Management. Unlike\nexisting algorithms sampling the parameters of new basis functions, this\nNonlinear Incremental Algorithm (NLIAlg) iteratively refines the value function\napproximation by optimizing these parameters. For larger instances, the\nTwo-Phase Incremental Algorithm (2PIAlg) modifies NLIAlg to leverage the\nefficiency of LP solvers. It reduces the size of a large-dimensional nonlinear\nproblem and transforms it into an LP by fixing the basis function parameters,\nwhich are then optimized in a second phase using the flow imbalance ideas from\nAdelman and Klabjan (2012). This marks the first application of these\ntechniques in a stochastic setting. The algorithms can operate in two modes:\n(1) Standalone mode, to construct a value function approximation from scratch,\nand (2) Add-on mode, to refine an existing approximation. Our numerical\nexperiments indicate that while NLIAlg and 2PIAlg in standalone mode are only\nfeasible for small-scale problems, the heuristic version of 2PIAlg (H-2PIAlg)\nin add-on mode, using the Affine Approximation and exponential ridge basis\nfunctions, can handle extremely large instances that may cause benchmark\nnetwork revenue management methods to run out of memory. In these scenarios,\nH-2PIAlg delivers substantially better policies and upper bounds than the\nAffine Approximation. Furthermore, H-2PIAlg achieves higher average revenues in\npolicy simulations compared to network revenue management benchmarks in\ninstances with limited capacity.",
        "For sink-free orientations in graphs of minimum degree at least $3$, we show\nthat there is a deterministic approximate counting algorithm that runs in time\n$O((n^{73}\/\\varepsilon^{72})\\log(n\/\\varepsilon))$, a near-linear time sampling\nalgorithm, and a randomised approximate counting algorithm that runs in time\n$O((n\/\\varepsilon)^2\\log(n\/\\varepsilon))$, where $n$ denotes the number of\nvertices of the input graph and $0<\\varepsilon<1$ is the desired accuracy. All\nthree algorithms are based on a local implementation of the sink popping method\n(Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling\nframework (Guo, Jerrum, and Liu, 2019).",
        "Main sequence stars of spectral types F, G, and K with low to moderate\nactivity levels exhibit a recognizable pattern known as the first ionization\npotential effect (FIP effect), where elements with lower first ionization\npotentials are more abundant in the stellar corona than in the photosphere. In\ncontrast, high activity main sequence stars such as AB Dor (K0), active\nbinaries, and M dwarfs exhibit an inverse pattern known as iFIP. We aim to\ndetermine whether or not the iFIP pattern persists in moderate-activity M\ndwarfs. We used XMM-Newton to observe the moderately active M dwarf HD 223889\nthat has an X-ray surface flux of log FX,surf = 5.26, the lowest for an M dwarf\nstudied so far for coronal abundance patterns. We used low-resolution CCD\nspectra of the star to calculate the strength of the FIP effect quantified by\nthe FIP bias (Fbias) to assess the persistence of the iFIP effect in M dwarfs.\nOur findings reveal an iFIP effect similar to that of another moderately active\nbinary star, GJ 338 AB, with a comparable error margin. The results hint at a\npossible plateau in the Teff-Fbias diagram for moderately active M dwarfs.\nTargeting stars with low coronal activity that have a coronal temperature\nbetween 2 MK and 4 MK is essential for refining our understanding of (i)FIP\npatterns and their causes.",
        "Counterdiabatic (CD) driving has the potential to speed up adiabatic quantum\nstate preparation by suppressing unwanted excitations. However, existing\napproaches either require intractable classical computations or are based on\napproximations which do not have performance guarantees. We propose and analyze\na non-variational, system-agnostic CD expansion method and analytically show\nthat it converges exponentially quickly in the expansion order. In finite\nsystems, the required resources scale inversely with the spectral gap, which we\nargue is asymptotically optimal. To extend our method to the thermodynamic\nlimit and suppress errors stemming from high-frequency transitions, we leverage\nfinite-time adiabatic protocols. In particular, we show that a time determined\nby the quantum speed limit is sufficient to prepare the desired ground state,\nwithout the need to optimize the adiabatic trajectory. Numerical tests of our\nmethod on the quantum Ising chain show that our method can outperform\nstate-of-the-art variational CD approaches.",
        "Lifecycle management of power converters continues to thrive with emerging\nartificial intelligence (AI) solutions, yet AI mathematical explainability\nremains unexplored in power electronics (PE) community. The lack of theoretical\nrigor challenges adoption in mission-critical applications. Therefore, this\nletter proposes a generic framework to evaluate mathematical explainability,\nhighlighting inference stability and training convergence from a Lipschitz\ncontinuity perspective. Inference stability governs consistent outputs under\ninput perturbations, essential for robust real-time control and fault\ndiagnosis. Training convergence guarantees stable learning dynamics,\nfacilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware\nlearning rate selection strategy is introduced to accelerate convergence while\nmitigating overshoots and oscillations. The feasibility of the proposed\nLipschitz-oriented framework is demonstrated by validating the mathematical\nexplainability of a state-of-the-art physics-in-architecture neural network,\nand substantiated through empirical case studies on dual-active-bridge\nconverters. This letter serves as a clarion call for the PE community to\nembrace mathematical explainability, heralding a transformative era of\ntrustworthy and explainable AI solutions that potentially redefine the future\nof power electronics.",
        "Electromagnetically induced transparency (EIT) is an important quantum\noptical phenomenon which provides a crucial tool for light manipulation.\nHowever, typically the transparency window is broad, limited by the coherence\ntime of the metastable state. Here we show that extremely narrow transparency\nwindow can be realized using nuclear spin induced transparency (NSIT), which is\nachieved by combining optical field, magnetic field and the spin-exchange\ninteraction between noble-gas nuclear spins and alkali-metal electronic spins.\nThe width of the NSIT window can be several orders of magnitude smaller than\nthat of conventional EIT, and even reaches sub-mHz range due to the long\ncoherence time of nuclear spins. The scheme holds great potential for\napplications in slow light and magnetic field sensing.",
        "Multi-object tracking (MOT) in UAV-based video is challenging due to\nvariations in viewpoint, low resolution, and the presence of small objects.\nWhile other research on MOT dedicated to aerial videos primarily focuses on the\nacademic aspect by developing sophisticated algorithms, there is a lack of\nattention to the practical aspect of these systems. In this paper, we propose a\nnovel real-time MOT framework that integrates Apache Kafka and Apache Spark for\nefficient and fault-tolerant video stream processing, along with\nstate-of-the-art deep learning models YOLOv8\/YOLOv10 and BYTETRACK\/BoTSORT for\naccurate object detection and tracking. Our work highlights the importance of\nnot only the advanced algorithms but also the integration of these methods with\nscalable and distributed systems. By leveraging these technologies, our system\nachieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set\nwhile maintaining a real-time processing speed of 28 FPS on a single GPU. Our\nwork demonstrates the potential of big data technologies and deep learning for\naddressing the challenges of MOT in UAV applications.",
        "We analyze simulation results from the TitanWRF global circulation model to\nunderstand the mechanisms that maintain the equatorial superrotation in Titan's\nstratosphere. We find that the eddies associated with wave activities can\ntransport angular momentum upgradient to zonal flow, leading to acceleration of\nthe equatorial superrotation. The dominant wave modes identified in this study\nare consistent with previous studies, with zonal wavenumber 1 being the major\ncontributor to the prograde acceleration. Despite the same conclusion of\nmaintenance of equatorial superrotation via wave-mean interactions, we find\nthat the way waves interact with the zonal flow in TitanWRF is slightly\ndifferent from some other studies. We confirm our previous findings that in\nTitanWRF this occurs primarily during a dozen or so annual, short-duration (a\nfew Titan sols) angular momentum \"transfer events,\" which have a repeatable\nseasonal pattern but differ slightly in timing and magnitude between years.\nThis is not the case in the Titan Atmosphere Model (TAM), which found milder\nangular momentum transfers that produced the strongest acceleration of\nsuperrotation around solstice in the upper stratosphere and more continuous\nyear-around acceleration in the lower stratosphere. Despite differences in\nangular momentum transfer across models, we further find that, similar to the\nTAM wave analysis results, eddies generated by Rossby-Kelvin instabilities may\nbe the major source of prograde angular momentum for the equatorial\nsuperrotation, although TitanWRF may also include contributions from the\nabsorption of vertically propagating equatorial Kelvin waves. This differs from\nour previous work, which suggested barotropic waves were responsible for\nTitanWRF's solsticial transfer event.",
        "In recent works we have explored a multi-photon extension of the celebrated\ntwo-photon Hong-Ou-Mandel (HOM) effect in which the quantum amplitudes for a\ntwo-photon input to a lossless, balanced 50:50 beamsplitter (BS) undergoes\ncomplete destructive interference. In the extended Hong-Ou-Mandel (eHOM) effect\nthe multi-photon scattering of photons from the two input ports to the two\noutput ports of the BS for Fock number basis input states (FS)\n$|n,m\\rangle_{12}$ exhibit complete destructive interference pairwise within\nthe quantum amplitudes containing many scattering components, generalizing the\ntwo-photon HOM effect. This has profound implications for arbitrary bipartite\nphotonic input states constructed from such basis states: if the input state to\none input port of the BS is of odd parity, i.e. constructed from only of odd\nnumbers of photons, then regardless of the input state to the second 50:50 BS\nport, there will be a central nodal line (CNL) of zeros in the joint output\nprobability distribution along the main diagonal for coincidence detection. The\nfirst goal of this present work is to show diagrammatically how the extended\nHOM effect can be seen as a succession of multi-photon HOM effects when the\nlatter is viewed as a pairwise cancellation of mirror image scattering\namplitudes. The second goal of this work is to explore considerations for the\nexperimental realization of the extended Hong-Ou-Mandel effect. We examine the\ncase of a single photon interfering with a coherent state (an idealized laser)\non a balanced 50:50 beamsplitter and consider prospects for experimental\ndetection of the output destructive interference by including additional\neffects such as imperfect detection efficiency, spatio-temporal mode functions,\nand time delay between the detected output photons.",
        "We study the high-temperature equilibrium for the C*-algebra $\\mathcal\nT(\\mathbb N^\\times \\ltimes \\mathbb Z)$ recently considered by an Huef, Laca and\nRaeburn. We show that the simplex of KMS$_\\beta$ states at each inverse\ntemperature $\\beta$ in the critical interval $(0,1]$ is a Bauer simplex whose\nspace of extreme points is homeomorphic to $\\mathbb N \\sqcup\\{\\infty\\}$. This\nis in contrast to the uniqueness of equilibrium at high temperature observed in\npreviously considered systems arising from number theory. We also establish a\nconnection between the phase transitions on quotients of our system and the\nBost-Connes phase transition.",
        "Quantum correlations, like entanglement, represent the characteristic trait\nof quantum mechanics, and pose essential issues and challenges to the\ninterpretation of this pillar of modern physics. Although quantum correlations\nare largely acknowledged as a major resource to achieve quantum advantage in\nmany tasks of quantum technologies, their full quantitative description and the\naxiomatic basis underlying them are still under investigation. Previous works\nsuggested that the origin of nonlocal correlations is grounded in principles\ncapturing (from outside the quantum formalism) the essence of quantum\nuncertainty. In particular, the recently-introduced principle of Relativistic\nIndependence gave rise to a new bound intertwining local and nonlocal\ncorrelations. Here we test such a bound by realizing together sequential and\njoint weak measurements on entangled photon pairs, allowing to simultaneously\nquantify both local and nonlocal correlations by measuring incompatible\nobservables on the same quantum system without collapsing its state, a task\ntypically forbidden in the traditional (projective) quantum measurement\nframework. Our results demonstrate the existence of a fundamental limit on the\nextent of quantum correlations, shedding light on the profound role of\nuncertainty in both enabling and balancing them.",
        "Recent studies identified an intriguing phenomenon in recursive generative\nmodel training known as model collapse, where models trained on data generated\nby previous models exhibit severe performance degradation. Addressing this\nissue and developing more effective training strategies have become central\nchallenges in generative model research. In this paper, we investigate this\nphenomenon theoretically within a novel framework, where generative models are\niteratively trained on a combination of newly collected real data and synthetic\ndata from the previous training step. To develop an optimal training strategy\nfor integrating real and synthetic data, we evaluate the performance of a\nweighted training scheme in various scenarios, including Gaussian distribution\nestimation and linear regression. We theoretically characterize the impact of\nthe mixing proportion and weighting scheme of synthetic data on the final\nmodel's performance. Our key finding is that, across different settings, the\noptimal weighting scheme under different proportions of synthetic data\nasymptotically follows a unified expression, revealing a fundamental trade-off\nbetween leveraging synthetic data and generative model performance. Notably, in\nsome cases, the optimal weight assigned to real data corresponds to the\nreciprocal of the golden ratio. Finally, we validate our theoretical results on\nextensive simulated datasets and a real tabular dataset.",
        "In arXiv:2410.21850, I proved that the quantum coherence between the mass\neigenstates of a neutrino will be destroyed if they are correlated with\ndifferent momenta. In arXiv:2411.01190, James M. Cline claimed that I had made\nthe unrealistic assumption that the neutrino is always in a nearly exact energy\neigenstate, and ignored the spatial dependence of the wavefunction in my paper.\nHowever, I did not assume that the neutrino is in a nearly exact eigenstate of\nenergy anywhere in my paper, and the wavefunction I wrote in the position\nrepresentation has a spatial dependence. The argumentation of arXiv:2411.01190\nis based on misinterpreting my claim, and on ignoring the critical fact that\nthe neutrino's wavepacket has a finite size and the detector has a large\nvolume.",
        "We demonstrate bipartite gaussian boson sampling with squeezed light in 6\nmixed time-frequency modes. Non-degenerate two-mode squeezing is generated in\ntwo time-bins from a silicon nitride microresonator with simultaneous high\nspectral purity (>0.86(3)) and indistinguishability (0.985(2)). An unbalanced\ninterferometer embedding electro-optic modulators, which is stabilized by\nexploiting the continuous energy-time entanglement of the generated photon\npairs, controls time and frequency-bin modes. We measure 144 collision-free\nevents with 4 photons at the output, achieving a fidelity >0.98 with the\ntheoretical probability distribution. We use this result to identify the\nsimilarity between families of isomorphic graphs with 6 vertices, and present\nan approach for the realization of universal operations on time-frequency\nmodes.",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet code generation remains a major challenge. Current\napproaches for obtaining high-quality code data primarily focus on (i)\ncollecting large-scale pre-training data and (ii) synthesizing instruction data\nthrough prompt engineering with powerful models. While pre-training data faces\nquality consistency issues, instruction-based synthesis suffers from limited\ninstruction diversity and inherent biases of LLMs. To address this gap, we\nintroduce UnitCoder, a systematic pipeline leveraging model-generated unit\ntests to both guide and validate the code generation process. Combined with\nlarge-scale package-based retrieval from pre-training corpus, we generate a\ndataset of 500K+ verifiable programs containing diverse API calls. Evaluations\non multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that\nmodels fine-tuned on our synthetic data exhibit consistent performance\nimprovements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and\n28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work\npresents a scalable approach that leverages model-generated unit tests to guide\nthe synthesis of high-quality code data from pre-training corpora,\ndemonstrating the potential for producing diverse and high-quality\npost-training data at scale. All code and data will be released\n(https:\/\/github.com).",
        "The silicon suboxide SiOx (x<2.0) offers promising industrial application\npossibilities ranging from electrodes in lithium-ion batteries, which are used\nwidely in electrical vehicles and portable devices to sensing applications.\nTherefore, a low cost, environmental friendly and high performance silicon\noxide materials are required for an appropriate operation of any electronic\ngadget. In this work, we report on the physical and electrical properties of a\nsuboxide layer of up to 1 {\\mu}m, which was grown on silicon during the\nformation of a dielectric layer, namely the ammonium silicon hexafluoride. It\nis a stable oxide exhibiting light emission from 400 nm to 1700 nm offering\nscalable and cost-effective large area processing capability. The measurement\nresults reveal interesting properties, which are required to be understood\nclearly before proceeding with any suitable application. The results have been\nanalyzed using state-of-the-art physical and electrical characterization\ntechniques such as ellipsometry, AFM, SEM, FTIR, photoluminescence lifetime and\nresistive switching measurements to determine structural, optical and\nelectrical properties. At 300 K the carrier lifetime measurements reveal the\nlifetime values ranging from about few tens of picosecond up to 4500\npicoseconds. Scanning probe analysis indicate a surface roughness of about 30\nAngstr\\\"om. Resistive memory forming was observed also in these layers at\nrelatively low power thresholds. We provide a comprehensive description of the\nphysical and electrical properties in order to clarify the origin of the\nobserved features. The wavelength dependent real {\\epsilon}_1 ({\\omega}) and\nthe imaginary {\\epsilon}_2 ({\\omega}) dielectric functions provided useful\ninsights on optical properties. A lookout is given for the possible\napplications of this special SiOx dielectric oxide layer.",
        "Recent breakthroughs in large language models (LLMs) exemplified by the\nimpressive mathematical and scientific reasoning capabilities of the o1 model\nhave spotlighted the critical importance of high-quality training data in\nadvancing LLM performance across STEM disciplines. While the mathematics\ncommunity has benefited from a growing body of curated datasets, the scientific\ndomain at the higher education level has long suffered from a scarcity of\ncomparable resources. To address this gap, we present SCP-116K, a new\nlarge-scale dataset of 116,756 high-quality problem-solution pairs,\nautomatically extracted from heterogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach involves stringent filtering to\nensure the scientific rigor and educational level of the extracted materials,\nwhile maintaining adaptability for future expansions or domain transfers. By\nopenly releasing both the dataset and the extraction pipeline, we seek to\nfoster research on scientific reasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier to replicating the successes of\nadvanced models like o1 in the broader science community. We believe SCP-116K\nwill serve as a critical resource, catalyzing progress in high-level scientific\nreasoning tasks and promoting further innovations in LLM development. The\ndataset and code are publicly available at\nhttps:\/\/github.com\/AQA6666\/SCP-116K-open.",
        "Retrieval-augmented generation (RAG) systems often suffer from performance\ndegradation when encountering noisy or irrelevant documents, driving\nresearchers to develop sophisticated training strategies to enhance their\nrobustness against such retrieval noise. However, as large language models\n(LLMs) continue to advance, the necessity of these complex training methods is\nincreasingly questioned. In this paper, we systematically investigate whether\ncomplex robust training strategies remain necessary as model capacity grows.\nThrough comprehensive experiments spanning multiple model architectures and\nparameter scales, we evaluate various document selection methods and\nadversarial training techniques across diverse datasets. Our extensive\nexperiments consistently demonstrate that as models become more powerful, the\nperformance gains brought by complex robust training methods drop off\ndramatically. We delve into the rationale and find that more powerful models\ninherently exhibit superior confidence calibration, better generalization\nacross datasets (even when trained with randomly selected documents), and\noptimal attention mechanisms learned with simpler strategies. Our findings\nsuggest that RAG systems can benefit from simpler architectures and training\nstrategies as models become more powerful, enabling more scalable applications\nwith minimal complexity.",
        "The average Relative Biological effectiveness (RBE) factors for neutron\nirradiation in the context of a BNCT treatment are studied. This research\nconsiders the various interactions and secondary particles of each process and\nestimates the RBE based on the damage induced in tissues by all of these\nparticles. A novel concept of estimating the biological dose by means of\nweighted kerma factors is introduced. These weighted kerma factors include the\nRBE of each energy deposition based on an RBE-LET relationship for secondary\ncharged particles and can be directly incorporated in weighted dose\ncalculations from Monte Carlo simulations. Furthermore, the dependence of the\nneutron weighting factor on neutron energy for standard soft tissue is\ndiscussed.",
        "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps:\/\/github.com\/AfriHate\/AfriHate",
        "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps:\/\/quanhaol.github.io\/magicmotion-site.",
        "Federated learning (FL) enables collaborative model training while preserving\ndata privacy, but its decentralized nature exposes it to client-side data\npoisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global\nmodel performance. While numerous proposed defenses claim substantial\neffectiveness, their evaluation is typically done in isolation with limited\nattack strategies, raising concerns about their validity. Additionally,\nexisting studies overlook the mutual effectiveness of defenses against both\nDPAs and MPAs, causing fragmentation in this field. This paper aims to provide\na unified benchmark and analysis of defenses against DPAs and MPAs, clarifying\nthe distinction between these two similar but slightly distinct domains. We\npresent a systematic taxonomy of poisoning attacks and defense strategies,\noutlining their design, strengths, and limitations. Then, a unified comparative\nevaluation across FL algorithms and data heterogeneity is conducted to validate\ntheir individual and mutual effectiveness and derive key insights for design\nprinciples and future research. Along with the analysis, we frame our work to a\nunified benchmark, FLPoison, with high modularity and scalability to evaluate\n15 representative poisoning attacks and 17 defense strategies, facilitating\nfuture research in this domain. Code is available at\nhttps:\/\/github.com\/vio1etus\/FLPoison.",
        "In this paper, we investigate the channel estimation problem in\nreconfigurable intelligent surface (RIS)-aided near-field communication\nsystems, where the extremely large number of RIS elements imposes considerable\npilot overhead and computational complexity. To address this, we employ a\ntwo-timescale channel estimation strategy that exploits the asymmetric\ncoherence times of the RIS-base station (BS) channel and the User-RIS channel.\nWe derive a time-scaling property indicating that for any two effective\nchannels within the longer coherence time, one effective channel can be\nrepresented as the product of a vector, referred to as the small-timescale\neffective channel, and the other effective channel. By utilizing the estimated\neffective channel along with processed observations from our piecewise beam\ntraining, we present an efficient method for estimating subsequent\nsmall-timescale effective channels. We theoretically establish the efficacy of\nthe proposed RIS design and demonstrate, through simulations, the superiority\nof our channel estimation method in terms of pilot overhead and computational\ncomplexity compared to existing methods across various realistic channel\nmodels.",
        "In the Large Language Model(LLM) reasoning scenario, people often estimate\nstate value via Monte Carlo sampling. Though Monte Carlo estimation is an\nelegant method with less inductive bias, noise and errors are inevitably\nintroduced due to the limited sampling. To handle the problem, we inject the\nstructural prior into the value representation and transfer the scalar value\ninto the expectation of a pre-defined categorical distribution, representing\nthe noise and errors from a distribution perspective. Specifically, by treating\nthe result of Monte Carlo sampling as a single sample from the prior\nground-truth Binomial distribution, we quantify the sampling error as the\nmismatch between posterior estimated distribution and ground-truth\ndistribution, which is thus optimized via distribution selection optimization.\nWe test the performance of value-based process verifiers on Best-of-N task and\nBeam search task. Compared with the scalar value representation, we show that\nreasonable structural prior injection induced by different objective functions\nor optimization methods can improve the performance of value-based process\nverifiers for about 1$\\sim$2 points at little-to-no cost. We also show that\nunder different structural prior, the verifiers' performances vary greatly\ndespite having the same optimal solution, indicating the importance of\nreasonable structural prior injection.",
        "Many-shot jailbreaking circumvents the safety alignment of large language\nmodels by exploiting their ability to process long input sequences. To achieve\nthis, the malicious target prompt is prefixed with hundreds of fabricated\nconversational turns between the user and the model. These fabricated exchanges\nare randomly sampled from a pool of malicious questions and responses, making\nit appear as though the model has already complied with harmful instructions.\nIn this paper, we present PANDAS: a hybrid technique that improves many-shot\njailbreaking by modifying these fabricated dialogues with positive\naffirmations, negative demonstrations, and an optimized adaptive sampling\nmethod tailored to the target prompt's topic. Extensive experiments on AdvBench\nand HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS\nsignificantly outperforms baseline methods in long-context scenarios. Through\nan attention analysis, we provide insights on how long-context vulnerabilities\nare exploited and show how PANDAS further improves upon many-shot jailbreaking."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
      ],
      "abstract":[
        "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Divisibility Relations Between Ring Homomorphisms and Surjective Group\n  Homomorphisms in Finite Cyclic Structures",
        "Moduli spaces of twisted maps to smooth pairs",
        "Parallel Collisionless Shocks in strongly Magnetized Electron-Ion\n  Plasma. I. Temperature anisotropies",
        "Probing Coherences and Itinerant Magnetism in a Dipolar Lattice Gas",
        "$NJ\/\\psi$ and $N\\eta_c$ interactions from lattice QCD",
        "The Ridge Integration Method and its Application to Molecular Sieving,\n  Demonstrated for Gas Purification via Graphdiyne Membranes",
        "Overcoming Quantum Metrology Singularity through Sequential Measurements",
        "Late-time growth weakly affects the significance of high-redshift\n  massive galaxies",
        "Weierstrass representations of discrete constant mean curvature surfaces\n  in isotropic space",
        "Online Optimization with Unknown Time-varying Parameters",
        "Exactness and the topology of the space of invariant random equivalence\n  relations",
        "Cusps and fundamental domains for congruence subgroups",
        "$H^\\infty$-control for a class of boundary controlled hyperbolic PDEs",
        "Accelerating Expansion of the Universe in Modified Symmetric\n  Teleparallel Gravity",
        "Neutron versus proton scattering on exotic nuclei: the $^9$He example",
        "Higgs near-criticality at future colliders",
        "A new mass estimate method with hydrodynamical atmospheres for very\n  massive WNh stars",
        "Recent Topics on Linear Dynamics",
        "Theory of Two-level Tunneling Systems in Superconductors",
        "Reinforced Galton--Watson processes III: Empirical offspring\n  distributions",
        "On Dirac and Motzkin problem in discrete geometry",
        "On Malliavin differentiability and absolute continuity of\n  one-dimensional doubly perturbed diffusion processes",
        "Online Assortment and Price Optimization Under Contextual Choice Models",
        "Empirical risk minimization algorithm for multiclass classification of\n  S.D.E. paths",
        "Bosonisation and BTZ Black Hole Microstates",
        "Response of liquid metal in a fusion reactor blanket to rapid variation\n  of magnetic field during a transient plasma event",
        "A practical review on promoting connectivity in topology optimization",
        "Learning Energy-Based Models by Self-normalising the Likelihood",
        "From Weyl Anomaly to Defect Supersymmetric R\\'enyi Entropy and Casimir\n  Energy"
      ],
      "abstract":[
        "In this article, we delve into the intricate relationship between the number\nof ring homomorphisms and surjective group homomorphisms between two finite\ncyclic structures, specifically $\\mathbb{Z}_m$ and $\\mathbb{Z}_n$. We\ndemonstrate that the number of ring homomorphisms from $\\mathbb{Z}_m$ to\n$\\mathbb{Z}_n$ is a divisor of the number of surjective group homomorphisms\nfrom $\\mathbb{Z}_m$ to $\\mathbb{Z}_n$, provided that $n$ is not of the form $2\n\\cdot \\alpha$, where each prime factor $p$ of $\\alpha$ satisfies $p \\equiv 3\n\\pmod{4}$.",
        "We study moduli spaces of twisted maps to a smooth pair in arbitrary genus,\nand give geometric explanations for previously known comparisons between\norbifold and logarithmic Gromov--Witten invariants. Namely, we study the space\nof twisted maps to the universal target and classify its irreducible components\nin terms of combinatorial\/tropical information. We also introduce natural\nmorphisms between these moduli spaces for different rooting parameters and\ncompute their degree on various strata. Combining this with additional\nhypotheses on the discrete data, we show these degrees are monomial of degree\nbetween $0$ and $\\max(0,2g-1)$ in the rooting parameter. We discuss the virtual\ntheory of the moduli spaces, and relate our polynomiality results to work of\nTseng and You on the higher genus orbifold Gromov--Witten invariants of smooth\npairs, recovering their results in genus $1$. We discuss what is needed to\ndeduce arbitrary genus comparison results using the previous sections. We\nconclude with some geometric examples, starting by re-framing the original\ngenus $1$ example of Maulik in this new formalism.",
        "Collisionless electron-ion shocks are fundamental to astrophysical plasmas,\nyet their behavior in strong magnetic fields remains poorly understood. Using\nParticle-in-Cell (PIC) simulations with the SHARP-1D3V code, we investigate the\nrole of the ion magnetization parameter $\\sigma_i$ in parallel shock\ntransitions. Strongly magnetized converging flows ($\\sigma_i > 1$) exhibit\nlower density compression ratios ($R \\sim 2$), smaller entropy jumps, and\nsuppressed particle acceleration, while maintaining pressure anisotropy\nstability due to conserved perpendicular temperatures across the shock,\nalongside increased parallel temperatures. In contrast, weakly magnetized\nshocks drive downstream mirror and firehose instabilities due to ion\ntemperature anisotropy, which are suppressed in strongly magnetized cases.\nAdditionally, weakly magnetized shocks exhibit the onset of a supra-thermal\npopulation induced by shock-drift acceleration, with most of the upstream\nkinetic energy thermalized for both electrons and ions in the downstream\nregion. Our results demonstrate that perpendicular temperatures for both\nspecies are conserved in strongly magnetized cases and highlight deviations\nfrom standard ideal magnetohydrodynamic (MHD) behavior. These findings provide\ncritical insights into the role of magnetic fields in parallel collisionless\nastrophysical shocks.",
        "We report on the study of itinerant magnetism of lattice-trapped magnetic\natoms, driven by magnetic dipole-dipole interactions, in the low-entropy and\nclose-to-unit filling regime. We have used advanced dynamical decoupling\ntechniques to efficiently suppress the sensitivity to magnetic field\nfluctuations. We have thus measured the spin coherence of an itinerant spin 3\nBose dipolar gas throughout a quantum phase transition from a superfluid phase\nto a Mott insulating phase. In the superfluid phase, a metastable ferromagnetic\nbehavior is observed below a dynamical instability which occurs at lattice\ndepths below the phase transition. In the insulating phase, the thermalization\ntowards a paramagnetic state is driven by an interplay between intersite and\nsuperexchange interactions.",
        "The interaction between nucleon and charmonia ($J\/\\psi$ and $\\eta_c$) is\nexpected to deepen our understanding of various aspects in nonperturbative QCD\nranging from the origin of nucleon mass to $J\/\\psi$ mass modification in\nnuclear medium and properties of hidden-charm pentaquark states. Here, we\npresent the low-energy $NJ\/\\psi$ and $N\\eta_c$ interactions based on ($2+1$)\nflavor lattice QCD simulations with nearly physical pion mass $m_\\pi=146$ MeV.\nThe interactions, extracted from the spacetime correlations of the nucleon and\ncharmonium system by using the HAL QCD method, are found to be attractive in\nall distances and manifest a characteristic long-range tail consistent with the\ntwo-pion exchange interaction. The resulting scattering lengths are around\n$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ\/\\psi$ with spin $3\/2$, with spin $1\/2$,\nand $N\\eta_c$, respectively. Our results are orders of magnitude larger than\nthose from the photoproduction experiments assuming the vector meson dominance.",
        "Eyring theory provides a convenient approximation to the rate of a chemical\nreaction, as it uses only local information evaluated near extremal points of a\ngiven potential energy surface. However, in cases of pronounced anharmonicity\nand particularly low-lying vibrational frequencies, deviations from the correct\nreaction rate can become substantial. Molecular Dynamics simulations, on the\nother hand, are very costly at higher levels of theory, and of limited use\nsince molecular reactions are `rare' events and hence statistically less\naccessible. In this article, we present an alternative description for problems\nof gas separation and storage via two-dimensional materials such as porous\ngraphene or flat metal-organic frameworks. Taking geometric advantage of the\ntypical problem setting, our method is based on a statistical analysis of\nmolecular trajectories near the so-called `ridge', a hypersurface which divides\nthe reaction volume into a reactant and a product side. It allows for more\nrealistic predictions of permeabilities and selectivities, e.g. derived from\ndensity functional theory, but without the considerable costs of a full\nmolecular dynamics simulation on the corresponding Born-Oppenheimer potential\nenergy surface. We test our method on the example of methane separation from\nnitrogen and carbon dioxide via a graphdiyne membrane.",
        "The simultaneous estimation of multiple unknown parameters is the most\ngeneral scenario in quantum sensing. Quantum multi-parameter estimation theory\nprovides fundamental bounds on the achievable precision of simultaneous\nestimation. However, these bounds can become singular (no finite bound exists)\nin multi-parameter sensing due to parameter interdependencies, limited probe\naccessibility, and insufficient measurement outcomes. Here, we address the\nsingularity issue in quantum sensing through a simple mechanism based on a\nsequential measurement strategy. This sensing scheme overcomes the singularity\nconstraint and enables the simultaneous estimation of multiple parameters with\na local and fixed measurement throughout the sensing protocol. This is because\nsequential measurements, involving consecutive steps of local measurements\nfollowed by probe evolution, inherently produce correlated measurement data\nthat grows exponentially with the number of sequential measurements. Finally,\nthrough two different examples, namely a strongly correlated probe and a\nlight-matter system, we demonstrate how such singularities are reflected when\ninferring the unknown parameters through Bayesian estimation.",
        "Recent observations by the James Webb Space Telescope have revealed massive\ngalaxies at very high redshift ($z\\simeq 7-15$). The question of whether the\nexistence of such galaxies is expected in the corresponding JWST surveys has\nreceived a lot of attention, though the answer straddles areas of cosmology and\ncomplex astrophysical details of high-redshift galaxy formation. The growth\nrate of density fluctuations determines the amplitude of overdensities that\ncollapse to form galaxies. Late-time modifications of growth, combined with\nmeasurements at both $z\\sim 1$ from large-scale structure and $z\\sim 1000$ from\nthe cosmic microwave background, affect the predictions for the abundance of\nfirst galaxies in the universe. In this paper, we point out that the late-time\ngrowth rate of structure affects the statistical significance of high-redshift,\nhigh-mass objects very weakly. Consequently, if the existence and abundance of\nthese objects are confirmed to be unexpected, the variations in the late-time\ngrowth history are unlikely to explain these anomalies.",
        "In this paper, we obtain Weierstrass representations for discrete constant\nmean curvature surfaces in isotropic 3-space, and use this to construct\nexamples with discrete closed-form parametrizations.",
        "In this paper, we study optimization problems where the cost function\ncontains time-varying parameters that are unmeasurable and evolve according to\nlinear, yet unknown, dynamics. We propose a solution that leverages control\ntheoretic tools to identify the dynamics of the parameters, predict their\nevolution, and ultimately compute a solution to the optimization problem. The\nidentification of the dynamics of the time-varying parameters is done online\nusing measurements of the gradient of the cost function. This system\nidentification problem is not standard, since the output matrix is known and\nthe dynamics of the parameters must be estimated in the original coordinates\nwithout similarity transformations. Interestingly, our analysis shows that,\nunder mild conditions that we characterize, the identification of the\nparameters dynamics and, consequently, the computation of a time-varying\nsolution to the optimization problem, requires only a finite number of\nmeasurements of the gradient of the cost function. We illustrate the\neffectiveness of our algorithm on a series of numerical examples.",
        "We characterize exactness of a countable group $\\Gamma$ in terms of invariant\nrandom equivalence relations (IREs) on $\\Gamma$. Specifically, we show that\n$\\Gamma$ is exact if and only if every weak limit of finite IREs is an amenable\nIRE. In particular, for exact groups this implies amenability of the restricted\nrerooting relation associated to the ideal Bernoulli Voronoi tessellation, the\ndiscrete analog of the ideal Poisson Voronoi tesselation.",
        "We characterize the cusp classes and their widths for the congruence\nsubgroups $\\Gamma(N), \\Gamma_1(N)$ and $\\Gamma_0(N)$. We relate the cusp\nclasses of $\\Gamma_0(N)$ with those produced by the connected fundamental\ndomain in the previous work of Nie and Parent. By further studying the\ninteresting functions $M$ and $W$ on ${\\mathbb Z}\/N$, we establish an identity\nrelating the widths.",
        "A solution to the suboptimal $H^\\infty$-control problem is given for a class\nof hyperbolic partial differential equations (PDEs). The first result of this\nmanuscript shows that the considered class of PDEs admits an equivalent\nrepresentation as an infinite-dimensional discrete-time system. Taking\nadvantage of this, this manuscript shows that it is equivalent to solve the\nsuboptimal $H^\\infty$-control problem for a finite-dimensional discrete-time\nsystem whose matrices are derived from the PDEs. After computing the solution\nto this much simpler problem, the solution to the original problem can be\ndeduced easily. In particular, the optimal compensator solution to the\nsuboptimal $H^\\infty$-control problem is governed by a set of hyperbolic PDEs,\nactuated and observed at the boundary. We illustrate our results with a\nboundary controlled and boundary observed vibrating string.",
        "In the last century, theoretical and experimental developments have\nestablished the General Relativity theory as the most successful theory for\ndescribing the gravitational phenomenon. On the other hand, in the last two\ndecades, multiple observational probes have strongly favored the discovery of\nthe acceleration of cosmic expansion. The observational enhancement and\ndevelopment in precision cosmology indicate a requirement to go beyond General\nRelativity and to search for an alternate description that can resolve the\npersistent issues. In Chapter 1, we highlight some important elements of\nobservational cosmology. In Chapters 2 and 3, we investigate the f(Q) gravity\nin the presence of viscosity in the cosmic fluid. In Chapters 4 and 5, we\nexplore the constraints on the various classes of non-linear f(Q) gravity\nmodels in both coincident and non-coincident formalism, respectively. In\nChapter 6, we present a covariant formulation and energy balance equation for\nthe f(Q,T) gravity, which is an extension of f(Q) gravity. Finally, in Chapter\n7, we briefly summarize the outcomes of the present thesis and the future\nscope.",
        "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5\/2$ and $T=3\/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5\/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5\/2$ resonances may be strongly affected by the\nisobaric-partner $T=3\/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated.",
        "The so-called metastability bound on the Higgs mass suggests that the\nsmallness of the Higgs mass may be a byproduct of the metastability of the\nelectroweak vacuum. A significantly strong bound requires new physics capable\nof lowering the scale where the Higgs quartic coupling turns negative through\nrenormalization group effects, without destabilizing the electroweak vacuum\nentirely. We analyze in this context the low-scale Majoron model of neutrino\nmasses, which automatically contains two key elements for a viable scenario:\nheavy fermions to lower the instability scale and a extended scalar sector to\nstabilize the potential and achieve realistic lifetimes for the electroweak\nvacuum. We show how the metastability bound can be generalized to theories with\nmultiple scalars and present an efficient way of calculating the tunneling rate\nin such potentials. We also demonstrate that FCC will probe regions of the\nparameter space relevant for metastability: large regions of the fermionic\nsector at FCC-ee and some reach to the scalar sector at FCC-hh.",
        "Very massive stars with masses over 100 Msun are key objects in the Universe\nfor our understanding of chemical and energetic feedback in the Universe, but\ntheir evolution and fate are almost entirely determined by their wind mass\nloss. We aim to determine the mass of the most massive star known in the Local\nGroup R136a1. For this we compute the first hydrodynamically consistent\nnon-local thermodynamical equilibrium atmosphere models for both R136a1 (WN5h)\nas well as the binary system R144 (WN5\/6h+WN6\/7h) in the Tarantula nebula.\nUsing the Potsdam Wolf-Rayet code, we simultaneously empirically derive and\ntheoretically predict mass-loss rates and wind velocities. By fitting synthetic\nspectra derived from these models to multi-wavelength observations, we\nconstrain the stellar and wind properties of R144 and R136a1. We first\ndetermine the clumping stratification required by our hydro-models to fit the\nspectra of R144 by using the available dynamical mass estimates for the two\ncomponents. We then utilise this clumping stratification in hydrodynamic models\nof R136a1 and estimate a mass of $M_\\mathrm{Hydro}$ of 233 Msun. Remarkably,\nthe estimated mass is close to and entirely consistent with chemical\nhomogeneous mass relations. This present-day mass of 233 Msun provides a lower\nlimit to the initial stellar mass, that could be far higher due to previous\nwind mass loss.",
        "Notes from a course on linear dynamics given by the author at the University\nof Da Nang in January 2024.",
        "We develop a field theory formulation for the interaction of an ensemble of\ntwo-level tunneling systems (TLS) with the electronic states of a\nsuperconductor. Predictions for the impact of two-level tunneling systems on\nsuperconductivity are presented, including $T_c$ and spectrum of quasiparticle\nstates for conventional BCS superconductors. We show that non-magnetic TLS\nimpurities in conventional s-wave superconductors can act as pair-breaking or\npair-enhancing defects depending on the level population of the distribution of\nTLS impurities. We present calculations of the enhancement of\nsuperconductivity, both $T_c$ and the order parameter, for TLS defects in\nthermal equilibrium with the electrons and lattice. The scattering of\nquasiparticles by TLS impurities leads to sub-gap states below the bulk\nexcitation gap, $\\Delta$, as well as resonances in the continuum above\n$\\Delta$. The energies and spectral weights of these states depend on the\ndistribution of tunnel splittings, while the spectral weights are particularly\nsensitive to the level occupation of the TLS impurities. Under microwave\nexcitation, or decoupling from the thermal bath, a nonequilibrium level\npopulation of the TLS distribution generates subgap quasiparticle states near\nthe Fermi level which contribute to dissipation and thus degrade the\nperformance of superconducting devices at low temperatures.",
        "Reinforced Galton--Watson processes describe the dynamics of a population\nwhere reproduction events are reinforced, in the sense that offspring numbers\nof forebears can be repeated randomly by descendants. More specifically, the\nevolution depends on the empirical offspring distribution of each individual\nalong its ancestral lineage. We are interested here in asymptotic properties of\nthe empirical distributions observed in the population, such as concentration,\nevanescence and persistence. For this, we incorporate tools from the theory of\nlarge deviations to our preceding analysis [arXiv:2306.02476,arXiv:2310.19030].",
        "Dirac and Motzkin conjectured that any set X of $n$ non-collinear points in\nthe plane has an element incident with at least $\\lceil \\frac{n}{2} \\rceil$\nlines spanned by X. In this paper we prove that any set X of $n$ non-collinear\npoints in the plane, distributed on three lines passing through a common point,\nhas an element incident with at least $\\lceil \\frac{n}{2} \\rceil$ lines spanned\nby X.",
        "In this paper, we establish Malliavin differentiability and absolute\ncontinuity for $\\alpha, \\beta$-doubly perturbed diffusion process with\nparameters $\\alpha <1$ and $\\beta <1$ such that $|\\rho| < 1$, where $ \\rho : =\n\\frac{\\alpha\\beta}{(1-\\alpha)(1-\\beta)}$. Furthermore, under some regularity\nconditions on the coefficients, we prove that the solution $X_t$ has a smooth\ndensity for all $t\\in(0, t_0)$ for some finite number $t_0>0$. Our results\nrecover earlier works by Yue and Zhang (2015) and Xue, Yue and Zhang (2016),\nand the proofs are based on the techniques of the Malliavin calculus.",
        "We consider an assortment selection and pricing problem in which a seller has\n$N$ different items available for sale. In each round, the seller observes a\n$d$-dimensional contextual preference information vector for the user, and\noffers to the user an assortment of $K$ items at prices chosen by the seller.\nThe user selects at most one of the products from the offered assortment\naccording to a multinomial logit choice model whose parameters are unknown. The\nseller observes which, if any, item is chosen at the end of each round, with\nthe goal of maximizing cumulative revenue over a selling horizon of length $T$.\nFor this problem, we propose an algorithm that learns from user feedback and\nachieves a revenue regret of order $\\widetilde{O}(d \\sqrt{K T} \/ L_0 )$ where\n$L_0$ is the minimum price sensitivity parameter. We also obtain a lower bound\nof order $\\Omega(d \\sqrt{T}\/ L_0)$ for the regret achievable by any algorithm.",
        "We address the multiclass classification problem for stochastic diffusion\npaths, assuming that the classes are distinguished by their drift functions,\nwhile the diffusion coefficient remains common across all classes. In this\nsetting, we propose a classification algorithm that relies on the minimization\nof the L 2 risk. We establish rates of convergence for the resulting predictor.\nNotably, we introduce a margin assumption under which we show that our\nprocedure can achieve fast rates of convergence. Finally, a simulation study\nhighlights the numerical performance of our classification algorithm.",
        "When the boundary dynamics of \\(AdS_3\\) gravity is governed by the collective\nfield theory Hamiltonian proposed by Jevicki and Sakita, its asymptotic\nsymmetry algebra becomes the centerless \\(U(1)\\) Kac-Moody algebra. We quantize\nthis system using the quantum bosonization of relativistic free fermions and\nrelate these to the dynamical fields of \\(AdS_3\\) gravity. This leads to a\ncorrespondence where different bulk configurations correspond to distinct\nstates (particle-hole pair excitations) in the fermionic Hilbert space. This\nmapping allows us to construct BTZ black hole microstates, represented by Young\ndiagrams of irreducible \\(U(\\infty)\\) representations. Notably, the logarithm\nof the microstate degeneracy exactly reproduces the classical entropy of the\nBTZ black hole.",
        "Transient plasma events, such as plasma disruptions, are anticipated in the\nfuture magnetic-confinement nuclear fusion reactors. The events are accompanied\nby a rapid change in the magnetic field generated by the plasma current and,\naccordingly, induction of strong eddy currents and Lorentz forces within the\nreactor structure. This work targets processes within liquid-metal components\nof the reactor's breeding blankets. Order-of-magnitude analysis and numerical\nsimulations are performed to understand the response of liquid metal to a\nrapidly changing magnetic field and to evaluate the accuracy of commonly used\nsimplifying model assumptions. The response is found to consist of two stages:\nan initial brief stage ($\\sim 1$ ms) characterized by a rapid increase in the\ninduced currents, forces, and fluid velocity; and a subsequent stage, which is\ntriggered by the growing velocity of the metal and marked by reversals of\nLorentz force, and oscillations and decreases in the amplitude of the induced\nfields. The transition to the second stage sets the upper limit of the velocity\n($\\sim 0.5$ m\/s in our tests), to which an initially quiescent metal can be\naccelerated during the event. The simulations indicate that many widely used\nmodel assumptions, such as the negligible role of Joule dissipation in the heat\nbalance and the constancy of physical property coefficients, remain valid\nduring the response. However, the assumption of liquid metal incompressibility\nis found to be questionable due to the potential significant effects of\npressure waves.",
        "Topology optimization facilitates the automated design of high-performance\nstructures across various engineering fields but, if unconstrained, often\nproduces designs that are complex and difficult to manufacture. A key attribute\nof the resulting designs is connectivity, which involves controlling the\npresence of solid and\/or void islands of material. This manuscript provides a\ncomprehensive overview of existing connectivity constraints developed for\ncontinuous design representations and highlights their advantages and\nlimitations in influencing design outcomes and performance. The review further\nincludes a practical comparison of five different connectivity constraints\nusing a topology optimization framework for sandwich panels that balances\nacoustic and structural performance. With Pareto-front analyses, the\nconstraints are evaluated based on computational cost, monotonicity, parameter\ndependency, and their impact on the optimized designs, their performance, and\nunderlying dynamics. From the comparison, practical insights and rule of thumbs\nhave been derived. The findings emphasize the critical role of selecting\nappropriate connectivity constraints, given their significant effect on the\noptimization results.",
        "Training an energy-based model (EBM) with maximum likelihood is challenging\ndue to the intractable normalisation constant. Traditional methods rely on\nexpensive Markov chain Monte Carlo (MCMC) sampling to estimate the gradient of\nlogartihm of the normalisation constant. We propose a novel objective called\nself-normalised log-likelihood (SNL) that introduces a single additional\nlearnable parameter representing the normalisation constant compared to the\nregular log-likelihood. SNL is a lower bound of the log-likelihood, and its\noptimum corresponds to both the maximum likelihood estimate of the model\nparameters and the normalisation constant. We show that the SNL objective is\nconcave in the model parameters for exponential family distributions. Unlike\nthe regular log-likelihood, the SNL can be directly optimised using stochastic\ngradient techniques by sampling from a crude proposal distribution. We validate\nthe effectiveness of our proposed method on various density estimation tasks as\nwell as EBMs for regression. Our results show that the proposed method, while\nsimpler to implement and tune, outperforms existing techniques.",
        "We present a closed-form expression for the contribution of surface defects\nto the supersymmetric R\\'enyi entropy in six-dimensional $(2,0)$ theories. Our\nresults show that this defect contribution is a linear function of $1\/n$ and is\ndirectly proportional to $2b-d_2$, where $b$ and $d_2$ are the surface defect\nWeyl anomaly coefficients. We also derive a closed-form expression for the\ndefect contribution to the supersymmetric Casimir energy, which simplifies to\n$-d_2$ (up to a proportionality constant) in the chiral algebra limit."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel",
    "start_abstract":"Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2",
        "b0"
      ],
      "title":[
        "Shannon capacity of signal transduction for multiple independent receptors",
        "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
      ],
      "abstract":[
        "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
        "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
      ],
      "categories":[
        "eess.SP",
        "cs.SY"
      ]
    },
    "list":{
      "title":[
        "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings",
        "Separate surface and bulk topological Anderson localization transitions\n  in disordered axion insulators",
        "Harmonic And Transposition Constraints Arising From The Use Of The\n  Roland TR-808 Bass Drum",
        "Quantifying Point Contributions: A Lightweight Framework for Efficient\n  and Effective Query-Driven Trajectory Simplification",
        "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
        "A Hybrid Model\/Data-Driven Solution to Channel, Position and Orientation\n  Tracking in mmWave Vehicular Systems",
        "Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset\n  Electroencephalography Encoding with Quantum Machine Learning",
        "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic\n  Data Generation and Fairness Algorithms",
        "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation",
        "Dango: A Mixed-Initiative Data Wrangling System using Large Language\n  Model",
        "6GStarLab -- A CubeSat Mission to support the development and\n  standardization of Non-Terrestrial Networks towards 6G",
        "Hints of Primordial Magnetic Fields at Recombination and Implications\n  for the Hubble Tension",
        "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in\n  Citation Network",
        "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention",
        "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models",
        "Grammar and Gameplay-aligned RL for Game Description Generation with\n  LLMs",
        "Classical elasticity meets quantum complexity: A connection from the\n  holographic lens",
        "Contact value theorem for electric double layers with modulated surface\n  charge density",
        "Leader-follower formation enabled by pressure sensing in free-swimming\n  undulatory robotic fish",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Three-stage dynamics of nonlinear pulse amplification in ultrafast\n  mid-infrared fiber amplifier with anomalous dispersion",
        "DNN-Powered MLOps Pipeline Optimization for Large Language Models: A\n  Framework for Automated Deployment and Resource Management",
        "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration",
        "Singular leaning coefficients and efficiency in learning theory",
        "Topological derivative approach for deep neural network architecture\n  adaptation",
        "Robust Moving-horizon Estimation for Nonlinear Systems: From Perfect to\n  Imperfect Optimization",
        "Brown dwarf number density in the JWST COSMOS-Web field",
        "Elemental and angular fragmentation cross section measurements with the\n  FOOT experiment"
      ],
      "abstract":[
        "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps:\/\/github.com\/ZeroNLP\/SEA.",
        "In topological phases of matter for which the bulk and boundary support\ndistinct electronic gaps, there exists the possibility of decoupled mobility\ngaps in the presence of disorder. This is in analogy with the well-studied\nproblem of realizing separate or concomitant bulk-boundary criticality in\nconventional Landau theory. Using a three-dimensional axion insulator having\nclean, gapped surfaces with $e^2\/2h$ quantized Hall conductance, we show the\nbulk and surface mobility gap evolve differently in the presence of disorder.\nThe decoupling of the bulk and surface topology yields a regime that realizes a\ntwo-dimensional, unquantized anomalous Hall metal in the Gaussian unitary\nensemble (GUE), which shares some spectral and response properties akin to the\nsurface states of a conventional three-dimensional (3D) topological insulator.\nThe generality of these results as well as extensions to other insulators and\nsuperconductors is discussed.",
        "The study investigates hip-hop music producer Scott Storch's approach to\ntonality, where the song's key is transposed to fit the Roland TR-808 bass drum\ninstead of tuning the drums to the song's key. This process, involving the\nadjustment of all tracks except the bass drum, suggests significant production\nmotives. The primary constraint stems from the limited usable pitch range of\nthe TR-808 bass drum if its characteristic sound is to be preserved. The\nresearch examines drum tuning practices, the role of the Roland TR-808 in\nmusic, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples\nreveals their characteristics and their integration into modern genres like\ntrap and hip-hop. The study also considers the impact of loudspeaker frequency\nresponse and human ear sensitivity on bass drum perception. The findings\nsuggest that Storch's method prioritizes the spectral properties of the bass\ndrum over traditional pitch values to enhance the bass response. The need to\nmaintain the unique sound of the TR-808 bass drum underscores the importance of\nspectral formants and register in contemporary popular music production.",
        "As large volumes of trajectory data accumulate, simplifying trajectories to\nreduce storage and querying costs is increasingly studied. Existing proposals\nface three main problems. First, they require numerous iterations to decide\nwhich GPS points to delete. Second, they focus only on the relationships\nbetween neighboring points (local information) while neglecting the overall\nstructure (global information), reducing the global similarity between the\nsimplified and original trajectories and making it difficult to maintain\nconsistency in query results, especially for similarity-based queries. Finally,\nthey fail to differentiate the importance of points with similar features,\nleading to suboptimal selection of points to retain the original trajectory\ninformation.\n  We propose MLSimp, a novel Mutual Learning query-driven trajectory\nsimplification framework that integrates two distinct models: GNN-TS, based on\ngraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates\nthe importance of a point according to its globality, capturing its correlation\nwith the entire trajectory, and its uniqueness, capturing its differences from\nneighboring points. It also incorporates attention mechanisms in the GNN\nlayers, enabling simultaneous data integration from all points within the same\ntrajectory and refining representations, thus avoiding iterative processes.\nDiff-TS generates amplified signals to enable the retention of the most\nimportant points at low compression rates. Experiments involving eight\nbaselines on three databases show that MLSimp reduces the simplification time\nby 42%--70% and improves query accuracy over simplified trajectories by up to\n34.6%.",
        "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
        "Channel tracking in millimeter wave (mmWave) vehicular systems is crucial for\nmaintaining robust vehicle-to-infrastructure (V2I) communication links, which\ncan be leveraged to achieve high accuracy vehicle position and orientation\ntracking as a byproduct of communication. While prior work tends to simplify\nthe system model by omitting critical system factors such as clock offsets,\nfiltering effects, antenna array orientation offsets, and channel estimation\nerrors, we address the challenges of a practical mmWave multiple-input\nmultiple-output (MIMO) communication system between a single base station (BS)\nand a vehicle while tracking the vehicle's position and orientation (PO)\nconsidering realistic driving behaviors. We first develop a channel tracking\nalgorithm based on multidimensional orthogonal matching pursuit (MOMP) with\nfactoring (F-MOMP) to reduce computational complexity and enable\nhigh-resolution channel estimates during the tracking stage, suitable for PO\nestimation. Then, we develop a network called VO-ChAT (Vehicle\nOrientation-Channel Attention for orientation Tracking), which processes the\nchannel estimate sequence for orientation prediction. Afterward, a weighted\nleast squares (WLS) problem that exploits the channel geometry is formulated to\ncreate an initial estimate of the vehicle's 2D position. A second network named\nVP-ChAT (Vehicle Position-Channel Attention for position Tracking) refines the\ngeometric position estimate. VP-ChAT is a Transformer inspired network\nprocessing the historical channel and position estimates to provide the\ncorrection for the initial geometric position estimate. The proposed solution\nis evaluated using raytracing generated channels in an urban canyon\nenvironment. For 80% of the cases it achieves a 2D position tracking accuracy\nof 26 cm while orientation errors are kept below 0.5 degree.",
        "Electroencephalography (EEG) is widely used in neuroscience and clinical\nresearch for analyzing brain activity. While deep learning models such as\nEEGNet have shown success in decoding EEG signals, they often struggle with\ndata complexity, inter-subject variability, and noise robustness. Recent\nadvancements in quantum machine learning (QML) offer new opportunities to\nenhance EEG analysis by leveraging quantum computing's unique properties. In\nthis study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a\nhybrid neural network incorporating quantum layers into EEGNet, to investigate\nits generalization ability across multiple EEG datasets. Our evaluation spans a\ndiverse set of cognitive and motor task datasets, assessing QEEGNet's\nperformance in different learning scenarios. Experimental results reveal that\nwhile QEEGNet demonstrates competitive performance and maintains robustness in\ncertain datasets, its improvements over traditional deep learning methods\nremain inconsistent. These findings suggest that hybrid quantum-classical\narchitectures require further optimization to fully leverage quantum advantages\nin EEG processing. Despite these limitations, our study provides new insights\ninto the applicability of QML in EEG research and highlights challenges that\nmust be addressed for future advancements.",
        "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
        "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
        "Data wrangling is a time-consuming and challenging task in a data science\npipeline. While many tools have been proposed to automate or facilitate data\nwrangling, they often misinterpret user intent, especially in complex tasks. We\npropose Dango, a mixed-initiative multi-agent system for data wrangling.\nCompared to existing tools, Dango enhances user communication of intent by\nallowing users to demonstrate on multiple tables and use natural language\nprompts in a conversation interface, enabling users to clarify their intent by\nanswering LLM-posed multiple-choice clarification questions, and providing\nmultiple forms of feedback such as step-by-step natural language explanations\nand data provenance to help users evaluate the data wrangling scripts. We\nconducted a within-subjects user study with 38 participants and demonstrated\nthat Dango's features can significantly improve intent clarification, accuracy,\nand efficiency in data wrangling. Furthermore, we demonstrated the\ngeneralizability of Dango by applying it to a broader set of data wrangling\ntasks.",
        "The emergence of the Non-Terrestrial Network (NTN) concept in the last years\nhas revolutionized the space industry. This novel network architecture composed\nof aircraft and spacecraft is currently being standardized by the 3GPP. This\nstandardization process follows dedicated phases in which experimentation of\nthe technology is needed. Although some missions have been conducted to\ndemonstrate specific and service-centric technologies, a open flexible in-orbit\ninfrastructure is demanded to support this standardization process. This work\npresents the 6GStarLab mission, which aims to address this gap. Specifically,\nthis mission envisions to provide a 6U CubeSat as the main in-orbit\ninfrastructure in which multiple technology validations can be uploaded. The\nconcept of this mission is depicted. Additionally, this work presents the\ndetails of the satellite platform and the payload. This last one is designed to\nenable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-,\nand Ka-bands) and an optical terminal. The launch of the satellite is scheduled\nfor Q2 2025, and it will contribute to the standardization of future NTN\narchitectures.",
        "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
        "With the explosive growth of academic literature, effectively evaluating the\nknowledge value of literature has become quite essential. However, most of the\nexisting methods focus on modeling the entire citation network, which is\nstructurally complex and often suffers from long sequence dependencies when\ndealing with text embeddings. Thus, they might have low efficiency and poor\nrobustness in different fields. To address these issues, a novel knowledge\nevaluation method is proposed, called EMK-KEN. The model consists of two\nmodules. Specifically, the first module utilizes MetaFP and Mamba to capture\nsemantic features of node metadata and text embeddings to learn contextual\nrepresentations of each paper. The second module utilizes KAN to further\ncapture the structural information of citation networks in order to learn the\ndifferences in different fields of networks. Extensive experiments based on ten\nbenchmark datasets show that our method outperforms the state-of-the-art\ncompetitors in effectiveness and robustness.",
        "Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and\/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.",
        "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.",
        "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.",
        "In this work, we explore the effects of shear deformations in a wide class of\nholographic amorphous solids. It is found that both the shear stress and the\ncomplexity of formation grow with the increase of the shear strain. Notably, in\nthe regime of very large shear, they exhibit coordinated behavior and adhere to\na universal scaling relation, uncovering a surprising connection between two\nseemingly unrelated aspects of amorphous systems. Furthermore, our findings\nalso provide a counterexample to the previous understanding that the complexity\nscales linearly with the Bekenstein-Hawking entropy for large static black\nholes.",
        "The contact value theorem was originally derived for Coulomb fluids of mobile\ncharged particles in thermal equilibrium, in the presence of interfaces\ncarrying a {\\em uniform} surface charge density and in the absence of\ndielectric discontinuities. It relates the pressure (the effective force)\nbetween two parallel electric double layers to the particle number density and\nthe surface charge density at the interface, separately for each of the two\nelectric double layers. In this paper, we generalise the contact value theorem\nto electric double layers with interfaces carrying a {\\em modulated} surface\ncharge density. The derivation is based on balance of forces exerted on\ninterfaces. The relevance of particular terms of the contact value theorem is\ntested on an exactly solvable two-dimensional Coulomb system with counterions\nonly at the coupling constant $\\Gamma=2$.",
        "Fish use their lateral lines to sense flows and pressure gradients, enabling\nthem to detect nearby objects and organisms. Towards replicating this\ncapability, we demonstrated successful leader-follower formation swimming using\nflow pressure sensing in our undulatory robotic fish ($\\mu$Bot\/MUBot). The\nfollower $\\mu$Bot is equipped at its head with bilateral pressure sensors to\ndetect signals excited by both its own and the leader's movements. First, using\nexperiments with static formations between an undulating leader and a\nstationary follower, we determined the formation that resulted in strong\npressure variations measured by the follower. This formation was then selected\nas the desired formation in free swimming for obtaining an expert policy. Next,\na long short-term memory neural network was used as the control policy that\nmaps the pressure signals along with the robot motor commands and the Euler\nangles (measured by the onboard IMU) to the steering command. The policy was\ntrained to imitate the expert policy using behavior cloning and Dataset\nAggregation (DAgger). The results show that with merely two bilateral pressure\nsensors and less than one hour of training data, the follower effectively\ntracked the leader within distances of up to 200 mm (= 1 body length) while\nswimming at speeds of 155 mm\/s (= 0.8 body lengths\/s). This work highlights the\npotential of fish-inspired robots to effectively navigate fluid environments\nand achieve formation swimming through the use of flow pressure feedback.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Nonlinear pulse amplification in optical fiber, with capability of breaking\nthe gain-bandwidth limitation, is a key technique for high-energy, ultrafast\npulse generation. In the longer wavelength region (including 1.55 {\\mu}m, 2\n{\\mu}m and 2.8 {\\mu}m) where the gain fiber has normally strong anomalous\ndispersion, the nonlinear amplification process over fiber exhibits more\ncomplicated dynamics than that of its 1-{\\mu}m counterpart, and the underlying\nmechanism of the nonlinear pulse propagation process in high-gain anomalous\nfiber is still elusive so far. Here, we demonstrate an in-depth study on the\nnonlinear amplification process in high-gain ultrafast mid-infrared fiber,\nproviding clear physical understanding on the debate of adiabatic soliton\ncompression. We unveil that under the high-gain condition, the ultrafast pulse\nlaunched into the anomalous gain fiber experiences successively three distinct\nstages, named as the balance between linear and nonlinear chirp,\nhigh-order-soliton-like pulse compression and pulse splitting due to high-order\neffects. While a relatively-clean ultrafast pulse can be obtained immediately\nafter the high-order-soliton-like compression stage, excessive gain fiber\nlength could hardly enhance further the pulse peak power due to soliton\nsplitting. Our findings can provide several critical guidelines for designing\nhigh-power ultrafast fiber amplifiers at near- and mid-infrared wavelengths.",
        "The exponential growth in the size and complexity of Large Language Models\n(LLMs) has introduced unprecedented challenges in their deployment and\noperational management. Traditional MLOps approaches often fail to efficiently\nhandle the scale, resource requirements, and dynamic nature of these models.\nThis research presents a novel framework that leverages Deep Neural Networks\n(DNNs) to optimize MLOps pipelines specifically for LLMs. Our approach\nintroduces an intelligent system that automates deployment decisions, resource\nallocation, and pipeline optimization while maintaining optimal performance and\ncost efficiency. Through extensive experimentation across multiple cloud\nenvironments and deployment scenarios, we demonstrate significant improvements:\n40% enhancement in resource utilization, 35% reduction in deployment latency,\nand 30% decrease in operational costs compared to traditional MLOps approaches.\nThe framework's ability to adapt to varying workloads and automatically\noptimize deployment strategies represents a significant advancement in\nautomated MLOps management for large-scale language models. Our framework\nintroduces several novel components including a multi-stream neural\narchitecture for processing heterogeneous operational metrics, an adaptive\nresource allocation system that continuously learns from deployment patterns,\nand a sophisticated deployment orchestration mechanism that automatically\nselects optimal strategies based on model characteristics and environmental\nconditions. The system demonstrates robust performance across various\ndeployment scenarios, including multi-cloud environments, high-throughput\nproduction systems, and cost-sensitive deployments. Through rigorous evaluation\nusing production workloads from multiple organizations, we validate our\napproach's effectiveness in reducing operational complexity while improving\nsystem reliability and cost efficiency.",
        "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582\/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61\/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.",
        "Singular learning models with non-positive Fisher information matrices\ninclude neural networks, reduced-rank regression, Boltzmann machines, normal\nmixture models, and others. These models have been widely used in the\ndevelopment of learning machines. However, theoretical analysis is still in its\nearly stages. In this paper, we examine learning coefficients, which indicate\nthe general learning efficiency of deep linear learning models and three-layer\nneural network models with ReLU units. Finally, we extend the results to\ninclude the case of the Softmax function.",
        "This work presents a novel algorithm for progressively adapting neural\nnetwork architecture along the depth. In particular, we attempt to address the\nfollowing questions in a mathematically principled way: i) Where to add a new\ncapacity (layer) during the training process? ii) How to initialize the new\ncapacity? At the heart of our approach are two key ingredients: i) the\nintroduction of a ``shape functional\" to be minimized, which depends on neural\nnetwork topology, and ii) the introduction of a topological derivative of the\nshape functional with respect to the neural network topology. Using an optimal\ncontrol viewpoint, we show that the network topological derivative exists under\ncertain conditions, and its closed-form expression is derived. In particular,\nwe explore, for the first time, the connection between the topological\nderivative from a topology optimization framework with the Hamiltonian from\noptimal control theory. Further, we show that the optimality condition for the\nshape functional leads to an eigenvalue problem for deep neural architecture\nadaptation. Our approach thus determines the most sensitive location along the\ndepth where a new layer needs to be inserted during the training phase and the\nassociated parametric initialization for the newly added layer. We also\ndemonstrate that our layer insertion strategy can be derived from an optimal\ntransport viewpoint as a solution to maximizing a topological derivative in\n$p$-Wasserstein space, where $p>= 1$. Numerical investigations with fully\nconnected network, convolutional neural network, and vision transformer on\nvarious regression and classification problems demonstrate that our proposed\napproach can outperform an ad-hoc baseline network and other architecture\nadaptation strategies. Further, we also demonstrate other applications of\ntopological derivative in fields such as transfer learning.",
        "Robust stability of moving-horizon estimators is investigated for nonlinear\ndiscrete-time systems that are detectable in the sense of incremental\ninput\/output-to-state stability and are affected by disturbances. The estimate\nof a moving-horizon estimator stems from the on-line solution of a\nleast-squares minimization problem at each time instant. The resulting\nstability guarantees depend on the optimization tolerance in solving such\nminimization problems. Specifically, two main contributions are established:\n(i) the robust stability of the estimation error, while supposing to solve\nexactly the on-line minimization problem; (ii) the practical robust stability\nof the estimation error with state estimates obtained by an imperfect\nminimization. Finally, the construction of such robust moving-horizon\nestimators and the performances resulting from the design based on the\ntheoretical findings are showcased with two numerical examples.",
        "Brown dwarfs are failed stars with very low mass (13 to 75 $M_J$), and an\neffective temperature lower than 2500 K. Thus, they play a key role in\nunderstanding the gap in the mass function between stars and planets. However,\ndue to their faint nature, previous searches are inevitably limited to the\nsolar neighbourhood (20 pc). To improve our knowledge of the low mass part of\nthe initial stellar mass function and the star formation history of the Milky\nWay, it is crucial to find more distant brown dwarfs. Using James Webb Space\nTelescope (JWST) COSMOS-Web data, this study seeks to enhance our comprehension\nof the physical characteristics of brown dwarfs situated at a distance of kpc\nscale. The exceptional sensitivity of the JWST enables the detection of brown\ndwarfs that are up to 100 times more distant than those discovered in the\nearlier all-sky infrared surveys. The large area coverage of the JWST\nCOSMOS-Web survey allows us to find more distant brown dwarfs than earlier JWST\nstudies with smaller area coverages. To capture prominent water absorption\nfeatures around 2.7 $\\mu$m, we apply two colour criteria,\nF115W-F277W+1<F277W-F444W and F277W-F444W>0.9. We then select point sources by\nCLASS_STAR, FLUX_RADIUS, and SPREAD_MODEL criteria. Faint sources are visually\nchecked to exclude possibly extended sources. We conduct SED fitting and MCMC\nsimulations to determine their physical properties and associated\nuncertainties. Our search reveals 25 T-dwarf and 2 Y-dwarf candidates, more\nthan any previous JWST brown dwarf searches. They are located from 0.3 kpc to 4\nkpc away from the Earth. The cumulative number count of our brown dwarf\ncandidates is consistent with the prediction from a standard double exponential\nmodel. Three of our brown dwarf candidates were detected by HST, with\ntransverse velocities $12\\pm5$ km s$^{-1}$, $12\\pm4$ km s$^{-1}$, and $17\\pm6$\nkm s$^{-1}$.",
        "The FOOT (FragmentatiOn Of Target) experiment was proposed to measure double\ndifferential nuclear fragmentation cross sections in angle and kinetic energy\nof the produced fragments in beam-target settings, interesting for\nhadrontherapy and space radioprotection applications. In particular, FOOT\nmeasures projectile and target fragmentations in the kinetic energy range\nbetween $200 \\text{MeV\/u}$ and $800 \\text{MeV\/u}$. In this contribution,\ndifferential cross section measurements of a $400 \\text{MeV\/u}$ $^{16}$O beam\non a Carbon and a polyethylene target with data acquired at GSI (Darmstadt,\nGermany) beam accelerator facility are presented, along with the extraction of\nthe first total fragmentation cross section for a Hydrogen target within the\nFOOT experiment."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Shannon capacity of signal transduction for multiple independent receptors",
    "start_abstract":"Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h",
        "Gravitational lensing by non-self-intersecting vortons",
        "Construction of self-similar energy forms and singularity of Sobolev\n  spaces on Laakso-type fractal spaces",
        "Exact Bayesian inference for Markov switching diffusions",
        "Thermoelectrically Elevated Hydrogel Evaporation for Personal Cooling\n  under Extreme Heat Stress",
        "Predicting House Rental Prices in Ghana Using Machine Learning",
        "Forward and Inverse Problems in Nonlinear Acoustics",
        "Moist Vortex Dynamics of Axisymmetric Tropical Cyclones Before Reaching\n  Symmetric Neutrality -- Part I: A Generalized Tangential Wind Formula",
        "Trajectories of light beams in a Kerr metric: the influence of the\n  rotation of an observer on the shadow of a black hole",
        "Matter Couplings in Supergravity. The first 10 years",
        "Crossover from Wannier-Stark localization to charge density waves for\n  interacting spinless fermions in one dimension",
        "Two categories of UV-upturn galaxies revealed by semi-analytic models",
        "Global boundedness in the higher-dimensional fully parabolic chemotaxis\n  with weak singular sensitivity and logistic source",
        "Unique extremality of affine maps on plane domains",
        "Star formation in low brightness galaxies and in the extended gaseous\n  disks of normal galaxies"
      ],
      "abstract":[
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc.",
        "We present exact solutions to the Nambu-Goto equations for thin vortons\nstabilized by chiral currents. The solutions describe a class of\nnon-self-intersecting, stationary loops with arbitrary shapes. In addition to\nthe trivial circular and the Kibble-Turok vortons, we also derive a\ntwo-parameter family that incorporates the first, second, and third harmonic\nmodes. We found that, in general, the vorton's constraints allow for\nconstructing families of solutions with arbitrary harmonic modes. We further\ninvestigate the gravitational lensing effects associated with these solutions\nunder the weak-field and thin-lens approximations. For circular vortons, the\nlensing exhibits a sharp discontinuity separating two regions with distinctly\ndifferent distortions. The corresponding Einstein ring co-exist alongside an\nalmost undistorted source image. This effect is significantly amplified in the\ncase of non-circular vortons, highlighting their potential observational\nsignatures.",
        "We construct self-similar $p$-energy forms as normalized limits of\ndiscretized $p$-energies on a rich class of Laakso-type fractal spaces.\nCollectively, we refer to them as IGS-fractals, where IGS stands for\n(edge-)iterated graph systems. We propose this framework as a rich source of\n\"toy models\" that can be consulted for tackling challenging questions that are\nnot well understood on most other fractal spaces. Supporting this, our\nframework uncovers a novel analytic phenomenon, which we term as singularity of\nSobolev spaces. This means that the associated Sobolev spaces\n$\\mathscr{F}_{p_1}$ and $\\mathscr{F}_{p_2}$ for distinct $p_1,p_2 \\in\n(1,\\infty)$ intersect only at constant functions. We provide the first example\nof a self-similar fractal on which this singularity phenomenon occurs for all\npairs of distinct exponents. In particular, we show that the Laakso diamond\nspace is one such example.",
        "We give the first exact Bayesian methodology for the problem of inference in\ndiscretely observed regime switching diffusions. We design an MCMC and an MCEM\nalgorithm that target the exact posterior of diffusion parameters and the\nlatent regime process. The algorithms are exact in the sense that they target\nthe correct posterior distribution of the continuous model, so that the errors\nare due to Monte Carlo only. Switching diffusion models extend ordinary\ndiffusions by allowing for jumps in instantaneous drift and volatility. The\njumps are driven by a latent, continuous time Markov switching process. We\nillustrate the method on numerical examples, including an empirical analysis of\nthe method's scalability in the length of the time series, and find that it is\ncomparable in computational cost with discrete approximations while avoiding\ntheir shortcomings.",
        "Hydrogel evaporative cooling has recently emerged as an appealing strategy\nfor personal cooling. However, with the increasing prevalence of extreme heat\nevents featuring high temperatures (above 40$^{\\circ}$C) and relative humidity\n(RH$> 30\\%$), hydrogel alone may not achieve thermal comfort under most\nconditions, as it must be maintained at a sufficiently high temperature-often\nexceeding the skin comfort temperature ($\\sim$35.8$^{\\circ}$C)-to achieve\neffective evaporation in hot, humid environments. This study integrates\nthermoelectric devices (TEDs) with hydrogels to create a personal cooling\nsolution suited to extreme heat. TEDs pump heat away from the skin, maintaining\nit at a comfortable temperature, while simultaneously raising the temperature\nof a hydrogel layer positioned on top of the TEDs to enhance its evaporation\nrate. The TED-hydrogel tandem device outperforms TEDs or hydrogel alone in\nextreme conditions (up to 55$^{\\circ}$C and RH$> 30\\%$). Furthermore, the\nactive temperature control enabled by the TEDs allows the system to adapt to\nvarying thermal loads and environmental conditions. With a manageable hydrogel\nand battery weight, this cooling system can operate for over six hours. These\nresults demonstrate the potential of hybrid evaporative and thermoelectric\ncooling as an efficient, adaptable, and sustainable personal cooling solution\nto combat extreme heat.",
        "This study investigates the efficacy of machine learning models for\npredicting house rental prices in Ghana, addressing the need for accurate and\naccessible housing market information. Utilising a comprehensive dataset of\nrental listings, we trained and evaluated various models, including CatBoost,\nXGBoost, and Random Forest. CatBoost emerged as the best-performing model,\nachieving an $R^2$ of 0.876, demonstrating its ability to effectively capture\ncomplex relationships within the housing market. Feature importance analysis\nrevealed that location-based features, number of bedrooms, bathrooms, and\nfurnishing status are key drivers of rental prices. Our findings provide\nvaluable insights for stakeholders, including real estate professionals,\ninvestors, and policymakers, while also highlighting opportunities for future\nresearch, such as incorporating temporal data and exploring regional\nvariations.",
        "The importance of ultrasound is well established in the imaging of human\ntissue. In order to enhance image quality by exploiting nonlinear effects,\nrecently techniques such as harmonic imaging and nonlinearity parameter\ntomography have been put forward. As soon as the pressure amplitude exceeds a\ncertain bound, the classical linear wave equation loses its validity and more\ngeneral nonlinear versions have to be used. Another characteristic property of\nultrasound propagation in human tissue is frequency power law attenuation,\nleading to fractional derivative damping models in time domain. In this\ncontribution we will first of all dwell on modeling nonlinearity on the one\nhand and fractional damping on the other hand. Moreover we will give an idea on\nthe challenges in the analysis of the resulting PDEs and discuss some parameter\nasymptotics. Finally, we address a relevant inverse problems in this context,\nthe above mentioned task of nonlinearity parameter imaging, which leads to a\ncoefficient identification problem for a quasilinear wave equation.",
        "The potential intensity (PI) theory of tropical cyclones (TCs) provides a\nreasonable estimate of the maximum intensity of a steady-state TC in a\nquiescent environment. The traditional PI theory relies on the symmetric\nneutrality (SN) assumption, where the isolines of absolute angular momentum (M)\nare parallel to the saturation entropy (s*) surfaces within the eyewall\nupdraft. When the SN is not valid, there is currently no quantitative theory\nthat explicitly describes how these surfaces directly relate to the maximum\ntangential wind (vmax) near the surface. In this study, the PI theory is\nrevisited without making the SN assumption. Under non-SN conditions, it is\nfound that the balance between the centrifugal torque and baroclinic torque\nprovides a strong constraint on the vortex structure and balanced intensity.\nSpecifically, it is shown that the gradient between s* and M along constant\ntemperature (T) throughout the saturated eyewall determines the structure of\nthe M surface and the balanced intensity at the low level. The same technique\ncan be applied to obtain the generalized terms that correspond to the\nunbalanced component. It is shown that this generalized vmax equation is the\nnatural extension of the traditional PI formula into the non-symmetric\nneutrality regime. Verifying against axisymmetric simulations, it is shown that\nthe generalized vmax equation can accurately quantify the various contributions\nto vmax during the rapid intensification process. The implications of these\nfindings on the TC rapid intensification, such as the TC inner-core structure\nand the upper-tropospheric mixing process, are examined.",
        "This paper investigates the trajectories of light beams in a Kerr metric,\nwhich describes the gravitational field in the neighborhood of a rotating black\nhole. After reduction by cyclic coordinates, this problem reduces to analysis\nof a Hamiltonian system with two degrees of freedom. A bifurcation diagram is\nconstructed and a classification is made of the types of trajectories of the\nsystem according to the values of first integrals. Relations describing the\nboundary of the shadow of the black hole are obtained for a stationary observer\nwho rotates with an arbitrary angular velocity about the axis of rotation of\nthe black hole.",
        "Following the initial construction of pure supergravity in 1976, various\nmethods were developed to couple supergravity with supersymmetric matter. This\ncontribution to 'Half a century of supergravity' provides a personal\nperspective on the key steps, techniques and results developed in the first\ndecade. These developments form the foundation for numerous applications in\nphenomenology, cosmology and string theories, while also revealing intriguing\nmathematical structures.",
        "We study spinless fermions on a finite chain with nearest-neighbor repulsion\nand in the presence of a Wannier-Stark linearly-varying electric field\npotential. In the absence of the interaction, the eigenstates are localized for\nthe system's sizes larger than the localization length. We present several\nanalytical expressions for the localization length, which is proportional to\nthe inverse of the electric field. Using the density matrix renormalization\ngroup numerical technique, we observe that the ground state exhibits a decrease\nof the occupation on the chain sites from the `bulk', with occupation 1, to the\nvacuum, with occupation 0. The width of this intermediate `edge' region is also\ninversely proportional to the electric field, increasing linearly with the\nstrength of the nearest-neighbor repulsion. For strong interactions, the\noccupations in the intermediate region exhibit a charge density wave. We also\npresent the local density of states for sites in the `edge' region. For the\nnon-interacting case, the spectrum shows an increasing energy-localized\nstructure as the field is increased, which is a consequence of the uniform\nenergy distribution of the localized states (Wannier-Stark ladder). This\nstructure survives for small interactions, and it smears out in the strongly\ninteracting limit. Experimental variations of the slope of the potential (the\nelectric field) on cold atom chains may test these predictions.",
        "UV-upturn galaxies are characterized by unusually excessive flux in the\nfar-ultraviolet (FUV) band, observed in some elliptical galaxies and the bulges\nof disk galaxies. We examine UV-upturn galaxies within the semi-analytic model\nGABE, which embeds the formation of extreme horizontal branch (EHB) stars --\nproposed as key candidates responsible for the UV-upturn phenomenon. We have\nanalyzed all related physical processes, including stellar evolution, initial\nmass functions (IMFs), dust attenuation, galaxy age, metallicity, and binary\nfractions, in an effort to determine which processes play significant roles.\nOur findings reveal two categories of UV-upturn galaxies in the semi-analytic\nmodel, each with distinct formation channels: old metal-rich quenched\nelliptical galaxies, which are intrinsic UV-upturn galaxies induced by EHB\nstars within their old stellar populations, and dusty star-forming galaxies,\nwhich are relatively young and may also be photometrically identified as\nUV-upturn galaxies when accounting for dust attenuation. Dust attenuation\ncontributes to 20%-60% of the UV-upturn galaxies, depending on the specific\ndust attenuation models adopted. With the binary star formation model of EHB\nstars, both of these formation channels exhibit strong preferences for high\nstellar metallicity. The high-mass end slope of the IMFs is found to have a\nmarginal effect, indicating that a universal IMF is adequate for studying the\nUV-upturn phenomenon.",
        "We consider the following chemotaxis system under homogeneous Neumann\nboundary conditions in a smooth, open, bounded domain $\\Omega \\subset\n\\mathbb{R}^n$ with $n \\geq 3$: \\begin{equation*}\n  \\begin{cases}\n  u_t = \\Delta u - \\chi \\nabla \\cdot \\left( \\frac{u}{v^k} \\nabla v \\right) + ru\n- \\mu u^2, & \\text{in } \\Omega \\times (0,T_{\\rm max}),\n  v_t = \\Delta v - \\alpha v + \\beta u, & \\text{in } \\Omega \\times (0,T_{\\rm\nmax}),\n  \\end{cases} \\end{equation*} where $k \\in (0,1)$, and $\\chi, r, \\mu, \\alpha,\n\\beta$ are positive parameters. In this paper, we demonstrate that for suitably\nsmooth initial data, the problem admits a unique nonnegative classical solution\nthat remains globally bounded in time when $\\mu$ is sufficiently large.",
        "We prove that affine maps are uniquely extremal quasiconformal maps on the\ncomplement of a well distribute set in the complex plane answering a conjecture\nfrom \\cite{markovic}. We construct the required Reich sequence using Bergman\nprojections, and meromorphic partitions of unity.",
        "We analyze the available observational data on the radial distribution of gas\nand young stellar populations in the disks of low surface brightness (LSB)\ngalaxies and in the outer regions or the extended disks of normal brightness\n(HSB) galaxies. These cases involve star formation under special conditions of\nlow volume and surface gas density. There is no well-defined boundary between\nthese subgroups of galaxies that we consider, but in non-dwarf LSB galaxies the\nrate of current star formation within the wide range of radial distances\nappears to be higher compared to the outer disks of most of HSB galaxies at\nsimilar values of the surface gas density. The factors that could stimulate the\ncompression of the rarefied gas at the periphery of galaxies are briefly\ndiscussed. Attention is drawn to the idea that the densities of LSB disks\nestimated from their brightness may be underestimated."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT",
    "start_abstract":"Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h",
        "Gravitational lensing by non-self-intersecting vortons",
        "Construction of self-similar energy forms and singularity of Sobolev\n  spaces on Laakso-type fractal spaces",
        "Exact Bayesian inference for Markov switching diffusions",
        "Thermoelectrically Elevated Hydrogel Evaporation for Personal Cooling\n  under Extreme Heat Stress",
        "Predicting House Rental Prices in Ghana Using Machine Learning",
        "Forward and Inverse Problems in Nonlinear Acoustics",
        "Moist Vortex Dynamics of Axisymmetric Tropical Cyclones Before Reaching\n  Symmetric Neutrality -- Part I: A Generalized Tangential Wind Formula",
        "Trajectories of light beams in a Kerr metric: the influence of the\n  rotation of an observer on the shadow of a black hole",
        "Matter Couplings in Supergravity. The first 10 years",
        "Crossover from Wannier-Stark localization to charge density waves for\n  interacting spinless fermions in one dimension",
        "Two categories of UV-upturn galaxies revealed by semi-analytic models",
        "Global boundedness in the higher-dimensional fully parabolic chemotaxis\n  with weak singular sensitivity and logistic source",
        "Unique extremality of affine maps on plane domains",
        "Star formation in low brightness galaxies and in the extended gaseous\n  disks of normal galaxies"
      ],
      "abstract":[
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc.",
        "We present exact solutions to the Nambu-Goto equations for thin vortons\nstabilized by chiral currents. The solutions describe a class of\nnon-self-intersecting, stationary loops with arbitrary shapes. In addition to\nthe trivial circular and the Kibble-Turok vortons, we also derive a\ntwo-parameter family that incorporates the first, second, and third harmonic\nmodes. We found that, in general, the vorton's constraints allow for\nconstructing families of solutions with arbitrary harmonic modes. We further\ninvestigate the gravitational lensing effects associated with these solutions\nunder the weak-field and thin-lens approximations. For circular vortons, the\nlensing exhibits a sharp discontinuity separating two regions with distinctly\ndifferent distortions. The corresponding Einstein ring co-exist alongside an\nalmost undistorted source image. This effect is significantly amplified in the\ncase of non-circular vortons, highlighting their potential observational\nsignatures.",
        "We construct self-similar $p$-energy forms as normalized limits of\ndiscretized $p$-energies on a rich class of Laakso-type fractal spaces.\nCollectively, we refer to them as IGS-fractals, where IGS stands for\n(edge-)iterated graph systems. We propose this framework as a rich source of\n\"toy models\" that can be consulted for tackling challenging questions that are\nnot well understood on most other fractal spaces. Supporting this, our\nframework uncovers a novel analytic phenomenon, which we term as singularity of\nSobolev spaces. This means that the associated Sobolev spaces\n$\\mathscr{F}_{p_1}$ and $\\mathscr{F}_{p_2}$ for distinct $p_1,p_2 \\in\n(1,\\infty)$ intersect only at constant functions. We provide the first example\nof a self-similar fractal on which this singularity phenomenon occurs for all\npairs of distinct exponents. In particular, we show that the Laakso diamond\nspace is one such example.",
        "We give the first exact Bayesian methodology for the problem of inference in\ndiscretely observed regime switching diffusions. We design an MCMC and an MCEM\nalgorithm that target the exact posterior of diffusion parameters and the\nlatent regime process. The algorithms are exact in the sense that they target\nthe correct posterior distribution of the continuous model, so that the errors\nare due to Monte Carlo only. Switching diffusion models extend ordinary\ndiffusions by allowing for jumps in instantaneous drift and volatility. The\njumps are driven by a latent, continuous time Markov switching process. We\nillustrate the method on numerical examples, including an empirical analysis of\nthe method's scalability in the length of the time series, and find that it is\ncomparable in computational cost with discrete approximations while avoiding\ntheir shortcomings.",
        "Hydrogel evaporative cooling has recently emerged as an appealing strategy\nfor personal cooling. However, with the increasing prevalence of extreme heat\nevents featuring high temperatures (above 40$^{\\circ}$C) and relative humidity\n(RH$> 30\\%$), hydrogel alone may not achieve thermal comfort under most\nconditions, as it must be maintained at a sufficiently high temperature-often\nexceeding the skin comfort temperature ($\\sim$35.8$^{\\circ}$C)-to achieve\neffective evaporation in hot, humid environments. This study integrates\nthermoelectric devices (TEDs) with hydrogels to create a personal cooling\nsolution suited to extreme heat. TEDs pump heat away from the skin, maintaining\nit at a comfortable temperature, while simultaneously raising the temperature\nof a hydrogel layer positioned on top of the TEDs to enhance its evaporation\nrate. The TED-hydrogel tandem device outperforms TEDs or hydrogel alone in\nextreme conditions (up to 55$^{\\circ}$C and RH$> 30\\%$). Furthermore, the\nactive temperature control enabled by the TEDs allows the system to adapt to\nvarying thermal loads and environmental conditions. With a manageable hydrogel\nand battery weight, this cooling system can operate for over six hours. These\nresults demonstrate the potential of hybrid evaporative and thermoelectric\ncooling as an efficient, adaptable, and sustainable personal cooling solution\nto combat extreme heat.",
        "This study investigates the efficacy of machine learning models for\npredicting house rental prices in Ghana, addressing the need for accurate and\naccessible housing market information. Utilising a comprehensive dataset of\nrental listings, we trained and evaluated various models, including CatBoost,\nXGBoost, and Random Forest. CatBoost emerged as the best-performing model,\nachieving an $R^2$ of 0.876, demonstrating its ability to effectively capture\ncomplex relationships within the housing market. Feature importance analysis\nrevealed that location-based features, number of bedrooms, bathrooms, and\nfurnishing status are key drivers of rental prices. Our findings provide\nvaluable insights for stakeholders, including real estate professionals,\ninvestors, and policymakers, while also highlighting opportunities for future\nresearch, such as incorporating temporal data and exploring regional\nvariations.",
        "The importance of ultrasound is well established in the imaging of human\ntissue. In order to enhance image quality by exploiting nonlinear effects,\nrecently techniques such as harmonic imaging and nonlinearity parameter\ntomography have been put forward. As soon as the pressure amplitude exceeds a\ncertain bound, the classical linear wave equation loses its validity and more\ngeneral nonlinear versions have to be used. Another characteristic property of\nultrasound propagation in human tissue is frequency power law attenuation,\nleading to fractional derivative damping models in time domain. In this\ncontribution we will first of all dwell on modeling nonlinearity on the one\nhand and fractional damping on the other hand. Moreover we will give an idea on\nthe challenges in the analysis of the resulting PDEs and discuss some parameter\nasymptotics. Finally, we address a relevant inverse problems in this context,\nthe above mentioned task of nonlinearity parameter imaging, which leads to a\ncoefficient identification problem for a quasilinear wave equation.",
        "The potential intensity (PI) theory of tropical cyclones (TCs) provides a\nreasonable estimate of the maximum intensity of a steady-state TC in a\nquiescent environment. The traditional PI theory relies on the symmetric\nneutrality (SN) assumption, where the isolines of absolute angular momentum (M)\nare parallel to the saturation entropy (s*) surfaces within the eyewall\nupdraft. When the SN is not valid, there is currently no quantitative theory\nthat explicitly describes how these surfaces directly relate to the maximum\ntangential wind (vmax) near the surface. In this study, the PI theory is\nrevisited without making the SN assumption. Under non-SN conditions, it is\nfound that the balance between the centrifugal torque and baroclinic torque\nprovides a strong constraint on the vortex structure and balanced intensity.\nSpecifically, it is shown that the gradient between s* and M along constant\ntemperature (T) throughout the saturated eyewall determines the structure of\nthe M surface and the balanced intensity at the low level. The same technique\ncan be applied to obtain the generalized terms that correspond to the\nunbalanced component. It is shown that this generalized vmax equation is the\nnatural extension of the traditional PI formula into the non-symmetric\nneutrality regime. Verifying against axisymmetric simulations, it is shown that\nthe generalized vmax equation can accurately quantify the various contributions\nto vmax during the rapid intensification process. The implications of these\nfindings on the TC rapid intensification, such as the TC inner-core structure\nand the upper-tropospheric mixing process, are examined.",
        "This paper investigates the trajectories of light beams in a Kerr metric,\nwhich describes the gravitational field in the neighborhood of a rotating black\nhole. After reduction by cyclic coordinates, this problem reduces to analysis\nof a Hamiltonian system with two degrees of freedom. A bifurcation diagram is\nconstructed and a classification is made of the types of trajectories of the\nsystem according to the values of first integrals. Relations describing the\nboundary of the shadow of the black hole are obtained for a stationary observer\nwho rotates with an arbitrary angular velocity about the axis of rotation of\nthe black hole.",
        "Following the initial construction of pure supergravity in 1976, various\nmethods were developed to couple supergravity with supersymmetric matter. This\ncontribution to 'Half a century of supergravity' provides a personal\nperspective on the key steps, techniques and results developed in the first\ndecade. These developments form the foundation for numerous applications in\nphenomenology, cosmology and string theories, while also revealing intriguing\nmathematical structures.",
        "We study spinless fermions on a finite chain with nearest-neighbor repulsion\nand in the presence of a Wannier-Stark linearly-varying electric field\npotential. In the absence of the interaction, the eigenstates are localized for\nthe system's sizes larger than the localization length. We present several\nanalytical expressions for the localization length, which is proportional to\nthe inverse of the electric field. Using the density matrix renormalization\ngroup numerical technique, we observe that the ground state exhibits a decrease\nof the occupation on the chain sites from the `bulk', with occupation 1, to the\nvacuum, with occupation 0. The width of this intermediate `edge' region is also\ninversely proportional to the electric field, increasing linearly with the\nstrength of the nearest-neighbor repulsion. For strong interactions, the\noccupations in the intermediate region exhibit a charge density wave. We also\npresent the local density of states for sites in the `edge' region. For the\nnon-interacting case, the spectrum shows an increasing energy-localized\nstructure as the field is increased, which is a consequence of the uniform\nenergy distribution of the localized states (Wannier-Stark ladder). This\nstructure survives for small interactions, and it smears out in the strongly\ninteracting limit. Experimental variations of the slope of the potential (the\nelectric field) on cold atom chains may test these predictions.",
        "UV-upturn galaxies are characterized by unusually excessive flux in the\nfar-ultraviolet (FUV) band, observed in some elliptical galaxies and the bulges\nof disk galaxies. We examine UV-upturn galaxies within the semi-analytic model\nGABE, which embeds the formation of extreme horizontal branch (EHB) stars --\nproposed as key candidates responsible for the UV-upturn phenomenon. We have\nanalyzed all related physical processes, including stellar evolution, initial\nmass functions (IMFs), dust attenuation, galaxy age, metallicity, and binary\nfractions, in an effort to determine which processes play significant roles.\nOur findings reveal two categories of UV-upturn galaxies in the semi-analytic\nmodel, each with distinct formation channels: old metal-rich quenched\nelliptical galaxies, which are intrinsic UV-upturn galaxies induced by EHB\nstars within their old stellar populations, and dusty star-forming galaxies,\nwhich are relatively young and may also be photometrically identified as\nUV-upturn galaxies when accounting for dust attenuation. Dust attenuation\ncontributes to 20%-60% of the UV-upturn galaxies, depending on the specific\ndust attenuation models adopted. With the binary star formation model of EHB\nstars, both of these formation channels exhibit strong preferences for high\nstellar metallicity. The high-mass end slope of the IMFs is found to have a\nmarginal effect, indicating that a universal IMF is adequate for studying the\nUV-upturn phenomenon.",
        "We consider the following chemotaxis system under homogeneous Neumann\nboundary conditions in a smooth, open, bounded domain $\\Omega \\subset\n\\mathbb{R}^n$ with $n \\geq 3$: \\begin{equation*}\n  \\begin{cases}\n  u_t = \\Delta u - \\chi \\nabla \\cdot \\left( \\frac{u}{v^k} \\nabla v \\right) + ru\n- \\mu u^2, & \\text{in } \\Omega \\times (0,T_{\\rm max}),\n  v_t = \\Delta v - \\alpha v + \\beta u, & \\text{in } \\Omega \\times (0,T_{\\rm\nmax}),\n  \\end{cases} \\end{equation*} where $k \\in (0,1)$, and $\\chi, r, \\mu, \\alpha,\n\\beta$ are positive parameters. In this paper, we demonstrate that for suitably\nsmooth initial data, the problem admits a unique nonnegative classical solution\nthat remains globally bounded in time when $\\mu$ is sufficiently large.",
        "We prove that affine maps are uniquely extremal quasiconformal maps on the\ncomplement of a well distribute set in the complex plane answering a conjecture\nfrom \\cite{markovic}. We construct the required Reich sequence using Bergman\nprojections, and meromorphic partitions of unity.",
        "We analyze the available observational data on the radial distribution of gas\nand young stellar populations in the disks of low surface brightness (LSB)\ngalaxies and in the outer regions or the extended disks of normal brightness\n(HSB) galaxies. These cases involve star formation under special conditions of\nlow volume and surface gas density. There is no well-defined boundary between\nthese subgroups of galaxies that we consider, but in non-dwarf LSB galaxies the\nrate of current star formation within the wide range of radial distances\nappears to be higher compared to the outer disks of most of HSB galaxies at\nsimilar values of the surface gas density. The factors that could stimulate the\ncompression of the rarefied gas at the periphery of galaxies are briefly\ndiscussed. Attention is drawn to the idea that the densities of LSB disks\nestimated from their brightness may be underestimated."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment",
        "Revolutionizing Gravitational Potential Analysis: From Clairaut to Lie\n  Groups",
        "Number of partitions of modular integers (with an Appendix by P.\n  Deligne)",
        "A Proof of Lieb--Wehrl Entropy conjecture for $SU(N,1)$",
        "A Bayesian Non-linear Mixed-Effects Model for Accurate Detection of the\n  Onset of Cognitive Decline in Longitudinal Aging Studies",
        "Rationalizability and Monotonocity in Games with Incomplete Information",
        "Predicting the detectability of sulphur-bearing molecules in the solid\n  phase with simulated spectra of JWST instruments",
        "Dialectics of antimicrobial peptides I: common mechanisms of offensive\n  and protecting roles of the peptides",
        "Assessment of the January 2025 Los Angeles County wildfires: A\n  multi-modal analysis of impact, response, and population exposure",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Normalizing flows for SU($N$) gauge theories employing singular value\n  decomposition",
        "White's conjecture for matroids and inner projections",
        "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of\n  Model Prediction and Scientific Understanding of Soil Organic Carbon",
        "Constructing reducibly geometrically finite subgroups of the mapping\n  class group",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain"
      ],
      "abstract":[
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies.",
        "This letter introduces an advanced novel theory for calculating non-linear\nNewtonian hydrostatic perturbations in the density, shape, and gravitational\nfield of fluid stars and planets subjected to external tidal and rotational\nforces. The theory employs a Lie group approach using exponential mappings to\nderive exact differential equations for large gravitational field perturbations\nand the shape function, which describes the finite deformation of the body's\nfigure. This approach lays the foundation for the precise analytic\ndetermination and numerical computation of the induced body's multipole moments\nand Love numbers with any desired degree of accuracy.",
        "For integers $n,k,s$, we give a formula for the number $T(n,k,s)$ of order\n$k$ subsets of the ring $\\mathbb{Z}\/n\\mathbb{Z}$ whose sum of elements is $s$\nmodulo $n$. To do so, we describe explicitly a sequence of matrices $M(k)$, for\npositive integers $k$, such that the size of $M(k)$ is the number of divisors\nof $k$, and for two coprime integers $k_{1},k_{2}$, the matrix $M(k_{1}k_{2})$\nis the Kronecker product of $M(k_{1})$ and $M(k_{2})$. For $s=0, 1, 2$, and for\n$s=k\/2$ when $k$ is even, the sequences $T(n,k,s)$ are related to the number of\nnecklaces with $k$ black beads and $n-k$ white beads, and to Lyndon words. This\nwork begins with empirical determinations of $M(k)$ up to $k=10000$, from which\nwe infer a closed formula that encompasses many entries in the Encyclopedia of\nInteger Sequences. Its proof comes from work on Ramanujan sums, by Ramanathan,\nwith a generalization to wider problems linked to representation theory and\nrecently described by Deligne.",
        "We investigate the sharp functional inequalities for the coherent state\ntransforms of $SU(N,1)$. These inequalities are rooted in Wehrl's definition of\nsemiclassical entropy and his conjecture about its minimum value. Lieb resolved\nthis conjecture in 1978, posing a similar question for Bloch coherent states of\n$SU(2)$. The $SU(2)$ conjecture was settled by Lieb and Solovej in 2014, and\nthe conjecture was extended for a wide class of Lie groups. The generalized\nLieb conjecture has been resolved for several Lie groups, including $SU(N),\\,\nN\\geq2$, $SU(1,1)$, and its $AX+B$ subgroup. With sharp functional inequalities\nfor the coherent state transforms of the group $SU(N,1)$, we confirm this\nLieb-Wehrl entropy conjecture for $SU(N,1),\\, N\\geq2$. Additionally, we explore\nthe Faber-Krahn inequality, which applies to the short-time Fourier transform\nwith a Gaussian window. This inequality was previously proven by Nicola and\nTilli and later extended by Ramos and Tilli to the wavelet transform. In this\npaper, we further extend this result within the framework of the Bergman space\n$\\mathcal A_{\\alpha}$.",
        "Change-point models are frequently considered when modeling phenomena where a\nregime shift occurs at an unknown time. In ageing research, these models are\ncommonly adopted to estimate of the onset of cognitive decline. Yet commonly\nused models present several limitations. Here, we present a Bayesian non-linear\nmixed-effects model based on a differential equation designed for longitudinal\nstudies to overcome some limitations of classical change point models used in\nageing research. We demonstrate the ability of the proposed model to avoid\nbiases in estimates of the onset of cognitive impairment in a simulated study.\nFinally, the methodology presented in this work is illustrated by analysing\nresults from memory tests from older adults who participated in the English\nLongitudinal Study of Ageing.",
        "This paper examines games with strategic complements or substitutes and\nincomplete information, where players are uncertain about the opponents'\nparameters. We assume that the players' beliefs about the opponent's parameters\nare selected from some given set of beliefs. One extreme is the case where\nthese sets only contain a single belief, representing a scenario where the\nplayers' actual beliefs about the parameters are commonly known among the\nplayers. Another extreme is the situation where these sets contain all possible\nbeliefs, representing a scenario where the players have no information about\nthe opponents' beliefs about parameters. But we also allow for intermediate\ncases, where these sets contain some, but not all, possible beliefs about the\nparameters. We introduce an assumption of weakly increasing differences that\ntakes both the choice belief and parameter belief of a player into account.\nUnder this assumption, we demonstrate that greater choice-parameter beliefs\nleads to greater optimal choices. Moreover, we show that the greatest and least\npoint rationalizable choice of a player is increasing in their parameter, and\nthese can be determined through an iterative procedure. In each round of the\niterative procedure, the lowest surviving choice is optimal for the lowest\nchoice-parameter belief, while the greatest surviving choice is optimal for the\nhighest choice-parameter belief.",
        "To date, gas phase observations of sulphur in dense interstellar environments\nhave only constrained the molecular carriers of 1% of its predicted cosmic\nabundance. An additional 5% is known to be locked up in molecular solids in\ndense clouds, leaving the main reservoir of depleted sulphur in the solid phase\nunknown. The spectral resolution and sensitivity of the JWST could make a\nsubstantial difference in detecting part of this missing sulphur, with its\nwavelength coverage that includes vibrational absorption features of the\nS-carriers H2S, OCS, SO2, CS2, SO, CS, and S8. The aim of this study is to\ndetermine whether these molecules may be viable candidates for detection. We\ncarried out new laboratory measurements of the IR absorption spectra of CS2 and\nS8 to update the IR band strength of the most intense CS2 absorption feature at\n6.8 {\\mu}m, as well as to determine that of S8 at 20.3 {\\mu}m for the first\ntime. These data, along with values previously reported in the literature,\nallow us to evaluate which S-bearing species could be potentially detected with\nJWST in interstellar ices. Taking the literature abundances of the major ice\nspecies determined by previous IR observations towards starless cores, LYSOs\nand MYSOs, we generated simulated IR spectra using the characteristics of the\ninstruments on the JWST. Thus, we have been able to establish a case study for\nthree stages of the star formation process. We conclude that the detection of\nS-bearing molecules remains challenging. Despite these obstacles, the detection\nof H2S and potentially SO2 should be possible in regions with favourable\nphysical and chemical conditions. In contrast, S8 would remain undetected.\nAlthough the sensitivity of JWST is insufficient to determine the sulphur\nbudget in the solid state, the detection of an additional icy sulphur compound\n(H2S, SO2) would enable us to elevate our knowledge of sulphur chemistry.",
        "Antimicrobial peptides (AMPs) have intrigued researchers for decades due to\nthe contradiction between their high potential against resistant bacteria and\nthe inability to find a structure-function relationship for the development of\nan effective and non-toxic agent. In the present study and the companion paper\n[Phys. Rev. E (2024)], we performed a comprehensive experimental and\ntheoretical analysis of various aspects of AMP-membrane interactions and\nAMP-induced pore formation. Using the well-known melittin and magainin as\nexamples, we showed, using patch-clamp and fluorescence measurements, that\nthese peptides, even at nanomolar concentrations, modify the membrane by making\nit permeable to protons (and, possibly, water), but not to ions, and protect\nthe membrane from large pore formation after subsequent addition of 20-fold\nhigher concentrations of AMPs. This protective effect is independent of the\nmembrane side (or both sides) of the peptide addition and is determined by the\npeptide-induced deformations of the membrane. Peptides create small,\nH+-permeable pores that incessantly connect the opposing membrane leaflets,\nallowing translocation of peptides and lipids and thus preventing further\ngeneration of large lateral pressure\/tension imbalance. At the same time, such\nan imbalance is a key to the formation of peptide-induced pores at high AMP\nconcentrations, with the main contribution coming from single ion-conducting\nevents rather than stable channel-like structures. Therefore, our results\nsuggest that lowering the AMP concentration, which is a common principle to\nreduce toxicity, may actually make bacteria resistant to AMP. However, a\nprotective pre-treatment with nanomolar concentrations of peptides may be the\nkey to protect eukaryotic cells from the high concentrations of AMPs.",
        "This study presents a comprehensive analysis of four significant California\nwildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts\nthrough multiple dimensions, including land cover change, jurisdictional\nmanagement, structural damage, and demographic vulnerability. Using the\nChebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the\nextent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares.\nOur analysis revealed that shrubland ecosystems were consistently the most\naffected, comprising 57.4-75.8% of burned areas across all events. The\njurisdictional assessment demonstrated varying management complexities, from\nsingular authority (98.7% in the Palisades Fire) to distributed management\nacross multiple agencies. A structural impact analysis revealed significant\ndisparities between urban interface fires (Eaton: 9,869 structures; Palisades:\n8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17\nstructures). The demographic analysis showed consistent gender distributions,\nwith 50.9% of the population identified as female and 49.1% as male.\nWorking-age populations made up the majority of the affected populations,\nranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods.\nThe study identified strong correlations between urban interface proximity,\nstructural damage, and population exposure. The Palisades and Eaton fires\naffected over 20,000 people each, compared to fewer than 500 in rural events.\nThese findings offer valuable insights for the development of targeted wildfire\nmanagement strategies, particularly in wildland urban interface zones, and\nemphasize the need for age- and gender-conscious approaches in emergency\nresponse planning.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "We present a progress report on the use of normalizing flows for generating\ngauge field configurations in pure SU(N) gauge theories. We discuss how the\nsingular value decomposition can be used to construct gauge-invariant\nquantities, which serve as the building blocks for designing gauge-equivariant\ntransformations of SU(N) gauge links. Using this novel approach, we build\nrepresentative models for the SU(3) Wilson action on a \\( 4^4 \\) lattice with\n\\( \\beta = 1 \\). We train these models and provide an analysis of their\nperformance, highlighting the effectiveness of the new technique for\ngauge-invariant transformations. We also provide a comparison between the\nefficiency of the proposed algorithm and the spectral flow of Wilson loops.",
        "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties.",
        "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels.",
        "In this article, we consider qualified notions of geometric finiteness in\nmapping class groups called parabolically geometrically finite (PGF) and\nreducibly geometrically finite (RGF). We examine several constructions of\nsubgroups and determine when they produce a PGF or RGF subgroup. These results\nprovide a variety of new examples of PGF and RGF subgroups. Firstly, we\nconsider the right-angled Artin subgroups constructed by Koberda and\nClay--Leininger--Mangahas, which are generated by high powers of given elements\nof the mapping class group. We give conditions on the supports of these\nelements that imply the resulting right-angled Artin subgroup is RGF. Secondly,\nwe prove combination theorems which provide conditions for when a collection of\nreducible subgroups, or sufficiently deep finite-index subgroups thereof,\ngenerate an RGF subgroup.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b33",
    "start_title":"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network",
    "start_abstract":"We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment",
        "Revolutionizing Gravitational Potential Analysis: From Clairaut to Lie\n  Groups",
        "Number of partitions of modular integers (with an Appendix by P.\n  Deligne)",
        "A Proof of Lieb--Wehrl Entropy conjecture for $SU(N,1)$",
        "A Bayesian Non-linear Mixed-Effects Model for Accurate Detection of the\n  Onset of Cognitive Decline in Longitudinal Aging Studies",
        "Rationalizability and Monotonocity in Games with Incomplete Information",
        "Predicting the detectability of sulphur-bearing molecules in the solid\n  phase with simulated spectra of JWST instruments",
        "Dialectics of antimicrobial peptides I: common mechanisms of offensive\n  and protecting roles of the peptides",
        "Assessment of the January 2025 Los Angeles County wildfires: A\n  multi-modal analysis of impact, response, and population exposure",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Normalizing flows for SU($N$) gauge theories employing singular value\n  decomposition",
        "White's conjecture for matroids and inner projections",
        "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of\n  Model Prediction and Scientific Understanding of Soil Organic Carbon",
        "Constructing reducibly geometrically finite subgroups of the mapping\n  class group",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain"
      ],
      "abstract":[
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies.",
        "This letter introduces an advanced novel theory for calculating non-linear\nNewtonian hydrostatic perturbations in the density, shape, and gravitational\nfield of fluid stars and planets subjected to external tidal and rotational\nforces. The theory employs a Lie group approach using exponential mappings to\nderive exact differential equations for large gravitational field perturbations\nand the shape function, which describes the finite deformation of the body's\nfigure. This approach lays the foundation for the precise analytic\ndetermination and numerical computation of the induced body's multipole moments\nand Love numbers with any desired degree of accuracy.",
        "For integers $n,k,s$, we give a formula for the number $T(n,k,s)$ of order\n$k$ subsets of the ring $\\mathbb{Z}\/n\\mathbb{Z}$ whose sum of elements is $s$\nmodulo $n$. To do so, we describe explicitly a sequence of matrices $M(k)$, for\npositive integers $k$, such that the size of $M(k)$ is the number of divisors\nof $k$, and for two coprime integers $k_{1},k_{2}$, the matrix $M(k_{1}k_{2})$\nis the Kronecker product of $M(k_{1})$ and $M(k_{2})$. For $s=0, 1, 2$, and for\n$s=k\/2$ when $k$ is even, the sequences $T(n,k,s)$ are related to the number of\nnecklaces with $k$ black beads and $n-k$ white beads, and to Lyndon words. This\nwork begins with empirical determinations of $M(k)$ up to $k=10000$, from which\nwe infer a closed formula that encompasses many entries in the Encyclopedia of\nInteger Sequences. Its proof comes from work on Ramanujan sums, by Ramanathan,\nwith a generalization to wider problems linked to representation theory and\nrecently described by Deligne.",
        "We investigate the sharp functional inequalities for the coherent state\ntransforms of $SU(N,1)$. These inequalities are rooted in Wehrl's definition of\nsemiclassical entropy and his conjecture about its minimum value. Lieb resolved\nthis conjecture in 1978, posing a similar question for Bloch coherent states of\n$SU(2)$. The $SU(2)$ conjecture was settled by Lieb and Solovej in 2014, and\nthe conjecture was extended for a wide class of Lie groups. The generalized\nLieb conjecture has been resolved for several Lie groups, including $SU(N),\\,\nN\\geq2$, $SU(1,1)$, and its $AX+B$ subgroup. With sharp functional inequalities\nfor the coherent state transforms of the group $SU(N,1)$, we confirm this\nLieb-Wehrl entropy conjecture for $SU(N,1),\\, N\\geq2$. Additionally, we explore\nthe Faber-Krahn inequality, which applies to the short-time Fourier transform\nwith a Gaussian window. This inequality was previously proven by Nicola and\nTilli and later extended by Ramos and Tilli to the wavelet transform. In this\npaper, we further extend this result within the framework of the Bergman space\n$\\mathcal A_{\\alpha}$.",
        "Change-point models are frequently considered when modeling phenomena where a\nregime shift occurs at an unknown time. In ageing research, these models are\ncommonly adopted to estimate of the onset of cognitive decline. Yet commonly\nused models present several limitations. Here, we present a Bayesian non-linear\nmixed-effects model based on a differential equation designed for longitudinal\nstudies to overcome some limitations of classical change point models used in\nageing research. We demonstrate the ability of the proposed model to avoid\nbiases in estimates of the onset of cognitive impairment in a simulated study.\nFinally, the methodology presented in this work is illustrated by analysing\nresults from memory tests from older adults who participated in the English\nLongitudinal Study of Ageing.",
        "This paper examines games with strategic complements or substitutes and\nincomplete information, where players are uncertain about the opponents'\nparameters. We assume that the players' beliefs about the opponent's parameters\nare selected from some given set of beliefs. One extreme is the case where\nthese sets only contain a single belief, representing a scenario where the\nplayers' actual beliefs about the parameters are commonly known among the\nplayers. Another extreme is the situation where these sets contain all possible\nbeliefs, representing a scenario where the players have no information about\nthe opponents' beliefs about parameters. But we also allow for intermediate\ncases, where these sets contain some, but not all, possible beliefs about the\nparameters. We introduce an assumption of weakly increasing differences that\ntakes both the choice belief and parameter belief of a player into account.\nUnder this assumption, we demonstrate that greater choice-parameter beliefs\nleads to greater optimal choices. Moreover, we show that the greatest and least\npoint rationalizable choice of a player is increasing in their parameter, and\nthese can be determined through an iterative procedure. In each round of the\niterative procedure, the lowest surviving choice is optimal for the lowest\nchoice-parameter belief, while the greatest surviving choice is optimal for the\nhighest choice-parameter belief.",
        "To date, gas phase observations of sulphur in dense interstellar environments\nhave only constrained the molecular carriers of 1% of its predicted cosmic\nabundance. An additional 5% is known to be locked up in molecular solids in\ndense clouds, leaving the main reservoir of depleted sulphur in the solid phase\nunknown. The spectral resolution and sensitivity of the JWST could make a\nsubstantial difference in detecting part of this missing sulphur, with its\nwavelength coverage that includes vibrational absorption features of the\nS-carriers H2S, OCS, SO2, CS2, SO, CS, and S8. The aim of this study is to\ndetermine whether these molecules may be viable candidates for detection. We\ncarried out new laboratory measurements of the IR absorption spectra of CS2 and\nS8 to update the IR band strength of the most intense CS2 absorption feature at\n6.8 {\\mu}m, as well as to determine that of S8 at 20.3 {\\mu}m for the first\ntime. These data, along with values previously reported in the literature,\nallow us to evaluate which S-bearing species could be potentially detected with\nJWST in interstellar ices. Taking the literature abundances of the major ice\nspecies determined by previous IR observations towards starless cores, LYSOs\nand MYSOs, we generated simulated IR spectra using the characteristics of the\ninstruments on the JWST. Thus, we have been able to establish a case study for\nthree stages of the star formation process. We conclude that the detection of\nS-bearing molecules remains challenging. Despite these obstacles, the detection\nof H2S and potentially SO2 should be possible in regions with favourable\nphysical and chemical conditions. In contrast, S8 would remain undetected.\nAlthough the sensitivity of JWST is insufficient to determine the sulphur\nbudget in the solid state, the detection of an additional icy sulphur compound\n(H2S, SO2) would enable us to elevate our knowledge of sulphur chemistry.",
        "Antimicrobial peptides (AMPs) have intrigued researchers for decades due to\nthe contradiction between their high potential against resistant bacteria and\nthe inability to find a structure-function relationship for the development of\nan effective and non-toxic agent. In the present study and the companion paper\n[Phys. Rev. E (2024)], we performed a comprehensive experimental and\ntheoretical analysis of various aspects of AMP-membrane interactions and\nAMP-induced pore formation. Using the well-known melittin and magainin as\nexamples, we showed, using patch-clamp and fluorescence measurements, that\nthese peptides, even at nanomolar concentrations, modify the membrane by making\nit permeable to protons (and, possibly, water), but not to ions, and protect\nthe membrane from large pore formation after subsequent addition of 20-fold\nhigher concentrations of AMPs. This protective effect is independent of the\nmembrane side (or both sides) of the peptide addition and is determined by the\npeptide-induced deformations of the membrane. Peptides create small,\nH+-permeable pores that incessantly connect the opposing membrane leaflets,\nallowing translocation of peptides and lipids and thus preventing further\ngeneration of large lateral pressure\/tension imbalance. At the same time, such\nan imbalance is a key to the formation of peptide-induced pores at high AMP\nconcentrations, with the main contribution coming from single ion-conducting\nevents rather than stable channel-like structures. Therefore, our results\nsuggest that lowering the AMP concentration, which is a common principle to\nreduce toxicity, may actually make bacteria resistant to AMP. However, a\nprotective pre-treatment with nanomolar concentrations of peptides may be the\nkey to protect eukaryotic cells from the high concentrations of AMPs.",
        "This study presents a comprehensive analysis of four significant California\nwildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts\nthrough multiple dimensions, including land cover change, jurisdictional\nmanagement, structural damage, and demographic vulnerability. Using the\nChebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the\nextent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares.\nOur analysis revealed that shrubland ecosystems were consistently the most\naffected, comprising 57.4-75.8% of burned areas across all events. The\njurisdictional assessment demonstrated varying management complexities, from\nsingular authority (98.7% in the Palisades Fire) to distributed management\nacross multiple agencies. A structural impact analysis revealed significant\ndisparities between urban interface fires (Eaton: 9,869 structures; Palisades:\n8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17\nstructures). The demographic analysis showed consistent gender distributions,\nwith 50.9% of the population identified as female and 49.1% as male.\nWorking-age populations made up the majority of the affected populations,\nranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods.\nThe study identified strong correlations between urban interface proximity,\nstructural damage, and population exposure. The Palisades and Eaton fires\naffected over 20,000 people each, compared to fewer than 500 in rural events.\nThese findings offer valuable insights for the development of targeted wildfire\nmanagement strategies, particularly in wildland urban interface zones, and\nemphasize the need for age- and gender-conscious approaches in emergency\nresponse planning.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "We present a progress report on the use of normalizing flows for generating\ngauge field configurations in pure SU(N) gauge theories. We discuss how the\nsingular value decomposition can be used to construct gauge-invariant\nquantities, which serve as the building blocks for designing gauge-equivariant\ntransformations of SU(N) gauge links. Using this novel approach, we build\nrepresentative models for the SU(3) Wilson action on a \\( 4^4 \\) lattice with\n\\( \\beta = 1 \\). We train these models and provide an analysis of their\nperformance, highlighting the effectiveness of the new technique for\ngauge-invariant transformations. We also provide a comparison between the\nefficiency of the proposed algorithm and the spectral flow of Wilson loops.",
        "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties.",
        "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels.",
        "In this article, we consider qualified notions of geometric finiteness in\nmapping class groups called parabolically geometrically finite (PGF) and\nreducibly geometrically finite (RGF). We examine several constructions of\nsubgroups and determine when they produce a PGF or RGF subgroup. These results\nprovide a variety of new examples of PGF and RGF subgroups. Firstly, we\nconsider the right-angled Artin subgroups constructed by Koberda and\nClay--Leininger--Mangahas, which are generated by high powers of given elements\nof the mapping class group. We give conditions on the supports of these\nelements that imply the resulting right-angled Artin subgroup is RGF. Secondly,\nwe prove combination theorems which provide conditions for when a collection of\nreducible subgroups, or sufficiently deep finite-index subgroups thereof,\ngenerate an RGF subgroup.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation",
    "start_abstract":"In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b36",
        "b33"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
        "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
      ],
      "categories":[
        "cs.CV",
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity\n  Grouping",
        "Coded Deep Learning: Framework and Algorithm",
        "Universal programmable and self-configuring optical filter",
        "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review",
        "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
        "Contracting low degree points on curves",
        "Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction",
        "A study of the Antlion Random Walk",
        "Evolving Skeletons: Motion Dynamics in Action Recognition",
        "B-Call: Integrating Ideological Position and Political Cohesion in\n  Legislative Voting Models",
        "Foliated Plateau problems, geometric rigidity and equidistribution of\n  closed $k$-surfaces",
        "Evidence for J\/$\\psi$ suppression in incoherent photonuclear production",
        "A Machine Learning Approach for Design of Frequency Selective Surface\n  based Radar Absorbing Material via Image Prediction",
        "Detection of chiral spin fluctuations driven by frustration in Mott\n  insulators",
        "Norms in equivariant homotopy theory",
        "MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language\n  Models for Biomedical In-Context Learning",
        "Robust Conformal Outlier Detection under Contaminated Reference Data",
        "Local damage detection in rolling element bearings based on a Single\n  Ensemble Empirical Mode Decomposition",
        "Single-crystalline CrSb(0001) thin films grown by dc magnetron\n  co-sputtering",
        "Parental Guidance: Efficient Lifelong Learning through Evolutionary\n  Distillation",
        "A Fully Self-Synchronized Control for Hybrid Series-Parallel\n  Electronized Power Networks",
        "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond",
        "Navigating Gender Disparities in Communication Research Leadership:\n  Academic Recognition, Career Development, and Compensation",
        "ASKAP and VLASS search for a radio-continuum counterpart of\n  ultra-high-energy neutrino event KM3-230213A",
        "Effective textures from a $[SU(3)]^3$ flavored scalar sector",
        "MAUCell: An Adaptive Multi-Attention Framework for Video Frame\n  Prediction"
      ],
      "abstract":[
        "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https:\/\/github.com\/AIGNLAI\/GDDSG.",
        "The success of deep learning (DL) is often achieved with large models and\nhigh complexity during both training and post-training inferences, hindering\ntraining in resource-limited settings. To alleviate these issues, this paper\nintroduces a new framework dubbed ``coded deep learning'' (CDL), which\nintegrates information-theoretic coding concepts into the inner workings of DL,\nto significantly compress model weights and activations, reduce computational\ncomplexity at both training and post-training inference stages, and enable\nefficient model\/data parallelism. Specifically, within CDL, (i) we first\npropose a novel probabilistic method for quantizing both model weights and\nactivations, and its soft differentiable variant which offers an analytic\nformula for gradient calculation during training; (ii) both the forward and\nbackward passes during training are executed over quantized weights and\nactivations, eliminating most floating-point operations and reducing training\ncomplexity; (iii) during training, both weights and activations are entropy\nconstrained so that they are compressible in an information-theoretic sense\nthroughout training, thus reducing communication costs in model\/data\nparallelism; and (iv) the trained model in CDL is by default in a quantized\nformat with compressible quantized weights, reducing post-training inference\nand storage complexity. Additionally, a variant of CDL, namely relaxed CDL\n(R-CDL), is presented to further improve the trade-off between validation\naccuracy and compression though requiring full precision in training with other\nadvantageous features of CDL intact. Extensive empirical results show that CDL\nand R-CDL outperform the state-of-the-art algorithms in DNN compression in the\nliterature.",
        "We propose an approach to integrated optical spectral filtering that allows\narbitrary programmability, can compensate automatically for imperfections in\nfilter fabrication, allows multiple simultaneous and separately programmable\nfilter functions on the same input, and can configure itself automatically to\nthe problem of interest, for example to filter or reject multiple arbitrarily\nchosen frequencies. The approach exploits splitting the input light into an\narray of multiple waveguides of different lengths that then feed a programmable\ninterferometer array that can also self-configure. It can give spectral\nresponse similar to arrayed waveguide gratings but offers many other filtering\nfunctions, as well as supporting other structures based on non-redundant arrays\nfor precise spectral filtering. Simultaneous filtering also allows, for the\nfirst time to our knowledge, an automatic measurement of the temporal coherency\nmatrix and physical separation into the Karhunen-Lo\\`eve expansion of\ntemporally partially coherent light fields.",
        "In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1\/\\sqrt{n}$.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
        "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https:\/\/huggingface.co\/datasets\/bytedance-research\/ToolHop.",
        "The main result of this article is that all but finitely many points of small\nenough degree on a curve can be written as a pullback of a smaller degree\npoint. The main theorem has several corollaries that yield improvements on\nresults of Kadets and Vogt, Khawaja and Siksek, and Vojta under a slightly\nstronger assumption on the degree of the points.",
        "Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.",
        "This paper combines two significant areas of political science research:\nmeasuring individual ideological position and cohesion. Although both\napproaches help analyze legislative behaviors, no unified model currently\nintegrates these dimensions. To fill this gap, the paper proposes a methodology\ncalled B-Call that combines ideological positioning with voting cohesion,\ntreating votes as random variables. The model is empirically validated using\nroll-call data from the United States, Brazil, and Chile legislatures, which\nrepresent diverse legislative dynamics. The analysis aims to capture the\ncomplexities of voting and legislative behaviors, resulting in a\ntwo-dimensional indicator. This study addresses gaps in current legislative\nvoting models, particularly in contexts with limited party control.",
        "In this note, we survey recent advances in the study of dynamical properties\nof the space of surfaces with constant curvature in three-dimensional manifolds\nof negative sectional curvature. We interpret this space as a two-dimensional\nanalogue of the geodesic flow and explore the extent to which the thermodynamic\nproperties of the latter can be generalized to the surface setting.\nAdditionally, we apply this theory to derive geometric rigidity results,\nincluding the rigidity of the hyperbolic marked area spectrum.",
        "According to quantum chromodynamics, at sufficiently high energy, the\nstructure of hadrons reveals a dynamic equilibrium between gluon splitting and\ngluon recombination -- a phenomenon known as saturation. The process of\ndiffractive photonuclear production of a J\/$\\psi$ vector meson provides a\ndirect insight into the gluon composition of hadrons. The J\/$\\psi$ production\nas a function of momentum transferred in the interaction, quantified by the\nMandelstam-$t$ variable, serves as an excellent probe for studying the\nstructure of hadrons within the impact-parameter plane, because different\nranges in $t$ are sensitive to the dynamics of the gluon field at varying\nspatial size scales. The ALICE collaboration has measured the energy dependence\nof incoherent photonuclear production of J\/$\\psi$ mesons off lead ions, at\n$\\sqrt{s_{\\rm NN}} = 5.02$ TeV, for three Mandelstam-$t$ intervals. The energy\ndependence of the photonuclear cross section at the highest $|t|$ range\nmeasured, $(0.81< |t| <1.44)$ GeV$^2$, is sensitive to subnucleonic structures\nof the Pb target. The increase of the cross section with energy at large $|t|$\nshows evidence of suppression with respect to the increase seen at low $|t|$.\nThe observed pattern of the energy evolution in data is similar to that of\ngluon saturation models.",
        "The paper presents an innovative methodology for designing frequency\nselective surface (FSS) based radar absorbing materials using machine learning\n(ML) technique. In conventional electromagnetic design, unit cell dimensions of\nFSS are used as input and absorption coefficient is then predicted for a given\ndesign. In this paper, absorption coefficient is considered as input to ML\nmodel and image of FSS unit cell is predicted. Later, this image is used for\ngenerating the FSS unit cell parameters. Eleven different ML models are studied\nover a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e.\n(a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid\nsearch regression, (d) Random Forest regression, (e) Decision tree\nclassification, and (f) Decision tree regression) show training accuracy more\nthan 90%. The absorption coefficients with varying frequencies of these\npredicted images are subsequently evaluated using commercial electromagnetic\nsolver. The performance of these ML models is encouraging, and it can be used\nfor accelerating design and optimization of high performance FSS based radar\nabsorbing material for advanced electromagnetic applications in future.",
        "Topologically ordered states, such as chiral spin liquids, have been proposed\nas candidates that host fractionalized excitations. However, detecting chiral\ncharacter or proximity to these non-trivial states remains a challenge.\nResonant Raman scattering can be a powerful tool for detecting chiral\nfluctuations, as the $A_{2g}$ channel probes excitations with broken\ntime-reversal symmetry and local chiral order. Here, we use exact\ndiagonalization to characterize the resonant $A_{2g}$ channel, alongside\ntwo-magnon scattering in $B_{1g}$ and $E_g$ channels, for the Hubbard model on\nlattices with increasing levels of geometric spin frustration, where tuning the\nincident energy near the Mott gap reveals strong chiral spin excitation\nintensity. Increased spin frustration in the Mott insulator results in an\noverall softening of the Raman $A_{2g}$ response, indicating a tendency toward\nlow energy chiral-chiral fluctuations in Mott insulators with magnetic\nfrustration and proximity to chiral spin liquid states that can potentially be\ntuned by external perturbations.",
        "We show that the $\\infty$-category of normed algebras in genuine $G$-spectra,\nas introduced by Bachmann-Hoyois, is modelled by strictly commutative algebras\nin $G$-symmetric spectra for any finite group $G$. We moreover provide an\nanalogous description of Schwede's ultra-commutative global ring spectra in\nhigher categorical terms.\n  Using these new descriptions, we exhibit the $\\infty$-category of\nultra-commutative global ring spectra as a partially lax limit of the\n$\\infty$-categories of genuine $G$-spectra for varying $G$, in analogy with the\nnon-multiplicative comparison of Nardin, Pol, and the second author.\n  Along the way, we establish various new results in parametrized higher\nalgebra, which we hope to be of independent interest.",
        "Objective: To optimize in-context learning in biomedical natural language\nprocessing by improving example selection. Methods: We introduce a novel\nmulti-mode retrieval-augmented generation (MMRAG) framework, which integrates\nfour retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2)\nTop Mode, retrieving the most relevant examples based on similarity; (3)\nDiversity Mode, ensuring variation in selected examples; and (4) Class Mode,\nselecting category-representative examples. This study evaluates MMRAG on three\ncore biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction\n(RE), and Text Classification (TC). The datasets used include BC2GM for gene\nand protein mention recognition (NER), DDI for drug-drug interaction extraction\n(RE), GIT for general biomedical information extraction (RE), and HealthAdvice\nfor health-related text classification (TC). The framework is tested with two\nlarge language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever,\nMedCPT, BGE-Large) to assess performance across different retrieval strategies.\nResults: The results from the Random mode indicate that providing more examples\nin the prompt improves the model's generation performance. Meanwhile, Top mode\nand Diversity mode significantly outperform Random mode on the RE (DDI) task,\nachieving an F1 score of 0.9669, a 26.4% improvement. Among the three\nretrievers tested, Contriever outperformed the other two in a greater number of\nexperiments. Additionally, Llama 2 and Llama 3 demonstrated varying\ncapabilities across different tasks, with Llama 3 showing a clear advantage in\nhandling NER tasks. Conclusion: MMRAG effectively enhances biomedical\nin-context learning by refining example selection, mitigating data scarcity\nissues, and demonstrating superior adaptability for NLP-driven healthcare\napplications.",
        "Conformal prediction is a flexible framework for calibrating machine learning\npredictions, providing distribution-free statistical guarantees. In outlier\ndetection, this calibration relies on a reference set of labeled inlier data to\ncontrol the type-I error rate. However, obtaining a perfectly labeled inlier\nreference set is often unrealistic, and a more practical scenario involves\naccess to a contaminated reference set containing a small fraction of outliers.\nThis paper analyzes the impact of such contamination on the validity of\nconformal methods. We prove that under realistic, non-adversarial settings,\ncalibration on contaminated data yields conservative type-I error control,\nshedding light on the inherent robustness of conformal methods. This\nconservativeness, however, typically results in a loss of power. To alleviate\nthis limitation, we propose a novel, active data-cleaning framework that\nleverages a limited labeling budget and an outlier detection model to\nselectively annotate data points in the contaminated reference set that are\nsuspected as outliers. By removing only the annotated outliers in this\n``suspicious'' subset, we can effectively enhance power while mitigating the\nrisk of inflating the type-I error rate, as supported by our theoretical\nanalysis. Experiments on real datasets validate the conservative behavior of\nconformal methods under contamination and show that the proposed data-cleaning\nstrategy improves power without sacrificing validity.",
        "A Single Ensemble Empirical Mode Decomposition (SEEMD) is proposed for\nlocating the damage in rolling element bearings. The SEEMD does not require a\nnumber of ensembles from the addition or subtraction of noise every time while\nprocessing the signals. The SEEMD requires just a single sifting process of a\nmodified raw signal to reduce the computation time significantly. The other\nadvantage of the SEEMD method is its success in dealing with non-Gaussian or\nnon-stationary perturbing signals. In SEEMD, initially, a fractional Gaussian\nnoise (FGN) is added to the raw signal to emphasize on high frequencies of the\nsignal. Then, a convoluted white Gaussian noise is multiplied to the resulting\nsignal which changes the spectral content of the signal which helps in\nextraction of the weak periodic signal. Finally, the obtained signal is\ndecomposed by using a single sifting process. The proposed methodology is\napplied to the raw signals obtained from the mining industry. These signals are\ndifficult to analyze since cyclic impulsive components are obscured by noise\nand other interference. Based on the results, the proposed method can\neffectively detect the fault where the signal of interest (SOI) has been\nextracted with good quality.",
        "The recent discovery of altermagnetism has sparked renewed interest in the\ngrowth of epitaxial films of the NiAs-phase polymorph of CrSb. This paper\ndescribes the magnetron sputtering-based fabrication and characterization of\nhigh-quality single crystalline CrSb(0001) thin films supported by an\nisostructural non-magnetic PtSb buffer. X-ray diffraction and scanning\ntransmission electron microscopy show that the films are phase-pure and possess\na very high crystalline quality (mosaicity ~0.05 deg), while also being free of\nextended crystallographic defects. Both scanning electron microscopy and atomic\nforce microscopy confirm their smooth and homogeneous topography. Additionally,\nthe elemental composition of our films was found to be close to stoichiometric\nvia electron probe microanalysis and X-ray fluorescence. Thus, the developed\nsamples represent an ideal platform for further investigation of the material\nproperties of CrSb.",
        "Developing robotic agents that can perform well in diverse environments while\nshowing a variety of behaviors is a key challenge in AI and robotics.\nTraditional reinforcement learning (RL) methods often create agents that\nspecialize in narrow tasks, limiting their adaptability and diversity. To\novercome this, we propose a preliminary, evolution-inspired framework that\nincludes a reproduction module, similar to natural species reproduction,\nbalancing diversity and specialization. By integrating RL, imitation learning\n(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents\ncontinuously through complex tasks. This approach promotes adaptability,\ninheritance of useful traits, and continual learning. Agents not only refine\ninherited skills but also surpass their predecessors. Our initial experiments\nshow that this method improves exploration efficiency and supports open-ended\nlearning, offering a scalable solution where sparse reward coupled with diverse\nterrain environments induces a multi-task setting.",
        "The hybrid series-parallel system is the final form of the power\nelectronics-enabled power system, which combines the advantages of both series\nand parallel connections. Although self-synchronization of parallel-type and\nseries-type systems is well known, self-synchronization of hybrid systems\nremains unrevealed. To fill in this gap, a fully self-synchronized control for\nhybrid series-parallel system is proposed in this paper. Based on the\nself-synchronization mechanism of power angle in parallel-type system and power\nfactor angle in series-type system, a decentralized control strategy by\nintegration of power droop and power factor angle droop can realize\nself-synchronization and power balancing of each module in the hybrid system.",
        "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.",
        "This study examines gender disparities in communication research through\ncitation metrics, authorship patterns, team composition, and faculty salaries.\nUsing data from 62,359 papers across 121 communication journals, we find that\nwhile female authors are increasingly represented, citation gaps persist, with\nsole-authored papers by women receiving fewer citations than those by men,\nespecially in smaller teams. Team composition analysis reveals a tendency\ntoward gender homophily, with single-gender teams being more common. In top\nU.S. communication journals, female authors face underrepresentation and\ncitation disparities favoring male authors. Salary analysis from leading U.S.\npublic universities shows that female faculty earn lower salaries at the\nAssistant Professor level, though disparities lessen at higher ranks. These\nfindings highlight the need for greater efforts to promote gender equity\nthrough inclusive collaboration, equitable citation practices, and fair\ncompensation.",
        "We present the results of an Australian Square Kilometre Array Pathfinder\n(ASKAP) 944 MHz and Very Large Array Sky Survey (VLASS) 3~GHz search for a\nradio-continuum counterpart of the recent ultra-high-energy (UHE) neutrino\nevent, KM3-230213A. Using (ASKAP), we catalog 1052 radio sources within the\n1.5$^\\circ$ radius search area (68% certainty region) around the particle's\ncalculated origin, 10 of which we classify as blazar candidates based on their\nradio spectra. The most prominent radio source in the search area is the nearby\nspiral galaxy UGCA 127 (nicknamed Phaedra, From Greek: $\\phi\\alpha\ni\\delta\\rho\\alpha$, a Cretan princess of Greek Mythology, derived from\nPhaidros, Greek: ${\\phi}{\\alpha}{\\iota}{\\delta}{\\rho}o{\\varsigma}$, meaning\n'bright'.). Its non-thermal radio spectrum classifies it as a non-blazar active\ngalactic nucleus (AGN). We also present an extended radio source, WISEA\nJ061715.89-075455.4 (nicknamed Hebe, From Greek: $H{\\beta}{\\eta}$, the Greek\ngoddess of youth.), located only ~7' from the geometric center of the search\narea, with a very unusual highly polarized compact component. Finally, we\npresent a strong radio source, EMU J062248-072246 (nicknamed Narcissus, From\nGreek $N{\\alpha}{\\rho}{\\kappa}{\\iota}{\\sigma}{\\sigma}o{\\zeta}$ was a\nself-absorbed hunter from Thespiae in Boeotia.), which has a maximum\nself-absorption spectral slope of +2.5 at low frequencies, and exhibits ~25%\nflux density variability over the ~5-year VLASS 3~GHz survey.",
        "Current constraints on flavor-changing neutral currents (FCNCs) strongly\nindicate that any new physics emerging at the 1-10 TeV scale must adhere to the\nMinimal Flavor Violation (MFV) principle, where Yukawa couplings are the sole\nsources of flavor violation. In this work, we present a model inspired by a\ngauged $SU(3)$ flavor symmetry that dynamically generates leptonic Yukawa\nmatrices through effective operators. The model incorporates a scalar sector\nwith two sets of flavons, characterized by their vacuum expectation values\n(VEVs), which govern the suppression scale of the Yukawa couplings and the\nhierarchy of neutrino masses. By leveraging phenomenologically viable Yukawa\ntextures, we derive restrictions on the flavon VEVs and demonstrate the\ncompatibility of the model with experimental neutrino oscillation data.\nFurthermore, the model predicts at least one neutrino mass to be strongly\nsuppressed, consistent with the normal mass ordering and experimental upper\nbounds. This framework provides a robust mechanism for dynamically generating\nneutrino masses and mixing while addressing key challenges in leptonic flavor\nphysics, such as FCNC suppression and CP-violating phases.",
        "Temporal sequence modeling stands as the fundamental foundation for video\nprediction systems and real-time forecasting operations as well as anomaly\ndetection applications. The achievement of accurate predictions through\nefficient resource consumption remains an ongoing issue in contemporary\ntemporal sequence modeling. We introduce the Multi-Attention Unit (MAUCell)\nwhich combines Generative Adversarial Networks (GANs) and spatio-temporal\nattention mechanisms to improve video frame prediction capabilities. Our\napproach implements three types of attention models to capture intricate motion\nsequences. A dynamic combination of these attention outputs allows the model to\nreach both advanced decision accuracy along with superior quality while\nremaining computationally efficient. The integration of GAN elements makes\ngenerated frames appear more true to life therefore the framework creates\noutput sequences which mimic real-world footage. The new design system\nmaintains equilibrium between temporal continuity and spatial accuracy to\ndeliver reliable video prediction. Through a comprehensive evaluation\nmethodology which merged the perceptual LPIPS measurement together with classic\ntests MSE, MAE, SSIM and PSNR exhibited enhancing capabilities than\ncontemporary approaches based on direct benchmark tests of Moving MNIST, KTH\nAction, and CASIA-B (Preprocessed) datasets. Our examination indicates that\nMAUCell shows promise for operational time requirements. The research findings\ndemonstrate how GANs work best with attention mechanisms to create better\napplications for predicting video sequences."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study",
    "start_abstract":"In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "What is a cell type, really? The quest to categorize life's myriad forms."
      ],
      "abstract":[
        "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Teacher-student training improves accuracy and efficiency of machine\n  learning inter-atomic potentials",
        "Partitions of unity and barycentric algebras",
        "Adiabatic Pumping of Orbital Magnetization by Spin Precession",
        "Non-positive energy quasidistributions in coherent collision models",
        "New properties of length-extremals in free step-2 rank-4 Carnot groups",
        "Covariant photon current",
        "PyClustrPath: An efficient Python package for generating clustering\n  paths with GPU acceleration",
        "Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced\n  Physical Demand",
        "Period Analysis of Eclipsing Cataclysmic Variable Stars",
        "OpenGERT: Open Source Automated Geometry Extraction with Geometric and\n  Electromagnetic Sensitivity Analyses for Ray-Tracing Propagation Models",
        "Black holes inside cosmic voids",
        "Poisson Vertex Algebras and Three-Dimensional Gauge Theory",
        "Investigating the Effects of Atmospheric Stratification on Coronal\n  Active Region Field Modelling",
        "Spall failure of alumina at high-strain rates using femtosecond laser\n  experiments and high-fidelity molecular dynamics simulations",
        "Bounded conciseness in the space of marked groups",
        "An open-closed Deligne-Mumford field theory associated to a Lagrangian\n  submanifold",
        "2-Adic quantum mechanics, continuous-time quantum walks, and the space\n  discreteness",
        "Photometric Decision-Making During the Dawn Choruses of Cicadas",
        "Self-ion irradiation effects on nanoindentation-induced plasticity of\n  crystalline iron: A joint experimental and computational study",
        "Exploring Large Language Models (LLMs) through interactive Python\n  activities",
        "Towards robust gravitational wave detections from individual\n  supermassive black hole binaries",
        "Elucidating the Dark Energy and Dark Matter Phenomena Within the\n  Scale-Invariant Vacuum (SIV) Paradigm",
        "Synthetic Data for Portfolios: A Throw of the Dice Will Never Abolish\n  Chance",
        "Misconceptions in Neutrino Oscillations in presence of a non-Unitary\n  Mixing",
        "Kernels, Distances, and Bridges",
        "Fabrication of Fibers with Complex Features Using Thermal Drawing of\n  3D-Printed Preforms",
        "Stueckelberg field and Cosmology",
        "Helfrich cylinders -- instabilities, bifurcations and amplitude\n  equations",
        "An exposition of recent list-size bounds of FRS Codes"
      ],
      "abstract":[
        "Machine learning inter-atomic potentials (MLIPs) are revolutionizing the\nfield of molecular dynamics (MD) simulations. Recent MLIPs have tended towards\nmore complex architectures trained on larger datasets. The resulting increase\nin computational and memory costs may prohibit the application of these MLIPs\nto perform large-scale MD simulations. Here, we present a teacher-student\ntraining framework in which the latent knowledge from the teacher (atomic\nenergies) is used to augment the students' training. We show that the\nlight-weight student MLIPs have faster MD speeds at a fraction of the memory\nfootprint compared to the teacher models. Remarkably, the student models can\neven surpass the accuracy of the teachers, even though both are trained on the\nsame quantum chemistry dataset. Our work highlights a practical method for\nMLIPs to reduce the resources required for large-scale MD simulations.",
        "Barycentric coordinates provide solutions to the problem of expressing an\nelement of a compact convex set as a convex combination of a finite number of\nextreme points of the set. They have been studied widely within the geometric\nliterature, typically in response to the demands of interpolation, numerical\nanalysis and computer graphics. In this note we bring an algebraic perspective\nto the problem, based on barycentric algebras. We focus on the discussion of\nrelations between different subclasses of partitions of unity, one arising in\nthe context of barycentric coordinates, based on the tautological map\nintroduced by Guessab.",
        "We propose adiabatic pumping of orbital magnetization driven by coherent spin\nprecession, facilitating the rectification of this precession. The orbital\nmagnetization originates from the adiabatic evolution of valence electrons with\na topological bulk contribution expressed as a Chern-Simons form. When the\nprecession cone angle of spin $\\mathbf{S}$ is small, the resulting\nmagnetization is proportional to $\\mathbf{S}\\times \\dot{\\mathbf{S}}$,\ncontributing to the magnon Zeeman effect. With a large cone angle, the\nmagnetization can reach its natural unit, $e\/T$, in an antiferromagnetic\ntopological insulator with $e$ as the elementary charge and $T$ as the\nprecession period. This significant magnetization is related to the global\nproperties of the electronic geometric phases in the parameter space spanned by\n$\\mathbf{S}$ and momentum $\\mathbf{k}$. When the pumped magnetization is\ninhomogeneous, induced by spin textures or electronic topological phase\ndomains, a dissipationless charge current is also pumped. At last, we discuss\nthe boundary contributions from the spin-driving edge states, which are\nintricately linked to the gauge-dependent quantum uncertainty of the\nChern-Simons form.",
        "We determine the Kirkwood-Dirac quasiprobability (KDQ) distribution\nassociated to the stochastic instances of internal energy variations for the\nquantum system and environment particles in coherent Markovian collision\nmodels. In the case the interactions between the quantum system and the\nparticles do not conserve energy, the KDQ of the non-energy-preserving\nstochastic work is also derived. These KDQ distributions can account for\nnon-commutativity, and return the unperturbed average values and variances for\na generic interaction-time, and generic local initial states of the quantum\nsystem and environment particles. Using this nonequilibrium-physics approach,\nwe certify the conditions under which the collision process of the model\nexhibits quantum traits, and we quantify the rate of energy exchanged by the\nquantum system by looking at the variance of the KDQ energy distributions.\nFinally, we propose an experimental test of our results on a superconducting\nquantum circuit implementing a qubit system, with microwave photons\nrepresenting the environment particles.",
        "In the free, step-2, rank-4 sub-Riemannian Carnot group, we give a clean\nexpression for length-extremals, we provide an explicit equation for conjugate\npoints, we relate it with the conjectured cut-locus of the origin. Finally, we\ngive some upper estimates for the cut-time of extremals.",
        "An inhomogeneous continuity equation for the photon four-current operator,\n$\\widehat{J}_{p}$, was derived in [M. Hawton, Phys. Rev. A, 109, 062221\n(2024)]. If the electromagnetic potential operator, $\\widehat{A}% =\\left(\n\\widehat{\\phi}\/c,\\widehat{\\mathbf{A}}\\right) $, is covariant then\n$\\widehat{J}_{p}$ is covariant and the continuity equation is invariant. Here\nwe start with the standard Lagrangian in a Lorentz invariant gauge and quantize\nboth transverse and longitudinal modes. The scalar potential\n$\\widehat{\\phi}=c\\widehat{A}_{\\Vert}$ is not independently second quantized, so\nall modes have positive definite norm. The continuity equation is generalized\nby separating the material source current into a nonabsorbing term describing\npropagation in a lossless transmission line and localized single photon\nemission and detection terms that do not require nonlocal separation of\ntransverse and longitudinal modes.",
        "Convex clustering is a popular clustering model without requiring the number\nof clusters as prior knowledge. It can generate a clustering path by\ncontinuously solving the model with a sequence of regularization parameter\nvalues. This paper introduces {\\it PyClustrPath}, a highly efficient Python\npackage for solving the convex clustering model with GPU acceleration. {\\it\nPyClustrPath} implements popular first-order and second-order algorithms with a\nclean modular design. Such a design makes {\\it PyClustrPath} more scalable to\nincorporate new algorithms for solving the convex clustering model in the\nfuture. We extensively test the numerical performance of {\\it PyClustrPath} on\npopular clustering datasets, demonstrating its superior performance compared to\nthe existing solvers for generating the clustering path based on the convex\nclustering model. The implementation of {\\it PyClustrPath} can be found at:\nhttps:\/\/github.com\/D3IntOpt\/PyClustrPath.",
        "We present a hybrid brain-machine interface (BMI) that integrates\nsteady-state visually evoked potential (SSVEP)-based EEG and facial EMG to\nimprove multimodal control and mitigate fatigue in assistive applications.\nTraditional BMIs relying solely on EEG or EMG suffer from inherent limitations;\nEEG-based control requires sustained visual focus, leading to cognitive\nfatigue, while EMG-based control induces muscular fatigue over time. Our system\ndynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP\nsignals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize\ncontrol based on task demands. In a virtual turtle navigation task, the hybrid\nsystem achieved task completion times comparable to an EMG-only approach, while\n90% of users reported reduced or equal physical demand. These findings\ndemonstrate that multimodal BMI systems can enhance usability, reduce strain,\nand improve long-term adherence in assistive technologies.",
        "We have performed a study of the orbital properties of seven eclipsing\ncataclysmic variable (CV) binary systems by analyzing photometric time series\nfrom the Transiting Exoplanet Survey Satellite (TESS). We employed Python code\nto determine the eclipse epochs and orbital periods for each system, and\nconstructed O-C diagrams from observed and predicted eclipse epochs. By\nanalyzing the O-C diagrams of our target CVs, we have constrained values for\nchanges in orbital period with time. Our targets include a sample of sources\nfrom each class of non-magnetic, eclipsing CVs: dwarf novae variables, Z Cam\ntype, and U Gem subclasses. We include in our study classical novae variables,\nnova-like variables (including the VY Scl and UX UMa subclasses), and recurrent\nnovae variable stars. We approached this project with goals of developing time\nseries analysis techniques for future undergraduate-level studies of eclipsing\nCVs, and how they may contribute to the understanding of their orbital\nevolution.",
        "Accurate RF propagation modeling in urban environments is critical for\ndeveloping digital spectrum twins and optimizing wireless communication\nsystems. We introduce OpenGERT, an open-source automated Geometry Extraction\ntool for Ray Tracing, which collects and processes terrain and building data\nfrom OpenStreetMap, Microsoft Global ML Building Footprints, and USGS elevation\ndata. Using the Blender Python API, it creates detailed urban models for\nhigh-fidelity simulations with NVIDIA Sionna RT. We perform sensitivity\nanalyses to examine how variations in building height, position, and\nelectromagnetic material properties affect ray-tracing accuracy. Specifically,\nwe present pairwise dispersion plots of channel statistics (path gain, mean\nexcess delay, delay spread, link outage, and Rician K-factor) and investigate\nhow their sensitivities change with distance from transmitters. We also\nvisualize the variance of these statistics for selected transmitter locations\nto gain deeper insights. Our study covers Munich and Etoile scenes, each with\n10 transmitter locations. For each location, we apply five types of\nperturbations: material, position, height, height-position, and all combined,\nwith 50 perturbations each. Results show that small changes in permittivity and\nconductivity minimally affect channel statistics, whereas variations in\nbuilding height and position significantly alter all statistics, even with\nnoise standard deviations of 1 meter in height and 0.4 meters in position.\nThese findings highlight the importance of precise environmental modeling for\naccurate propagation predictions, essential for digital spectrum twins and\nadvanced communication networks. The code for geometry extraction and\nsensitivity analyses is available at github.com\/serhatadik\/OpenGERT\/.",
        "This study examines the gravitational and thermodynamic properties of static,\nspherically symmetric black holes within cosmic voids -- vast underdense\nregions of the universe. By deriving a novel solution based on a universal\ndensity profile for voids, we analyze its spacetime structure, which reveals\ntwo horizons: One of the black hole and the other related to the de Sitter-like\nbehavior. As the void approaches a perfect vacuum, the black hole horizon\ndiminishes, tending to that of the Schwarzschild solution, while the outer\nhorizon increases. We also study the solution stability via sound speed of the\nfluid, as well as the thermodynamic properties, including Hawking temperature,\nevaporation time, entropy, and specific heat. Our results show that as the void\nempties, the Hawking temperature rises, shortening evaporation times. The\nentropy follows the area's law and specific heat exhibits a minimum for a given\nblack hole size, indicating a thermal transition and highlighting the role of\nvoids in the black hole evolution. These findings offer new insights into the\nrelationship between local gravitational collapse and large-scale cosmic\nstructure, enhancing our understanding of the black hole behavior in underdense\nenvironments. We also provide a glimpse of a potential thermodynamic\ninteraction between the event horizon and the cosmological horizon.",
        "We introduce a mixed holomorphic-topological gauge theory in three dimensions\nassociated to a (freely generated) Poisson vertex algebra. The\n$\\lambda$-bracket of the PVA plays the role of the structure constants of the\ngauge algebra and the gauge invariance of the theory holds if and only if the\n$\\lambda$-bracket Jacobi identity is satisfied. We show that the\nholomorphic-topological symmetry of the theory enhances to full topological\nsymmetry if the Poisson vertex algebra contains a Virasoro element. We outline\nexamples associated to PVAs of $\\mathcal{W}$-type and demonstrate their\nconnections to various versions of $3d$ gravity. We expect the\nthree-dimensional Poisson sigma model to play an important role in the\ndeformation quantization of Poisson vertex algebras.",
        "Understanding the evolution of the complex magnetic fields found in solar\nactive regions is an active area of research. There are numerous models for\nsuch fields which range in their complexity due to the number of known physical\neffects included in them, the one common factor being they all extrapolate the\nfield up from the photosphere. In this study we focus on the fact that, above\nthe photosphere, and below the corona, lies the relatively cool and dense\nchromosphere -- which is often neglected in coronal models due to it being\ncomparatively thin and difficult hard to model. We isolate and examine the\neffect including this boundary layer has on a 2.5D class of driven MHD models\nof an active region eruption. We find that it can result in significant changes\nto the dynamics of an erupting field far higher in the atmosphere than the\nchromosphere itself, generally delaying eruption and increasing the magnetic\nenergy released in each eruption. We also test whether these effects can be\napproximated using a variation of the more computationally efficient\nmagnetofrictional model, finding a number of simple adaptations of the standard\nmagnetofrictional model capture the effect the chromospheric stratification\nwell.",
        "Ceramic materials are widely used in high-strain-rate applications due to\ntheir exceptional strength-to-weight ratio. However, under these extreme\nconditions, spall failure becomes a critical concern, which is driven by a\nlarge hydrostatic tensile stress state. This study introduces a novel two-laser\nsetup to generate controlled hydrostatic stress states at specific locations\nwithin test specimens. By inducing and manipulating shock wave interactions, we\nachieve large hydrostatic compressive and tensile stresses at very\nhigh-strain-rates, enabling the controlled nucleation and growth of nanovoids\nleading to spall failure. Our experiments demonstrate that shock wave\ninterference can precisely trigger spallation at arbitrary locations in the\nspecimen thickness. To further validate our approach, we investigate alumina\nspall failure using molecular dynamics (MD) simulations with a custom-designed\ngraph neural network potential. The MD results show strong agreement with\nexperimentally estimated spall strength. These findings highlight the potential\nof the two-laser technique as a powerful tool for studying the early stages of\nspall failure in ceramics, paving the way for advanced materials testing\nmethodologies.",
        "We prove that bounded conciseness is a closed property in the space of marked\ngroups. As a consequence, we reformulate a conjecture of Fern\\'andez-Alcober\nand Shumyatsky [7] about conciseness in the class of residually finite groups.",
        "Let $L \\subset X$ be a compact embedded Lagrangian in a compact symplectic\nmanifold. We present the moduli spaces of holomorphic maps of arbitrary genus\nwith boundary on $L$ as a global Kuranishi chart, generalising the work of\nAbouzaid-McLean-Smith and Hirschi-Swaminathan. We use this to define an\nopen-closed Deligne-Mumford theory whose open genus zero part is the Fukaya\n$A_\\infty$ algebra associated to $L$, and whose closed part gives the\nGromov--Witten theory of $X$. Combined with results of Costello, this has\napplications in obtaining Gromov--Witten invariants from the Fukaya category.",
        "Using techniques of p-adic analysis, it is possible to formulate a rigorous\nversion of the quantum mechanics (QM), in the sense of Dirac-von Neumann,\nconsistent with the existence of the Planck length. Such a model cannot be\nformulated if we use R^{3} as a model for physical space. The experimental\ntestability of physical theories at the Planck scale is currently impossible.\nHere, we provide an indirect, theoretical argument that shows that the p-adic\nQM has physical content. We show that a large class of Schr\\\"odinger equations\ndescribes the scaling limits of continuous-time quantum walks on graphs\n(stochastic automata). These quantum walks appear as fundamental tools in\nquantum computing. We conjecture that this interpretation is valid in a general\nframework. The `new theory' does not have Lorentz symmetry, and the Einstein\ncausality is violated. This fact does not contradict the so-called\nno-communication theorem; such a result requires as a primary hypothesis that\nR^{4} be a valid model for space-time at the Planck scale. Thus, the\nno-communication theorem under the discreteness of the space is an open\nproblem.",
        "We report the first quantitative study of the onset of dawn choruses of\ncicadas in several natural habitats. A time-frequency analysis of the\nacoustical signals is used to define an order parameter for the development of\ncollective singing. The ensemble of recordings reveals that the chorus onset\ntimes accurately track the changing sunrise times over the course of many\nweeks, occurring within civil twilight at a solar elevation of -$3.8^\\circ \\pm\n0.2^\\circ$. Despite day-to-day variations in the amplitude of fully developed\nchoruses, the order parameter data collapse to a common sigmoidal curve when\nscaled by those amplitudes and shifted by the onset time, revealing a\ncharacteristic rise time of ~60 s for a chorus to reach saturation amplitude.\nThe results are used to obtain the cumulative distribution function of singing\nas a function of ground illumination, from which is obtained a generalized\nsusceptibility which exhibits a narrow peak with a half-width of $\\sim\\! 12\\%$.\nThe variance of the order parameter exhibits a similar peak, suggesting that a\ngeneralized fluctuation-dissipation theorem holds for this system. A model of\ndecision-making under ramps of a control parameter is developed and can achieve\na quantitative match to the data. It suggest that sharpness of the\nsusceptibility peak reflects cooperative decision-making arising from acoustic\ncommunication.",
        "In this paper, experimental work is supported by multi-scale numerical\nmodeling to investigate nanomechanical response of pristine and ion irradiated\nwith Fe2+ ions with energy 5 MeV high purity iron specimens by nanoindentation\nand Electron Backscatter Diffraction. The appearance of a sudden displacement\nburst that is observed during the loading process in the load-displacement\ncurves is connected with increased shear stress in a small subsurface volume\ndue to dislocation slip activation and mobilization of pre-existing\ndislocations by irradiation. The molecular dynamics (MD) and 3D-discrete\ndislocation dynamics (3D-DDD) simulations are applied to model geometrically\nnecessary dislocations (GNDs) nucleation mechanisms at early stages of\nnanoindentation test; providing an insight to the mechanical response of the\nmaterial and its plastic instability and are in a qualitative agreement with\nGNDs density mapping images. Finally, we noted that dislocations and defects\nnucleated are responsible the material hardness increase, as observed in\nrecorded load-displacement curves and pop-ins analysis.",
        "This paper presents an approach to introduce physics students to the basic\nconcepts of Large Language Models (LLMs) using Python-based activities in\nGoogle Colab. The teaching strategy integrates active learning strategies and\ncombines theoretical ideas with practical, physics-related examples. Students\nengage with key technical concepts, such as word embeddings, through hands-on\nexploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot\nof attention in 2019 for its ability to generate coherent and plausible text\nfrom simple prompts.\n  The activities highlight how words acquire meaning and how LLMs predict\nsubsequent tokens by simulating simplified scenarios related to physics. By\nfocusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles\nunderlying modern LLMs, such as semantic representation and contextual\nprediction. Through interactive experimenting in Google Colab, students observe\nthe relationship between model parameters (such as temperature) in GPT-2 and\noutput behaviour, understand scaling laws relating data quantity to model\nperformance, and gain practical insights into the predictive capabilities of\nLLMs. This approach allows students to begin to understand how these systems\nwork by linking them to physics concepts - systems that will shape their\nacademic studies, professional careers and roles in society.",
        "The recent discovery of the stochastic gravitational-wave background via\npulsar timing arrays will likely be followed by the detection of individual\nblack hole binaries that stand out above the background. However, to\nconfidently claim the detection of an individual binary, we need not only more\nand better data, but also more sophisticated analysis techniques. In this\npaper, we develop two new approaches that can help us more robustly ascertain\nif a candidate found by a search algorithm is indeed an individual supermassive\nblack hole binary. One of these is a coherence test that directly compares the\nfull signal model to an incoherent version of that. The other is a model\nscrambling approach that builds null distributions of our detection statistic\nand compares that with the measured value to quantify our confidence in signal\ncoherence. Both of these rely on finding the coherence between pulsars\ncharacteristic to gravitational waves from a binary system. We test these\nmethods on simple simulated datasets and find that they work well in correctly\nidentifying both true gravitational waves and false positives. However, as\nexpected for such a flexible and simple signal model, confidently identifying\nsignal coherence is significantly harder than simply finding a candidate in\nmost scenarios. Our analyses also indicate that the confidence with which we\ncan identify a true signal depends not only on the signal-to-noise ratio, but\nalso on the number of contributing pulsars and the amount of frequency\nevolution shown by the signal.",
        "The enigmatic phenomenon of dark energy (DE) is regarded as the elusive\nentity driving the accelerated expansion of our Universe. A plausible candidate\nfor DE is the non-zero Einstein Cosmological Constant $\\Lambda_{E}$ manifested\nas a constant energy density of the vacuum, yet it seemingly defies\ngravitational effects. In this work, we interpret the non-zero $\\Lambda_{E}$\nthrough the lens of scale-invariant cosmology. We revisit the conformal scale\nfactor $\\lambda$ and its defining equations within the Scale-Invariant Vacuum\n(SIV) paradigm. Furthermore, we address the profound problem of the missing\nmass across galactic and extragalactic scales by deriving an MOND-like\nrelation, $g \\sim \\sqrt{a_0\\,g_N}$, within the SIV context. Remarkably, the\nvalues obtained for $\\Lambda_{E}$ and the MOND fundamental acceleration, $a_0$,\nalign with observed magnitudes, specifically, $a_0 \\approx 10^{-10} \\,\n\\mathrm{m} \\, \\mathrm{s}^{-2}$ and $\\Lambda_{E} \\approx 1.8 \\times 10^{-52} \\,\n\\mathrm{m}^{-2}$. Moreover, we propose a novel early dark energy term,\n$\\tilde{T}_{\\mu\\nu} \\sim \\kappa H$, within the SIV paradigm, which holds\npotential relevance for addressing the Hubble tension.\n  Keywords: cosmology; theory; dark energy; dark matter; MOND; Weyl integrable\ngeometry.",
        "Simulation methods have always been instrumental in finance, and data-driven\nmethods with minimal model specification, commonly referred to as generative\nmodels, have attracted increasing attention, especially after the success of\ndeep learning in a broad range of fields. However, the adoption of these models\nin financial applications has not kept pace with the growing interest, probably\ndue to the unique complexities and challenges of financial markets. This paper\naims to contribute to a deeper understanding of the limitations of generative\nmodels, particularly in portfolio and risk management. To this end, we begin by\npresenting theoretical results on the importance of initial sample size, and\npoint out the potential pitfalls of generating far more data than originally\navailable. We then highlight the inseparable nature of model development and\nthe desired use case by touching on a paradox: generic generative models\ninherently care less about what is important for constructing portfolios (in\nparticular the long-short ones). Based on these findings, we propose a pipeline\nfor the generation of multivariate returns that meets conventional evaluation\nstandards on a large universe of US equities while being compliant with\nstylized facts observed in asset returns and turning around the pitfalls we\npreviously identified. Moreover, we insist on the need for more delicate\nevaluation methods, and suggest, through an example of mean-reversion\nstrategies, a method designed to identify poor models for a given application\nbased on regurgitative training, i.e. retraining the model using the data it\nhas itself generated, which is commonly referred to in statistics as\nidentifiability.",
        "Deviations from unitarity of the CKM matrix in the quark sector are\nconsidered excellent windows to probe physics beyond the Standard Model. In its\nleptonic counterpart, the PMNS matrix, these searches are particularly\nmotivated, as the new physics needed to generate neutrino masses often leads to\nnon-unitary mixing among the standard neutrinos. It is then interesting to\nconsider how neutrino oscillations are affected in such scenario. This simple\nquestion is, however, subject to several subtleties: What is the correct way to\ndefine oscillation probabilities for a non-unitary mixing matrix? Do these\nprobabilities add up to one? Does a non-unitary mixing matrix lead to\nobservable flavor transitions at zero distance? What is the interplay between\nunitarity constraints obtained from neutrino oscillations and from electroweak\nprecision data? This work aims to shed light on these issues and to clarify the\ncorresponding misconceptions commonly found in the literature. We also compile\nupdated bounds from neutrino oscillation searches to compare with those from\nflavour and electroweak precision observables.",
        "The purpose of this paper is to study more general real-valued functions of\ntwo variables than just metrics on a set X. We concentrate mainly on the\nclasses of distances and almost distances. We also introduce the notion of a\nbridge on the disjoint union of two sets and show that it induces a symmetric\ndistance on the disjoint union.",
        "High-aspect-ratio polymer materials are widely utilized in applications\nranging from everyday materials such as clothing to specialized equipment in\nindustrial and medical fields. Traditional fabrication methods, such as\nextrusion and molding, face challenges in integrating diverse materials and\nachieving complex geometries. Additionally, these methods are limited in their\nability to provide low-cost and rapid prototyping, which are critical for\nresearch and development processes. In this work, we investigated the use of\ncommercially available 3D printers to fabricate fiber preforms, which were\nsubsequently thermally drawn into fibers. By optimizing 3D printing parameters,\nwe achieved the fabrication of fibers with diameters as small as 200 um having\ncomplex shapes, with features down to a few microns. We demonstrated the\nversatility of this method by fabricating fibers from diverse set of materials,\nsuch as fibers with different stiffnesses and fibers with magnetic\ncharacteristics, which are beneficial for developing tendon-driven and\nmagnetically actuated robotic fibers. In addition, by designing novel preform\ngeometries, we produced tapered fibers and fibers with interlocking mechanisms,\nalso tailored for use in medical steerable catheter applications. These\nadvancements highlight the scalability and versatility of this approach,\noffering a robust platform for producing high-precision polymer fibers for\ndiverse applications.",
        "Stueckelberg introduced an axion like scalar field to provide mass to the\ngauge electromagnetic field without breaking gauge invariance. This can be\nconsidered as a precursor to the spontaneously broken abelian Higgs model. We\nwill consider its role in cosmology to provide a novel candidate to the dark\nmatter question. In addition its implications to deeper issues will be pointed\nout.",
        "Combining local bifurcation analysis with numerical continuation and\nbifurcation methods we study bifurcations from cylindrical vesicles described\nby the Helfrich equation with volume and area constraints, with a prescribed\nperiodicity along the cylindrical axis. The bifurcating solutions are in two\nmain classes, axisymmetric (pearling), and non-axisymmetric (coiling, buckling,\nand wrinkling), and depending on the spontaneous curvature and the prescribed\nperiodicity along the cylinder axis we obtain different stabilities of the\nbifurcating branches, and different secondary bifurcations.",
        "In the last year, there have been some remarkable improvements in the\ncombinatorial list-size bounds of Folded Reed Solomon codes and multiplicity\ncodes. Starting from the work on Kopparty, Ron-Zewi, Saraf and Wootters (SIAM\nJ. Comput. 2023) (and subsequent simplifications due to Tamo (IEEE Trans.\nInform. Theory 2024), we have had dramatic improvements in the list-size bounds\nof FRS codes due to Srivastava (SODA 2025) and Chen & Zhang (STOC 2025). In\nthis note, we give a short exposition of these three results (Tamo, Srivastava\nand Chen-Zhang)."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"What is a cell type, really? The quest to categorize life's myriad forms.",
    "start_abstract":"The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
      ],
      "abstract":[
        "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based\n  Tiling Generation",
        "A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning",
        "Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf",
        "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization",
        "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "Parallelizing Multi-objective A* Search",
        "The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes",
        "Algorithmical Aspects of Some Bio Inspired Operations",
        "Proximal Flow Inspired Multi-Step Methods",
        "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization",
        "Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving",
        "One-Loop QCD Corrections to $\\bar{u}d \\rightarrow t\\bar{t}W$ at\n  $\\mathcal{O}(\\varepsilon^2)$",
        "Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches",
        "Realization of Two-dimensional Discrete Time Crystals with Anisotropic\n  Heisenberg Coupling",
        "Dissipative quantum phase transitions monitored by current fluctuations",
        "High-frequency coronal Alfv\\'enic waves observed with DKIST\/Cryo-NIRSP",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Controlling Large Language Models Through Concept Activation Vectors",
        "Publish on Ping: A Better Way to Publish Reservations in Memory\n  Reclamation for Concurrent Data Structures",
        "Design and Benchmarks for Emulating Kondo Dynamics on a Quantum Chip",
        "A note on the Sauvageot density principle",
        "Web Phishing Net (WPN): A scalable machine learning approach for\n  real-time phishing campaign detection",
        "Prompting in the Dark: Assessing Human Performance in Prompt Engineering\n  for Data Labeling When Gold Labels Are Absent",
        "An Analysis for Reasoning Bias of Language Models with Small\n  Initialization",
        "Disordered Weyl semimetal as an array of coupled Hubbard chains",
        "Process-based Self-Rewarding Language Models",
        "Isogeny graphs with level structures arrising from the Verschiebung map",
        "Labeling abelian varieties over finite fields"
      ],
      "abstract":[
        "As an artistic aid in tiled level design, Constraint Based Tiling Generation\n(CBTG) algorithms can help to automatically create level realizations from a\nset of tiles and placement constraints. Merrell's Modify in Blocks Model\nSynthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as\nConstraint Based Tiling Generation (CBTG) algorithms that work well for many\nscenarios but have limitations in problem size, problem setup and solution\nbiasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling\nGeneration algorithm, that can handle large problem sizes, requires minimal\nassumptions for setup and can help mitigate solution biasing. POMS attempts to\nresolve indeterminate grid regions by trying to progressively realize\nsub-blocks, performing a stochastic boundary erosion on previously resolved\nregions should sub-block resolution fail. We highlight the results of running a\nreference implementation on different tile sets and discuss a tile correlation\nlength, implied by the tile constraints, and its role in choosing an\nappropriate block size to aid POMS in successfully finding grid realizations.",
        "Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.",
        "In this paper, we give an explicit chain map, which induces the algebra\nisomorphism between the Hochschild cohomology ${\\bf HH}^{\\bullet}(B)$ and the\n$H$-invariant subalgebra ${\\bf H}^{\\bullet}(A, B)^{H}$ under two mild\nhypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$\nis an $H$-Galois extension of $A$. In particular, the smash product $B=A\\#H$\nalways satisfies the mild hypotheses. The isomorphism between ${\\bf\nHH}^{\\bullet}(A\\#H)$ and ${\\bf H}^{\\bullet}(A, A\\#H)^{H}$ generalizes the\nclassical result of group actions. As an application, Hochschild cohomology and\ncup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin\nHopf algebra are computed.",
        "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.",
        "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
        "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
        "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension.",
        "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
        "This thesis investigates three biologically inspired operations:\nprefix-suffix duplication, bounded prefix-suffix duplication, and\nprefix-suffix-square completion. Duplication, a common genetic mutation,\ninvolves repeating DNA sequences and is modeled here as formal operations on\nwords. The prefix-suffix duplication generates non-context-free languages, even\nfrom simple initial words. To better reflect biological processes, we propose a\nbounded variant that limits duplication length, resolving unsolved problems and\naligning with biochemical realities.\n  We also introduce the prefix-suffix-square completion operation, which\ngenerates squares at sequence ends. This operation enables the generation of\ninfinite words such as Fibonacci, Period-doubling, and Thue-Morse, which\ncontain squares but avoid higher exponent repetitions, highlighting unique\nstructural properties. In contrast, prefix-suffix duplication cannot generate\ncertain infinite words, such as Thue-Morse, but can produce cube-free words.\n  Additionally, we address the detection of gapped repeats and\npalindromes-structures important in DNA and RNA analysis. These involve\nrepeating or reversed factors flanking a central gap. Previous studies imposed\nconstraints on gap length or arm-gap relationships; we extend this by solving\nthe problem in three novel settings. This work advances theoretical insights\ninto biologically inspired operations and their computational applications in\ngenetic modeling.",
        "We investigate a family of approximate multi-step proximal point methods,\nframed as implicit linear discretizations of gradient flow. The resulting\nmethods are multi-step proximal point methods, with similar computational cost\nin each update as the proximal point method. We explore several optimization\nmethods where applying an approximate multistep proximal points method results\nin improved convergence behavior. We also include convergence analysis for the\nproposed method in several problem settings: quadratic problems, general\nproblems that are strongly or weakly (non)convex, and accelerated results for\nalternating projections.",
        "We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.",
        "The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems",
        "We present a computation of the one-loop QCD corrections to top-quark pair\nproduction in association with a $W$ boson, including terms up to order\n$\\varepsilon^2$ in dimensional regularization. Providing a first glimpse into\nthe complexity of the corresponding two-loop amplitude, this result is a first\nstep towards a description of this process at next-to-next-to-leading order\n(NNLO) in QCD. We perform a tensor decomposition and express the corresponding\nform factors in terms of a basis of independent special functions with compact\nrational coefficients, providing a structured framework for future\ndevelopments. In addition, we derive an explicit analytic representation of the\nform factors, valid up to order $\\varepsilon^0$, expressed in terms of\nlogarithms and dilogarithms. For the complete set of special functions\nrequired, we obtain a semi-numerical solution based on generalized power series\nexpansion.",
        "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
        "A discrete time crystal (DTC) is the paradigmatic example of a phase of\nmatter that occurs exclusively in systems out of equilibrium. This phenomenon\nis characterized by the spontaneous symmetry breaking of discrete\ntime-translation and provides a rich playground to study a fundamental question\nin statistical physics: what mechanism allows for driven quantum systems to\nexhibit emergent behavior that deviates from their counterparts with\ntime-independent evolution? Unlike equilibrium phases, DTCs exhibit macroscopic\nmanifestations of coherent quantum dynamics, challenging the conventional\nnarrative that thermodynamic behavior universally erases quantum signatures.\nHowever, due to the difficulty of simulating these systems with either\nclassical or quantum computers, previous studies have been limited to a set of\nmodels with Ising-like couplings -- and mostly only in one dimension -- thus\nprecluding our understanding of the existence (or not) of DTCs in models with\ninteractions that closely align with what occurs in nature. In this work, by\ncombining the latest generation of IBM quantum processors with state-of-the-art\ntensor network methods, we are able to demonstrate the existence of a DTC in a\ntwo-dimensional system governed by anisotropic Heisenberg interactions. Our\ncomprehensive analysis reveals a rich phase diagram encompassing spin-glass,\nergodic, and time-crystalline phases, highlighting the tunability of these\nphases through multiple control parameters. Crucially, our results emphasize\nthe interplay of initialization, interaction anisotropy, and driving protocols\nin stabilizing the DTC phase. By extending the study of Floquet matter beyond\nsimplified models, we lay the groundwork for exploring how driven systems\nbridge the gap between quantum coherence and emergent non-equilibrium\nthermodynamics.",
        "Dissipative phase transitions (DPT) are defined by sudden changes in the\nphysical properties of nonequilibrium open quantum systems and they present\ncharacteristics that have no analogue in closed and thermal systems. Several\nmethods to detect and characterize DPT have been suggested in the literature,\nthe most famous of which -- the $\\textit{Liouvillian gap}$ -- can be derived\nfrom a spectral analysis of the Liouvillian super-operator that governs the\ncomplex interplay between coherent and dissipative dynamics. Here, we consider\nthe $\\textit{output current}$, defined as the average total quantum jumps per\nunit time between the open quantum system and the environment. We propose that\noutput current fluctuations, and in particular their dynamical correlations,\ntheir power spectrum, and their characteristic timescale can provide valuable\ninformation about DPT, confirming a dramatic change of behavior at the critical\npoint. We validate our proposal using the dissipative XYZ model and the\nnonlinear driven-dissipative Kerr model, showing good agreement with previous\nestimates of the location of the critical point. Compared to previous\napproaches, our proposal could be already experimentally tested in optical\nsystems, providing a practical method to detect criticality in quantum open\nsystems.",
        "The presence and nature of low-frequency (0.1-10~mHz) Alfv\\'enic waves in the\ncorona has been established over the last decade, with many of these results\ncoming from coronagraphic observations of the infrared Fe XIII line. The\nCryo-NIRSP instrument situated at DKIST has recently begun acquiring science\nquality data of the same Fe XIII line, with at least a factor of 9 improvement\nin spatial resolution, a factor 30 increase in temporal resolution and an\nincrease in signal-to-noise, when compared to the majority of previously\navailable data. Here we present an analysis of 1~s cadence sit-and-stare data\nfrom Cryo-NIRSP, examining the Doppler velocity fluctuations associated with\nthe Fe XIII 1074~nm coronal line. We are able to confirm previous results of\nAlfv\\'enic waves in the corona as well as explore a new frequency regime. The\ndata reveals that the power law behaviour of the Doppler velocity power\nspectrum extends to higher frequencies. This result appears to challenge some\nmodels of photospheric-driven Alfv\\'enic waves that predict a lack of high\nfrequency wave power in the corona due to strong chromospheric damping.\nMoreover, the high-frequency waves do not transport as much energy as their\nlow-frequency counterparts, with less time-averaged energy per frequency\ninterval. We are also able to confirm the incompressible nature of the\nfluctuations with little coherence between the line amplitude and Doppler\nvelocity time-series.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.",
        "Safe memory reclamation techniques that utilize per read reservations, such\nas hazard pointers, often cause significant overhead in traversals of linked\nconcurrent data structures. This is primarily due to the need to announce a\nreservation, and fence to enforce appropriate ordering, before each read. In\nread-intensive workloads, this overhead is amplified because, even if\nrelatively little memory reclamation actually occurs, the full overhead of\nreserving records is still incurred while traversing data structures.\n  In this paper, we propose a novel memory reclamation technique by combining\nPOSIX signals and delayed reclamation, introducing a publish-on-ping approach.\nThis method eliminates the need to make reservations globally visible before\nuse. Instead, threads privately track which records they are accessing, and\nshare this information on demand with threads that intend to reclaim memory.\nThe approach can serve as a drop-in replacement for hazard pointers and hazard\neras. Furthermore, the capability to retain reservations during traversals in\ndata structure operations and publish them on demand facilitates the\nconstruction of a variant of hazard pointers (EpochPOP). This variant uses\nepochs to approach the performance of epoch-based reclamation in the common\ncase where threads are not frequently delayed (while retaining the robustness\nof hazard pointers).\n  Our publish-on-ping implementations based on hazard pointers (HP) and hazard\neras, when applied to various data structures, exhibit significant performance\nimprovements. The improvements across various workloads and data structures\nrange from 1.2X to 4X over the original HP, up to 20% compared to a heavily\noptimized HP implementation similar to the one in the Folly open-source\nlibrary, and up to 3X faster than hazard eras. EpochPOP delivers performance\nsimilar to epoch-based reclamation while providing stronger guarantees.",
        "Motivated by recent advances in digital quantum simulation and the overall\nprospective of solving correlated many-electron problems using quantum\nalgorithms, we design a gate-based quantum circuit that emulates the dynamics\nof the Kondo impurity model. We numerically determine the impurity\nmagnetization, entanglement between impurity and fermionic sites and energy as\na function of time (i.e.~circuit depth) for various initial states and find\nuniversal long-time dynamics. We complement the numerical simulations for\nmoderate system size with an asymptotically exact analytical solution that is\neffective in the limit of large system sizes and for starting states\ncorresponding to a filled Fermi sea. This work opens up the perspective of\nstudying the dynamics of electronic quantum many-body states on quantum chips\nof the NISQ era.",
        "In this short note, we address a gap in the proof of Sauvageot's density\nprinciple, which was pointed out in a paper by Nelson-Venkatesh.",
        "Phishing is the most prevalent type of cyber-attack today and is recognized\nas the leading source of data breaches with significant consequences for both\nindividuals and corporations. Web-based phishing attacks are the most frequent\nwith vectors such as social media posts and emails containing links to phishing\nURLs that once clicked on render host systems vulnerable to more sinister\nattacks. Research efforts to detect phishing URLs have involved the use of\nsupervised learning techniques that use large amounts of data to train models\nand have high computational requirements. They also involve analysis of\nfeatures derived from vectors including email contents thus affecting user\nprivacy. Additionally, they suffer from a lack of resilience against evolution\nof threats especially with the advent of generative AI techniques to bypass\nthese systems as with AI-generated phishing URLs. Unsupervised methods such as\nclustering techniques have also been used in phishing detection in the past,\nhowever, they are at times unscalable due to the use of pair-wise comparisons.\nThey also lack high detection rates while detecting phishing campaigns. In this\npaper, we propose an unsupervised learning approach that is not only fast but\nscalable, as it does not involve pair-wise comparisons. It is able to detect\nentire campaigns at a time with a high detection rate while preserving user\nprivacy; this includes the recent surge of campaigns with targeted phishing\nURLs generated by malicious entities using generative AI techniques.",
        "Millions of users prompt large language models (LLMs) for various tasks, but\nhow good are people at prompt engineering? Do users actually get closer to\ntheir desired outcome over multiple iterations of their prompts? These\nquestions are crucial when no gold-standard labels are available to measure\nprogress. This paper investigates a scenario in LLM-powered data labeling,\n\"prompting in the dark,\" where users iteratively prompt LLMs to label data\nwithout using manually-labeled benchmarks. We developed PromptingSheet, a\nGoogle Sheets add-on that enables users to compose, revise, and iteratively\nlabel data through spreadsheets. Through a study with 20 participants, we found\nthat prompting in the dark was highly unreliable-only 9 participants improved\nlabeling accuracy after four or more iterations. Automated prompt optimization\ntools like DSPy also struggled when few gold labels were available. Our\nfindings highlight the importance of gold labels and the needs, as well as the\nrisks, of automated support in human prompt engineering, providing insights for\nfuture tool design.",
        "Transformer-based Large Language Models (LLMs) have revolutionized Natural\nLanguage Processing by demonstrating exceptional performance across diverse\ntasks. This study investigates the impact of the parameter initialization scale\non the training behavior and task preferences of LLMs. We discover that smaller\ninitialization scales encourage models to favor reasoning tasks, whereas larger\ninitialization scales lead to a preference for memorization tasks. We validate\nthis reasoning bias via real datasets and meticulously designed anchor\nfunctions. Further analysis of initial training dynamics suggests that specific\nmodel components, particularly the embedding space and self-attention\nmechanisms, play pivotal roles in shaping these learning biases. We provide a\ntheoretical framework from the perspective of model training dynamics to\nexplain these phenomena. Additionally, experiments on real-world language tasks\ncorroborate our theoretical insights. This work enhances our understanding of\nhow initialization strategies influence LLM performance on reasoning tasks and\noffers valuable guidelines for training models.",
        "We demonstrate that a disordered magnetic Weyl semimetal may be mapped onto a\ntwo-dimensional array of coupled replicated Hubbard chains, where the Hubbard\n$U$ is directly related to the variance of the disorder potential. This is a\nthree-dimensional generalization of a similar mapping of the two-dimensional\nquantum Hall plateau transition to a one-dimensional Hubbard chain. We\ndemonstrate that this mapping leads to the conclusion that the Weyl semimetal\nbecomes a diffusive metal with a nonzero density of states at arbitrarily weak\ndisorder, in agreement with recent work. We also discuss the absence of\nlocalization in strongly disordered Weyl semimetals from the viewpoint of this\nmapping.",
        "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
        "We enhance an isogeny graph of elliptic curves by incorporating level\nstructures defined by bases of the kernels of iterates of the Verschiebung map.\nWe extend several previous results on isogeny graphs with level structures\ndefined by geometric points to these graphs. Firstly, we prove that these\ngraphs form $\\mathbb{Z}_p$-towers of graph coverings as the power of the\nVerschiebung map varies. Secondly, we prove that the connected components of\nthese graphs display a volcanic structure.",
        "We describe a deterministic process to associate a practical, permanent label\nto isomorphism classes of abelian varieties defined over finite fields and the\npolarizations they admit, for use in the mathematical literature."
      ]
    }
  },
  {
    "id":2411.01144,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images.",
    "start_abstract":"Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods.",
    "start_categories":[
      "Lung Ultrasound"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A Simple Framework for Contrastive Learning of Visual Representations"
      ],
      "abstract":[
        "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Invisible Labor: The Backbone of Open Source Software",
        "CardioTabNet: A Novel Hybrid Transformer Model for Heart Disease\n  Prediction using Tabular Medical Data",
        "A Multiple Transferable Neural Network Method with Domain Decomposition\n  for Elliptic Interface Problems",
        "Clinical Inspired MRI Lesion Segmentation",
        "Generalized quantum two level model and its application in astrophysics",
        "Algebras of analytic functionals and homological epimorphisms",
        "Formation of filaments and feathers in disc galaxies: Is self-gravity\n  enough?",
        "The State of Post-Hoc Local XAI Techniques for Image Processing:\n  Challenges and Motivations",
        "Network-centric optimal hybrid sensing hole recovery and self-healing in\n  IPV6 WSNs",
        "A spinless crystal for a high-performance solid-state $^{229}$Th nuclear\n  clock",
        "Learned Bayesian Cram\\'er-Rao Bound for Unknown Measurement Models Using\n  Score Neural Networks",
        "Ludwig-Soret microscopy with vibrational photothermal effect",
        "Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of\n  Phase Transitions in Weight Space",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Spectroscopic signatures of biexcitons: A case study in\n  Ruddlesden-Popper lead-halides",
        "The Layered Catalan Monoids: Structure and Determinants",
        "Beyond Interaction Patterns: Assessing Claims of Coordinated Inter-State\n  Information Operations on Twitter\/X",
        "On the time constant of high dimensional first passage percolation,\n  revisited",
        "Assessing Teamwork Dynamics in Software Development Projects",
        "A Lightweight and Secure Deep Learning Model for Privacy-Preserving\n  Federated Learning in Intelligent Enterprises",
        "Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates\n  Algorithm for Protecting Neural Networks",
        "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks",
        "Supersymmetric scale-separated AdS$_3$ vacua of type IIB",
        "MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic\n  Computed Tomography",
        "Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised\n  Disentanglement",
        "CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified\n  Intermediate Representation",
        "The Quantum Internet (Technical Version)",
        "Spectral properties from an efficient analytical representation of the\n  $GW$ self-energy within a multipole approximation",
        "The Economics of Large Language Models: Token Allocation, Fine-Tuning,\n  and Optimal Pricing"
      ],
      "abstract":[
        "Invisible labor is an intrinsic part of the modern workplace, and includes\nlabor that is undervalued or unrecognized such as creating collaborative\natmospheres. Open source software (OSS) is software that is viewable, editable\nand shareable by anyone with internet access. Contributors are mostly\nvolunteers, who participate for personal edification and because they believe\nin the spirit of OSS rather than for employment. Volunteerism often leads to\nhigh personnel turnover, poor maintenance and inconsistent project management.\nThis in turn, leads to a difficulty with sustainability long term. We believe\nthat the key to sustainable management is the invisible labor that occurs\nbehind the scenes. It is unclear how OSS contributors think about the invisible\nlabor they perform or how that affects OSS sustainability. We interviewed OSS\ncontributors and asked them about their invisible labor contributions,\nleadership departure, membership turnover and sustainability. We found that\ninvisible labor is responsible for good leadership, reducing contributor\nturnover, and creating legitimacy for the project as an organization.",
        "The early detection and prediction of cardiovascular diseases are crucial for\nreducing the severe morbidity and mortality associated with these conditions\nworldwide. A multi-headed self-attention mechanism, widely used in natural\nlanguage processing (NLP), is operated by Transformers to understand feature\ninteractions in feature spaces. However, the relationships between various\nfeatures within biological systems remain ambiguous in these spaces,\nhighlighting the necessity of early detection and prediction of cardiovascular\ndiseases to reduce the severe morbidity and mortality with these conditions\nworldwide. We handle this issue with CardioTabNet, which exploits the strength\nof tab transformer to extract feature space which carries strong understanding\nof clinical cardiovascular data and its feature ranking. As a result,\nperformance of downstream classical models significantly showed outstanding\nresult. Our study utilizes the open-source dataset for heart disease prediction\nwith 1190 instances and 11 features. In total, 11 features are divided into\nnumerical (age, resting blood pressure, cholesterol, maximum heart rate, old\npeak, weight, and fasting blood sugar) and categorical (resting ECG, exercise\nangina, and ST slope). Tab transformer was used to extract important features\nand ranked them using random forest (RF) feature ranking algorithm. Ten\nmachine-learning models were used to predict heart disease using selected\nfeatures. After extracting high-quality features, the top downstream model (a\nhyper-tuned ExtraTree classifier) achieved an average accuracy rate of 94.1%\nand an average Area Under Curve (AUC) of 95.0%. Furthermore, a nomogram\nanalysis was conducted to evaluate the model's effectiveness in cardiovascular\nrisk assessment. A benchmarking study was conducted using state-of-the-art\nmodels to evaluate our transformer-driven framework.",
        "The transferable neural network (TransNet) is a two-layer shallow neural\nnetwork with pre-determined and uniformly distributed neurons in the hidden\nlayer, and the least-squares solvers can be particularly used to compute the\nparameters of its output layer when applied to the solution of partial\ndifferential equations. In this paper, we integrate the TransNet technique with\nthe nonoverlapping domain decomposition and the interface conditions to develop\na novel multiple transferable neural network (Multi-TransNet) method for\nsolving elliptic interface problems, which typically contain discontinuities in\nboth solutions and their derivatives across interfaces. We first propose an\nempirical formula for the TransNet to characterize the relationship between the\nradius of the domain-covering ball, the number of hidden-layer neurons, and the\noptimal neuron shape. In the Multi-TransNet method, we assign each subdomain\none distinct TransNet with an adaptively determined number of hidden-layer\nneurons to maintain the globally uniform neuron distribution across the entire\ncomputational domain, and then unite all the subdomain TransNets together by\nincorporating the interface condition terms into the loss function. The\nempirical formula is also extended to the Multi-TransNet and further employed\nto estimate appropriate neuron shapes for the subdomain TransNets, greatly\nreducing the parameter tuning cost. Additionally, we propose a normalization\napproach to adaptively select the weighting parameters for the terms in the\nloss function. Ablation studies and extensive experiments with comparison tests\non different types of elliptic interface problems with low to high contrast\ndiffusion coefficients in two and three dimensions are carried out to\nnumerically demonstrate the superior accuracy, efficiency, and robustness of\nthe proposed Multi-TransNet method.",
        "Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting\npathological tissues in various diseases. Different MRI sequences have\ndifferent contrast mechanisms and sensitivities for different types of lesions,\nwhich pose challenges to accurate and consistent lesion segmentation. In\nclinical practice, radiologists commonly use the sub-sequence feature, i.e. the\ndifference between post contrast-enhanced T1-weighted (post) and\npre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we\npropose a residual fusion method to learn subsequence representation for MRI\nlesion segmentation. Specifically, we iteratively and adaptively fuse features\nfrom pre- and post-contrast sequences at multiple resolutions, using dynamic\nweights to achieve optimal fusion and address diverse lesion enhancement\npatterns. Our method achieves state-of-the-art performances on BraTS2023\ndataset for brain tumor segmentation and our in-house breast MRI dataset for\nbreast lesion segmentation. Our method is clinically inspired and has the\npotential to facilitate lesion segmentation in various applications.",
        "Complicated time-dependent curved spacetime and electric field are involved\nin many astrophysical situations, including the early universe, Hawking\nradiation, the Schwinger effect, and gravitational pair production. In this\nLetter, a generalized quantum two-level model (GQTLM) is developed, which is\napplicable to arbitrary time-dependent curved spacetime and electric field. The\nmodel is found to be consistent with quantum kinetic theory, and is\ncharacterized by its simplicity and versatility. The momentum distribution of\nparticles and the effects of gravitational distortions can be correctly\ndescribed. Quantum properties concerning vortex structures, such as the\nintrinsic orbital angular momentum of particles and antiparticles can also be\nconveniently calculated. The model is expected to significantly advance the\nquantum exploration of the universe. It could refine the prediction of\nprimordial gravitational waves and relevant non-Gaussian signals, extend the\ncalculation of Hawking radiation to general black hole configurations, help to\ndistinguish neutron stars from strange quark stars, and elucidate the\ngravitational pair production mechanism.",
        "It has been proved by the author [arXiv: 2404.19433] that the Arens-Michael\nenvelope of a solvable Lie algebra is a homological epimorphism. We show here\nthat for algebras of analytic functionals on a connected complex Lie group the\nanalogous statement is satisfied without the assumption of solvability, and\nfurthermore the completion homomorphisms of a more general form are also\nhomological epimorphisms, including the envelope with respect to the class of\nBanach PI-algebras.",
        "Context. Dense filaments\/feathers are kpc-scale dusty features present in\nnearby main sequence galaxies. Distinct from the spiral arms, filaments\nconstitute a major portion of dense gas concentration. They are expected to\nplay an important role in star formation and are known to harbour star-forming\nregions and H II regions.\n  Aims. We explore the origin of filaments\/feathers in disc galaxies via global\ngravitational instability.\n  Methods. We conduct a parameter study using three-dimensional hydrodynamical\nsimulations of isolated disc galaxies that are isothermal, self-gravitating and\ninitialised in equilibrium. Our galaxies are uniquely characterised by two\ndimensionless parameters, the Toomre $Q$ and the rotational Mach number,\n$\\mathcal{M}_{\\rm c} = v_{\\rm c}\/c_{\\rm s}$ (ratio of circular velocity to\nsound speed). We carry out simulations covering a wide range in both.\n  Results. We find that galaxies with $Q = 1$ form filaments within a single\nrotation, while galaxies with $Q \\geq 2$ do not. These filaments are kpc long\nand are semi-regularly spaced along the azimuth. Their morphology, density\ncontrast and formation timescale vary with $\\mathcal{M}_{\\rm c}$, with filament\nspacing and instability onset time both inversely proportional to\n$\\mathcal{M}_{\\rm c}$ and the density contrast increasing with\n$\\mathcal{M}_{\\rm c}$. However, their growth rates in all $Q = 1$ galaxies are\n$\\sim 0.5~\\Omega$, where $\\Omega$ is the angular frequency. We compare the\nfilament spacing in our simulations with the ones from JWST\/MIRI and HST\nobservations of nearby galaxies and find them in agreement.\n  Conclusions. Our study suggests that self-gravity and rotation are sufficient\nto form filaments, even in the absence of spiral arms or magnetic fields. Their\nmorphologies are primarily determined by $\\mathcal{M}_{\\rm c}$, which\nparametrises the importance of thermal versus rotational support.",
        "As complex AI systems further prove to be an integral part of our lives, a\npersistent and critical problem is the underlying black-box nature of such\nproducts and systems. In pursuit of productivity enhancements, one must not\nforget the need for various technology to boost the overall trustworthiness of\nsuch AI systems. One example, which is studied extensively in this work, is the\ndomain of Explainable Artificial Intelligence (XAI). Research works in this\nscope are centred around the objective of making AI systems more transparent\nand interpretable, to further boost reliability and trust in using them. In\nthis work, we discuss the various motivation for XAI and its approaches, the\nunderlying challenges that XAI faces, and some open problems that we believe\ndeserve further efforts to look into. We also provide a brief discussion of\nvarious XAI approaches for image processing, and finally discuss some future\ndirections, to hopefully express and motivate the positive development of the\nXAI research space.",
        "In our earlier work, Network-Centric Optimal Hybrid Mobility for IPv6\nwireless sensor networks, in which the work sought to control mobility of\nsensor nodes from an external network was proposed. It was a major improvement\non earlier works such as Cluster Sensor Proxy Mobile IPv6 (CSPMIPv6) and\nNetwork of Proxies (NoP). In this work, the Network-Centric optimal hybrid\nmobility scenario was used to detect and fill sensing holes occurring as a\nresult damaged or energy depleted sensing nodes. Various sensor networks\nself-healing and recovery, and deployment algorithms such as Enhanced Virtual\nForces Algorithm with Boundary Forces (EVFA-B); Coverage - Aware Sensor\nAutomation protocol (CASA); Sensor Self-Organizing Algorithm (SSOA); VorLag and\nthe use of the use of anchor and relay nodes were reviewed. With node density\nthresholds set for various scenarios, the recovery efficiency using various\nparameters were measured. Comparably, our method provides the most efficient\nnode relocation and self-healing mechanism for sensor networks. Compared to\nSensor Self-Organizing Algorithm (SSOA), Hybrid Mobile IP showed superiority in\ncoverage, shorter period of recovery, less computational cost and lower energy\ndepletion. With processing and mobility costs shifted to the external network,\nHybrid Mobile IP extends the life span of the network.",
        "Solid-state $^{229}$Th nuclear clocks require a host material whose band gap\nis larger than the 8.4 eV nuclear transition energy. As such, excitation of the\n$^{229}$Th nuclear state has so far only been demonstrated in metal fluorides,\nspecifically CaF$_2$, LiSrAlF$_6$, and ThF$_4$, where the large\nelectronegativity of the halogen leads to sufficient band gaps. However, it is\nexpected that the nuclear magnetic moment of the fluorine gives rise to a\nleading order broadening mechanism that limits the clock stability. Here, we\nuse concepts of molecular design to identify a polyatomic anion, SO$_4^{2-}$,\nthat is both nuclear spin free and of sufficient electron affinity to result in\na high band gap metal sulfate system. Using state-of-the-art calculations, we\nfind that the band gap of Th(SO$_4$)$_2$ is approximately 9 eV, large enough\nfor direct laser excitation of $^{229}$Th. Low concentrations of $^{229}$Th in\nthe otherwise spinless $^{232}$Th(SO$_4$)$_2$ crystal mitigate\n$^{229}$Th-$^{229}$Th interactions. Furthermore, the introduction of $^{229}$Th\ndoes not modify the material band gap nor introduce electronic states\nassociated with nuclear quenching. By removing one of the primary sources of\nnuclear line broadening in the crystal, the nuclear magnetic dipole-dipole\ninteraction, a nuclear clock with instability as low as $\\sigma =\n4.6\\times10^{-23}\/\\sqrt{\\tau}$, where ${\\tau}$ is the averaging time, may be\nrealized. This is roughly six orders of magnitude lower than previously thought\npossible.",
        "The Bayesian Cram\\'er-Rao bound (BCRB) is a crucial tool in signal processing\nfor assessing the fundamental limitations of any estimation problem as well as\nbenchmarking within a Bayesian frameworks. However, the BCRB cannot be computed\nwithout full knowledge of the prior and the measurement distributions. In this\nwork, we propose a fully learned Bayesian Cram\\'er-Rao bound (LBCRB) that\nlearns both the prior and the measurement distributions. Specifically, we\nsuggest two approaches to obtain the LBCRB: the Posterior Approach and the\nMeasurement-Prior Approach. The Posterior Approach provides a simple method to\nobtain the LBCRB, whereas the Measurement-Prior Approach enables us to\nincorporate domain knowledge to improve the sample complexity and\n{interpretability}. To achieve this, we introduce a Physics-encoded score\nneural network which enables us to easily incorporate such domain knowledge\ninto a neural network. We {study the learning} errors of the two suggested\napproaches theoretically, and validate them numerically. We demonstrate the two\napproaches on several signal processing examples, including a linear\nmeasurement problem with unknown mixing and Gaussian noise covariance matrices,\nfrequency estimation, and quantized measurement. In addition, we test our\napproach on a nonlinear signal processing problem of frequency estimation with\nreal-world underwater ambient noise.",
        "Vibrational microscopy provides label-free, bond-selective chemical contrast\nby detecting molecular vibrations, making it invaluable for biomedical\nresearch. While conventional methods rely on the direct detection of Raman\nscattering or infrared absorption, recently developed vibrational photothermal\n(ViP) microscopy achieves chemical contrast indirectly through refractive index\n(RI) changes. This indirect approach enables unique imaging capabilities beyond\ntraditional chemical imaging. Here, we introduce a novel application of ViP\nmicroscopy: label-free intracellular thermophoretic (Soret) imaging, which\nvisualizes biomolecular transport driven by temperature gradients. ViP-induced\nSoret (ViPS) imaging leverages a steady-state temperature distribution\ngenerated by optical heating through vibrational photothermal effect, combined\nwith time-resolved RI imaging via optical diffraction tomography (ODT). Using\nViPS imaging, we measured thermophoretic behavior in living COS7 cells,\ndetermining intracellular diffusion and Soret coefficients. Notably, we\nobserved a reversed direction of molecular transport (negative Soret effect) in\nthe cytoplasm compared to the nucleus, possibly driven by\nthermophoresis-induced diffusiophoresis. Furthermore, time-lapse imaging under\nCO2-depleted conditions revealed a remarkable reduction in thermophoretic\nactivity, suggesting glass formation during the dying process, likely due to\npolymer aggregation. ViPS imaging represents a new frontier in intracellular\nthermophoretic studies, expanding the capabilities of vibrational microscopy.",
        "Neural quantum states (NQS) have emerged as a powerful tool for approximating\nquantum wavefunctions using deep learning. While these models achieve\nremarkable accuracy, understanding how they encode physical information remains\nan open challenge. In this work, we introduce adiabatic fine-tuning, a scheme\nthat trains NQS across a phase diagram, leading to strongly correlated weight\nrepresentations across different models. This correlation in weight space\nenables the detection of phase transitions in quantum systems by analyzing the\ntrained network weights alone. We validate our approach on the transverse field\nIsing model and the J1-J2 Heisenberg model, demonstrating that phase\ntransitions manifest as distinct structures in weight space. Our results\nestablish a connection between physical phase transitions and the geometry of\nneural network parameters, opening new directions for the interpretability of\nmachine learning models in physics.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Exciton-exciton interactions are fundamental to the light-emitting properties\nof semiconductors, influencing applications from lasers to quantum light\nsources. In this study, we investigate the spectroscopic signatures and binding\nenergy of biexcitons in a metal halide two-dimensional Ruddlesden-Popper\nstructure, which is known for hosting distinct excitonic resonances with unique\nlattice coupling. Using three spectroscopic techniques - photoluminescence (PL)\nand two variations of two-dimensional electronic spectroscopy (2DES) - we map\ncoherent one-quantum and two-quantum correlations to gain deeper insight into\nthe biexciton characteristics. While PL spectroscopy is hindered by spectral\nbroadening and reabsorption, 2DES provides a more accurate characterization,\nrevealing multiple biexciton states and uncovering a mixed biexciton species\narising from exciton cross-coupling. These findings highlight the importance of\nadvanced spectroscopic approaches in accurately determining biexciton binding\nenergies and offer new perspectives on many-body interactions in\nexciton-polarons within layered perovskites.",
        "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
        "Social media platforms have become key tools for coordinated influence\noperations, enabling state actors to manipulate public opinion through\nstrategic, collective actions. While previous research has suggested\ncollaboration between states, such research failed to leverage state-of-the-art\ncoordination indicators or control datasets. In this study, we investigate\ninter-state coordination by analyzing multiple online behavioral traces and\nusing sophisticated coordination detection models. By incorporating a control\ndataset to differentiate organic user activity from coordinated efforts, our\nfindings reveal no evidence of inter-state coordination. These results\nchallenge earlier claims and underscore the importance of robust methodologies\nand control datasets in accurately detecting online coordination.",
        "In [2], it was claimed that the time constant $\\mu_{d}(e_{1})$ for the\nfirst-passage percolation model on $\\mathbb Z^{d}$ is $\\mu_{d}(e_{1}) \\sim \\log\nd\/(2ad)$ as $d\\to \\infty$, if the passage times $(\\tau_{e})_{e\\in \\mathbb\nE^{d}}$ are i.i.d., with a common c.d.f. $F$ satisfying\n$\\left|\\frac{F(x)}{x}-a\\right| \\le \\frac{C}{|\\log x|}$ for some constants $a,\nC$ and sufficiently small $x$.\n  However, the proof of the upper bound, namely, Equation (2.1) in [2]\n\\begin{align} \\limsup_{d\\to\\infty} \\frac{\\mu_{d}(e_{1})ad}{\\log d} \\le\n\\frac{1}{2} \\end{align} is incorrect. In this article, we provide a different\napproach that establishes this inequality. As a side product of this new\nmethod, we also show that the variance of the non-backtracking passage time to\nthe first hyperplane is of order $o\\big((\\log d\/d)^{2}\\big)$ as $d\\to \\infty$\nin the case of the when the edge weights are exponentially distributed.",
        "This study investigates teamwork dynamics in student software development\nprojects through a mixed-method approach combining quantitative analysis of\nGitLab commit logs and qualitative survey data. We analyzed individual\ncontributions across six project phases, comparing self-reported and actual\ncontributions to measure discrepancies. Additionally, a survey captured\ninsights on team leadership, conflict resolution, communication practices, and\nworkload perceptions. Findings reveal that teams with minimal contribution\ndiscrepancies achieved higher project grades and exam pass rates. In contrast,\nteams with more significant discrepancies experienced lower performance,\npotentially due to role clarity and communication issues. These results\nunderscore the value of shared leadership, structured conflict resolution, and\nregular feedback in fostering effective teamwork, offering educators strategies\nto enhance collaboration in software engineering education through\nself-reflection and balanced workload allocation.",
        "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).",
        "Neural network models implemented in embedded devices have been shown to be\nsusceptible to side-channel attacks (SCAs), allowing recovery of proprietary\nmodel parameters, such as weights and biases. There are already available\ncountermeasure methods currently used for protecting cryptographic\nimplementations that can be tailored to protect embedded neural network models.\nShuffling, a hiding-based countermeasure that randomly shuffles the order of\ncomputations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm\nis used. In this paper, we propose a design of an SCA-secure version of the\nFisher-Yates algorithm. By integrating the masking technique for modular\nreduction and Blakely's method for modular multiplication, we effectively\nremove the vulnerability in the division operation that led to side-channel\nleakage in the original version of the algorithm. We experimentally evaluate\nthat the countermeasure is effective against SCA by implementing a correlation\npower analysis attack on an embedded neural network model implemented on ARM\nCortex-M4. Compared to the original proposal, the memory overhead is $2\\times$\nthe biggest layer of the network, while the time overhead varies from $4\\%$ to\n$0.49\\%$ for a layer with $100$ and $1000$ neurons, respectively.",
        "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
        "I construct supersymmetric, parametrically scale-separated AdS$_3$ vacua of\ntype IIB string theory. These arise as compactifications with orientifold\nplanes on specific seven-dimensional solvmanifolds admitting co-closed\n$G_2$-structures, preserving minimal supersymmetry. There are solutions that\ninclude either one set or four sets of intersecting O5-planes in the smeared\napproximation, and parametric scale separation can be achieved by tuning\nunbounded fluxes to infinity. Additionally, the putative holographic field\ntheory operators that are dual to the lightest scalars in the gravitational\ntheory have integer conformal dimensions at tree level, aligning with other\nscale-separated models of type II string theory.",
        "Chinese acupuncture practitioners primarily depend on muscle memory and\ntactile feedback to insert needles and accurately target acupuncture points, as\nthe current workflow lacks imaging modalities and visual aids. Consequently,\nnew practitioners often learn through trial and error, requiring years of\nexperience to become proficient and earn the trust of patients. Medical\nstudents face similar challenges in mastering this skill. To address these\nchallenges, we developed an innovative system, MRUCT, that integrates\nultrasonic computed tomography (UCT) with mixed reality (MR) technology to\nvisualize acupuncture points in real-time. This system offers offline image\nregistration and real-time guidance during needle insertion, enabling them to\naccurately position needles based on anatomical structures such as bones,\nmuscles, and auto-generated reference points, with the potential for clinical\nimplementation. In this paper, we outline the non-rigid registration methods\nused to reconstruct anatomical structures from UCT data, as well as the key\ndesign considerations of the MR system. We evaluated two different 3D user\ninterface (3DUI) designs and compared the performance of our system to\ntraditional workflows for both new practitioners and medical students. The\nresults highlight the potential of MR to enhance therapeutic medical practices\nand demonstrate the effectiveness of the system we developed.",
        "The imitation of voice, targeted on specific speech attributes such as timbre\nand speaking style, is crucial in speech generation. However, existing methods\nrely heavily on annotated data, and struggle with effectively disentangling\ntimbre and style, leading to challenges in achieving controllable generation,\nespecially in zero-shot scenarios. To address these issues, we propose Vevo, a\nversatile zero-shot voice imitation framework with controllable timbre and\nstyle. Vevo operates in two core stages: (1) Content-Style Modeling: Given\neither text or speech's content tokens as input, we utilize an autoregressive\ntransformer to generate the content-style tokens, which is prompted by a style\nreference; (2) Acoustic Modeling: Given the content-style tokens as input, we\nemploy a flow-matching transformer to produce acoustic representations, which\nis prompted by a timbre reference. To obtain the content and content-style\ntokens of speech, we design a fully self-supervised approach that progressively\ndecouples the timbre, style, and linguistic content of speech. Specifically, we\nadopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We\ntreat the vocabulary size of the VQ-VAE codebook as the information bottleneck,\nand adjust it carefully to obtain the disentangled speech representations.\nSolely self-supervised trained on 60K hours of audiobook speech data, without\nany fine-tuning on style-specific corpora, Vevo matches or surpasses existing\nmethods in accent and emotion conversion tasks. Additionally, Vevo's\neffectiveness in zero-shot voice conversion and text-to-speech tasks further\ndemonstrates its strong generalization and versatility. Audio samples are\navailable at https:\/\/versavoice.github.io.",
        "Geospatial imaging leverages data from diverse sensing modalities-such as EO,\nSAR, and LiDAR, ranging from ground-level drones to satellite views. These\nheterogeneous inputs offer significant opportunities for scene understanding\nbut present challenges in interpreting geometry accurately, particularly in the\nabsence of precise ground truth data. To address this, we propose\nCrossModalityDiffusion, a modular framework designed to generate images across\ndifferent modalities and viewpoints without prior knowledge of scene geometry.\nCrossModalityDiffusion employs modality-specific encoders that take multiple\ninput images and produce geometry-aware feature volumes that encode scene\nstructure relative to their input camera positions. The space where the feature\nvolumes are placed acts as a common ground for unifying input modalities. These\nfeature volumes are overlapped and rendered into feature images from novel\nperspectives using volumetric rendering techniques. The rendered feature images\nare used as conditioning inputs for a modality-specific diffusion model,\nenabling the synthesis of novel images for the desired output modality. In this\npaper, we show that jointly training different modules ensures consistent\ngeometric understanding across all modalities within the framework. We validate\nCrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,\ndemonstrating its effectiveness in generating accurate and consistent novel\nviews across multiple imaging modalities and perspectives.",
        "Following the emergence of quantum computing, the subsequent quantum\nrevolution will be that of interconnecting individual quantum computers at\nglobal level. In the same way that classical computers only realised their full\npotential with the emergence of the internet, a fully realised quantum internet\nis the next stage of evolution for quantum computation. This work examines in\ndetail how the quantum internet would evolve in practice, focusing not only on\nthe technology itself but also on the implications it will have economically\nand politically. We present both original ideas, as well as an extensive review\nof relevant and related background material. This work begins with a\ndescription of classical networks before introducing the key concepts behind\nquantum networks, such as quantum internet protocols, quantum cryptography, and\ncloud quantum computing. The work is divided into technical sections (requiring\nonly a basic knowledge of the notation of quantum mechanics), for those\ninterested in mathematical details, as well as non-technical sections for those\nseeking a more general understanding. We target this work very broadly at\nquantum and classical computer scientists, classical computer systems, software\nand network engineers, physicists, economists, artists, musicians, and those\njust generally curious about the future of quantum technologies and what they\nmight bring to humanity.",
        "We propose an efficient analytical representation of the frequency-dependent\n$GW$ self-energy $\\Sigma$ via a multipole approximation (MPA-$\\Sigma$). The\nmultipole-Pad\\'e model for the self-energy is interpolated from a small set of\nnumerical evaluations of $\\Sigma$ in the complex frequency plane, similarly to\nthe previously multipole representation developed for the screened Coulomb\ninteraction (MPA-$W$) [Phys. Rev. B \\textbf{104}, 115157 (2021)]. We show that,\nlikewise MPA-$W$, an appropriate choice of frequency sampling in MPA-$\\Sigma$\nis critical to guarantee computational efficiency and high accuracy. The\ncombined MPA-$W$ and MPA-$\\Sigma$ scheme considerably reduces the cost of\nfull-frequency self-energy calculations, especially for spectral band\nstructures over a wide energy range. Crucially, MPA-$\\Sigma$ enables a\nmultipole representation for the interacting Green's function $G$ (MPA-$G$),\nproviding a straightforward evaluation of all the spectral properties, and a\nmore general way to define the renormalization factor $Z$. We validate the\nMPA-$\\Sigma$ and MPA-$G$ approaches for diverse systems: bulk Si, Na and Cu,\nmonolayer MoS$_2$, the NaCl ion-pair and the F$_2$ molecule. Moreover, we\nintroduce toy MPA-$\\Sigma$\/$G$ models to examine the quasiparticle picture in\ndifferent regimens of weak and strong correlation. With these models, we expose\nlimitations in defining $Z$ from the local derivative of $\\Sigma$.",
        "We develop an economic framework to analyze the optimal pricing and product\ndesign of Large Language Models (LLM). Our framework captures several key\nfeatures of LLMs: variable operational costs of processing input and output\ntokens; the ability to customize models through fine-tuning; and\nhigh-dimensional user heterogeneity in terms of task requirements and error\nsensitivity. In our model, a monopolistic seller offers multiple versions of\nLLMs through a menu of products. The optimal pricing structure depends on\nwhether token allocation across tasks is contractible and whether users face\nscale constraints. Users with similar aggregate value-scale characteristics\nchoose similar levels of fine-tuning and token consumption. The optimal\nmechanism can be implemented through menus of two-part tariffs, with higher\nmarkups for more intensive users. Our results rationalize observed industry\npractices such as tiered pricing based on model customization and usage levels."
      ]
    }
  },
  {
    "id":2411.01144,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A Simple Framework for Contrastive Learning of Visual Representations",
    "start_abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images."
      ],
      "abstract":[
        "Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods."
      ],
      "categories":[
        "Lung Ultrasound"
      ]
    },
    "list":{
      "title":[
        "The 200 Gbps Challenge: Imagining HL-LHC analysis facilities",
        "Cryoscope: A Cryogenic Infrared Survey Telescope in Antarctica",
        "Thermal investigation of bistability in high index doped silica\n  integrated ring resonators",
        "Affineness on Noetherian graded rings, algebras and Hopf algebras",
        "Genuine Multipartite Nonlocality sharing under sequential measurement",
        "Hamiltonian Learning at Heisenberg Limit for Hybrid Quantum Systems",
        "Diagnosing Quantum Many-body Chaos in Non-Hermitian Quantum Spin Chain\n  via Krylov Complexity",
        "Gamma-Ray Bursts Calibrated from the Observational $H(z)$ Data in\n  Artificial Neural Network Framework",
        "Exploring the link between galaxy assembly and dark matter halo assembly\n  in IllustrisTNG: Insights from the Mutual Information",
        "Stability in affine logic",
        "Kink breathers on a traveling wave background in the defocusing modified\n  Korteweg--de Vries equation",
        "CoRe: Coherency Regularization for Hierarchical Time Series",
        "Long-time asymptotics of 3-solitary waves for the damped nonlinear\n  Klein-Gordon equation",
        "Some Problems on Intrinsically Harmonic Forms",
        "Stress-induced phase transformations in Ti-15Mo alloy at elevated\n  temperature",
        "Quantitative Theory for Critical Conditions of Like-Charge Attraction\n  Between Polarizable Spheres",
        "A Probabilistic Parking Process and Labeled IDLA",
        "Enhancement of sensitivity near exceptional points in dissipative\n  qubit-resonator systems",
        "Pulsation Properties of Blazhko and Non-Blazhko RRab Stars",
        "AMPEL workflows for LSST: Modular and reproducible real-time photometric\n  classification",
        "Score-Preserving Targeted Maximum Likelihood Estimation",
        "Distinguishing Cause from Effect with Causal Velocity Models",
        "A method to optimize antipodal coloring span of graphs and its\n  application",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Linear Optics to Scalable Photonic Quantum Computing",
        "A New Proof of Sub-Gaussian Norm Concentration Inequality",
        "Forecasting Italian daily electricity generation disaggregated by\n  geographical zones and energy sources using coherent forecast combination",
        "Quark-Antiquark Potential as a Probe for Holographic Phase Transitions",
        "Enhanced Vascular Flow Simulations in Aortic Aneurysm via\n  Physics-Informed Neural Networks and Deep Operator Networks"
      ],
      "abstract":[
        "The IRIS-HEP software institute, as a contributor to the broader HEP Python\necosystem, is developing scalable analysis infrastructure and software tools to\naddress the upcoming HL-LHC computing challenges with new approaches and\nparadigms, driven by our vision of what HL-LHC analysis will require. The\ninstitute uses a \"Grand Challenge\" format, constructing a series of\nincreasingly large, complex, and realistic exercises to show the vision of\nHL-LHC analysis. Recently, the focus has been demonstrating the IRIS-HEP\nanalysis infrastructure at scale and evaluating technology readiness for\nproduction.\n  As a part of the Analysis Grand Challenge activities, the institute executed\na \"200 Gbps Challenge\", aiming to show sustained data rates into the event\nprocessing of multiple analysis pipelines. The challenge integrated teams\ninternal and external to the institute, including operations and facilities,\nanalysis software tools, innovative data delivery and management services, and\nscalable analysis infrastructure. The challenge showcases the prototypes -\nincluding software, services, and facilities - built to process around 200 TB\nof data in both the CMS NanoAOD and ATLAS PHYSLITE data formats with test\npipelines.\n  The teams were able to sustain the 200 Gbps target across multiple pipelines.\nThe pipelines focusing on event rate were able to process at over 30 MHz. These\ntarget rates are demanding; the activity revealed considerations for future\ntesting at this scale and changes necessary for physicists to work at this\nscale in the future. The 200 Gbps Challenge has established a baseline on\ntoday's facilities, setting the stage for the next exercise at twice the scale.",
        "We present Cryoscope--a new 50 deg$^2$ field-of-view, 1.2 m aperture,\n$K_{dark}$ survey telescope to be located at Dome C, Antarctica. Cryoscope has\nan innovative optical-thermal design wherein the entire telescope is\ncryogenically cooled. Cryoscope also explores new detector technology to\ncost-effectively tile the full focal plane. Leveraging the dark Antarctic sky\nand minimizing telescope thermal emission, Cryoscope achieves unprecedented\ndeep, wide, fast and red observations, matching and exceeding volumetric survey\nspeeds from the Ultraviolet Explorer, Vera Rubin Observatory, Nancy Grace Roman\nSpace Telescope, SPHEREx, and NEO Surveyor. By providing coverage beyond\nwavelengths of 2 $\\mu$m, we aim to create the most comprehensive dynamic movie\nof the most obscured reaches of the Universe. Cryoscope will be a dedicated\ndiscovery engine for electromagnetic emission from coalescing compact binaries,\nEarth-like exoplanets orbiting cold stars, and multiple facets of time-domain,\nstellar and solar system science. In this paper, we describe the scientific\ndrivers and technical innovations for this new discovery engine operating in\nthe $K_{dark}$ passband, why we choose to deploy it in Antarctica, and the\nstatus of a fifth-scale prototype designed as a Pathfinder to retire\ntechnological risks prior to full-scale implementation. We plan to deploy the\nCryoscope Pathfinder to Dome C in December 2026 and the full-scale telescope by\n2030.",
        "The utilization and engineering of thermo-optic effects have found broad\napplications in integrated photonic devices, facilitating efficient light\nmanipulation to achieve various functionalities. Here, we perform both an\nexperimental characterization and theoretical analysis of these effects in\nintegrated micro-ring resonators in high index doped silica (HIDS), which has\nhad many applications in integrated photonics and nonlinear optics. By fitting\nthe experimental results with theory, we obtain fundamental parameters that\ncharacterize their thermo-optic performance, including the thermo-optic\ncoefficient, the efficiency for the optically induced thermo-optic process, and\nthe thermal conductivity. The characteristics of these parameters are compared\nto those of other materials commonly used for integrated photonic platforms,\nsuch as silicon, silicon nitride, and silica. These results offer a\ncomprehensive insight into the thermo-optic properties of HIDS based devices.\nUnderstanding these properties is essential for efficiently controlling and\nengineering them in many practical applications.",
        "In this note, we show that every Noetherian graded ring with an affine degree\nzero part is affine. As a result, a Noetherian graded Hopf algebra whose degree\nzero component is a commutative or a cocommutative Hopf subalgebra is affine.\nMoreover, we show that the braided Hopf algebra of a Noetherian graded Hopf\nalgebra is affine.",
        "The study of quantum nonlocality sharing has garnered significant attention,\nparticularly for two-qubit and three-qubit entangled systems. In this paper, we\nextend the investigation to $n$-qubit Greenberger-Horne-Zeilinger (GHZ)\nsystems, analyzing nonlocality sharing under unbiased unsharp measurements.\nEmploying the Seevink and Svetlichny inequalities, we explore both unilateral\nand multilateral sequential measurement scenarios. In the unilateral scenario,\nwe derive the range for which an observer's multiple copies can share genuine\n$n$-partite nonlocality with single copies of the remaining parties. In the\nmultilateral scenario, we identify the maximum number of independent observers\non $m$ sides who can share genuine $n$-partite nonlocality with other parties.\nA crucial aspect of our results is that all findings stem from a measurement\nstrategy where each sequential observer utilizes unbiased unsharp measurements.\nAs a specific case, for the four-qubit maximally entangled GHZ state, we\ndemonstrate that at most two copies of an observer (e.g., Alice) can share\nnonlocality in the unilateral sequential measurement scenario. However, in the\nmultilateral scenario, no additional sharing is possible compared to the\nunilateral case. This finding highlights the significance of unsharp\nmeasurements in optimizing the recycling of qubits for generating quantum\nnonlocality.",
        "Hybrid quantum systems with different particle species are fundamental in\nquantum materials and quantum information science. In this work, we demonstrate\nthat Hamiltonian learning in hybrid spin-boson systems can achieve the\nHeisenberg limit. Specifically, we establish a rigorous theoretical framework\nproving that, given access to an unknown hybrid Hamiltonian system, our\nalgorithm can estimate the Hamiltonian coupling parameters up to root mean\nsquare error (RMSE) $\\epsilon$ with a total evolution time scaling as $T \\sim\n\\mathcal{O}(\\epsilon^{-1})$ using only $\\mathcal{O}({\\rm\npolylog}(\\epsilon^{-1}))$ measurements. Furthermore, it remains robust against\nsmall state preparation and measurement (SPAM) errors. In addition, we also\nprovide an alternative algorithm based on distributed quantum sensing, which\nsignificantly reduces the maximum evolution time per measurement. To validate\nour method, we apply it to the generalized Dicke model for Hamiltonian learning\nand the spin-boson model for spectrum learning, demonstrating its efficiency in\npractical quantum systems. These results provide a scalable and robust\nframework for precision quantum sensing and Hamiltonian characterization in\nhybrid quantum platforms.",
        "We investigate the phase transitions from chaotic to non-chaotic dynamics in\na quantum spin chain with a local non-Hermitian disorder, which can be realized\nwith a Rydberg atom array setting. As the disorder strength increases, the\nemergence of non-chaotic dynamics is qualitatively captured through the\nsuppressed growth of Krylov complexity, and quantitatively identified through\nthe reciprocity breaking of Krylov space. We further find that the localization\nin Krylov space generates another transition in the weak disorder regime,\nsuggesting a weak ergodicity breaking. Our results closely align with\nconventional methods, such as the entanglement entropy and complex level\nspacing statistics, and pave the way to explore non-Hermitian phase transitions\nusing Krylov complexity and associated metrics.",
        "In this paper, we calibrate the luminosity relation of gamma-ray bursts\n(GRBs) from an Artificial Neural Network (ANN) framework for reconstructing the\nHubble parameter \\unboldmath{$H(z)$} from the latest observational Hubble data\n(OHD) obtained with the cosmic chronometers method in a cosmology-independent\nway. We consider the physical relationships between the data to introduce the\ncovariance matrix and KL divergence of the data to construct the loss function\nand calibrate the Amati relation ($E_{\\rm p}$--$E_{\\rm iso}$) by selecting the\noptimal ANN model with the A219 sample and the J220 sample at low redshift.\nCombining the Pantheon+ sample of type Ia supernovae (SNe Ia) and Baryon\nacoustic oscillations (BAOs) with GRBs at high redshift in the Hubble diagram\nwith Markov Chain Monte Carlo numerical method, we find that the $\\Lambda$CDM\nmodel is preferred over the $w$CDM and CPL models with the joint constraints by\nthe Akaike Information Criterion (AIC) and Bayesian Information Criterion\n(BIC).",
        "We employed Mutual Information (MI) analysis to investigate the relationship\nbetween galaxy properties and the assembly history of their host dark matter\n(DM) haloes from the IllustrisTNG simulations. Focusing on central and\nsatellite galaxies with stellar masses between $10^{9} \\, - \\, 10^{11.5}\\,\nh^{-1} M_\\odot$, we examined the correlation between halo assembly time and\ngalaxy assembly time, specific star formation rate (sSFR), color $(g-i)$, and\ngalaxy formation efficiency $F_\\star$. Our results indicate a strong\ncorrelation between $F_\\star$ and the halo assembly time for low-mass central\ngalaxies, suggesting a co-evolutionary relationship. In contrast, sSFR and\ncolor $(g-i)$ exhibit weaker correlations with halo assembly time, indicating\nthat additional factors should influence these galaxy properties. Satellite\ngalaxies show negligible correlation between their properties and halo assembly\ntime, highlighting the impact of environmental processes on their evolution. We\nfurther extended our analysis to cluster observables, including the magnitude\ngap, the satellite richness, and the distances to the satellites. Although\nthese cluster properties display weak overall correlations with halo assembly\ntime, the richness consistently increases with stellar mass. This trend\nsuggests that richness is more closely linked to formation history in more\nmassive haloes, where satellite accretion dominates the growth of their host DM\nhaloes. These findings establish $F_\\star$ as a more sensitive indicator of\nhalo assembly history than colour $(g-i)$, sSFR, or cluster observables,\noffering new insights into the complex interplay between galaxy evolution and\nthe hierarchical growth of their host dark matter haloes.",
        "We develop foundational aspects of stability theory in affine logic. On the\none hand, we prove appropriate affine versions of many classical results,\nincluding definability of types, existence of non-forking extensions, and other\nfundamental properties of forking calculus. Most notably, stationarity holds\nover arbitrary sets (in fact, every type is Lascar strong). On the other hand,\nwe prove that stability is preserved under direct integrals of measurable\nfields of structures. We deduce that stability in the extremal models of an\naffine theory implies stability of the theory. We also deduce that the affine\npart of a stable continuous logic theory is affinely stable, generalising the\nresult of preservation of stability under randomisations.",
        "We characterize a general traveling periodic wave of the defocusing mKdV\n(modified Korteweg--de Vries) equation by using a quotient of products of\nJacobi's elliptic theta functions. Compared to the standing periodic wave of\nthe defocusing NLS (nonlinear Schr\\\"{o}dinger) equation, these solutions are\nspecial cases of Riemann's theta function of genus two. Based on our\ncharacterization, we derive a new two-parameter solution form which defines a\ngeneral three-parameter solution form with the scaling transformation.\nEigenfunctions of the Lax system for the general traveling periodic wave are\nalso characterized as quotients of products of Jacobi's theta functions. As the\nmain outcome of our analytical computations, we derive a new solution of the\ndefocusing mKdV equation which describes the kink breather propagating on a\ngeneral traveling wave background.",
        "Hierarchical time series forecasting presents unique challenges, particularly\nwhen dealing with noisy data that may not perfectly adhere to aggregation\nconstraints. This paper introduces a novel approach to soft coherency in\nhierarchical time series forecasting using neural networks. We present a\nnetwork coherency regularization method, which we denote as CoRe (Coherency\nRegularization), a technique that trains neural networks to produce forecasts\nthat are inherently coherent across hierarchies, without strictly enforcing\naggregation constraints. Our method offers several key advantages. (1) It\nprovides theoretical guarantees on the coherency of forecasts, even for\nout-of-sample data. (2) It is adaptable to scenarios where data may contain\nerrors or missing values, making it more robust than strict coherency methods.\n(3) It can be easily integrated into existing neural network architectures for\ntime series forecasting. We demonstrate the effectiveness of our approach on\nmultiple benchmark datasets, comparing it against state-of-the-art methods in\nboth coherent and noisy data scenarios. Additionally, our method can be used\nwithin existing generative probabilistic forecasting frameworks to generate\ncoherent probabilistic forecasts. Our results show improved generalization and\nforecast accuracy, particularly in the presence of data inconsistencies. On a\nvariety of datasets, including both strictly hierarchically coherent and noisy\ndata, our training method has either equal or better accuracy at all levels of\nthe hierarchy while being strictly more coherent out-of-sample than existing\nsoft-coherency methods.",
        "We consider the damped nonlinear Klein-Gordon equation: \\begin{align*}\n\\partial_{t}^2u-\\Delta u+2\\alpha \\partial_{t}u+u-|u|^{p-1}u=0, \\ & (t,x) \\in\n\\mathbb{R} \\times \\mathbb{R}^d, \\end{align*} where $\\alpha>0$, $1\\leq d\\leq 5$\nand energy sub-critical exponents $p>2$. In this paper, we prove that\n3-solitary waves behave as if the three solitons are on a line. Furthermore,\nthe solitary waves have alternative signs and their distances are of order\n$\\log{t}$.",
        "In this short note we recall the definition of intrinsically harmonic forms,\nsome known results and some open problems.",
        "Controlled mechanical loading was applied to Ti-15Mo alloy during annealing\nat 550 {\\deg}C. Massive formation of the $\\omega_{\\textrm{iso}}$ phase from the\nparent $\\beta$-phase occurred during annealing at 550 {\\deg}C without external\nstress or with stress well below the yield stress. Moreover, a massive $\\alpha$\nphase precipitation takes place under simultaneous annealing and plastic\ndeformation. Plastic deformation plays a key role in $\\beta\\rightarrow\\alpha$\ntransformation and achieving refined $\\alpha+\\beta$ type microstructure\nresulted in improved mechanical properties. Studying phase transformations\nduring plastic deformation is critical for understanding and optimizing\nthermomechanical processing of metastable $\\beta$-Ti alloys.",
        "Despite extensive experimental and theoretical efforts, a concise\nquantitative theory to predict the occurrence of like-charge attraction (LCA)\nbetween polarizable spheres remains elusive. In this work, we first derive a\nnovel three-point image formula, based on a key observation that connects the\nclassical Neumann's image principle with the incomplete beta function. This\napproach naturally yields simple yet precise critical conditions for LCA, with\na relative discrepancy of less than $1\\%$ compared to numerical simulations,\nvalidated across diverse parameter settings. The obtained critical conditions\nmay provide physical insights into various processes potentially involving LCA,\nsuch as self-assembly, crystallization, and phase separation across different\nlength scales. Additionally, the new image formula is directly applicable to\nenhance the efficiency of polarizable force field calculations involving\npolarizable spheres.",
        "We introduce and study a new probabilistic variant of the classical parking\nprotocol of Konheim and Weiss [29], which is closely related to Internal\nDiffusion Limited Aggregation, or IDLA, introduced in 1991 by Diaconis and\nFulton [15]. In particular, we show that if one runs our parking protocol\nstarting with a parking function whose outcome permutation (in the sense of the\nclassical parking process of Konheim and Weiss) is the identity permutation,\nthen we can compute the exact probability that all of the cars park.\nFurthermore, we compute the expected time it takes for the protocol to complete\nassuming all of the cars park, and prove that the parking process is negatively\ncorrelated. We also study statistics of uniformly random weakly increasing\nparking functions, a subset of parking functions whose outcome is the identity\npermutation. We give the distribution of the last entry, along with the\nprobability that a specific set of cars is lucky, and the expected number of\nlucky cars.",
        "Dissipation usually plays a negative role in quantum metrological\ntechnologies, which aim to improve measurement precision by leveraging quantum\neffects that are vulnerable to environment-induced decoherence. Recently, it\nhas been demonstrated that dissipation can actually be used as a favorable\nresource for enhancing the susceptibility of signal detection. However,\ndemonstrations of such enhancement for detecting physical quantities in open\nquantum systems are still lacking. Here we propose and demonstrate a protocol\nfor realizing such non-Hermitian quantum sensors for probing the coupling\nbetween a qubit and a resonator subjecting to energy dissipations. The\nexcitation-number conversion associated with the no-jump evolution trajectory\nenables removal of the noisy outcomes with quantum jumps, implementing the\nexceptional point (EP), where the Rabi splitting exhibits a divergent behavior\nin response to a tiny variation of the effective coupling. The sensitivity\nenhancement near the EP is confirmed by both theoretical calculation and\nexperimental measurement.",
        "In this study, we conduct a comparative analysis of the properties of Blazhko\nand non-Blazhko RRab stars. We identified 1054 non-Blazhko and 785 Blazhko RRab\nstars in the photometric data observed by K2 mission, which, combined with\nthose 37 stars observed in the original Kepler field, constituted our study\nsample. Using the Fourier Decomposition method, we calculated the pulsation\nparameters, including phase differences and amplitude ratios, for these RRab\nstars, revealing significant discrepancies in the pulsation parameters between\nBlazhko and non-Blazhko RRab stars. However, distinguishing between Blazhko and\nNon-Blazhko RRab stars based on Fourier parameters remains challenging due to\nthe significant overlap in their distributions. By cross-matching our sample\nwith the LRS of LAMOST DR12, we identified 147 Blazhko and 111 non-Blazhko RRab\nstars, which exhibit similar metallicity distributions. Furthermore,\ncross-matching with Gaia DR3 data yielded 766 Blazhko and 950 non-Blazhko RRab\nstars, showing differences in color indices but not in absolute magnitudes. Our\nfindings suggested the Blazhko effect is linked to pulsation parameters and\ncolors, rather than metallicities or absolute magnitude.",
        "Modern time-domain astronomical surveys produce high throughput data streams\nwhich require tools for processing and analysis. This will be critical for\nprograms making full use of the alert stream from the Vera Rubin Observatory\n(VRO), where spectroscopic labels will only be available for a small subset of\nall transients. In this context, the AMPEL toolset can work as a code-to-data\nplatform for the development of efficient, reproducible and flexible workflows\nfor real-time astronomical application.\n  We here introduce three different AMPEL channels constructed to highlight\ndifferent uses of alert streams: to rapidly find infant transients (SNGuess),\nto provide unbiased transient samples for follow-up (FollowMe) and to deliver\nfinal transient classifications (FinalBet). These pipelines already contain\nplaceholders for mechanisms which will be essential for the optimal usage of\nVRO alerts: combining different classifiers, including host galaxy information,\npopulation priors and sampling non-gaussian photometric redshift distributions.\nBased on the ELAsTiCC simulation, all three channels are already working at a\nhigh level: SNGuess correctly tags 99% of all young supernovae, FollowMe\nillustrates how an unbiased subset of alerts can be selected for spectroscopic\nfollow-up in the context of cosmological probes and FinalBet includes priors to\nachieve successful classifications for >~80% of all extragalactic transients.\n  The fully functional workflows presented here are all public and can be used\nas starting points for any group wishing to optimize pipelines for their\nspecific VRO science programs. AMPEL is designed to allow this to be done in\naccordance with FAIR principles: both software and results can be easily shared\nand results reproduced. The code-to-data environment ensures that models\ndeveloped this way can be directly applied to the real-time LSST stream parsed\nby AMPEL.",
        "Targeted maximum likelihood estimators (TMLEs) are asymptotically optimal\namong regular, asymptotically linear estimators. In small samples, however, we\nmay be far from \"asymptopia\" and not reap the benefits of optimality. Here we\npropose a variant (score-preserving TMLE; SP-TMLE) that leverages an initial\nestimator defined as the solution of a large number of possibly data-dependent\nscore equations. Instead of targeting only the efficient influence function in\nthe TMLE update to knock out the plug-in bias, we also target the\nalready-solved scores. Solving additional scores reduces the remainder term in\nthe von-Mises expansion of our estimator because these scores may come close to\nspanning higher-order influence functions. The result is an estimator with\nbetter finite-sample performance. We demonstrate our approach in simulation\nstudies leveraging the (relaxed) highly adaptive lasso (HAL) as our initial\nestimator. These simulations show that in small samples SP-TMLE has reduced\nbias relative to plug-in HAL and reduced variance relative to vanilla TMLE,\nblending the advantages of the two approaches. We also observe improved\nestimation of standard errors in small samples.",
        "Bivariate structural causal models (SCM) are often used to infer causal\ndirection by examining their goodness-of-fit under restricted model classes. In\nthis paper, we describe a parametrization of bivariate SCMs in terms of a\ncausal velocity by viewing the cause variable as time in a dynamical system.\nThe velocity implicitly defines counterfactual curves via the solution of\ninitial value problems where the observation specifies the initial condition.\nUsing tools from measure transport, we obtain a unique correspondence between\nSCMs and the score function of the generated distribution via its causal\nvelocity. Based on this, we derive an objective function that directly\nregresses the velocity against the score function, the latter of which can be\nestimated non-parametrically from observational data. We use this to develop a\nmethod for bivariate causal discovery that extends beyond known model classes\nsuch as additive or location scale noise, and that requires no assumptions on\nthe noise distributions. When the score is estimated well, the objective is\nalso useful for detecting model non-identifiability and misspecification. We\npresent positive results in simulation and benchmark experiments where many\nexisting methods fail, and perform ablation studies to examine the method's\nsensitivity to accurate score estimation.",
        "In this article, we study radio \\(k\\)-colorings of simple connected graphs\n\\(G\\) with diameter \\(d\\), where a radio \\(k\\)-coloring \\(g\\) assigns\nnon-negative integers to \\(V(G)\\) (vertices of \\(G\\)) such that \\(|g(u) - g(v)|\n\\geq 1 + k - d(u, v)\\) for any two vertices \\(u, v\\) with \\(1 \\leq k \\leq d\\).\nThe span of a radio \\(k\\)-coloring \\(g\\), expressed by \\(rc_k(g)\\), is the\nmaximum integer assigned by \\(g\\), and the radio \\(k\\)-chromatic number\n\\(rc_k(G)\\) is the minimum span among all radio \\(k\\)-colorings of \\(G\\). A\ncoloring \\(g\\) is minimal if \\(rc_k(g) = rc_k(G)\\). When \\(k = d-1\\), this\ncoloring is known as the antipodal coloring, and \\(rc_{d-1}(G)\\) referred to as\nthe antipodal number, is denoted by \\(ac(G)\\). We derive a sufficient condition\nfor an antipodal coloring to be minimal and apply this criterion to determine\nthe antipodal number of the generalized Petersen graph \\(GP(n,1)\\) for all\n\\(n\\) except when \\(n \\equiv 2 \\pmod{8}\\), and for toroidal grids \\(T_{r,s} =\nC_r \\square C_s\\) when \\(rs\\) is even. Additionally, we establish a lower bound\nfor \\(ac(T_{r,s})\\) when \\(rs\\) is odd.",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Recent advancements in quantum photonics have driven significant progress in\nphotonic quantum computing (PQC), addressing challenges in scalability,\nefficiency, and fault tolerance. Experimental efforts have focused on\nintegrated photonic platforms utilizing materials such as silicon photonics and\nlithium niobate to enhance performance. Parameters like photon loss rates,\ncoupling efficiencies, and fidelities have been pivotal, with state-of-the-art\nsystems achieving coupling efficiencies above 90% and photon\nindistinguishability exceeding 99%. Quantum error correction schemes have\nreduced logical error rates to below $10^{-3}$, marking a step toward\nfault-tolerant PQC. Photon generation has also advanced with deterministic\nsources, such as quantum dots, achieving brightness levels exceeding $10^6$\nphoton pairs\/s\/mW and time-bin encoding enabling scalable entanglement.\nHeralded single-photon sources now exhibit purities above 99%, driven by\ninnovations in fabrication techniques. High-efficiency photon detectors, such\nas superconducting nanowire single-photon detectors (SNSPDs), have demonstrated\ndetection efficiencies exceeding 98%, dark count rates below 1 Hz, and timing\njitters as low as 15 ps, ensuring precise photon counting and manipulation.\nMoreover, demonstrations of boson sampling with over 100 photons underscore the\ngrowing computational power of photonic systems, surpassing classical limits.\nThe integration of machine learning has optimized photonic circuit design,\nwhile frequency multiplexing and time-bin encoding have increased system\nscalability. Together, these advances bridge the gap between theoretical\npotential and practical implementation, positioning PQC as a transformative\ntechnology for computing, communication, and quantum sensing.",
        "We present a new proof of the sub-Gaussian norm concentration inequality. Our\nproof is based on an averaged version of the moment generating function termed\nthe averaged moment generating function. Compared with the widely adopted\n$\\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration\ninequality, our method does not rely on the union bound and promises a tighter\nconcentration bound.",
        "A novel approach is applied for improving forecast accuracy and achieving\ncoherence in forecasting the Italian daily energy generation time series. In\nhierarchical frameworks such as national energy generation disaggregated by\ngeographical zones and energy sources, independently generated base forecasts\noften result in inconsistencies across the constraints. We deal with this issue\nthrough a coherent balanced multi-task forecast combination approach, which\ncombines unbiased forecasts from multiple experts while ensuring coherence.\nApplied to the daily Italian electricity generation data, our method shows\nsuperior accuracy compared to single-task base and combined forecasts, and a\nstate-of-the-art single-expert reconciliation technique, demonstrating to be an\neffective approach to forecasting linearly constrained multiple time series.",
        "In the recent paper (Phys.Rev.Lett. 133 (2024) 12, 121601), a higher-order\nphase transition between the planar, charged, 5-dimensional\nReissner-Nordstr\\\"om-Anti-de Sitter black hole and a hairy black hole solution\nof the type IIB supergravity was investigated. Here, we set out to investigate\nthese two phases of the theory by means of the holographic probe that describes\na quark-antiquark in the dual gauge theory. We show that the study of the\nquark-antiquark potential turns out to be a useful method to investigate the\nchange of behavior at different values of the parameter that controls the phase\ntransition, this parameter being the ratio between the chemical potential and\nthe temperature. In other words, the string serves as a probe to detect the\nphase transition.",
        "Due to the limited accuracy of 4D Magnetic Resonance Imaging (MRI) in\nidentifying hemodynamics in cardiovascular diseases, the challenges in\nobtaining patient-specific flow boundary conditions, and the computationally\ndemanding and time-consuming nature of Computational Fluid Dynamics (CFD)\nsimulations, it is crucial to explore new data assimilation algorithms that\noffer possible alternatives to these limitations. In the present work, we study\nPhysics-Informed Neural Networks (PINNs), Deep Operator Networks (DeepONets),\nand their Physics-Informed extensions (PI-DeepONets) in predicting vascular\nflow simulations in the context of a 3D Abdominal Aortic Aneurysm (AAA)\nidealized model. PINN is a technique that combines deep neural networks with\nthe fundamental principles of physics, incorporating the physics laws, which\nare given as partial differential equations, directly into loss functions used\nduring the training process. On the other hand, DeepONet is designed to learn\nnonlinear operators from data and is particularly useful in studying parametric\npartial differential equations (PDEs), e.g., families of PDEs with different\nsource terms, boundary conditions, or initial conditions. Here, we adapt the\napproaches to address the particular use case of AAA by integrating the 3D\nNavier-Stokes equations (NSE) as the physical laws governing fluid dynamics. In\naddition, we follow best practices to enhance the capabilities of the models by\neffectively capturing the underlying physics of the problem under study. The\nadvantages and limitations of each approach are highlighted through a series of\nrelevant application cases. We validate our results by comparing them with CFD\nsimulations for benchmark datasets, demonstrating good agreements and\nemphasizing those cases where improvements in computational efficiency are\nobserved."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images",
    "start_abstract":"Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "HI-MaNGA: Results from (21cm-HI) single-dish observations of MaNGA\n  Survey Galaxies",
        "\\'El\\'ements de comptage sur les g\\'en\\'erateurs du groupe modulaire et\n  les $\\lambda$-quiddit\\'es",
        "Dark Energy Survey Year 6 Results: Synthetic-source Injection Across the\n  Full Survey Using Balrog",
        "On the Commuting Problem of Toeplitz Operators on the Harmonic Bergman\n  Space",
        "Generalization Performance of Hypergraph Neural Networks",
        "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
        "Properties of the one-component Coulomb gas on a sphere with two\n  macroscopic external charges",
        "Norm-one points in convex combinations of relatively weakly open subsets\n  of the unit ball in the spaces $L_1(\\mu,X)$",
        "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "Deriving motivic coactions and single-valued maps at genus zero from\n  zeta generators",
        "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to\n  Counteract Alpha Decay",
        "Relative knot probabilities in confined lattice polygons",
        "Global branching of solutions to ODEs and integrability",
        "Kink dynamics for the Yang-Mills field in an extremal\n  Reissner-Nordstr\\\"om black hole",
        "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party\n  Conversation Contexts",
        "70 MW-level picosecond mid-infrared radiation generation by difference\n  frequency generation in AgGaS2, BaGa4Se7, LiGaSe2, and LiGaS2",
        "A life in Mathematical Analysis: a conversation with Luigi Rodino",
        "Higher Riemann-Hilbert correspondence for foliations",
        "Performance of Practical Quantum Oblivious Key Distribution",
        "Human-Like Robot Impedance Regulation Skill Learning from Human-Human\n  Demonstrations",
        "Transient Chirality in the Gelation of Adhesive Spinner Monolayers",
        "Normal and inverse magnetocaloric effects in structurally disordered\n  Laves phase Y$_{1-x}$Gd$_{x}$Co$_{2}$ (0 $\\leq$ x $\\leq$ 1) compounds",
        "Keeping up with dynamic attackers: Certifying robustness to adaptive\n  online data poisoning",
        "Unified Multivariate Ordinal Model for analysis of sensory attributes",
        "Analog QAOA with Bayesian Optimisation on a neutral atom QPU",
        "Can one size fit all?: Measuring Failure in Multi-Document Summarization\n  Domain Transfer",
        "$S$, $T$, $U$ Parameters in The B-LSSM",
        "IRIS: An Immersive Robot Interaction System",
        "A Label-Free High-Precision Residual Moveout Picking Method for Travel\n  Time Tomography based on Deep Learning"
      ],
      "abstract":[
        "In a poster presentation for IAU Symposium 392: \"Neutral hydrogen in and\naround galaxies in the SKA era\", we gave an overview of the HI-MaNGA project\nwhich is working to obtain complementary information about the cold gas\n(neutral hydrogen traced by the radio 21cm line) content of Mapping Nearby\nGalaxies at Apache Point Observatory (MaNGA) sample galaxies. MaNGA, part of\nthe fourth incarnation of the Sloan Digital Sky Surveys (SDSS-IV), obtained\nspatially resolved spectral maps for 10,000 nearby galaxies selected to create\na representative sample out of the SDSS Main Galaxy Sample. MaNGA data have\nprovided a census of the stellar and ionized gas content of these galaxies, as\nwell as kinematics of both stars and gas. Adding HI information via the\nHI-MaNGA program, which has observed or collected 21cm line data for 70% of the\nfull MaNGA sample, has been crucial for a number of applications, but\nespecially understanding the physical mechanisms that regulate gas accretion,\nand through that star formation and quenching of star formation. This\nconference proceedings article accompanies the release of the DR3 version of\nHI-MaNGA data.",
        "The aim of this article is to count the $n$-tuples of positive integers\n$(a_{1},\\ldots,a_{n})$ solutions of the equation $\\begin{pmatrix} a_{n} & -1\n\\\\[4pt] 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1} & -1 \\\\[4pt] 1 & 0\n\\end{pmatrix} \\cdots \\begin{pmatrix} a_{1} & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}=\\pm\nM$ when $M$ is equal to the generators of the modular group $S=\\begin{pmatrix}\n0 & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}$ and $T=\\begin{pmatrix} 1 & 1 \\\\[4pt] 0 & 1\n\\end{pmatrix}$. To count these elements, we will study the\n$\\lambda$-quiddities, which are the solutions of the equation in the case\n$M=Id$ (related to Coxeter's friezes), whose last component is fixed.",
        "Synthetic source injection (SSI), the insertion of sources into pixel-level\non-sky images, is a powerful method for characterizing object detection and\nmeasurement in wide-field, astronomical imaging surveys. Within the Dark Energy\nSurvey (DES), SSI plays a critical role in characterizing all necessary\nalgorithms used in converting images to catalogs, and in deriving quantities\nneeded for the cosmology analysis, such as object detection rates, galaxy\nredshift estimation, galaxy magnification, star-galaxy classification, and\nphotometric performance. We present here a source injection catalog of $146$\nmillion injections spanning the entire 5000 deg$^2$ DES footprint, generated\nusing the Balrog SSI pipeline. Through this sample, we demonstrate that the DES\nYear 6 (Y6) image processing pipeline provides accurate estimates of the object\nproperties, for both galaxies and stars, at the percent-level, and we highlight\nspecific regimes where the accuracy is reduced. We then show the consistency\nbetween SSI and data catalogs, for all galaxy samples developed within the weak\nlensing and galaxy clustering analyses of DES Y6. The consistency between the\ntwo catalogs also extends to their correlations with survey observing\nproperties (seeing, airmass, depth, extinction, etc.). Finally, we highlight a\nnumber of applications of this catalog to the DES Y6 cosmology analysis. This\ndataset is the largest SSI catalog produced at this fidelity and will serve as\na key testing ground for exploring the utility of SSI catalogs in upcoming\nsurveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time.",
        "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
        "Hypergraph neural networks have been promising tools for handling learning\ntasks involving higher-order data, with notable applications in web graphs,\nsuch as modeling multi-way hyperlink structures and complex user interactions.\nYet, their generalization abilities in theory are less clear to us. In this\npaper, we seek to develop margin-based generalization bounds for four\nrepresentative classes of hypergraph neural networks, including\nconvolutional-based methods (UniGCN), set-based aggregation (AllDeepSets),\ninvariant and equivariant transformations (M-IGN), and tensor-based approaches\n(T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in\nwhich hypergraph structure and spectral norms of the learned weights can affect\nthe generalization bounds, where the key technical challenge lies in developing\nnew perturbation analysis for hypergraph neural networks, which offers a\nrigorous understanding of how variations in the model's weights and hypergraph\nstructure impact its generalization behavior. Our empirical study examines the\nrelationship between the practical performance and theoretical bounds of the\nmodels over synthetic and real-world datasets. One of our primary observations\nis the strong correlation between the theoretical bounds and empirical loss,\nwith statistically significant consistency in most cases.",
        "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
        "The one-component Coulomb gas on the sphere, consisting on $N$ unit charges\ninteracting via a logarithmic potential, and in the presence of two external\ncharges each of strength proportional to $N$, is considered. There are two\nspherical caps naturally associated with the external charges, giving rise to\ntwo distinct phases depending on them not overlapping (post-critical) or\noverlapping (pre-critical). The equilibrium measure in the post-critical phase\nis known from earlier work. We determine the equilibrium measure in the\npre-critical phase using a particular conformal map, with the parameters\ntherein specified in terms of a root of a certain fourth order polynomial. This\nis used to determine the exact form of the electrostatic energy for the\npre-critical phase. Using a duality relation from random matrix theory, the\npartition function for the Coulomb gas at the inverse temperature $\\beta = 2$\ncan be expanded for large $N$ in the post-critical phase, and in a scaling\nregion of the post and pre-critical boundary. For the pre-critical phase, the\nduality identity implies a relation between two electrostatic energies, one for\nthe present sphere system, and the other for a certain constrained log-gas\nrelating to the Jacobi unitary ensemble.",
        "In a paper published in 2020 in Studia Mathematica, Abrahamsen et al. proved\nthat in the real space $L_1(\\mu)$, where $\\mu$ is a non-zero $\\sigma$-finite\n(countably additive non-negative) measure, norm-one elements in finite convex\ncombinations of relatively weakly open subsets of the unit ball are interior\npoints of these convex combinations in the relative weak topology. In this\npaper that result is generalised by proving that the same is true in the (real\nor complex) Lebesgue--Bochner spaces $L_1(\\mu,X)$ where $X$ is a weakly\nuniformly rotund Banach space.",
        "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
        "Multiple polylogarithms are equipped with rich algebraic structures including\nthe motivic coaction and the single-valued map which both found fruitful\napplications in high-energy physics. In recent work arXiv:2312.00697, the\ncurrent authors presented a conjectural reformulation of the motivic coaction\nand the single-valued map via zeta generators, certain operations on\nnon-commuting variables in suitable generating series of multiple\npolylogarithms. In this work, the conjectures of the reference will be proven\nfor multiple polylogarithms that depend on any number of variables on the\nRiemann sphere.",
        "Alpha mining, a critical component in quantitative investment, focuses on\ndiscovering predictive signals for future asset returns in increasingly complex\nfinancial markets. However, the pervasive issue of alpha decay, where factors\nlose their predictive power over time, poses a significant challenge for alpha\nmining. Traditional methods like genetic programming face rapid alpha decay\nfrom overfitting and complexity, while approaches driven by Large Language\nModels (LLMs), despite their promise, often rely too heavily on existing\nknowledge, creating homogeneous factors that worsen crowding and accelerate\ndecay. To address this challenge, we propose AlphaAgent, an autonomous\nframework that effectively integrates LLM agents with ad hoc regularizations\nfor mining decay-resistant alpha factors. AlphaAgent employs three key\nmechanisms: (i) originality enforcement through a similarity measure based on\nabstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor\nalignment via LLM-evaluated semantic consistency between market hypotheses and\ngenerated factors, and (iii) complexity control via AST-based structural\nconstraints, preventing over-engineered constructions that are prone to\noverfitting. These mechanisms collectively guide the alpha generation process\nto balance originality, financial rationale, and adaptability to evolving\nmarket conditions, mitigating the risk of alpha decay. Extensive evaluations\nshow that AlphaAgent outperforms traditional and LLM-based methods in\nmitigating alpha decay across bull and bear markets, consistently delivering\nsignificant alpha in Chinese CSI 500 and US S&P 500 markets over the past four\nyears. Notably, AlphaAgent showcases remarkable resistance to alpha decay,\nelevating the potential for yielding powerful factors.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "We consider a natural generalisation of the Painlev\\'e property and use it to\nidentify the known integrable cases of the Lane-Emden equation with a real\npositive index. We classify certain first-order ordinary differential equations\nwith this property and find necessary conditions for a large family of\nsecond-order equations. We consider ODEs such that, given any simply connected\ndomain $\\Omega$ not containing fixed singularities of the equation, the Riemann\nsurface of any solution obtained by analytic continuation along curves in\n$\\Omega$ has a finite number of sheets over $\\Omega$.",
        "Considered in this work is the Yang-Mills field in an extremal\nReissner-Nordstr\\\"om black hole, a physically motivated mathematical model\nintroduced by Bizo\\'n and Kahl. The kink is a fundamental, strongly unstable\nstationary solution in this non-perturbative, variable coefficients model, with\na polynomial tail and no explicit form. In this paper, we introduce and extend\nseveral virial techniques, adapt them to the inhomogeneous medium setting, and\nconstruct a finite codimensional manifold of the energy space where the kink is\nasymptotically stable. In particular, we handle, using virial techniques, the\nemergence of a weak threshold resonance in the description of the stable\nmanifold.",
        "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities\/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis.",
        "Comparative study of nonlinear crystals for picosecond difference frequency\ngeneration in mid-IR is presented. Nonlinear crystals of AgGaS$_2$,\nBaGa$_4$Se$_7$, LiGaSe$_2$, and LiGaS$_2$ were studied. Samples of AgGaS$_2$,\nBaGa$_4$Se$_7$, LiGaSe$_2$, and LiGaS$_2$ were tested in thee sets having\nlengths of 2, 4, or 8 mm. In order to investigate the dependence of efficiency\non the crystal length, three sets of crystals with lengths of 2, 4, or 8 mm\nwere tested. The developed tunable DFG system was driven by the 1.03 $\\mu$m,\n1.8 ps, Yb:YAG thin-disk laser system operated at the repetition rate of 10 or\n100 Hz. As the best result, picosecond mid-IR pulses at a wavelength of $\\sim$7\n$\\mu$m with the energy up to 130 $\\mu$J corresponding to the peak power of\n$\\sim$72 MW were generated using the 8 mm long LiGaS$_2$ crystal. Using the\nBaGa$_4$Se$_7$ crystal, DFG tunability in the wavelength range from 6 up to 13\n$\\mu$m was achieved.",
        "This note is the transcription of an interview with Professor Luigi Rodino,\non the occasion of the ISAAC-ICMAM Conference of Analysis in Developing\nCountries (December 2, 2024 - Bogot\\`a), that was dedicated to him. Luigi\nRodino is at present Emeritus Professor at the University of Turin, and a\nmember of the Accademia delle Scienze di Torino.",
        "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Humans are experts in collaborating with others physically by regulating\ncompliance behaviors based on the perception of their partner states and the\ntask requirements. Enabling robots to develop proficiency in human\ncollaboration skills can facilitate more efficient human-robot collaboration\n(HRC). This paper introduces an innovative impedance regulation skill learning\nframework for achieving HRC in multiple physical collaborative tasks. The\nframework is designed to adjust the robot compliance to the human partner\nstates while adhering to reference trajectories provided by human-human\ndemonstrations. Specifically, electromyography (EMG) signals from human muscles\nare collected and analyzed to extract limb impedance, representing compliance\nbehaviors during demonstrations. Human endpoint motions are captured and\nrepresented using a probabilistic learning method to create reference\ntrajectories and corresponding impedance profiles. Meanwhile, an LSTMbased\nmodule is implemented to develop task-oriented impedance regulation policies by\nmapping the muscle synergistic contributions between two demonstrators.\nFinally, we propose a wholebody impedance controller for a human-like robot,\ncoordinating joint outputs to achieve the desired impedance and reference\ntrajectory during task execution. Experimental validation was conducted through\na collaborative transportation task and two interactive Tai Chi pushing hands\ntasks, demonstrating superior performance from the perspective of interactive\nforces compared to a constant impedance control method.",
        "Active systems of self-rotating elements inherently exhibit chirality, making\nthem of fundamental interest due to parity violation. Using large-scale\nhydrodynamic simulations, we investigate the gelation of adhesive spinners\nconfined to quasi-2D monolayers at low Reynolds numbers. Unlike the coarsening\ndynamics of passive colloids, spinner gelation follows a different pathway,\ndisplaying structural chirality during the early stages of aggregation.\nHowever, this chirality dissipates upon dynamical arrest, resulting in a final\ngel structure that resembles a conventional colloidal gel. As a result, we find\nno sign of odd mechanical responses. Nonetheless, the elastic modulus and\ngelation time remain tunable through spinning activity, providing a new avenue\nfor the bottom-up design of programmable soft materials.",
        "Magnetic and magnetocaloric properties of Y$_{1-x}$Gd$_{x}$Co$_{2}$\ncompounds, where x = 0.2, 0.4, 0.6, 0.8 and 1.0, were investigated\nexperimentally and theoretically. Crystal structures were characterized by\nX-ray diffraction (Rietveld analysis) and investigated samples possess the\nMgCu$_{2}$-type single phase with Fd-3m space group. Melt-spinning process\nintroduced a chemical and topological disorder, which directly affected the\nmagnetic properties. Refrigerant capacity (RC), strictly connected to the full\nwidth at half maximum $\\delta$TFWHM of the $\\Delta$S$_M$(T) curve and the\nmaximum of magnetic entropy changes $\\Delta$S$_{Mpk}$(T)(T,$\\Delta$H),\nincreases from 29 to 148 J\/kg with replacement of Y by Gd atoms from x = 0.2 to\nx = 0.8. RC and $\\delta$TFWHM indicate the presence of disorder. Temperature\ndependences of magnetic entropy change $\\Delta$S$_M$(T,$\\Delta$H) and RC were\nmeasured in as-quenched and annealed state for Y$_{0.4}$Gd$_{0.6}$Co$_{2}$.\nThis particular composition was chosen for detailed investigation mainly due to\nits Curie point (T$_C$ = 282 K), which is close to the room temperature. After\nisothermal annealing ($\\tau_a$ = 60 min, Ta = 700$^o$C) RC decreased from 122\nto 104 J\/kg, which clearly indicates the homogenization of the heat treated\nsample. Furthermore, observed inverse magnetocaloric effect is associated with\nthe presence of antiferromagnetically coupled Gd and Co magnetic moments. The\nphase transition temperature increases with increasing Gd content from 74 to\n407 K for Y$_{0.8}$Gd$_{0.2}$Co$_{2}$ and GdCo2, respectively. Within the\nFPLO-LDA DFT method, the non-magnetic ground state for YCo$_{2}$ and the\nmagnetic ground state for GdCo$_{2}$ are predicted in agreement with\nexperiment. The dependence of calculated total and species-resolved magnetic\nmoments on Gd concentration reasonably agrees with available experimental data.",
        "The rise of foundation models fine-tuned on human feedback from potentially\nuntrusted users has increased the risk of adversarial data poisoning,\nnecessitating the study of robustness of learning algorithms against such\nattacks. Existing research on provable certified robustness against data\npoisoning attacks primarily focuses on certifying robustness for static\nadversaries who modify a fraction of the dataset used to train the model before\nthe training algorithm is applied. In practice, particularly when learning from\nhuman feedback in an online sense, adversaries can observe and react to the\nlearning process and inject poisoned samples that optimize adversarial\nobjectives better than when they are restricted to poisoning a static dataset\nonce, before the learning algorithm is applied. Indeed, it has been shown in\nprior work that online dynamic adversaries can be significantly more powerful\nthan static ones. We present a novel framework for computing certified bounds\non the impact of dynamic poisoning, and use these certificates to design robust\nlearning algorithms. We give an illustration of the framework for the mean\nestimation and binary classification problems and outline directions for\nextending this in further work. The code to implement our certificates and\nreplicate our results is available at\nhttps:\/\/github.com\/Avinandan22\/Certified-Robustness.",
        "Experiments involving sensory analysis of foods and beverages are beneficial\nfor selecting healthy products and assessing the preferences of potential\nconsumers. They are generally planned in incomplete blocks, and their\nattributes, such as aroma, colour, and flavour, are evaluated using a 9-point\nhedonic scale, characterising an ordinal variable response. Also, the\ngeneralised logit model with random effects for panellists is one of the\nappropriate models to relate the multivariate response to the covariates. This\nstudy aims to present a method for analysing sensory attributes through a\nunified multivariate model. Due to the nature of the variable, each separate\nmodel already corresponds to a multivariate analysis, so our proposal would\nincorporate a complete analysis with solely one model. This proposal is based\non multivariate methods for categorical data and maximum likelihood theory. Our\nmethod was evaluated through a simulation study, in which we consider three\ndistinct formulations with two attributes to represent various formulation\nselection scenarios via mixed discrete models. The simulated results\ndemonstrated overall concordance rates exceeding 80\\% for the unified model\ncompared to the separate models. Moreover, as motivation is presented, a study\nof 13 prebiotic beverages based on cashew nut almonds added to grape juice,\nwith 130 potential consumers. The attributes evaluated were overall impression,\naroma, Body, sweetness and flavour, using a 9-point hedonic scale. The selected\nunified model considering all attributes was the non-proportional odds\nmixed-effect model. According to this model, the prebiotic beverage\nformulations most likely to be accepted were: 8\\% sugar and 40\\% grape juice\n($F_4$), 6\\% sugar and 44\\% grape juice ($F_6$), and 9\\% sugar and 30\\% grape\njuice ($F_{13}$). The unified analysis and computational time showed the\nadvantages of this proposal.",
        "This study explores the implementation of the Quantum Approximate\nOptimisation Algorithm (QAOA) in its analog form using a neutral atom quantum\nprocessing unit to solve the Maximum Independent Set problem. The analog QAOA\nleverages the natural encoding of problem Hamiltonians by Rydberg atom\ninteractions, while employing Bayesian Optimisation to navigate the\nquantum-classical parameter space effectively under the constraints of hardware\nnoise and resource limitations. We evaluate the approach through a combination\nof simulations and experimental runs on Pasqal's first commercial quantum\nprocessing unit, Orion Alpha, demonstrating effective parameter optimisation\nand noise mitigation strategies, such as selective bitstring discarding and\ndetection error corrections. Results show that a limited number of measurements\nstill allows for a quick convergence to a solution, making it a viable solution\nfor resource-efficient scenarios.",
        "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.",
        "Using the pinch technique, we compute the one-loop vertices of weak\ninteractions in the B-LSSM and incorporate their pinch contributions into the\ngauge boson self-energies. Compared to the definitions of the $S$, $T$, and $U$\nparameters in the Standard Model based on the $SU(2)_L\\otimes U(1)_Y$ group,\nthe corresponding parameters in the B-LSSM are modified. We provide these\nredefined $S$, $T$, and $U$ parameters and demonstrate the convergence of the\nresults. In the framework of the low-energy effective Lagrangian for weak\ninteractions, the $S$, $T$, and $U$ parameters can be expressed as functions of\ncertain parameters in the B-LSSM. The updated experimental and fitting results\nconstrain the parameter space of the B-LSSM strongly.",
        "This paper introduces IRIS, an immersive Robot Interaction System leveraging\nExtended Reality (XR), designed for robot data collection and interaction\nacross multiple simulators, benchmarks, and real-world scenarios. While\nexisting XR-based data collection systems provide efficient and intuitive\nsolutions for large-scale data collection, they are often challenging to\nreproduce and reuse. This limitation arises because current systems are highly\ntailored to simulator-specific use cases and environments. IRIS is a novel,\neasily extendable framework that already supports multiple simulators,\nbenchmarks, and even headsets. Furthermore, IRIS is able to include additional\ninformation from real-world sensors, such as point clouds captured through\ndepth cameras. A unified scene specification is generated directly from\nsimulators or real-world sensors and transmitted to XR headsets, creating\nidentical scenes in XR. This specification allows IRIS to support any of the\nobjects, assets, and robots provided by the simulators. In addition, IRIS\nintroduces shared spatial anchors and a robust communication protocol that\nlinks simulations between multiple XR headsets. This feature enables multiple\nXR headsets to share a synchronized scene, facilitating collaborative and\nmulti-user data collection. IRIS can be deployed on any device that supports\nthe Unity Framework, encompassing the vast majority of commercially available\nheadsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and\nthe HoloLens 2. IRIS showcased its versatility across a wide range of\nreal-world and simulated scenarios, using current popular robot simulators such\nas MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study\nevaluates IRIS on a data collection task for the LIBERO benchmark. The study\nshows that IRIS significantly outperforms the baseline in both objective and\nsubjective metrics.",
        "Residual moveout (RMO) provides critical information for travel time\ntomography. The current industry-standard method for fitting RMO involves\nscanning high-order polynomial equations. However, this analytical approach\ndoes not accurately capture local saltation, leading to low iteration\nefficiency in tomographic inversion. Supervised learning-based image\nsegmentation methods for picking can effectively capture local variations;\nhowever, they encounter challenges such as a scarcity of reliable training\nsamples and the high complexity of post-processing. To address these issues,\nthis study proposes a deep learning-based cascade picking method. It\ndistinguishes accurate and robust RMOs using a segmentation network and a\npost-processing technique based on trend regression. Additionally, a data\nsynthesis method is introduced, enabling the segmentation network to be trained\non synthetic datasets for effective picking in field data. Furthermore, a set\nof metrics is proposed to quantify the quality of automatically picked RMOs.\nExperimental results based on both model and real data demonstrate that,\ncompared to semblance-based methods, our approach achieves greater picking\ndensity and accuracy."
      ]
    }
  }
]